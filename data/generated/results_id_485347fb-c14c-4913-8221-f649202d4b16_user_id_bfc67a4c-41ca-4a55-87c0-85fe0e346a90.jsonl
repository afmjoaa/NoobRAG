{"question":"atomic clock sync vs bamboo ci server time notifications - features comparison","answer":"Atomic Clock Sync offers direct synchronization with NIST atomic time servers, includes a repair utility for Windows Time Service, and allows setting synchronization intervals. In contrast, the Bamboo CI server lacks built-in time sync features, though it could potentially benefit from adding notifications to alert admins about agent clock drift, which currently must be manually monitored and can cause build version conflicts when drift occurs.","context":["If your Windows is not synchronizing system time correctly, use these freeware to force Windows to sync time at every startup. By default Windows 11/10/8/7 syncs your system time with Internet servers on a weekly basis. And it does the job well. But there are users who report that the Time Synchronization fails because Windows Time Service was not working, or that Windows System Time jumps backward. If you face this issue, you can consider using free Time synchronization software on your Windows system.\nFree Time synchronization software for PC\nThis time synchronization issues with your computer may be caused due to some changed settings or the interference dues to a 3rd-party software or due to a faulty CMOS battery or other hardware issues. When Windows fails to synchronize your system time correctly, this might result in the wrong date and time being displayed every time you turn on your computer – and that could cause several computing or browsing problems. To fix this issue, we have a lot of third-party time synchronization software that can update your system time from the internet.\n1] NetTime or Network Time\nNetTime is a free open source tool that works upon the Simple Network Time Protocol (SNTP). To get started with this tool, all you need is the address of time servers you want to synchronize your system time with. Although, the tool comes preloaded with the default NetTime time servers apparently my computer was unable to connect to some of them. You can add up to five Time servers by specifying their IP Address or Hostname.\nNetTime supports communication over three protocols, namely SNTP, TCP, and UDP. So you can choose the protocol as per your time server. The tool is designed to automatically keep the system time in sync. So once you’ve configured the tool, it will automatically sync the time in the background. You can also force sync the time or specify the interval after which the time should be synced again. There are few other settings that you can customize as per the requirement. Click here to download NetTime.\n2] Atomic Clock Sync\nAtomic clocks are considered to be the most precise clocks ever built. These devices are standard for international time distribution services. These services are then used in television broadcasts, satellites, etc. If you ever wanted to sync your computer’s time to an atomic clock’s time, then you might be interested in Atomic Clock Sync.\nThis tool can help you sync your computer’s time with the atomic time servers maintained by the National Institute of Standards and Technology (NIST) in the United States. The time provided by these servers is considered to be the most accurate and correct. You can specify a synchronization interval and also force synchronize at any time. Not many customization features are available, but the tool does come with a repair utility that can help you fix Windows Time Service if anything goes wrong. Click here to download Atomic Clock Sync.\n3] Dimension 4\nDimension 4 is yet another time synchronization tool available for Windows. It works in a way somewhat similar to NetTime. The thing I liked the most about Dimension 4 was the inbuilt list of worldwide time servers. The tool comes with a built-in list of time servers with their location. So you need not search for time servers online. Dimension 4 comes with all the advanced and basic features to keep Windows time in sync with the best time servers. Click here at thinkman.com to download Dimension 4.\nThese were some of the tools that can help you keep your computer’s time in sync with the international servers. Do you use any other tool? If so, let us know.","You know what they say, time is money. But for us engineers, time needs to be correct. When it isn’t you lose a lot of it and thus money. Trouble with the time can even pop-up in unexpected places: Your lab.\nThe lab is a playground for software engineers where they get things done. Viewed by corporate IT as a necessary evil and block from the rest of the world. IT will generally give zero to no support for machines in the lab, so the engineers are left maintaining the machines themselves. Although they are creative, they are not the best operators.\nThe last thing I came across in our lab was trouble with the clock. The last months I noticed, when we had rapid build/test cycles, the latest build was not picked up by our machines that we’re running the tests on. In general it was OK, but when you needed it the most it sometimes failed to pick up the most recent build.\nWe have a fairly complex setup with different servers and agents. We have our Bamboo CI-server pushing our artifacts to the repository. On our build agents will let Maven poll the repository for the latest artifacts. The Maven Parent POM’s on our build agents are slightly different from the POM’s that the developers have on their machines. The agent POM’s allow getting SNAPSHOT build so we can move our not-released artifacts to our agents, for further processing and testing.\nOver the past months the problem got worse so it was time to start investigating. The problem was quickly found: clock drift. Because our agents where in an IP range that had no access to the internet the default time sync servers where not accessible. In about half a year we had drift of about 30 minutes. So if we had a build/test cycle in that 30 minute slot we where testing an older release. Lets go over what happened:\n- Bamboo Server (14:30) -> Repo (14:30)\n- Agent (14:35 + 30m drift) asks for new artifact, my last is from 9:12?\n- Repo -> I got a newer file (14:30)\n- Agent downloads, and saves to disk (14:35 + 30m drift = 15:05)\n- Bamboo Server (14:45) -> Repo (14:45)\n- Agent (14:50 + 30m drift) asks for new artifact, my last is from 15:05?\n- Repo -> My file is older, it’s from (14:45)\n- Agent uses the previous older version\nThe biggest problem was that this went unnoticed for a long time, because the problem only occurred when a new cycle is started on the same agent (we have multiple agents) within the slot created by the clock drift.\nThe solution is simple though, use one of your server that have access to the standard time-server and use that as a delegate for the sync requests.\n# in not yet installed sudo yum -y install ntp # sync the time from your internal NTP server sudo /usr/sbin/ntpdate -v 192.168.42.42 # edit the ntp config and start/restart the NTP deamon sudo vim /etc/ntp.conf # set server to 192.168.42.42 sudo /etc/init.d/ntpd start sudo /sbin/chkconfig ntpd on\nMaking sure that the clocks of your machines are in sync is not only a matter for the production server. It’s also important in your lab. It would be easier if IT didn’t disconnect the lab from the rest of the world, but this is just reality. We now added a new item on our agent build checklist: Make sure the machines a synced automatically with a sync server it can access. Could be an interesting feature for our Atlassian Bamboo CI server though: Send a notification to the admins that the agents have clock drift."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:146c970d-076c-4c50-bd1c-216c251ce4d0>","<urn:uuid:10748c6f-3df4-4b0e-af13-837d35a16e6e>"],"error":null}
{"question":"How do the objective sorting filter and the direct painting technique differ in their approaches to creative decision-making? Could you contrast their key principles and limitations in artistic contexts?","answer":"The objective sorting filter and direct painting technique represent contrasting approaches to creative decision-making. Sorting is an objective filter that creates strictly two groups based on measurable criteria - for instance, sorting paintings based on whether they're wider than 20 inches. There's always a clear right or wrong way to sort. In contrast, direct painting (alla prima) requires simultaneous consideration of multiple aspects - location, size, shape, modeling, color, and how each element relates to surrounding colors. While sorting follows rigid, objective rules, direct painting demands that artists deal with all problems at once, making complex decisions about each stroke. Though some artists later retouch their direct paintings, the core principle remains trying to achieve the final effect immediately with each stroke, rather than following a binary sorting system.","context":["Organization starts in the mind. When we develop a sense of order in our minds and apply it to the world, we organize. Interestingly, our sense of order has predictable attributes. The first pair of attributes describes perspective, which can be subjective or objective. The second pair of attributes determine if we organize into two groups or many groups. A perspective combined with the ability to form groups is called a filter. Our two perspectives—objectivity and subjectivity—when combined with our ability form groups—two groups and many groups—reveal four fundamental filter types: sorting, categorizing, branding, and curating. These four filters are the fundamental ways we organize content and are revealed by The Content Filter Framework.\nYou must be “this tall” to ride—no shoes, no service—needle in a haystack—wheat from the chaff; each of these phrases imply sorting. Without the ability to sort we would eat food that is both fresh and rotten, scald ourselves on stove-tops that are both on and off, and sit on park benches with paint that is both wet and dry: We simply couldn’t sort them out.\nSorting is an objective filter that yields 2 groups, like an “in” and an “out” group. For example, you could sort a collection of paintings based on the objective measurement of 20 inches. If a painting’s width is greater than 20 inches it would be “in” while all narrower paintings are “out”. Because the filter criteria is objectively measurable—20 inches—there is a right and wrong way to sort.\nA place for everything and everything in its place—Is it vegetable, mineral, or animal? What industry do you work in? These expressions rely on categorizing. Without the ability to categorize, our world would be chaotic as simple concepts like putting things away or alphabetizing cease to function.\nCategorizing is an objective filter that yields 3 or more groups of content, akin to the phrase “this, that, and the other thing”. Considering the same paintings as before, you could categorize them based on their medium type, such as “oil” and “acrylic”. If a painting is oil based, it would fall into the “oil” category and if it’s “acrylic” based, it would fall into the “acrylic” category. All other medium types, like watercolor, ink, or pastels could be “other”. Because the filter is objectively measurable—medium type—categorization lends itself to right and wrong answers.\nEditorial & Branding\nAll the news that’s fit to print—family friendly entertainment—the ultimate driving machine; these are examples of editorial & branding. Without them, all the media you consume and every purchase you make may not be of good quality. Imagine buying tomato soup from a bare, silver can with “tomato soup” written on it with a Sharpie. Would it be safe to eat? Would it be tasty? Would one can of soup taste the same as the next? Absent a brand, such as Campbell’s, you can’t be sure what you will get. A brand sets an expectation of quality, like an editor at a newspaper. Soup buyers rely on the Campbell’s brand to only sell soup that meets their standard. And while Campbell’s standards may not meet your own, that would be a subjective difference in opinion.\nEditorial & branding is a subjective filter that yields 2 groups of content—an “in” and an “out” group. Because the filter is subjective, it requires the “human element” of emotion, creativity, imagination, and culture, to name a few. For example, consider a museum with the mission to display important paintings, like the Metropolitan Museum of Art (MET). If given the opportunity to display the Mona Lisa on loan, the MET would jump at the chance while, if given the opportunity to display a Power Puff Girls portrait, they may consider it insignificant. If the brand, however, is not the MET, but rather the Cartoon Network (CN), then the CN may arrive at the opposite conclusion and consider the Power Puff Girls portrait a great addition and the Mona Lisa irrelevant. It’s possible for two brands to have an identical mission, to “display important paintings”, but because “important” is subjective, the brand yields different results.\nBeauty is in the eye of the beholder—one man’s trash is another man’s treasure—that’s not my cup of tea; these phrases rely on the concept of curation. Without curation, we lose the ability to find content interesting, attractive, and likable in different ways. It allows us to avoid a “police state” of preferences where everything is either “good” or “bad”.\nCurating is a subjective filter that yields 3 or more groups of content. Like the branding filter, curation is subjective and requires the “human element”. Unlike branding, curation acknowledges that people appreciate content in complex ways. For example, someone may want to collect paintings in meaningful ways. The Mona Lisa may be put into a collection of “classic” paintings while the Power Puff Girls portrait is arranged with “happy” art. One could imagine a collection called “painted women” that includes both the Mona Lisa and the Power Puff Girls, or a collection called “modern art” that includes neither. Existing outside the rigid confines of “good” or “bad”, the curation filter groups content in complex ways that resonate with people. Like our ancestors who collected beautiful, useful, and fashionable shells, so we curate collections in meaningful ways.\nImage Credit: the picture of the four buttons on the homepage leading to this article was made by Megan Gouws.","MATERIALS & METHODS - Painting - Oil Painting\nCharacteristics - Painting Methods & Techniques - Materials and Equipment - Work Space & Storage - Manufacture of Pigments - Protection of the Picture\nDirect Painting /Alla Prima or Premier Coup\nDirect methods have been used since ancient times, and the work done during the earliest periods in most cultures is single-layer direct painting. More recently, in Italy in the sixteenth century, in Holland in the seventeenth century, and in France in the nineteenth and twentieth centuries, direct painting techniques have been vehicles for a wide range of pictorial ideas, from the rich sparkle of Frans Hals, to the color directness of Manet, and the frank immediacy of much of the Synthetic Cubist work\nUsing the direct method the painter, having visualized the way that a portion of the picture should work, attempts to get the effect completely, all at once, without [p. 76] planning to paint over it when it is dry to make it darker or lighter, or warmer or cooler. As each succeeding part is treated, the artist tries to maintain the same pitch, first planning or sensing the final impact of the passage and then trying to bring it to completion on the first attempt. As the work progresses, each stroke must relate accurately to every other color that has already been put on the picture. Ideally when the last bit of bare canvas is covered, the picture is finished, and no retouching is needed.\nThe great difficulty and challenge of pure direct painting is that the painter must be able to deal with all the problems of the picture at the same moment. For example if the artist is painting a head, when the chin is being painted, each stroke of color must be put on the canvas so that it states simultaneously the location, size, and shape of the chin; the modeling or volume of the chin; the color of the chin under the given circumstance of light; and the way the color unites with all the other colors surrounding it in the picture.\nNaturally very few painters have felt bound to adhere strictly to pure direct methods, and so, many pictures, begun in the spirit of alla prima paintings, are retouched, corrected, and elaborated upon after the first layer of paint has dried. However, it is my opinion that the painting is still direct in spirit to the extent that the artist tries to make each stroke count as the final effect. If it is later decided that a particular passage is unsatisfactory, the painter may obliterate the faulty section by scraping it out or by painting an opaque neutral tone over it. The artist may than repaint that area, trying once more to realize the final effect immediately. Technically this may not be direct painting in a single layer or on the first try, but the thought process and range of effects nevertheless relate to the spirit of direct painting. [p. 76-77]\nThis section . . . . is intended as a general guide and not as a substitute for the personal instruction of an experienced artist. The procedures of other artists and traditions may be useful when they serve as a base from which experiments may be conducted consistent with the individual aims of the artist.\nIn the case of direct painting, from a technical point of view the procedure is often kept quite simple.\nl. On a clean canvas, the general location of large masses can be put in lightly with pencil or charcoal.\n2. The palette is set with a complete range of the colors to be used during the sitting, the colors usually being placed near the outer edge of the palette. The palette cup is fastened to the edge of the palette, and a small amount of painting medium is put into it.\n3. The colors are thinned on the palette, as they are used, by dipping a brush into the palette cup, bringing out the desired amount of painting medium, and mixing this with a brushful of paint. Colors may be intermixed, although most writers suggest that the mixtures should be kept as simple as possible--that is, they should consist of no more than two or three colors.\n4. Colors are applied to the canvas with brushes, palette knives, rags, sponges, or any other instruments. Unsuccessful passages may be removed by scraping them off with the palette knife.\nSome painters begin with the darks and gradually work up to the lights; other reverse the procedure. Some employ brilliant intensities from the beginning; other begin with neutral tones. The organization of the picture is, of course, a highly personal matter, and a method that is most suitable and efficient for one painter may be a total waste of time for another. [p. 78]\n[Kay, Reed. The Painter's Guide to Studio Methods and Materials. Englewood Cliffs, NJ: Prentice-Hall, Inc., 1983. pp. 127-129]\nThe contents of this site, including all images and text, are for personal, educational, non-commercial use only. The contents of this site may not be reproduced in any form without proper reference to Text, Author, Publisher, and Date of Publication [and page #s when suitable]."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:843b8be7-344f-49d3-8944-13783242be3b>","<urn:uuid:f923c921-b521-49ee-8713-e0efa0cedd26>"],"error":null}
{"question":"What are the similarities between how the human genome defends against retrotransposons and how the body's immune system fights viruses?","answer":"Both defense systems involve specialized proteins that recognize and neutralize threats. In retrotransposon defense, KZNF proteins act like \"beat cops\" that patrol the genome looking for specific retrotransposons, then recruit KAP1 to suppress their activity. Similarly, in viral defense, the immune system uses B cells to produce antibodies that specifically recognize and bind to molecules on virus surfaces to neutralize them. Both systems also show evidence of ongoing evolutionary adaptation - KZNFs have rapidly evolved with around 400 variants in primates including 170 primate-specific innovations, while viral vaccines need regular updating because viruses frequently mutate to evade immune system recognition. Additionally, both systems can sometimes affect more than just their targets - KZNFs suppress genes adjacent to retrotransposons, while immune responses to viruses can affect healthy cells.","context":["A moth evolves ears that can hear the sonar of bats, and bats adapt by hushing their calls to whispers. A newt evolves powerful poisons that can kill would-be predators, and a snake evolves immunity to those poisons. A gazelle becomes faster to outrun its hunter, and a cheetah becomes faster still. The natural world is full of these evolutionary arms races—endless battles where one party’s adaptations are met by counter-adaptations from its opponent. Both sides move in and out of check, changing all the time but locked in a perpetual stalemate.\nThe human genome is engaged in a similar evolutionary arms race… against itself.\nThe opponents are jumping genes called retrotransposons that can hop around the genome. They increase in number by copying themselves and pasting the duplicates into new locations. This mobile lifestyle is so successful that retrotransposons make up more than 40 percent of the human genome. Some have settled down, and are now static shadows of their once-active selves. Others are still on the move.\nIf the copies land in the right place, they could act as clay for building new adaptations. If they land in the wrong place, which is perhaps more likely, they could cause diseases by disrupting important genes. So genomes have ways of keeping these wandering sequences under control. One involves a gene called KAP1. It’s a kind of tranquiliser—it sticks to retrotransposons and stops them from activating.\nKAP1 works differently in different species, targeting those retrotransposons that are active in that owner’s genome. Our KAP1 won’t keep a mouse’s jumping genes in line, and vice versa. Some scientists believe that this specificity is caused by another group of genes called KZNFs. They tell KAP1 where to go by searching for, and sticking to, specific retrotransposons. They’re like beat cops that patrol a neighbourhood, look for crime, and radio for back-up. Each KZNF targets a different type of retrotransposon and different species have their own set.\nAt least, that’s what happens in theory. In reality, it has been hard to confirm this idea, partly because these cops do such a good job that it’s hard to see jumping genes in action.\nFrank Jacobs and David Greenberg from the University of California, Santa Cruz solved this problem by sticking the retrotransposons in mouse cells—a less policed environment. They filled the mouse stem cells with a single human chromosome. Mice are adapted to control their own retrotransposons, so they’re oblivious to ours. The jumping genes on the human chromosome, freed from their usual restraints, started spreading, much like an invasive species running amok on an island with no native predators. Now, the team could pit different human KZNFs against these restless genes to see if any could bring them to heel.\nThey found two that could—ZNF91 and ZNF93. Each of these represses a major class of retrotransposons—SVAs and L1s, respectively—that are still jumping about in the human genome today.\nZNF91 and ZNF93 are only found in primates, but they have changed a lot even without our narrow lineage. For example, the human version of ZNF91 has deluxe features that are shared by gorillas but not by monkeys. To understand the value of these changes, Ngan Nguyen and Benedict Paten took the modern genes and worked backwards, reconstructing their ancestral versions at different stages of their evolution.\nThey found that between 8 and 12 million years ago, ZNF91 gained features that dramatically improved its ability to keep retrotransposons in line. That’s the point in primate evolution before humans diverged from gorillas and chimps. ZNF93 went through similarly dramatic changes between 12 and 18 million years ago, before the we (and the other great apes) diverged from orang-utans.\nThese results suggest that ape KZNFs have rapidly evolved to keep jumping genes in check. Indeed, the KZNFs are one of the fastest growing families of primate genes. We have around 400 of them, and some 170 of these are primate-only innovations. This expanded police force reflects our ongoing genomic arms race.\nAnd the jumping genes are starting to fight back. For example, the team found that ZNF93 represses L1 genes by recognising a short signature sequence that most of them have. But some L1s, especially the most recently evolved ones, have lost this signature entirely. They can jump unnoticed.\nThe missing sequence would normally makes the jumping genes better at jumping. But this booster rocket ended up as a wheel clamp, since ZNF93 evolved to recognise it. So some of the L1s lost the rocket. They jumped less effectively, but at least they could still jump.\nThis is a classic evolutionary arms race. The hosts thrusts, the parasite parries, and the duel continues. But unlike more familiar battles between snakes and toads, or hosts and viruses, this is a case where we’re waging war against our own DNA.\nThere’s a sense of futility about this. Much of our genome seems to be engaged in an ultimately pointless duel whether neither side can give or gain any ground. But these battles aren’t quite as fruitless as they might seem.\nThe team found that KZNFs partly suppress the genes around a retrotransposon too. When the cops finds their target, they tell all the bystanders to the lie on the ground too. This is important because it seriously affects the activity of many human genes, beyond retrotransposons. It means that KZNFs can eventually be used to control the activity of genes that jumping genes land next to. (“Excuse me, officer, but while you’re manhandling your suspect, would you mind also rescuing my cat?”) This arms race could have given rise to more complicated networks of genes, and perhaps more complicated bodies or behaviours.\nReference: Jacobs, Greenberg, Nguyen, Haeussler, Ewing, Katzman, Paten, Salama & Haussler. 2014. An evolutionary arms race betweenKRAB zinc-finger genes ZNF91/93 and SVA/L1 retrotransposons. Nature http://dx.doi.org/10.1038/nature13760\nMore on jumping genes","Genomics and Virology\nViruses are all around us. What are they and how can they affect human health?\nThe Big Picture\nViruses are bundles of genetic material wrapped in a protein coat that can infect living things. Viruses cause damage by hijacking a host cell's machinery to make copies of themselves, often disrupting normal cell function.\nViral vaccines can protect individuals from contracting and spreading common diseases caused by viruses, such as the flu, measles and COVID-19.\nWhat is a virus?\nViruses are tiny infectious particles that are halfway between living and nonliving organisms. They are so small (a millionth of a millimeter) that it would take hundreds to thousands of them to cover the end of a human hair. Each virus is composed of genetic material wrapped in a protein coat. Viruses that infect plants and animals also have a layer of fat molecules. Viruses cannot reproduce on their own. Instead, viruses replicate by infecting a host cell (such as humans, other animals, plants or bacteria), hijacking the host's biological machinery and turning the host cell into a virus-producing factory.\nWhat are viruses made of?\nMost viruses have the same basic structure:\na genetic information molecule in the form of nucleic acids such as DNA or RNA.\na protein layer, or coat, that surrounds and protects the nucleic acids.\nThe protein layer allows viruses to fuse with the outer layer of the cells they attack. The nucleic acid portion encodes genes to make proteins that are essential for the virus to function. These proteins direct viral replication and carry out other activities, such as evading host defenses.\nHow many viruses exist on Earth?\nResearchers estimate that 10 nonillion (10 followed by 30 zeroes) individual viruses exist on Earth. If all the 1030 viruses were organized in a single-file fashion, they would stretch for over 100 million light-years (a single light-year is 6 trillion miles) — which is four times the distance from Earth to the Canis Major Dwarf, our closest galaxy!\nBut only a tiny fraction of the viruses on Earth affect humans. Approximately 200 different viruses are known to cause disease in humans, including:\nAcquired immunodeficiency syndrome (AIDS)\nViral agent: Human immunodeficiency virus (HIV)\nViral agent: Poliovirus\nViral agents: Influenzavirus A, B, C or D\nCoronavirus disease 2019 (COVID-19)\nViral agent: SARS-CoV-2\nHow are viruses transmitted?\nViruses can be transmitted in many ways, including by:\nDirect contact with an infected organism, such as being bitten by a mosquito infected with the West Nile virus. When viruses (and other pathogens) move from one species to another, it is known as a spillover event.\nIndirect contact with someone who is infected, such as through respiratory droplets from a person who is coughing or sneezing.\nAirborne or surface transmission, such as touching a surface where infectious viruses are still located minutes to hours after they landed there, can also result in viral transmission.\nHow do viruses infect living organisms?\nViruses have proteins on their surface that typically latch onto a specific molecule on the surface of a host cell, called a receptor. The viral surface molecule can be likened to a specific key, while the host cell receptor is a lock. When the key meets the lock, it opens a door for the virus to enter the cell.\nViruses enter host cells as particles. Once a viral particle enters a host cell, its nucleic acid material interferes with the host cell's functions, essentially hijacking the proteins and other materials of the host cell to make more copies of the viral particles. One infected cell can release hundreds to thousands of new viral particles, with each of the new viral particles being capable of infecting another cell.\nOnce a virus successfully replicates itself, it leaves the host cell to infect other cells. Some viral infections cause no symptoms. However, when many viral particles infect an organism's cells at the same time, they may cause anything from uncomfortable symptoms to severe illness and even death.\nWhy don't all viruses cause human disease?\nThe exact reason why some viruses infect humans to cause disease and others do not remains a biological mystery. For example, humans usually die if infected by the rabies virus. But while human cells can be infected by circoviruses, they do not seem to cause disease. But circovirus infections in other mammals such as dogs and pigs can cause severe diseases.\nIf it is difficult for viruses to cause human disease, how are some able to do so?\nThere are a few major ways by which certain viruses can cause disease.\nViruses that encode information with RNA rather than DNA tend to have a higher rate of mutations. These mutations allow the viruses to be diverse in their genetic makeup, increasing the probability and pace by which they evade the human immune system.\nIn other cases, two different viruses interested in attacking the same host cell can swap regions of their nucleic acid and make a hybrid virus.\nSome viruses vastly benefit from staying inside their hosts for an extended period of time without being deadly. The lengthier the infection, the longer the virus has to adapt and spread to other hosts.\nFlu viruses can do both — mutate at a high rate and mix with other viruses.\nEach year, scientists measure and predict which versions of the influenza virus, or strains, will be prevalent around the world during the next flu season. Then, they produce an influenza vaccine that works against the new strains. These new strains usually acquire changes in their nucleic acid that make the viruses work differently in the host organism, such as by changing the viral protein coat slightly. That is why scientists must tailor vaccines to specific strains of viruses.\nHow does genomics help us understand viruses?\nGenomics is an interdisciplinary field that focuses on studying an organism's entire nucleic acid material (such as DNA or RNA), which is known as its genome. In 1977, researchers sequenced the first viral genome — phi X174, a virus that attacks bacteria.\nBut much progress has been made since then. For example, as of September 2021, there were 11,465 viral genome sequences available.\nIt is important to generate the complete genome sequence of viruses for several public health reasons. Knowing the viral sequence allows researchers to detect whether a virus is present in a host organism, and it provides clues for how a virus attacks and infects the host cell.\nViruses need to be able to use the functions of host cells to replicate themselves. Inhibiting some of the host cell's functions can potentially make viruses vulnerable. Researchers are studying genome sequences of both viruses and their hosts so as to target specific cell pathways that can be used for treatment.\nStudying viral genomes is key for understanding viral mutations and their evolution over time. Understanding viral genomes also helps researchers track outbreaks and consider how best to treat viral infections or vaccinate against a virus.\nHow can we reduce the spread of viruses?\nMasking, proper handwashing, use of hand sanitizers and social distancing reduce the spread of viruses of many viruses. Antiviral medications and vaccines can eliminate or reduce the severity of diseases caused by viruses.\nMedicines used to treat bacterial infections do not kill viruses.\nHow do viral vaccines work?\nPreviously, viral vaccines contained weakened or dead viruses, with both forms being incapable of causing disease. Now, scientists have an additional tool in their toolkit, producing vaccines using a virus's genome sequence. The viral genome has the information needed to create viral proteins, the active component of the vaccine to which the immune system responds. When injected, these DNA or RNA molecules are used by the host to produce specific viral proteins, and the immune system then recognizes the viral proteins as foreign, sparking a response from multiple types of white blood cells.\nOne such class of white blood cells, called B cells, produces a particular type of protein called an antibody. Antibodies bind to molecules on the surface of the virus and neutralize the virus to prevent it from replicating.\nOnce the human body successfully produces antibodies against a virus, its arsenal is ready for defense when the immune system comes in contact with the same virus in the future.\nWhy do some viruses affect certain people more negatively than others?\nThe exact reason why viruses affect people in different ways is under active study. Researchers attribute it to a combination of genetic and environmental factors. People with existing health conditions, such as diabetes, cardiovascular disease or cancer, are more vulnerable to a severe viral infection.\nSome individuals also have specific genomic variants that can influence how a virus interacts with their body. For example, some relatively rare genomic variants make people susceptible to severe viral and other infections. On the flip side, some genomic variants protect specific individuals from viral infections. Researchers continue to study these mechanisms, including the relationship between the level of viral infection and specific genomic variants.\nDoes our body have viral DNA that doesn’t cause disease?\nYes. The human genome contains a considerable amount of DNA that previously existed in viruses. These viral sequences are remnants of past viral infections. Most of these sequences originally came from retroviruses, a type of virus that can insert one copy of its genome into the DNA of a host organism (such as a human). As the host cells make copies of its own genome, it copies the viral DNA as well. These sequences can pass from one generation to the next, becoming a permanent part of the human genome (like a fossil record).\nAt present, DNA from these retroviruses accounts for about 9% of the human genome, but most are thought to be incapable of producing new viral particles.\nLast updated: November 12, 2021"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3153dc10-86e4-4a78-bc7d-95e70d374030>","<urn:uuid:58828205-e370-4a88-8c53-4c44be012379>"],"error":null}
{"question":"How do hemp and cotton compare in terms of water usage and environmental impact for textile production?","answer":"Hemp demonstrates significantly better environmental performance compared to cotton. Hemp requires less water and land to produce, and can be grown without pesticides, while cotton's overfarming creates environmental pollution. Hemp is carbon-negative and sequesters carbon both in soil and through its fibers when used in products. Cotton, while durable and commonly used, has higher water requirements than hemp. Additionally, hemp offers extra environmental benefits by improving soil health through its strong taproot system that reduces compaction and improves drainage, while also supporting biodiversity and fighting pests naturally. Hemp can also be grown without pesticides or seed treatments, making it more environmentally sustainable than conventional cotton production.","context":["Choosing environmentally friendly fabrics is an essential step towards reducing your impact on the environment. The main factors to consider are the use of biodegradable materials, the production process, and the disposal of waste. To find out more about environmentally friendly fabrics, keep reading.\nAn excellent method to lessen the influence of your apparel is to use environmentally friendly fabrics. These fabrics are made from natural, recyclable materials and are often biodegradable. These materials also use fewer resources in production.\nLinen is grown without chemicals or pesticides and is naturally moth-resistant. It’s also soft and absorbent. Some fabrics, like linen, require minimal water to produce.\nBamboo is an eco-friendly fabric. It doesn’t require much water to grow and feels silky. Bamboo proliferates as a plant. The bamboo plant may be picked without harming it.\nAnother type of fabric that is biodegradable is modal. It is made from spinning cellulose from beech trees. It is more potent than traditional rayon and uses 10-20 times less water.\nHemp is another eco-friendly fabric. It doesn’t require much water to grow and is considered carbon-negative. It also helps to fertilize poor soil.\nUnlike cotton, hemp is a plant that requires little water and does not require pesticides. It also provides a great source of oxygen, making it an eco-friendly fabric.\nHemp is also a naturally antibacterial, hypoallergenic, and thermoregulating material. It is also durable and breathable and has high UV resistance. Hemp also provides excellent warmth and moisture retention, making it an ideal fabric for humid climates.\nHemp is a fast-growing plant that requires less water and land to produce. It also has a large tap root that penetrates deep into the soil, preventing soil erosion and aerating the ground for future crops.\nHemp is also a great source of carbon sequestration and traps about 162 kg of carbon per tonne of hemp harvested. It is a much higher yield than cotton.\nIndustrial hemp has been used for various purposes, including food, medicine, and textiles. It is also the oldest cultivated plant in the world. Its fibers have been used to weave flax since the Neolithic period.\nThe plant is fast-growing, uses little water, and is pest resistant. It can be harmful if not grown in a controlled environment. Buying fabrics made from bamboo is a great way to help the environment.\nBamboo lyocell or bamboo viscose can be used to create bamboo cloth. The latter is made using an automated procedure. Chemical processes are used to develop the latter.\nBamboo lyocell is made in a closed-loop process, which means that the chemicals used are recycled and not disposed of into the environment. Compared to the rayon production process, this method is also environmentally friendly.\nBamboo viscose is a newer version of rayon. In this process, bamboo is mechanically crushed and processed into fibers. These fibers are then exposed to sulfuric acid, which softens the strands.\nThe bamboo fiber is then processed into yarn. The yarn is spun into fabric, then woven into valuable products.\nA great way to lessen your carbon footprint is to fill your wardrobe with eco-friendly fabrics. All-natural, non-toxic ingredients and dyes are used to create these textiles. Additionally, they generate less water and energy. Your apparel will last longer as a result.\nSome of the most popular fabrics in the fashion industry include cotton, nylon, and rayon. The latter two can contain poisonous compounds that are bad for your health.\nMany eco-conscious brands now use recycled fabrics, natural fabrics, and other eco-friendly materials. They are also more durable than conventional cotton and often require less water and energy to produce.\nAnother great way to support the environment is to use fabrics that are 100% biodegradable. They are also hypoallergenic and durable.\nOther fabrics that are considered eco-friendly include hemp, linen, and silk. Silk is a strong, durable, absorbent, and excellent material. It is also very soft against the skin. It can last up to 10 years if it is correctly cared for.\nCradle to Cradle(r) certification\nMBDC certification allows companies to assess their products’ environmental and social responsibility. It determines various factors, such as how a product uses energy, water, and materials. It is also essential to identify strategies for social responsibility.\nProducts are evaluated for material health, circularity, and potential renewable energy impacts. MBDC certification has several different levels. The lowest level, or Silver, awards a product with third-party verification that it meets the Cradle to Cradle Certified Product Standard. The highest level, Bronze, grants a product with a second-party assurance that it meets the Cradle To Cradle Certified Product Standard. Products are then subject to regular standards review and assessment.\nCompanies that produce cradle-to-cradle products can increase their profits. Cradle-to-Cradle Certified Products are advantageous for both the environment and business. Outcomes are assessed for their material health, renewable energy impacts, social fairness, and ethical treatment of workers.\nAn eco-friendly wardrobe is essential to anyone concerned with the environment. Fortunately, there are many options for sustainable fabrics. These fabrics can be made from organic or recycled materials and are often produced by sustainable brands. They also require less energy and water to grow.\nOne of the most commonly known eco-friendly fabrics is cotton. Cotton is grown using less water and requires less energy than other textiles. Cotton is also known for its durability. However, when cotton is overfarmed, the pollution from the farming process affects the environment.\nAnother type of eco-friendly fabric is organic cotton. Organic cotton is produced without pesticides and requires less water than conventional cotton. Similarly, hemp is a harmful carbon fiber, which requires less water to grow.\nAnother popular eco-friendly fabric is linen. Linen is made from flax, a plant that does not require much water to grow. It is also breathable and cool to wear.","Hemp, which is also known as cannabis, is a plant that can grow in the UK without causing damage to soil and water. It’s a cash crop, providing a source of income for the British farming industry but also a way to help the environment.\nHemp has been cultivated for thousands of years for its fibrous stalk, used in many applications including clothing and paper. It’s also a highly nutritious food, containing protein, fibre, vitamins and minerals. But, due to its appearance, it has never been grown commercially. Hemp is considered to be a low crop risk due to its short lifespan and high organic yield. Its seed is also considered to be an excellent source of protein, and is now being investigated by the pharmaceutical industry as a means of rapidly producing proteins for human consumption. This article will discuss the need for hemp to be grown in the UK, and the potential benefits of doing so.\nIndustrial hemp could be a valuable tool for UK agriculture, helping with the transition to more sustainable practices and providing a new crop, according to a farmers’ group.\nTo further explore the potential of hemp, five farmers are working with researchers from Cranfield University and the British Hemp Alliance as part of the Association of Soil Scientists’ Innovative Farmers programme.\nThey plan to conduct on-farm field trials to gather science-based data on the ecological benefits of this crop.\nHemp has long been stigmatized as belonging to the cannabis family (Cannabis Stiva L strains have low THC content and are not used for drugs), hemp requires no agrochemicals.\nResearchers say there is growing evidence that it can help increase biodiversity, fight pests, improve soil and sequester carbon.\nIt could also be a useful alternative to rapeseed, which is becoming increasingly difficult to grow in the UK.\nHowever, unlike many other countries, industrial hemp in the UK is still classified as a controlled drug and farmers wishing to grow the plant must apply for a licence from the Home Office.\nAs a result, there are only about 20 licensed producers in the UK with a total area of 2 000 hectares.\nWith post-Brexit agricultural policy focused on delivering public goods such as biodiversity and carbon sequestration, we need to rethink the approach to cannabis cultivation in the UK, says Nathaniel Loxley, project coordinator at Innovative Farmers and co-founder of the British Hemp Alliance.\nHemp could be a very valuable tool in the arsenal of UK farmers, but the UK is currently lagging behind internationally and there is a distinct lack of data in the UK context, said Mr Loxley.\nWe hope that by better understanding the benefits of cannabis and gathering evidence of it, we can pave the way for more producers in the UK.\nIndustrial hemp is a versatile plant used for building materials, clothing textiles, animal bedding and as a plastic substitute.\nManufacturers in the construction, fashion and packaging industries are increasingly looking to the factory for environmental solutions.\nBritain has a long history of hemp cultivation, and during the reign of Henry VIII hemp was so important to the British Navy that landowners were not allowed to grow it.\nHenry Ford even built a car from and with hemp, and the plant was used to remove pollutants after the Chernobyl nuclear disaster.\nMore and more benefits are being discovered from this incredibly versatile plant, says Helen Oldis, Innovative Farmers program manager.\nThere is real potential for farmers in the UK to use cannabis to restore their land, deliver public goods and grow a new crop, so it’s great that Innovative Farmers can get involved.\nIt is vital that farmers and researchers work together on these on-farm trials so that we can determine the true environmental benefits of cannabis and share this knowledge with more farmers.\nCurrently, most of the evidence for the environmental benefits of cannabis is anecdotal or from a limited number of studies in the U.S. and Europe, said Dr. Linda Dix, a researcher at Cranfield University who has conducted research on the plant.\nIt’s been shown to be good for the environment, both on the ground and above ground, but it could potentially provide other environmental benefits, and that’s something we need to understand, says Dr. Dix.\nOne of the areas to be studied is the carbon sequestration potential of cannabis. According to Dr Dix, the plant has the potential to store carbon in the soil and, through the use of its fibres in biocomposite materials, for example for construction or the automotive industry.\nData from other studies show different rates of carbon sequestration, likely related to soil types. So we need to know what that would look like under British conditions, she says.\nIt also has a strong taproot that can contribute to soil health by reducing compaction, improving drainage and aerating the soil.\nWhen the roots eventually break down, the carbon should provide a more stable soil structure.\nStudy participant Nick Voise and his company East Yorkshire Hemp are at the forefront of cannabis cultivation and product innovation in the UK.\nFifteen years ago, he converted most of his farmland to cannabis and now has 450 acres planted with cannabis.\nMr Voaz processes the harvest into fibre, animal bedding, HempLogz (firewood briquettes) and hemp concrete (for construction).\nAlthough we’re not an organic farm, we grow cannabis without pesticides, seed treatments, whatever – it works without them, says Voaz.\nCurrently, there are no fungal diseases or insect pests causing significant damage.\nThat makes it very useful for insects, and at this time of year cattails often congregate on the hemp, he said.\nI’m also interested in the carbon sequestration potential – if we turn hemp into hemp concrete, we may be sequestering carbon as long as the building stands.\nCamilla Heiselden-Ashby from Kent is growing cannabis for the first time on her family’s 320-acre mixed farm. She plans to add hemp to the farm’s crop rotation.\nWhen hemp takes root, it gets a very dense canopy, which shades the weeds – we like that, because we have a problem with quackgrass, says Haiselden-Eshby.\nThe roots of the plant go deep, so they take up nutrients from depth and make them available for future crops.\nI hope that by collecting data on the environmental benefits of cannabis, we can facilitate the licensing of cannabis.\nIt is not a dangerous drug, but a crop with great potential, and farmers should be encouraged to grow it, just as they encourage the cultivation of legumes to improve the soil.\nAll the farms participating in the Innovative Farmers survey are licensed cannabis growers and were selected to represent a wide range of different climatic, topographical and soil conditions in England.\nThey will grow commercial hemp at two sites, including a control plot, and analyze the soil for organic carbon content, soil structure, soil biology and biomass nutrients. They will also conduct biodiversity surveys to observe butterflies, insects and birds.\nPre-seeding for baseline data is currently underway.\nhemp treeshemp global warminghemp climate changehemp and soil carbonembodied carbon hempcretehow fast does hemp grow compared to trees,People also search for,Privacy settings,How Search works,hemp trees,how fast does hemp grow compared to trees,hemp global warming,hemp climate change,hemp and soil carbon,how much co2 does an acre of grass absorb,do different trees have different carbon sequestration abilities,embodied carbon hempcrete"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:282b7ab9-a556-4cbc-a29f-86f2f41d32f2>","<urn:uuid:eac3e8af-fbcb-4937-970e-7200e5ca1c11>"],"error":null}
{"question":"What are the technological advances in LIDAR-based navigation systems, and how do they contribute to practical training safety in high-risk situations?","answer":"LIDAR-based navigation systems have achieved state-of-the-art performance in real-time odometry and mapping, demonstrated by implementations like the large-scale LIDAR Odometry that can map areas as large as 3km x 0.9km x 0.1km. These systems integrate multiple sensors including Velodyne VLP-16, cameras, IMU and GPS for comprehensive environmental perception. Regarding training safety, virtual reality environments provide a safer alternative for initial practical training, allowing trainees to make mistakes without real-world consequences. This is particularly valuable for high-risk situations, as students can repeat tasks multiple times at their own pace while maintaining direct interaction with trainers and receiving detailed 3D representations of objects and surroundings.","context":["Tranversibility extraction from 16-beam LIDAR\nWe extract in real-time the tranversible of an on-road vehicle under dynamic mobile situation.\nA recent collection of 3D perception\nHere is a collection of our recent results in 3D perception in the context of autonomous driving, including key object detection, path planning, local mapping, tranversibility extraction, etc.\nA recent collection of research in autonomous boat\nA recent collection of results on autonomous boat system including planning, tranversibility extraction, mapping, etc.\nLIDAR Odometry Demo\nLeft shows multi-sensors mounted on a car, Velodyne VLP-16, Occam Omni Camera IMU and GPS are equipped.\nRight is and a demo of large-scale LIDAR Odometry. The data is recorded in Hong Kong, from Hang Hau to HKUST. Total size of the map is larger than 3km x 0.9km x 0.1km.\nAuthor: Haoyang Ye, Yuying Chen, Ming Liu\nLiDAR and Inertial Fusion\nWe demonstrate the state-of-the-art performance for real-time LiDAR-based odometry.\n- Haoyang Ye, Ming Liu, LiDAR and Inertial Fusion for Pose Estimation by Non-linear Optimization (for the review of ICRA), or please visit: Link\nSocially-compliant Navigation through Generative Adversarial Imitation Learning\n- Lei Tai, Jingwei Zhang, Ming Liu, Wolfram Burgard, Socially-compliant Navigation through Raw Depth Inputs with Generative Adversarial Imitation Learning, International Conference on Robotics and Automation (ICRA), May 21-25, 2018, Brisbane, Australia, PDF bibtex dataset\nReal-time Nonholonomic Robot Navigation in Dynamic Environment\n- Yuying Chen, Ming Liu, RRT* Combined with GVO for Real-time Nonholonomic Robot Navigation in Dynamic Environment (for the review of)\n- Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Burgard, Ming Liu, Neural SLAM: Learning to Explore with External Memory, arXiv 1706.09520. arXiv\nDeep Reinforcement Learning in Robotics Navigation\n- Lei Tai, Giuseppe Paolo, Ming Liu, Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), September 24-28, 2017, Vancouver, Canada. Link PDF bibtex\nMulti-agent warehouse simulation and task allocation\n- Yandong Liu, Lujia Wang, Cheng-zhong Xu, Ming Liu, A Novel Swarm Robot Simulation Platform for Warehousing Logistics, ROBIO 2017\nRobotic engineering-aided neural engineering\nWe built and validated a high-speed high-precision tracking system for mouse test subjects, which aids neural engineering experiments on animals. The result is published recently on Biomaterials Link\nDeep-learning-based human-like decision-making and exploration\n- Lei Tai, Shaohua Li, and Ming Liu, A Deep-network Solution Towards Model-less Obstacle Avoidence, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea, 2016 PDF ,\nDeep Visual Homing throug Omnidirectional Camera\n- Lei Tai and Ming Liu, A Technical Report on Deep Visual Homing through Omnidirectional Camera, 2016 PDF ,\nAsynchronous Deep Reinforcement Learning\n- A PyTorch Implementation of A3C GitHub\nOne of the first real Cloud Robotic Systems\nA remote controlled mobile robot based on IntoRobot Cloud (www.intorobot.com) and IntoRobot Atom board. The control of the robot is through Internet. The robot can be shared to other via WeChat and WhatsApp easily.\nOmnidirectional Camera Calibration Toolbox\nThis is a calibration toolbox for omnidirectional cameras which can be used with MATLAB. The most common omnidirectional camera models can be used for calibration and the toolbox has good corner extraction capabilities even for quite distorted omnidirectional images.\nThe toolbox is based on LIBOMNICAL.\nThis toolbox fixes some bugs, adds some additional features and was mainly used and tested with the Mei model. Therefore the other model implementations need probably some bug fixes too and everybody is welcome to contribute!\nThe idea is to have a general omnidirectional camera calibration toolbox for all the common models and a platform for future model implementations.\nMore Information see our GitHub\nTango-based application (on-going)\nlibcnn: A library for real-time robotic recognition applications based on CNN\nlibcnn is a modular deep learning libraray, useful for robotics and computer vision. More Information see our GitHub\nVisible Light Communication-based Localization and Path-planning\nLocalization using a photonic sensor mounted on a tablet. It reaches to high localization precision as shown in the video. Please refer to:\nKejie Qiu, Fangyi Zhang, Ming Liu, Visible Light Communication-based Indoor Localization using Gaussian Process, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hamburg, Germany, 2015\nMing Liu, Kejie Qiu, Shaohua Li, Fengyu Che, Liang Wu, C. Patrick Yue, Towards Indoor Localization using Visible Light Communication for Consumer Electronic Devices, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2014\nMulti-robot 3D SLAM based on LSD-SLAM (collaboration with ETH - ongoing)\nThe individual maps generated by LSD-SLAM over multiple robots are fused.\nMulti-robot 2D SLAM without known initialization\nMultiple robots will move across unknown environments, so that a complete map will be constructed once the co-localization can be achieved.\nInternet-of-things control of a flowerpot\nOnline Extrinsic calibration between extroceptive Sensors\nGuoyang Xie, Tao Xu, Carsten Isert, Michael Aeberhard, Shaohua Li, Ming Liu, Online Active Calibration for a Multi-LRF System, IEEE 18th International Conference on Intelligent Transportation Systems (ITS), 2015\nVisual Homing-based topological Navigation\nThe mobile robot navigates in an indoor environment with illuminant changes by using an Omnidirectional Camera. Please refer to:\nMing Liu, Cedric Pradalier, Roland Siegwart, Visual Homing from Scale with an Uncalibrated Omnidirectional Camera, IEEE Transactions on Robotics (TRO), Vol. 29, Issue 6, pp.1353 - 1365, Dec. 2013.\n3D modeling and mapping in real-time\nThis a demo of 3D registration by ICP, based on the library proposed in:\nFrancois Pomerleau, Stephane Magnenat, Francis Colas, Ming Liu, Roland Siegwart, Tracking a Depth Camera: Parameter Exploration for Fast ICP, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2011\nLocalization by 1-bit sensor for educational Robots\nThis is a joint work with EPFL, U Leuven, INRIA and CityU. We localize an educational robots using a inferred sensor on gray-scale images.\nShiling Wang, Francis Colas, Ming Liu, Francesco Mondada, Stéphane Magnenat, Localization of inexpensive robots with low-bandwidth sensors, DARS 2016, PDF\nPoint-cloud analysis for semantic labelling using Tensor Voting\nMing Liu, Francois Pomerleau, Francis Colas and Roland Siegwart, Normal Estimation for Pointcloud using GPU based Sparse Tensor Voting, IEEE International Conference on Robotics and Biomimetics (ROBIO), 2012,\nMing Liu, Efficient Segmentation and Plane Modeling of point-cloud for structured environment by Normal Clustering and Tensor Voting, in Proceedings of the IEEE International Conference on Robotics and Biomimetics, (ROBIO) 2014\nMing Liu, Roland Siegwart, Information Theory based Validation for Point-cloud Segmentation aided by Tensor Voting, Best Paper in Information IEEE International Conference on Information and Automation (ICIA), 2013\nIncremental regional topological segmentation for environment surveillance\nMing Liu, Luc Oth, Francis Colas, Roland Siegwart, Incremental Topological Segmentation for Semi-structured Environments, Autonomous Robots (AURO), Vol. 37, Issue 3, Aug. 2014.\nColor-based description of environment and topological scene recognition\nMing Liu, Roland Siegwart, Topological mapping and scene recognition with lightweight color descriptors for omnidirectional camera, IEEE Transactions on Robotics (TRO), Vol. 30, Issue 2, pp. 310- 324, April. 2014.\nResource allocation among networked robots using real-time wireless communication\nMatthew Tan, Ming Liu and Roland Siegwart, An Experimental Evaluation of the RT-WMP Routing Protocol in an Indoor Environment, Best Paper Finalist IEEE International Conference on Information and Automation (ICIA), 2013Last modified on Tue, Nov 17, 2015","TECHNICAL AND PRACTICAL TRAINING FOR EMPLOYEES PRESENTS SEVERAL DIFFICULTIES, FROM THE COSTS INVOLVED IN TRAINING WITH REAL MACHINERY TO THE HEALTH AND SAFETY RISKS THAT SUCH TRAINING CAN POSE.\nImmersive learning technology provides a valuable opportunity to engage in practical training in an easier, safer and more flexible way and, unlike other alternative methods, maintains a high level of interaction with the learning content.\nA research study conducted by the Virtual Human Interaction Lab at Stanford University, “The Effects of Fully Immersive Virtual Reality on the Learning of Physical Tasks”, highlights the positive impacts that virtual reality can have on technical training.(1) Its findings show that virtual reality technology obtained better results than traditional video methods in the learning of practical tasks and participants in the study also reported feeling a higher social presence within the virtual environment.\nResearch has shown that repeating practical tasks reinforces learning. Immersive learning environments allow students to repeat tasks as many times as they like at their own pace and, importantly, away from the social pressures of a real classroom. For training in high-risk situations, virtual reality provides a much safer environment in which to make initial mistakes. Like face-to-face training, virtual training also allows students to interact directly with their trainer and environment. Detailed 3D imagery provides life-like representations of objects and their surroundings, giving students a very ‘real’ training experience.\nTechnical training in an immersive learning environment\n“Learning is the development of experience into experience” (James, 1892).\nThe learning and understanding of practical tasks relies on experience and virtual reality training is highly experiential and immersive. Students can interact with objects and machinery and view them up-close, as well as experiencing how to operate them. Effective training is also reliant on students’ interest and motivation (2); immersive learning technology can create stimulating and engaging learning environments for students, increasing their motivation to learn. Such environments are flexible and programmable, meaning that they can be tailored to meet individual needs.\nAccording to Fabrizia Mantovani (3), students learn more effectively when they engage directly with learning content and build their own understanding of it. Immersion in and interaction with learning content encourages the active engagement and motivation of students.\nExperience, repetition and interaction are important processes in practical training. Virtual reality training provides this in a controlled and programmable environment. The application of virtual reality with a live trainer yields particularly high motivation from students and allows for technical tasks to be carried out under guidance in a safe and engaging environment.\n(1) Patel, K., Bailenson, J.N., Hack-Jung, S., Diankov , R., & Bajcsy , R. (2006). The effects of fully immersive virtual reality on the learning of physical tasks. Proceedings of PRESENCE 2006: The 9th Annual International Workshop on Presence. August 24 – 26, Cleveland, Ohio, USA.\n(2) Bricken, M. (1991). Virtual reality learning environments: potentials and challenges. Human Interface Technology Lab (HITL) Washington Technology Center, University of Washington\n(3) Mantovani, F. Virtual Reality Learning: Potential and Challenges for the Use of 3D Environments in Education and Training in Towards Cyberpsychology: Mind, Cognition and Society in the Internet Age, Riva, G. and Galimberti, C., 2001"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ea5ed5b4-9e50-4972-85ef-0e70c4bc6384>","<urn:uuid:fad1c0af-b656-4505-8236-99aff755e94c>"],"error":null}
{"question":"When did LinkedIn first launch native video and what was the impact in the first year?","answer":"LinkedIn launched native video in 2017. In the first year, LinkedIn video posts generated more than 300 million impressions on the platform. They earned an average of three times the engagement of text posts, and LinkedIn's beta program showed that native videos were five times more likely than other content to start conversations among LinkedIn members.","context":["Since the launch of LinkedIn native video in 2017, LinkedIn has proven that it’s more than just a platform for…\nSince the launch of LinkedIn native video in 2017, LinkedIn has proven that it’s more than just a platform for long-form B2B content.\nIn one year, LinkedIn video posts generated more than 300 million impressions on the platform. They also earn an average of three times the engagement of text posts. Plus, early findings from LinkedIn’s beta program show that LinkedIn native videos are five times more likely than other content to start a conversation among LinkedIn members.\nImpressive engagement stats aside, video marketing has been shown to boost revenue across social platforms. According to Aberdeen Group, brands that use video marketing grow their revenue 49 percent faster than companies that don’t.\nReady to get on board yet? This guide will cover everything you need to know about LinkedIn video, from the basics on how to use LinkedIn native video, to technical specifications.\nAnd if you’re looking for that spark of inspiration, scroll down for a round up of examples and ideas.\nBonus: Want to know how a viral social video creator makes millions of dollars in sales? Download the free guide now.\nTypes of LinkedIn video\nIt’s still common practice for many brands to upload to a video-hosting platform such as YouTube or Vimeo, and then share the link on LinkedIn. This works, but for many reasons, LinkedIn native videos tend to be a more effective strategy.\nLinkedIn native video\n“Native video” is video that is uploaded directly to LinkedIn or created on the platform itself.\nUnlike embedded videos, LinkedIn native video autoplays in-feed, which is more likely to grab attention. Metrics show that Facebook native videos garner 10 times more shares than linked videos, a boost that likely also holds true for LinkedIn native videos.\nLinkedIn video ads\nLinkedIn video ads are sponsored company videos that appear in the LinkedIn feed. Video ad campaigns have greater potential to increase brand awareness, brand consideration, and lead generation since they are typically served to a larger, more targeted audience.\nUnlike LinkedIn native video, which can be a maximum of 10 minutes long, LinkedIn video ads can run for up to 30 minutes.\nCompany page administrators can set up a video ad campaign using Campaign Manager, or elect to sponsor an existing post.\nHow to use LinkedIn native video\nOn desktop or mobile, sharing LinkedIn native video is pretty much a three-step process. Mobile allows you to record and post in-app and add text and stickers, whereas desktop requires a pre-recorded video.\n1. From the homepage, click Share an article, photo, video, or idea.\n2. Click the video icon.\n3. Upload the video you want to share.\n1. Look for the share box (iOS) or post button (Android) at the top of the feed.\n2. Tap the video icon.\n3. Record a video in the app, or upload something you re-recorded.\n4. Tap the filters or text button.\n5. Add filters and/or text.\nAfter posting a video you’ll have access to audience insights, including how many views, likes, and comments your post is receiving. You’ll also be able to see the top companies, titles, and locations of viewers. Learn which video metrics matter most.\nHow to launch a LinkedIn video ad campaign\nHere’s a quick guide to setting up a LinkedIn video ad campaign:\n1. Log in to Campaign Manager to create your campaign.\n2. Select Sponsored Content.\n3. Name your campaign.\n4. Choose your main objective. Options include: get website visits, collect leads, or get video views.\n5. Select video as your ad type format and click Next.\n6. Click Create new video.\n7. Fill out the form, upload your video, and hit Save.\n8. After your video has uploaded, select the video by clicking the checkbox next to it and then hit Next.\n9. Choose your target audience criteria and click Next.\n10. Set up your bid, budget, the duration for your campaign, and click Launch Campaign.\nLinkedIn video ads provide richer analytics than LinkedIn native video. Learn more about LinkedIn video ad analytics here.\nLinkedIn video specs\nPlan and adhere to these technical specifications when creating video for LinkedIn.\nThese specifications vary between standard native videos and LinkedIn video ads, so make sure to take note of the difference.\nLinkedIn Native Video Specs\n- Minimum video length: 3 seconds\n- Maximum video length: 10 minutes\n- Minimum file size: 75KB\n- Maximum file size: 5 GB\n- Orientation: Horizontal or vertical. Note: Vertical videos are cropped into a square in the feed.\n- Aspect ratio: 1:2.4 or 2.4:1\n- Resolution range: 256×144 to 4096×2304\n- Frame rates: 10 – 60 frames per second\n- Bit rates: 192 kbps – 30 Mbps\n- File formats: ASF, AVI, FLV, MPEG-1, MPEG-4, MKV, QuickTime, WebM, H264/AVC, MP4, VP8, VP9, WMV2, and WMV3.\n- Formats that are not supported include: ProRes, MPEG-2, Raw Video, VP6, WMV1as.\nLinkedIn Video Ad Specs\n- Minimum video length: 3 seconds\n- Maximum video length: 30 minutes\n- Minimum file size: 75KB\n- Maximum file size: 200MB\n- Orientation: Only horizontal. Vertical videos are not supported by LinkedIn video ads.\n- Pixel and aspect ratio:\n- 360p (480 x 360; wide 640 x 360)\n- 480p (640 x 480)\n- 720p (960 x 720; wide 1280 x 720)\n- 1080p (1440 x 1080; wide 1920 x 1080)\n- File format: MP4\n- Frame rate: Maximum of 30 frames per second.\n- Audio format: AAC or MPEG4\n- Audio size: Less than 64KHz\nPlanning to serve your video on more than on social network? Check out our complete guide to social media video specs.\n10 LinkedIn video best practices\n1. Optimize your setup\nBefore going into selfie mode and hitting the record button, here are a few things you should consider.\n- Lighting: Choose a well-lighted place. Natural light is often best, but artificial light can work in a pinch—just look out for shadows. Also, make sure subjects aren’t back lit, otherwise they’ll become a silhouette.\n- Camera position: No one wants to see up your nose. Take a test video, and adjust the tripod or add or remove a few books under the camera setup as needed.\n- Camera: If recording from your phone, use the rear camera. Most phones have larger apertures and offer higher resolution from the rear cam. Use a tripod or makeshift mount to keep the camera steady.\n- Background: Avoid a cluttered or distracting background. Also, if you’re shooting in an office environment, make sure confidential materials and other brand logos are tucked away. You don’t want to inadvertently endorse another brand on your company’s behalf.\n- Body language: In his research, psychologist Albert Mehrabian found that 55 percent of communication is transmitted through body language. Only seven percent is given through words, and 38 percent through tone. Maintain a relaxed presence by rehearsing your script. Look directly at the camera, smile, and breathe naturally.\n2. Aim to capture attention from the start\nLinkedIn recommends that videos include a hook within the first 1-2 seconds.\n3. Put essential information upfront\nAttention that wanes after the first few seconds will typically drop off after the 10 second mark, LinkedIn research finds. That’s backed up by Facebook findings, which show 65 percent of people who watch the first three seconds of a Facebook video will watch for at least 10 seconds, while only 45 percent will watch for 30 seconds.\nPlan to share your message, or show your audience what you want them to see, early on. That way you increase the likelihood of leaving an impression with more viewers.\n4. Design for sound off\nUp to 85 percent of social media videos are played with no sound. That means most LinkedIn members will be watching your video as if it’s a silent film. Prepare accordingly by including descriptive images, explanatory infographics, and even expressive body language.\n5. Include closed captions\nEven if your video isn’t speech heavy, closed captioning will make them more accessible. Plus, since LinkedIn just added a closed captioning feature, there’s no excuse for your videos to not have subtitles.\nTo add captions:\n- Click the video icon in the share box on desktop and choose the video you want to share.\n- When the preview shows up, click the edit icon on the top right to see the video settings and then click select file to attach the associated SubRip Subtitle file.\n6. Vary the shot\nA single shot video can get boring, and with viewers dropping off by the second, varying the shot is one way to keep them engaged. Even if you’re shooting an interview, borrow a second camera to record from different angles. Or, film some b-roll to use under voiceover.\n7. Choose the right video length\nAccording to LinkedIn, the most successful video ads are less than 15 seconds long. But lengths can vary when it comes to LinkedIn native video. Here are a few things to consider:\n- For brand awareness and brand consideration videos, LinkedIn recommends to keep length under 30 seconds.\n- Videos that meet upper-funnel marketing goals should stick to a 30-90 second video length.\n- Opt for longer-form video to tell a brand or product story. A LinkedIn study found that long-form video can drive as many clicks as short-form video if it effective tells a more complex story.\n- Don’t exceed 10 minutes. LinkedIn considers 10 minutes the informal cut-off point for video.\n8. Close with a strong call to action\nWhat do you want viewers to do after they’ve watched the video? Leave them with a clear direction. Here are some tips for writing CTAs.\n9. Don’t forget supporting copy\nA recent study from Slidely found that 44 percent of video viewers on Facebook read caption text often, and 45 percent of viewers read captions sometimes.\nThe same likely goes for LinkedIn, so don’t miss this opportunity to describe your video or drive home a message. But keep it short and direct. We recommend 150 characters or fewer.\nAdding LinkedIn hashtags and @ mentioning relevant companies or members in your caption is a useful way to increase reach and expose your video to more viewers.\nAnd don’t forget to include a link, especially if the point of the video is to drive visits to your website or product page. As a bonus, LinkedIn finds that posts with links tend to have 45 percent higher engagement than those without one.\n10. Use the word “video” for promotions\nLinkedIn’s Video Ad Guide notes that promotional posts or emails that include the word video “can vastly increase the click-through rate.” If you’ve put in the effort to create a video, make sure to promote it—and use the keyword.\n12 ideas for LinkedIn native video\nTypically, most branded video content on LinkedIn falls into four main categories: culture, products and services, news, and events.\nIf you have a company blog, you can also analyze your best performing content and consider how it could be transformed into a LinkedIn video.\n1. Share company news and updates\nChanges to the board, new initiatives, acquisitions, partnerships, and more are all fodder for video content.\nExample: Coca Cola company news\nBonus: Want to know how a viral social video creator makes millions of dollars in sales? Download the free guide now.Get the free social video guide now!\n2. Announce the launch of a new product or service\nUse LinkedIn video to get customers excited with an announcement of things to come.\nExample: MyTaxi city launch\n3. Take customers behind-the-scenes\nShow viewers where the magic happens. This is a great opportunity to impress customers with the skill, craftsmanship, or technology behind your operation. Or, show off your super cool office culture.\nExample: Lego Behind the Scenes\n4. Offer an explainer\nInstructional or educational videos are particularly useful if you’re in an industry that uses complicated jargon or involves complex understanding. See this as an opportunity to teach your audience something new.\nExample: The World Bank for the African Green Revolution Forum – AGRF:\n5. Preview an upcoming event\nLooking to register more attendees for an upcoming conference? Create a video guide or highlight some of the reasons they may want to enrol.\n6. Provide insider coverage of an industry event\nSpeaker highlights, product demos, and interviews can form a winning package of an event’s top moments.\nExample: Pulse Africa\n7. Introduce C-suite members\nPosition your company as a thought leader with interviews that share the vision of executive team members.\nExample: Bill Gates\n8. Tell a story with a case study\nTestimonials are a great way to share how your products or services have helped customers.\n9. Let your customers know what you stand for\nUse LinkedIn video to let your clients, employees, and prospective employees know what your company stands for.\nExample: Boeing Pride\n10. Spotlight inspiring employees\nIntroduce customers to the people who make things happen.\nExample: UN Women\n11. Highlight the good you’re doing\nVideos about corporate social responsibility initiatives can bring attention to the social good your company is doing, and more importantly, to a good cause.\n12. Share something fun\nIf your company gets mentioned on Jeopardy, you kind of have to share the video.\nManage your brand’s LinkedIn presence the smart way—use Hootsuite to schedule videos and updates, target posts, engage with followers, and measure the impact of your efforts. Try it free today."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2aa5410a-0284-4921-88e2-0fd58e1e72d4>"],"error":null}
{"question":"What's the difference between ad retargeting conversion rate vs email conversion rate measurement?","answer":"Ad retargeting conversion rate is measured by tracking how many calls or sales result from people who previously interacted with ads or visited the website, while email conversion rate is calculated by comparing the total number of conversions to the number of messages delivered. For emails, there's also an adjusted conversion rate that compares total conversions to total clicks. Both metrics help measure campaign ROI, but they track different types of user interactions.","context":["Ad retargeting is a marketing strategy used to zero in on website visitors who, for whatever reason, didn’t make a purchase. Retargeting campaigns continue to perform better than other types of advertising, and for good reason: After a prospect has visited your site, you can say with almost complete certainty that they’re somewhat qualified.\nAnd while an increasing number of companies are adopting Google Ad’s call-only ads, many marketers don’t realize that retargeting can be a highly effective way to boost returns on call-only campaigns.\nVery little has been written about the process of designing a successful retargeting campaign for call-only ads. In this post, we provide a step-by-step framework to demystify the process of creating call-only retargeting campaigns.\nWhat is call-only ad retargeting?\nIn general, retargeting involves showing ads to prospects who have already responded to one of your previous ads or visited your website. Retargeting helps you keep your brand in front of potential customers after they’ve left your website without converting. This approach has a higher chance of winning conversions than just displaying ads to random web users, who might not be familiar with your brand.\nCall-only ad retargeting works the same way, but the key difference is that instead of targeting based on a website visit by the prospect, you’re targeting based on a call to your company by the prospect. With retargeting, people who called after viewing your company’s call-only ad will then be shown more of your ads in the future.\nAnother variation of the call-only retargeting campaign is to serve call-only ads specifically to prospects who have previously visited your website, although this approach is more suited to niche scenarios and longer sales cycles.\nHow to retarget for call-only ad campaigns\nWhen it comes to retargeting your call-only ads, there several steps that can increase your odds of success — let’s review them below.\n1) Segment your audience\nIt’s important to segment your audience based on different factors so you can personalize the ads more effectively. Here are some criteria to keep in mind when segmenting:\n- Keywords and search terms: Consider what keywords and search terms your call audiences used, and divide them into separate groups based on the different keywords. You can then create different versions of the ad that include the relevant keywords for each one of these groups. By doing this, you can show each segment of your audience even more specific and tailored ad content.\n- Geographic location: Where are your website visitors or callers located? If large segments of your audience hail from specific areas, you might want to divide them into groups based on geographic location. This is especially relevant for companies who are running call-only ads for local audiences. They can then better tailor the ads to this local audience.\n- Callers versus website visitors: You can also divide your audience based on those who called and those who visited your website. In this case, those who contacted you through a call-only ad receive regular online display ads, while those who visited your site are shown call-only ads. Focusing on callers may be the best approach, since callers tend to be highly engaged prospects who are ideal for retargeting. With callers, you also have the added advantage of being able to tap into your call data to gain insight into how to retarget your unconverted audience more effectively\n2) Define your campaign goals and success metrics\nWhen developing a call-only retargeting campaign, it’s important to have a clear sense of what your goals are. Most companies have two main goals when running a retargeting campaign.\nOne goal is retargeting for awareness, which involves getting your prospects more familiar and comfortable with your brand. With this approach, your main intention is to increase the volume of searches for your brand and direct more prospects to your website in the hopes of winning them over.\nThis kind of marketing can be tracked via ‘view-through’ conversions, but this metric is notoriously difficult to measure, and also unreliable when it comes to drawing big-picture conclusions. (It’s often quite difficult to prove that a single ad is the entire reason behind a conversion.)\nAnother popular approach is retargeting for conversions. If this is your main goal for your campaigns, you’ll need to focus on retargeting prospects who have indicated some degree of interest in your company by visiting your site or engaging with an ad.\nAfter you’ve determined your campaign goals, you’ll then need to set your success metrics, which are concrete indicators that will help you measure the effectiveness of your campaign:\n- Number of calls: When it comes to call-only ads, how many calls do the ads actually generate? In there an increase in call volume since you started running the ads?\n- Call length: On average, how long are the calls that come in? You can set a minimum call length as one of your success metrics, since longer calls tend to indicate better leads.\n- Number of conversions: When evaluating your calls, it’s essential to uncover how many calls lead to a sale and how much revenue they generate.\n- Call quantity versus quality: Remember that, in many cases, call quality can be a better measure of campaign success than call quantity. A smaller number of calls with a high conversion rate is much more valuable than a larger number of calls with an extremely low conversion rate.\n3) Implement ad campaigns\nAfter segmenting your audience and clarifying your goals, it’s time to design and implement your retargeting campaign.\nOne of the best places to develop your retargeting ads is Google Ads — in addition to serving ads on the SERP (search engine results page), Google Ads also lets you serve ads on other platforms like YouTube, Gmail, via in-app advertising, and the Google Display Network (GDN). You can also use a use a DSP (demand side platform) such as Transitiv to serve programmatic display ads as well.\nWhen building your retargeting campaign in Google Ads, the process is actually very similar to creating a regular ad, right down to the design and budget. The main difference between regular ads and retargeting ads involves the targeting itself: Regular ads are also displayed to people who haven’t contacted your company, while retargeted ads are only displayed to people who’ve interacted with your company in some way.\nWhen it comes to displaying your ads, be mindful of how many you’re sending. You want your audience to see your ads enough to become familiar with your company, but you need to avoid overwhelming them. Retargeter recommends sending an average of 17-20 retargeted ads per customer each month, but it’s advisable to test your ad frequency out to see what works best for you.\n4) Test for effectiveness\nAfter your retargeting campaign is up and running for a designated period of time, you’ll need to start evaluating the results.\nFor retargeted display ads sent to callers, you’ll want to look at traditional KPIs and monitor how these prospects are interacting with your business. (Are they visiting the site regularly, are they converting, and so on?)\nA/B testing is a must for your retargeted ads, because it lets you try out different variations of your ads to see which ones are the most effective. In most instances you’ll be monitoring ads for benchmark CTRs (click through rates) and conversions, although it’s worth keeping in mind that conversion rate is a more telling indicator of the quality of the traffic and landing page than of the ad itself.\nYour call tracking software is another important tool for helping you measure the effectiveness of each ad. Call tracking software allows you to record your phone calls and provides you with valuable call data. With call tracking, you can see which conversations resulted in quality conversations, and which ones are responsible for bringing in the most revenue.\nCall tracking also help you uncover similarities and commonalities between these calls, to inform your future campaigns. For example, how long are the calls? How did your sales representatives or employees win over the prospects? Were there certain phrases they said that helped convince prospects to make a purchase?\n5) Evaluate your results and refine your strategy\nAfter running your tests, it’s time to evaluate your results and revise your strategy accordingly. When analyzing your results, make a note of any areas that could use improvement. For example, are your ads effectively driving website visits and phone calls? If not, take a closer look at the ads themselves. Is the copy tailored and relevant to the segment you’re targeting? If your ads are prompting website visits and calls, how many of these calls and visits actually result in conversions?\nBe sure to closely observe the phone conversations themselves as well. Note the language that convinces prospects to convert to see if there are any common trends. And if the calls fail to convert, review the data and transcripts to see where things went wrong. This kind of info is valuable, and could inform future ad copy or sales training materials.\nAfter evaluating your results, use the insights you gained to refine your display and call-only retargeting ads. Once you’ve revised your ads, run them again for a designated period of time and then conduct another testing phase. Continue testing, refining, and running your ads until they meet your success metrics. And once you meet your target metrics, keep on iterating! By making testing an ongoing process, you’ll develop even more effective ads and campaigns.","The only way to measure the return on investment (ROI) of an email marketing campaign is to monitor performance down to the individual message level.\nThis means that marketers must dive head first into the sea of analytics in order to find insights into the successes and failures of their email campaigns. By doing this, marketers can learn how to optimize future messages for better performance.\nHowever, there is currently little in the way of standards when it comes to email analytics – even though there are undoubtedly some metrics that every marketer should be aware of. Luckily, Website Magazine has compiled a list of some of the most valuable metrics available, in order to help marketers take a step closer to receiving better results in the inboxes. Check them out below:\nOpen Rate –\nThis metric measures the number of emails opened against the number of emails that were delivered. A high open rate is a good sign because it shows that a message was not only delivered to the intended recipients, but that the subject line was also enticing enough to receive a click. That being said, marketers should keep in mind that an opened email doesn’t necessarily mean that the recipient engaged with the message.\nBounce Rate (Soft and Hard) –\nThe bounce rate refers to the proportion of email addresses that failed to receive a message. However, there are two different types of bounces – a soft bounce and a hard bounce. A hard bounce is the result of a bad address or technical failure, which means that marketers should remove these addresses from their list. Conversely, a soft bounce occurs when a mail server is temporarily unable to accept an email, which could be the result of a full inbox or even an automatic vacation message. This means that the message may be seen eventually, but marketers should pay attention to these subscribers and remove them from their list if messages are continuously bounced.\nClick (and Click-Through) Rate –\nThe click rate compares the total number of clicks a message receives to the total number of messages that were opened, which can be valuable for measuring engagement rates with specific messages. That being said, marketers can also pay attention to the click-through rate, which refers to the total number of clicks a message receives compared to the total number of messages delivered.\nConversion (and Adjusted Conversion) Rate –\nConversion rate is among the most important metrics to monitor because it is a very good indicator of the return on investment of an email campaign. This metric refers to the total number of conversions compared to the number of messages that have been delivered. On the other hand, the adjusted conversion rate compares the total number of conversions to the total number of clicks.\nAverage Order Value –\nMarketers can measure their email campaign's average order value by dividing the total revenue of conversions by the number of total conversions.\nRevenue per Email –\nTo determine a campaign's success, marketers can measure how much revenue each individual message brings in by dividing their campaign’s total revenue by the number of messages that have been delivered.\nDelivery Rate –\nThis metric is a good indicator of how strong a subscriber list is, marketers simply need to divide the number of messages delivered by the number of messages sent.\nUnsubscribe Rate –\nA high unsubscribe rate is a sign that an email campaign’s content isn’t relevant to the marketer’s subscriber list. This metric can be discovered by dividing the total number of unsubscribed addresses with the number of messages delivered.\nEmail Forward Rate –\nThis metric tells marketers how many times a message was forwarded from the original recipient to a new recipient. The forward rate can be calculated by dividing the number of messages forwarded by the number of messages delivered."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b2759be2-dfb7-452d-8a57-bf3c0a224bad>","<urn:uuid:6fc0f55a-60b3-4d57-bf53-13fc0175697a>"],"error":null}
{"question":"How do breweries manage energy costs through heat recovery, and what worker protections exist for high-temperature environments?","answer":"Breweries manage energy costs through heat recovery systems, particularly in the brew house where vapor condensers on exhaust stacks recover heat from boiling wort to pre-heat future batches, reducing steam demand and operational costs. For worker protection in high-temperature environments, companies must implement controls including proper ventilation, shielding from hot surfaces, scheduled rest breaks based on temperature (more frequent as temperature increases), provision of drinking water (4-6 liters per shift), and limiting exposure through adjusted work schedules. Workers must also undergo pre-employment health assessments and receive training on recognizing heat stress symptoms.","context":["Cost Saving Opportunities for Breweries\nFactory Manager, YBL, Melli (Sikkim)\nBijay Bahadur has more than 29 years of experience in Indian and multinational breweries, IMFL bottling plants, brewery green-field and brown-field projects and factory operations such as maintenance, production, environment health and safety, and project management. He is also the author of Brewing – A Practical Approach, which is for practicing brewers, students pursuing careers in Food Technology, Biochemical Engineering or Brewing Science, and consultants who provide technical advice to breweries and entrepreneurs. He is a practicing brewer and specializes in the manufacturing of beer, including the formulation of recipes, overseeing new technical developments, implementing standard operating procedures, planning budgets, warehousing and inventory control, and liaising with Quality Assurance Department (QAD). He is a lifetime member and elected as FELLOW of the Institution of Engineers (India), the Indian Institute of Chemical Engineers and the Association of Food Scientists & Technologists (India). He has been authorized to use the style and title of Chartered Engineer (India) by the Institution of Engineers (India) as well as been elected as Professional Engineer by Engineering Council of India, New Delhi for a period of five years.\nBrewing industries are looking for opportunities to reduce production costs without negatively affecting product yield or quality. Uncertain energy prices in today’s marketplace negatively affect predictable earnings, a concern for the management of the beer industry. For breweries, increasing energy prices are driving up costs and decreasing their value add. Successful, cost-effective investment into energy-efficient technologies and\npractices meet the challenge ofmaintaining the output of a high-quality product despite reduced production costs. This is especially important, as energy-efficient technologies often include “additional” benefits, such as increasing the productivity per employee of the company.\nEnergy consumption is approximately 3–8 percent of the production costs of beer, making energy efficiency improvement an important way to reduce costs, especially in times of high energy price volatility. There are many methods to reduce energy consumption cost-effectively in the brewing industry. Since brewer’s value the quality, taste and texture of their beer, brewing industries are expected to continue spending capital on cost-effective energy conservation measures that meet these requirements.\nFor breweries, increasing energy prices are driving up costs and decreasing their value add. Successful, cost-effective investment into energy-efficient technologies and practices meet the challenge of maintaining the output of a high-quality product despite reduced production costs.\nOptions For Energy Efficiency\nA variety of opportunities exist within breweries to reduce energy consumption while maintaining or enhancing the product quality and productivity of the brewery. Few measures taken may be:\n- Regular maintenance and proper operation of motors, pumps and compressors, and replacement with more efficient models whenever possible.\n- Proper and efficient operation of the process, that is optimization and ensuring the most productive technology is in place.\n- Establishing a strong organizational energy-management framework to ensure that energy efficiency measures are implemented effectively.\nEnergy-Efficient Boiler Plant\nA blow down economizer can be added to the boilers to reclaim heat from excess boiler water. The heat can be reused to preheat feed water for the boiler.\nBy preheating the makeup water, the rate of heat input of the boiler drops while the rate of heat output remains the same.\nOxygen in the condensate is scrubbed before it returns to the boiler by a deaerator. This decreases the amount of boiler blow down and decreases makeup water, improving energy efficiency and hence appreciable reduction of requirement of chemicals to treat the makeup water.\nEnergy-Conservation In Brew House\nThe reduction of heat energyconsumption in the brewhouse by means of technology is an important contribution to the preservation of fossil fuel resources and to significantly reduce CO2emissions.\nThe carbon footprint of a brewery can be optimized further with spent-grain combustion and the use of solar thermal energy. The technology and applied technique of biomass combustion must be improved for use in the brewing industry.\nIn the brew house, energy recovery reduces the need for steam. Entire heat required for the brew house comes from steam supplied by the boiler. Heat lost from the boiling wort in the wort kettle can be recovered and used to pre-heat future batches of wort with a vapor condenser on the exhaust stack. This will translate into less energy demand, lower operational costs, and faster brew times.\nBesides reducing energy demand, there is another benefit to energy recovery – steam from the wort kettle condenses into water. In this way, vapor from brewing beer stays in the brew house and the area outside does not “smell like a brewery.”\nOne way is to explore using water flowing out of the pasteurizer as an initial rinse in the bottle washer section, or to collect and reuse it for makeup water back to the pasteurizers.\nRaising beer-out temperature limits on pasteurizers can also reduce pasteurizer water losses. Bottle rinse water can also be reclaimed and used for pasteurizers or as dilution water for conveyor lubrication systems.\nBottle rinse water can also be used as a source for virtually any cleaning-in-place rinse in the brewery.\nBeer, Energy and the Environment\nThe brewing process is energy intensive, especially in the brew house, where mashing and wort boiling are the main heat consuming processes. The attention to reduce energy consumption has led to the development of new processes and technical solutions that consume less energy.\nDynamic wort boiling with an internal boiler and use of the Jetstar (Huppmann GmbH, Germany) internal boiler for a simmering boil, with a submerged wort flow and stripping phase to reduce undesired volatility, is a good example of a sustainable improvement in wort boiling combined with reduced thermal stress and increased wort quality.\nThe target for every brewing company should be the development of a sustainable process with efficient energy consumption to achieve savings in fuel and energy costs. Furnace oil has been pushed continuously to higher levels (fluctuates on daily basis) and there is no sign of a significant price decrease in the future. The demand for heat energy in the brewery can be reduced through the use of waste heat as process heat or energy rich by-products or waste material for thermal energy. The combustion of spent grains is one possibility for generating thermal heat and electrical power. The installations for heat generation through spent-grain combustion require huge investment on the design and technique for partial dewatering of spent grains.\nThe sun can be seen as the lowest cost energy provider. Breweries located in sunny regions should think about installation of solar collectors to take advantage of cheap solar energy. It is expected that the cost and installation of such equipment will decrease rapidly because the demand for this technology will lead to an increase in production volume. Solar thermal energy can be used for heating processes in CIP plants, bottle washing machines, and pasteurizers or for cooling processes with absorption chillers.\nThe largest consumers of electricity in breweries are refrigeration (44%), packaging (20%), and compressed air (10%). In general, it is recommended to invest in insulation and check whether that is dry. Regular inspection of the pipe system for the compressed air supply and installed valves is necessary to avoid losses and will help reduce electricity costs with little effort.\nIn addition, to fuels and energy, water is the other resource that is limited in quantity and good quality. Therefore, breweries should always aim at efficient water consumption for cleaning and cooling purposes, the prevention of losses, and the reuse of treated wastewater.\nMany opportunities exist in the brewery to reduce water consumption or recycle water. In India, use of 4 to 5 hectar litre (hl ) of water per hl beer output is considered good practice. Like heat recovery and reuse, water conservation and reuse approaches seek to best match and reuse high quality, medium quality, and low-quality water in various applications. Reduced water use will not only reduce effluent charges, but will also reduce water purchase bills, water treatment costs, as well as energy for water treatment and pumping.\nOther opportunities include the installation of recirculation tanks with vacuum pump beer filling installations, optimizing bottle washing installations, cleaning in place plants (CIP), the reduction of rinse water after CIP, and cascading of water for various uses.\nBenchmarks For European Breweries\n|Fresh water consumption||hl/hl||3.7 – 4.7|\n|Thermal energy consumption||kwh/hl||23.6 – 33|\n|Electricity consumption||kwh/hl||7.5 – 11.5|\n|Kieselguhr consumption||g/hl||90 – 160|","WORK FATIGUE POLICY\nHEAT STRESS PREVENTION PROCEDURE\nITC management recognizes and accepts the duty of care they owe to their\nemployees and subcontract personnel. They are also aware that much of their\nactivities are carried out in a hostile climatic environment often over protracted\nperiods. The objective of this procedure is to state the worker fatigue policy of ITC\nand to detail the measures taken to manage and minimise risks that may give rise\nto heat stress.\nWork Fatigue Policy\nPurpose of this POLICY\nTo manage employee work hours to minimise the potential for injury through work\nfatigue, consistent with our aim of achieving a zero safety incident site.\nScope of the document\nThis applies to all ITC employees & subcontractors.\nIt is recognised that where an employee works long hours, they may be subjected\nto excessive physical demands liable to increase the risk of injury. In addition, their\nperformance may be impaired by fatigue, reducing efficiency and safety\nThe provision of a safe workplace and safe system of work are obligations upon the\ncompany, both legal and moral. It is also the obligation of the employee to work\nresponsibly, including taking positive steps to avoid dangerous situations from\ndeveloping that could lead to accidents.\nNormal Hours of Work\nThe normal hours of work will be up to 12 hours per day with a maximum of 16 hours\nallowed in special circumstances, where there is a specific requirement.\nAll employees working on a roster must have a minimum of 10 hours rest between\nOnly in life threatening circumstances and with the relevant Department\nManager’s approval will employees be permitted to work in excess of the 16 hours\nIn these circumstances, employees must be transported to their place of residence\nat the end of the period of work and will not be allowed back on site for a\nminimum of 12 hours after completing their shift.\nConsecutive Work Days\nThe maximum number of hours worked in any 7 day period shall not exceed 84\nhours. Further to this, the number of hours worked in any consecutive 14 days\nperiod shall not exceed 168 hours.\nSupervisors and employee should be aware that fatigue could lead to unsafe\nworking conditions. However, fatigue can occur not only as a result of long working\nhours, but occurs during the normal work cycle. For example, people accustomed\nto working day shift, when temporarily transferred to nightshifts, will experience\nsymptoms of tiredness between the hours of 2.00 and 6.00 a.m. It is also common\nfor people to feel sleepy between 2.00 and 4.00 p. after taking a midday meal.\nCritical operations should, so far as practicable, be planned outside of these hours.\nRegular rest breaks should be taken throughout the shift, and 15 minutes taken after\ntwo hours of work would be reasonable, depending on the exigencies of the work.\nFor example, a crane driver should have a break immediately before commencing\ncritical lift activities.\nOutdoor activities and operations conducted in hot weather, especially those that\nrequire workers to wear semi-permeable or impermeable protective clothing, are\nlikely to cause thermal stress among exposed workers. Consequently, prevention of\nthermal stress is an important consideration for ITC and its Subcontractors working in\nUAE, particularly during the summer months when temperatures are commonly\nabove 45* C and humidity reaching 90%.\nRefer to table given as appendix 1 for level of risk associated with varying\ntemperature and humidity, it will be evident from this table that for the majority of\nthe time the ambient temperature/humidity prevailing in UAE places the worker in\nthe category of Extreme Caution, Danger or Extreme danger. This risk must be\ncarefully and diligently managed.\nROLES & RESPONSIBILITIES\nThe ITC Project Manager holds the ultimate responsibility for ensuring that this\nprocedure is effectively communicated to, and implemented by, all ITC and\nSubcontractor employees. The ITC Project Manager will be responsible for ensuring\n1. This procedure is effectively implemented to prevent the workforce from\nneedless exposure to extreme temperature conditions resulting thermal\nstress related illnesses or injuries.\n2. Adequate resources are provided to monitor the effectiveness of this\n3. Suitable and adequate welfare (including drinking water) and rest facilities\nare provided in work areas.\n4. Suitable personal protective equipment (PPE) is provided including\nindividual personal 1ltr water flask.\n5. All construction activities requiring a person to be exposed to extreme\ntemperatures are properly planned, coordinated and executed.\n6. Arrangements are in place with local clinics / hospitals for the immediate\ntreatment of potential thermal stress victims.\nSenior Project HSE Officer\nThe responsibilities of the ITC HSE Officer will include, but not be limited to the\n1. Implementing and maintaining the procedure throughout the duration of the\n2. Ensuring that this procedure is effectively implemented and diligently observed\nby all employees and subcontractors.\n3. Ensure each employee is in position of and is using their individual personal 1ltr\n4. A proper work-rest regime has been planned and is being executed.\n5. Creating awareness on thermal stress related illnesses and injuries through\ntraining, notice boards and distribution of information leaflets.\nTYPES & CAUSES OF THERMAL STRESS ILLNESSES / INJURIES\n5.1 It is important that all employees are adequately trained to enable them to\nunderstand and recognize the various types and causes of thermal stress\nrelated illnesses / injuries.\nThis will assist employees in preventing the occurrence of such illnesses /\ninjuries and will enable them to provide immediate treatment to persons\ndisplaying the symptoms of thermal stress.\n5.1 Types of Thermal Stress illnesses / injuries\n5.1.1 Heat Stroke\nHeat stroke is a condition caused by prolonged exposure to high\ntemperatures, in which\npeople experience high fever, headaches, hot dry skin and physical\nexhaustion. In extreme causes, a sufferer may physically collapse and into a\nWhen a person is assessed to have suffered from heat stroke, emergency\nmedical treatment must be immediately sought.\n• Extremely high body temperature.\n• Red, hot and dry skin (no sweating)\n• Rapid and strong pulse.\n• Throbbing headache.\n• Immediately call for emergency medical assistance.\n• Move the person to a cool and shady area. Do not leave the person\n• Cool the person rapidly with running water, cold compresses and / or\n• Provide cool water if they are alert. Avoid caffeine and alcohol.\n• If the person is unconscious, do not give them water.\n• Continue to cool their body temperature until medical assistance arrives\nand they can be taken to medical facility for further cooling and\nmonitoring of body temperature and functions.\nHeat exhaustion is a condition caused by excessive fluid loss due to\nsweating, resulting in the depletion of body fluid volume, which creates an\nimbalance of the electrolytes in the body.\n• Heavy sweating.\n• Paleness of Skin. Skin may be cool and moist.\n• Muscle cramps.\n• Weakness. Pulse rate may be fast but weak. Breathing may be fast but Shallow.\n• Nausea or vomiting.\n• Immediately call for emergency medical assistance.\n• Move the worker suffering from heat exhaustion to a cool and shady\narea. Do not leave the person alone.\n• Cool the person rapidly with running water, cold compresses and/or\nrapid fanning, if possible.\n• Provide cool drinking water, or electrolyte / ion replacement drinks, if\nthey are alert. Avoid caffeine and alcohol.\n• Remove extra clothing when necessary and continue to cool their body\ntemperature until medical assistance arrives.\nHeat cramps are painful spasms of the muscles generally thought to be\ncaused by an imbalance of electrolytes (e.g. essential minerals, such as\nsodium, potassium, calcium and magnesium) in the body. This may occur\nwhen a person carries out strenuous physical activity, drinks large quantities\nof water, but fails to replace salts / electrolytes lost through sweating.\nCramps can occur during or after working hours, and may be relieved by\ndrinking saline / electrolyte solutions. In more serious cases, and if,\ndetermined necessary by a doctor, a saline solution should be administered\nintravenously for quicker relief.\nMuscle pains or spasm usually in the abdomen, arms or legs.\n• Immediately call for emergency medical assistance.\n• Stop all activity and sit quietly in a cool and shady area. Inform nearby\nworkers of the problems encountered.\n• Drink clear juice or a liquid containing salt (e.g. electrolyte / ion\n• Do not returns to strenuous activity for at least a few hours after the\ncramps subside. It should be noted that further exertion might lead to\nheat exhaustion and/or heat stroke.\n• Seek medical attention if heat cramps do not subside within 1 hour.\nHear rash, also known as prickly heat, may occur in hot and humid\nenvironments where sweat is not easily removed from the surface of the skin\nby evaporation. When extensive, or complicated by infection, heat rash\ncan be so uncomfortable that in inhibits sleep and impedes a worker’s\nperformance or even results in temporary total disability. In most cases, heat\nrashes will disappear when the affected individual returns to a cooler\nCauses of Thermal Stress illnesses / injuries\nIt is difficult to predict just who will be affected by thermal stress, and when,\nbecause individual susceptibility varies. In addition, environmental factors\ncausing thermal stress are not restricted to ambient air temperature, as\nradiant heat, air movement, conduction and relative humidity all affect an\nindividual’s response to heat.\nAge, weight, degree of physical fitness, degree of acclimatization,\nmetabolism, use of alcohol or drugs, and a variety of medical conditions,\nsuch as hypertension, also affect a person’s sensitivity to heat. Even the type\nof clothing worn must be considered.\nAll ITC and subcontractor employees will undergo a pre-employment health\nassessment, performed by a health practitioner to ensure that they are\nphysically fit and do not have a serious pre-existing medical condition prior\nto being assigned to the project.\nThe higher the air temperature, the less heat the body can lose by\nconvection, conduction and radiation. If the temperature of the\nenvironment increases above skin temperature, the body will actually gain\nheat from the environment instead of losing heat to it.\nThe amount of moisture present in the air determines whether moisture\n(sweat) in vapour form flows from the skin to the environment, or vice versa.\nIn general, the moisture concentration at the skin will be higher than in the\nenvironment, making evaporative heat loss from the skin possible.\nConvective and evaporative heat losses increase with increasing wind\nClothing functions as a barrier to heat and moisture transfer between the\nskin and the environment. In this way it can protect against extreme heat\nand cold but, at the same time, it hampers the loss of excessive body heat\ngenerated during physical activity / effort.\nTHERMAL STRESS PREVENTION & CONTROL MEASURES\nITC shall consider the implementation of either or all of the following\nengineering control measures during the construction Phase to minimize the\npotential to expose workers to conditions that may cause thermal stress.\nGeneral ventilation to dilute hot air with cooler air (generally cooler air that\nis brought in from the outside), such control measures will be considered for\nthe workshops and confined spaces.\nConduction & Radiation Methods\nInsulating hot surfaces that generate heat, to reduce the amount of heat to\nwhich a worker may be exposed.\nShields can be used to reduce radiant heat, i.e. heat coming from hot\nsurfaces within the worker’s line of sight. Surface that exceed 35*C are\nsources of infrared radiation that can add to the worker’s heat load.\nShields should be located so that they do not interface with airflow, unless\nthey are also being used to reduce convective heating. The reflective\nsurface of the shield should be kept clean to maintain its effectiveness.\nAll buildings at site, particularly offices and accommodation / camp\nbuildings, shall be cooled using air-conditioning units.\nCanopies of awnings shall be provided over sections of the site where work\nis being carried out to shield workers from the U/V rays of the sun, as well as\nfrom the direct heat of the sun. Canopies, awnings or tents will also be\nerected to provide shade of workers taking their predetermined rest breaks,\nto minimize their exposure to the direct sun (refer to section 5.2 below).\nLimiting Exposure Time and/or Temperature (Work-Rest Regime)\nThe following measures will be implemented to minimize the exposure of\nworkers to conditions that may cause thermal stress.\nWhen possible during the hot summer season, schedule strenuous or hot\njobs for the cooler part of the day (early morning, late afternoon, or night\nAdd extra personnel to reduce exposure time for each member of the\nPermit freedom to interrupt work when a worker feels extreme heat\nAdjust schedule when possible so that hot operations are not performed at\nthe same time and place as other operations that require the presence of\nProvide regular rest breaks during hot weather to allow the body to cool\ndown, especially where the work is hard, physical. The following table\nprovides guidance as to the minimum rest breaks to be provided every hour\nfor outside work during temperatures.\nProvision of a thermometer to monitor ambient shade temperature.\nEach worker is issued with a personal 1ltr water flask (refer to 5.5) cool water\nshall be readily available in close proximity to replenish these, workers are\nadvised to drink adequate amounts of water frequently (preferably every\n20-30 minutes) to replace the water lost through sweating.\nEach worker is advised to drink at least 4 liters of water during an 8 hour shift\nand 6 liters during a 10 hour shift.\nIt is recommended that workers do not take salt tablets (or large amounts of\nsalt on their food), as more water will be required by the body to remove\nexcess salt, which increases the amount of work for the kidneys and further\nincreases the risk of dehydration. Salts tablets (or excess salt) also increase\nthe risk of high blood pressure.\nHealth Assessment / Monitoring.\nOnly fit, healthy workers will be allowed to work in conditions where severe\nthermal stress is a possibility. All employees are required to undergo a health\nassessment prior to their employment, or deployment to site.\nProtective Clothing /PPE\nWorkers on site are required to wear Personal Protective Equipment (PPE),\nsuch as safety helmets, safety shoes / boots, coveralls and safety glasses\n(tinted for the sun or bright conditions). This PPE provides some protection\nfrom the sun’s harmful U/V rays, but does not prevent thermal stress. In\naddition each employee is issued with a personal 1ltr water flask this is\nconsidered as part of the mandatory PPE requirements; employees are\nresponsible for ensuring that the flask is replenished as and when emptied.\nThe Project HSE Officer, will perform routine informal and formal inspections\non site to check that the requirements of this procedure are being\nimplemented. Formal inspection will be recorded on the Thermal Stress\nPrevention Checklist provided as Appendix 1.\nThe findings of these inspections will be reported to the project and\ncorporate HSE Managers.\nPrior to any work being undertaken, a risk assessment is prepared in\naccordance with the Hazard identification and Risk Analysis Procedure ITCHSE-\n002. During a job specific risk assessment, issues such as thermal stress\nwill be included and appropriate protection and mitigation measures\nADDITIONAL PRECAUTIONARY MEASURES\nIn addition to the Thermal Stress Prevention and Control Measures provided\nin section 5, the following precautionary measures shall also be observed:\n• Provide accurate verbal and written instructions, (refer to self check\nurine colors dehydration indicator chart appendix 3, this chart is\ndisplayed in all toilet facilities), frequent training programs, and other\ninformation about thermal stress related conditions.\n• Assure co-worker observation to detect signs and symptoms of\n• Pay extra attention to those who take medications that compromise\nnormal cardiovascular, blood pressure, body temperature\nregulation, renal, or sweat gland functions.\n• Ensure that first aiders are properly trained in the recognition of\nthermal stress symptoms and required treatment.\n• Ensure that temperature and humidity are measured and recorded\nregularly (at least twice daily).\nTRANING & EDUCATION\nTraining is the key to good work practices. Unless all employees understand\nthe reasons for using new, or changing old work practices, the chances of\nsuch a program succeeding are greatly reduced.\nThe HSE induction / Orientation provided to all employees will explain the\ncauses and symptoms of thermal stress related illnesses and injuries,\nminimum PPE requirements, the need to take regular rest breaks in the\nshade, and the need to drink plenty of water (to replace body fluids lost\nthrough sweating) when working in hot climates and conditions.\nSpecific Thermal Stress Prevention training will be provided to all Managers,\nSuperintendents and Supervisors and will include, but not be limited to, the\n• Knowledge of the hazards of thermal stress.\n• Recognition of predisposing factors, danger signs, symptoms and potential\neffects of thermal stress related illnesses and injuries.\n• Awareness of firs-aid procedures for treatment of thermal stress related\nillnesses and injuries, including heat stroke, heat exhaustion and heat\n• Employee responsibilities in avoiding thermal stress.\n• Dangers of using drugs, including therapeutic ones, and alcohol in hot work\n• Use of protective clothing and equipment.\n• Planning of strenuous / physical activities and hot operations for cooler\nperiods, wherever possible, e.g. early morning or evening / night.\n• Importance of regular rest breaks in the shade or air-conditioning during hot\nperiods as per the requirements of section 5.2 of this procedure.\n• Requirement to regularly drink water or electrolyte drinks to replace body\nfluids lost through sweating. Personnel should be made aware of the\nrequirement to drink at least 4 liters of water during an 8 hours shift and 6\nliters during a 10 hour shift.\n• Use of relief workers where possible.\nREVIEW & IMPROVEMENT\nThis Thermal Stress Prevention Procedure will be subjected to audit by HSE\nDepartment in accordance with the internal Audit Procedure. ITC-HSE-02"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:27d0a9be-b3fa-46bd-88fd-a7c7b7e5c73d>","<urn:uuid:042c6cbd-cc7c-46b0-a4a8-655a89e4e2cd>"],"error":null}
{"question":"What significant discoveries have been made in the Mariana Back-arc hydrothermal vent exploration, and how have these deep-sea organisms contributed to practical applications in modern technology?","answer":"During the Mariana Back-arc exploration, researchers discovered four new hydrothermal vent sites, including one with venting from a recently erupted lava flow, which represents the deepest historical eruption ever discovered (4100-4450 m) and the first documented on a slow-spreading ridge. These discoveries of deep-sea organisms have led to valuable practical applications. Enzymes from vent microbes are now used to enhance oil extraction from deep reservoirs, while ice-nucleating proteins from deep-sea organisms are utilized in ice cream production. Most notably, DNA polymerases isolated from vent life, which can withstand alternating cycles of heat and cold, were instrumental in creating the polymerase chain reaction (PCR), a revolutionary process that allows scientists to quickly generate millions of copies of DNA from a single strand.","context":["During the Hydrothermal Hunt, we have been searching for new hydrothermal vents in a vast unexplored stretch of the Mariana Back-arc, a zone where we know submarine volcanism occurs (at least occasionally), providing the heat for seafloor hot springs and their weird-and-wonderful chemosynthetic ecosystems. A few days ago, we had only 2 segments still to go (segments 8 & 9) of the 10 back-arc segments we set out to survey, and the results of our betting pool hinged on the results. The end of the cruise was coming up fast and we knew we only had time for two more tow-yos and one more AUV Sentry dive.\nYou Never Know Until You Look\nWe first surveyed Segment 9 with a CTD tow-yo because it has a long axial ridge with volcanic morphology and appeared to be a better candidate to have hydrothermal venting. But alas, no plume signals were found there. That left only Segment 8, which seemed even more of a long shot, because its morphology is enigmatic. The bathymetry at the shallowest part of the segment appears unusually smooth – neither obviously volcanic nor tectonic – and we’ve been debating among ourselves all cruise about what that might mean. Is that because it is heavily sedimented and long-dormant – therefore unlikely to have active vents? Or has it been repaved by young fluid sheet flows of lava over a very large area (which would be encouraging, but very unusual for a slow-spreading ridge)? Facts were unclear. I, personally, was a doubter, and had already written Segment 8 off in my head – in fact, I was already planning our last Sentry dive for another site. But I was wrong, and this demonstrates why exploration is so exciting: we don’t really know what we are going to find until we actually look! Segment 8 had another surprise in store of us (or at least for me). The CTD tow-yo showed a large hydrothermal plume right at the shallowest part of Segment 8, so we quickly planned our final AUV Sentry dive there to localize the vents on the seafloor and to do a photo survey.\nAnd the Winner is…\nBut how does the Segment 8 plume compare to the others we’ve found during this expedition? It was a big one, with strong signals from both chemical and optical sensors (suggesting an high-temperature vent field), but it was not big enough to de-throne our previous leader. Now that we have surveyed all the 10 back-arc segments, we can now announce that the winner of the “biggest plume competition” is… Segment 10! Our winner on board is Falkor crewmember, Todor Gerasimov, the Electro Technical Officer.\nShore-side, we’re happy to announce that Rafi Latif from London, United Kingdom is the winner of the online competition. He was randomly selected from the pool of correct responses.\nThe discovery of a hydrothermal plume at Segment 8 adds to our total for the cruise. Overall, we have discovered four new hydrothermal vent sites, and at one of them we found that at least some of the venting was coming from a recently erupted lava flow! That lava flow is the deepest historical eruption yet discovered (at 4100-4450 m), and the first to be documented on a slow-spreading ridge. In addition, we were able to map the previously known Alice Springs hydrothermal site (on Map 10) in unprecedented detail with AUV Sentry. We are grateful to the captain and crew of R/V Falkor and the AUV Sentry team for all the fantastic support they have given us during this expedition. They have all been extraordinarily friendly, accommodating, and engaged in helping us accomplish our goals on this research cruise. We also greatly appreciate the NOAA Ocean Exploration and Research Program and NOAA’s Pacific Island Regional Office who supported the science on this expedition.\nOur results from this cruise give us a set of great targets for next year, when we will return with R/V Falkor and a remotely operated vehicle to make dives at these new hydrothermal sites. We will then be able to see what they look like on the seafloor, take samples of the vent fluids for chemical analysis, and determine what animals and microbes are living at these new back-arc vents. This will allow us answer our “big picture” questions: are all the back-arc vents similar to each other geologically, chemically, and biologically? Do the same suite of chemosynthetic animals live at all the back-arc vents? How connected are the back-arc vents to one another? And how different are they from the better-known hydrothermal vents in the shallow volcanic arc? In the end we will have a better understanding of the global biogeography of vent organisms and their dependence on geologic and chemical environment.","What is Life at Vents and Seeps?\nHydrothermal vents and cold seeps are places where chemical-rich fluids emanate from the seafloor, often providing the energy to sustain lush communities of life in some very harsh environments. Cold seeps and hydrothermal vents differ from one another in the underlying conditions that form and drive them. This has implications for the kinds of animals that are able to survive at each.\nOn land and near the ocean surface, sunlight provides the energy that allows photosynthetic plants to convert carbon dioxide and water into the organic carbon, the fundamental source of nutrients for animals higher up the food chain. Below the photic zone—the sunlit, upper reaches of the ocean—many microbes have evolved chemosynthetic (instead of photosynthetic) processes that create organic matter by using oxygen in seawater to oxidize hydrogen sulfide, methane, and other chemicals present in vent and seep fluids.\nAnimals such as clams, mussels, snails, and shrimp feed on the microbes, and in turn, provide food for fish and other predators. Some vent and seep animals, such as tubeworms and shrimp, also host chemosynthetic microbes on or within their bodies, providing a place for the microbes to live in exchange for nutrients produced by the microbes.\nHydrothermal vents are driven by heat from volcanism beneath the seafloor. In this environment, chemical reactions take place as seawater percolates through cracks in the seafloor to produce hot (more than 400°C or 750°C), acidic fluids that eventually rise back to the seafloor. Vents, and the ecosystems they support, are created and destroyed as underlying volcanic activity waxes and wanes over tens or hundreds of years. Cold seeps, on the other hand, are less ephemeral and volatile.\nThey produce a diffuse flow of lower-temperature fluids, often composed of natural gas and a mixture of hydrocarbons, at slower rates for longer periods. Some seeps may be thousands of years old\nWhat is Life at Vents and Seeps?\nThe discovery of life at vents and seeps revolutionized understanding of how and where life can exist on Earth. The organisms that thrive at deep-sea vents and seeps have to survive freezing cold, perpetual darkness, high-pressure, and toxic chemicals. For this reason, they are often called extremophiles for the extreme nature of their living conditions.\nStudying these organisms expands understanding of how life first took hold and slowly evolved on our planet as well as where it might exist elsewhere in the solar system and beyond. In addition, previously unknown enzymes, molecules, and metabolic processes discovered in extremophiles offer great potential for development into valuable and life-saving biomedical and commercial applications. Enzymes from vent microbes are already being used to enhance the flow of oil extracted from deep reservoirs, for example, and ice-nucleating proteins found in deep-sea organisms are used to make ice cream.\nAnother example are DNA polymerases isolated from vent life, which can withstand alternating cycles of heat and cold. These enzymes were used to create the polymerase chain reaction, or PCR, the revolutionary process that has allow scientists to quickly generate millions of copies of DNA from a single strand.\nFrom Oceanus Magazine\nMy eyelids were tightly pressed down as I mustered all the tricks I could think of to get myself to…\nIn 1977, WHOI scientists made a discovery that revolutionized our understanding of how and where life could exist on Earth and other planetary bodies.\nScientists can’t really know if new oceanographic instruments will really work until they try them in actual conditions in the real ocean. In this case, the rubber hit the road at the bottom of the sea.\nIt’s 3 a.m., and Jesse McNichol is struggling to stay awake. Since midafternoon, he’s been in his lab, tending to…\nFrom orange octopi and furry yeti crabs to the largest known anemone, pilots and scientists diving in the Alvin submersible continue to find amazing marine creatures."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:64cecefc-5401-49d7-8722-2f0ab75ff5bd>","<urn:uuid:2cae5105-9356-4280-8e8d-bd56dfdc82a0>"],"error":null}
{"question":"How do individual paper-saving strategies in daily life connect to the broader efforts to protect Indonesia's forests and reduce greenhouse emissions? Could you outline the relationship?","answer":"Individual paper-saving strategies help reduce demand for paper products, which directly impacts forest preservation. Some key individual actions include using digital documents, e-readers, and reusable alternatives to paper products. These personal choices support larger conservation efforts, particularly in regions like Indonesia where deforestation for pulp and paper is a leading cause of greenhouse gas emissions. The connection is especially important for Indonesia's peatlands, which store 60 billion metric tons of carbon. Major companies like APRIL are now implementing forest protection policies, including ending deforestation and protecting peatlands, which complement individual conservation efforts to create a more comprehensive approach to forest preservation.","context":["30+ Striking Ways to Use Less Paper to Save World’s Forests\nPaper is all around us and we use it for a countless number of things. With so many people using so much paper all the time, we have to exhaust huge amount of energy and our natural resources to meet this need by cutting down trees and using equipment that pollutes air and water, destroys the homes of numerous animals and contributes to negative changes in our climate.\nAmericans use lot of paper everyday. They are the heaviest users of paper in the world. The average American uses about seven trees or an average of 700 pounds in paper, wood, and other products made from trees every year. By reducing the mount of paper you use everyday, you can reduce your carbon footprint on forests, cut energy usage, limit water, air and other pollution and produce less waste. Use these 30+ tips to help save the trees we so desperately need to preserve our environment.\n1. Get a dry erase board. A dry erase board will save paper because it keeps you from randomly making notes on any sheet of paper that you see. It’s also reusable for a long period of time and aids in organization and efficiency.\n2. Make use of the virtual world. From telecommuting to creating and signing documents and virtual faxing, there’s many ways to do things online. The more you learn about different ways to communicate and collaborate via the Internet, the less paper you will use and the more time you’ll save.\n3. Cut back on using paper dishes. Wash your dishes instead of buying paper dishware. With the production of excess paper leading to the annual destruction of millions of acres of trees, paper dishes may be easier for cleanup but they’re extremely harmful to the environment. If you must use paper dishes, make sure to recycle.\n4. Use less paper towels. Paper towels aren’t as long-lasting as dish rags that can be washed and reused many times. Also, dish towels are more attractive and can be bought in assorted collections for different uses.\n5. Sign up for paperless billing. Countless service providers now offer the option to deliver your bills via email instead of by snail mail using envelopes and printed paper. If your bills come to you through email, it’s also easier to keep up with them and track what you need to pay.\n6. Use your phone more often. Our phones are so diverse and filled with apps that the possibilities are endless for taking notes and writing down just about anything you would need to remind yourself of on paper.\n7. Find out how you can stop receiving junk mail. Tons of junk mail is delivered daily to homes and businesses and it simply gets thrown away because it isn’t useful to the recipients. Find out ways you can opt out of receiving junk mail to keep it from being delivered to your home. .\n8. Be more resourceful with notebooks and paper that you already have used. Its common practice that when we write in notebooks and on other types of paper that we only use one side of the page. Before you buy a new notebook, just take the old one and use the blank sides of the pages.\n9. Take a second to think about your habits. It may seem simple, but we’re so used to wasting paper that we don’t think about it as much as we should. Before you use paper products, ask yourself if it’s really necessary. The more you do this, you’ll find that your new ways of thinking will lead to behavioral changes that will allow you to use less paper and save more money.\n11. Have a plan. If you write out a plan and set it in motion when it comes to using less of the paper products that contribute to environmental degradation, then it will be much easier to stick to your goals and help others to do the same.\n12. Have a PDF file storage system. Instead of printing out papers for filing, create PDF document and upload them to a cloud storage system, along with saving them in specific folders on your computer. This way, they are less likely to be lost and you don’t have to use as much paper.\n13. Change the way you buy groceries. Use reusable shopping bags. If you have to use plastic bags, check with the store to see if they take bags back. Many stores do this now as a way to help customers shop sustainably and consciously.\n14. Opt for paperless subscriptions to magazines and news sources. It’s tradition in our culture to order our favorite magazines, feeling the excitement of the new issue arriving at our door. Save paper by signing up for a digital subscription instead of buying a magazine that you will most likely throw away after reading, or hold on to and never read again. You are also more likely to stay updated on the latest news and promotions from your favorite publications with online subscriptions.\n15. Use an e-reader for books. We can’t deny the appeal and value of books that are printed. You can hold it in your hand and keep it as part of a prized collection, but you can do the same thing with a Kindle reader. E-readers will also take up less room in your house and your books will always be saved and easily accessible from your device.\n16. Store important files on an external flash drive instead of printing. What’s more convenient than a USB? It’s small, able to be shared, and can hold a large amount of information and files. Use this device instead of printing paper documents and putting them in folder.\n17. Think of ways you can upcycle paper instead of throwing it away. Many hobbyists and creatives use unwanted paper for scrapbooking and other artistic endeavors.\n18. Use handkerchiefs instead of tissues. Handkerchiefs are absorbent and sanitary alternatives to toilet paper and Kleenex. They are better for the environment as they keep us from wasting so much tissue that has to be thrown away.\n19. Repurpose junk mail that you already have. If you already have junk mail, think of what you can use it for instead of throwing it away. For example, paper can be used as shipping material to protect fragile items.\n20. Use recycled toilet paper. Toilet paper made from recycled material is of great quality and just as durable as new toilet paper while helping to save the lives of trees that are often lost in the creation process of paper.\n21. Print wisely. When you have to print a document, adjust your margins and use a smaller font size. These changes will reduce the amount paper that is used during printing. Also, printing in draft mode can have a paper-saving effect.\n22. Share files and documents with others so that they don’t have to print them. If you have an important document or news that needs to be shared, consider uploading it into a file that can be emailed to everyone. If you’re in an office or enclosed setting, simply pass the information around to those who need it or allow them to upload it themselves.\n23. Use online tools for business collaboration. Google Docs and other digital work spaces allow everyone to meet and edit files in a shared space without needing to constantly print material.\n24. Always recycle. Paper products that you use should always be recycled. Likewise, when you recycle, you’re allowing sustainable manufacturers to create more usable material without destroying the environment.\n25. Use cloth napkins instead of paper ones. Paper napkins and towels have to be thrown away after usage, but cloth is easy to wash and reusable.\n26. Have store receipts emailed whenever possible. Many stores now offer the option to have your receipts emailed to you instead of printed on paper. Take this option and you’ll quickly become comfortable with digital receipts that can be saved and easily organized while being less likely to get lost.\n27. Use old newspaper for eco-friendly gardening. Newspaper can help to keep weeds from growing in your garden and eliminates the need for chemicals and other environmentally destructive products created to keep weeds away.\n28. Use paper as compost material. Use paper to give back to the Earth. Various types of paper can be used when composting to enrich the soil and encourage plants to grow.\n29. Use send and return envelopes. Mail is constantly coming and going, but you don’t have to throw away every envelope that comes to you via mail when the sender uses a send and return envelope. For mail that you send, make use of this option to save the recipient the trouble of purchasing a new envelope and to eliminate the need to trash the original.\n30. Buy treeless paper. Did you know there are alternative ways to make paper without using trees? Buy treeless paper products to show that you support the company’s mission and value what they are doing to sustain our environment.\n31. Wrap gifts without paper. During the holidays, everyone buys wrapping paper to conceal gifts. Gift wrapping is usually discarded, but we can cut back on this waste of paper by using fabric and reusable gift bags.\n32. Talk about it. Explain to friends, families, and coworkers how much of the forest is being destroyed at such a rapid rate to produce paper. This may encourage them to use less paper, recycle, and make more eco-friendly purchases.\nLatest posts by Rinkesh (see all)\n- Think Green: Importance of Recycling Ink and Toner Cartridges - June 24, 2017\n- Sustainable Living: Principles, Benefits and Examples - June 18, 2017\n- Shipping Container Architecture: The New Era of Sustainable Housing - June 16, 2017","Deforestation for pulp and paper, and palm oil, is a leading cause of greenhouse gas emissions in Indonesia. If properly implemented, APRIL’s pledge will prove to be another major step by business towards protecting Indonesia’s rainforests and peatlands.\nAPRIL’s parent company, the Royal Golden Eagle group, has also announced that new sustainability policies will be implemented by all other pulp companies in the group, including an end to deforestation. Greenpeace is suspending its campaign to give APRIL and other RGE group companies time to put these new policies into practice.\n“We commend APRIL for agreeing to end its deforestation, although we will be watching closely to make sure that today’s announcement leads to real change on the ground,” said Bustar Maitar, Head of Greenpeace’s Forest Campaign in Indonesia. “Today’s commitment from APRIL and the RGE Group is yet more proof that forest protection is the way forward for plantation companies in Indonesia.”\nAPRIL has agreed to a number of new conservation measures, including using the High Carbon Stock Approach to identify and protect forest areas remaining in their concessions. The company has also agreed to protect forested peatlands and has established a Peat Expert Working Group to help its develop international best practice for managing peatlands to reduce greenhouse gas emissions.\nIndonesia’s peatlands store an estimated 60 billion metric tons of carbon. When peatlands are drained for plantations this carbon is released, and the landscape becomes susceptible to smouldering fires which blanket the region in an annual haze.\nAPRIL has also agreed to work collaboratively and transparently to resolve its outstanding social conflicts and to support development opportunities for local communities that do not involve deforestation.\nAPRIL’s announcement follows similar decisions by other major players in the pulp and palm oil sectors to protect Indonesia’s forests and peatlands. In September, some of the biggest palm oil producers in Indonesia, including APRIL’s sister companies Asian Agri and Apical, agreed to end deforestation, and there is growing support from the business community in Indonesia for a development model based on forest protection.\n“President Jokowi promised to stop plantation companies damaging the environment or harming communities. Yet even though Indonesia’s biggest pulpwood and palm oil companies are moving away from deforestation, the destruction on the ground continues. The government must now act to reform the forest sector so it works for people and the environment,” said Maitar.\nNotes to Editors\n- Largely as a result of the rapid expansion of the palm oil and pulp and paper sector into Indonesia’s rainforests and peatlands, by 2005 Indonesia ranked as the world’s third-highest source of greenhouse gas emissions. Loss of its forest habitat to pulp and palm oil concessions has driven endangered wildlife species such as Sumatran tiger and the orangutan closer to extinction.\n- Asia Pacific Resources International Ltd. (APRIL) is the second largest pulp and paper company in Indonesia, and one of the largest pulp and paper producers in the world. Its new Sustainable Forest Management Plan (SFMP) is available on its website, http://www.aprilasia.com.\n- Alongside APRIL’s new policy, the Royal Golden Eagle Group has published sustainability principles that are to be implemented by its pulp companies, including Toba Pulp Lestari, Asia Symbol and Sateri. Those principles are available on its website, http://www.rgei.com. The palm oil companies in the RGE Group, Asian Agri and Apical, announced new sustainability policies, including an end to deforestation, in September 2014. http://www.apicalgroup.com/sustainability\n- APRIL and Asia Pulp & Paper (APP) together account for approximately 80% of Indonesian pulp production. APP announced a new Forest Conservation Policy that included an end to deforestation in 2013. These companies were the only large-scale producers of pulp using rainforest fibre. The other pulp companies either use plantation acacia only, or produce very small volumes of pulp.\nSummary of APRIL’s Sustainable Forest Management Plan commitments\n- An end to further development of any forested land, including peat forests.\n- Working with international experts to develop best practice peatland management at landscape level to reduce and avoid greenhouse gas emissions.\n- Protocols to ensure that communities can exercise their right to free prior and informed consent, and to resolve conflicts with communities affected by current operations.\n- Conserving an area of land equal in size to APRIL’s plantations.\n- Additional measures to support responsible forest management throughout APRIL’s global supply chain.\nGreenpeace UK Press Office firstname.lastname@example.org phone +44 (0)20 7865 8255"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:eacadd90-8252-4ef1-ac3a-a506503ef1d5>","<urn:uuid:677d998f-9c6b-40f9-9a3d-0bee50a7ea6c>"],"error":null}
{"question":"Could you provide a detailed historical comparison of how both seal populations were affected by human activities in the past, particularly focusing on the Hawaiian monk seals and Northern elephant seals?","answer":"Both seal species faced severe population threats from human activities. The Hawaiian monk seal population was drastically reduced, with current numbers estimated at only between 1,200 and 1,500 individuals, making them an endangered species. The Northern elephant seals faced an even more severe decline - they were hunted extensively for their blubber oil from the 18th century and were nearly extinct by the end of the 19th century, with their population possibly dropping to as low as 20 individuals. They found refuge in Mexican waters, with a single surviving rookery on Guadalupe Island. After receiving protection from both Mexican and U.S. governments in the early 20th century, and particularly after the Marine Mammal Protection Act, Northern elephant seals have made a remarkable recovery with numbers now exceeding 100,000, though they face a genetic bottleneck that could make them susceptible to disease and pollution.","context":["Spectacular, surprising seals\nAround the world, seals and sea lions represent different things to different people.\nTo marine theme park visitors, they are cute and talented performers able to balance balls and walk on their flippers; to environmentalists, they are defenceless pups slaughtered for their pristine fur; to commercial fishermen they are a threat to fish stocks; and to wild-life enthusiasts they are among the most spectacular creatures to watch at play in the wild.\nThroughout history, seals have played a substantial role in many cultures, providing food, fuel and clothing for indigenous tribes in the Northern Hemisphere’s frozen regions. Because of their expressive faces, these marine mammals have also been the focus of many legends, ranging from the ‘selchie’ stories of north-western Europe (in which seals are believed to be women and children condemned to a life where neither land nor sea provide a permanent home) to the superstitions that it is bad luck to kill a seal because they embody the souls of dead sailors.1\nSeals, sea lions and walruses are grouped together under the name ‘pinnipeds’ (from Latin: ‘wing-footed’ or ‘fin-footed’).\nPinnipeds are divided into three families: the Phocidae, Otariidae and Odobenidae.\nThe Phocidae are known in scientific circles as ‘true seals’. They are fantastic swimmers, using their hind flippers to propel themselves through the water while their small front flippers act as rudders or stabilizers. They lack external ears, hearing through small holes on either side of their head (see How they hear without ears below).\nThe Otariidae are known as ‘eared seals’ because, as the name suggests, they have (small) external ears. They also have long front flippers (measuring up to one-third of their body length), which propel them when swimming, while their hind flippers serve mainly as a rudder for steering. These animals are commonly referred to as sea lions.\nAll members of the Odobenidae family are walruses, and they combine features of both other families: they lack ears, but use both front flippers and hind flippers to move in the water.\nIt’s easy to remember the difference between ‘true seals’ and ‘sea lions’: true seals (no external ears) have small front flippers and can only move awkwardly on land by shuffling their large bodies, while the eared sea lions not only have the larger front flippers on which they can walk, but they are also able to turn their hind flippers forward under their body, thus enabling ‘four-footed’ locomotion. The relative agility of eared seals on land helps to explain why the Californian sea lion is the most commonly used pinniped in marine theme parks.\nBaby sea lions and true seals are called ‘pups’ and baby walruses ‘calves’. For simplicity, we will refer to all pinnipeds as ‘seals’ in this article.\nSeals are most abundant in cold circumpolar seas, where there are 14 species of eared seals (including fur seals) and 19 species of true seals (including harp, leopard, elephant and harbour seals). Exceptions to this include the Hawaiian monk seal, Californian sea lion, and species in the Mediterranean and Galápagos, all found in warmer climates.\nFor most seals, a good meal consists of fish, cuttlefish, octopuses and crustaceans, although several seals are more predatory, with the Australian sea lion and leopard seal feeding largely on penguins.2 The crabeater seal, the most abundant seal in the world, mostly lives on krill, a prawn-like crustacean abundant in polar waters.2\nSeals have a number of amazing design features that make them perfectly suited to their life on both land and sea. Many live and hunt their food in the ocean, but come ashore to rest, mate and bear their young. The harbour seal (a ‘true seal’) can be found sleeping belly-up on top of the water,3 or can even lie asleep on the sea bed for up to an hour. Although eared seals spend less time in the water than ‘true seals’, they too can sleep at sea while ‘sailing’ in the ‘tea cup’ position, i.e. floating belly-up with one front flipper tucked under the other.4\nAt birth, a newborn pup can travel on land and swim, although it takes a few weeks for them to develop enough blubber to keep them floating and insulate them against the cold. During this time, their mother’s milk is low in water content and high in fat, encouraging rapid growth. This also helps the mother in habitats where water conservation is important.5\nThe seal’s body shape not only helps it move efficiently in the water, but also effectively retains heat as the mammals move in and out of often freezing water. Along with whales and dolphins, seals have a circulatory system that allows arteries carrying warm blood to transfer heat to veins carrying cooler blood from the body surface, helping to stop warmth being lost.6\nIn addition to these amazing features, seals have sophisticated methods of insulation involving an outer coat of protective guard hairs, dense water-repelling undercoat, and a special layer of blubber. During their deep diving feats (see Effortless divers in the deep below), this fat layer does not compress, thus maintaining its heat-retaining properties.\nSeals have good sight underwater, even at night, and their eyes are able to adjust quickly to changing light, both when emerging from dark water into bright daylight,7 and when descending to the gloomy depths.8 Sensitive whiskers help the animal locate prey, particularly in those species feeding on the bottom of the ocean.2\nMost people would recognize the sound of a seal ‘barking’. This vocalization may play a role in navigation, social behaviour and foraging. Males may bark to show dominance and defend territory, while females may use the communication to locate their young on returning from feeding in the sea. Seals can also be heard to roar, honk, chirp, bleat, grunt or cough.7\nOne of the most engaging sights in the wild is a large number of seals swimming close together and waving their fore-flippers in the air. This is not, of course, a trick to entertain tourists, but a way for pinnipeds such as fur seals and sea lions to cool down; their insulation is effective in the cold, but can lead to over-heating problems when the temperature rises, and this action provides relief. On land, elephant seals tackle the problem by throwing sand over themselves with their flippers, keeping the sun off their skin.9\nWhile there are many seals in the wild, several types are endangered, such as monk seals (likely named for their preference for solitude or the loose skin around their necks, which resembles the hood of a monk’s robe). The total number of Hawaiian monk seals is estimated at between 1,200 and 1,500, the Mediterranean monk seal is rarely seen, and the Caribbean monk seal, last seen in 1952, is considered extinct.10\nEvolutionists tend to disagree on the ‘natural history’ of seals, sea lions and walruses, but agree that the ‘earliest’ fossil records—supposedly 20 million years old—reveal seals that look very much like those alive today.11 So much so that monk seals are often referred to as ‘living fossils’ because ‘they have remained virtually unchanged for 15 million years’.10\nThe fact that the three families share a number of characteristics creates lively debate among evolutionist scientists. One theory suggests sea lions, fur seals (Otariids) and walruses evolved from a bear-like ancestor on the shores of the Pacific, while the true seals (Phocids) arose ‘more recently’ from an otter-like ancestor around the Atlantic.11There is no fossil evidence to justify that claim,12 or any other theory on the evolution of pinnipeds.\nClaims of ‘convergent’ evolution—that two types of animals could evolve similar adaptations and features separately—are also without evidence, and, in fact, lack credibility, given the highly specialized features all three families possess (for heat retention and diving, as mentioned above).\nThere is no mystery if we accept that these specialized features are not the result of some chance succession of evolutionary flukes, but were perfectly designed by the Creator, who not only created animals perfectly suited to their environment and life, but made creatures that are beautiful to watch and study.\nHow they hear without ears\nHow are earless seals (and dolphins) able to hear? With only pinprick holes for ears, somehow marine mammals collect sounds and conduct them to the middle and inner ears, deep in their heads. Scientists suspected facial fat might be involved, but they couldn’t explain how the fat conducted sound.\nNow sophisticated technology has revealed that bundles of fat in the lower jaws and ear canals conduct sound extraordinarily well. ‘These fats have a shape like an ear trumpet,’ says one researcher, describing seals and dolphins as ‘acoustic fatheads’.1\n- How dolphins hear without ears, New Scientist 164(2218):17, December 25, 1999 / January 1, 2000.\nEffortless divers in the deep\nUsing sonic transmitters, researchers have been astounded to find that some seals are able to dive repeatedly to depths of more than 100 m (330 ft) in search of food. Weddell seals—with recorded deep-water dive times of up to 73 minutes1—are capable of diving to almost 600 m (2,000 ft). A northern bull elephant seal was recorded diving to depths below 1,500 m (4,900 ft), deeper than some whales are known to venture.2 When diving, a muscular reflex not only closes the nostrils, but also the larynx and oesophagus, so seals can open their mouth to catch prey without swallowing water.3\nBefore submerging, seals do not take in a large breath of air. This would create difficulties with buoyancy and could lead to ‘bends’ when they resurface (caused by dissolved nitrogen forming bubbles in the blood). Instead, seals breathe out first, and carry the oxygen they need attached to special pigments in their blood and muscle tissue. Seals have 10 times the amount of oxygen-carrying myoglobin (a protein pigment) than humans in their muscles. Their lungs are designed to collapse under pressure, so what little air there is, is forced back into the windpipe, where nitrogen cannot be absorbed into the blood.3,4 The increasing lung collapse, as the pressure increases with depth, changes the animal’s volume, decreasing its buoyancy. This makes for effortless deep diving, involving little or no expenditure of energy.5\nSeals also reduce their heart rate by 10 to 20% while diving, at the same time diverting blood from parts of the body where it is less needed, such as the liver, to essential organs such as the brain.2\n- Miller, D., Seals & Sea Lions, Voyager Press, Stillwater, MN, USA, p. 36, 1998.\n- Ref. 1, p. 36.\n- Ref. 1, p. 35.\n- New Scientist 166(2233):73, April 8, 2000.\n- Camera catches secrets of the deep divers, New Scientist 166(2234):19, April 15, 2000\nReferences and notes\n- Miller, D., Seals & Sea Lions, Voyager Press, Stillwater, MN, USA, p. 7, 1998. Return to text.\n- The New Encyclopaedia Britannica, 15th Edition, 23:423, 1992. Return to text.\n- Harbor Seal, www.letsfindout.com/subjects/undersea/rfiharse.html May 5, 2000. Return to text.\n- Conversation with Sea World, Gold Coast, Australia, May 5, 2000. Return to text.\n- Ref. 2, p. 424. Return to text.\n- Ref. 1, p. 31. Return to text.\n- Ref. 2, p. 424. Return to text.\n- New Scientist 163(2199):23, July 31, 1999. Return to text.\n- Ref. 1, p. 32. Return to text.\n- Hawaiian Monk Seals, leahi.kcc.hawaii.edu/~et/wlcurric/seals.html February 8, 2000. Return to text.\n- Ref. 1, p. 9. Return to text.\n- Flynn, J.J., Ancestry of sea mammals and Wyss, A.R., Evidence from flipper structure for a single origin of pinnipeds, Nature 334(6181):383–384, 427–428, 1988. Return to text.","Northern Elephant Seal\nThe Northern Elephant Seal (Mirounga angustirostris) is one of two species of elephant seal (the other is the Southern Elephant Seal). It is a member of the Phocidae (\"true seals\") family. Elephant seals derive their name from their great size and from the male's large proboscis, which is used in making extraordinarily loud roaring noises, especially during the mating competition. There is a great sexual dimorphism in size. The males can grow to 14 ft (4 m) and 5,000 lb (2,300 kg), while the females grow to 11 ft (3 m) and 1,400 lb (640 kg). Correspondingly, there is a highly polygynous mating system, with a successful male able to impregnate up to 50 females in one season.\nBoth adult and juvenile elephant seals are bare-skinned and black before molting. After molting they generally have a silver to dark gray coat that fades to brown yellow and tan. Adult males have hairless necks and chests speckled with pink, white and light brown. Pups are mostly black at birth and molt to a silver gray after weaning.\nThe eyes are large, round and black. The width of the eyes and a high concentration of low light pigments suggests that sight plays an important role in the capture of prey. Like all seals, elephant seals have atrophied hind limbs whose underdeveloped ends form the tail and tail fin. Each of the \"feet\" can deploy five long webbed fingers. This agile, dual palm is used to propel water. The pectoral fins are used little while swimming. While the hind limbs are unfit for locomotion on land, elephant seals use their fins as support to propel their bodies. They are able to propel themselves quickly (as fast as 8 km/h) in this way for short-distance travel, to return to water, catch up with a female or chase an intruder.\nLike other seals, elephant seals have a bloodstream adapted to the cold in which a mixture of small veins surrounds arteries capturing heat from them. This structure is present in extremities such as the hind legs.\nRange and ecology\nThe Northern Elephant Seal lives in the Eastern Pacific Ocean. Feeding grounds extend from northern Baja California to northern Vancouver Island. Males migrate as far north as Alaska and British Columbia, while females migrate as far west as Hawaii. They come ashore to breed, give birth and molt, mostly on offshore islands. While the pelagic range covers an enormous span, there are only about seven principal breeding areas, four of which are on islands off the coast of California. Recently increasing numbers have been observed in the Gulf of California. Breeding colonies can be observed at Channel Islands, Año Nuevo State Reserve, Piedras Blancas Light, Morro Bay State Park and the Farallon Islands in the US and Isla Guadalupe, Isla Benito del Este and Isla Cedros in Mexico. They also breed on Shell Island, Oregon and in January, 2009 the first elephant seal births were recorded in British Columbia at Race Rocks Ecological Reserve/Marine Protected Area. The California breeding population is now demographically isolated from the population in Baja California.\nThe Northern Elephant Seals are nocturnal deep feeders famous for the long time intervals they remain underwater  This species dives to great depths while feeding, typically between 300 m (1,000 ft) and 800 m (2,600 ft); moreover, the Northern elephant seal will generally not feed in depths of less than 200 m (700 ft). Both sexes eat a variety of prey including pelagic, deep water squid, Pacific hake, sharks, rays, and ratfish. Elephant seals don't need to drink as they get their water from food and broken down fats.\nWhile hunting in the dark depths, it is partly thanks to the use of vision that the elephant seals seem to locate their prey; the bioluminescence of some prey animals can facilitate their capture. Elephant seals do not have a developed a system of echolocation in the manner of cetaceans, but it is assumed that their vibrissae, which are sensitive to vibrations, play a role in search of food. Males and females differ in both diving behavior. Males tend to hug the continental shelf while making deep dives and forage along the bottom, while females have more jagged routes and forage in the open ocean. Males return to the same feeding ground every year while female have less predictable feeding migrations. Major predators of elephant seals are killer whales and white sharks.\nSocial behavior and reproduction\nThe Northern elephant seal returns to its terrestrial breeding ground in December and January, with the bulls arriving first. The bulls haul out on isolated or otherwise protected beaches typically on islands or very remote mainland locations. It is important that these beach areas offer protection from the winter storms and high surf wave action. The bulls engage in fights of supremacy to determine which few bulls will achieve a harem.\nAfter the males have arrived the beach, the females arrive to give birth. Females fast for 5 weeks and nurse their single pup for 4 weeks; in last few days of lactation, females come into estrus and mate. In this polygynous society, a bull will typically have a harem of 30 to 100 cows depending on the size and strength of the bull. Low ranking males are kept out of the harem so they attempt to copulate with females on the periphery or in the water with departing females. The higher a male's rank, the more freedom he has to copulate without interference, and the more frequently he interferes with the copulation of others. However, dominant males commonly break off a copulation to chase off a rival.\nWhile fights are not usually to the death, they are brutal and often with significant bloodshed and injury; however, in many cases of mismatched opponents, the younger, less capable males are simply chased away, often to upland dunes. In a lifetime a successful bull could easily sire over 500 pups. Only A few males are responsible for the majority of copulations. Less than one third of the bulls in a breeding colony will be able to mate with a female. Pups are sometimes crushed during battles between bulls.\nAfter arrival on shore males fast for three months, and females fast for five weeks during mating and nursing of her single pup. The gestation period is approximately eleven months. Sometimes, a female can become very aggressive after giving birth, defending her pup from other females. Such aggression is more common in crowded beaches. While most females nurse their own pup and reject nursings from alien pups some do accept alien pups with their own. An orphaned pup may try to find another female to suckle and are commonly adopted at least on Año Nuevo island. Pups nurse about four weeks and are weaned abruptly approximately two months before being abandoned by their mother. Left alone weaned pups will gather into groups and stay on shore for 12 more weeks. The pups learn how to swim in the surf and eventually swim farther to forage. Thus their first long journey at sea begins.\nHistory and status\nBeginning in the 18th century Northern elephant seals were hunted extensively almost to extinction by the end of the 19th century, being prized for oil that could be made from their blubber, and the population may have fallen as low as 20. Finding refuge in Mexican waters, by the turn of the 20th century, there was only a sole surviving rookery, on Guadalupe Island, Mexico; and this colony was granted protection by the Mexican government. Since the early 20th century, they have been protected by law in both Mexico and in the United States. Subsequently the U.S. protection was strengthened after passage of the Marine Mammal Protection Act, and numbers have now recovered to over 100,000.\nNevertheless, there is a genetic bottleneck in the existing population, which could make it more susceptible to disease and pollution  In California, the population is continuing to grow at around 25 percent per year, and new colonies are being established; they are now probably limited mostly by the availability of haulout space. However, numbers can be adversely affected by El Niño events and the resultant weather conditions, and the 1997-98 El Niño may have caused the loss of about 80 percent of that year's pups. Presently the Northern elephant seal is protected under the Federal Marine Mammal Act and under California law has a fully protected status.\nPopulations of rookery sites in California have increased during the past century. At Año Nuevo State Park, for example, there were no individuals observed whatsoever until the 1950s; the first pup born there was observed in the early 1960s. Currently, thousands of pups are born every year at Año Nuevo, on both the island and mainland. The growth of the site near San Simeon has proved even more spectacular; there were no animals there prior to 1990. Currently, the San Simeon site hosts more breeding animals than Año Nuevo State Park during winter season.\n- ^ a b c d e Campagna, C. (2008). Mirounga angustirostris. In: IUCN 2008. IUCN Red List of Threatened Species. Downloaded on 28 January 2009.\n- ^ a b c d e Le Boeuf, B., D. Crocker, D. Costa, S. Blackwell, P. Webb. 2000. Foraging ecology of northern elephant seals. Ecological Monographs, 70 (3): 353-382.\n- ^ a b c R. Condit and B.J. LeBoeuf, 1984 \"Feeding Habits and Feeding Grounds of the Northern elephant seal\", J. Mammal, 65:281-290\n- ^ a b c Northern Elephant Seal (Mirounga angustirostris): California Breeding Stock\n- ^ http://www.racerocks.com/racerock/eco/taxalab/miroungaa/newborn/jan3009.htm\n- ^ a b G.V. Morejohn and D.M. Beltz, \"Contents of the Stomach of an Elephant Seal\", Journal of Mammalogy 51:173-174\n- ^ a b c d M.L. Riedman and B.J. LeBoeuf, \"Mother-pup separation and adoption in northern elephant seals\", Behav. Ecol. Sociobiol. 11: 203-213\n- ^ a b c d Leboeuf BJ. \"Sexual behavior in the Northern Elephant seal Mirounga angustirostris\". Behaviour. 1972;41(1):1-26.\n- ^ a b c d Le Boeuf, B. J. 1974. Male-male competition and reproductive success in elephant seals. Amer. Zool. 14:163-176.\n- ^ a b c Le Boeuf, B. J., R. J. Whiting, and R. F. Gantt. 1972. \"Perinatal behavior of northern elephant seal females and their young\". Behavior 43:121-156.\n- ^ a b T. E. Christenson and B. J. Le Boeuf \"Aggression in the Female Northern Elephant Seal, Mirounga angustirostris.\" Behaviour Vol. 64, No. 1/2 (1978), pp. 158-172.\n- ^ Hoelzel, A. R., Fleischer, R. C., Campagna, C., Le Boeuf, B. J. & Alvord, G. 2002 \"Impact of a population bottleneck on symmetry and genetic diversity in the northern elephant seal.\" Journal of Evolutionary Biology 15, 567-575.\n- ^ Weber, D. S., Stewart, B. S., Garza, J. C. & Lehman, N. 2000 \"An empirical genetic assessment of the severity of the northern elephant seal population bottleneck.\" Current Biology 10, 1287-1290."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:de0a0ffb-49d4-4c88-8136-6664718eb9c8>","<urn:uuid:75ef75a6-a221-4c28-aab6-84341b1a05bd>"],"error":null}
{"question":"What's the difference between farm crop production technicians and livestock advisors in terms of their training requirements?","answer":"Farm crop production technicians and livestock advisors have different training requirements. Farm crop production technicians need specialized knowledge in areas like soil preparation, plant treatment, harvesting, and may work in various roles including processing, seed production, and inspection. Livestock advisors, on the other hand, must complete 60 hours of animal science training and contribute 50 hours of volunteer time over two years. After the initial period, livestock advisors must complete 20 hours of volunteer service and 5 hours of advanced training annually to maintain certification.","context":["You are currently signed in as .\n0 Items in Your Cart\nVault Guides are THE source for insider insight on career information and employer reviews. Shop Vault Guides\nIndustries & Professions /\nFarm Crop Production Technicians\nCorn, soybeans, wheat, cotton, fruit—these are some of the top crops in the agricultural industry. The farms and orchards that produce these crops have very specific needs, differing from the needs of livestock and dairy farms. Farm crop production technicians understand how to best prepare soil, treat plants, and harvest crops. These technicians may have different employers, from scientists to government agencies to the farmers themselves, but they share intentions—to use their knowledge of crops and production to help farmers increase yields and market their products. And the work can be varied, involving grading and handling, pest and disease control, finding new uses for crops, and other tasks.\nNearly everything used on a farm is now purchased from outside suppliers: seed, fertilizer, pesticides, machinery, fuels, and general supplies. Companies selling these products need farm-trained technicians who understand buyers' farming problems and needs. Farm supply companies also need technicians to assist in research and development. These technicians work under the supervision of feed or chemical company scientists, carrying out the details of the testing program.\nIn the production phase of crop technology, some technicians make soil or tissue tests to determine the efficiency of fertilizer programs. Others are responsible for the maintenance of farm machinery. More experienced farm crop production technicians may oversee the complete management of a farm, including personnel, machinery, and finances.\nPractically all agricultural products need some processing before they reach the consumer. Processing involves testing, grading, packaging, and transporting. Some of the technicians in this area work closely with farmers and need to know a great deal about crop production. For example, field-contact technicians employed by food-processing companies monitor crop production on the farms from which the companies buy products. In some processing companies, technicians supervise the entire crop operation. In others, they act as buyers or determine when crops will be harvested for processing and shipping.\nSome technicians may work for the government or businesses performing quality-control work or nutrition research; others work as inspectors. This work is usually done in a laboratory.\nIn addition to the positions mentioned above, farm crop production technicians may take on the following titles and responsibilities.\nProcessing and distributing technicians may find jobs with canneries, freezing and packing plants, cooperatives, or distributors to make sure the work is up to government standards and to advise on matters of efficiency and profitability. They may work either in the laboratory or in the field with the grower. Laboratory technicians work with scientists to maintain quality control, test, grade, measure, and keep records. Field technicians supervise seed selection and planting, weed and pest control, irrigation, harvesting, and on-the-spot testing to ensure that crops are harvested at precisely the right state of maturity.\nSeed production field supervisors help coordinate the activities of farmers who produce seed for commercial seed companies. They inspect and analyze soil and water supplies for farms and study other growing conditions in order to plan production of planted crops. They distribute seed stock to farmers, specify areas and numbers of acres to be planted, and give instructions to workers engaged in cultivation procedures, such as fertilization, tilling, and detasseling. They may also determine dates and methods for harvesting, storing, and shipping seed crops.\nAgricultural inspectors work for state, county, and federal departments of agriculture. In order to inspect grain, vegetables, or seed, they must know grades and standards and be able to recognize common pests and disease damage. They may work in the field, at a packing shed or shipping station, or at a terminal market.\nBiological aides and technicians assist research workers in biology, bacteriology, plant pathology, mycology, and related agricultural sciences. They set up laboratory and field equipment, perform routine tests, and clean up and maintain field and laboratory equipment. They also keep records of plant growth, experimental plots, greenhouse activity, insecticide use, and other agricultural experimentation.\nDisease and insect control field inspectors inspect fields to detect the presence of harmful insects and plant diseases. Inspectors count the numbers of insects on plants or of diseased plants within a sample area. They record the results of their counts on field work sheets. They also collect samples of unidentifiable insects or diseased plants for identification by a supervisor.\nSpray equipment operators work for pest-control companies. They select and apply the proper herbicides or pesticides for particular jobs, formulate mixtures, and operate various types of spraying and dusting equipment. A specialized technician within this occupation is the aircraft crop duster or sprayer.\nComplete your Vault Profile and get seen by top employers","What is the WSU Livestock Advisor Program?\nWSU Livestock Advisors are trained and certified members of a volunteer program that is sponsored by Washington State University Extension. The program assists WSU Extension in providing information to livestock producers on raising, breeding, and housing livestock using the best farm management practices. The demand for such information has grown rapidly in recent years and the methods of delivery have diversified.\nWho can become a Livestock Advisor?\nAnyone with an interest in livestock and the desire and enthusiasm to learn, and then share their knowledge with the community, can apply to become a certified WSU Extension Livestock Advisor. The only limitation is the time available to the applicant and space in the class.\nThe first time commitment is the 60 hours of animal science training. The second requirement is the 50 hours of volunteer time the new Livestock Advisor contributes to the program over two years.\nCommitments to continue as a certified Livestock Advisor after the first two years are reduced to 20 hours of volunteer service and 5 hours of advanced training per year. Many Livestock Advisors considerably exceed the minimum.\nWhat training are the Livestock Advisors given?\nThe training sessions cover a variety of subjects such as poultry, sheep, beef, swine, horse, rabbits, goats, vertebrate pest management, nutrition, water quality, and mud management. Other areas also taught are pasture management and organic farming.\nThe trainees receive a course outline and free extension publications as well. A take-home quiz is given on each major subject matter area to verify that the new Livestock Advisors are ready to serve as resource people in the community.\nLivestock Advisor training is held in the evenings on Wednesdays from 6:00 – 9:00 PM for 12 weeks. Past training locations include the University Train Station in Everett and the WSU Skagit County Extension office in Burlington. Training is held annually, usually in the fall.\nWho decides what each Livestock Advisor will do?\nEach Livestock Advisor Volunteer must complete a total of 50 hours of volunteer service in the first two years. This includes each new Livestock Advisor spending a minimum of 15 hours at the Evergreen State Fair in Monroe staffing the Livestock Advisor Foundation Petting Farm.\nThe remaining activity hours during the first year, and in subsequent years, are fairly flexible, as long as they are an accepted part of the WSU Extension Livestock Education Program. Activities for volunteers include writing and editing articles for our web page, giving talks and demonstrations to groups in the community, and staffing booths at fairs and civic events, making presentations at Small Farm Workshops, returning phone queries on livestock issues and making farm visits.\nWhat is in the Livestock Advisor Program for me?\nServing the public through this program is very satisfying to most volunteer Livestock Advisors. Many of them stay with the program year after year providing depth and continuity to the program as well as keeping themselves up-to-date on the latest research and gardening information.\nAs a WSU Extension Livestock Advisor you will meet many other people who share a mutual interest in livestock, increase your public relations skills, and acquire information and develop abilities that will prove useful to you in a variety of situations. You will develop and increase confidence in your capacity as a livestock raiser and problem-solver. You will learn, and help others learn, practices that are safer for the environment. You will have access to all the latest research-based information pertaining to livestock management, too\nHow do I apply?\nFor an application, contact your local WSU Extension office or click here to see the application. You can reach our Livestock Advisors Program via e-mail at firstname.lastname@example.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:83a0e4a6-befc-47eb-8d23-6f1fd2c1da59>","<urn:uuid:02939295-8200-46fe-a69c-791851564478>"],"error":null}
{"question":"What attributes of congregational singing are prioritized in both Irish melodies and the Christ the King festival service?","answer":"Both Irish melodies and the Christ the King festival service prioritize simple singability and inclusivity across generations. Irish melodies, as described in relation to 'In Christ Alone', have tremendous strength in being easily sung by large groups of people in various settings, with or without instruments. Similarly, the Christ the King festival guidance emphasizes selecting hymns that different age groups can sing - 'songs children could sing and the elderly would know' - and recommends including a broad range of styles to ensure widespread participation.","context":["“For this issue of Worship Leader with a thematic focus on the use of “story” in a service of worship we decided to tell the story of the song “In Christ Alone.” Really, it was an obvious choice. It is one of the best modern examples of storytelling conveying the power of the gospel. And as it turns out, the story behind the song is the testimony of Keith Getty, one of the song’s co-writers. It includes his personal journey towards worship leadership and the many nuances of art and craft that he has learned along the way.\nWL: We know that you co-wrote “In Christ Alone” with Stuart Townend, him on the lyric side and you mostly focusing on the melody and music. But you still have a deep connection to the origin of this song. Tell us your side of the journey towards writing “In Christ Alone” and towards your passion for writing modern hymns.\nGetty: I grew up in a home where we sang contemporary Christian songs and also traditional church music. During high school I was influenced by teachers and musicians who loved historic church music. This continued at university, when I lived beside Durham Cathedral, and experienced the music there-the genius and artistry of timeless melodies married to the texts of some of the most fabulous poetry ever written. Around that same time, I struggled to understand and fully embrace my faith amid an unbelieving, universalist, and multi-religious culture. It was a journey to believe in the uniqueness of Christ, the Scriptures and the gospel story. By the time I came through this, my faith was stronger, and I really wanted to write songs for the Church that brought the full, rich, life-giving story of the gospel into believers’ hearts and minds.\nI then had the privilege of connecting with songwriter Stuart Townend, who had penned the hymn “How Deep the Father’s Love for Us.” When we first met, there wasn’t an immediate personal connection. But we said, “Let’s write one song.” I knew I wanted the song to be story-driven, so I said, “Let’s do a song that tells the whole story of Christ coming to earth-the whole gospel story in one song. Let’s have as many verses as we want, and let’s just go for it.”\nOur song originally was called “My Hope Rests Firm in Christ Alone,” but we later changed it to “In Christ Alone My Hope Is Found.”\nWL: Where did the melody come from?\nGetty: My melodies tend to be heavily influenced by Irish music, and the Irish melodic style is essentially congregational. Although Irish music isn’t particularly spectacular compared to say, African rhythm or to the unusual tones of Chinese music, or even the sophistication of much contemporary music, it has tremendous strength in its ability to be experienced and sung by large groups of people-whether in our homes, schools, or even at a sports match. It can be sung with or without instrumental accompaniment. I think the underlying sense of lilting pathos in Celtic melodies (which can also be heard in our speaking voices and is tied closely to our history) also helps the songs tell a story with all its raw emotion and passion. All Irish music centers on stories, whether of love or war or of people and places.\nIn this case I just sent several of the melodies I’d worked on to Stuart and he chose this one to build the song on. He has a fabulous ear for melody.\nWL: This song has been classified by many as a “modern hymn.” Could you describe what that is?\nGetty: Well I should say first of all that I’m not sure modern hymns are a genre yet. As with any form of art, it’s hard to describe, but there are two particularly clear distinctives with our music we try to achieve: the depth of lyric and the simple singability and flexibility of melody.\nThe first aim is modeled for us throughout Scripture and is specifically seen in Colossians 3 when Paul commands the Church to let the word of Christ dwell in us richly when we meet together, singing psalms, hymns, and spiritual songs. It seems clear here that style is not the important issue as much as the depth of content.\nWhat we sing is so fundamental in the formation of a believer’s heart and mind as it also shapes our imagination and memory. The congregations in front of us each Sunday are in need of this gospel story to sustain them throughout the week. We must feed them on the truths of the gospel. Our congregations today are exposed to every religion and philosophy through media and the Internet in a greater way than ever before, so we must have something of substance to say in our worship that reminds us why Christ’s story is so unique and so utterly essential.\nSecond, it needs to be a song every generation can sing. The message of some churches today seems to be, “Only sing if it appeals to your sense of style, or your demographic.” Yet when we look at heaven, we see that every tribe and every nation will sing together. Worship should not be primarily about the worship leader up front, but about the worship leader serving as a conduit to allow the family of God to sing together to their creator. The family of God is the choir, and God is their audience.\nI think it’s of huge importance to us as worship leaders in preparation and in reviewing Sunday services to ask ourselves these two questions: What were the words we put into our congregation’s mouths, minds, and memories? And how well did our congregation sing? Our role is simply to be an accompaniment to them as they sing.\nThese were our priorities when Stuart and I wrote “In Christ Alone.”\nWL: You are working in a very familiar style, yet you have been able to keep things quite fresh. How are you able to avoid clichés?\nGetty: Well I’m glad you think we are! For me, it’s just hard work.\nI know that’s a really boring answer, but when we wrote the Awaken the Dawn project, we recorded about 700 different song ideas in part or in whole. And in the end we used eight of them, in addition to four old songs.\nStuart and Kristyn spend anywhere from two to 15 months on every song’s lyrics. “In Christ Alone” took Stuart about three months. So we work at it. I also encourage lyricists to read beautiful poetry. Consider the fact that almost 20 percent of the story of scripture is told through poetry. This speaks to the power of words. And to the enduring power of beauty. And perhaps most of all to the unending creative potential the story of the gospel releases in each of us.”","Christ the King: A festival of scripture and song for the last Sunday of the Christian year\nMany churches observe the Feast of Christ of King on the last Sunday of the Christian year, which falls on the third or fourth Sunday of November and celebrates Jesus’ conquering of sin and victory over death, his eternal reign, and our identity as a royal priesthood that shares in his reign. This festival was established in 1925 by the Roman Catholic Church as a proclamation to combat the secularization of society and to call on everyone, including governments, to submit to Christ.\nLadd Harris, the rector at St. Mark’s Episcopal Church in Grand Rapids, Michigan, compiled ten lessons corresponding to the significant events of Jesus’ life; we collaborated on selecting the hymns.\nI loved working on this service. Nearly every hymn selected has some reference to the kingship of Christ, and because they are sung out of their usual context, there is much potential for the congregation to experience their great depth and meaning (“This, this is Christ the King, whom shepherds guard and angels sing”; “the power of Satan breaking, our peace eternal making”). The rich texts, along with a wide variety of tunes in different meters chosen from throughout the church’s history can help weave the kingship theme throughout the service.\nThe Thanksgiving hymns we used also contained references to Christ the King that we might not normally take note of: “the Son and him who reigns with them in highest heaven”; “give his angels charge at last in the fires the tares to cast.” Searching for particular themes is an excellent way to get to know your hymnal (and other hymnals).\nThe service can be as “bare bones” or as elaborate as needed; choirs, soloists, and instrumentalists can play an important role in conveying the message. Above all, the service encapsulates the entire life of Christ using readings from Isaiah and the gospels, and is a way of “singing through” the Christian Year. And for those of us who love singing, a service of thirteen hymns is a rare and wonderful opportunity.\nYou may be interested in following all the suggestions from the service presented here, but more likely you will want to explore other options. Here are some suggestions:\n- Using the topical index in your hymnal, look for headings such as God/ Christ as King, Kingdom, Lordship of Christ, Ascension and Reign of Christ, and Worthy Is the Lamb.\n- Read through all the stanzas of a hymn. Don’t assume that because the title includes the word King it is appropriate (“The King of Love My Shepherd Is” probably wouldn’t work in this service). Make sure that the hymn selected helps to explain or elaborates on the specific lesson. Omit stanzas if they don’t relate to the text, but make sure that the remaining ones are cohesive. Check hymnals from other denominations to explore other texts, tunes, and copyright information.\n- Try to include a broad range of styles and genres; songs children could sing and the elderly would know; mostly familiar but a few unfamiliar. Consider using one or more choirs to introduce initial stanzas of an unknown song (then sing the rest in unison), or even a beloved one (a children’s choir could sing a stanza of “Beautiful Savior”).\n- It may be difficult to pick a different “style” of hymn for every single lesson, but do watch the sequence of hymns, considering key, tempo, meter, tone/mood, date written, instrumentation, and how these will best reflect the given lesson.\n- A festival of lessons and hymns doesn’t need to be boring—don’t sing each hymn the same way! Read and play it through, and consider how the introduction, stanza assignment, harmonization and instrumentation can best convey the spirit of the piece. Our service plan designates stanzas for choir only, women, men, a cappella, in harmony, in unison, in canon. Know your congregation’s strengths and weaknesses and choose accordingly (if they’ve never sung a canon, you might skip it here). I wouldn’t use an alternate harmonization on the last stanza of every hymn, only on a few where the text calls for heightened treatment. Let the organ alone interpret one of the stanzas; let the congregation speak a stanza out loud. Use a flute or a violin to introduce a song. Use descants sparingly (after four or five they’re anticlimactic, but do use them, especially for the Resurrection, Ascension, and Presentation hymns). Have two trumpets play the fanfare for the majestic and stately “A Hymn of Glory Let Us Sing” as an introduction and as an interlude between each stanza. Add a tambourine and drums on the Israeli tune “Open Your Ears, O Faithful People.” Consider where piano would accompany best and ask a student to play. If the adult choir is singing, change “Entrance” to “Processional” Hymn and “Closing” to “Recessional” Hymn.\n- You may use a more general praise text for the Offertory anthem and Presentation hymn, although kingship references would be a plus. Try to find an anthem based on an appropriate hymn. The text of “Come, We That Love the Lord” is most appropriate (“Let those refuse to sing who never knew our God, but children of the heavenly King may speak their joys abroad. . . . We’re marching through Emmanuel’s grounds to fairer worlds on high. Hosanna . . .”); if the tune is unfamiliar, teach the more singable Southern Harmony one. Ralph Vaughan Williams’s arrangement of the Doxology (old hundredth) can be a jubilant outburst of praise at the presentation of the offerings, the highest point of the service. Have the adult choir lead (only one stanza), with the director cueing the congregation. Organ accompaniment is glorious; it’s even better with trumpeters.\n- The congregation sits or stands according to the content of the service (standing at the Resurrection and Ascension lessons, the Presentation Hymn, Closing Prayers, Blessing, and Closing Hymn). At the Passion and Death lessons, kneeling (or sitting) is optional.\n- Select readers based on age, experience, skill, vocal tone and speed, and how these relate to the lesson (for example, use a child for Birth, Epiphany or Childhood lessons). You might use more than one person or a group for a reading.\n- If your church has a banner for Christ the King Sunday, hang it in a suitable place or consider making a new one. You might also use the same design for the bulletin cover—think of what symbols (scepter, crown, a lamb) would best convey the themes from the texts and music you’ve selected. The liturgical color for the day is white (gold is also fitting).\n- Perhaps someone in your congregation could create a liturgical dance for one of the hymns or anthems. Or you might introduce a children’s education or worship program. With the basic form of the service given, the creative possibilities are endless.\n- Be sure to select instrumental pieces that are festive, majestic, solemn, and glorious. (I listed several organ options that are easy to moderate in difficulty.) Whether or not they are based on hymn tunes, they should reflect these qualities so that the worshiper is mindful of the majesty and awe of God, the glory of Christ’s reign, and our sharing in it.\nFestival of Lessons and Hymns\nSeveral options are given for each hymn and instrumental piece. All hymns can be found in Hymnal 1982 (Episcopal). Performance suggestions refer to that hymnal; other hymnals may have different or number of stanzas.\nDeo gracias (Agincourt Hymn); arr. Johnson\nDeo Gracias (Agincourt Hymn); arr. Walloon\n“Come, Ye Thankful People, Come,” st. george’s windsor PsH 527, PH 551, RL 18, TH 715, TWC 381\n“Praise, My Soul, the King of Heaven” lauda anima PsH 475, PH 478, RL 144, TH 76, TWC 26\n“Crown Him with Many Crowns” diademata PsH 410, PH 151, RL 600, SFL 181, TH 295, TWC 92\n*The Bidding and Lord’s Prayer\nDear people of God, on this Feast of Christ the King, let us hear and heed in Holy Scripture the story of God’s loving purpose from the time of our rebellion against him until the glorious redemption brought to us by his holy Son, Jesus, and let us sing the praises of our deliverance with hymns and songs of praise.\nBut first, let us pray for the needs of the world, for peace and justice on earth, for the unity and mission of the church, especially in our country and in this city. Let us remember the poor and the helpless, the cold, the hungry and the oppressed, the sick and those who mourn, the lonely and unloved, the aged and little children, as well as those who do not know and love the Lord Jesus Christ.\nAnd let us now pray the words which Christ himself taught us, saying . . .\nAdvent of Jesus\nLesson 1: Isaiah 11:1-10\n“There’s a Voice in the Wilderness Crying” ascension\nstanza 1, choir; stanzas 2-3, all\n“O Day of Peace That Dimly Shines” jerusalem PH 450\nBirth of Jesus\nLesson 2: Matthew 1:18-23\n“What Child Is This” greensleeves PH 53, RL 217, TH 213, TWC 150\nstanza 1, all; stanza 2, women; stanza 3, men\n“Break Forth, O Beauteous Heavenly Light” ermuntre dich PsH 343, PH 26, TWC 158\nchoir sings through once, then congregation\nEpiphany of Jesus\nLesson 3: Matthew 2:1-12\n“What Star Is This, with Beams So Bright” puer nobis PH 68, RL 227\nstanza 1, all; stanzas 2-3, all in canon, right side starts, then 4 notes later, left side starts\n“Earth Has Many a Noble City” stuttgart\nstanza 2, men; stanza 4, women\nChildhood of Jesus\nLesson 4: Luke 2:41-52\n“Fairest Lord Jesus” (“Beautiful Savior”)\nst. elizabeth PsH 461, PH 306, RL 370, TH 170, TWC 115\nstanzas 1-2, all in harmony; stanza 3, unison\n“Blest Are the Pure in Heart” franconia RL 236\nBaptism of Jesus\nLesson 5: Mark 1:9-11\n“Christ, When for Us You Were Baptized” caithness PH 70, RL 241\nstanza 1, choir; stanza 2, all; stanza 3 spoken\n“The Sinless One to Jordan Came” SOLEMNIS HAEC FESTIVALS\nstanza 1, choir; stanzas 2-4, all\nTemptation of Jesus\nLesson 6: Matthew 4:1-11\n“Forty Days and Forty Nights” aus der tiefe rufe ich PH 77\nstanzas 1-3, all in harmony; stanza 2, a cappella\n“Lord, Who Throughout These Forty Days” st. flavian PH 81, TWC 200\nstanzas 1-3 only\nLife and Ministry of Jesus\nLesson 7: Isaiah 61:1-4\n“O Zion, Haste” tidings PsH 525, RL 421, TH 444, TWC 731\nstanzas 1-2 only\n“Open Your Ears, O Faithful People” TORAH SONG\nchoir on stanzas, congregation on refrain or stanza 2, women; stanza 3, men\nPassion and Death of Jesus\nLesson 8: Luke 23:26-49\n“Were You There” PsH 377, PH 102, SFL 167, TH 260, TWC 218\nstanza 1, solo/cantor; stanza 2, organ solo; stanza 4, all\n“Go to Dark Gethsemane” petra PsH 381, PH 97, TWC 225\nstanza 3, a cappella\n[“Ah, Dearest Jesus” or “O Sacred Head, Now Wounded” are also fitting here]\n*Resurrection of Jesus\nLesson 9: Luke 24:1-12\n“He Is Risen” unser herrscher\nstanza 1, men; stanza 2, women; stanza 3, all\n“Jesus Lives! Thy Terrors Now” st. Albinus RL 320, TH 706\n*Ascension of Jesus\nLesson 10: Luke 24:50-53\n“A Hymn of Glory Let Us Sing” deo gracias/ agincourt hymn PH 141, TH 289, TWC 259\n[“O Love, How Deep, How Broad, How High” is also appropriate PsH 364, PH 83, RL 342, TH 155, TWC 193]\n“King Jesus Is A’Listening” (tenor/bass ensemble); arr. Dawson\n“To God Sing Praises” (youth choir with two flutes); Buxtehude\n“Praise God, from Whom All Blessings Flow” old hundredth PsH 638, PH 591, RL 556, SFL 11, TH 731, TWC 808\narr. Vaughan Williams\n“Come, We That Love the Lord” vineyard haven RL 575/576, TH 700\n“To God with Gladness Sing,” Camano\nClosing Prayers and Blessings\nThe Almighty God bless us with his grace; Christ give us the joys of everlasting life; and to the fellowship of the citizens of heaven may the King of Kings bring us all. Amen.\n“Crown Him with Many Crowns” diademata PsH 410, PH 151, RL 600, SFL 181, TH 715, TWC 92\n“Now Thank We All Our God” nun danket alle gott PsH 454, PH 555, RL 61, SFL 33, TH 98, TWC 374\n“Crown Him with Many Crowns”; arr. Wyton\n“Deo Gracias” (Agincourt Hymn); arr. Dunstable\n“Majestic Piece No. 1”; Arnatt\nAdditional Song Suggestions\nThe authors prepared their song choices from the Episcopal Hymnal 1982 and encouraged others to check their own hymnals for other options. Here are some suggestions that emphasize the kingship of Christ from Sing! A New Creation (Faith Alive Christian Resources, 2001). The song numbers are included in parentheses.\nWe Will Glorify (21)\nWe Bow Down (42)\nHe Is Exalted (41)\nPsalm 24: The King of Glory Comes (15)\nPsalm 145: I Will Exalt My God, My King (27)\nI Love You, Lord (16)\nJesus Is Our King (18)\nMake Way (98)\nSoon and Very Soon (106)\nMeekness and Majesty (109)\nMary Had a Baby (107)\nLos magos que llegaron a Belén/The Magi Went to Bethl’hem Long Ago (118)\nBring Forth the Kingdom (123)\nTake Me to the Water (236)\nThroughout these Lenten Days and Nights\n(sung to tallis canon) (129)\nLife and Ministry\nBlest Are They (122)\nBring Forth the Kingdom (123)\nPassion and Death\nMantos y palmas/Filled with Excitement (133)\nPsalm 22: What Wondrous Love (142)\nPsalm 118: Hail and Hosanna! (146)\nCeltic Alleluia (148)\nGod Has Gone Up with Shouts of Joy! (154)\nYou Are Crowned with Many Crowns (158)\nGracias, Señor/We Give You Thanks (219)\nI Love the Lord (226, 227)\nAny of the songs from the Entrance section, plus:\nAlleluia, Alleluia, for the Lord God Almighty Reigns (39)\nLord Most High (47)\nYou, Lord, Are Both Lamb and Shepherd (182)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:00215cfb-0a30-4a36-b29a-0e791709e647>","<urn:uuid:1d263055-7afa-417f-bfdb-e78ef7ca22d9>"],"error":null}
{"question":"What is the difference in power consumption between the Nvidia GTX 1080 Ti and GTX 1070 Ti graphics cards, and how does this relate to PSU efficiency ratings?","answer":"The Nvidia GTX 1080 Ti draws 250W while the GTX 1070 Ti draws 180W, a difference of 70W. This power consumption directly affects PSU requirements and efficiency. According to the 80 PLUS rating system, even the best power supplies (Gold and Platinum rated) only achieve around 90% efficiency, meaning some power is always lost during AC to DC conversion. Therefore, the lower power draw of the GTX 1070 Ti would result in less overall power loss during conversion.","context":["Ask any good computer technician what the most important component is in your PC and the likely answer will be the power supply (also known as the PSU, or power supply unit).\nThe PSU is designed to take an input voltage from mains power (here in Australia we use 240V) and then reduce and regulate the output to 12V or less, to power the components inside the PC. Looking at the power supply you will see a range of different plug connectors, designed to power such things as motherboards, hard drives, optical drives, and graphics cards.\nMost manufacturers will give a power rating to their particular PSU models such as 500 Watt or 850 Watt. Most people tend to think that the power rating of the PSU is all they need to know, but this is not the case. There are very good quality high efficiency power supplies on the market, but, conversely, there are many poor quality PSUs on the market also. Some cheap and nasty PSUs are lucky to achieve a sustained output of half their claimed power rating, meaning that your new 500W PSU may indeed only be capable of 250W of continuous output. Certainly not enough to power your new high-end gaming system!\nIn an effort to rationalise the labelling of PSUs and to promote energy efficiency, industry leaders devised the 80 PLUS rating system as far back as 2004. The 80 PLUS idea created a list of efficiency specifications that a PSU model needed to achieve across a range of their rated power loads. Any PSU submitted for testing which was able to meet these requirements was then awarded an 80 PLUS rating and is allowed to advertise this certification, as well as use the 80 PLUS certified logo. In early 2008 the 80 PLUS standard was revised to cater for newer, more energy efficient models and the 80 PLUS Bronze, Silver and Gold categories were created. To this list was added the 80 PLUS Platinum certification in October 2009.\nBelow is a table for Internal Non-redundant Power Supplies and the efficiency rating required at 20% load 50% load and 100% load to achieve certification in one of the 80 PLUS categories.\n|Fraction of rated load||20%||50%||100%|\n|80 PLUS Bronze||82%||85%||82%|\n|80 PLUS Silver||85%||88%||85%|\n|80 PLUS Gold||87%||90%||87%|\n|80 PLUS Platinum||90%||92%||89%|\nAs you can see by this list, the most energy efficient Gold and Platinum rated PSUs are averaging close to or slightly above 90% of their rated output.\nSome unscrupulous companies have used the 80 PLUS logos in their advertising or on the product packaging, when in fact their PSU has not been tested or certified. At DCA Computers we see such impostors on a regular basis. The eager vendor will put his hand on his heart and swear the unit has 80 PLUS certification while holding up a PSU that is adorned with a bright gold 80 PLUS sticker. Apart from the PSU being not much heavier than the cardboard carton it emerged from, it’s hard to tell the difference. When the PSU is fitted into a tower case, it then becomes a more challenging ruse. The only way to be absolutely sure that you are getting the genuine article is to check the validity of any certification claims by going to the following website. This site lists all manufacturers and models which have had certification status awarded to them. So far 2824 PSUs have been awarded 80 PLUS or higher certification, so there are definitely plenty of quality choices currently available on the market.","What You Need to Know About Picking a Power Supply for a Gaming PC\nFrom next generation CPUs, to the fastest RAM, to the beefiest graphics cards—if you’re building a gaming PC, chances are you’ve already got your heart set on the latest and greatest PC components.\nBut there’s one more component that often gets overlooked: the power supply. It won’t improve your framerate and it isn’t the most aesthetically appealing part of a build, but without a power supply, your gaming rig is literally dead weight.\nBeginners often make the mistake of settling for a cheap off-brand unit that boasts a large power supply rating. Others might overspend on a larger power supply rating to ensure they’ll have enough juice for their high-end components.\nBut if you’re looking to build a gaming PC that will last, a good power supply is a must. Here’s what you need to know about picking a power supply for a gaming PC.\nHow much wattage do you need?\nIt’s no secret that high-speed circuits such as GPUs and CPUs are energy hogs. Add fans, lighting, and liquid cooling to the mix, and you begin to understand how much energy you’ll need to power your rig.\nYou can find power consumption requirements for different components online from manufacturers or on sites such as Tom’s Hardware. Or you could just use a handy power supply calculator like this one from OuterVision. It lets you build a hypothetical rig by selecting parts from drop-down menus and calculate the recommended wattage for your system.\nHere are some typical benchmarks for power draw requirements for high performance PC components:\nCPU: Intel Core i7-9700K CPU, 95W\nVideo Card: Nvidia GTX 1080 Ti, 250W\nRAM: DDR4 2133 8GB, 3W\nSSD: Samsung SSD 850 EVO 500GB, 4W\nYou’ll notice that the video card takes up a lot of power. Maybe you don’t really need the latest flagship Nvidia GTX Titan series card, and can settle for last year’s model. The Nvidia GTX 1070 Ti only draws 180W. It won’t only save you on the cost of the card, but also the cost of your energy bill and the power supply unit for your rig.\nIt’s not all about size: 80 plus rating system\nBesides the raw power rating listed on a power supply, energy efficiency is another important metric that you’ll want to understand.\nYour GPU, CPU, and other computer components all run off of DC (direct current) power. Your wall outlet only supplies AC (alternating current) power. When your power supply converts AC power from the wall into the DC power used by the rest of the components in your gaming rig, at least some of that energy is lost as heat.\nThat means you’re never getting 100% of the power drawn from your wall. So how do you compare two power supplies that have close to or the same wattage rating?\nEnter the 80 PLUS rating system. It was created to encourage manufacturers to create more energy efficient power supplies. The 80 PLUS rating system certifies power supplies are able to convert more than 80% of the AC power received from your wall into the DC power needed by your components. There are six levels for non-redundant (power) power supplies:\n80 PLUS: 80%\n80 PLUS Bronze: 82%\n80 PLUS Silver: 85%\n80 PLUS Gold: 87%\n80 PLUS Platinum: 89%\n80 PLUS Titanium: 90%\nAll efficiencies were based on 100% of rated load. Efficiencies can be higher at lower loads, as with 80 PLUS Titanium hitting 94% efficiency at 50%.\nBudget for a quality power supply in your wattage range\nWhile it can be tempting to snag deals on power supplies with high wattage ratings, it’s important to do your due diligence and check user reviews, manufacturer reputations, power draw requirements, and energy efficiency ratings. Often times if a deal sounds too good to be true, it probably is.\nThe right way to save money on power supplies is to pick a quality unit from a reputable manufacturer that’s within your required wattage range. Be sure to add enough margin to account for any upgrades you plan to make in the future. Choose the best power supply for your needs.\nAbout the AuthorFollow on Linkedin Visit Website More Content by Cadence PCB Solutions"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:87961c32-dda2-45e2-bf92-99b28cf1d461>","<urn:uuid:b7d0371e-4029-4226-9649-59e5142a08f8>"],"error":null}
{"question":"How do green acouchies warn others about danger?","answer":"When green acouchies notice a potential threat, they communicate by rapidly stamping their feet in a timed pattern to warn other group members. When other acouchies hear this warning, they become still and then begin stamping themselves to relay the warning.","context":["Green acouchies live in arid-land forest and tropical rainforest habitats, making their homes in the dense undergrowth. They dig shallow holes which serve as nests, or otherwise use hollow logs or abandoned burrows. Acouchies prefer habitats near rivers and marshes. Although most populations inhabit lowland forests, the subspecies Myoprocta pratti archidonae lives along the eastern face of the Andes at an elevation of 2,400 ft (731 m). (Emmons, 1997; Lönnberg, 1925; Morris, 1962)\nGreen acouchies are small, hystricomorphous rodents weighing from 0.8 to 1.2 kg, with body lengths ranging from 298 to 383 mm. Like other members of the family Dasyproctidae, green acouchies have long, thin legs, short ears, a short tail, and generally cylindrical body shape. On each hind foot are three toes with long hoof-like claws. They have four toes, and a vestigial thumb with claw on each forefoot. Their whiskers are long and black. The dorsum is covered in grizzled, olive-green, fur, and each individual hair has alternating black and yellow bands. Their course pelage is longer over the rump but does not overhang the area as it does in red acouchies. Ventral pelage is pale orange, fading to white on the throat, chest, and mid-ventral region. Typically they have a bright yellow stripe behind the ear, although in the Andean subspecies, this patch is ochraceous orange. Young green acouchies have a similar appearance to adults, with more pronounced orange coloration in their fur. (Emmons, 1997; Kleiman, 1970; Lönnberg, 1925)\nGreen acouchies form monogamous pair-bonds, and the success and growth rate of offspring is dependent on the maintenance of parental pairs. When preparing to mate, males select a single female with whom they begin courtship. After an introductory approach, males initiate anogenital sniffing, followed by heavy tail-wagging, trembling, and rapid stepping or drumming of the front feet. If the female remains in place, the pair may begin grooming one another; however, foot drumming typically initiates a chase. Females continually walk away from males, pausing periodically to perform a “sex-crouch,” in which the back is lowered, and the rear and tail are raised to encourage pursuit. Males follow and frequently try to spray females with urine. If females continue to wander, males begin to move more rapidly and forcefully chase them with leaps and squeals until they either mount the female or courtship is discontinued. (Kleiman, 1970; Kleiman, 1971; Morris, 1962)\nGreen acouchies reach sexual maturity at 8 to 12 months, with specific timing depending more on body weight than on age. Like most other hystricomorph rodents, female green acouchies have a vaginal closure membrane, which is open during estrus or when birthing young. Estrus lasts approximately 40 days. Breeding takes place year-round, however, peak reproductive activity occurs from January to June, resulting in an elevated summer birthrate. Gestation lasts for an average of 99 days, with young typically born in litters of one or two. Newborn green acouchies are precocial, born with eyes open, and are capable of walking outside the nest within 1 to 2 days after birth. Although newborns can eat solid foods during their first week, they are not truly weaned until 6 to 8 weeks after birth. If they are weaned before two weeks old, acouchies show stunted growth and development. (Kleiman, 1970; Weir, 1971)\nIn addition to nursing, green acouchy mothers exhibit several types of parent care toward their offspring. Prior to birthing young, females, and sometimes males, construct small nests where they give birth. Once birthed, young are groomed extensively by their mothers, which continues periodically for more than two months after parturition. New born acouchies are capable of self grooming 1 week after birth. For two weeks after parturition, mother acouchies emit a purring sound to encourage nursing. Purring is also though to help young identify their mother from other acouchies. Nursing is only initiated in the nest. If young stray from the nest, mothers grab them in their mouths and carry them back to the nest. If another animal approaches the nest, mothers respond aggressively by attacking the intruder or threatening it with whining and piloerection. Female green acouchies are known to cross-foster (i.e., nurse and care for) the young of other females. (Kleiman, 1972)\nThere is no information available regarding the lifespan of green acouchies.\nGreen acouchies are diurnal but show peak activity towards dusk. Although behaviors in the wild are extremely difficult to observe due to the species’ evasive behavior, laboratory studies have identified a number of traits and social behaviors that are assumed to be widespread. Green acouchies live in small social groups with established hierarchies, maintained in part through agonistic behavior, including rearing and biting at one another, mouth-gaping, teeth-gnashing, and piloerection. To show submission, acouchies may respond by prostrating themselves (i.e., playing dead) or with high-pitched squeals. Males also occasionally scent-mark following an agonistic encounter, which may be related to territory defense; however, due to the limitations of laboratory study, the presence and extent of true territoriality in green agouchies is unknown. (Morris, 1962)\nGreen acouchies create and maintain a series of trails through the forest underbrush which they use on a daily basis, particularly when fleeing or chasing a rival. All acouchies in a social group may share the same set of paths, and all participate in maintaining them by fastidiously clearing away obstacles and packing the surface with their feet. Even as fully-grown adults, green acouchies practice “play-prancing,” a behavior in which they jump repeatedly up and down, while turning. Play-prancing does not appear to occur in groups exhibiting social tensions. (Morris, 1962)\nThere is no information available regarding the home ranges of green acouchies. However, it is possible its ranges are similar to those of red acouchies, which span from 9,600 to 12,000 m^2 during the wet season, and from 6,500 to 7,300 m^2. during the dry season. (Nowak, 1999)\nDespite the prevalence of intraspecific attacks, green acouchies display many cooperative communication behaviors. When individuals notice a potential threat, they rapidly stamp their feet in a timed pattern, using the noise to warn fellow group members. Those who hear the warning, become still and soon begin stamping themselves. Additionally, if exploring new territory, an isolated acouchy may perform “lost-calling” to contact other individuals, though this is more typically performed by juveniles than adults. (Morris, 1962)\nGreen acouchies are herbivorous, with a broad diet that consists of roots, fruits, nuts and seeds. Acouchies clean and prepare their food very carefully before consuming it, a habit assumed to prevent ingestion of toxins present in many vegetable and fruit peels. They also exhibit the unique behavior of scatter-hoarding in which individual food items are deliberately buried in separate locations rather than in a single cache. Not all edible items are cached; green acouchies prefer to cache those that are relatively large and hard. The chosen object is carried in the mouth while a pit is dug. The seed or nut is then deposited, stamped into place, and re-buried with the forefeet. Often, acouchies use a leaf or piece of bark to mark the location of the cache. (Morris, 1962)\nGreen acouchies are prey to a large number of rainforest-dwelling species, including humans. Known predators include South American coatis, white-nosed coatis, and jaguars. Their coloration likely helps camouflage them from predators and their burrowing lifestyle helps reduce risk of predation. (Cooke and Bruce, 2004; Emmons, 1997; \"Red Acouchi\", 2011)\nGreen acouchies are herbivorous. As scatter-hoarders, they aid in the dispersal of many plant species, particularly as nut and seed deposits are deliberately isolated and may easily be forgotten. Common throughout their geographic range, green acouchies are an important prey item for a number of different predatory species. (Cooke and Bruce, 2004; \"Red Acouchi\", 2011; Morris, 1962)\nThere are no known negative effects ofon humans.\nMyoprocta acouchy were once considered the same species. As a result, out-dated literature may refer to the green acouchy as Myoprocta acouchy, which is now considered a separate species, the red acouchy. In South America, green acouchies are known as \"cutiaras,\" \"chacures,\" \"punchanas,\" and \"tintíns.\" (Emmons, 1997)and\nElyssa Juni (author), University of Michigan-Ann Arbor, Phil Myers (editor), University of Michigan-Ann Arbor, John Berini (editor), Animal Diversity Web Staff.\nliving in the southern part of the New World. In other words, Central and South America.\nuses sound to communicate\nhaving body symmetry such that the animal can be divided in one plane into two mirror-image halves. Animals with bilateral symmetry have dorsal and ventral sides, as well as anterior and posterior ends. Synapomorphy of the Bilateria.\nuses smells or other chemicals to communicate\nhaving markings, coloration, shapes, or other features that cause an animal to be camouflaged in its natural environment; being difficult to see or otherwise detect.\nranking system or pecking order among members of a long-term social group, where dominance status affects access to resources or mates\nanimals that use metabolically generated heat to regulate body temperature independently of ambient temperature. Endothermy is a synapomorphy of the Mammalia, although it may have arisen in a (now extinct) synapsid ancestor; the fossil record does not distinguish these possibilities. Convergent in birds.\nparental care is carried out by females\nA substance that provides both nutrients and energy to a living thing.\nforest biomes are dominated by trees, otherwise forest biomes can vary widely in amount of precipitation and seasonality.\nan animal that mainly eats fruit\nan animal that mainly eats seeds\nAn animal that eats mainly plants or parts of plants.\noffspring are produced in more than one group (litters, clutches, etc.) and across multiple seasons (or other periods hospitable to reproduction). Iteroparous animals must, by definition, survive over multiple seasons (or periodic condition changes).\nparental care is carried out by males\nHaving one mate at a time.\nhaving the capacity to move from one place to another.\nthe area in which the animal is naturally found, the region in which it is endemic.\nrainforests, both temperate and tropical, are dominated by trees often forming a closed canopy with little light reaching the ground. Epiphytes and climbing plants are also abundant. Precipitation is typically not limiting, but may be somewhat seasonal.\ncommunicates by producing scents from special gland(s) and placing them on a surface whether others can smell or taste them\nreproduction that includes combining the genetic contribution of two individuals, a male and a female\nassociates with others of its species; forms social groups.\nplaces a food item in a special place to be eaten later. Also called \"hoarding\"\nuses touch to communicate\nthe region of the earth that surrounds the equator, from 23.5 degrees north to 23.5 degrees south.\nuses sight to communicate\nreproduction in which fertilization and development take place within the female body and the developing embryo derives nourishment from the female.\nbreeding takes place throughout the year\nyoung are relatively well-developed when born\nITV Studios. 2011. \"Red Acouchi\" (On-line video). itvWILD. Accessed April 17, 2011 at http://www.itvwild.com/clip/1468.\nCatzeflis, F., M. Weksler, C. Bonvicino. 2010. \"Myoprocta pratti\" (On-line). The IUCN Red List of Threatened Species. Accessed April 06, 2011 at http://www.iucnredlist.org/apps/redlist/details/136663/0.\nCooke, F., J. Bruce. 2004. The Encyclopedia of Animals, A Complete Visual Guide. Berkeley and Los Angeles: University of California Press.\nEmmons, L. 1997. Neotropical Rainforest Mammals: A Field Guide. Chicago: The University of Chicago Press.\nKleiman, D. 1972. Maternal Behavior of the Green Acouchi (Myoprocta pratti Pocock), a South American Caviomorph Rodent. Behaviour, Vol. 43, No. 1/4: 48-84.\nKleiman, D. 1970. Reproduction in the Female Green Acouchi, Myoprocta pratti Pocock. Journal of Reproduction and Fertility, 23: 55-65.\nKleiman, D. 1971. The Courtship and Copulatory Behavior of the green Acouchi, Myoprocta Pratti. Zeitschrift für Tierpsychologie, Vol. 29, Issue 3: 259-278.\nLord, R. 2007. Mammals of South America. Baltimore: John Hopkins University Press.\nLönnberg, E. 1925. Notes on Some Mammals from Ecuador. Journal of Mammalogy, Vol. 6, No. 4: 271-275.\nMorris, D. 1962. The behavior of the green acouchy (Myoprocta pratti) with special reference to scatter hoarding. Proceedings of the Zoological Society of London, 139: 701-733.\nNowak, R. 1999. Walker's Mammals of the World, Sixth Edition. Baltimore and London: John Hopkins University Press.\nWeir, B. 1971. Some Observations on Reproduction in the Female Green Acouchi, Myoprocta pratti. Journal of Reproduction and Fertility, 24: 193-201.\nWilson, D., D. Reeder. 2005. Mammal Species of the World, a Taxonomic and Geographic Reference. Washington, D.C.: Smithsonian Institution Press."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4021db4c-b300-4cc0-a846-5a466d081e16>"],"error":null}
{"question":"How does waste disposal works in incinerator system and what steps business can take reduce waste?","answer":"The incinerator system works through a sequential process: first, fuel is loaded into the combustion chamber, where burning occurs through primary and secondary air combination. Hot flue gases form and pass through an insulated chamber, then around the heat exchanger to heat water. For waste reduction, businesses can implement several steps: change operations or processes, improve material input methods like bulk loading systems, enhance maintenance regimes, better 'nesting' to reduce offcuts, and reuse waste material on site. For unavoidable waste, businesses should look for recycling opportunities and review waste collection contracts to ensure they suit business needs.","context":["In present condition solid waste management is a big issue, the solid waste is generated at domestic, commercial and agricultural levels which is to be managed and treated in a proper manner so that it can be used as energy source and also be less polluting. The traditional systems of dumping solid waste causes land pollution and the process of burning solid waste causes air pollution which is not desirable. Solid waste incinerator is the most useful device for managing wastes and it is the desirable technique which can be used for disposing all types of hazardous wastes. At domestic level such as house, the waste such as plastics, clothes, packing materials, cardboard, paper, garden wastes, Coconut wastes, etc. are not disposed properly. And if this is the condition of a single house then what would be of one city and that of whole country we cant imagine. This waste is just taken and dumped at the places out of city which causes the land pollution or burned which causes the air pollution. This condition can become critical if proper steps are no taken before its too late.\nTechnology Intervention :-\nThis traditional method of burning is modified in such a way that the burning process is carried out in a closed compact system called Solid Waste Incinerator. A Solid Waste incinerator, which is used for domestic hot water generation was designed, successfully constructed and tested. The natural draft system, primary and secondary air system is introduced along with it for clean burning. Performance testing of the system was carried out on large observation basis and concluded that the system is reliable for daily use. It could be very useful for urban and rural areas as well, especially in school/college hostels where papers and other non toxic solid wastes are generated in large quantities. It can prove to be useful in hospitals and villages for domestic use.\nTechnology Package : Components of Incinerator :-\n• Fuel loading door\n• Combustion chamber\n• Hot water tank (Heat Exchanger)\n• Cold water Inlet tube.\n• Pressure release valve\n• Ash box\n• Combustion hot flew gases passing chamber\n1. Fuel loading door : 400 mm x 300 mm Angle : 50 mm x 50 mm. Thickness : 5 mm\n2. Combustion Chamber : Diameter : 400 mm. Height 400 mm.\nCast Iron (CI) grate 300 mm x 300 mm. Thickness 16 mm. Construction in fire bricks\n3. Hot water Tank (Heat Exchanger) : Stainless steel (SS). Grade 302. Thickness 3 mm. Capacity 50 ltrs. Size : Diameter 400 mm. Height 450 mm.\n4. Cold water Inlet tube : Stainless Steel (SS). Diameter 25 mm. Inserted into heat exchanger at center. 25 mm above the bottom of the heat exchanger.\n5. Pressure release valve : At the top of heat exchanger. Stainless Steel (SS)\n6. Ash Box : 280 mm x 280 mm : Constructed in bricks.\n7. Flue gasses chamber : Diameter 600 mm. Height 650 mm.\n8. Chimney Height : 4.5 m. Diameter 150 mm. Thickness 3 mm (Mild steel)\nDisposal material : Cardboard, waste paper, empty milk can, garden waste, hospital waste, waste cartoon boxes, plastic with organic waste\n1. Load fuel from fuel chamber to combustion chamber.\n2. Fuel burning in combustion chamber.\n3. Combustion process takes place due to combination of primary and secondary air.\n4. Hot flue gases formed in combustion chamber, which passes in the flue gas chamber which is insulated by fire bricks.\n5. Hot flue gases passing around the heat exchanger.\n6. Thus heated tank surface heats the water.\nOperating cost : Rs.100/- per hour including fuel and labour.\nEmission : CO and PM tested with CO and PM Monitors separately. The results are within the permissible limits.\nExtension of Technology :-\nSolid waste disposal incinerator was successfully demonstrated and installed at corporate sector canteens, hospitals, Institutes training centres, Hostels, agro-tourisum centres, industries, village level gram panchayat and domestic use.\nCare should be taken while using incinerator\n• Ash near combustion chamber should be cleared regularly.\n• Chimney should be cleaned once in a month and carbon particulates should be removed by opening the chamber and then close the chamber.\n• Top hood of bricks should be opened and cleaned once in 3 months and then close the hood as before\n• Incinerator should be cleaned from inside by opening the drain valve once in three months\n• When dry solid wastes catches the fire load half dry waste inside combustion chamber\n• Before burning the hospital waste first burn the dry waste or wood and the load hospital waste.\n• Combustion chamber should be loaded by light weight waste of 2 to 5 kg and then burn it\n• It takes about 30 min to get hot water\n• Pressure wall used in the system, should be used regularly\n• Combustion chamber door should be always closed.\n• After removing hot water from the system cold water should be filled in the system\nClick Here For Download Technology Details","Good waste management starts with understanding the ‘real or true cost’ of material which ends up as waste. The real cost factors in the energy used to process the material on site, the handling and storage costs associated with the waste materials and the proportional purchase price of the unused materials. Taking into account all these costs, the real cost of waste to a business can be far greater than simply the cost of paying someone to take it away.\nThe first step to reducing waste in a business is to look in the waste bins to see exactly what is being disposed. Keep an eye out for high value materials because the ‘purchase to disposal ratio’ can be very high, and for wasted finished product, the real cost of waste can be hundreds of times the disposal cost (see case studies below).\nSegregation of waste is important as this allows a business to easily see what and how much is being wasted and, then, to identify what types of waste can be avoided. Segregation also separates waste to avoid contamination, making it easier for a business to either reuse wasted material in its operations or to sell the material for recycling.\nAt all times the key objective should be eliminating or reducing waste, especially high value waste materials. Actions a business could take to reduce waste include:\n- changes to operations or processes (see Ryan & McNulty case study)\n- improvements to how materials are inputted into process e.g. bulk loading systems (see plastic manufacturer Welvic Australia case study)\n- improvements to maintenance regimes, retooling or replacing equipment (see Mackay Industries and Devilee case studies)\n- better ‘nesting’ to reduce offcuts (see Pemara Printing case study)\n- reuse of waste material on site (see PACIA case study on Orica).\nFor unavoidable waste:\n- Look for opportunities to sell the material for recycling.\n- Review waste collection contracts. There is ‘no one size that fits all’ with waste contracts. Businesses should ensure that contracts suit the business’ needs, and there are significant savings that can be made.\n- For many businesses, especially retail, food and beverage industries, packaging can be a considerable cost. Business should look at reducing packaging or redesigning packaging to use fewer materials or so packaging can be reused. See Packaging for more ideas on how to reduce packaging waste.\nA resource assessment partly funded by Sustainability Victoria which reviewed the operations of clothing manufacturer Nobody Denim found that the real cost of disposed finished wasted garment was 425 times what the business was paying to dispose of it. Nobody Denim introduced changes to operations to reduce the amount of wasted ‘finished’ product. Read the Nobody Denim case study.\nA similar resource assessment of commercial printer Pemara Printing, found that the ratio of purchase price to disposal cost of its two key materials, ink and polypropylene plastic, was 70:1 and 55:1 respectively. Pemara introduced process changes including the optimising of layout of process. Read the Pemara case study.\nZero Waste SA's Waste Management and Reduction Guide for Retail Industry includes a number of case studies and sections on Identifying Waste Minimisation Opportunities (Section 4) and Marketing alternatives (economic advantages) (Section 6 ).\nThe USA EPA’s Business Guide for Reducing Solid Waste is a detailed guide providing step-by-step instructions designed to assist medium and large businesses to establish a waste reduction program. Key chapters include Getting Started and How to conduct a Waste Assessment. Overall emphasis is on avoidance and understanding the true cost of waste.\nThe Victorian EPA and Australian Industry Group have developed a step by step guide Developing a waste minimisation culture which summarises how to harness and enhance in-house knowledge.\nThe Australian Grape and Wine Authority’s publication The Lean Guide for the Australian Wine Industry discusses ways to better manage waste and to increase efficiency and productivity."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:78f4e64a-edc3-4c55-b6cb-fc0e406da466>","<urn:uuid:16160651-06dd-4663-8bc6-e4c5cf0e683a>"],"error":null}
{"question":"How do body positioning and balance considerations differ between lawn bowls delivery stance and baseball strength training exercises?","answer":"In lawn bowls, proper stance requires at least one foot on the mat at delivery, with feet ideally parallel for better balance and pointing in the bowl's intended direction, while the opposite foot moves forward during the bowling action. In baseball training, there's a shift toward front-loaded exercises like the Front-Loaded Split Squat instead of traditional Back Squats, as this allows athletes to better feel getting in and out of their lead hip and transfer rotational force from the ground up. Both sports emphasize balance, but lawn bowls focuses on static delivery position while baseball training incorporates more dynamic balance positions that better match playing field movements.","context":["- Games, Toys, and Hobbies»\n- Outdoor Games\nLawn Bowls - delivering the bowl\nThe whole essence of the game of bowls is getting your bowl to exactly where you want it to be, which means sending it down the green in the right direction and at the right speed. You must decide what are the correct line to aim for and the force that will be needed to do the job, but these will not be achieved if your technique is all wrong.\nThere are therefore some premlinaries that you need to get right, because unless you do, your efforts will get you nowhere!\nHolding the bowl\nThere is no absolute right and wrong here, as each player must do what works for them. The pictures on the right show two alternatives. In the first picture, the fingers are spread out underneath the bowl and the thumb provides support on one side. The weight of the bowl is held by the fingers and thumb and not the palm of the hand. You will see that the edge of the bowl has a ring of indentations on each side. These are known as \"grips\" and are intended to help the thumb grip the bowl more securely. (Not all bowls have grips, by the way).\nIn the second picture, the bowl is \"cradled\" rather than gripped. The fingers and thumb are not spread out and the bowl is supported mainly in the palm of the hand. This is a less popular way of holding the bowl than that shown above, although it is the method that I prefer myself. As it happens, that is my hand doing the holding here!\nBoth pictures are of left-handed bowlers who are about to send a forehand delivery, meaning that the bowl will bend from left to right as it rolls up the green. You can see the small club sticker on the bowl, which tells the bowler which way round to hold the bowl. For a backhand shot, the bowl would be held the other way round. For a right-hander, forehand means a bowl turning from right to left, and backhand means one going from left to right.\nStand and Deliver!\nThe rules state that at least one foot must be on or over the mat at the point of delivery, but that leaves plenty of room for choice as to how you arrange your feet when preparing to bowl. The important thing is that you are well-balanced and in control of the bowl at all times.\nIn the top picture, the bowler has one foot slightly in front of the other, but in the second the feet are parallel. I would suggest that the second bowler is more likely to be balanced than the first. Also, the feet should point in the direction that the bowl is going to go, which will not be in a straight line for most shots. Again, the bowler in the top picture appears to be pointing his feet straight down the mat, which is likely to mean that he will be sending his bowl on a line other than that in which his body is pointing, which is not recommended.\nThe bowling action of most bowlers is to bring the arm back at the same time as the opposite foot moves forwards. That foot should be firmly in place by the time that the arm comes forwards and releases the bowl at ground level, alongside the forward foot.\nHowever, some bowlers prefer to place the foot in postion before taking the arm back, and that is entirely their choice - whatever works best for you is the right thing to do.\nMany bowlers have a bad habit of releasing the bowl above ground level and letting it hit the ground a foot or more in front of them. Not only is this practice less likely to produce good results, but it also risks damaging the green. You will never see a good greenkeeper who bowls like this!\nAs the bowl is released, the arm should follow through and end up pointing in the direction that the bowl has travelled. It is worth practising the arm motion and getting someone else to watch exactly what you do. One of the most common faults in bowling is for the arm to come across the body rather than go in a straight line. This will send the bowl \"narrow\", rather like a hook shot in golf, and it will end up a long way from where you want it!\nAiming the bowl\nFor a \"draw\" shot, in which you want your bowl to end up close to the jack without necessarily moving it or any other bowls, you must send the bowl along a line that will allow it to describe an arc, going away from the jack to start with but then swinging back towards it as it slows down.\nDifferent weights and makes of bowl have different properties as to how much they swing as they move down the green. You will soon get to know your bowls and therefore how much \"green\" to give them.\nThere are all sorts of conditions that can apply to a particular delivery and determine whether you need more or less green on a specific occasion. These can include the state of the particular green you are bowling on, for example, whether it runs fast or slow, or whether it has \"runs\" in it that make a bowl run off course. If you are bowling to a short jack you will need more green than if bowling to a long jack, because a bowl swings more when running slowly, and you need less speed for a short jack. Even the strength of the wind can effect how a bowl runs.\nWhen you have decided the line you want to take, focus on a point and bowl in that direction, without looking at the jack itself. The point could be a mark on the bank at the far end of the green, another bowl that has already been sent, or even the foot of a player who is standing behind the \"head\" and facing towards you. When starting to play, your skip may indicate the line you need to bowl at.\nWhen you have released the bowl, stay on the mat for a few seconds to watch the course that the bowl follows. You will learn from this whether your delivery has been correct, and whether you chose the right line. Do NOT, under any circumstances, run down the green in pursuit of your bowl. It holds up the game and greenkeepers hate it when players do this!\nA final tip about delivering the bowl. Always check which way round you are holding the bowl before you send it. If you are trying to send a forehand delivery and you are holding it as if for a backhand shot, it will go merrily on its way and, instead of going from right to left (for a right-hander) go from left to right, and probably right off the rink altogether. This is called \"wrong bias\", leading not only to a wastewd shot bus also considerable merriment on the part of all the other bowlers and the bill for the first round in the bar landing on you!\nTalking of which, the last picture above shows a perfect delivery - of the drinks, that is!","Every baseball player wants their training to pay off on the field.\nAlthough most players will get some benefit out of following a general athlete program they downloaded from the internet, there are some key tweaks that can be made to classic exercises to make them both more effective and more safe for baseball players.\nThis article outlines five common exercise substitutions to consider when you’re programming for baseball players.\nDisclaimer: Always program workouts based on the athlete, not the sport. Though these exercise substitutions have relevance to be included in a baseball program, a training program is created in response to the athlete’s unique needs, not the sport. These substitutions are for athletes who require similar training adaptations to the original exercise, which will be many baseball players, but not necessarily all of them.\n1. Front-Load Split Squat Instead of Back Squat\nThe Back Squat has been a staple in training programs since the advent of the barbell. Recently, many strength coaches are moving away from it in favor of front-loaded exercises that better position players to utilize core musculature during workouts. This avoids gross lumbar extension patterns and cranky shoulders that have to hold the bar in place.\nIn my eyes, the Front-Loaded Split Squat holds two key advantages over a Barbell Back Squat for baseball players.\nFirst, it allows an athlete to “feel” getting in and out of their lead hip. This is vital for players in transferring rotational force from the ground up.\nSecondly, this exercise moves the muscular “weak link” from the middle of the body to the legs, a valuable safety consideration.\nFailure during a Front-Loaded Split Squat is normally due to inadequate lower-body strength. Failure during a Back Squat is often due to insufficient core bracing, which puts the lower back at risk during fatigued reps.\nBy replacing the Back Squat with the Front-Loaded Split Squat, we train toward a pattern that athletes more commonly use on the playing field. We also reduce the risk of injury during heavy and fatigued training sets.\n2. Resisted Push-Ups Instead of Bench Press\nThe Bench Press can help almost anyone gain muscle in their chest and shoulders.\nWhile muscular development is generally great for a baseball player, the Bench Press does have its drawbacks.\nWhile lying on a bench, the scapulae are pressed between the athlete and the pad, compressed and immobile. The limited range of motion needed for the exercise means most lifters complete repetitions without their scapulae ever moving. As throwing and hitting require above average scapular movement, training done with fixed scaps inhibits the player from training vital movement quality that will be demanded of them in competition.\nResisted Push-Ups solve this problem by allowing the scapulae to freely move on the ribcage during pressing. Push-Ups require a more neutral bracing strategy, guiding an athlete out of the lumbar extension pattern and into a more neutral, back-friendly position for pressing.\n3. Landmine Press Instead of Overhead Press\nHistorically, baseball players struggle with overhead exercises.\nThrowing sports place great demands on the latissimus dorsi and pectoralis, musculature that opposes overhead motion.\nHowever, not training any sort of overhead motion is a missed opportunity.\nThe throwing motion demands the ability to upwardly rotate the scapula and rotate the arm overhead at rates greater than 7,000 degrees/second. Recently in baseball-specific training, a greater emphasis has been placed on overhead mobility and strength exercises, but ballplayers still often struggle to achieve the 150-180 degrees of shoulder flexion needed to create a good Dumbbell Overhead Press.\nThe Landmine Press demands smaller angles and can be used as a regression to achieve the overhead training effect without loading a pattern the athlete can’t perform with quality.\nBy adjusting from half-kneeling to standing, a trainer can progress an athlete safely and effectively, challenging shoulder flexion as the athlete improves.\n4. 3-Point Row Instead of Bent-Over Row\nAlthough baseball demands a strong upper back, the common Bent-Over Row can negatively impact athletes for a variety of reasons.\nChallenging the row pattern in the bottom half of a hip hinge can hamper loading by placing undue stress on the lower back while neglecting the true purpose of training the row pattern.\nBy replacing the Bent-Over Row with a Single-Arm Row variation, we achieve a unilateral training effect and can load the pull aggressively without sacrificing lumbar health.\n5. Zottman Curl Instead of Barbell Bicep Curl\nDirect arm training is a topic of debate among coaches, but the fact is most players will want to train their biceps. And when done right, it can definitely benefit the athlete.\nThe Zottman Curl is an upgrade over the Barbell Bicep Curl for two key reasons.\nFirst, the exercise allows for the hands to rotate, allowing a natural flexion/extension pattern at the elbow joint.\nSecondly, this technique trains isometric wrist extension, an often-overlooked training effect that can stabilize the elbow during high velocity throwing and hitting.\nWith these techniques arising in prominence for baseball training, these five exercises can bring value and variety to your program. While sport-specific training only goes as far as the athlete’s unique needs, paying attention to what the sport of baseball demands can help make these critical exercise selection decisions.\nPhoto Credit: RBFried/iStock"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:cc4c66ae-c5d0-4a8b-b348-8f22596a6450>","<urn:uuid:296f6499-8fce-4e10-bb69-7ab9120430e7>"],"error":null}
{"question":"What services does Creative Humanities offer to organizations seeking innovation and strategic planning support?","answer":"Creative Humanities offers four main services: 1) Creative Problem Solving (CPS) sessions that bring together diverse perspectives to generate innovative ideas, 2) Strategic Planning services that help organizations imagine and achieve future goals, particularly for art and culture organizations with limited resources, 3) Innovation Training programs designed to enhance team creative talents through hands-on, research-based workshops, and 4) Knowledge Mobilization services that transform knowledge into accessible outputs through various formats including art installations, illustrations, zines, videos, reports, and interactive experiences.","context":["Creative Humanities is a platform for applied research and innovation at Sheridan College. In 2016, the Creative Humanities was founded by Dr. Brandon McFarlane with the ambition of inspiring new approaches grounded in creative inquiry and critical innovation. We imagine optimistic futures for the humanities and higher education, and engage in the struggle work necessary to manifest those utopian visions in our everyday lives.This website features our projects, celebrates the accomplishments of student researchers, and invites community members to reach out to explore opportunities for collaboration.\nWe’re mobilizing our expertise in creative problem solving, innovation management, and the humanities to solve pertinent challenges in higher education and the creative industries.\nCanadian Literary Censorship Project\nWe’re trying to discover and document the extent of banned and challenged books in Canada. Insofar as literary works communicate the values, aspirations, and anxieties of a society, the censorship of those texts might be taken as a bellwether for the overall health of that society, a measure of its citizens’ willingness to engage in unorthodox ideas or even their complicity in inequitable systems of power.\nData from the American Library Association show that literary censorship disproportionately targets authors and audiences of systemically marginalized groups: people of colour and those who identify within the LGBTQ2S community are, historically, much more likely to have their literary works censored. In Canada, however, there is currently no comprehensive, systematic, or readily-searchable compilation of censored literary works. Led by Prof. Alexander Hollenberg (Principal Investigator) with support from Dr. Brandon McFarlane (Co-investigator), and Dr. Owen Percy (Co-Investigator), we seek to remedy this knowledge gap.Learn More\nChildren’s Book Jam 2021-22\nA partnership between the School of Humanities and Creativity & School of Visual and Creative Arts which pilots a new model for the creation of illustrated children’s books that explore themes of social justice. Breaking from current industry practices, the publisher, author, and illustrator collaborate throughout the creative and publishing process.\nSheridan: Everybody’s Got a Story 2021-22\nInspired by the Covid-19 lockdown and the desire to foster a sense of community, this project asks Sheridan community members open-ended, emotion-driven questions to get to the heart of their stories. The narratives will be published in an online, interactive map which shows how people from around the world converge at Sheridan College to explore their creativity. Once campus reopens, the stories may also be shared in an installation at the Creative Campus Galleries.\nRemaking Critical Theory 2020-21\nA partnership between the School of Humanities and Creativity & the School of Visual and Creative Arts that synthesized innovation management and artistic practices to help undergraduates better understand critical theory, and apply insights to solve challenges pertaining to equity, diversity, and inclusivity in their industries.Learn More\nMobilizing Social Innovation to Train the Next Generation of Theatre Entrepreneurs 2018-21\nPartnered with the Toronto Fringe Festival to enhance and expand its Theatre Entrepreneurs’ Networking and Training (TENT) program. Researched and formalized curriculum to provide year-to-year efficiencies and enhance rigor. Applied Creative Problem Solving to eliminate factors that were preventing equity-seeking communities from participating in TENT programming and recommended models for long-term fiscal sustainability.LEARN MORE\nBreaking Through Barriers to Gender Equity in Canadian Film 2018-19\nSynthesized existing research to identify key obstacles to gender equity in the North American film and television sector. Three undergraduate researchers facilitated CPS sessions with Breakthroughs Film Festival’s Board of Directors to create the organization’s first strategic plan which reimagined the not-for-profit’s mandate from the perspective of intersectional feminism.LEARN MORE\nCreative Humanities is an experiment in talent-orientated research. We’ve created new models that boost the career development of undergraduate students, regardless of their area of practice. Most humanities projects make ideas; we make talent.\nCommunity & Collaboration in the Time of Social Distancing\nTaylor Zantingh wanted to be part of something bigger than herself. Find out how her humanities research provided community and empowerment.\nFaizal Eidoo is applying his creativity to make Toronto’s art sector more inclusive.\nChoosing My Own Adventure\nSometimes the perfect opportunity doesn’t rush up to meet your while you’re looking for it. Robyn Miller shares how her humanities research experience helped her create her own opportunities.\nMaking Friends While Making a Difference\nLynne Li was looking to make friends after transferring to Sheridan. Learn how her new community is making a difference in Toronto’s theatre sector.\nLeap of Faith\nVictoria Webb’s professors and friends encouraged her to apply for a humanities research position. Find out how her leap of faith transformed her career.\nEmerging from the Background\nAura Torres shares how humanities research helped her become a more confident and assertive filmmaker.\nUsing Art to Inspire\nBy using art to inspire those around her, Sarah is giving back to her community, one project at a time. Sarah gives her take on professional illustration.\nThinking Outside the Box\nCreative thinking is possible in every field, and Ayesha Qamar is the perfect example. Even in computer coding, there is still room for imagination!\nTENT Spotlight: Mike Rugo\nMike Rugo shares his take on UX design, and what its takes to succeed in the industry. Click to learn more about his journey in the field.\nCall for Possibilities\nWhat would you do if you had unlimited time and resources for a scholarly, research, or creative activity? Wield up to 2,500 words to pitch, imagine, or dramatize your most daring, creative, and/or ambitious idea via a special research cluster to be published in English Studies in Canada. Guest edited by Brandon McFarlane and Sarah Banting, you can find full details by clicking ‘learn more.’LEARN MORE\nSpecial Issue of University of Toronto Quarterly\nThe University of Toronto Quarterly published a special issue on the Creative Humanities guest edited by Brandon McFarlane. Authors include Andrea Charise, Stefan Krecsy, Glenn Clifton, Edmund Martin Nolan, Dale Tracy, David Gauntlett, and Mary Kay Culpepper. Click ‘learn more’ to find out how humanities scholars are transforming higher education in Canada.LEARN MORE\nTo celebrate the first five years of the Creative Humanities initiative we created a little report showcasing how our projects impacted students, Sheridan College, and creative communities in the Greater Toronto Area.\nSpecial thanks to Jacquelyn Ferguson (Managing Editor), Faizal Eidoo (Layout & Design), and Sarah Whang (Illustrator) who collaborated with Dr. Brandon McFarlane to create the document.LEARN MORE\nSeeking New Perspectives\nOver the course of her career as an artist and professor, Hyein Lee has learned the importance of being exposed to diverse perspectives. Find out how contributing to Remaking Critical Theory transformed how Prof. Lee approaches art and education.LEARN MORE\nOpening Up to New Experiences\nLearning to take risks has brought Melodie Downey to a year of fulfilling social, academic, and professional growth. Find out how serving as a Research Assistant for Remaking Critical Theory helped Melodie thrive during lockdown.LEARN MORE\nAt the Crossroads of Critical Theory and Creative Practice\nWith Covid-19 turning the digital age into the virtual age, students and professors everywhere are adjusting to new ways of learning and engaging with education. Amidst this struggle, Dr. Alexander Hollenberg (Professor of Storytelling & Narrativity) wants to introduce new ways of approaching critical theory, content that typically feels “impenetrable”.Learn More\nStudent Success Award\nCongrats to Dr. Glenn Clifton (Professor of Literary Studies and Creative Writing) who received the 2020 Student Success Award for his outstanding contributions to undergraduate learning on the TENT project. Dr. Clifton facilitated debrief sessions with undergraduate researchers and helped them share their learnings through blog posts.\nJacquelyn Ferguson (BA, Creative Writing & Publishing) helped us develop curriculum for TENT. She transformed the 12 units into a blog series that introduces the basics of art-based entrepreneurship. Are you ready to unleash your inner artrepreneur?LEARN MORE\nSheridan Students Take Centre Stage at Toronto Fringe\nOavkille News featured our project work at this year’s Toronto Fringe Festival. “Sheridan College research students have a front row seat at this year’s Toronto Fringe Festival, which kicks off July 3. Sheridan students are generating a buzz in Toronto’s theatre industry for their pioneering work in expanding Toronto Fringe’s Theatre Entrepreneurs’ Networking and Training (TENT) program.”LEARN MORE\nTaking Care of Business\nThe Toronto Raptors aren’t the only ones bringing home hardware this spring: Victoria Webb (BBA Marketing Management) and Karly-Anna O’Brien (BBA Supply Chain Management 2019) won two major awards for their outstanding research. The Pilon School of Business all-stars have been applying their creativity and entrepreneurial know-how to train emerging producers in Toronto’s theatre community. These two exceptional women are slam dunking their way to success!LEARN MORE\nCreative Humanities Featured in Curiosities\nFind out how an ecceltic team of undergraduate researchers at Sheridan College are collaborating with Toronto Fringe to bring new ideas to the Theatre Entrepreneurs’ Networking & Training program. Together, they’re helping emerging producers thrive in the theatre business.LEARN MORE\nNew Opportunities for Toronto Artists\nWe designed and instructed innovation workshops for TENT & Toronto Fringe in July 2018. Find out what we did and what’s in store for fall 2018!LEARN MORE\nTENT Collaboration Featured in Alchemy\nThe word spreads thanks to Brittany German’s pen. The collaboration’s lead wordsmith, German (BA, Creative Writing & Publishing) is raising awareness about the impact we’re bringing to Sheridan students and Toronto’s theatre community.LEARN MORE\nCreative Problem Solving at the Canadian Association of Fringe Festivals\nWho doesn’t love a road trip? The mere suggestion invokes the idea of an adventure: discoveries to be unearthed, friendships formed, strengthened, deepened. All of this is exactly what happened when we took our collaboration with Toronto Fringe on the road one grey day in early November and headed west to the town of Windsor.LEARN MORE\nWe are building upon our track-record of impact by forging an alliance of community partners. Our goal is to mobilize Sheridan’s expertise in creativity, innovation, and culture in partnership with private, public, and not-for-profit organizations to bring transformative change to creative communities and industries.\nCreativity & Innovation Services\nCREATIVE PROBLEM SOLVING\nNeed new ideas? Craving diverse perspectives to evaluate and improve ideas? We specialize in creative problem solving (CPS), a methodology grounded in peer-reviewed research that facilitates the reliable generation of new and innovative ideas. Our CPS sessions bring together students and researchers from Sheridan’s diverse educational programs to help community partners solve challenges and explore opportunities. Whether you’re intersted in a one-off session or long-term collaboration, connect with the Creative Humanities.\nOur creative approach to strategic planning incorporates diverse perspectives to imagine utopian futures and how to get there. We specialize in helping art, culture, and other organizations that may have limited resources identify how to best serve their communities and find creative ways to do so. Researchers custom-design community engagement initiatives to collect input, and quantitative and qualitative methodologies for monitoring the sometimes abstract impact of art, culture, and creativity. Reach out to the Creative Humanities to find out how we can help you research, imagine, and actualize dream-visions for the future.\nBuild the capacity for sustainable innovation by enhancing the creative talents of your team. We design workshops and long-term training programs grounded in scholarly research on creativity and innovation. Our training programs are hands-on, fun, and accessible. Indeed, our past collaborators reported partnerships radically transformed their everyday approach to innovation, and boosted the creative confidence, skills, and engagement of their teams. Engage the Creative Humanities for no-bullshit innovation training grounded in decades or rigorous research.\nDo you want to share knowledge, research, ideas, expertise, or experience with a community? We excel at transforming knowledge into engaging and accessible outputs that are custom designed for your audience. Our knowledge mobilziation teams comprise of artist, designers, educators, and other creative practioneers who produce novel and aesthetically stunning ouputs. We’ve create novel outputs such as art installations, illustrations, zines, videos, and interactive experiences, and more traditional formats such as reports, presentations, blogs, thought leadership, and events. Partner with the Creative Humanities to enhance your organization’s impact and leadership through knowledge mobilization.\nPraise from Partners\n“The opportunity to work with Sheridan students over the course of the three-year partnership was invaluable. Hearing their feedback, listening to their questions and having insight to their processes of creative thinking were tangible tools we could take back into the organisation and implement at TENT. Beyond that we were also able to bring Brandon, Prof. Jennifer Phenix, and Paola Di Barbora to our CAFF conference (back in 2019, in Windsor) where they led a creative problem solving (CPS) workshop with Fringe leaders from across Canada and the US. We received such positive feedback as to how this opportunity opened their eyes to ways of problem solving that they had not considered before.”\nExecutive Director, Toronto Fringe Festival\n“After a year, we can say the benefit and value of the strategic planning sessions has been far-reaching in the organization and the implementation of these objectives has helped Breakthroughs Film Festival move forward in a positive and sustainable direction. We wouldn’t hesitate to collaborate with the Creative Humanities again, and encourage any arts leader to learn more about the impact their expertise in creative problem solving and creative leadership can bring to your organization.”\nBoard of Directors\nBreakthroughs Film Festival\nConnect to find out how to leverage Creative Humanities talent, expertise, and national funding programs in the post-secondary sector.Connect with the Creative Humanities\nWe are grateful for the funding provided by Sheridan College and the Social Sciences and Humanities Research Council."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:fb0c9451-1894-49e2-8596-e09ee371a71c>"],"error":null}
{"question":"How did Latin-inspired music and Spaghetti Westerns use whistling differently in their compositions?","answer":"In Latin music, as performed by Quartet San Francisco, whistling technique was represented through 'latigo' - a quick slide or glissando on string instruments that simulates a whip sound. In contrast, in Spaghetti Westerns, actual human whistling was used, most famously by Alessandro Alessandroni, who provided iconic whistling for films like 'For a Few Dollars More' and collaborated extensively with composer Ennio Morricone. While 'latigo' was just one of many techniques used in Latin compositions, Alessandroni's whistling became a defining characteristic of Western movie soundtracks.","context":["Latin music from\nQuartet San Francisco -\n'... constantly alive to dynamic nuances and lyrical subtleties ...'\nWhat is it about these Cohens? Minneapolis-born filmmakers Joel and Ethan are acclaimed worldwide for dry humour, sharp irony, shocking visuals and a growing string of top awards. While over on the West coast Jeremy and Joel Cohen (no apparent relation) are lead violin and cello (respectively) of the astounding, hugely invigorating, white-hot Quartet San Francisco; unchallenged exponents of driving, life-enhancing Latin rhythms.\nTogether with violinist Kayo Miki, violist Emily Onderdonk and John Santos on percussion they present Latigo -- sixteen irrepressible tango-style items which found a place in my top ten CDs for 2008.\nFor all their outward 'frisson' the QSF are constantly alive to dynamic nuances and lyrical subtleties that abound within this captivating repertoire.\nThe music is by outstanding masters of the genre; among them Oscar Hernández (born 1954), Armando Pontier (1917-1982), Matos Rodriguez (1897-1948), Chick Corea (born 1941), Leonard Bernstein (1918-1990) and Astor Piazzolla (1921-1992). It was largely Piazzolla who brought about the modern efflorescence of nuevo tango, capturing most of the international limelight the idiom occasioned. Here however we have a far greater spread of idiomatic, inspired Latin composition.\nIncidentally the title 'Latigo' refers to a tango performance technique: a quick slide or glissando of a left hand finger on a string instrument, simulating the sound of a whip. It derives in turn from a West US/Latin American word for the strap used in tightening a saddle girth. Latigo technique does not figure in all the sixteen QSF items.\nHernández' irresistible opening item Cachita catapults us headlong into a world of high-spirited euphoria: what better way to start?\nListen -- Rafael Hernandez: Cachita\n(track 1, 0:10-0:57) © 2006 ViolinJazz Recordings\nNext up is the tongue-in-cheek Milongueando en al Cuarenta, very stylish, very suave -- then Crowdambo, complete with syncopated rhythm, pizzicato, col legno, tremolando, portamento, a brief cadenza (à la Saint-Saëns) -- very catchy.\nArmando Pontier's A los amigos introduces a note of melancholy, punctuated by high drama and some complex polyphonic writing; music such as this is unlikely to pall over repeated hearings.\nFelicia places us more in the café and bar than on the ballroom floor; furthermore, one can hardly escape unexpected echoes of Hungarian/Viennese dance tradition. It's from the pen of Uruguayan pianist, composer and dance teacher Enrique Saborido (1877-1941).\nAnd what's this I hear, could it be the 'ghost' of Scott Joplin on track 6, Felipe? Then La Cumparsita -- a beguiling example of mood swings and as near to chamber music as Latigo gets.\nListen -- Gerardo H Matos Rodriguez: La Cumparsita\n(track 7, 2:28-3:42) © 2006 ViolinJazz Recordings\nBittersweet strings are heard again in Astor Piazzolla's tragic Melodia en la Menor, followed by his furiously upbeat Libertango, taking up thematic material from the preceding item.\nThe inexhaustible inventiveness of jazzman Chick Corea is exemplified to wonderful, swinging effect in his catchy Armando's Rhumba, though couples on a dance floor may be 'entrapped' to simply stop and listen.\nAn achingly lovely Carlos Gardel (1890-1935) lyrical El dia que me quieras brings more tranquil reflections while hard on Gardel's heels we're treated to a display of peerless virtuosity in Taquito militar by Mariano Mores (born 1922) -- four minutes enlivened with dazzling glissandi and a spotlight break with the 'kitchenware' of Santos.\nListen -- Mariano Mores: Taquito militar\n(track 12, 2:55-3:59) © 2006 ViolinJazz Recordings\nBack to the dance as Comme il Faut by Eduardo Arolas (1892-1924) and Gallo Ciego by Agustin Bardi (1884-1941) bring the listener straightforward tango rhythms, the latter highlighted with some luscious violin breaks.\nThe QSF's spin on Cool from Bernstein's West Side Story would surely have had Lenny cheering on the sidelines. These players pinpoint to perfection the uneasy (1950s) gang culture among New York's backstreet tenements.\nTo conclude we hear the unremitting, headlong drama of Piazzolla's (not for dancing) Nuevo Tango -- quite simply a tour de force in a programme of blazing ebullience, melting poetry and high drama.\nIf you're committed to close-knit strings and Latin rhythms then giving Latigo a miss would be doing yourself a disservice.\nThis 'knockout' QSF release was awarded two nominations for the 49th Annual Grammy Awards (covering the period 15 September 2005 till 14 September 2006). Latigo was recorded 22/24 August 2005 at Skywalker Sound (Marin County, California).\nCopyright © 17 November 2009\nRarotonga, Cook Islands\nCD INFORMATION: LATIGO - QUARTET SAN FRANCISCO","Goodbye to Alessandro Alessandroni, the western world's most famous ‘whistle’\nThe composer, conductor and arranger Alessandro Alessandroni died in Rome. He had just turned 92 years-old. Celebrated for his 'whistle' which made many great soundtracks of the spaghetti western genre. 'For a Few Dollars More' is its 'booed' most iconic.\nLa Repubblica ·\nBy Valeria Rusconi and Ernesto Assante\nMarch 27, 2017\n\"It's very simple. I phoned Ennio Morricone and he told me: 'Sandro, come down here for a moment, in the room, we need you to whistle. Well, it was really a whistle, nothing more, but think about what happened next ... When we saw the film, I have to admit that no one thought it would make a penny\". And instead. Instead the 'whistling' really did change everything. Alessandro Alessandroni, the master - it is right to call him that - says the opening words of the most famous of his career and the most iconic of Western movies song that for a Fistful of Dollars, made up by Morricone, which made the film music of Sergio Leone - and practically made all the best western movies - even bigger. \"It was a great professional partnership, we had a wonderful collaboration,\" he told La Repubblica. Morricone, \"knew very well I could play the guitar and was the director of the choir and this was superb. And he knew very well that I could whistle. He had worked on A Fistful of Dollars and on other occasions. Why I chose him to whistle? by chance, I needed a whistle, I asked the musicians working with me who was able to whistle well and others I liked less. He had the courage to try\".\nThe composer, conductor and arranger Alessandro Alessandroni died in Rome, in the city that gave him birth on March 18, 1925, on March 26th. He had just turned 92 years of age. The announcement came on the official Facebook page of the composer: \"It is with great sorrow that I inform you of the death yesterday of the master Alessandro Alessandroni born in Rome on March 18, 1925, composer, multi-instrumentalist, arranger and choir director. There will be a memorial service at his home in Namibia with music and musicians directed by his son Alex Jr. Alessandroni\".\nAlessandroni approached music when he was still a boy. At the time he lived in the country of his mother, in the province of Viterbo. He was 11 years old and listened insistently, whenever he could to classical music. He began playing the guitar with assistance from a friend. The place is one of those details. He told in an interview to the blog Planet Hexacord: \"I started in the barber shop, because in small countries it is a reference point: there were the instruments, the guitar, the mandolin. They worked a little, but it sounded a lot. .. \". While he was attending the last year of high school he formed his first band, with whom he performed for local dance halls. Quick to learn, in a short time he become proficient on several instruments, which he alternates during his performances: as a teenager he already is able to play the guitar, the piano, the accordion, sax, flute, mandolin and sitar, one of the first Italians to try their hand on this complex stringed instrument. He obtained his diploma at the Conservatory in Rome, and found a job in the film production company Fonolux There he meets the great Nino Rota, his senior by 14 years, who wants him in his orchestra. Then came the whistle. It was almost by accident. Alessandroni, at some point, when Rota asked for a volunteer to whistle. Whistling become his new tool to play with and one of the moments that characterized the soundtracks of the Spaghetti Westerns. Music in effect: \"My whistle parts are on the staff,\" explained Alessandroni, \"and woe to miss the pitch, to make mistakes.\" That thought also by Federico Fellini, author of his soprannonme: Alessandroni for him was simply \"The Whistler\".\nIn 1962 he founded the octet I Cantori Moderni, a formation that takes the place of his previous group, the Caravels Quartet. With him, the band is formed by soprano Edda Dell'Orso, Augustus Garden, Franco Cossacks, Nino Dei, Enzo Gioieni, Gianna Spagnuolo and, not the least, his wife Julia De Mutiis.\nThe most important co-operation, long-lived and linked by a sincere esteem Alessandroni remains to this day one with Ennio Morricone: besides the famous whistle of For a Fistful of Dollars he also worked on For a Few Dollars More and The Good, the Bad and the Ugly. Alessandroni was used by all the most important Italian composers of the time, in the 1960s, such as Piero Umiliani, for which he sang along with his wife Giulia in great song Mah-na Mah-na, extracted from the soundtrack of Svezia inferno e paradiso by Louis Scattini (1968) and the master Armando Trovajoli. With the arrival of the seventies, for ARC of the RCA label which was dedicated to the ‘young Italian music’, between beats and 'world exotico', a public-disc collection of twelve songs in the race to the edition of 1969 of Canzonissima. Are recorded, of course, the tune and work on the Hammond organ solo is credited to Ron Alexander, his pseudonym.\nThe name of Alessandroni had become one of worship across the board, and had crossed generations and musical styles, especially he had conquered the library music enthusiasts. Among the last to want in their drive Baustelle, group of Montepulciano, who have chosen it for one of their best albums. \"Alessandro Alessandroni is the oldest guest,\" explained Francesco Bianconi, the singer, \"a wonderful eighty-four and played the sitar, accordion, acoustic guitar and he did blow the whistle\". The song title, not surprisingly, was Spaghetti Western. The Album, Amen.\nBorn: 3/18/1925, Rome, Lazio, Italy\nDied: 3/26/2017, Rome, Lazio, Italy\nAlessandro Alessandroni’s westerns – composer, musician, whistler, choir:\nA Fistful of Dollars – 1964 [guitar, whistle, choir]\nMassacre at Marble City – 1964 [choir]\nFor a Few Dollars More – 1965 [guitar, whistle]\nThe Good, the Bad and the Ugly – 1966 [guitar]\nSeven Dollars on the Red – 1966 [choir]\nAny Gun Can Play – 1967 [composer]\nPayment in Blood – 1967 [choir]\nWanted – 1967 [choir]\nOnce Upon a Time in the West – 1968 [whistle]\nThe Wild and the Dirty – 1968 [composer]\nEl Puro – 1969 [composer]\nRaise Your Hands, Dead Man, You're Under Arrest – 1971 [composer]\nZorro the Invincible – 1971 [composer]\nThe Crazy Bunch – 1974 [composer]\nWhite Fang and the Gold Diggers – 1975 [composer]\nWhite Fang and the Hunter – 1975 [composer]\nLucky Luke – 1991 [whistle]\nLucky Luke (TV) – 1991-1992 [whistle]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:03346407-e4c8-4f6f-945c-98406199e0e7>","<urn:uuid:1a728149-280f-4449-aa0c-f52566e200f4>"],"error":null}
{"question":"Compare the renewable energy systems used in the UN headquarters in Montenegro and the Sabah Al Ahmad Financial Centre - what are their main power sources?","answer":"The UN headquarters uses a photovoltaic roof system spanning 1400 square metres that can meet the building's entire annual energy demand. The Sabah Al Ahmad Financial Centre utilizes both solar thermal energy and wind turbines as renewable energy sources.","context":["Architect in ChargeDaniel Fügenschuh\nStructural EngineeringMIchael shaw\nElectricalKing Shaw Asocciates Ltd\nMechanicalKing Shaw Assoicates Ltd\nBuilding ServicesKing Shaw Associates\nText description provided by the architects. The site is situated adjacent to the river Moraca with its picturesque and park like river landscape. To the North it is neighbouring the new millennium bridge a landmark suspension bridge with a span of 140 Metres and a 57 Metre high Pylon. Our proposal for the new UN-headquarter for Montenegro is to lower the landscape to the level of a proposed promenade walkway for pedestrians and cyclists connecting the city centre with its surrounding hinterland along the river.\nThe new building is set into the landscape to further extend its west facing riverbank to profit from both the river climate and the spectacular views of the new landmark bridge. The negative impact of a mayor traffic route in and out of town can be reduced.\nWhen crossing the river the building is received as a horizontal pendant to the vertical pylon and the bridges suspension cables.\nThe three defining elements of the UN Shared Eco-Premises are the different UN departments, the commonly shared facilities and the public areas all of which are situated on one single level. The six departments are expressed as differently shaped volumes and are unified by one shared roof slab that is perforated with large openings for natural ventilation and lighting.\nOpposite a generous corridor zone that works as a backbone to the building the mutual functions like meeting areas and conference rooms are located. A patio like strip of gardens and the retaining L-shaped perimeter wall is spatially defining these areas and is setting a secure border to the land behind. The wall is constructed out of rocks delivered by the water of the river and is constructed to cut back and retain the land to the back. A screen of Photovoltaic Cells floating above the roof slab works as a shading device and is to provide sufficient energy for the buildings entire electrical demand.\nThe building is designed to respond to the climate with a similar approach as is found in the vernacular architecture of the Mediterranean and Adriatic regions. The building is heavily shaded against the heat of the summer with a high degree of exposed thermal mass to regulate the internal temperature.\nThe ventilation system works on the displacement principal, using the heat generated in the office spaces to drive the air movement and allowing natural ventilation to work effectively for much of the year.\nHeating & cooling\nThe river flowing by the site provides a great source of environmental energy. In the summer the river is cooler than the air and can be used to cool the building. In the winter the flowing water is slower to chill down and can be used to heat the building using a heat pump. The energy to run the heat pump is derived from solar electricity generated by the photovoltaic roof. The energy generation from the photovoltaic system is greatest in the summer when the greatest cooling is needed.\nAt 1400 square metres the photovoltaic roof is large enough to meet the entire annual energy demand of the building. In this way the new UN headquarters building can be self sufficient in energy terms and not impose a burden on the regional energy economy, allowing other projects to be more readily developed. The capital cost of the photovoltaic system need not be borne by the UN headquarters project. We anticipate that the installation would be funded by an energy supplier who would then charelevation get the occupants for the supply of the electricity.","Sabah Al Ahmad Financial Centre Kuwait\nThe Sabah Al Ahmad Financial Centre, Kuwait is a mixed use development consisting of Offices and Hotel building spanning over 1,228,800 Sq ft of built-up area on a site area of 167,800 Sq Ft. Ten stories of the building (16th to 25 th) are earmarked for Hotel comprising a built up area for hotel of 2463,714 Sft. The Basement will be of two levels comprising total 129,120 Sft.\n|Owner:||Al Shaab National Real Estate Company|\n|Project Managers:||KEO International Consultants|\n|Architects:||KEO International Consultants|\n|Civil Engineers:||KEO International Consultants|\n|Landscape Architects||KEO International Consultants|\n|MEP Engineers:||KEO International Consultants|\n|LEED ® Consultant:||Green Technologies FZCO|\n- Energy Savings of 29.1% as a minimum\n- Potable water Savings in excess of 42%\n- Use of renewable energy sources such as solar thermal\n- Blackwater / graywater treatment to tertiary standards, for reuse\n- Use of native / adapted plant species for landscaping\n- 75% of Recycling of construction waste, to divert it from landfill\n- Use of environmentally friendly interior materials such as paints, adhesives and sealants\n- Use of environmentally friendly, biodegradable cleaning chemicals in the entire project\nThere will be ground floor, two mezzanine floors and 38 stories of this high-rise building. The Client Shaab National Real Estate Co. wants to pursue LEED rating from US Green Building Council in line with their commitment on Sustainable Design. The Development is pursuing LEED for Core & Shell v2.0. The project is one of the largest, prestigious sustainable developments in Kuwait and Middle East and the first building in Kuwait to pursue LEED.\nThe Architects and Engineers M/s KEO International Consultants are tasked with the design brief of the Client and have completed the design adopting sustainable strategies under their respective disciplines based on the board outlines of the Client’s requirements of sustainability.\nSustainable Features of the Development\nThe development pursuing LEED for the Building have incorporated a number of Green Strategies and adopted proven architectural and engineering technologies, some of which are mentioned as follows:\n- Site selection and development on an urban site\n- Adopting Erosion and Sediment Control Measures\n- Treating highly hazardous substance H2S present in underground water at site by taking remediation measures.\n- Availability of more than one bus stop with in ¼ Mile distance\n- 100% Underground Covered Car Parking Facility\n- Providing high Albedo Paints with SRI greater than 78 for the entire roof\n- Recycling of Gray Water through treatment plant for Irrigation use\n- Use of Water efficient Flush and Flow Fixtures\n- Elimination of CFC(s) for the Project\n- Compliance with ASHRAE 90.1 – 2004 and energy simulation modeling\n- Use of Renewable Energy by using Wind Turbine\n- Improved Façade/Envelope Design (Roof, Wall & Fenestration)\n- Using Energy Efficient Light (Lower Lighting Power Density)\n- Using Daylight Control and Occupancy Sensors\n- Use of Demand Controlled Ventilation System\n- Use of Chilled Water using compliant refrigerant R123 (Refrigerant Impact Total, considering ODP and GWP of Refrigerants.\n- Measurement and Verification – Installation of BMS, adopting M & V plan for the Base Building and Tenant Sub-metering\n- Provision of Storage and collection of recyclables for the Project\n- Construction waste Management – Diverting from Landfill\n- Use of Non-toxic eco-friendly materials\n- Use of Materials with high Recycled content\n- Endeavoring to maximize use of Materials with high Regional Material Content."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:b405609a-44ed-4243-9e03-f278cbeacb52>","<urn:uuid:a38319d8-a4db-4455-8efa-40590d69a790>"],"error":null}
{"question":"How do immigration authorities evaluate both criminal history and unlawful presence when determining admissibility to the United States?","answer":"Immigration authorities consider multiple factors: For criminal history, they look at Crimes Involving Moral Turpitude (CIMTs), where two CIMTs within the first 5 years of admission make someone deportable and inadmissible, though a petty offense exception exists for single minor crimes. For unlawful presence, someone who stays illegally for more than one year becomes inadmissible for 10 years after leaving the U.S. However, they may qualify for a waiver if they can prove extreme hardship to a U.S. citizen or permanent resident spouse/parent. The authorities evaluate the entire circumstances, including family ties, severity of violations, rehabilitation evidence, and community contributions when making final decisions.","context":["This has been discussed numerous times, but since there is a lot of confusion regarding this, it might be wise to clear things up. 2 CIMTs (not arising out of a single scheme) within the first 5 years of admission into the United States will make you deportable and inadmissible into the United States. A single CIMT within the first 5 years of admission also makes you deportable and inadmissible. However, there is an exception to this rule, ie. the petty offence exception. Basically, what it says is that if the offence (a CIMT and the only CIMT) is not punishable by a term of more than a year and the actual sentence served is less than 6 months. A suspended sentence is also considered a as having spent time in jail for immigration purposes. That's pretty clear I guess. But then comes the debate about Naturalization. Some say that the petty offence exception does not apply to Naturalization as it does to Deportation and Inadmissibility, others say it does. I'm talking within the 5 or 3 year statutory period here. This is from the USCIS: (2) Conditional Bars to Establishing Good Moral Character . (A) Effect of Crime Involving Moral Turpitude (CIMT) Convictions . An applicant who commits and is convicted of or admits to committing one or more crimes involving moral turpitude during the statutory period cannot establish good moral character and is ineligible for naturalization. See 8 CFR 316.10(b)(2)(i) . However, there is an exception to the general rule, which you must consider. See section 212(a)(2)(A)(ii)(II) of the Act. The exception applies if the applicant has committed only one CIMT and the crime is a petty offense. A petty offense is defined as a crime for which the maximum penalty possible for the crime does not exceed imprisonment for one year and , if there is a conviction, the term of imprisonment does not exceed six months, regardless of suspension. Thus, an individual convicted of a CIMT will only qualify for the exception if the two conditions are satisfied. Not only must the sentence imposed have been less than six months, the maximum possible sentence that could have been imposed must not exceed one year. For example, suppose an applicant commits petty theft in the statutory period. This was his or her only conviction. The applicant was fined and sentenced to one year of probation and community service. The maximum possible sentence for this conviction is 364 days. No term of imprisonment was imposed. The applicant meets both conditions of the petty offense exception and is not precluded from establishing GMC. In the above example, if the applicant had also received a suspended jail sentence of eight months, he or she would not meet the exception because he or she does not meet the second condition of the petty offense exception. The petty offense exception is inapplicable to an alien who has been convicted of or who admits the commission of more than one crime involving moral turpitude, even if only one of the two or more CIMTs was committed during the statutory period. An applicant who has committed more than one petty offense, only one of which is a CIMT, remains eligible for consideration of the petty offense exception. See Legal Opinion 95-12 . Of course, the IO has the discretion of establishing GMC. But according to the USCIS, they may also consider: - Is this the applicant’s only offense? - Did the unlawful act occur early or late in the statutory period? - What was the final outcome of the arrest? - How long was the applicant on probation? - Did the applicant comply with all conditions of the probation? So anybody care to comment on this. Has anyone been Naturalized having committed a petty offence within the statutory period?","I-601 Waiver Legal News\nThe applicant is a native and citizen of India who was found to be inadmissible to the United States pursuant to section 212(a)(9)(B)(i)(II) of the Immigration and Nationality Act (the Act), 8 U.S.C. § 1182(a)(9)(B)(i)(II), for having been unlawfully present in the United States for more than one year and seeking readmission within 10 years of his last departure from the United States.\nThe applicant entered the United States with a valid C1/D nonimmigrant visa in October 2003 and remained beyond the period of authorized stay. The applicant did not depart the United States until March 2008. The applicant is therefore inadmissible under section 212(a)(9)(B)(i)(II) of the Act for having been unlawfully present in the United States for more than one year.\nThe applicant sought a waiver of inadmissibility in order to reside in the United States with his U.S. citizen spouse and child. The field office director found that the applicant failed to establish that extreme hardship would be imposed on a qualifying relative and denied the Application for Waiver of Grounds of Inadmissibility (Form 1-601) accordingly.\nOn appeal, the AAO determined that the applicant had failed to establish that extreme hardship would be imposed on a qualifying relative. The appeal was subsequently dismissed.\nOn motion, the prior decision of the AAO was withdrawn and the I-601 Extreme Hardship Waiver approved.\nSection 212( a )(9) of the Act provides, in pertinent part:\n(B) Aliens Unlawfully Present. –\n(i) In general. – Any alien (other than an alien lawfully admitted for permanent residence) who-\n(I) was unlawfully present in the United States for a period of more than 180 days but less than 1 year … and again seeks admission within 3 years of the date of such alien’s departure or removal, or\n(II) has been unlawfully present in the United States for one year or more, and who again seeks admission within 10 years of the date of such alien’s departure or removal from the United States, is inadmissible.\n(v) Waiver. – The Attorney General [now the Secretary of Homeland Security (Secretary)] has sole discretion to waive clause (i) in the case of an immigrant who is the spouse or son or daughter of a United States citizen or of an alien lawfully admitted for permanent residence, if it is established to the satisfaction of the Attorney General (Secretary) that the refusal of admission to such immigrant alien would result in extreme hardship to the citizen or lawfully resident spouse or parent of such alien …\nA waiver of inadmissibility under section 212(a)(9)(B)(v) of the Act is dependent on a showing that the bar to admission imposes extreme hardship on a qualifying relative, which includes the U.S. citizen or lawfully resident spouse or parent of the applicant. The applicant’s U.S. citizen spouse is the only qualifying relative in this case. Hardship to the applicant or their child, born in 2012, can be considered only insofar as it results in hardship to a qualifying relative. If extreme hardship to a qualifying relative is established, the applicant is statutorily eligible for a waiver, and USCIS then assesses whether a favorable exercise of discretion is warranted. See Matter of Mendez-Moralez, 21 I&N Dec. 296, 301 (BIA 1996).\nExtreme hardship is “not a definable term of fixed and inflexible content or meaning,” but “necessarily depends upon the facts and circumstances peculiar to each case.” Matter of Hwang, 10 I&N Dec. 448, 451 (BIA 1964). In Matter of Cervantes-Gonzalez, the Board provided a list of factors it deemed relevant in determining whether an alien has established extreme hardship to a qualifying relative. 22 I&N Dec. 560, 565 (BIA 1999). The factors include the presence of a lawful permanent resident or United States citizen spouse or parent in this country; the qualifying relative’s family ties outside the United States; the conditions in the country or countries to which the qualifying relative would relocate and the extent of the qualifying relative’s ties in such countries; the financial impact of departure from this country; and significant conditions of health, particularly when tied to an unavailability of suitable medical care in the country to which the qualifying relative would relocate. Id. The Board added that not all of the foregoing factors need be analyzed in any given case and emphasized that the list of factors was not exclusive. Id. at 566.\nThe Board has also held that the common or typical results of removal and inadmissibility do not constitute extreme hardship, and has listed certain individual hardship factors considered common rather than extreme. These factors include: economic disadvantage, loss of current employment, inability to maintain one’s present standard of living, inability to pursue a chosen profession, separation from family members, severing community ties, cultural readjustment after living in the United States for many years, cultural adjustment of qualifying relatives who have never lived outside the United States, inferior economic and educational opportunities in the foreign country, or inferior medical facilities in the foreign country. See generally Matter of Cervantes-Gonzalez, 22 I&N Dec. at 568; Matter of Pilch, 21 I&N Dec. 627, 632-33 (BIA 1996); Matter of Ige, 20 I&N Dec. 880, 883 (BIA 1994); Matter of Ngai, 19 I&N Dec. 245, 246-47 (Comm’r 1984); Matter of Kim, 15 I&N Dec. 88, 89-90 (BIA 1974); Matter of Shaughnessy, 12 I&N Dec. 810, 813 (BIA 1968).\nHowever, though hardships may not be extreme when considered abstractly or individually, the Board has made it clear that “[r]elevant factors, though not extreme in themselves, must be considered in the aggregate in determining whether extreme hardship exists.” Matter of 0-J-0-, 21 I&N Dec. 381, 383 (BIA 1996) (quoting Matter of Ige, 20 I&N Dec. at 882). The adjudicator “must consider the entire range of factors concerning hardship in their totality and determine whether the combination of hardships takes the case beyond those hardships ordinarily associated with deportation.” Id.\nThe actual hardship associated with an abstract hardship factor such as family separation, economic disadvantage, cultural readjustment, etcetera, differs in nature and severity depending on the unique circumstances of each case, as does the cumulative hardship a qualifying relative experiences as a result of aggregated individual hardships. See, e.g., Matter of Bing Chih Kao and Mei Tsui Lin, 23 I&N Dec. 45, 51 (BIA 2001) (distinguishing Matter of Pilch regarding hardship faced by qualifying relatives on the basis of variations in the length of residence in the United States and the ability to speak the language of the country to which they would relocate).\nFor example, though family separation has been found to be a common result of inadmissibility or removal, separation from family living in the United States can also be the most important single hardship factor in considering hardship in the aggregate. See Salcido-Salcido v. I.N.S., 138 F.3d 1292, 1293 (9th Cir. 1998 (quoting Contreras-Buenfil v. INS, 712 F.2d 401, 403 (9th Cir. 1983)); but see Matter of Ngai, 19 I&N Dec. at 24 7 (separation of spouse and children from applicant not extreme hardship due to conflicting evidence in the record and because applicant and spouse had been voluntarily separated from one another for 28 years).\nTherefore, the AAO considers the totality of the circumstances in determining whether denial of admission would result in extreme hardship to a qualifying relative.\nThis case is useful to examine in what the applicant initially did WRONG when preparing their I-601 waiver application:\n- The I-601 waiver and supporting documentation submitted failed to specify the applicant’s spouse’s medical condition, the short and long-term treatment plan, the severity of the situation and what hardships the applicant’s spouse would experience were her husband be unable to assist her with the care of their child.\n- As for the emotional hardship referenced, the I-601 waiver and supporting documentation failed to establish that said hardships were beyond the normal hardships associated when a spouse relocates abroad due to inadmissibility.\n- With respect to the applicant’s spouse’s assertions that she would experience financial hardship were her husband to relocate abroad, no documentation was provided establishing the applicant’s spouse’s expenses and assets and liabilities to establish that the applicant’s relocation would cause his wife financial hardship.\n- The waiver and supporting documentation failed to establish that the applicant’s spouse would be unable to properly care for herself and her child while continuing her work as a physician.\n- Alternatively, it was not established that the applicant would be unable to obtain gainful employment abroad that would permit him to assist his wife financially should the need arise.\n- Finally, the applicant’s spouse had a support network in the United States, including her parents and sibling, and it was not established that the applicant’s spouse’s relatives would be unable to provide needed assistance to the applicant’s spouse.\nOn motion, counsel effectively addressed the issues raised by the AAO:\n- In a declaration the applicant’s spouse details that she is going through turmoil and anguish knowing that she and her child may be separated from the applicant for a ten-year period.\n- She explains that as a physician, her career will be in jeopardy if she shows any evidence of mental or physical anguish.\n- She contends that at times she has been so distraught at the idea of her husband relocating abroad that she has had to fight back tears while at work.\n- The applicant’s spouse further asserts that she has no support to help take care of her daughter as her parents are old and suffer from many ailments and the rest of her family does not live close by.\n- Moreover, the applicant’s spouse maintains that she and the applicant work part-time to ensure that one of them is with their daughter as much as possible and a change in that arrangement would cause her and her child hardship.\n- In support, counsel re-submitted an evaluation from a doctor that states that the applicant’s spouse’s anxiety and depression are a direct result of the circumstances surrounding her husband’s case.\n- The doctor concludes that were the applicant to re-locate abroad while his spouse remains in the United States, the applicant’s spouse will slip into a protracted depression.\n- The applicant’s spouse’s pastors have also provided letters outlining the hardships the applicant’s spouse and child would face were the applicant to re-locate abroad, including emotional turmoil and day to day hardships.\n- Moreover, numerous letters have been provided from the applicant’s friends outlining the hardships the applicant’s family will face without the applicant’s daily presence.\n- Finally, counsel submitted financial documentation establishing the applicant’s and his spouse’s income and expenses and noting that due to business losses, the applicant’s spouse may not be able to cover all the family expenses without her husband’s financial support.\nAll this thus established on motion that the applicant’s spouse would experience extreme hardship were she to remain in the United States while her husband relocates abroad as a result of his inadmissibility.\nHowever, the grant or denial of the waiver does not turn only on the issue of the meaning of “extreme hardship.” It also hinges on the discretion of the Secretary and pursuant to such terms, conditions and procedures as she may by regulations prescribe. In discretionary matters, the alien bears the burden of proving eligibility in terms of equities in the United States which are not outweighed by adverse factors. See Matter of T-S-Y-, 7 I&N Dec. 582 (BIA 1957).\nIn evaluating whether relief is warranted in the exercise of discretion, the factors adverse to the alien include the nature and underlying circumstances of the exclusion ground at issue, the presence of additional significant violations of this country’s immigration laws, the existence of a criminal record, and if so, its nature and seriousness, and the presence of other evidence indicative of the alien’s bad character or undesirability as a permanent resident of this country.\nThe favorable considerations include family ties in the United States, residence of long duration in this country (particularly where alien began residency at a young age), evidence of hardship to the alien and his family if he is excluded and deported, service in this country’s Armed Forces, a history of stable employment, the existence of property or business ties, evidence of value or service in the community, evidence of genuine rehabilitation if a criminal record exists, and other evidence attesting to the alien’s good character (e.g., affidavits from family, friends and responsible community representatives). See Matter of Mendez-Moralez,”-21 i&N bee. 296, 301 (BIA 1996).\nThe AAO must then balance the adverse factors evidencing an alien’s undesirability as a permanent resident with the social and humane considerations presented on the alien’s behalf to determine whether the grant of relief in the exercise of discretion appears to be in the best interests of the country.” Id. at 300. (Citations omitted).\nIn this case, the favorable factors are:\n- the extreme hardship the applicant’s U.S. citizen spouse and child would face if the applicant were to relocate to India, regardless of whether they accompanied the applicant or stayed in the United States;\n- community ties;\n- support letters from the church and friends;\n- the payment of taxes;\n- the apparent lack of a criminal record;\n- financial contributions to the church;\n- and the applipant’ s obtainment of an F -1 Visa and lawful entry after having accrued unlawful presence in the United States.\nThe unfavorable factors in this matter are the applicant’s periods of unlawful presence in the United States.\nAlthough the violations committed by the applicant were considered serious in nature, the AAO found that the applicant has established that the favorable factors in her application outweigh the unfavorable factors. Therefore, a favorable exercise of the Secretary’s discretion was considered warranted and the I-601 extreme hardship waiver approved."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c56af18d-7fca-4805-b76d-e7c572ce78c8>","<urn:uuid:2ce7498c-bf2c-48ba-b1d9-8ceb583559aa>"],"error":null}
{"question":"How does the concept of value in Saussure's linguistic theory compare to how alphabetic signs gained their phonetic value historically?","answer":"In Saussure's theory, linguistic value emerges purely from differences and relationships within the language system - a term's value results solely from its opposition to other terms, with no positive content of its own. In contrast, the historical development of alphabetic signs involved a gradual shift from pictorial to phonetic value through the rebus principle - where signs originally representing concrete objects came to represent similar-sounding words. For example, a sun symbol could come to represent the word 'son' due to phonetic similarity. This shows how phonetic values emerged through concrete connections, rather than just systemic differences.","context":["An alphabet is a particular form of script in which discrete graphic symbols or letters correspond with individual sounds of speech. It is thus a sign system that links graphemes, the smallest written mark that causes a change in meaning, with phonemes, the smallest unit of sound that distinguishes one word from another.1 In the Latin alphabet, there are 26 characters, each named and allotted a certain position in a sequential order. Each of these 26 characters can be represented by several forms, or graphs.2 Capital and lower-case script, or print and script letterforms are some of the most common English graphs. The word “alphabet” derives from the Latin alphabetum, which in turn came from the Greek alphabetos, a composition of the first two letters of that alphabet, alpha and beta. Alpha and beta evolved from the Phoenician letters aleph and beth, meaning “ox” and “house” respectively.3\nAlphabetic script is only one among many graphic ways of representing language. Others include pictographic, ideographic, logographic and syllabic scripts.4 Pictographs are simplified images of the things that they represent. For instance, a circle with lines radiating outward might be a pictograph for the sun. Ideographs represent a message or idea as a whole, rather than a particular instantiation of it. A picture of an arrow, indicating the direction to be followed, is an example of an ideograph. Logographic systems represent words with simple graphic signs containing no phonetic clues. For example, the $ sign, or # sign are logograms embedded within English. The signs that form logograms, pictograms and ideograms do not correspond with pronunciation. Both alphabetic and syllabic scripts, on the other hand, are phonograms, that is, they comprise signs representing phonetic units. In syllabic systems, unlike alphabets, each character stands for an entire syllable or sound group.5\nAll modern alphabets are derived from a script constructed by Semitic tribes in the Sinai and Canaan around the 2nd millennium BCE. Egyptian scribes had used a system of 24 or 25 unique phonetic symbols (all consonants) to record foreign proper names, but never replaced hieroglyphic pictographs.6 Because many of the earliest alphabetical inscriptions by Semitic tribes were found in copper mines in the Sinai, many scholars propose that these uneducated Semitic workers borrowed the idea of alphabetic script from Egyptian phonetic symbols, which were far simpler than hieroglyphs themselves.7\nThe Phoenician and Aramaic alphabets developed from this Proto-Sinaitic writing system. By the 8th century BCE, the Greeks had adopted the alphabet from the Phoenicians. Because vowels were more phonetically significant in Greek than in Phoenician, the meaning of five unused consonantal signs shifted to represent five of these vowel-sounds.8 The Greek alphabet was therefore the first to graphically represent both consonantal and vowel sounds, and thus this often considered the first true alphabet.\nRoman writing developed from a mixture of Etruscan and western Greek alphabets around the 5th century BCE. By the 3rd century BCE, this alphabet comprised 21 of the 26 letters we currently use. The last five letters, j, u, w, y, and z, were added over the following centuries. The modern English alphabet was not fixed until the advent large-scale publishing in the 19th century.9 [image: alphabetevolution.jpg]\nThe alphabet was derived from earlier, non-phonetic graphic systems. According to I.J. Gelb in A Study of Writing there were three ways in which the first logograms (in scripts like Sumerian and Egyptian Hieroglyphics) created meaning. The first type of logogram suggests a concrete object or action simply by means of a visual resemblance. The second operates through association. That is, a picture of a circle with lines radiating outward might not only mean ‘sun’, but also ‘bright’ or ‘day’. A third class of signs represented concepts with geometric forms (such as marks for documenting numerical value).10\nThe development that bridged non-phonetic to phonetic writing systems was the rebus. A rebus is a graphic sign whose meaning has shifted from the word depicted by the original pictograph to a word or word-part that is phonetically similar to that original word. For instance, Roy Harris, in The Origins of Writing, describes how a pictogram representing sun – that is a circle with lines radiating outward – might gradually come to indicate the word son as well. In this case, the symbol of the circle with lines radiating outwards would be a rebus, linked to both a visual (for sun) and phonetic (for son) reference.11 The sign for ‘arrow’ in Sumerian is a historical example of a rebus. In Sumerian, both the word ‘arrow’ and the word ‘life’ are pronounced ti. Gradually the sign depicting the word ‘arrow’ also came to stand for the word ‘life’.12 The shift from pictorial to alphabetic writing systems occurred as the original semantic value of signs was gradually displaced by phonetic value.13\nThe significance of pictorial and alphabetic representations as well as the meaning of the linguistic sign has been central to linguistic and philosophical discussion. The linguist Charles Sanders Peirce, in his 1898 essay “Logic as Semiotic: The Theory of Signs” distinguishes between three types of signs: the Icon, Index, and Symbol [link].14 The Icon refers which an Object by referencing some quality of the Object. Pictograms and ideograms are examples of icons. An Index refers to an Object by means of a causal relationship. For instance, smoke might be an index for a fire, or footprints for a passerby. A Symbol refers to an Object by virtue of a pre-determined law. Words formed by letters thus constitute Symbols. For Peirce, Icons and Indices imply an intrinsic relationship between Object and Sign, whereas Symbols operate on a relationship established through convention.15\nSaussure also asserts that convention, rather than an intrinsic relation to the object, forms the basis of the linguistic sign [link]. In his Course in General Linguistics, published in 1916, he divides this sign into two components – the ‘concept’ and the ‘sound-image’, or ‘signified’ and ‘signifier’ – that share no logical relation.16 He claims that language operates on difference rather than similarity. It makes meaning through an infinite variation of a finite alphabet. [image: sign.jpg]\nJacques Derrida also addresses the sign, composed of signifier and signified, in his book Of Grammatology (1967). For Derrida, however, each signified also serves as a signifier for some other object. For instance, the written word tree might be a signifier for the spoken word ‘tree’, which is a signifier for a physical tree, which could serve as a signifier for the idea ‘growth’, etc. This chain of signifiers and signifieds ends when a final signified, incapable of becoming a signifier, is reached. The end of this chain is pure objectivity, pure logos, pure presence. Derrida asserts that we currently live in an “epoch of logos”, characterized by the “philosophy of presence”, or a pre-occupation with the final signified and objective truth.17 This epoch began around the 8th century BCE with the advent of the full Greek alphabet. For Derrida, it was the development of a phonetically based writing system that introduced the very idea of ideal objects and scientific objectivity, and thus allowed for abstract thought and rational science (he borrowed this idea from Husserl’s Origin of Geometry).18 In Plato’s dichotomy of the intelligible and the sensible, Derrida interprets sensible objects as signifiers for intelligible objects, which he considers final signifieds. Alphabetic writing additionally made possible the existence of history and historicity.\nWhat Derrida proposes is an abandonment of the search for the final signified, for objective truth, as well as a rejection of the assumption that signifier and signified are equivalent. Rather, we should recognize that there exists an endless chain of signifiers and signifieds, which all vary slightly from one another. He calls this inherent variation between signifiers and signifieds differance (difference + deference). In other words, there are no pure ideas unmediated by language.\nW. J. T. Mitchell, in his 2003 article “Word and Image”, questions the model that casts words as arbitrary signs and images as merely relational ones. He not only points out that “many things resemble each other without being images,” but also that resemblance is not enough to invest a certain icon (or image) with meaning.19 Many icons do not look much like the objects that they represent. Icons (such as pictograms and ideograms) are also based in part on convention. He points out that Saussure’s diagram of the sign assumes that an image of a tree is equivalent to the concept “tree”, and ignores the fact that this image itself is a kind of representation that must be examined.20\nDerrida’s suggestion that the alphabet was crucial to the development of abstract philosophy and modern science in the west is echoed throughout linguistic and philosophical literature. Robert K. Logan, in his book The Alphabetic Effect (1986), an expansion from an article co-authored with Media theorist Marshall McLuhan, argues for what he calls “the intellectual by-products of the alphabet.” These by-products are, “abstraction, analysis, rationality, and classification, which form the essence of the alphabet effect and the basis for Western abstract scientific and logical thinking.”21 They result from the fact that an alphabet is a digital system. In other words, it transforms data into characters that are arranged in a code [link], requiring one to constantly code and decode, while only memorizing a limited number of letters.\nIn addition to encouraging logical and abstract thinking, many argue that the alphabet was an agent for democratization and for the development of democratizing technology. Because it phonetically based, it eases oral recording, and its 26 easily learned characters facilitate mass-literacy.22 Furthermore, the existence of only 26 shifting signs transferred easily to moveable type, which opened up reading and information to an even wider audience. Marshall McLuhan, in Understanding Media: the Extensions of Man (1964), describes the transition from hieroglyphs and ideograms to the alphabet as an example of a transition from what he calls a cool media, (lower data, higher interaction) to a hot media (higher data/sensory content). He then proposes a direct evolutionary link between the phonetic alphabet and the even hotter medium of typography.23\nHowever, many limitations to alphabetic script are often overlooked in traditional alphabet-based investigations, in what linguist Roy Harris calls the “alphabet bias”.24 The correlation between the graphic and the phonetic means that different languages using alphabets cannot share a common way of representing ideas, as is the case across language groups in China. Likewise, languages recorded in phonetically based script are separated in time from their own past forms, because pronunciation and vocabulary shifts are mirrored in writing. A modern Anglophone has difficulty reading Shakespeare, and is hardly able to understand Chaucer or Beowulf. Chinese speakers, on the other hand, can easily read texts dating from long before Chaucer’s time.25 The alphabet is not superior to other forms of graphic representation, but it does suggest a particular relationship between sounds and images that has significantly influenced modes of thought and modes of action in the west.\n1 G, in EL2nded.ed.DC (UP:NY, 1998), .\n3 “Alphabet,” Oxford English Dictionary Online (Oxford University Press, 2009) <http://dictionary.oed.com.proxy.uchicago.edu/> 27 Jan 2010\n4 Roy Harris, The Origin of Writing, (Gerald Duckworth & co. Ltd: London, 1986), 30.\n5I bid., 32.\n6 Robert K Logan, The Alphabet Effect (William Morrow and Co. Inc: New York, 1986), 33.\n7 Ibid., 34.\n8 “Alphabet,” in . e. PB (OUP: NeY), .\n10 I.J. Gelb, A Study of Writing (The University of Chicago Press: Chicago, 1952), 99.\n11 Harris, 32-33.\n12 Gelb, 104.\n13 Encyclopedia of Semiotics, 22-24.\n14 Charles Sanders Peirce, “Logic as Semiotic: The Theory of Sign,” In Philosophical Writings of Peirce, ed. Justus Buchler (Dover Publication, Inc: New York, 1955), 102.\n15 Ibid., 104.\n16 Ferdinand de Saussure, Course in General Linguistics, ed. Charles Bally and Albert Sechehaye. Trans. Wade Baskin (McGraw-Hill Book Co.: New York, 1965), 67.\n17 David Potts, “The Continental Origins of Postmodernism.” Cyberseminar. 1999 <http://www.objectivistcenter.org/obj-studies/cyber/DPDerr.asp> 27 Jan. 2010.\n18 Jacques Derrida, Of Grammatology (Johns Hopkins University Press, 1974), Ch. 2.\n19 WJT Mitchell, “Word and Image,” in Critical Terms for Art History, ed. Robert s. Nelson and Richard Shift (The University of Chicago Press: Chicago, 2003), 52.\n20 Ibid., 54.\n21 Logan, 21.\n22 Jack Goody, Interface between the Written and the Oral (University Press: Cambridge, 1987), 56.\n23 McLuhan, 23.\n24 Harris, 40.\n25 Encyclopedia of Communication and Information, 21-25.\n“Alphabet.” In , eited byPB, . OUP: NeY.\n“Alphabet.” Oxford English Dictionary Online. Oxford University Press. 2009 <http://dictionary.oed.com.proxy.uchicago.edu/> 27 Jan 2010\nDerrida, Jacques. Of Grammatology, Johns Hopkins University Press, 1974.\nGelb, I. J. A Study of Writing. The University of Chicago Press: Chicago, 1952.\nGoody, Jack. Interface between the Written and the Oral. University Press: Cambridge, 1987.\nG. EL2nded.,editedDC, . UP:NY, 1998.\nHarris, Roy. The Origin of Writing. Gerald Duckworth & co. Ltd: London, 1986.\nLogan, Robert K. The Alphabet Effect. William Morrow and Co. Inc: New York, 1986.\nMcLuhan, Marshall (1964). Understanding Media: The Extensions of Man. MIT Press: Cambridge, 1994.\nMitchell, WJT. “Word and Image.” in Critical Terms for Art History, edited by Robert s. Nelson and Richard Shift, 38-57. The University of Chicago Press: Chicago, 2003.\nPeirce, Charles Sanders (1898). “Logic as Semiotic: The Theory of Sign.” In Philosophical Writings of Peirce, edited by Justus Buchler, 98-115. Dover Publication, Inc: New York, 1955.\nSaussure, Ferdinand de (1916). Course in General Linguistics, edited by Charles Bally and Albert Sechehaye. Trans. Wade Baskin. McGraw-Hill Book Co.: New York, 1965.","Language is a system of differences without positive terms Essay\nLanguage is a system of differences without positive terms\nFerdinand Saussure was the first structural linguist to reorient the study of linguistics and to take as an object of study the analysis of an arbitrary order of signs and their correlation with language. The arbitrariness of the sign is pervasive and is visible in the sense that there is no intrinsic connection between the signifier and the signified and a sign can be analyzed without its semantic context. This placed the sign within a system of differential relationships between signs and language.\nThereby it became possible to study the basic elements of a language system as arrangements of contrasts and oppositions and arrive at “differences with no positive terms”. Saussure says a “linguistic sign exists only by virtue of its opposition to other signs; just as coins have values only within a particular system of coinage, and the identity of trains is only in terms of a particular railway system, so the links established between ‘significants’ and ‘signifies’ exist only through the system of oppositions by which, literally, that particular language is formed. The conclusion is stark and radical.” Hence, in a “language system there are only differences with no positive terms” (Saussure 972).\nIn order to arrive at an understanding of the “differences with no positive terms” Saussure divides language into two components. The first component is Langue which is an abstract system of language that has been internalized by a speech community. The second component is parole or the act of speaking or practice of language. While Parole is composed of heterogeneous, unrelated and differing elements, language is homogeneous union of concept and “sound image” or the signified and the signifier (both psychological).\nThis notion of Lang has challenged translators of the text in English. There have also been a number of debates on the status of this term. There have been questions as to whether this refers to a mental entity—“a sort of platonic idea or merely designates a methodological concept, an abstraction that is a part of a heuristic strategy. The issue has been, and remains, the articulation of the twin notions of langue and parole, the latter being no less difficult to translate into English than the former.\nSome have opted for an ontological distinction on the model of the philosophical tradition that opposes essence and existence or “accidents”; others have reduced the difference to the pragmatic necessity of evaluating instances of “languaging” with respect to the opposite poles of a continuum going from the normative, idealized representation of a language to the open-ended actual utterances that are usually observed in verbal interactions. That Saussure himself was not entirely satisfied with these correlate notions of langue and parole seems obvious from his numerous attempts to specify the distinction” (Bouissac 6).\nSaussure contended that language is systematic and it is possible to investigate it using methodology that is used in investigating pure science. Hence, he calls the “life of the sign”, a science. He names this science semiotics or the science that “studies the life of signs within society” (Saussure 962).\nThe task of the linguist, in investigating this science is to “find out what makes language a special system within the mass of semiological data” (Saussure 962) and if we must “discover the true nature of language we must learn what it has in common with all other semiological systems” (Saussure 962). Therefore, Saussure feels a need to begin with an understanding of the sign.\nSaussure offers a dyadic model of a sign in which the signifier and the signified are two parts of a whole. This is a mental model in which a sign must have a signifier and a signified and the relationship between the two–a signification. Thus the sign itself is “immaterial” (not abstract), as it does not fix the signification of the signified.\n“The linguistic sign unites, not a thing and a name, but a concept and a sound image. The latter is not the material sound, a purely physical thing, but the psychological imprint of the sound, the impression that it makes on our senses” (Saussure 963). He further elucidates the point: “without moving our lips we can talk to ourselves or recite mentally a selection of verse” (Saussure 963). Thus the definition of the linguistic sign is “a combination of a concept and a sign image” and consequently, Saussure proposes to “retain the sign [signe] to designate the whole and to replace concept and sound image respectively by signified [signifie] and signifier [significant]” (Saussure 963).\nIt logically follows, that the sign has two primordial principles: a) The sign is arbitrary by nature and b) The signifier is linear by nature. The arbitrary nature of the sign:\nThe linguistic sign is arbitrary and the consequences of this arbitrariness are infinite. The discovery of the arbitrariness is also not easy and requires many “detours” before they can be discovered. However, the discovery uncovers the primordial importance of this principle of linguistic signs. This very arbitrariness of the sign makes it ideal for semiological study and it is this principle that makes language the model for all other branches of semiology (Saussure 965).\nMoving on to examining the arbitrary nature of the linguistic sign, Saussure realized that reducing a sign to a symbol makes it less arbitrary because it creates a bond between the signifier and the signified. The linguistic sign is not arbitrary because there is no natural connection between the two. (Saussure 965). The argument that Onomatopoeia proves that a sign is not always arbitrary is dismissed as onomatopoeic “formations are never organic elements of the linguistic system” (Saussure 965). Interjections too show that there is no “fixed bond between the signified and signifier” (Saussure 966) and “Onomatopoeic formations and interjections are of secondary importance and their symbolic origin is in part open to dispute” (Saussure 965).\nThe linear nature of the Signifier\nThe auditory nature of the signifier implies that it has a span and the “span is measurable in a single dimension; it is a line” (Saussure 966). This principle, according to Saussure is very important because “the whole mechanism of language depends on it” (Saussure 966). Auditory signifiers “command the dimension of time” and “their elements are presented in succession; they form a chain” (Saussure 966). This linearity is visible in writing where “the spatial line of graphic marks is substituted for succession in time” (Saussure 966).\nHaving said this, Saussure moves on to consider language in terms of an organized system of pure values consisting of ideas and sound in order to arrive at the “differences without positive terms”.\nLinguistic Value: Language as organized thought coupled with sound\nIn examining language as organized thought and sound, Saussure finds that “there are no pre-existing ideas, and nothing is distinct before the appearance of language” (Saussure 967). Moreover “phonic substance is neither more fixed nor more rigid than thought; it is not a mold into which thought must of necessity fit but a plastic substance divided in turn into distinct parts to furnish the signifiers needed by thought” (Saussure 967). Therefore, language forms a “link between thought and sound under conditions that bring about the reciprocal delimitations of units” (Saussure 967) and becomes an “articulus in which an idea is fixed in a sound and a sound becomes the sign of an idea” (Saussure 967).\nIt follows that the signifier and the signified are intimately connected. The two cannot be separated just as two side of a paper cannot be separated. “Thought is one side of the sheet and sound the reverse side. Just as it is impossible to take a pair of scissors and cut one side of paper without at the same time cutting the other, so it is impossible in the language to isolate the sound from thought, or thought up from sound.” (Saussure 967).\nNevertheless, the “combination produces a form, not a substance” (Saussure 967) because it remains completely arbitrary. It is this arbitrariness that makes it possible to create a linguistic system. However, Saussure warns that it must not be assumed that it is possible to construct the system from the parts but the parts can be obtained from the whole by a process of analysis (Saussure 968).\nLinguistic Value: Conceptual View point\nThe next logical question that occurs to Saussure is “How does value differ from signification?” He concludes that, while conceptually signification is an element of value, it is not the same as value. It is in fact distinct from it. This is because “language is a system of interdependent terms in which the value of each term results solely from the simultaneous presence of others” (Saussure 969). “Initially a concept is nothing … is only a value determined by its relations with other similar values, that without them the signification would not exist” (Saussure 971). To better appreciate the significance of his finding he compares the concepts of value and signification as they exist outside of language. He finds that the same paradoxical principle governs values outside language.\nValues are composed of a) “dissimilar things that can be exchanged for the thing of which the value is to be determined” (Saussure 969) like a coin can be exchanged for a fixed value of another thing; b) “similar things that can be compared with the thing of which the value is to be determined” (Saussure 969) such as a two penny coin can be compared to another two penny coin.\nThe value of a word, therefore, “is not fixed so long as one simply states that it can be “exchanged” for a given concept, i.e. that it has this or that signification: one must also compare it with similar values, with other words that stand in opposition to it. Its content is really fixed only by the concurrence of everything that exists outside it. Being part of a system, it is endowed not only with signification but also and specially with a value, and this is something quite different” (Saussure 969).\nLinguistic Value from a Material Viewpoint\nDo these relations and differences between the terms of language and their value stand up to the test of linguistic value from the material viewpoint? Saussure thinks so. In his view the most important fact is that “the word is not the sound alone but the phonic differences that make it possible to distinguish it from all others, for differences carry signification” (Saussure 971). He does not find this surprising because “one vocal image is no better suited than the next for what is commissioned to express” (Saussure 971). Hence any analysis of a segment of language must be based on the “noncoincidence with the rest” (Saussure 971) and the “arbitrary and differential” are two correlative qualities of language.\nThe arbitrary and differential qualities of language are validated by the fact that the terms in a language are free “to change according to the laws that are unrelated to its signifying function” (Saussure 971). For instance no positive sign characterizes the genitive plural in Zen. Still Zena and Zenb function very well even if they replace the earlier forms of the word.\nIt has value because it is different. This quality of language is also validated by the fact that “signs function…not through their intrinsic value but through their relative position” (Saussure 971). This reveals the “systematic role of phonic functions”. For instance there is similarity in the formation of the words ephen and esten. However, the former is an imperfect and the latter is an aorist.\nIn this context Saussure notes that the sound is a secondary thing to language—a substance that must be put to use in language. The “conventional values” must not be confused with the “tangible elements” that support them. The linguistic signifier is “incorporeal” and “is constituted not by its material substance but by the differences that separate its sound image from all others”. This basic principle then applies to all material elements of language. He therefore, concludes that “every language forms its words on the basis of a system of sonorous elements, each element being a clearly delimited unit and one of a fixed number of units” (Saussure 971)\nFinally, considering the sign in its totality, Saussure quickly sums up his findings as “in language there are only differences” (Saussure 972). What are these differences?\nFirst language has “neither ideas nor sounds that existed before the linguistic system, but only conceptual and phonic differences that have issued from the system” (Saussure 972). In fact the idea or the phonic substance contained in the sign is of secondary importance as a change in the value of the term does not affect its meaning or its sound “solely because a neighboring term has been modified” (Saussure 973).\nSecond when we consider a sign in its totality (Signifier / Signified) there are no negative terms. Therefore “a linguistic system is a series of differences of sound combined with a series of differences of ideas” and the “the pairing of a certain number of acoustical signs with as many cuts made from the mass of thought engenders a system of values” (Saussure 973).\nThis system, then serves to “link the phonic and psychological elements within each sign” (Saussure 973). The combination is a positive fact that language uses to maintain classes of differences. The “entire mechanism of language” then “is based on oppositions of this kind and on the phonic and conceptual differences that they imply” (Saussure 973). This can also be applied to units and the characteristics of units can be seen to blend into the units themselves. So “difference makes character just as it makes value and the unit” (Saussure 973).\nSyntagmatic and Associative Relations\nSince Saussure views language as a something that is based on relationships, he divides relations and differences between linguistic terms into two distinct groups. These groups are associated with two types of mental activity that are essential to the life of language.\nWithin the discourse “words acquire relations based on ..linear nature of language because they are chained together” (Saussure 974). These are syntagnms. These syntagnms “acquire value because they stand in opposition to everything that precedes and follows them” (Saussure 974). Outside the discourse words can acquire a different relation. The syntagnms relations are in praesentia in which two or more terms occur in an effective series. Language belongs to syntagnmatic relationships built on regular forms. Associative relations are created by memory of the forms by comparing terms.\nUniversity/College: University of California\nType of paper: Thesis/Dissertation Chapter\nDate: 26 September 2016\nLet us write you a custom essay sample on Language is a system of differences without positive terms\nfor only $16.38 $13.9/page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ed856cc3-9cea-4865-85a4-7203cfb9c175>","<urn:uuid:a3865a0e-815a-4467-a8b5-5c6884cc34d0>"],"error":null}
{"question":"How do American and British interpretations of yards and gardens differ, and what potential contamination risks exist when managing these outdoor spaces?","answer":"In American English, 'yard' is the common term for outdoor space, while 'garden' specifically refers to areas for growing flowers and vegetables. In British usage, 'yard' traditionally refers to depots and uncultivated land near workplace buildings. Gardens are planned spaces for cultivating and displaying plants, with control being the key feature. Regarding contamination risks in these spaces, several factors must be considered. Surrounding vegetation can harbor pests and diseases that may infect the space. Air intake filtration is crucial, as unfiltered air will inevitably lead to contamination. Additionally, clothes and shoes can transport insects and mold spores, making it essential to implement decontamination protocols like changing clothes and using sanitizing foot baths before entering these spaces.","context":["What is considered garden?\nA garden is a planned space, usually outdoors, set aside for the cultivation, display, and enjoyment of plants and other forms of nature. The single feature identifying even the wildest wild garden is control. The garden can incorporate both natural and artificial materials.\nWhat is environmental gardening?\nSustainable Gardening is about maximising the benefits and reducing negative impacts to our natural environment. Gardening enriches the environment in many ways. For example, if we plant local plants we provide food and shelter for birds and butterflies.\nWhat is difference between backyard and courtyard?\nAs nouns the difference between courtyard and backyard is that courtyard is an area, open to the sky, partially or wholly surrounded by walls or buildings while backyard is a yard to the rear of a house or similar residence.\nWhat type of garden is best for the environment?\nRain gardens By planting native plants with deep roots in an area that will catch rainwater, you will help keep waterways cleaner, reduce mosquito breeding, create new habitat for birds & butterflies, and reduce erosion and flash floods.26 Apr 2019\nWhat does garden mean in the US?\nIn American English, the usual word is yard, and a garden refers only to land which is used for growing flowers and vegetables.\nWhat is a yard in the UK?\nThe yard (symbol: yd) is an English unit of length in both the British imperial and US customary systems of measurement equalling 3 feet or 36 inches. Since 1959 it has been by international agreement standardized as exactly 0.9144 meter.\nWhy is yard called a yard?\nYard: A yard was originally the length of a man’s belt or girdle, as it was called. In the 12th century, King Henry I of England fixed the yard as the distance from his nose to the thumb of his out-stretched arm. Today it is 36 inches.\nWhat is known as gardening?\nGardening is the practice of growing and cultivating plants as part of horticulture.\nIs gardening is an example of exercise?\nAccording to the Centers for Disease Control and Prevention, gardening qualifies as exercise. In fact, getting out in the yard for just 30-45 minutes can burn up to 300 calories.10 Aug 2016\nWhat does yard mean in UK?\nIn modern Britain, the term yard is often used for depots and land adjacent to or among workplace buildings, as well as uncultivated land adjoining a building.\nWhat is the difference between backyard and garden?\nA garden is more general and refers to an entire plot of land where one can plant vegetables, flowers, or fruits. A backyard, on the other hand, is particular to that area behind a house. Additionally, the location of a garden is not only restricted to the home. One can find gardens in many other places.9 Oct 2021\nWhat is an example of gardening?\nExamples include trellis, garden furniture, statues, outdoor fireplaces, fountains, rain chains, urns, bird baths and feeders, wind chimes, and garden lighting such as candle lanterns and oil lamps. The use of these items can be part of the expression of a gardener’s gardening personality.\nIs a yard the same as a lawn?\nTechnically the yard is the area that surrounds the house (mulch beds, pool, garden, lawn), while the lawn is the grass area within the yard.27 June 2018\nWhat is the difference between a yard and a garden?\nIn North America, the term “garden” refers only to the area that contains plots of vegetables, herbs, flowers, and/or ornamental plants; and the term “yard” does not refer to the “garden”, although the flower garden or vegetable garden may be within the yard.\nWhy do they call it a yard?\nThe term, yard derives from the Old English gerd, gyrd etc., which was used for branches, staves and measuring rods. It is first attested in the late 7th century laws of Ine of Wessex, where the “yard of land” mentioned is the yardland, an old English unit of tax assessment equal to 1⁄4 hide.\nWhat are the things in garden?\nNatural elements present in a garden principally comprise flora (such as trees and weeds), fauna (such as arthropods and birds), soil, water, air and light.\nWhat is the meaning of backyard gardening?\nA “Backyard Garden” simply refers to a home garden that can supply your family with fresh greens and vegetables daily. Making a garden near your home can help you to: • Have access to supply of fresh vegetables and fruits. • Saves time and money for not going to the market.26 Mar 2020","Broad mites, russet mites, spider mites, aphids, whiteflies, fungus gnats and powdery mildew are just a few of the threats for cannabis plants and cultivation facilities, be it indoor, outdoor or a greenhouse. As many cannabis pests as there are, there are about just as many potential infection/contamination sources.\nHere are some primary sources of contamination:\n- Home gardens. Some employees may have home grows (i.e., they grow cannabis and other crops in gardens at home). If said grow/garden is contaminated, it is possible to bring that contaminant to other locations, including that employee’s workplace. Some cannabis companies have put policies in place banning employees from participating in home cultivation. While this policy does indeed protect the business’s crop from home garden contamination, employers risk alienating talented, passionate prospective employees. Companies considering implementing these policies must weigh them carefully.\n- Unsterilized materials/supplies. Even if a product leaves the manufacturer’s facility in sterile condition, it can become compromised in shipping or by a distributor. For example, improperly storing soil or growing media in unsatisfactory conditions (e.g., a warm and wet/damp environment, near high traffic areas or near other plant matter), can lead to that medium getting infected. Always make sure you have a sterilized media/soil source by testing them in-house or with a trusted third-party lab.\n- Poor air intake filtration. For indoor and greenhouse facilities, there is a multitude of air filtration options. Greenhouses often utilize specific-sized bug screens on their air intake systems/vents, while most indoor facilities incorporate air filtration capabilities into their HVAC systems (including, but not limited to, UV light filters to kill powdery mildew spores). Failure to address air filtration in either facility is simply asking for problems. If a cultivation operation is not filtering incoming air, it’s not a question of “if” a facility will become contaminated, but “when” it will become contaminated.\n- Vegetation/crops surrounding a facility. Many indoor facilities and greenhouses containing cannabis or other crops have been infected by pests or diseases that were present on crops or vegetation that surrounded that operation. Cannabis cultivators should periodically inspect all surrounding trees, shrubs, and/or weeds, as well as any neighboring crops, for pests and disease.\n- Employee clothes/shoes. Insects and mold spores can hitch rides on your employees’ clothes, even in the time it takes for them to get from their parked car to the facility. Growers should set up decontamination protocols for all employees each time they enter the greenhouse or facility. Preferably all employees change clothes and shoes before entering a facility. Growers also can provide sanitizing foot bath floor mats at all doors, and/or disinfecting foam containing hydrogen peroxide at the entrance of a greenhouse. An employee decontamination chamber is also advisable for both indoor and greenhouse operations. To prevent the threat of infection by hand, all employees must wash or sanitize their hands prior to entering the facility, as well.\n- Clones/new genetics. In my experience, the No. 1 source of pest and disease in a cannabis cultivation facility is from new clones, especially those introduced during the pursuit of new genetics/cultivars. It is much less time-consuming to start a given cultivar via a clone of a preferred genotype that expresses preferred phenotypes than it is to start from seed. With seeds, growers must search for desirable traits and perform a “pheno-hunt,” a search for a phenotype from which to take clones to begin production. While easier to manage than seeds, sometimes those clones are indeed contaminated with a pest or disease (or multiples of both). All clones should be properly quarantined and guaranteed to be pest- and disease-free prior to introducing it to any production area. Preferably, new clones should be kept off-site (this discourages employees from checking in on new clones before moving to other production areas). At minimum, they should be held in a separate, contained space for at least one month. Assign dedicated employees who will inspect the new clones for signs of pests, disease or nutrient deficiencies, then have those employees sanitize themselves or change their clothing before moving on to production areas.\nRegarding neighboring farm crops, one must always be aware of how and when an infected neighbor/farmer uses preventative applications or treats their infected crops, as well, as those pesticides or harmful chemicals can affect your cannabis cultivation facility. Plan for the event and take necessary action such as closing ventilation openings and turning off any ventilation fans to prevent the intake of any potentially harmful chemicals. For instance, some Central Valley California greenhouses are in agriculture zones where airplanes crop dust large tracts of acreage. The potential for chemical drift is possible; therefore, it is best to know when a neighbor is planning any application of chemicals that could be harmful to cannabis plants or the humans consuming it.\nA Closer Look At Clones\nTypical past behavior has been to obtain a given desired cultivar in clone form and, after quarantine, to deem the clone a “mother,” meaning that the original clone is grown to maturity in order to produce more clones. Those clones could be grown to produce more clones (by turning them into mother plants), or simply utilized for production.\nThis has been the most common way cannabis growers have propagated crops for the past two decades (not counting autoflowering plants or feminized seeds). This system, unfortunately, led to what we now consider reckless behaviors and practices.\nAbout 20 or so years ago, I started noticing anomalies and unusual signs and symptoms on cannabis plants in multiple facilities in multiple states, starting in California. The symptoms were multiple: leaves curved sideways, with some showing discoloration as if it had been painted a faint yellow. Along with other strange deformities in new young growth, at its extreme, symptoms included diminished yields of up to 50% in combination with a major drop in both potency (THC content) and aromatic qualities (terpene content).\nAfter seeing these unexplained symptoms multiple times, I brought the observations up with one of my mentors, Robert C. Clarke, to see if he had heard of any disease or virus that would cause what I was observing. (Editor’s note: Clarke, co-founder of BioAgronomics Group, is a regular contributor to CBT and is featured in this issue.) He explained it could very well be a virus or another form of viroid contamination, but that there were no test kits available to positively identify the infection other than in a university setting, which was impossible at the time because the infected tissue was cannabis. But Clarke surmised the infection was either a virus or viroid and that possibly one could eliminate the infection via meristematic tissue culture—essentially taking a few cells from the newest plant material that had yet to be contaminated by the virus from which to propagate infection-free plants. Meristem tissue culture was a common practice in many other markets, including produce such as berries, but few if any cannabis growers used this process at the start of the millennium.\nHow a California Problem Became (Inter)National\nDuring the next 20 years, I continued to see the same symptoms in various gardens in multiple states but could never confirm what it was. I heard it hypostatized as hemp streak virus, then Sunn-hemp mosaic virus, but none of these were ever confirmed in any way. I’ve heard others describe their experiences dealing with similar symptoms as a broad mite infection to “genetic drift,” neither of which are correct.\nThen a few years ago I heard a term utilized to describe the symptoms I had been witnessing for years: “dudders disease” or “dudding disease.” While the terms did pick up in popularity, I’ve always had issues with them—I believed that whatever was causing these symptoms was not new, that it already had a name, and, as such, didn’t need a new one.\nWhile only a brave few would risk admitting it, in 2015, there were thousands of potentially virus- or viroid-infected clones being shipped to every emerging legal cannabis market in the U.S. from California—this is how a California-focused problem became a national one. If infected clones entered new legal markets, it could ultimately lead to the loss of many cultivars and create quality issues in those new markets. All because it was easier to have California clones sent eastbound than starting from seed.\nThe problem even crossed international borders.\nIn Canada, licensed production facilities had a given amount of time to obtain desired genetics, which they had to declare to the government. During the window of opportunity, some Canadian groups chose to obtain the most desirable genetics in the form of seeds and clones from the U.S. With those clones came the possibilities of viral infection. That may have been a blessing in disguise, however, as federal legalization in Canada opened the door to this infection being studied by government officials, university researchers, and private research labs (although little published research has been done on the viroid in cannabis).\nIn recent conversations with a friend who is part owner of a Canadian tissue culture company, I learned that some of the U.S.-born clones did indeed contain a viroid of some form or another, and that Canadian researchers were able to confirm the identity (although they were not the first) of the root cause of the symptoms I’ve been seeing for two decades: hop latent viroid, or HpLVd.\nHe also shared that, to prevent the possibilities of contamination caused by clones from the U.S., some Canadian cannabis producers chose to import seeds from America and other countries. I also learned that HpLVd is transmittable from parent plant to the seed it produces: if the mother is infected, so is the seed. (Canadian researchers traced HpLVd back to imported seeds.)\nThe most likely explanation for how the virus propagates is from the current cloning practices that have been employed for decades. When a clone is cut off the parent plant, the potential exists to contaminate the clone with a virus on the surface of the branch. When the cut was made, the virus was introduced to the cutting’s inner tissue.\nBecause the plant itself can become its own vector for disease, and because mother plants are especially at risk for viral contamination as the plant’s immune system gets weaker over time, infections generally can only be treated by destroying the crop, the mother plants and any clones and seed taken from those mother plants, and then starting from scratch.\nThat is unless growers can leverage meristematic tissue culture to save their genetics, something that Clarke had proposed years ago: meristematic tissue culture can rid the plant of certain viruses. (Not unrelatedly, tissue culture is also a great way to preserve a given cultivar for later use, or for storing useful male specimens for use in breeding projects. A cultivator could literally store 5,000 cultivar specimens in a relatively small area with a fairly low maintenance cost compared to upkeeping mother plants from which to clone, costs compounded by the potential risks associated with those clonal methods.)\nThe practice of cloning from a host mother is potentially problematic, and that tissue culture of cannabis for mass production is the only viable answer when one considers the risks of introducing a viral contamination by using clones and/or seed.\nAll of this to say: When considering disease vectors, cannabis cultivators also should look at their long-held belief and practices with an objective lens. Only unbiased evaluation should decide whether current practices put their crop, and their business, at risk."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:92869c33-c6e4-484a-9698-6f0db9ae9746>","<urn:uuid:25420ef1-8938-4fe9-8b7b-d3f91442cb61>"],"error":null}
{"question":"When is the radiant position most favorable for meteor observation, and what role does the Moon's phase play in viewing conditions?","answer":"The radiant position is most favorable when it is highest in the sky, which typically occurs when it lies on the meridian. For example, the Alpha Capricornids radiant is best placed near 0100 local daylight time, while the Antihelion radiant is optimal near 0200 LDT. The Moon's phase significantly impacts viewing conditions - for instance, during the Lyrids, a first quarter Moon can brighten the sky and drown out fainter meteors. However, this effect diminishes when the Moon sets (around 3 AM), coinciding with when the radiant gets higher in the sky, creating better viewing conditions for meteor observation.","context":["During this period the moon reaches its first quarter phase on Wednesday the 25th. At this time the moon will be located ninety degrees east of the sun and set near midnight local daylight time (LDT). This weekend the waxing crescent moon will set during the late evening hours and will not hamper observing efforts during the more active morning hours. The estimated total hourly meteor rates for evening observers this week is near four no matter your location. For morning observers the estimated total hourly rates should be near sixteen from the mid-northern hemisphere and fourteen from the mid-southern hemisphere. The actual rates will also depend on factors such as personal light and motion perception, local weather conditions, alertness and experience in watching meteor activity.\nThe radiant (the area of the sky where meteors appear to shoot from) positions and rates listed below are exact for Saturday night/Sunday morning July 21/22. These positions do not change greatly day to day so the listed coordinates may be used during this entire period. Most star atlases (available at science stores and planetariums) will provide maps with grid lines of the celestial coordinates so that you may find out exactly where these positions are located in the sky. A planisphere or computer planetarium program is also useful in showing the sky at any time of night on any date of the year. Activity from each radiant is best seen when it is positioned highest in the sky, either due north or south along the meridian, depending on your latitude. It must be remembered that meteor activity is rarely seen at the radiant position. Rather they shoot outwards from the radiant so it is best to center your field of view so that the radiant lies at the edge and not the center. Viewing there will allow you to easily trace the path of each meteor back to the radiant (if it is a shower member) or in another direction if it is a sporadic. Meteor activity is not seen from radiants that are located below the horizon. The positions below are listed in a west to east manner in order of right ascension (celestial longitude). The positions listed first are located further west therefore are accessible earlier in the night while those listed further down the list rise later in the night.\nThe following radiants are expected to be active this week:\nA new source found by the IMO video cameras to be active this time of year are the July Zeta Draconids (ZED). This radiant has been found to be active from July 19-29. Maximum activity activity occurs on the 19th from a position ofÂ 17:23 (263) +61, which is situated in southern Draco, five degrees southeast of the third magnitude star Al dhibain (Zeta Draconis). Due to a low amount of data the mean position of activity shifts quite a bit night to night so consider this a wide radiant until better parameters can be obtained. Current rates would most likely be less than one per hour no matter your location. The radiant is best placed near 2200 (10pm) LDT when it lie highest above the horizon. With an entry velocity of 23 km/sec., the average July Zeta Draconid meteor would be slow.\nThe Alpha Capricornids (CAP) are active from a wide radiant located at 20:04 (301) -12. This position lies near the Sagittarius, Aquila, Capricornus border, three degrees west of the third magnitude double star Alpha Capricornii. The radiant is best placed near 0100 local daylight time (LDT), when it lies on the meridian and is highest in the sky. Current rates should be near one per hour no matter your location. Don’t confuse these meteors with the antihelion meteors, which have a radiant just to the east. Both radiants need to be in your field of view to properly sort these meteors. With an entry velocity of 25 km/sec., most activity from this radiant would be slow, a bit slower than the antihelions. This radiant is well seen except for far northern latitudes where it remains twilight all night long and the radiant does not rise as high into their sky.\nThe large Antihelion (ANT) radiant is currently located at 20:48 (312) -17. This position lies in central Capricornus, four degrees west of the fourthÂ magnitude star Dorsum (Theta Capricorni). Due to the large size of this radiant, Antihelion activity may also appear from southern Aquila, Microscopium, eastern Sagittarius, northwestern Aquarius,Â and western Piscis Austrinus as well as Capricornus. This radiant is best placed near 0200 LDT, when it lies on the meridian and is located highest in the sky. Rates at this time are near their lowest of the year with one per hour no matter your location . With an entry velocity of 30 km/sec., the average Antihelion meteor would be of slow velocity.\nActivity from the Delta Aquariids (SDA) will begin this weekend from a radiant located at 22:18 (334) -18. This position is located in southwestern Aquarius, eight degrees southwest of the third magnitude Delta Aquarii. Maximum activity is expected on July 29th. Hourly rates will depend on your latitude. Those viewing from the southern tropics will see the best rates of near 1-2 per hour. Rates seen from mid-northern latitudes will range from 0-1 per hour, depending on the haziness of your skies. The radiant rises near 2200 (10pm) LDT for observers located in the mid northern latitudes, but is best placed near 0300 LDT, when it lies highest in the sky. With an entry velocity of 42 km/sec., most activity from\nthis radiant would be of average velocities.\nThe Piscids Austrinids (PAU) are a minor shower not well seen from the northern hemisphere. This radiant is active from July 15 through August 10. Maximum activity occurs on July 28 when the zenith hourly rate (ZHR) may reach five. These rates are only seen from the southern hemisphere where the radiant passes overhead. From mid-northern latitudes, rates of one per hour at maximum are usually seen. The radiant is currently located at 22:23 (336) -32. This position lies in central Piscis Austrinus, eight degrees southwest of the bright first magnitude star Fomalhaut (Alpha Piscis Austrinus). The radiant is best placed near 0300 LDT, when it lies highest in the sky. With an entry velocity of 35km/sec., most\nactivity from this radiant would be of average velocities.\nThe July Pegasids (JPE) are active with low rates during most of July with maximum activity occurring on the 10th. The radiant is currently located at 23:52 (358) +14. This area of the sky lies in southeastern Pegasus, four degrees west of the third magnitude star Algenib (Gamma Pegasi). This radiant is best placed during the last dark hour before dawn, when it lies highest above the horizon in a dark sky. Rates at this time should be less than one per hour no matter your location. With an entry velocity of 68 km/sec., the average July Pegasid meteor would be of swift speed.\nThe Perseids (PER) are active from a radiant located at 01:04 (016) +52. This position lies in southern Cassiopeia, six degrees southeast of the second magnitude star Shedar (Alpha Cassiopeiae). The radiant is best placed during the last hour before the start of morning twilight when it lies highest in a dark sky. Since the maximum is not until August 12th, current rates would be only two to three per hour at best, as seen from the northern hemisphere. Activity from this source is not visible south of 40 degrees south latitude. With an entry velocity of 61 km/sec., most activity from this radiant would be swift.\nAnother new source found by the IMO video cameras to be active this time of year is the Phi Piscids (PPS). This radiant has been found to be active from June 14 through July 30 with maximum activity occurring on July 1st. Current rates would most likely be less than one per hour no matter your location.Â The radiant is currently located at 01:56 (029) +36, which is situated on the Andromeda/Triangulum border, three degrees northwest of the third magnitude star Beta Triangulum. The radiant rises near midnight LDT but does not reach a sufficient altitude above the horizon until three hours later. Activity would best seen during the last dark hour of the morning when the radiant is located highest in a dark sky. With an entry velocity of 71 km/sec., the average Pi Piscid meteor would be swift.\nAs seen from the mid-northern hemisphere (45N) one would expect to see approximately ten sporadic meteors per hour during the last hour before dawn as seen from rural observing sites. Evening rates would be near three per hour. As seen from the mid-southern hemisphere (45S), morning rates would be near eight per hour as seen from rural observing sites and three per hour during the evening hours. Locations between these two extremes would see activity between the listed figures.\nThe table below presents a condensed version of the expected activity this week. Rates and positions are exact for Saturday night/Sunday morning .\n|SHOWER||DATE OF MAXIMUM ACTIVITY||CELESTIAL POSITION||ENTRY VELOCITY||CULMINATION||HOURLY RATE||CLASS|\n|RA (RA in Deg.) DEC||Km/Sec||Local Daylight Time||North-South|\n|July Zeta Draconids (ZED)||July 19||17:23 (263) +61||23||22:00||<1 – <1||II|\n|Alpha Capricornids (CAP)||July 29||20:04 (301) -12||25||01:00||1 – 1||II|\n|Antihelions (ANT)||–||20:48 (312) -17||30||02:00||1 – 1||II|\n|Delta Aquariids (SDA)||July 29||22:18 (334) -18||42||03:00||1 – 2||I|\n|Piscids Austrinids (PAU)||July 28||22:23 (336) -32||35||03:00||<1 – <1||II|\n|July Pegasids (JPE)||July 10||23:52 (358) +14||68||05:00||<1 – <1||IV|\n|Perseids (PER)||Aug 12||01:04 (016) +52||61||06:00||3 – 1||I|\n|Phi Piscids (PPS)||July 01||01:56 (029) +36||71||07:00||<1 – <1||IV|","The spring is not usually the best time for meteors – shooting stars – but a welcome exception is the Lyrid meteor shower. These appear around the third week of April, with their peak on 22 April, and this will be a good year to view them, given clear skies, and as long as you’re prepared to wait until the early hours of the morning!\nThe Lyrids aren’t as prolific as some other showers, but given good conditions you could see one every few minutes. They have a theoretical hourly rate of 15–20, and that’s the number you’ll probably see quoted in the media, but in practice numbers will always be lower. That figure assumes perfect skies so that you can see even the faintest of meteors, and with the source of meteors – known as the radiant – overhead.\nThe best time to see the Lyrids will be after midnight. Don’t just peek out of the kitchen door for a few minutes after the News at Ten and give up because you haven’t seen any!\nWhen to look\nThere is a broad peak in numbers, which this year will probably be around 6 pm BST, which of course is in daylight as seen from the UK. And the radiant is also low in the evening, and much higher in the early morning, so the early morning of Saturday–Sunday April 21–22 will probably be best. But the early morning of Sunday–Monday April 22–23 could also be good.\nThis year the Moon is at first quarter, so it will brighten the sky somewhat and drown out the fainter meteors. But it gets quite low down in the early morning, just as the radiant gets higher in the sky, and sets around around 3 am.\nSo the best time to see the Lyrids will be after midnight. Don’t just peek out of the kitchen door for a few minutes after the News at Ten and give up because you haven’t seen any! The best plan is to settle down in a garden lounger that allows you to see as much of the sky as possible – and wrap up warm, as it can get quite chilly at 3 am!\nWhere to look\nNow, this radiant business. It is actually very close to the very bright star Vega, which is low down in the north-east in the evening and high in the east in the early morning. Don’t mix up Vega and Jupiter, even brighter but down in the south-east to south. Vega twinkles but Jupiter doesn’t. And there’s another equally bright star, Arcturus, well above Jupiter but with a yellowish tinge to it, whereas Vega is pure white.\nVega is in the constellation of Lyra, the Lyre, which is why these meteors are called Lyrids – they will appear to radiate away from this area. If you trace back the path of a shooting star and it seems to have come from close to Vega, it’s probably a Lyrid. There will be random meteors appearing as well, as happens all the time.\nBut the advice is that if you want to watch for Lyrids, don’t gaze directly at Vega, but look about 45º to its left or right – that’s two or three widths of your outstretched hand at arm’s length – and in mid sky, about 50º above the horizon. This is where the paths of the meteors are likely to be most noticeable.\nAnd while observing, you can use the illustration at top to identify some of the other sights of the sky, including Saturn, Mars (which rises to the left of Saturn at about 3 am) and, if your site is dark enough, the glorious summer Milky Way.\nMeteors are over so quickly that there’s no hope of snapping one as it appears. The only way to do it is to keep the camera shutter open for a long time in the hopes that one will cross the field of view. This is only really possible if you can control your camera manually for exposure time, aperture and focus, which usually means using a more advanced camera than a compact or phone camera.\nIn brief, fix the camera on a tripod pointing at a likely part of the sky and keep the shutter open for as long as possible for each exposure – 20 or 30 seconds if possible. The camera should be focused on infinity and the aperture fully open, on a wide-angle lens setting. Set the ISO rating as high as you can before the photos become too bright from light pollution or moonlight – ideally around ISO 800 or more. If the results are too bright at ISO 800 you’ll have to give shorter exposures. There’s plenty more information in our guide to photographing meteors.\nGood luck – photos of Lyrid meteors are quite rare!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2d9a3e73-2266-48eb-9508-9e6b8513ee64>","<urn:uuid:d5309546-758b-4aaf-bbe4-f56f33d351ed>"],"error":null}
{"question":"What is the typical timeframe during pregnancy when spina bifida develops, and what preventive measure can reduce its risk?","answer":"Spina bifida develops just a few weeks after conception, specifically between 21 to 28 days, usually before a woman knows she is pregnant. Taking folic acid can reduce the risk of having a baby with spina bifida. It's available in most multivitamins and should be taken daily by women who could become pregnant.","context":["Spina Bifida information from trusted sources:\nSpina bifida (cleft spine) is a birth defect affecting the spinal column. Spina bifida progresses from a cleft, or splitlike opening, in the back part of the backbones (the spinal vertebrae). In more severe cases, it involves the spinal cord. Spina bifida is the most common of a group of birth defects known as neural tube defects, which affect the central nervous system (brain and spinal cord). Spina bifida begins in the womb, when the tissues that fold to form the neural tube do not close or do not stay closed completely. This causes an opening in the vertebrae, which surround and protect the spinal cord. This occurs just a few weeks (21 to 28 days) after conceptionusually before the woman knows that she is pregnant. There are 3 types of spina bifida. Spina bifida occulta: Occulta means hidden, and the defect is not visible. Spina bifida occulta is rarely linked with complications or symptoms. Spina bifida occulta is usually discovered accidentally when the person has an x-ray or MRI for some other reason. The prevalence of occulta is not known, but it is probably the most common type of spina bifida. Estimates of prevalence from 5% to as high as 40% have been proposed. Meningocele: The membrane that surrounds the spinal cord may enlarge, creating a lump or cyst. This is often invisible through the skin and causes no problems. If the spinal canal is cleft, or bifid, the cyst may expand and come to the surface. In such cases, since the cyst does not enclose the spinal cord, the cord is not exposed. The cyst varies in size, but it can almost always be removed surgically if necessary, leaving no permanent disability. This is an uncommon type of spina bifida. Spina bifida cystica (myelomeningocele): This is the most complex and severe form of spina bifida. Spina bifida cystica usually involves neurological problems that can be very serious or even fatal. A section of the spinal cord and the nerves that stem from the cord are exposed and visible on the outside of the body. Or, if there is a cyst, it encloses part of the cord and the nerves. This condition, which was documented 4000 years ago, accounts for 94% of cases of true spina bifida. The term spina bifida often is used interchangeably with myelomeningocele, since this is the type of spina bifida that causes the vast majority of disability. Fortunately, surgery is an effective treatment in most people with spina bifida. Most infants with an open spine or myelomeningocele undergo surgery within the first 48 hours of life to close the defect. Antibiotics are given to prevent infection of the exposed spinal cord and nerves until these structures can be protected by surgery. Before antibiotics were available, most children born with myelomeningocele died soon after birth. Those who survived were severely disabled. With modern treatment, almost all children with myelomeningocele survive and most are able to live productive lives with some degree of independence. Even with these treatments, however, most have some degree of permanent leg paralysis and often difficulties with bowel and bladder function. The extent of paralysis depends on which part of the spinal cord is involved. The higher the defect on the body, the more severe the paralysis. About 80% of myelomeningoceles occur in the lumbar (lower back) and sacral (tailbone) regions of the spine.\nSpina bifida is the most common disabling birth defect in the United States. It is a type of neural tube defect, which is a problem with the spinal cord or its coverings. It happens if the fetal spinal column doesn't close completely during the first month of pregnancy. There is usually nerve damage that causes at least some paralysis of the legs. Many people with spina bifida will need assistive devices such as braces, crutches or wheelchairs. They may have learning difficulties, urinary and bowel problems or hydrocephalus, a buildup of fluid in the brain. There is no cure. Treatments focus on the complications, and can include surgery, medicine and physiotherapy. Taking folic acid can reduce the risk of having a baby with spina bifida. It's in most multivitamins. Women who could become pregnant should take it daily.\nSpina bifida is part of a group of birth defects called neural tube defects. The neural tube is the embryonic structure that eventually develops into the baby's brain and spinal cord and the tissues that enclose them."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:177bbf5f-5cc2-4667-b369-a4c116acdcbf>"],"error":null}
{"question":"What is Tifinagh script's historical significance for preservation, and how does its durability compare to modern museum preservation challenges for artifacts?","answer":"Tifinagh script demonstrates remarkable durability, having survived for over 1,500 years through political threats, cultural changes, and harsh Saharan conditions. It was designed not only to communicate but to endure, as evidenced by cave inscriptions that served as caravan instructions. In contrast, modern museums face significant challenges preserving artifacts, particularly due to humidity fluctuations. Objects like paintings, sculptures, and photographs require precise relative humidity levels between 50-55% (±5%) and controlled temperatures of 20°C to prevent damage. Without proper conditions, materials can crack, chip, or deteriorate, highlighting how remarkable Tifinagh's natural preservation has been across millennia.","context":["The site is called the Wadi Matkhandouch Prehistoric Art Gallery, near Germa in Libya. It’s startling to find any evidence of human presence in such an inhospitable place, so far from what we think of as civilization. And, frankly, this writing, in a script called Tifinagh, doesn’t look much like what we think of as writing. It’s a meandering string of simple symbols, some of which more like mathematics than writing: Is that a plus sign? A zero? A percentage, sign, for heaven’s sake?\nOr perhaps code. There’s no attempt to include little explanatory pictograms, though in fact the same set of rocks and caves has an incredible array of carvings of animals: giraffes, lions, crocodiles, elephants, ostriches, two cats apparently fighting. No, this writing seems to be just as likely to be about concealing its message as about revealing it.\nOr perhaps another kind of code, for this twisting strand of language looks so old and so deep it might just be the DNA of writing. Oh, and did I mention that the symbols or letters are in such a strange and vivid red pigment that they look as if they’ve been written in blood?\nIt’s like a missing link, the verbal equivalent of the famous prehistoric cave paintings at Lascaux in southwestern France. Written language was here, it says (and in other Saharan sites, too, such as the one between Tamanrasset, chief city of the Algerian Tuareg, an oasis high in the Ahaggar Mountains, and Djanet, deep in the desert), long before anyone thought to write in straight and level lines. The individual letters have the same combination of angular purpose yet prehistoric crudity that challenge the sense at Stonehenge. Something is being born. It is a defining moment in human intellectual history: not just representation, a panorama of hunting, but early, early, unbelievably early symbolism. It’s as if we’re looking at the invention of meaning itself.\nAmong these are the 1,500 year old monumental tomb of the Tuareg queen Tin Hinan, where vestiges of a Tifinagh inscription have been found on one of its walls.\nTifinagh is an illustration of the ability of a written language to survive utterly everything: political and military threat, social and cultural change, wind, weather, the biting sands of the Sahara.\nThis, surely, is one of the quintessential qualities of written language: it was designed to communicate, yes, but also to survive. These cave inscriptions are apparently instructions left by one Berber caravan for anyone else crossing the wastes — notes on where the caravan was heading, where other caravans are, where to find water. Those caravans are long gone, but the embodiment of ideas in physical form can survive for thousands of years.\nTraditionally nomadic, the Berbers or Amazighs once moved throughout North Africa from the Atlantic to the Red Sea, a region called Tamazgha. One of the languages of the region is still called Tamazight or Tamasheq. The script used to write it is Tifinagh. (Technically neo-Tifinagh, though the Amazigh people don’t use that term.)\nNowadays, after centuries of incursions and colonization, the Berber are scattered throughout Algeria, Libya, Niger, Mali and Burkina Faso. As the other occupants of the region created permanent cities, kingdoms and countries, the Berber people found themselves no longer kings and queens of the desert but outcasts. They have been subject to a depressing range of human rights abuses, not least in countries which have declared Arabic the official language and have endeavored to suppress or stamp out Berber language and culture.\nIn Morocco, as recently as the 1980s and 1990s anyone using the Tifinagh script might be arrested and imprisoned. In Algeria, one of the points of contention during the political unrest of 2001 (during which roughly 100 Berber people were killed, 5,000 wounded, and thousands of other were subject to torture and arbitrary detention) was the suppression of Berber languages. And in Libya, the Gaddafi regime banned Tifinagh from use in public displays.\nTifinagh may be a descendant of one of the oldest scripts in the world. Its name may mean “Phoenician letters,” and Phoenician was the alphabet from which Ancient Greek was developed. And, like Mandaean and Samaritan, it may have survived in something like its original form because it was an outsider language.\n“Tifinagh has resisted the influence of foreign systems like the Roman and Arabic alphabets,” writes Saki Mafundikwa in Afrikan Alphabets. “It is used in coded messages in games, or directions inscribed on rocks as a guide to finding water or game in the desert.” It is also used, especially by Berber women, to pass on secret messages and write love letters.\nAs an outsiders’ script, Mafundikwa suggests, endangerment becomes an advantage, even a necessity. “It is written without spaces between the words. It has a geometric style that makes it convenient for inscriptions on rock or tracing on the palm of a conspiring hand.”\nGeneral Script, Language, and Culture Resources\n- Tifinagh AncientScripts\n- Wikipedia (Berber Languages)\n- Tifinagh: Learn Tamazight Language\n- The Poetics of Tifinagh\n- Learn World Alphabets App\n- Rebirth of Berber Culture Article\n- A dissertation on Tifinagh typeface design\n- Tifinagh carving in a sandstone boulder\n- Tifinagh inscriptions at the Acacus Mountains in Libya\n- Berber Writing: Libyan and Tifinagh","Museums around the world include a broad range of objects with widely differing ages: dinosaur bones, stone-age flutes made of mammoth tusks, sensitive photographs, paintings with thick layers of paint, contemporary sculptures. In all cases, the building has to maintain and protect the objects displayed and stored inside.\nExcessively dry air\nDry air absorbs humidity from objects, their weight is reduced and they contract. In the case of humid air, it is the other way round. Climatic fluctuations thus keep the objects in permanent movement and sooner or later a crack appears on the canvas or the color gilding chips off the baroque sculpture. Stabilization of the relative air humidity helps avoid tension in the material texture of the exhibits, the Building Climate Institute emphasizes.\nThe preservation of enshrined cultural artifacts generally requires a constant indoor climate which is defined within relatively tight limits . This climate has to be technically created. The air requires humidification — at least periodically. The values reached are measured using measuring systems. Nowadays, due mainly to lease agreements, international indoor climate values of between 50 and 55 ±5% RH and 20°C are required . The American ASHRAE standard formulates corridors for the indoor climate in even greater detail — from the narrowest AA to D. The narrowest climate corridor specifies RH = ±5% and T = ±5 K as long-term tolerance with seasonal adjustment. Positive from a conservational viewpoint is that there is a slow, seasonal adjustment of the indoor climate to the outdoor climate which lies within these limits.\nThe external climate and the relative air humidity show significant seasonal fluctuations. In winter, the RH is sometimes extremely low. In summer during rainfall, 100%. The external space and inner areas are more or less closely related at all times. This means that a change of the external climate is also noticeable indoors and can be even more pronounced there. Especially short-lived fluctuations of the indoor climate are harmful in the long term. Therefore, a change of the RH during one day may not be allowed to exceed 5%. During one hour, the fluctuations have to be below 2.5%. Basically, a change should be as minor as possible, while the frequency of fluctuations should be kept as low as possible . For particularly sensitive exhibits, there are special display cases. They may be damp-proof only, equipped with humidity regulation, or even fully air conditioned.\nSensitive wood products\nEach material has specific demands on its ambient climate. Metal, stone, canvas, oil, wood, leather, paper or ivory react differently to humidity and temperature fluctuations. Works on paper, wood, canvas or parchment are among the most sensitive objects. The main raw materials of our papers are plant fibers, textile fibers and wood pulp. These are strongly hygroscopic materials. By absorbing indoor humidity and releasing material humidity, they follow all humidity fluctuations in the environment. These exchange processes require the expansion or contraction of the material through a change of dimensions of the wood cells.\nThis is expressed by warping of parchment or paper, or by tears or bubbles on panel and canvas paintings or on color-gilded sculptures. On papers, humidity fluctuations lead to a displacement of soluble components such as the ink. Specialist terms here are ink corrosion and copper corrosion.\nFabrics, photographs, metal and stone\nIn textile objects, excessively low air humidity advances the fragility of the tissue. In photographic objects, substrates and binding agents become fragile and brittle in environments with low RH. Comfortable in a climate of between 20 and 60% RH, stone and ceramic can tolerate a low air humidity.\n- Anderson Art Gallery\n- Canadian Museum of HIstory\n- Guggenheim Museum\n- Harvard FOGG Art Museum\n- Milwaukee Public Museum\n- Royal Ontario Museum\n- San Francisco Museum of Modern Art\n- Smithsonian Museum"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4bb0a5bb-f745-4584-a270-3e890a78e734>","<urn:uuid:1ae26b49-f1f7-4bd6-aa60-84ecab652c5e>"],"error":null}
{"question":"How do the Freedom of Information Act (FOIA) and HIPAA's Privacy Rule differ in their approaches to making records accessible to the public versus protecting individual privacy?","answer":"The Freedom of Information Act promotes public access to federal government records and requires federal agencies to provide these records electronically. In contrast, HIPAA's Privacy Rule specifically restricts access to individuals' health information, requiring covered entities (like healthcare providers and insurance companies) to protect patient privacy while still allowing necessary flow of health information for quality healthcare. HIPAA mandates that covered entities must obtain proper patient authorization before sharing protected health information with outside parties, and must provide patients with a Notice of Privacy Practices explaining their rights.","context":["Public Records Law: Know Your Rights and Limitations\nWhen searching for public records and the information contained in them, you may not be aware of the several laws existing at both the state and federal levels which will affect your efforts. There are basically two types of laws and statutes that concern public records; these include both “right to access” or “right to know” laws as well as “right to privacy” laws. Following is a discussion of both.\nRight to Know Laws Affecting Public Records\nThere are several federal laws which directly affect your ability to access public records. These include the Freedom of Information Act, open meeting laws, and even the First Amendment of the Constitution.\nThe Freedom of Information Act, also known as FOIA, was signed into law in 1966 and provides for public access to records maintained by the federal government. Previously, these records were unavailable to the public. Furthermore, in 1996 an amendment was passed which requires federal agencies to provide these records to the public electronically. Each state generally has its own version of the FOIA implemented to ensure access to local public records.\nIn an effort to promote transparency in government operations, open meeting laws or “sunshine” laws allow the public to attend government agency meetings where official business is conducted. These laws vary from state to state, and may even be known by a different name. For example, California’s open meeting law is the Brown Act.\nThe First Amendment to the United States Constitution states that the U.S. Congress may not create any statute which directly impedes the freedom of the press. In 1938, the press was defined as “every sort of publication which affords a vehicle of information and opinion” in Lovell v. City of Griffin. However, this fundamental right is always subject to privacy laws.\nRight to Privacy Laws Affecting Public Records\nThere are several different privacy laws which affect the public’s ability to access specific types of records or information found in them. These include the federal Privacy Act, the Driver’s Privacy Protection Act, HIPAA, FERPA and FCRA.\nThe federal Privacy Act of 1974 outlines the requirements and restrictions for concerned agencies which gather, maintain and provide information that concerns an individual. The Privacy Act also aims to protect personal identifying information such as Social Security numbers while allowing the public to request copies or amendments to federal records. Similarly, the Driver’s Privacy Protection Act of 1994 protects sensitive information about an individual stored by the Department of Motor Vehicles.\nHIPAA, or the Health Insurance Privacy and Portability Act, was passed in 1996 by Congress. This act protects sensitive information contained in an individual’s medical records, and requires express written consent from a patient before they may be disclosed to an outside party.\nThe Federal Educational Rights and Privacy Act, or FERPA, was passed into law in 1974. This law protects the information contained in a student’s records such as institutions attended, grades and other personal details. Students over the age of 18 must provide written consent for anyone else to access them. Parents or guardians of minor students are the only parties able to access these records without consent.\nThe Fair Credit Reporting Act regulates information concerning an individual’s credit. This law also determines who can access the information, and requires employers to obtain an appropriate release form from potential employees prior to conducting a background check.\nFinally, regardless of whether you are concerned with your rights of privacy or access concerning public records, become familiar with your local state laws. Each state has its own laws and regulations concerning the public records maintained by state agencies such as property and vital records, and these will directly affect your efforts to obtain them.","Healthcare facilities gather and manage volumes of critical patient information that, if lost or stolen, could result in patient identity theft and delayed care. In 1996, the Health Insurance Portability and Accountability Act, or HIPAA, prompted lawmakers to build a set of privacy laws governing the management and security of patient information.\nUsing this HIPAA security rule checklist, you can see how these standards apply to your organization and take steps to obtain compliance.\nWhat is the HIPAA Privacy Rule?\nThe Department of Health and Human Services issued a set of orders that standardized privacy law for all individuals and organizations that would manage patient health data. These accountable organizations are known as covered entities and are liable for all mandates expressed in the Standards for Privacy of Individually Identifiable Health Information, also known as the HIPAA Privacy Rule.\n“A major goal of the Privacy Rule is to assure that individuals’ health information is\nproperly protected while allowing the flow of health information needed to provide\nand promote high quality health care and to protect the public’s health and well being.” – United States Department of Health and Human Services\nThese privacy standards arrived as medical professionals started to digitize medical records. Taking advantage of digital documentation allows all healthcare-related organizations to better serve patients, since managing digital records is far more efficient than managing hard copies of medical records.\nTo Whom Does the HIPAA Privacy Rule Apply?\nThe HIPAA Privacy Rule applies to what are referred to as covered entities. These agencies assist in the administration of healthcare services, to include treatments, insurance payments, and more.\nWhat are Covered Entities?\nA covered entity includes private medical practices, hospitals, and any auxiliary organization that must access protected health information to operate. Often, there are several healthcare-related agencies working together to assist a patient in receiving the medical care that they require.\nThe Privacy Rule identifies a covered entity as one of the following:\n- Health Plans\n- Insurance providers\n- Medicare/Medicaid insurers or supplemental insurers\n- Employer-sponsored plans\n- Government-sponsored plans\n- Church-sponsored plans\n- Coop plans\n- Healthcare Providers\n- Healthcare Clearinghouses\nThe Privacy Rule also applies to non-covered entities that serve as third-party vendors or business associates to a covered entity.\nWhat is Protected Health Information (PHI)?\nProtected Health Information, or PHI, is the formal term for “individually identifiable health information.” Covered entities manage PHI in accordance with their duties and are under scrutiny to protect patient identities and privacy by abiding by all HIPAA compliance standards pertaining to lawful use of PHI.\nDownload Our HIPAA Compliance Checklist\nWhat are HIPAA Authorizations?\nIn the event that a covered entity needs to share PHI with another individual or agency, but that individual or agency is not otherwise permitted access to a patient’s PHI under HIPAA Privacy law, covered entities may seek a patient authorization.\nHIPAA authorizations must be signed by the patient and lay out clearly who the authorization is for, the purpose of the authorization, and when the authorization expires. The covered entity should also define any contingencies or parameters laid out by the patient to meet the authorization’s purpose.\nAn example of a HIPAA authorization could be a mental health patient that agrees to share his/her therapy notes in a full psychological evaluation. This is a common scenario for veterans providing medical evidence during a PTSD disability claim. Even though filing a disability claim involves due process and legal discovery, investigators may not access those medical records without a signed patient authorization.\nHIPAA Protected Health Information Uses and Disclosures\nWhat is a Notice of Privacy Practices (NPP)?\nAll covered entities must disclose a notice of privacy practices, or NPP, that outlines the patient’s rights according to HIPAA privacy law and PHI. The NPP should also explain how a patient may file a complaint against a covered entity that they feel violated their rights under HIPAA privacy law.\nNPPs are items commonly found in registration paperwork when a patient sees a medical professional for the first time. The documentation explains how a covered entity may use the patient’s PHI within the bounds of HIPAA compliance.\nYour HIPAA Security Checklist:\nA HIPAA security checklist can help you identify where your business operations fail to meet HIPAA privacy requirements. You can use the checklist below to perform an internal audit. Or you can use the checklist as a way to gauge how seriously your organization takes HIPAA compliance.\nPatient Access and Consent\n- Have you established a process to help patients access their PHI? In this day and age, many covered entities make sure that patients can access their PHI safely online, even if another covered entity maintains the PHI database. Regardless, your organization should have clear policies and procedures to help patients view their PHI.\n- Do you have a process for accepting and fulfilling PHI copy requests from patients? When a patient requests copies of their PHI, HIPAA compliance dictates that you give the patient a copy in the requested format (hard copy or digital) within 30 days of their request.\n- If your firm decides to charge patients a fee for copies of their PHI, do you make those fees accessible? HIPAA compliance requires covered entities to fulfill PHI copy requests to patients at a reasonable cost. Prohibitive costs do not properly reflect the amount of labor and expenses required to fulfill PHI requests. Agencies that charge too much may be doing so intentionally to keep from having to be HIPAA compliant.\n- Are your authorizations specific, to include uses, recipients, disclosures, and expiration dates? Vague HIPAA authorizations do not protect your organization or the patient. All critical details of the authorization should be clearly spelled out according to the patient’s expectations.\n- Do your authorizations use “plain English,” as opposed to medical jargon and elusive clinical terms that the patient will not understand? If it appears that the patient had no clue of what they were signing because of convoluted words and phrases, your authorization could be in violation of HIPAA privacy law. It’s critical that your authorizations use language that is understandable to the average patient.\n- Do you secure the patient’s signature and date on every authorization? HIPAA authorizations are invalid unless they have the patient’s signature, as well as the date on which it was signed.\n- Do you store your HIPAA authorizations in a secure location and properly dispose of them once they are no longer needed? Losing a HIPAA authorization could open your organization up to legal action from the patient. It’s critical that you properly store and share authorizations in accordance with HIPAA privacy law.\nNotice of Privacy Practices (NPP)\n- Do you have an NPP included in your new patient paperwork? To be HIPAA compliant, you should onboard every new patient or client with an NPP so that those individuals understand their PHI rights from the start of your services.\n- Do you have your patients or clients confirm that you informed them of their rights according to HIPAA privacy law? Having your patients or clients confirm in writing and with a signature that they have read and understood your NPP protects you as a covered entity.\n- Do you prominently display your NPP on the premises and/or clearly on your website? Demonstrating that you publicize your NPP for all to see further protects you against patients claiming that you did not inform them of their rights under HIPAA privacy law.\n- Do you have policies and procedures in place to manage patients with concerns that you’re not complying with your NPP? It is possible that some patients may accuse you of not advising them of their rights per HIPAA privacy law. More importantly, a patient or client may fall through the cracks. Either way, you should have a clear process on how to manage those complaints and rectify them immediately.\n- Do your day-to-day operations align with your NPP and HIPAA Privacy Law? You should perform routine audits of your business operations to ensure that you’re not merely paying lip service to HIPAA privacy law.\nEmployees and Business Associates\n- Do all of your staff members understand HIPAA privacy law, as well as workplace policies and procedures relevant to PHI? Much of your HIPAA compliance pertains to consistent adherence by your staff. As a covered entity, it is your responsibility to ensure that every employee understands HIPAA privacy law and how they must manage PHI in their current role.\n- Have you trained your employees and collected proof (such as signed documentation) that they received the proper HIPAA compliance training? Similar to how you have patients sign and confirm that they had read and understood your NPP, you should include attestation documentation at the conclusion of HIPAA compliance training for your staff.\n- Do you have a process in place for employees to report HIPAA non-compliance without fear of reprisal? Ideally, you should create a way for employees to report non-compliance anonymously. This approach ensures that managers and lower-level employees alike are held accountable in accordance with HIPAA privacy law.\n- Do you collect confidentiality agreements from your employees and independent contractors? Employees and independent contractors of covered entities are known as non-business associates. Since it is likely that these people will come into contact with or manage PHI as part of their job description, it’s important that you collect confidentiality agreements from each of them.\n- Do you choose your business associates carefully, to include carrying out due diligence on that organization’s privacy policies and procedures? You could be held liable if one of your vendors mismanages PHI that your business maintains. Part of being HIPAA-compliant is ensuring that you only work with vendors that also understand HIPAA privacy law.\n- Do you maintain a list of all your business associates and third-party vendors? If your organization manages PHI, it’s very likely that most or all of your business associates and third-party vendors may come into contact with that PHI. It’s critical that you maintain an up-to-date record of all external parties with whom you do business.\n- Have you established the proper contracts (business associate agreements) with your business associates and third-party vendors that contain HIPAA-compliant directives on all matters pertaining to PHI? You should disclose to your business associates that managing PHI is part of your business operations. This informs your vendors that they must maintain HIPAA compliance, especially if their services also involve the use of PHI.\n- Do you reexamine your business associate agreements every year, to include updating your list of business associates? The nature of your relationship with third-party vendors and business associates can change year-over-year as you and the other party scale your respective operations. As such, you may need to update portions of your business associate agreements to remain HIPAA-compliant.\n- Do you have an up-to-date network diagram? Network diagrams show you all possible attack vectors from which hackers and malware might enter and try to steal or destroy PHI.\n- Do you have basic cybersecurity protocols in place? Due to the sensitivity of PHI, it is critical that you maintain all the necessary firewalls, malware protection, and monitoring to keep your and your patient’s information secure.\n- Do you have a plan to respond to an incident or breach? The initial moments after a security breach are often the most critical. Having a plan in place to quarantine the incident, diagnose the root cause, patch the intrusion, and report any damage will protect PHI under your organization’s care. Also, it will help your cybersecurity team update its tools, policies, and procedures to deal with similar intrusions more efficiently.\n- Has your staff received training on phishing attacks and how to prevent them? Sometimes the biggest threat to your organization is an employee clicking on an unknown link and releasing malware onto the company network. Making sure that your employees know how to safely deal with phishing attacks can drastically reduce your cyber risk.\nKey Takeaways: HIPAA Security Rule Compliance Checklist\nUsing the checklist above, you can take initial steps to become HIPAA-compliant in accordance with privacy laws pertaining to PHI. Failing to comply with HIPAA Privacy Law can result in financial penalties and patient lawsuits.\nRSI Security is an agency dedicated to assisting covered entities in their quest to acquire and maintain HIPAA security compliance. Our team of cybersecurity specialists can help you create a personalized HIPAA security rule compliance checklist and establish the necessary safeguards to protect your PHI against negligence or abuse.\nDownload Our Complete Guide to Navigating Healthcare Compliance Whitepaper\nNot sure if your HIPAA or healthcare compliance efforts are up to snuff? Unsure about where to even start? Download RSI Security’s comprehensive guide to navigating the HIPAA and healthcare compliance labyrinth. Upon filling out this brief form you will receive the whitepaper via email."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f8f3812d-e597-4bcc-a098-a8e374ea8fe0>","<urn:uuid:e152642d-e4fa-4a0b-9ec7-584621db2de8>"],"error":null}
{"question":"What's the key difference between Diffusion and Reflection in acoustic settings?","answer":"Diffusion is the scattering or random reflection of sound waves that creates a sensation of sound coming from all directions at equal levels, while Reflection is simply the amount of sound wave energy that bounces off a surface. Hard non-porous surfaces reflect more sound than soft-porous surfaces. Diffusion specifically involves complex surfaces that break up the sound to create a more complex impulse response, as seen in concert halls and recording studios with bumpy, geometrically complex surfaces.","context":["Easy-to-follow video tutorials help you learn software, creative, and business skills.Become a member\nWhile reverb Time, Pre-Delay, Wet/Dry Mix, and the spectral manipulation of Reverb Time are common, nearly universal parameters, you'll see other terms that are less well defined and aren't particularly consistent from one make and model of reverb to another. Terms like Diffusion, Density, Room Size, and similar words sometimes appear as adjustable parameters on our reverbs. Remember, all of the parameters are there to give us some ways recording engineers to interact with qualities of the reverb, but reverb is itself such a complex and musical sound, and we savored it in part by listening to fine details and attributes not captured by the core reverb parameters.\nThe number of individual contributing reflections within the impulse response is sometimes an audible trait. Surfaces, like the wall behind me, are common design features in concert halls and recording studios. These highly articulated, geometrically complex surfaces come in many forms, all with the goal of breaking up the sound. In architecture, bumpy surfaces like this diffuse sound creating a more complex impulse response. So parameters like Diffusion and Density drive the digital algorithm to emulate this feature of the architecture.\nThey don't change the Reverb Time or pre-delay, they just drive the build up of complexity within the reverb sound. I hear the effect of these parameters is changing the sonic texture of the reverb from open and gauzy to thick and smooth. While Reverb Time and pre-delay are easily defined and quantified, parameters like Diffusion and Density aren't. One plug-in might have a slightly different interpretation than another. These are simply adjustable parameters that go from some low value to some high value.\nThe numbers have no universal meaning across our industry. The approach is simply to fine tune and listen. A Room Size parameter likely tries to approximate the qualities associated with the big room versus a small one. Reverb time itself is of course intimately tied to room size, so adjusting the Room Size parameter often changes your Reverb Time setting as well, but it may also adjust the timing of the early reflections, close together in a small room and spread out in a large hall.\nDepending on the particular make and model of your reverb a room size parameter might also be available influencing how the density of the impulse response builds over time. Again, Room Size is a concept with no universal definition for how it changes a reverb processor's algorithm, we can only fine-tune and listen for these types of changes. Some reverbs provide an image of a room, sort of an architectural blueprint of the space and allow you to adjust qualities in the architecture.\nYou can change qualities of the space like the number of walls and the dimensions of the room. Maybe there are parameters for changing the acoustic reflectivity of the materials that make up the floor and the ceiling and the walls. This is all an interesting way to interact with a concept of artificial reverb, but it's important to realize that this is just an alternative user interface for driving subtle sonic qualities of your reverb processor. You're not hearing the actual sound of the variable room you've just envisioned.\nThe reverb output is influenced by the science of acoustics for sure, but it isn't actually calculating the precise reverberant qualities of the space you modified with these parameters. While Reverb Time, even as it varies with frequency and pre-delay, are crisp concepts that are straightforward to understand and relatively easy for the reverb unit to generate, these other parameters are more abstract. You don't need to worry about hearing the precise audible changes of a small tweak to a parameter labeled Density or Diffusion, instead we view them as ways to coax the sound in different directions.\nThe typical approach is to listen to it at its preset value, then crank it up to an extreme setting and listen for sonic differences. Then crank it down to a value near the other extreme and listen again. Then return to the middle and listen. If global trends are revealed, and you like what you're hearing then adjust it to taste. If you don't hear much change, don't sweat it, it may be too subtle to bother with given the spectral and temporal qualities of the tracks you're mixing today.\nIn those cases, I just leave those other parameters at their default values and move on.\nGet unlimited access to all courses for just $25/month.Become a member\n73 Video lessons · 17824 Viewers\n110 Video lessons · 10911 Viewers\n130 Video lessons · 19146 Viewers\n71 Video lessons · 14859 Viewers\nAccess exercise files from a button right under the course name.\nSearch within course videos and transcripts, and jump right to the results.\nRemove icons showing you already watched videos if you want to start over.\nMake the video wide, narrow, full-screen, or pop the player out of the page into its own window.\nClick on text in the transcript to jump to that spot in the video. As the video plays, the relevant spot in the transcript will be highlighted.","Acoustic treatmentUsed in architectural acoustics to isolate noise or vibration and to correct acoustical faults in spaces by addition of absorption devices, reflectors or other devices, sometimes including electronic systems.\nAcousticalThe properties of a material to absorb or reflect Sound (adjective) Acoustically, (Adverb).\nAcoustical AnalysisA review of a space to determine the level of reverberation or reflected sound in the space (in seconds) influenced by the building materials used to construct the space. Also the amount of acoustical absorption required to reduce reverberation and noise.\nAcoustical EnvironmentThe acoustical characteristics of a space or room influenced by the amount of acoustical absorption, or lack of it in the space.\nAcousticsThe science of Sound. Its production, transmission and effects. The branch of physics that treats the phenomena and laws of sounds as it effects people.\nAirborne SoundSound that reaches the point of interest by propagation through air.\nAirborne Sound Insulation Index : Ia''Former name for Weighted Apparent Sound Reduction Index : R'w\nAmbient noiseBackground or general noise level characteristic of a location, often used in comparison with a specific noise source. The metric most often used in the United Kingdom to describe this is the sound pressure level in dB(A) exceeded for 90% of the time, i.e L90, although L95, or even L99 are used as the measure of background in some regions.\nAnechoic chamberA room designed to suppress internal sound reflections. Used for acoustical measurements. Because there are so few reflections, any sound will come from one indirection only, it is used in microphone directivity measurements.\nANSIThe American National Standards Institute. They set USA standards, that in acoustics are usually VERY different to the International (IEC) standards and are often incompatible. The ANSI sound level meter standard is ANSI S1.4-1983 (R2006). ANSI standards can be bought on-line from http://webstore.ansi.org\nApparent Sound Reduction Index : R'Field measurements of Sound Reduction Index include Flanking and any other 'on-site' acoustic limitations.\nR' = D + 10 lg S/A (dB)\nD = Level Difference\nS = area of the test specimen (m2)\nA = Equivalent Sound Absorption area of the receiving room\nAttenuateTo reduce the level of an acoustical signal.\nBackground noiseThe sum total of all noise generated from all direct and reflected sound sources in a space that can represent an interface to good listening and speech intelligibility. (Hearing impaired persons are especially victimized by background noise).\nCoincidence effectMass Law provides a good working rule to predict the airborne Sound Insulation of a partition up to the region of the Critical Frequency and the coincidence effect. The coincidence effect occurs when the Wavelength of the sound in air is the same as the bending waves in the partition.\nAt a certain frequency and angle of incidence, the bending oscillation of the partition will be amplified and the Acoustic Energy will be transmitted through the partition almost without attenuation.\nCritical FrequencyThe lowest frequency at which the Coincidence Effect occurs in a partition is obtained when the incident sound waves graze the partition (parallel with it). This frequency is called the critical frequency in building acoustics.\nCycleIn acoustics, the cycle is the complete oscillation of pressure above and below the atmospheric static pressure.\nCycles per secondThe number of oscillations that occur in the time frame of one second. (See FREQUENCY.) Low frequency sounds have fewer and longer oscillations.\nDampingThe dissipation of vibratory energy in solid media and structures with time or distance. It is analogous to the absorption of sound in air.\nDecibel (dB)Sound level in decibels as a logarithmic ratio. Sound intensity described in decibels. i.e.:\nBreathing – 5 dB\nOffice Activity – 50 dB\nJet Aircraft During Takeoff at 300′ Distance – 130 dB\nDeflectionThe distance an elastic body or spring moves when subjected to a static or dynamic force. Typical units are inches or mm.\nDiffusionThe scattering or random reflection of a sound wave from a surface. The directions of reflected sound is changed so that listeners may have sensation of sound coming from all directions at equal levels.\nEarAn incredible hearing mechanism consisting of outer, middle and inner ear segments that cause sound pressures to be picked up by the ear that are transmitted through auditory nerves where signals are interpreted by brain as sound.\nEchoReflected sound producing a distinct repetition of the original sound. Echo in mountains is distinct by reason of distance of travel after original signal has ceased.\nEcho FlutterShort echoes in a small reverberative spaces that produce a clicking, ringing or hissing sound after the original sound signal has ceased. Flutter echoes may be present in long narrow spaces with parallel walls.\nFlanking transmissionFlanking is the transmission of sound from a source room to a receiving room by paths other than through the separating partition. For example, impact sound may be transmitted from one room to another through a common timber floor. Other common mechanisms for flanking transmission include suspended ceilings, pipework, ducting, etc.\nFlanking sound is always present, except in the 'ideal' acoustics laboratory. In practice the sound insulation is often limited by the flanking transmission.\nFrequencyThe number of oscillations or cycles per unit of time. Acoustical frequency is usually expressed in units of Hertz (Hz) where one Hz is equal to one cycle per second.\nHelmholtz resonator – A reactive, tuned, sound absorber. A bottle is such a resonator. Many good sound calibrators incorporate a Helmholtz resonator, to increase their equivalent volume. Named after Hermann von Helmholtz a German physicist.\nHearing range16-20000 Hz (Speech Intelligibility)\n600-4800 Hz (Speech Privacy)\n250-2500 Hz (Typical Small Table Radio)\nHertz (Hz)Frequency of sound expressed by cycles per second.\nImpact Isolation Class (IIC)The methods to measure the degree of impact noise isolation provided by a floor/ceiling assembly, in laboratory conditions, are described in the ASTM E 492 or ISO 140/6 standards. For field measurements refer to ASTM E 1007 or ISO 140/7. The impacts for these measurements are produced by the “Standard Tapping Machine”, an electrically operated mechanism consisting of five 0.5 kg hammers which fall regularly and freely onto floor surface from 40 mm height at a rate of 10 impacts/second. The sound pressure levels generated in the room directly below the floor/ceiling assembly undergoing testing are then measured, for each of the 16 third-octave-bands between 100 Hz and 3150 Hz, and they are normalized according to:\nAn absorption equal to 10 metric Sabins, or\nA reverberation time of 0.5 seconds (ISO 140/7)\nThe Normalized Impact Sound Pressure Levels (NISPL) are then plotted on a standard graph.\nThe IIC rating of the tested floor/ceiling assemblers determined by sliding the classification curve on the graph representing the normalized sound pressure levels, until the following conditions described in the ASTM E 989 (ISO 717/2) standards, are met:\nThe sum of the deviations above the normalizing curve should not exceed 32 dB.\nThe maximum deviation above the normalizing curve should not exceed 8 dB (see previous note on the classification of the isolation of airborne noise according to the ISO standard).\nWhen the IIC contour is positioned in such a way that these two requirements are satisfied the Impact Isolation Class (IIC) can be obtained by reading the normalized impact sound pressure level at the intersection of the IIC contour frequencies of 500 Hz and by subtracting this value from the number 110.\nImpact SoundThe sound produced by the collision of two solid objects. Typical sources are footsteps, dropped objects, etc., on an interior surface (wall, floor, or ceiling) of a building.\nImpact Sound InsulationIs expressed by a single value Ln,w or L' n,w\nISOThe International Organization for Standardisation. They are a similar organisation to IEC, but ISO set standards for measurements methods NOT for the instrument. They are available from www.iso.org/\nLevel difference : DAirborne sound insulation - field measurements. The difference in the space and time averaged Sound Pressure Levels.\nD = L1 - L2\nL1 = average Sound Pressure Level in the source room\nL2 = average Sound Pressure Level in the receiving room\nLn : Normalized Impact Sound Pressure Level : laboratory measurement.\nL'n : Normalized Impact Sound Pressure Level : field measurement.\nLnT : Standardized Impact Sound Pressure Level : laboratory measurement.\nL'nT : Standardized Impact Sound Pressure Level : field measurement.\nLn,w : Weighted Normalized Impact Sound Pressure Level : laboratory measurement.\nLnT,w : Weighted Standardized Impact Sound Pressure Level : based on laboratory measurement of LnT.\nL'nT,w : Weighted Standardized Impact Sound Pressure Level : based on field measurement of L'nT.\nMass LawA doubling in Mass or Frequency results in a 6 dB increase in the sound insulation of a single leaf partition over a defined frequency range.\nMass Law provides a good working rule to predict the airborne sound insulation of a partition up to the region of the Critical Frequency and the Coincidence Effect\nModeA room resonance. Axial modes are associated with pairs of parallel walls. Tangential modes involve four room surfaces and oblique modes all six surfaces. Their effect is greatest at low frequencies and for small rooms.\nNoise reduction coefficient (NRC)The NRC of an acoustical material is the arithmetic average to the nearest multiple of 0.05 of its absorption coefficients at 4 one third octave bands with center frequencies of 250, 500, 1000, 2000 Hertz.\nNormalized impact sound pressure level : L'nField measurement.\nThe Normalisation formulae for Ln directly above also applies for L'n.\nNormalized impact sound pressure level : LnLaboratory measurement.\nLn = Li + 10 lg A/A0 dB\nA = measured Equivalent Sound Absorption area in the receiving room\nA0 = Reference Absorption area.\nIn all cases where it is uncertain whether results are obtained without flanking transmission the normalized impact sound pressure level should be denoted by L'n.\nNormalized level difference : DnAirborne sound transmission. The sound insulation index measured under field conditions, between 'real' rooms and therefore includes effects due to Flanking, different room sizes and other on-site considerations.\nDn = D - 10 lg A/A0\nD = level difference in dB\nA = Equivalent Sound Absorption area of the receiving room in square metres\nA0 = Reference Absorption area in square metres (10 m²)\nR : Sound Reduction Index : laboratory measurement.\nR' : Apparent Sound Reduction Index : field measurement.\nRw : Weighted Sound Reduction Index : laboratory measurement.\nR'w : Weighted Apparent Sound Reduction Index : field measurement.\nCertified Sound Insulation Test Equipment\nPink noiseA noise signal whose spectrum level decreases at 3dB per octave rate. This gives the noise equal energy per octave and is used to test many acoustic devices.\nPorous absorber – Sound absorbing finish where the sound energy falling on it is dissipated by viscous losses within the pores of the material and converted to heat.\nReflectionThe amount of sound wave energy (sound) that is reflected off a surface. Hard non-porous surfaces reflect more sound than soft-porous surfaces. Some sound reflection can enhance quality of signal of speech and music. (See Echo).\nResonanceThe emphasis of sound at a particular frequency.\nResonant frequencyA frequency at which resonance exists.\nReverberationThe time taken for sound to decay 60 dB to 1/1,000,000 of its original sound level after the sound source has stopped. Sound after it has ended will continue to reflect off surfaces until the wave loses enough energy by absorption to eventually die out. Reverberation time is the basic acoustical property of a room which depends only on its dimensions and the absorptive properties of its surfaces and contents. Reverberation has an important impact on speech intelligibility.\nReverberation timeSound after it is ended at the source will continue to reflect off surfaces until the sound wave loses energy by absorption to eventually die out.\nSound absorption coefficient – Of a surface or material at a given frequency and under specified conditions : the complement of the sound energy reflection coefficient under those conditions, i.e., it is equal to 1 minus the sound energy reflection coefficient of the surface or material.\nSound insulationIs the ability of building elements or structures to reduce sound transmission\nTo compare sound insulation properties you need to take into account the area of the dividing partition/wall, as well as the volume and sound absorption properties of the receiving room. To do this, measurements are Normalized to a Reference Absorption value or Standardized Reverberation Time.\nAbsorption and Reverberation Time are mathematically related so if the reverberation time is measured in the receiving room then both procedures are catered for.\nA single number to present the results and compare products would be useful, this is where the Weighted term comes in.\nThe sound insulation is measured at different frequencies, normally 100-3150 Hz.\nAirborne sound insulation is expressed by a single value, Dn,t,w, Rw or R'w.\nImpact sound insulation is expressed by a single value Ln,w or L' n,w\nSound reduction index : RThe measured quantity which characterises the sound insulating properties of a material or building element in a stated frequency band - laboratory measurement.\nR = L1 - L2 + 10 lg S/A (dB)\nL1: average Sound Pressure Level in the source room\nL2: average sound pressure level in the receiving room\nS: area of the test specimen (m2)\nA: Equivalent Sound Absorption area of the receiving room\nSound transmission class (STC)A single-number rating obtained by classifying the measured values of Sound Transmission Loss in accordance with ASTM Standard E 413, “Classification for Sound Rating Insulations”. It provides a quick indication of the performance of a partition for certain common sound insulation problems.\nTo determine the Sound Transmission Class (STC) in conformance to the ASTM E 413 (lSO 71 7/1) one must slide the STC contour along its Y-axis of the graph on which the transmission loss curve is plotted until the following conditions are met:\nThe sum of the deviation below the STC contour does not exceed 32 dB.\nNo deviation below the STC contour exceeds 8 dB.\nNote: The ISO standard excludes this last requirement. One should indicate however in the test report, the frequencies at which a difference of 8 dB or more occurs between the noise reduction curve and the STC contour.\nWhen the STC contour is positioned in such a way that these two requirements are satisfied the sound transmission class can be obtained by reading the transmission loss value at the intersection of the STC contour at the frequency of 500 Hz. This value corresponds to the STC of the partition.\nSound transmission class (STC)American single number rating of a partition's isolation value based on laboratory measurement. Results may not be compatible with Rw as a different range of frequencies is used.\nSoundproofingBuilding materials that make structures impervious to sound or insulates against sound.\nSpatial averagingTaking measurements at various positions and averaging the results. Mandatory in sound insulation measurements and recommended anywhere multiple reflections are present. See other types of Averaging.\nSpectrum Adaptation Terms : C and Ctr :The single number rating method defined in BS EN ISO 717 uses a standard reference curve to determine the weighted value of airborne sound insulation.\nThe spectrum adaptation terms C and Ctr may be used to take into account different source spectra as indicated in the standard.\nC is an A-weighted Pink Noise spectrum.\nCtr is an A-weighted urban traffic noise spectrum.\nCtr can also be added to DnT,w or Rw to take into account low frequency noise\nSPL: sound pressure levelQuantity used to describe the loudness of a sound. The sound pressure level is expressed in decibels and is measured with a sound level meter. For example, a conversation between two people inside an average-size room will produce an average “A” weighted sound pressure level of 50 to 55 lb.\nStandardized Impact Sound Pressure Level : LnTThe impact Sound Pressure Level in a stated frequency band, corrected for the standardized reverberation time of 0.5 seconds. Laboratory measurement.\nLnT = Li - 10 lg T/T0 dB\nT = measured Reverberation Time in seconds\nT0 for dwellings = 0.5 seconds.\nStandardized Impact Sound Pressure Level : L'nTThe impact Sound Pressure Level in a stated frequency band, corrected for the standardized reverberation time of 0.5 seconds. Field measurement, written L'nT to differentiate between LnT\nStandardized Level Difference : DnTAirborne sound transmission. Similar to the Dn, but this index corrects the measured difference to a standardized reverberation time of 0.5 seconds. This RT value is often cited as approximately average for a medium sized, carpeted and furnished living room. It does not require detailed and accurate knowledge of the dimensions of the test rooms.\nDnT = D + 10 lg T/T0\nD = level difference\nT = reverberation time in the receiving room\nT0 = reference Reverberation Time, 0.5 seconds for dwellings."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2472f99e-563c-4fab-afc9-d9b5d74de99a>","<urn:uuid:f0caaa02-31c5-4038-a8c8-bf919423cd38>"],"error":null}
{"question":"¿Por qué el pan de molde es más calórico que el pan tradicional, y cuál es el método óptimo para congelarlo y descongelarlo?","answer":"Sliced bread is slightly more caloric because it contains more sugar and fat from added sunflower oil or olive oil, though this difference is minimal - only about 20 kilocalories/100 grams (1% of daily caloric intake). For freezing, the bread should first cool to room temperature, then be sliced and tightly sealed in plastic wrap with air removed before freezing below 32°F/0°C. For thawing, you can either let individual slices thaw for 2-4 hours, or leave a whole loaf overnight. To restore freshly-baked taste, you can reheat the thawed bread at 375°F for 3-5 minutes, which will re-gelatinize the loaf making it soft and springy.","context":["Updated on Tuesday, 5 October 2021 – 08:47\nThe OCU invites “invites to prioritize this type of bread” after an analysis of 12 white mold loaves with crust, 12 whole grains and 10 multigrain\nSliced bread with seeds THE WORLD\nWholegrain and multigrain sliced bread are healthier than white sliced bread, but also more expensive, as well as being higher in calories than traditional loaf bread.\nThis is concluded by an analysis that the Organization of Consumers and Users (OCU) has carried out in 12 white sliced breads with crust, 12 whole grains and 10 multigrain to compare the quality of the sliced breads for sale in the main supermarkets as well as their nutritional contribution, hygiene, texture, homogeneity of the slices, the tendency to stale and labeling, according to a statement .\nIn parallel, it has studied their sale prices and a dozen trained tasters have carried out a tasting and among the main conclusions of the research highlights the greater contribution of fiber from wholemeal and multigrain breads, which, according to OCU, “usually doubles” the of white breads as a consequence of the use of whole grain flour as the main ingredient and the addition of bran made by some brands, a matter that, in his opinion, “invites prioritize this type of bread “.\nLikewise, he underlined that, although in multigrain bread the fat content is “higher” than the rest, since they are polyunsaturated fats that come from added seeds and cereals, they are considered “healthy”.\nAlong with this, he admitted that the hygiene is “correct” in all the products analyzed, at same as texture and labeling, except in the case of the Rustic Bakery cereal bar, whose slices are the least tender of those analyzed, although also the only ones that do not incorporate additives, while the rest have between two and 10.\nIn front of the loaf of bread, the bread is “slightly” more caloric because it contains “a little more” sugar and fat from sunflower oil that is added, although some incorporate olive oil. However, according to the OCU analysis, this difference “barely” reaches 20 kilocalories / 100 grams, or, what is the same, 1% of the daily caloric intake.\nIn contrast, sliced bread contains “a little less” salt, especially white sliced bread, and will last longer fresh.\nRegarding the price, according to the OCU research, this is usually higher in whole grain and multigrain breads, standing at 2.00 euros / kilo and 3.02 euros / kilo respectively, compared to 1.62 euros / kilo for the price. white sliced bread. However, he cited as “exceptions” the whole wheat bread La Cestera, from Lidl, which costs 1.09 euros / kilo and the multigrain bread El Horno de Aldi, on sale for 2.04 euros / kilo, products that, in addition, stand out for their good value for money and its good tasting results.\nRegarding conservation, OCU recommended store the sliced bread in a cool and dry place, but not in the fridge, since the cold “spoils” its texture, and pointed out that, in this way, it can be consumed even after its best before date, which is around 15 days from the time of purchase.\nAccording to the criteria of\nThe Trust Project Learn more","Once the bread has cooled to room temperature, the bread needs to be put in a sealed container. This could be a bread box or plastic bag, and the bread can be put in a plastic bag before being stored in the bread box.\nThe question for many is: is this enough? After all, you’re advised to keep the bread in a cool, dark place.\nThe good news is that the average bread box is good enough. Your home should be at room temperature, and the bread box is slightly cooler since it blocks out the sun and electric lights. Furthermore, sitting in a plastic bag in a dark pantry is fine, too.\nYou can store the bread at 20 to 22 degrees C or 68 to 72 degrees F that maximizing the time the loaf remains fresh and soft.\nCan I Store the Bread at a Colder Temperature?\nAt colder temperatures, you run the risk of the bread drying out faster than desired if it isn’t properly covered. The worst-case scenario is storing the bread in the refrigerator.\nIt may be edible for several days, but it will make better stuffing than sandwiches. This is because it crystalizes faster at temperatures below room temperature. In fact, the bread crystalizes or stales faster at cooler temperatures than warmer ones.\nWhat Temperature Should Frozen Bread Be Stored At?\nIf you want to keep your homemade bread fresh as long as possible, there are several steps you need to take. One is to let it cool on a cooling rack before you wrap it in plastic.\nThe next is to put it in the freezer almost immediately after it has cooled. In this scenario, the bread should be kept well below the freezing point (32 degrees F, 0 degrees C). It must be tightly sealed in plastic wrap so that it doesn’t essentially freeze dry.\nOnce you take out the bread to thaw, you can choose to leave it exposed to the air to thaw it faster or keep it wrapped in plastic knowing it will take more time.\nHow Can You Prep the Bread when You’re about to Freeze It?\nDo not freeze it when it is still piping hot since the heat from the bread will prevent it from cooling down as fast and can even affect the safety of the surrounding food. Let it cool down to room temperature before you try to freeze it.\nThe best approach is to slice it before you freeze it. Not only is this the easiest time to slice it, but then you have the option of removing and thawing individual slices of bread.\nThey’ll thaw in two to four hours instead of having to wait overnight for a whole loaf to freeze. Try to get all of the air out of the bread bag before you freeze the loaf so that it freezes solid as quickly as possible.\nWhat Happens If the Bread Is Stored in a Warm Place?\nLet’s take the opposite scenario of the ones we’ve just described.\nWhat happens if the bread is stored in a location much warmer than standard room temperature? If it is stored in a plastic bag or paper bag after cooling, the bread could be just as edible after 6 to 12 hours at near 100 degrees F or 38 degrees C as if it had sat at room temperature.\nNote that we say it had to be sealed. If it was exposed to the air at that temperature, it would not only dry out but foster mold and bacteria growth because of the warmer temperatures. For this reason, you don’t want to store the bread in the oven\nIf the bread has not yet vented the excess moisture before being put in a bag, it would become a breeding ground for mold. And the damage is made worse by higher temperatures.\nThis means you can store the bread in the same warm place you let the dough rise, though it needs to be protected from mold and bacteria in the air.\nCan You Store Bread in a Warming Drawer?\nSuppose you just made dinner rolls or a loaf of French bread. You want to keep the bread warm until dinner. The hard part is maintaining the perfect texture.\nPut the bread in a warming drawer of the oven at up to 50 or 60 degrees C or 110 to 120 degrees F. However, it needs to be covered to help it maintain its moisture.\nAnd, ideally, you’re putting the bread there immediately after you baked it, so it is still essentially cooling. Then you can put it on the table shortly before serving everyone.\nYou can leave bagged loaves from the store in the warming drawer for several hours, too, to warm it up from room temperature to a higher one to serve it.\nWhat Is the Best Way to Thaw Bread?\nSuppose you kept the bread frozen for several weeks until you’re ready to eat it. You can let it thaw for several hours in the fridge. If you are dealing with a large loaf, it will need to be allowed to thaw out overnight.\nTake it out of the plastic or foil and serve. You can also “refresh” the bread to make it have that fresh-baked taste by putting it in the oven for three to five minutes at 375 degrees F once it has thawed.\nThis is especially true if you froze it immediately after it cooled down. However, this tactic works if you froze a loaf that was a day or two old, as well.\nRe-heating it will re-gelatinize the loaf, making it almost as springy and soft as it was when you froze it.\nHowever, this loaf shouldn’t be stored in a hot drawer for long or it will dry out rather quickly."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0b2d903c-3e70-4269-8a4b-c76fb676d745>","<urn:uuid:0fb616c6-8e9f-4927-ae32-30fba5c3d031>"],"error":null}
{"question":"What are the primary benefits of artificial insemination in cattle breeding, and how does this relate to constitutional provisions for agricultural corporations?","answer":"Artificial insemination in cattle breeding offers numerous benefits: it allows insemination of 300-400 cows from a single collection of semen, enables production of high-productive hybrid cows, reduces ox rearing costs, and increases pregnancy rates. The semen can be preserved for years and transported worldwide. Regarding corporations, the California Constitution of 1849 established that corporations could be formed under general laws, with all corporations having the right to sue and be sued in courts like natural persons. This legal framework supports the establishment and operation of agricultural businesses, including those involved in cattle breeding.","context":["Report of the Debates in the Convention of California, on the Formation of the State Constitution, in September and October, 1849\nJohn Ross Browne (1817-1875) of Kentucky, the official reporter for the California State Constitutional Convention of September-October 1849, came to California in 1849 as an employee of the government revenue service. He traveled widely in the next two decades before settling down in Oakland. Report of the debates of the Convention of California (1850) comprises the official records of the convention. Browne had been a shorthand reporter for the U.S. Senate before coming west, and he provides transcripts of the proclamation calling the convention, proceedings of the convention, text of the state constitution adopted by the delegates, and official correspondence regarding the convention and the institution of state government under that constitution.\nMnenja - Napišite recenzijo\nNa običajnih mestih nismo našli nobenih recenzij.\nDruge izdaje - Prikaži vse\naction admitted adopted amendment appeal appointed authority bank believe body Borts boundary California called citizens civil clause Committee Congress consider consideration Constitution Convention Court decided desire discussion district duties election entirely established existing expenses fact favor fixed fund gentleman give Governor Gwin Halleck hope House important insert interests Jones judge justice lands leave Legislature limits majority matter McCarver means Monterey motion moved necessary object offered officers opinion opposed party pass persons population portion present President Price principle proper proposed proposition question reason received reference regard representatives require resolution San Francisco Senators session settle Sherwood South strike submitted taken territory thing thought thousand tion Union United vote whole wish\nStran 476 - The term corporations, as used in this article, shall be construed to include all associations and joint-stock companies having any of the powers or privileges of corporations not possessed by individuals or partnerships. And all corporations shall have the right to sue, and shall be subject to be sued, in all courts in like cases as natural persons.\nStran 474 - ... and naval forces in time of war, or which this State may keep with the consent of Congress in time of peace; and in cases of petit larceny, under the regulation of the Legislature), unless on presentment or indictment of a grand jury, and in any trial in any court whatever the party accused shall be allowed to appear and defend in person and with counsel as in civil actions. No person shall be subject to be twice put in jeopardy for the same offense...\nStran 246 - I do solemnly swear, (or affirm, as the case may be,) that I will support the constitution of the United States,, and the constitution of the State of California; and that I will faithfully discharge the duties of the office of according to the best of my ability.\nStran 331 - The Governor shall have the power to grant reprieves, commutations and pardons after conviction, for all offenses except treason and cases of impeachment, upon such conditions, and with such restrictions and limitations, as he may think proper, subject to such regulations as may be provided by law relative to the manner of applying for pardons. Upon conviction for treason, he shall have power to suspend the execution of the sentence, until the case shall be reported to the Legislature at its next...\nStran 104 - Corporations may be formed under general laws; but shall not be created by special act, except for municipal purposes, and in cases where, in the judgment of the Legislature, the objects of the corporation cannot be attained under general laws. All general laws and special acts passed pursuant to this section, may be altered from time to time or repealed.\nStran 33 - Every citizen may freely speak, write and publish his sentiments on all subjects, being responsible for the abuse of that right ; and no law shall be passed to restrain or abridge the liberty of speech or of the press.\nStran 156 - ... unless the same shall be authorized by law for some single object or work to be distinctly specified therein, which law shall provide ways and means, exclusive of loans, for the payment of the interest of such debt or liability as it falls due, and also to pay and discharge the principal of such debt or liability within twenty years of the time of the contracting thereof...\nStran 33 - When private property shall be taken for any public use, the compensation to be made therefor, when such compensation is not made by the State, shall be ascertained by a jury or by not less than three commissioners appointed by a court of record, as shall be prescribed by law.\nStran 22 - No person shall be held to answer for a capital or otherwise infamous crime (except in cases of impeachment, and in cases of militia when in actual service; and the land and naval forces in time of war, or which this State may keep with the consent of Congress in time of peace, and in cases of petit larceny, under the regulation of the Legislature), unless on presentment or indictment of a grand jury...","In which method the artificially collected semen kept in the uterus of cow is called artificial insemination. The calf produced in this method contain 50% suchness of hybrid ox. By artificially inseminating of this new cow it will produce a new calf which contain 75% suchness of ox. It takes 8-9 years to produce fully hybrid cow through artificial insemination. The cow containing 75% suchness of hybrid cow is suitable for rearing according to the weather condition and management system of our country.\nUsefulness of Artificial Insemination\n- We can get hybrid calf within short time.\n- Produce more meat and milk.\n- We can get strong and operative cows.\n- We can increase our income.\n- Unemployment problem can be eradicated.\n- We can increase the production of protein.\nPurpose of Artificial Insemination\n- Producing hybrid cow in low cost and short time.\n- Making source of employment.\n- Meet up the demand of meat and milk.\n- Produce strong cow for cultivation.\n- For increasing the production of protein.\n- Facing the insemination lacking and many diseases.\n- Produce calf according to our choice.\n- Advantages of Artificial Insemination.\n- Up to 300-400 cows can be inseminated by collecting semen from ox once.\n- We can get high productive cows by using the semen of hybrid ox.\n- The semen can preserved for many years.\n- Artificial insemination helps to prevent different types of disease.\n- No need to raise so many ox, as a result rearing cost of ox get reduced.\n- Increases the probability and rate of pregnancy.\n- No risk of accident.\n- The semen can transported anywhere throughout the world.\n- High productive cow can produce by mixing various species together.\n- Artificial process takes less time to inseminate the cow.\n- Productivity of local species can be developed by artificial insemination.\nDifficulty of Artificial Insemination\n- Expert person needed for inseminating and collecting semen.\n- Requires physical cleanliness.\n- It will be best if insemination process done within 12-18 hours of excitement. So find out the actual excitement time carefully.\n- The ox needs more care and management which is used for collecting semen.\n- The price of equipment used for artificial insemination of cattle is very high.\n- The cow will not be pregnant if there any fault in artificial insemination method.\n- Requires very expert person.\n- This process can’t prevent disease or any other difficulties.\n- Detect the right excitement period otherwise inseminating in wrong time will result bad.\n- Select the suitable ox, otherwise it will hinder the main purpose of artificial insemination.\n- It is very difficult to find out the proper excitement time of cow.\n- Equipments used in this process needs to keep clean and germ free always.\n- It is very difficult to transport the semen in adverse weather condition.\n- The semen loos its potency if it not transported or preserved in proper way.\n- Using semen of disease affected ox is very harmful for cow.\nCauses of Failure in Artificial Insemination\n- By not inseminating in correct method.\n- Inseminating by damaged semen.\n- By inseminating the cow in primary and final excitement period.\n- Inseminating in rubbish environment.\n- Not using germ free equipment.\n- Lack of care and management during artificial insemination.\nDuties of Successful Artificial Insemination\n- Collect the semen from ox in hygienic method.\n- Preserve and transport the semen properly.\n- Ensure the quality of semen through experiment.\n- Keep the insemination center from all types of pollution.\n- Clean the equipment with germicide before using.\n- Make sure about the proper excitement of the cow.\n- Inseminate the cow regularly in due time.\n- Inseminate them in correct artificial insemination method."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f41165f5-988e-4039-8433-e1a011daa44c>","<urn:uuid:214f92bc-6217-47c8-a48c-a127b9aae12d>"],"error":null}
{"question":"What are the key considerations for designing outdoor steps in terms of safety and lighting? List both structural requirements and illumination guidelines.","answer":"For outdoor steps, structural requirements include: 1) A well-compacted 100mm minimum depth of type 1 hardcore with 150mm consolidated subgrade 2) Proper riser height and tread length for safe carrying of items without handholds 3) Quality materials like York stone or properly installed Indian sandstone 4) 25mm mortar bed for paving stone treads. For lighting considerations: Steps must be well-illuminated for safety after dark, using ground lights or spotlights. Lighting can be incorporated into side retaining walls or the steps themselves. Motion sensors or photocells that automatically activate at dusk are excellent energy-saving options for outdoor step lighting.","context":["“Garden design” tends to conjure up a picture of grand designs and fanciful creativity but very often the London or urban garden designer finds themself dealing with the technical minutiae of building steps and retaining walls with obviously a good deal of paving and brickwork thrown in for good measure. Here I deal with steps. The first thing to bear in mind is finding a good landscape contractor who actually will follow your specifications and not go off piste. If you can’t oversee the build of your design you may as well wash your hands of it because the chances of the hard landscaping being built to your satisfaction are zero!\nIt is very important to have a good length of tread and a good riser that neither trips you because it is too shallow nor makes you stagger because of its height. I always mention the tray trick to my clients: garden steps seldom have any form of hand hold or bannister so the best way to be sure that steps “work” is to imagine carrying a small tray with glasses up or down them without feeling insecure about what your feet are doing. Generosity of steps is important. There are many situations where there is simply not enough space for generosity so the steps should at least have a good tread.\nHere are some good proportions between riser and tread:\nRISER (top to bottom) TREAD (back to front)\nAny GOOD landscape contractor knows how to build steps but the you, as the garden designer, should discuss with the contractor that he has covered the following details: they should be standing on a well compacted 100mm min depth of type 1 hardcore, and with 150 mm well consolidated subgrade. The rise is likely to be 100ml brick on end and the paving stone step/tread should be sittng on a 25mm mortar bed.\nAnd then it is important that the landscape contractor is guided by you on the actual materials…things can go horribly wrong if the wrong or inferior materials are used. OR alternatively if because of economy you are dealing with cheaper materials they should be handled well. The detail is very important.\nLighting can be inserted into the side retaining walls or into the step itself and steel, steel and glass or perspex sides can be incorporated for hand holding if necessary.\nSteps can very often be a truly focal point of a small London garden and it is therefore important for the garden designer to get them right. Here are some particularly precipitous steps coming from street level into a tiny London garden. In order to make them safe to walk down (as you can see there is no hand hold) they have been made pretty generous despite coming into such a small space. They are also incredibly good looking!! Nothing like a bit of York stone coping to smarten up the effect. This is a fine example of how good materials make all the difference. Good workmanship and good product – this is optimum.\nHere below is a picture of some badly built steps. The materials are not the greatest, grey Indian sandstone (and yet Indian sandstone can look very good) and the grouting is awful. You can also see that the stones have been cut incorrectly.\nAnd here below is an example of a Indian sandstone being used well. It is not the best material; limes, Yorks and granites etc being smarter, stronger and better looking, but it is a cheaper material and if cut and built well can be very good looking. As usual it is all to do with the detail, finish and expertise of the builder.\nHere below you can see what a difference the materials make. First of all this front step has been built by an expert but also you can see that the material, basalt, speaks for itself.\nIncredible York stone below – nothing really beats it, but again the builder has done a brilliant job.\nAnd below, once again using Indian sandstone, but looking pretty good because of good workmanship.\nAnd below back to York; there’s no doubt about its quality.\nBut here below for simplicity, practicality AND economy Indian sandstone works so well.\nSo the moral of the story is to choose your materials well, within your budget of course, but make sure you have an excellent builder/contractor to put the steps together.","When renovating your home or building a new one there are many design elements to consider. Good lighting is one of the most important design elements in your home but it is often the one most frequently overlooked. Poor lighting can wash out colours and textures and generally make the space unappealing and in some instances unsafe. It is important to understand the basics of functional and decorative lighting so that it can be incorporated and planned for in the initial design stages.\nDecorative lighting combines general or ambient lighting, task lighting and accent lighting and incorporates a variation in light levels and sources. It creates mood and meaning and can create a focal point in a room or create visual interest like the illusion of height for example by bouncing light off a low ceiling.\nGeneral or ambient lighting is the principal lighting in your home and its purpose is to illuminate spaces for visibility and safety by bouncing off walls and ceilings and covering as much area as possible. It should exude a comfortable level of brightness without glare and is achieved through the use of down lighting and up lighting. Down lighting casts light down from the ceiling or walls through the use of pot lights and track lights. Up lighting illuminates light towards the ceiling and can be achieved through the use of torches and wall sconces.\nTask lighting is more intense and is focused on smaller work areas and should be free from distracting glare and shadows. Good task lighting can be accomplished through well placed recessed lighting, track lighting, pendants and under the counter lights. The general rule of thumb for task lighting is that it should be three times as bright as general or ambient lighting.\nAccent Lighting adds drama to a room by creating visual interest. It is used to highlight treasured possessions or interesting architectural features. Place the light fixture so the light does not block your sightline and to avoid creating a glare or hot spot, aim the light at a 30 degree angle from the vertical.\nWhen creating your lighting design, consider the activities that will take place in the various areas of your home. Most rooms will be used for different activities throughout the day and evening hours so it is important to vary the intensity of the lighting. Lighting controls will give you added flexibility to adjust and adapt the mood and ambience of the different spaces in your home.\nOutside your home, a well lit entryway and a well thought out exterior lighting plan, gives your home a welcoming look, increases curb appeal and provides safety and security for your family and guests. Steps, pathways and driveways should be illuminated to ensure there is a safe path for family and visitors to follow after dark. Consider installing ground lights or spotlights in gardens, around shrubs or trees and in shadowed areas of your home for added visual interest and increased security. Grilling and serving areas are excellent outdoor locations for bright task lighting while eating or conversation areas are more suited to ambient lighting. Fibre optic lighting, motion sensors or a photocell that turns the lights on at dusk and off at dawn save energy are excellent options.\nA well thought out lighting plan is an excellent decorating tool. It will add sparkle and interest to your home, complement your lifestyle and meet the various needs of your family. When shopping for lighting choices do not forget your floor plans and other decorating notes as these will be valuable tools when you are consulting with a lighting expert."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:74fdafc8-81e2-44a0-9afc-541ba63c429f>","<urn:uuid:760b6bf2-d111-4007-a186-54e6dbfedf01>"],"error":null}
{"question":"How does the method of authority influence information credibility in both traditional and social media contexts, and what challenges does this pose for evaluating news trustworthiness?","answer":"The method of authority involves acquiring beliefs from trusted sources, traditionally including parents, teachers, ministers, news sources, and politicians. In recent years, the internet and social media have become increasingly important trusted sources. However, without proper skills to evaluate source veracity, this has led to an unprecedented rate of false beliefs being accepted by the general population. Research shows that while social media platforms are heavily depended upon for information needs (71.8% of users), most users only perceive this information as moderately trustworthy rather than very trustworthy (60.7%). The credibility challenge is complicated by the fact that online users have easy access to abundant sources but also risk getting false information, with less control and gate-keeping compared to traditional print publications.","context":["How do we find out what we believe in, or what are the methods that we have of knowing?\nAccording to Peirce (1877), there are four methods of knowing information, method of authority, method of tenacity, a priori method, and the scientific method. I will review each one of them, and consider how they impact our learning and how we can influence them through our teaching. I will consider the method of tenacity and the a priori method first\nIn both the method of tenacity and the a priori method, there is often no way to identify where the knowledge or the belief came from, it just is. The fundamental difference is the willingness to change a belief.\nA Priori Method\nIn the a priori method, the belief is there because it seems reasonable and rational within the cultural context of the day. Our society has certain beliefs that we accept, without question, simply because our society holds to those beliefs. As an example, in our modern democracies, we all know that democracy is the best form of government – at least until something better comes along. We don’t question that belief, and it becomes one of the assumptions that we live with. It is reasonable, and we have no reason to question it. If asked, we usually have no idea where the belief came from, it just is. We accept it as a part of our belief system without close examination or consideration, we accept it before (a priori) really thinking about it. It is one of the beliefs that we simply have.\nMethod of Tenacity\nLike beliefs that could be classified as a priori, beliefs that fall under the banner of the method of tenacity don’t usually have an identifiable source. Method of tenacity beliefs are just beliefs that we acquire. How they differ from a priori beliefs is our unwillingness to abandon these beliefs in light of new evidence.\nBeliefs that become method of tenacity beliefs allow us to live in a fixed world. A world that doesn’t change and our beliefs reflect that world. For most of us, gravity is a constant that never changes. Through inductive reasoning, practiced from an early age, we fix in our minds our belief about gravity. It allows us to contextualize our physical world, and we can be secure that our belief system, along with the fixed physical properties that our belief represents, will provide us with a stable world through which we can live and move. Evidence available from the world of advanced theoretical physics that tells us that gravity isn’t exactly what we believe it has virtually no impact on how we live our lives in the physical world. It has virtually no impact on our belief in gravity and the pragmatic effects it has on our physical beings. Our belief in gravity is a fixed belief that we don’t change, and it allows us to live in a fixed and settled world.\nExamples of method of tenacity beliefs that are problematic are racial biases and bigotry. Although they allow people to live in a fixed and unchanging world, these beliefs are founded on falsehoods and cause no end of problems. Because they are fixed and people refuse to change them, in spite of evidence, the problems that attend adherence to these beliefs can be extremely problematic.\nOne of the sources of problems with any false beliefs, but especially ones that are classified as method of tenacity beliefs, is that people seek out others who hold to the same beliefs in order to validate their beliefs. In this day and age, the internet allows people to gather in larger and larger clusters to support false beliefs, and keep their belief system protected from the inconvenience of evidence that might cause someone to question what he or she believes in.\nChanging A Priori and Method of Tenacity Beliefs\nChanging either a priori or method of tenacity belief systems is difficult. In order to address deeply held belief systems, people have to begin to understand their thought processes. They have to begin to understand that not everything that they think is necessarily true. They have to begin to think about where their thoughts come from. In our current education system, although beliefs are challenged, thinking about where the beliefs come from is never a question that is asked.\nA child who watches when a bit of sodium is placed in water that then bursts into flames has a belief system that is thrown into turmoil. Things shouldn’t burn in water. When this is shown to a child in our current education system, the observation is simply filed away as something new that they learned. Rarely, if ever, are opportunities taken to explore where beliefs might come from. Instead, a new fact is simply introduced that is memorized and filed away for future reference. A belief is slightly altered to include exceptions but basically remains essentially the same.\nWhen a person encounters evidence that runs counter to deeply held belief systems, negative emotions arise. Nobody wants to be wrong. Rather than dealing with new information and carefully considering the evidence, most people will simply reject the new information and fall comfortably back onto their old belief systems.\nIn teaching, we can begin to lay the groundwork for thinking about where our belief systems come from when the beliefs are concrete and the evidence is easily observed. However, really considering our thoughts and thought processes requires abstract thinking, something that doesn’t emerge until the frontal lobe neurons are in the process of undergoing or completed myelination. This process relies on the hormones that become active with adolescence and is completed between late teens and young adulthood (the late twenties for males).\nThe ability to self-correct is available once someone can understand and manipulate abstract thoughts. Self-correction is one of the abilities that underlie critical thinking. Something that isn’t actually taught in higher education, but is simply assumed (it is a skill that must be taught). Even worse, the learning of critical thinking is largely context dependent. This means that when someone learns to think critically, the abilities that underlie critical thinking are tied to the context in which they are learned. Just because someone can think critically about economics does not enable them to think critically in other contexts. They must be exposed to other contexts and coached on using their economic critical thinking skills in the new context. After doing this in a couple of different contexts, the skills become transferable, and someone can apply his or her critical thinking skills in any context.\nA lofty goal, but only achieved in a single area about 50% of the time and made transferable in fewer than 10% of people. Something we need to work on.\nMethod of Authority\nThe third method to consider is the method of authority. The method of authority encompasses the beliefs that we acquire because someone we trust (a trusted source) has told us about something. Most of the knowledge we have, and most of our beliefs, are acquired through the method of authority. Even our beliefs that we would classify as a priori and method of tenacity, are acquired because someone we trust has told us, in some form, the information.\nWhen we are told something from a trusted source that we have no reason to disbelieve, we are acquiring information through the method of authority. Traditionally, the trusted sources in our society are parents, teachers, ministers, news sources, politicians, etc. More recently, the internet (surfing, blogs, facebook, twitter, LinkedIn – which is, at least this part, a reliable trusted source – etc.) has taken on an increasingly important role as a trusted source. In the absence of an understanding of how to evaluate the veracity of a source, something that must be learned and practiced (can’t happen in our world of education, because there’s too much content to cover), we live in an age where false beliefs are accepted and incorporated at an unprecedented rate in the general population. Fake news has become a new tool for politicians – not the fake news itself, but the phrase, which covers anything the politicians don’t agree with. Which brings us to the final method of acquiring belief systems – the scientific method.\nThe Scientific Method\nThe scientific method of acquiring knowledge (or beliefs) is really quite simple, even though it has become shrouded in misconceptions. The scientific method really involves four steps.\n- Make an observation – measure something.\n- Evaluate that measurement in light of what we already know about what you have measured.\n- Publish the measurement and the evaluation that has been carried out.\n- Listen to and respond to the feedback that you receive about the measurements that you have made and the evaluation you have published.\nSimple really – measure, evaluate, publish, and then listen. This cycle of acquiring knowledge means that the community studying something will go through cycles of understanding, as they incorporate new findings into the communal understanding of something.\nThe scientific method works and has resulted in a world that is awash with findings and understanding that have arisen through the scientific method of acquiring knowledge. We used it to put a man on the moon! This method can (and has) been derailed at any step. If the measurements are not precise, if an evaluation is biased or flawed for any reason, if the publication of methods and results is prevented (something that happens increasingly in the proprietary world of corporate or secret government research programmes), or if the feedback to the community is not incorporated.\nIn addition, the scientific method can be undermined if there is distrust in the method itself. Think about the popular (movies, books, magazines) portrayal of scientists. They are almost always negative, with the scientists portrayed as evil, stupid, or conniving. Politicians (trusted sources) are also increasingly undermining the scientific method as a means to acquire beliefs.\nAs a result, the primary method of acquiring beliefs in our society today is the method of authority. This means that individuals have no choice but to rely on a trusted source, and we (the educators) have not taught students how to evaluate sources for veracity (too much content). Is it any wonder that we have huge swaths of the populace embroiled in scientific controversies for which there is no controversy? This societal problem must be laid squarely on our shoulders. Higher education is 100% responsible, and, for the most part, has no intention of changing anything in order to fix it.\nWe can, and must, do better. At least we can teach our students transferable critical thinking skills (the willingness to self-correct) and how to evaluate a source against available evidence.","CREDIBILITY OF SOCIAL MEDIA AS ALTERNATIVE NEWS SOURCES: AN EVALUATIVE STUDY\nThe traditional media have been acknowledged for years as reliable sources of getting news but the advent of new technology, especially the internet variant which has introduced the interactive social medium, seems to be changing that notion. The Internet’s credibility is a major concern since information seeking is one of the main purposes of using the Internet. Online users have easy access to abundant sources but also run the risk of getting false information. This has prompted much academic research geared to ascertain the situation and to clarify the controversy. This credibility problem associated with social media news is of much concern to the questions of trustworthiness, objectivity, believability and expertise input on social media content, etc, which are cardinal issues in ethics of communication. It was on this premise that this study became imperative to determine the extent to which social media users perceive the news content on social media as credible. The following issues were interrogated in the study’s objectives: (a) to what extent do users depend on social media for information needs? (b) to what level do social media users perceive social media news as trustworthy? (c)What is the extent to which social media users perceive expertise in social media news content? (d) to what extent do social media users perceive social media news as objective? (e) to what extent do social media users believe social media news content. The survey research design was adopted and a structured questionnaire was used to gather data from a sample of 222 respondents through the use of the Australian Calculator. The quantitative data generated through the questionnaire were analysed using frequency distribution and percentages. The three alternate hypotheses were tested with a combination of T-test and Analysis of variance (ANOVA). While the t-test examined if there was dependence or independence of one variable on the outcome of another variable between two groups, the ANOVA tested mean differences among several groups. Findings showed that: (i) Social media platforms were depended upon by majority of the respondents for their information needs (71.8%); ii) Majority, (60.7%) of the respondents perceive information from social media network as trustworthy but not as very trustworthy; iii) Majority of the respondents said there was expertise in handling social media news (63.1%); iv) Majority, (63.6%) perceive social media news as objective news sources; and (v) those who rated social media news as believable, were in the majority (65%). T-test showed that there was no significant difference (t=1.152, df =203, P >.05) between gender and dependence on social media for news. Analysis of variance showed that perception of social media as credible news sources was dependent ((F=2.640), df=204, P <.05) on level of education. Finally, analysis of variance also showed that there was significant difference ((F=3.179), df=204, P. <.05) in the perception of social media as objective news and this difference occurred due to variations in age categories. The study concludes that social media news content is credible based on the parameters of believability, trustworthiness and objectivity. Also, the credibility of the social media as was established in this study, lends them as alternative news sources. It is recommended among others that, the social media be increasingly used to share important information, especially in disseminating official information.\n1.1 Background to the Study\nInternet technology has made communication much easier and less expensive. It has attracted many people and has penetrated into people’s daily lives. The mass media also have accepted the Internet. Almost all forms of traditional media, such as radio, television, and newspapers have extended their work into this new field. Online media distinguish themselves from traditional media. Online media allows readers to enjoy browsing their content and offer not only texts but also digital images. Online media can present the most recent information and links to related news articles from local to international topics. The interactive features of the Internet seemingly imply that online media have more advantages than traditional media forms. People’s expectations for new types of journalism are driving them online. Some have begun seeing the Internet as an alternative to traditional media and this has raised credibility questions.\nIn communication research, the credibility of the communicator has widely been suggested to influence the processing of the communicated content and the change of audience attitudes and beliefs (Kang, 2010). Also, it has been suggested that the credibility of the channel/medium of communication influences the selective involvement of the audience with the medium (Metzger et al., 2010; Metzger et al; 2003). Accordingly, individual audiences are paying closer attention to the media that they perceive to be credible (Johnson and Kaye, 2010). When individual audiences rely more on a certain communication medium for information seeking, they are likely to rate the medium more credible than other media (Johnson & Kaye, 2010; Kiousis, 2010; Kiousis, 2001).\nGenerally, credibility refers to the objective and subjective components of the believability of a source or message. News credibility has traditionally been considered a multidimensional construct, although the composition of credibility dimensions has been inconsistent across studies (Mitchelstein & Boczkowski, 2010; Mertzger et al; 2010; Rahman et al; 2009; Kiousis & Dimitrova, 2006). Along with believability, the most common components of media credibility emerging from past studies are accuracy, fairness, lack of bias, completeness, depth, and trustworthiness (Flanagin & Metzger, 2001; Johnson & Kaye, 1998, 2002).\nThe concept of credibility has been researched along source, message, and media dimensions, and most literature on new media relies on these measures (Metzger et al., 2003). For example, early research focused on source credibility by examining the accuracy of reporting in news (Gaziano & McGrath, 1986). Internet-related credibility research draws mainly from source credibility and the interpersonal communication literature, but also includes items from the traditional media credibility literature. Common variables include the extent to which websites and information online is believable, trustworthy, unbiased, etc. Other dimensions include assessments of accuracy, relevance, and comprehensiveness (Metzger et al., 2003). However, since Hovland, Janis, and Kelly (1953) first began studying credibility, communication scholars have identified trustworthiness and expertise as two primary dimensions of credibility assessment (Chung et al., 2010; Fogg & Tseng, 1999).\nThe credibility of the interpersonal channels became a subject of debate among communication scholars recently (Chung et al, 2012). Gradually, the modern media systems emerged to close the obvious gaps of audiences’ inability to promptly get the news as they break while putting some checks in place to ensure credibility of the news items they communicate. However, ownership factor and other interests led to the politicization and commercialization of news items among the conventional media. At a point, audience members took the news from these media with a pinch of salt. Today, various social media have emerged and they have become sources of news dissemination. Some of such media are: Blackberry, Facebook, YouTube, Skype, Badoo, Nimbuzz and Whatsapp.\nThe traditional media have been acknowledged for years as reliable sources of getting news (Mitchelstein & Boczkowski, 2010), but the same cannot be said of the social media. Recently, some rumours spread like wild fire among Nigerian undergraduates. These rumours later turned out to be untrue. For example, a popular pop singer in Nigeria, Tu Face Idibia, was said to have died and the longest bridge in Nigeria, Third Mainland Bridge, was also going to be blown up by the terrorist group known as Boko Haram. Social network users also circulated the rumour that certain food items from northern Nigeria have been poisoned, causing panic among people. Since social networks involve users‟ generated content, the sources and truthfulness of these contents are hardly ascertained. Notwithstanding, news are continuously spread via these social media, some of which many times have been discovered to be false.\nFast-developing technology, especially in the digital media environment, has empowered individuals and other stakeholder organizations to be creators of communication messages rather than remaining as the static receiver of communication content (Kang, 2010; McClure, 2007). This has raised serious credibility issues due to the problem of checks and verification of the credibility of such news contents and the sources. As a result, the communication process has increasingly become multidirectional.\n1.2 Statement of the Problem\nNews and source credibility over time has engaged the interest of research in communication. Because of technological affordances, resulting in the advent of the internet and other online media-generally referred to as alternative media, the research focus has shifted to investigating the credibility of information emanating from these alternative news sources. This new trend has generated corpus body of debate in the research literature. However, despite the fact that findings show the existence of misinformation on the internet, other researchers found that the online media has believability as high as the traditional media in certain circumstances. For instance, Internet users judged online political information sources as more credible than traditional media counterparts like newspapers and radio.\nThe Internet’s credibility is a major concern since information seeking is one of the main purposes of using the Internet. Online users have easy access to abundant sources but also run the risk of getting false information. Apparently, there is less control and gate-keeping on the web than for print publications. For example, as a result of the recent Ebola outbreak in Nigeria, Nigerian citizens mobbed stores to purchase salt, fueled by a rumor sent via social media. “People were under the false impression that consuming enough iodized salt would protect against infection of Ebola virus disease (Pierson, 2011).\n. Previous researches have focused more on the perception of credibility of content and news sources in the mainstream media, meanwhile not much is known about the credibility challenge introduced by alternative media of facebook, twitter, blogs, etc. This credibility problem associated with social media news is of much concern to the questions of trustworthiness, objectivity, believability and expertise input on social media content, etc, which are cardinal issues in ethics of communication.\nIt is on this premise that this study became imperative, to determine the extent to which social media users perceive the news content on social media as credible.\n1.3 Objectives of the Study\nThe main objective of the study is to ascertain the extent to which social media users perceive the news content of social media networks as credible. Hence, in specific terms, the study has the following objectives:\n- to determine users’ level of dependence on social media for information needs.\n- to ascertain the level to which social media users perceive social media news as trustworthy.\n- to examine the level to which social media users perceive professionalism in social media news content.\n- to evaluate the extent to which social media users perceive social media news as objective.\n- to determine the extent to which users believe social media news content.\n1.4 Research Questions\nTo meet the above stated objectives, the following research questions will guide the study:\n- to what extent do users depend on social media for information needs?\n- to what level do social media users perceive social media news as trustworthy?\n- What is the extent to which social media users perceive professionalism in social media news content?\n- to what extent do social media users perceive social media news as objective?\n- to what extent do social media users believe social media news content?\nThe following hypotheses were predicted in relation to the research questions and the demographic variables:\n- H1: There is significant difference between dependence on social media for news and respondents’ gender.\n- H1: Perception of social media as credible news source will be dependent on level of education.\n- H1: There is positive significant difference in the perception of social media as objective news sources across age variable.\n1.6 Significance of the Study\nThe study has both theoretical and practical uses.\nPractically, the findings of the study will shed more light on an understanding of how social media could serve creditably as an alternative news source to users and the entire citizenry. Especially, it will help reveal the preference of young people between the social networks and traditional mass media as news sources and the extent to which young people believe social network news/information.\nTo the government, as policy makers, the findings of the study will be useful in providing information on the use of social media to disseminate government news for better governance. Also, without valid and reliable measurement of credibility, the management of credibility in social media will be hardly feasible for policy action.\nFor the media industry in general, this study will unravel best ways to improve the credibility of alternative media content.\nTheoretically, this study will contribute social scientific literature on credibility of the use of social media as sources of news. Finally, the academia will benefit from the study, as the findings will discover other areas rich for further researches to help commemorate the study.\n1.7 Scope of the Study\nThis research is focused on evaluating the credibility of social media as sources of news. Since the numbers of those who use social media are many, it would be impracticable to sample all their opinions. Therefore, this study would be limited to examining the phenomenon in Enugu state.\n1.8 Definition of Terms\nCredibility: the extent to which information is believable, accurate, complete, and trustworthy.\nSocial media: are modern interactive communication channels through which people connect to one another, share ideas, experiences, pictures, movies, messages and information of common interest.\nNews: are information disseminated through the channels of communication.\nAlternative: this refers to resorting to other means or measure to satisfy users or audience information needs. In this study, the social media is seen as the alternative.\nEvaluative: In this study, evaluative means an attempt to assess the efficacy of the social media as an alternative news source."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:589ebed5-1f5b-4000-aba9-0734b81ede27>","<urn:uuid:5352e2ca-15e1-410e-b632-2735fe522681>"],"error":null}
{"question":"What are the best companion plants for growing tomatoes organically, and how do they help?","answer":"Marigolds and basil are two of the best companion plants for tomatoes. They contain components or fragrances that repel pests like aphids, thrips, and fruit fly. Additionally, mint helps keep tomatoes healthy and improves their flavor and growing conditions. Dill and borage can be planted to prevent tomato grub and improve the overall health of tomatoes.","context":["If you are of the opinion tomatoes have lost their flavour 'Gardener's Delight' is a must for you.\nPacked with bite-size fruit which are extremely sweet in flavour, they are ideal for salads and sandwiches. The true tangy flavour of tomatoes of a century or more ago, it has earned its reputation as one of our most popular tomato varieties.\nHeavy cropping with long trusses of small size fruits each weighing around 14 to 18 grams, Gardeners Delight is reliable and prolific whether grown outdoors or in the greenhouse. It can be grown either as a bush or as a single stem and is often grown in containers on the patio.\nTomato 'Gardeners Delight' has been awarded the RHS Award of Garden Merit.(AGM).\nAs they cannot tolerate any degree of frost the timing for sowing and planting outside is key to successfully growing tomatoes. Where the seeds are sown under cover or indoors, aim to sow the seeds so that they reach the stage to be transplanted outside three weeks after the last frost date. Tomato plants take roughly seven weeks from sowing to reach the transplanting stage. For example, if your last frost date is early May, the seeds should be planted in early April to allow transplanting at the end of May.\nTomatoes require a full sun position. Two or three weeks before planting, dig the soil over and incorporate as much organic matter as possible. They will grow well in growbags and large pots if regularly watered and fed. The best soil used for containers is half potting compost and half a soil-based type loam: this gives some weight to the soil.\nSowing: Sow from the end January to April.\nPlant about 3mm (1/8in) deep, in small pots using seed starting compost. Water lightly and keep consistently moist until germination occurs. Tomato seeds usually germinate within 5 -10 days when kept in the optimum temperature range of 21-27°C (70-80°F). As soon as they emerge, place them in a location that receives a lot of light and a cooler temperature (60-70°F); a south-facing window should work.\nWhen the plants develop their first true leaves, and before they become root bound, they should be transplanted into larger into 10cm (4in) pots. Young plants are very tender and susceptible to frost damage, as well as sunburn. Protect young plants by placing a large plastic milk jug, with the bottom removed, over them to form a miniature greenhouse.\nDepending on the components of your compost, you may need to begin fertilising. If you do fertilise, do it very, very sparingly with a weak dilution.\nTransplant into their final positions when they are about 15cm (6in) high. Two to three weeks prior to this, the plants should be hardened off.\nJust before transplanting the tomato plants to their final position drive a strong stake into the ground 5cm (2in) from the planting position. The stake should be at least 30cm (1ft) deep in the ground and 1.2m (4ft) above ground level - the further into the ground the better the support. As the plant grows, tie in the main stem to the support stake - check previous ties to ensure that they do not cut into the stem as the plant grows.\nDig a hole 45cm (18in) apart in the bed to the same depth as the pot and water if conditions are at all dry. Ease the plant out of the pot, keeping the root ball as undisturbed as far as possible. Place it in the hole and fill around the plant with soil. The soil should be a little higher than it was in the pot. Loosely tie the plant's stem to the support stake using soft garden twine –allow some slack for future growth.\nA constant supply of moisture is essential, dry periods significantly increase the risk of the fruit splitting. Feed with a liquid tomato fertiliser (high in potash) starting when the first fruits start to form, and every two or three weeks up to the end of August. In September, feed with a general fertiliser (higher in nitrogen) in order to help the plant support it's foliage.\nOver watering may help to produce larger fruit, but flavour may be reduced. Additionally, splitting and cracking can result from uneven and excessive watering.\nCordon (Indeterminate) varieties of tomatoes have a central stem that produces leaves and trusses of fruit. Once a truss has been established and the first fruits begin to form, pinch out the side shoots that emerge from joins between the leaves and the stem. The plant will then concentrate on growing your chosen truss. Also remove any lower leaves which show any signs of yellowing to avoid infection and as the plant grows, remember to tie it loosely to its supporting cane.\nBush varieties don't need this intensive training. They should be sturdy enough to cope without support and there's no need to limit the amount of shoots that they develop.\nHarvesting: June to September.\nHarvest tomatoes as soon as the fruits are ripe, when they are fully coloured and firm, this also encourages the production of more fruit.\nAbout a month before the average first autumn frosts, clip all blossoms and any undersized fruit off the plant. This will steer all the plant’s remaining energy into ripening what’s left.\nIf you have a lot of under ripe tomatoes near the end of the season, and a frost is approaching, pick them and store them indoors in a single layer away from direct sunlight to ripen.\nThe best way to grow tomatoes is to grow them chemical free and as organically as possible. Tomatoes require help to grow and this can often amount to an uphill struggle to keep them bug and disease free. However there are some clever little methods you can utilise to ensure a lush, organically grown crop of tomatoes.\nOne of the best and most universally employed methods in successful tomato growing is companion planting. Companion planting means carefully placing pest repellent plants in amongst your tomatoes so that unwanted bugs are kept away.\nTwo of the best companion plants for tomatoes are marigolds and basil. Both of these plants contain components, or a fragrance, that acts as a pest repellent. Bugs such as aphids, thrips, fruit fly and others are kept at bay and away from your tomato plants as they grow.\nAnother great herb is mint. If you plant tomatoes near a runner of mint it helps keep tomatoes healthy and even improves their flavour and growing conditions. To avoid tomato grub, plant dill and borage. These also improve overall health of tomatoes.\nRemembering that potatoes, tomatoes and eggplants are all part of the Nightshade family, it is known that the leaves of these contain toxins which attract “friendly bugs,” such as ladybirds. Soak approximately two cups of tomato leaves in two cups of water the overnight, and then squeeze the water from the leaves. Strain this mixture through a fine sieve and add equal amounts of water then use as a spray. Spray above and under leaves of your tomato plants. This deters aphids and attracts bug-eating insects.\nDiseases such as anthracnose, early blight and similar fungal problems are best controlled right from the word go. Follow these steps to avoid fungal problems:\n1) Start with a good, clean friable soil, preferably one that has not had tomatoes planted more than once or twice in previous seasons.\n2) Avoid over-composting as composts can harbour bacteria, which is harmful to tomato plants.\n3) Mulch well around tomatoes to prevent excess moisture and “steaming.”\n4) Water tomato plants at root level, avoiding wetting the leaves.\n5) Do not tread over or disturb the root systems around tomato plants.\nAnother point to remember with growing tomatoes is to water regularly once tomatoes start to appear and grow. Irregular watering can initiate cracks in tomatoes. Too much water can cause them to swell faster and with the skin unable to cope, will cause cracking. Also allowing them to go without water and then watering hard to compensate for under-watering, will also result in the same problem.\nFollowing these simple steps can help ensure you grow a crop of tomatoes you can be proud of – there really is no tomato like a home-grown tomato!\n- Additional Information\nPacket Size 50mg Average Seed Count 50 Seeds Common Name Cherry Tomato\nVine / Cordon (Indeterminate)\nFamily Solanaceae Genus Lycopersicon Species esculentum Cultivar Gardeners Delight Hardiness Half Hardy Annual Fruit Red cherry, 14 to 18 grams Height To 150cm (60in) Spacing 60cm (24in) Season Mid-Season Time to Sow Early April to End May\nEight weeks before the last frosts\nTime to Harvest 65 Days from Transplanting"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1ebe6a86-6d5a-4421-9824-cd0f809d3672>"],"error":null}
{"question":"Which race team had a stronger presence in Formula 1: Tyrrell Racing or Honda?","answer":"Tyrrell Racing had a more sustained presence in Formula 1. Tyrrell operated continuously from 1968 to 1998, winning multiple championships including three World Championships with Jackie Stewart (1969, 1971, 1973) and a Constructors' title in 1971. Honda, in contrast, had more sporadic involvement - first entering as a constructor from 1964-1968, then returning as an engine supplier from 1983-1992, and later as both an engine supplier and constructor in the 2000s until their exit in 2008. However, Honda was highly successful during their peak period, powering six consecutive Formula One Constructors Championships from the late 1980s to early 1990s with Williams and McLaren teams.","context":["CONSTRUCTORS: TYRRELL RACING ORGANISATION\nName: Tyrrell Racing Organisation\nAfter serving with Royal Air Force during World War II, Ken Tyrrell went into the timber business. It was a good move and Tyrrell quickly made a fortune. Eventually he decided to have some fun and bought himself a 500cc Formula 3 car and began racing. He was a front-runner in F3 throughout the 1950s and in 1958, in addition to racing himself, he ran a Formula 2 Cooper for Henry Taylor in the colors of Nixon's Garage.\nIn 1960 the Tyrrell Racing Organisation was established to run the factory Coopers in Formula Junior. The team was expanded the following year to run Mini Coopers in touring car races.\nAt the start of 1964 Tyrrell was looking for a driver for the new Formula 3 and chanced upon the pairing of Jackie Stewart and Warwick Banks. The combination was a great success. Stewart won the British Championship and was signed to race for BRM in Formula 1 in 1965.\nTyrrell kept up his links with Stewart, running him and Jacky Ickx in Formula 2 for Cooper. And after John Cooper was seriously injured in a road accident , Tyrrell took over the running of the Cooper Formula 1 team. In F2 he switched to Matra chassis and it was with the French company that he entered F1 in 1968 as Equipe Matra International. The team had the new Cosworth DFV engine in the back of a Matra chassis - and Jackie Stewart was the driver. The result was three victories and an expansion as the team ran a second car for rising star Johnny Servoz-Gavin.\nThe technical package remained unchanged in 1969 but Servoz-Gavin was replaced by Jean-Pierre Beltoise. Stewart won the World Championship. Success convinced Matra that it should not continue in 1970 and so Tyrrell looked around for a new chassis supplier. He ordered March chassis but secretly began to build his own Tyrrell 001 chassis. This was not ready until August 1970 and did not race until the Canadian GP in September but Stewart gave it pole position on its debut and led until the car suffered a stub axle failure.\nFor the 1971 season the car was developed into the Tyrrell 003 and Stewart continued the 1970 partnership with another French rising star Francois Cevert. The 003 won its first race in Spain and the Scotsman went on to five further victories to win his second World Championship. Cevert added another victory and Tyrrell took its first Constructors' title. Things were more difficult in 1972 with Lotus a much stronger rival and despite four wins, Stewart finished runner-up to Emerson Fittipaldi. In 1973, however, Stewart won another five victories to win his third title, although Lotus won the Constructors' title after Stewart decided not to race in the United States Grand Prix at Watkins Glen, following the death in practice of Cevert.\nWith Stewart deciding to retire, Tyrrell had to find two new drivers for 1974 and chose rising stars Jody Scheckter and Patrick Depailler. There were occasional successes - notably in 1976 when the team produced the famous Tyrrell six-wheeler - but then Scheckter moved to Ferrari in 1977. His place was taken for a year by Ronnie Peterson but he moved to Lotus after just a season and Tyrrell took another young Frenchman - Elf protege Didier Pironi.\nBy the end of 1978, however, there was increasing pressure on Elf - Tyrrell's major sponsor for most of the decade - to support the Renault Sport and Ligier teams. Backing was found from Candy and Pironi was joined by Jean-Pierre Jarier. At the end of that season Pironi went to Ligier and for 1980 Tyrrell hired the Irish driver Derek Daly. It was the start of a policy of hiring young (cheap) drivers in the hope that their success would attract sponsorship. The danger - as was seen in 1980 - was that young drivers tend to have more accidents. That year the team also ran Mike Thackwell - the 19-year-old New Zealander becoming in Canada the youngest man ever to start a Grand Prix.\nIn 1981 money was very short but a deal with Imola Ceramica enabled the team to sign up Michele Alboreto as team mate to Eddie Cheever. Alboreto stayed in 1982 when the team slipped once more, having to resort to sponsorship from Swedish pay-driver Slim Borgudd.\nThe team was fortunate to find Benetton sponsorship in 1983 - when Alboreto was joined by Danny Sullivan but Tyrrell failed to hold on to the backing in 1984 when the team took a big risk on youngsters Martin Brundle and Stefan Bellof. When Brundle was injured that year Stefan Johansson and Thackwell stood in for him and the following year - after Bellof was killed in a sportscar accident - the team tried out Ivan Capelli and Philippe Streiff.\nBy 1985 it was very clear that if the team was to survive Tyrrell needed a turbocharged engine and was going to have to pay for one. A deal was struck with Renault Sport. More competitive engines meant that there was a better chance of big sponsorship and for three years the team enjoyed the backing of Data General and Courtaulds. The team continued to use young drivers, notably Jonathan Palmer, Streiff and Julian Bailey, but in 1988 was fortunate to pick up Michele Alboreto when a planned deal with Williams fell through. At the same time Tyrrell picked up Ferrari engineers Harvey Postlethwaite and Jean-Claude Migeot after one of the many palace coups at Maranello.\nThe Tyrrell 018 was a good car and in the 1989 midseason Tyrrell was offered money by Camel and decided to do the deal, driving Marlboro man Alboreto out of the team. Tyrrell picked up Jean Alesi as a replacement and found that it had stumbled on another star. A deal to run Satoru Nakajima was agreed and Postlethwaite and Migeot produced the stunning 019 chassis with its revolutionary raised nose. Alesi set the F1 world talking when he diced with Senna in Phoenix.\nAlthough Alesi was soon on his way to Ferrari, Tyrrell looked set for a revival when in 1991 it did a deal with Honda to use old V10 engines and McLaren agreed to raise money for Tyrrell. Nakajima was retained and rising star Stefano Modena was signed. Sponsorship was found from Braun but, despite some good showings, the year was a disappointment. McLaren ended the relationship and the team struggled again. Strong links with Japan developed in the mid-1990s with Yamaha engines and backing from Japan Tobacco for Ukyo Katayama but the team failed to take advantage of its opportunities once again. A major sponsorship deal with Nokia ended after a year and although Postlethwaite (who had gone to Ferrari) was lured back with an offer of equity in the team, the operation never had the money to do the job properly. In the last few years it became increasingly Harvey's team in spirit if not in shares.\nAt the end of 1997 the Tyrrell Family admitted defeat. There was no-one willing or capable of running the team. The family took a $30m check from British American Racing and handed over the operation to BAR management. It was a disaster. Soon Tyrrell engineers were disappearing off to join the new Honda Racing Developments team and at the end of the year Tyrrell closed its doors. A small number of staff moved to the new BAR headquarters in Brackley.","|Rubens Barrichello driving for Honda|\nHonda entered Formula One as a constructor for the first time in the 1964 season at the German Grand Prix with Ronnie Bucknum at the wheel. 1965 saw the addition of Richie Ginther to the team, who scored Honda's first point at the Belgian Grand Prix, and Honda's first win at the Mexican Grand Prix. 1967 saw their next win at the Italian Grand Prix with John Surtees as their driver. In 1968, Jo Schlesser was killed in a Honda RA302 at the French Grand Prix. This racing tragedy, coupled with their commercial difficulties selling automobiles in the United States, prompted Honda to withdraw from all international motorsport that year.\nAfter a learning year in 1965, Honda-powered Brabhams dominated the 1966 French Formula Two championship in the hands of Jack Brabham and Denny Hulme. As there was no European Championship that season, this was the top F2 championship that year. In the early 1980s Honda returned to F2, supplying engines to Ron Tauranac's Ralt team. Tauranac had designed the Brabham cars for their earlier involvement. They were again extremely successful. In a related exercise, John Judd's Engine Developments company produced a turbo \"Brabham-Honda\" engine for use in IndyCar racing. It won only one race, in 1988 for Bobby Rahal at Pocono.\nHonda returned to Formula One in 1983, initially with another Formula Two partner, the Spirit team, before switching abruptly to Williams in 1984. In the late 1980s and early 1990s, Honda powered cars won six consecutive Formula One Constructors Championships. WilliamsF1 won the crown in 1986 and 1987. Honda switched allegiance again in 1988. New partners Team McLaren won the title in 1988, 1989, 1990 and 1991. Honda withdrew from Formula One at the end of 1992, although the related Mugen-Honda company maintained a presence up to the end of 1999, winning four races with Ligier and Jordan Grand Prix.\nHonda debuted in the CART IndyCar World Series as a works supplier in 1994. The engines were far from competitive at first, but after development, the company powered six consecutive drivers championships. In 2003, Honda transferred its effort to the rival IRL IndyCar Series. In 2004, Honda-powered cars overwhelmingly dominated the IndyCar Series, winning 14 of 16 IndyCar races, including the Indianapolis 500, and claimed the IndyCar Series Manufacturers' Championship, Drivers' Championship and Rookie of the Year titles. In 2006, Honda became the sole engine supplier for the IndyCar Series, including the Indianapolis 500. In the 2006 Indianapolis 500, for the first time in Indianapolis 500 history, the race was run without a single engine problem.\nDuring 1998, Honda considered returning to Formula One with their own team. The project was aborted after the death of its technical director, Harvey Postlethwaite. Honda instead came back as an official engine supplier to British American Racing (BAR) and Jordan Grand Prix. Honda bought a stake in the BAR team in 2004 before buying the team outright at the end of 2005, becoming a constructor for the first time since the 1960s. Honda won the 2006 Hungarian Grand Prix with driver Jenson Button.\nIt was announced on 5 December 2008, that Honda would be exiting Formula One with immediate effect due to the 2008 global economic crisis.The team was sold to former team principal Ross Brawn, renamed Brawn GP and subsequently Mercedes GP.\nHonda became an official works team in the British Touring Car Championship in 2010.\n|Cars in India\nAshok Leyland, Aston,Martin, Audi, Bentley, BMW, Bugatti, Chevrolet, Ferrari,Fiat, Force Motors, Ford, Hindustan Motors, Honda, Hyundai, Jaguar, Jeep,Koenigsegg, Lamborghini, Land,Rover, Mahindra, Maruti,Suzuki, Maserati,Maybach, Mercedes-Benz, Mini, Mitsubishi, Nissan, Porsche, Premier,Renault, Rolls-Royce, Skoda, SsangYong, Tata, Toyota, Volkswagen, Volvo\n|Bikes in India\nBajaj, BMW, Ducati, Harley-Davidson, Hero Motor Corp, Honda, Hyosung,Kawasaki, KTM, Mahindra, Piaggio, Royal Enfield, Suzuki, TVS, Yamaha\nSearches related to top watches brands in the world,top 10 watches brand in the world\nworld top 5 watches brand,top 10 best watches in the world,top 10 watch brands in the world\nwatches expensive brands list,top ten watch brand in the world,10 best watch brands,top 10 wrist watch brands"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:7c27d50e-5f91-473a-9b14-3fa532e432f8>","<urn:uuid:7321bad5-f1c0-4903-bcc4-07307ccffd05>"],"error":null}
{"question":"How do PCB manufacturing issues relate to the fundamental principles of digital electronics?","answer":"PCB manufacturing issues directly impact the binary signal processing fundamental to digital electronics. While digital electronics operate on discrete 1's and 0's values, PCB manufacturing defects like solder cavities, acid traps, and electromagnetic interference can disrupt this binary operation. For instance, corrosion of conductive areas or uneven copper deposition can interfere with the proper flow of digital signals. Since digital electronics rely on Boolean logic functions and discrete values for reliable operation, these PCB manufacturing problems can compromise the inherent reliability advantage that digital systems have over analog systems.","context":["The first concept of printed circuit boards (PCB) was proposed by the German engineer Albert Hansen in 1903. Today, these are an essential element of almost every electronic device. They can be made both manually, using dielectric and ferric chloride, or with the help of special automation equipment in the factory (Read more: Best Software for PCB Design). Either way, for the correct and uninterrupted operation of the board, it is necessary to observe a number of rules – from the initial design stage to the final stages of production. Below we discuss seven typical errors that can be significant impediments to the performance of a PCB.\nMost Common Problems that Can Make a PCB Non-operational\nSo, let us review the faults that can happen when designing and manufacturing printed circuit boards.\n1. Solder cavities\nThe formation of hollows in solder joints is one of the phenomena inherent to reflow soldering. Cavities negatively affect the mechanical properties of soldered joints and the thermic behavior of electronic components and obstruct heat dissipation.\nThe first types of hollow that potentially reduce the durability of a PCB are called “champagne bubbles.” They appear in the interface plane between the contact pads of the board and the solder and occur when copper on the surface of contact pads under the IAg finish layer starts caverning up. They can be eliminated by strict adherence to the compositions of all solutions and etchants used to create the IAg coating, as well as continuous monitoring of other critical parameters of the process of applying finishes to the surface of the board.\nHollows of the next kind appear due to the presence of microtransitions in the contact pads. The formation of such cavities can be kept to a minimum by closing the microtransfer holes with a top coat or completely filling them with solder paste, performing screen printing in two passes.\nThere are also voids, called “pricks,” which are located inside or immediately above the layer of intermetallic compounds. Currently, these are successfully eliminated by applying automated control systems to the process of depositing the copper coatings.\nAnd, finally, micro-hollows that occur in compounds between copper and high-tin solders, including SAC and tin-lead solders, due to the Kirkendall effect. They are formed due to the high-temperature aging or during thermocycling of soldered joints and cannot be detected using x-ray control. One of the ways to reduce the number of Kirkendall micro-voids is to alloy the solder with certain elements, in particular zinc.\n2. Corrosion of conductive areas\nIn the manufacture of PCBs, one of the central tasks is the need to tin the tracks. This is necessary to ensure that conductors bind with solder solidly and to protect the tracks from corrosion.\nThe most used conductive material in PCB production currently is copper. Its disadvantage is its susceptibility to oxidation; therefore, usually, copper conductors are covered with additional protective materials (for example, to strengthen the coating structure and increase corrosion resistance, bismuth is often added to a chemical tinning solution). However, when trimming the PCB, you can involuntarily expose the copper layer and thus significantly reduce its service life. Moreover, if this exposed area randomly contacts a conductor, a short circuit may occur on the PCB. In order to prevent this problem, we recommend making sure that a certain distance is observed between areas with a copper coating and the edges of the board.\n3. Pieces of excess metal\nIf you do not adhere to the basic rules of PCB manufacturing (for example, by corroding a very long thin track, or exceeding the allowable depth for part cutting), so-called “chips” can form in the process of applying a metal layer. These chips, in turn, often form redundant connections or branches from planned conductive routes. Obviously, such cases violate the predictability of operation or may even provoke a short circuit. Whatever it may be, such chips potentially reduce the service life of the board and make a device prone to failures. In order to avoid this, it makes sense to create a design without long tracks and to observe the maximum cutting depth for individual sections.\n4. Not using solder masks in PCB manufacture\nA solder mask is a layer of a durable material designed to protect the conductors from solder/flux penetration and from overheating during soldering. The mask covers the conductors and leaves contact pads and knife connectors open. The method of applying the solder mask is similar to applying a photoresistive mask when creating a microprocessor. However, there are situations in which such a mask is absent in some parts of the board (in particular, this sometimes happens due to a change in the scale of the PCB pattern). This can result in oxidation of the metal and a reduction in the service life of the board.\n5. Uneven copper deposition in perforations\nIn order to ensure the passage of current on both sides of the PCB, through holes are made on its surface, which are subsequently deposited with a layer of copper. In addition, to ensure a uniform metallic coating, etching is carried out. In practice, these two processes can be implemented with a number of errors that may result in the formation of hollows in the copper layer. This, in turn, would obstruct the normal running of current through the board thus making it inoperative. The causes of uneven deposition of copper may be pollution of the dielectric, air trapped during the etching process, or improper drilling. It is logical that in order to avoid these factors, the materials used in the deposition process must be thoroughly cleaned (decreased), and the correct direction and speed of drilling must be observed.\n6. Acid accumulation\nPCB design elements in which acid accumulates due to surface tension during the etching process are called acid traps. These hold the acid for a longer time than is acceptable, resulting in excessive etching, which inhibits the normal flow of current through connections. From a practical point of view, acid traps are formed when the PCB pattern itself includes sharp corners. Typically, this problem occurs due to erroneous code in the circuit design programs; however, some newcomers who manually develop PCB designs also encounter these issues.\n7. Electromagnetic Interference (EMI)\nElectromagnetic interference is often caused by improper PCB design. To reduce EMI on a PCB, it is necessary to group elements according to their functional purpose – for example, analog and digital blocks, power supply section, low-speed circuits, high-speed circuits, etc. In addition, it makes sense to reduce the number of square angles in the joints as well as to use a metal container and shielded cables to absorb interference.\nIn order to avoid supplying defective boards, we recommend conducting a DFM (Design for Manufacturability) check, which will allow the detection of design flaws up front. If you want to get a warranty for your PCB design quality, contact us. We carefully monitor all PCB parameters during design and production to fully comply with the requirements for their further operation (See this case as an example: In-building Radio Communications System with Cloud Service). As a result, you will receive a fully functional device that will work smoothly and will not cause the breakdown of expensive components.","What do you mean by digital electronics?\nDigital electronics is a field of electronics involving the study of digital signals and the engineering of devices that use or produce them. This is in contrast to analog electronics and analog signals. … Complex devices may have simple electronic representations of Boolean logic functions.\nWhat is the importance of digital electronics?\nDigital electronic circuits are main thing in digital electronics which is usually made from large assemblies of logic gates. The system which process discrete values is known as digital system. The significance of digital electronics is that are inherently more reliable than analog, in terms of information processing.\nWho is the father of digital electronics?\nProfessor Emeritus Claude E. Shannon\nHow does digital electronics work?\nDigital electronics are electric circuits that work on only two fixed values: “1” and “0”. They use a series of 1’s and 0’s to store and communicate information. They can also perform math using just 1’s and 0’s. This is called Boolean math or Boolean logic.\nWhat mean electronics?\nElectronics comprises the physics, engineering, technology and applications that deal with the emission, flow and control of electrons in vacuum and matter.\nWhat is the best example of digital system?\nThe digital computer, more commonly called the computer, is an example of a typical digital system. A computer manipulates information in digital, or more precisely, binary form.\nWhat are the advantages of digital electronics over analog electronics?\nAdvantages of Digital Communication\nDigital circuits are more reliable. Digital circuits are easy to design and cheaper than analog circuits. The hardware implementation in digital circuits, is more flexible than analog.\nWhere are digital circuits used?\nDigital circuits are the most common mechanical representation of Boolean algebra and are the basis of all digital computers. They can also be used to process digital information without being connected up as a computer. Such circuits are referred to as “random logic”.\nWhat are the advantages of digital system?\nAdvantages of Digital Systems\n- Ease of programmability. The digital systems can be used for different applications by simply changing the program without additional changes in hardware.\n- Reduction in cost of hardware. …\n- High speed. …\n- High Reliability. …\n- Design is easy. …\n- Result can be reproduced easily.\nWhat was the first electronics?\nThe first electronic device ever invented is the relay, a remote switch controlled by electricity that was invented in 1835 by Joseph Henry, an American scientist, although it is also claimed that the English inventor Edward Davy “certainly invented the electric relay” in his electric telegraph c. 1835.\nWhat is the history of electronics?\nElectronics’ actual history began with the invention of vacuum diode by J.A. Fleming, in 1897; and, after that, a vacuum triode was implemented by Lee De Forest to amplify electrical signals. This led to the introduction of tetrode and pentode tubes that dominated the world until the World War II.\nWhat are examples of electronics?\nElectronic equipment, systems, etc. Electronics is defined as devices run by electric power or the field of studying such items. An example of electronics are radios, computers, and televisions.\nWhat comes under digital electronics?\n- Number System and Representation :\n- Programs :\n- Boolean Algebra and Logic Gates :\n- Gate Level Minimization :\n- Combinational Logic Circuits :\n- Flip-Flops and Sequential Circuits :\n- Register and Counters :\n- Memory and Programmable Logic :\nWhat is the difference between analogue and digital electronics?\nAnalog Circuits and Digital Circuits is a classic way of differentiating between two types of electronic circuits based on the signals they process. To put it in simple words, Analog Circuits deals with continuous analog signals whereas Digital Circuits deals with discrete digital signals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:79d16a5d-67c2-4641-a486-232dae735245>","<urn:uuid:2ef0d142-6efc-4541-a88c-a4f4ca868f7d>"],"error":null}
{"question":"What is the primary function comparison between the circulatory system and lymphatic system - fluid transport or immunity?","answer":"Both systems have distinct primary functions. The circulatory system's main function is transporting materials throughout the body, delivering nutrients, water, and oxygen to billions of body cells and carrying away wastes like carbon dioxide. The lymphatic system, on the other hand, has two key functions: maintaining fluid balance in the internal environment and promoting the body's immunity.","context":["Human circulatory system\nThe circulatory system is one of the most important systems in the body. Made up of the heart, blood and blood vessels, the circulatory system is your body's delivery system. Your heart plays and important part in being healthy. It keeps all the blood in your circulatory system flowing. Blood helps oxygen get around your body. When you exercise you can feel your pulse, it tells you how fast your heart is pumping.\nThe body's circulatory system is responsible for transporting materials throughout the entire body. It delivers nutrients, water, and oxygen to your billions of body cells and carries away wastes such as carbon dioxide that body cells produce. It is an amazing highway that travels through your entire body connecting all your body cells.\nAt the centre of this system is the heart, an amazing organ. The heart beats about 3 billion times during an average lifetime. It is a muscle about the size of the fist. The heart is located in the centre of the chest slightly to the left. Its job is to pump blood and keep the blood moving throughout the body. The blood is pumped around a complex network of blood vessels extending to every part of the body.\nBlood carries the oxygen and nutrients needed to fuel the activities of the body’s tissues and organs, and it plays a vital role in removing the body’s waste products. An average-sized adult carries about 5 litres (9 pints) of blood.\nTop 10 facts\n- If you were to lay out all of the arteries, capillaries and veins in one adult, end-to-end, they would stretch about 60,000 miles (100,000 kilometres).\n- It takes 20 seconds for blood to circulate the entire body. Oxygenated blood leaves the aorta at about 1 mile an hour.\n- The power output of the heart ranges from 1-5 watts per minute, which is the equivalent to the usage of a 60 watt bulb. It has been said that enough energy is produced by the human body in a day to drive a truck 20 miles.\n- Red blood cells live for up to 4 months and make approximately 250,000 round trips around the body before returning to the bone marrow, where they were born, to die.\n- Between 2.5 and 3 million red blood cells (called erythrocytes) are lost and replaced every second.\n- Across the animal kingdom, heart rate is related to body size: in general, the bigger the animal, the slower its resting heart rate. An adult human has an average resting heart rate of about 75 beats per minute, the same rate as an adult sheep. But a blue whale's heart is about the size of a small car, and only beats five times per minute. A shrew, on the other hand, has a heart rate of about 1,000 beats per minute.\n- The ancient Egyptians believed the heart, rather than the brain, was the source of emotions, wisdom and memory, among other things.\n- After circulating within the body for about 120 days, a red blood cell will die from aging or damage. Bone marrow constantly manufactures new red blood cells to replace those that perish.\n- The oxygen-rich blood that flows through your arteries and capillaries is bright red. After giving up its oxygen to your bodily tissues, your blood becomes dark red as it races back to your heart through your veins.\n- “Ventricle” means “little belly\".\nDid you know?\n- We see and hear about hearts everywhere. A long time ago, people even thought that their emotions came from their hearts, maybe because the heart beats faster when a person is scared or excited. Now we know that emotions come from the brain, and in this case, the brain tells the heart to speed up.\n- Your heart is a very strong muscle that pumps blood around your body. It is made of four chambers, two upper chambers and two lower chambers. Blood enters the upper chambers. These squeeze and push the blood into the lower chambers, which then squeeze and push the blood out of your heart.\n- The heart works tirelessly – more than 2.5 billion times over an average lifetime – to pump blood around the body. The heart’s contractions or ‘squeezes’ are triggered by electrical impulses that come from a specialised area of heart tissue.\n- Your pulse is a measure of how fast your heart is beating. It is the number of beats your heart makes in one minute. Your heart beats faster or slower depending on what you are doing. You can feel your pulse at certain points on your body. The easiest place to feel it is in your wrist, using the first two fingers of your other hand. When you sit, the average heart beats about 80 times per minute. However, everybody is different, so your pulse could be higher or lower than this.\n- When you exercise, your heart beats more quickly. This is because your muscles are working harder and need more oxygen to keep going. Your lungs also work harder, making you breathe more quickly to get more oxygen. When you sleep, your muscles need less oxygen, so your heart slows down.\nLook through the gallery and see if you can spot the following:\n- Magnified red blood cells\n- A diagram of the human heart\n- Blood vessels in the body\n- A blood pressure check\n- Doctors use stethoscopes to listen to heart and lung sounds\n- A diagram of the circulatory system\nThe circulatory system is centred on the heart, an amazing organ that constantly works to pump blood around blood vessels in every part of the body. Blood carries all the things, like oxygen, that cells need to thrive and keep us healthy.\nThe heart is made up of four different blood-filled areas, and each of these areas is called a chamber. There are two chambers on each side of the heart. One chamber is on the top and one chamber is on the bottom.\nThe two chambers on top are called the atria. (If you're talking only about one, call it an atrium.) The atria are the chambers that fill with the blood returning to the heart from the body and lungs. The heart has a left atrium and a right atrium.\nThe two chambers on the bottom are called the ventricles. The heart has a left ventricle and a right ventricle. Their job is to squirt out the blood to the body and lungs. Running down the middle of the heart is a thick wall of muscle called the septum. The septum's job is to separate the left side and the right side of the heart.\nThe atria and ventricles work as a team — the atria fill with blood, then push it into the ventricles. The ventricles then squeeze, pumping blood out of the heart. While the ventricles are squeezing, the atria refill and get ready for the next contraction.\nThe blood moves through many tubes called arteries and veins, which together are called blood vessels. These blood vessels are attached to the heart. The blood vessels that carry blood away from the heart are called arteries. The ones that carry blood back to the heart are called veins.\nThe human body needs a steady supply of blood to keep it working right. Blood delivers oxygen to all the body's cells. To stay alive, a person needs healthy, living cells. Without oxygen, these cells would die. If that oxygen-rich blood doesn't circulate as it should, a person could die.\nRemember that your heart is a muscle. In order for it to be strong, you need to exercise it. How do you do it? By being active in a way that gets you slightly out of breath, like skipping, dancing, or playing tennis or football. Try to be active every day for at least 30 minutes.\nWords to know:\nAorta - the main artery in mammals that carries blood from the left ventricle of the heart to all the branch arteries in the body except those in the lungs.\nArteries - a blood vessel that is part of the system carrying blood under pressure from the heart to the rest of the body.\nAtrium - one of the upper chambers of the heart that takes blood from the veins and pumps it into a ventricle.\nCapillaries - an extremely narrow thin-walled blood vessel that connects small arteries arterioles with small veins to form a network throughout the body.\nCarbon dioxide - a heavy, colourless, odourless gas.\nCells - the cell is the basic unit of life. Some organisms are made up of a single cell, like bacteria, while others are made up of trillions of cells. Human beings are made up of cells, too.\nCirculatory - relating to the circulation of the blood.\nComplex - made up of many interrelated parts.\nContractions - a tightening or narrowing of a muscle, organ, or other body part.\nNutrients - a substance that provides nourishment.\nOrgan - a complete and independent part of a plant or animal that has a specific function.\nOxygen - a colourless, odourless gas that is essential for plant and animal respiration.\nPerish - to come to an end or cease to exist.\nPulse - the regular expansion and contraction of an artery, caused by the heart pumping blood through the body.\nTransporting - to carry somebody or something from one place to another.\nVeins - a blood vessel that carries blood to the heart.\nWaste - unwanted or unusable remains, or by-products.\nJust for fun...\n- Play a BBC Bitesize Circulation game\n- Some circulatory games and puzzles\n- Complete a heart and circulatory system quiz\n- Download free circulatory system quizzes, puzzles and memory games from Curiscope\n- A heart to print out and label\n- Guide a red blood cell on its journey through the body\n- Play The Pulse game and find out how fit you are\n- A heart diagram to print out and colour in\n- Enter a virtual operating theatre and carry out a heart transplant\n- Complete a heart disease and circulation activity to understand more about the heart's blood supply\n- Try the heartbeat calculator\n- Play the Blood Typing game and find out more about blood types\n- Design a clinical trial to investigate the causes of heart disease in this Centre of the Cell interactive game\nBest kids' books about the circulatory system and the heart\nFind out more\n- A kids' guide to the heart and blood\n- Look at images of blood cells under the microscope\n- Heart and Circulatory system quiz\n- Circulatory facts for older children\n- Further explanation about the circulatory system\n- The role of blood in the circulatory system\n- What effect does activity have on the heart? Find out with an online experiment\n- Learn how to measure your pulse\n- Watch a medical video of how the chambers of the heart contract and relax to push blood through the circulatory system\n- See and hear hearts pumping\n- The difference between arteries and veins","What are the two most important functions of the lymphatic system?\n-Maintain fluid balance in the internal environment\n-Promote body's immunity\nWhat act as \"drains\" to collect excess tissue fluid and return it to the venous blood just before it returns to the heart?\nWhat is the specialized component of the circulatory system; made up of lymph, lymphatic vessels, and isolated structures containing lymphoid tissue: lymph nodes, aggregated lymphoid nodules (e.g., Peyer's patches), tonsils, thymus, spleen, and bone marrow?\nWhat transports tissue fluid, proteins, fats, and other substances to the general circulation?\nWhat begin blindly in the intercellular spaces of the soft tissues; and do not form a closed circuit?\nWhat is the complex, organized fluid that fills the spaces between the cells and is part of the ECM and resembles blood plasma in composition with a lower percentage of protein?\nInterstitial fluid (IF)\nWhat closely resembles blood plasma in composition but has a lower percentage of protein; and is isotonic?\nLymph (lymphatic fluid)\nWhat are the microscopic blind-end vessels where lymphatic vessels originate; their wall consists of a single layer of flattened endothelial cells; networks branch and anastomose freely?\nLymph from right upper quadrant empties into what?\nRight lymphatic duct and then into right subclavian vein\nLymph from rest of the body (minus upper right quad) empties into what?\nThoracic duct, which then drains into left subclavian vein;\nAs the diameter of lymphatic vessels increases from capillary size what happens to the walls of the vessel?\nThe walls become thicker and have three layers\nWhat are present every few millimeters in large lymphatics and even more frequently in smaller lymphatics?\nWhat are the functions of the lymphatic vessels?\n-Remove high molecular weight substances and even particulate matter from interstitial spaces\n-Lacteals absorb fats and other nutrients from the small intestine\nWhat can happen if anything blocks lymphatic return?\n-Blood protein concentration, and blood osmotic pressure drops\nFrom lymphatic capillaries, lymph flows through progressively larger lymphatic vessels to eventually reenter blood at the junction of what?\nLymph moves through the system in the right direction as a result of what?\nThe large number of valves\nWhat establishes a fluid pressure gradient in the lymphatic system?\n-Skeletal muscle contractions\nWhat are oval structures enclosed by a fibrous capsule and are a type of biological filter?\nWhat are lined with specialized reticuloendothelial cells capable of phagocytosis?\nCortical and medullary sinuses of a lymph node\nWhat are the location of groups with greatest clinical importance?\n-Submental and submaxillary group\n-Inguinal lymph nodes\nWhat nodes are located in front of the ear and drain superficial tissues and skin on the lateral side of the head and face?\nPreauricular lymph nodes\nWhat type of filtration physically stops particles from progressing further in the body?\nWhat type of filtration does the biological activity of cells destroy and removes particles?\nDistribution of lymphatics in the breast\nare drained by what two sets of lymphatic vessels?\n-Lymphatics that drain skin over the breast with the exception of areola and nipple\n-Lymphatics that drain substance of breast itself, as well as skin of areola and nipple\nWhat are located under areola surrounding nipple; where communication between cutaneous plexus and large lymphatics that drain the secretory tissue and ducts of the breast occurs?\nWhat tonsil is located near posterior opening of nasal cavity and is known as \"adenoids\" when swollen?\nWhat protect against bacteria that may invade tissues around the openings between the nasal and oral cavities?\nWhat is the single, unpaired organ located in the mediastinum, extending upward to lower edge of thyroid and inferiorly as far as fourth costal cartilage?\nWhat is the structure of the thymus?\n-Pyramid-shaped lobes are subdivided into small lobules\n-Each lobule is composed of a dense cellular cortex and an inner, less dense, medulla\n-Medullary tissue can be identified by presence of thymic corpuscles\nShortly after birth, what does the Thymus secrete?\nThymosin and other regulators, which enables lymphocytes to develop into T cells\nWhat is the location of the spleen?\n-In left Hypochondrium,\n-directly below diaphragm, above left kidney and descending colon, and behind fundus of stomach\nWhat is found near outer regions of the spleen, which are made up of a network of fine reticular fibers submerged in blood that comes from nearby arterioles?\nWhat are the functions of the spleen?\n-Red blood cell and platelet destruction\nLymphatic system benefits the whole body by acomplising what?\n-Maintaining fluid balance\n-Promoting freedom from disease\nWhat is the elevated protein concentration in the thoracic duct due to?\nProtein-rich lymph coming from the liver and the small intestine\nHow much total blood proteins leak out of the capillaries and into the tissue fluid and is returned to the blood via the lymphatic vessels?\nWhat serve as the first line of defense from the exterior and as such are subject to chronic infection?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2c131f4c-0eb8-43c0-99d9-353df513e966>","<urn:uuid:14c2a0ca-98d1-4794-90e9-b29256dfec2a>"],"error":null}
{"question":"What is similar about the recommended selling approaches for pianos and cars in terms of dealing with potential buyers?","answer":"Both piano and car sales processes emphasize the importance of being well-informed and transparent with potential buyers. For pianos, sellers should prepare detailed documentation about the instrument's condition and maintenance history to answer buyer questions effectively. Similarly, for cars, sellers must provide relevant information including make, model, year, mileage, selling points, and mechanical issues. Both processes involve direct interaction with potential buyers - though piano sales focus on finding a 'fitting home' for the instrument, while car sales require additional steps like checking buyer's driver's licenses for test drives and formal contract creation.","context":["The only way to make sense out of change is to plunge into it, move with it, and join the dance.Alan Watts, from The Wisdom of Insecurity [ 2 ]\nParting with any of your possessions can be difficult, as nostalgia seems to creep into every box, memento or broken chair. That piano, on which you first learned Für Elise, or watched your children pounding out chopsticks, taps directly into your sense memory. These charged emotions are not to be trusted when selling your piano.\nThat’s where this guide comes in, to help navigate through the logistics during this seemingly difficult task; and avoid rash, uninformed decisions. You want to approach selling your piano with as much zeal as you did in buying it—so be prepared.\nThe first step is to get your piano tuned professionally and make sure there aren’t any necessary repairs. Buyers may be turned off if the piano is too out of tune. And showing your instrument in the best condition is an important selling tool. Keep the receipt, and while you’re at it, dig up any old receipts; an often-overlooked step that authenticates the care you’ve taken to preserve your piano (maintenance & cleaning).\nThe best way to inform is to be informed. Prepare as much documentation about your piano to ensure you can communicate its value effectively and answer any questions the buyer may pose. This will also help you succeed in getting an appropriate price for your piano.\nThe following is a simple list of things to prepare in advance of selling:\n1. General information: year, make, model, size, color.\n2. Describe current condition: surface/finish, piano legs, keyboard, pedals, strings, hammers & soundboard.\n3. List extra features.\n4. Reasons for selling.\n5. Locate serial number.\n6. Last date tuned and serviced.\n7. Any repairs needed?\nThe next step is to figure out what your piano is worth. The Blue Book of Pianos is a free online resource that can help you get started. Or you can hire a piano technician to provide an evaluation. There is a fee associated with this path that could be up to $250. But it may be worth it to properly ensure you are getting the most you can.\nNow that you are fully armed with information, it’s time to decide how you will sell it. Things to consider: Do you want to place an ad in the paper, online or both and sell it yourself? You will probably get more money if you sold privately and can choose a fitting home for your beloved instrument. However, it may take a while. This involves a level of patience and negotiating for which you may not have the time or energy. Besides, you will have to contend with strangers in your private space.\nAn alternate possibility is to sell it through a dealer or merchant such as a piano store. As an incentive, they will offer a free valuation. It may be as simple as filling out a form and sending pictures, along with a serial number. Unlike a private sale, you won’t have to sell it yourself; you’ll receive immediate payment, and avoid strangers in your home. However, you may not get as much for it as you would on your own. But remember you have done all your research so you know what your piano is worth and can negotiate smartly.\nSo that’s all there is to selling your piano. Still feelin’ kinda blue? Perhaps knowing that you have informed yourself and the buyer as best you can, will help you say goodbye to an old friend.","After all your intense negotiating when selling your car, it’s time to draw up the papers and sign on the dotted line. Every state requires a sales contract between seller and buyer when selling a vehicle, according to the Cars Direct website. The point of the contract is to ensure that both you and the buyer agree on all terms and everyone knows the deal.\nNegotiate the Deal\nAdvertise your car for sale to attract potential buyers. Place a listing online or in your local newspaper. Or go the old-school route and park your car on a busy street with a “for sale” sign in the window. List all relevant information, including make, model, year, mileage, selling points and possible mechanical issues, for buyers to read. Include your asking price, too.\nShow the car to potential buyers. Answer questions and allow people to examine the automobile. If buyers want to drive the car, check for a valid driver’s license and then ride along to make sure they drive carefully.\nNegotiate with an interested buyer to agree on a purchase price. You may need to reduce your asking price slightly during negotiation, depending on the buyer’s position. When you come to an agreement on price, shake hands on the deal.\nCreate the Contract\nDraw up a car sales contract to put the details of the deal in writing. Generally, a car sales contract identifies the seller – you – by name and then lists your address, county and state. Next, write out the agreed price in words and figures, state that you have received this payment, and write the current date. State that you are conveying or selling the car to the buyer. Include the buyer’s full name, and list the address, county and state of the buyer.\nAdd important details to the car sales contract. Include the make, model, year, color, body style, registering state, the Vehicle Identification Number and any accessories you are including with the car. Write the exact mileage on the car sales contract and state that the odometer is accurate to the best of your knowledge, if applicable. If you know that there is an odometer discrepancy, you must state this in the sales contract.\nList all the representations by you, as the seller, about the car. For example, include that you are the legal owner, that you have the right to transfer the title (no liens exist) and that all information provided by you about the car is accurate. Include an “as-is” clause to make sure that the buyer knows you are selling the car “as is” (without a warranty).\nCreate signature lines for the seller and buyer to sign the contract. Create a line beneath the signature lines for printed names. Add lines for the dates as well.\nMake two copies of the sales contract – one for you and one for the buyer.\nFind out if you need to have your car sales contract notarized. If you live in West Virginia, New Hampshire, Louisiana, Maryland or Nebraska, you must have your contract notarized, according to the Cars Direct website. In this case, don’t sign the contract until you’re in the presence of a notary public. If you live in one of the 45 other states, notarization is optional. You can just sign and date both copies of the contract to seal the deal.\n- If you don’t know what to ask for your car, visit the Kelley Blue Book website or the NADA Guides website to find out. Enter the make, model, year, condition and features of your car into the value tool, and you will get a value estimation of your car.\n- Creatas Images/Creatas/Getty Images\n- What Is the Difference Between Under Contract & Sold in Houses?\n- How to Return a New Financed Car\n- Can a House Under Contract Be Sold?\n- How to Sell a Car to a Private Party Through an Installment Plan\n- How to Buy a New Car & Not Get Screwed\n- How to Sell a Nonoperating Vehicle\n- How to Buy a For Sale by Owner Car\n- Documents Needed to Sell Your Own Home Without an Agent"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0cb315ca-8dd5-4c39-8d6c-af35a12e8d14>","<urn:uuid:055c37e5-7530-4dbb-968b-0694bd17cd1b>"],"error":null}
{"question":"Could you explain how synchronization dependencies work in persistent representations, and what role do different consistency patterns play in managing these dependencies across distributed components?","answer":"Synchronization dependencies in persistent representations operate through symbolic synchronization, which provides logical descriptions of dynamic dependencies between persistency units using logical clocks, and actual synchronization which deals with persistent representation of objects controlled by physical clocks. These dependencies can be expressed as rules. In distributed systems, three consistency patterns manage these dependencies: strong consistency ensures immediate synchronous updates, weak consistency allows updates without immediate propagation, and eventual consistency provides asynchronous replication where updates eventually propagate to all locations.","context":["To be of any use, context representations must be split into individual units upholding their identity whatever may happen. Such consistency is necessary to support the continuity of corporate identities.\nHence the distinction between transactional and non-transactional (aka master) data entities.\nEvery object whose states have to be managed independently of activities must be consistently identified across systems. Those identities may be native (live beings) or built (devices), proper or derived, redundant (singleton) or irrelevant (copies).\nIn order to keep track of changes, representations mapped to individuals must be consistently identified by the business processes targeting them. That holds true for all symbolic objects, whether they represent individuals, physical or cultural, and whatever the coordinate system (address space), real or mythological, used to chart them.\nA clear distinction must be made between identity and identifier. The former is an underlying constituent of business objects or processes, while the latter is only a symbolic setup used to fetch representations. As a corollary, identifiers are groundless until identities are founded. Looking for identifying attributes in order to establish object identities is like looking for stripes to identify horses.\nTherefore, and before looking for identifying features, the point to consider is how identification mechanisms can affect the coupling between contexts and system, and consequently the architectures.\nSome objects may come out as native individuals with built-in identities, so long as they can be sorted out from business concerns; others will directly begin as business constructs.\nObjects with inbred identities are necessarily physical yet the opposite is not true since the identity of physical objects can be bound to other ones. Moreover, to be worthy of business consideration, they must be involved in some activities whose proper processing call for their state to be consistently and persistently recorded.\nAnd since innate identities are set independently of business avatars, roles of objects with inbred identities, active or passive, must be documented as detached aspects.\nIt must be noted that whereas objects with business-based identities are necessarily documental, the opposite is not true: business aspects of objects with built-in physical identities are yet documental. As a corollary, objects with unique inbred identities may use alternative business identifiers as they appear in different business domains.\nWhereas business-identified objects could theoretically be reduced to a single aspect, that would induce a frozen description of business concerns. Hence, if business objects have to support changing opportunities, identities and aspects must be documented apart from the outset.\nIf models are to support architectural concerns, principles must be set regarding the definition of persistency units:\n- Single semantics: parts cannot be used without the root being in the know; otherwise individual changes in semantics or business rules will necessitate agreement between different organizational units. As a corollary parts and root must belong to the same domain.\n- Single location: parts must be located where the root is, lest individual accesses involve unexpected dependencies.\n- Single history: no event can affect a part without the knowledge of the root lest individual accesses by business processes be dependent on control rules set by different operational units.\nEquivalent principles are used for execution units.\nPersistency units are not limited to the representation of objects and may also record events and states of objects or processes. Some are represented individually, some as dependents, but the rationale behind the choice of representation must be straightforward: the symbolic representation of events and states cannot be modified. For instance:\n- A customer (active physical object) gives his chariot (passive physical object) for repair (a documental object, open to amendments, is created and an actual process is initiated).\n- Along the process a discussion occurs (event), with reference to the expected state (snapshots figured in grey) as documented with the repair agreement.\n- Symbolic synchronization provides a logical description of dynamic dependencies between persistency units independently of actual events, i.e as if controlled by logical clocks. Local synchronization is to be described by partitions of objects states, otherwise synchronization is to be applied to roles using time-dependent relationships.\n- Actual synchronization deals with the persistent representation of objects whose states depends of actual events as generated by contexts, i.e is controlled by physical clocks. Events can remain implicit if synchronization is local, otherwise explicit descriptions are to be consolidated across domains.\nThe synchronization dependencies between persistent representations can also be expressed as rules.","Everything you need to know about Week, Strong and Eventual Consistency\nBefore we talk about the Consistency Patterns, we should know what a distributed system is. Simply put, a distributed system is a system that consists of more than one components, and each component is responsible for one part of the application.\nA distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal. - Wikipedia\nImagine we have an e-commerce application where we are selling books. This application may consist of multiple different components. For example, one server might be responsible for the accounts, another might be responsible for the payments, one might be responsible for storing orders, one might be responsible for loyalty points and relevant functionalities, and another might be responsible for maintaining the books inventory and so on.\nNow, if a user buys a book, there might be different services involved in placing the order; order service for storing the order, payment service for handling the payments, and inventory service for keeping the stock of that ordered book up to date. This is an example of a distributed system, an application that consists of multiple different components, each of which is responsible for a different part of the application.\nWhen working with distributed systems, we need to think about managing the data across different servers. If we take the above example of the e-commerce application, we can see that the inventory service must have up-to-date stock information for the ordered items if the user places an order. Now, there might be two different users looking at the same book. Now imagine if one of the customers places a successful order, and before the inventory service can update the stock, the second customer also places the order for the same book. In that case, when the inventory wasn’t updated, we will have the wrong stock information when the second order was placed, i.e., the ordered book may or may not be available in stock. This is where different consistency patterns come into play. They help ensure that the data is consistent across the application.\nConsistency patterns refer to the ways in which data is stored and managed in a distributed system and how that data is made available to users and applications. There are three main types of consistency patterns:\nEach of these patterns has its own advantages and disadvantages, and the choice of which pattern to use will depend on the specific requirements of the application or system.\nAfter an update is made to the data, it will be immediately visible to any subsequent read operations. The data is replicated in a synchronous manner, ensuring that all copies of the data are updated at the same time.\nIn a strong consistency system, any updates to some data are immediately propagated to all locations. This ensures that all locations have the same version of the data, but it also means that the system is not highly available and has high latency.\nAn example of strong consistency is a financial system where users can transfer money between accounts. The system is designed for high data integrity, so the data is stored in a single location and updates to that data are immediately propagated to all other locations. This ensures that all users and applications are working with the same, accurate data. For instance, when a user initiates a transfer of funds from one account to another, the system immediately updates the balance of both accounts and all other system components are immediately aware of the change. This ensures that all users can see the updated balance of both accounts and prevents any discrepancies.\nAfter an update is made to the data, it is not guaranteed that any subsequent read operation will immediately reflect the changes made. The read may or may not see the recent write.\nIn a weakly consistent system, updates to the data may not be immediately propagated. This can lead to inconsistencies and conflicts between different versions of the data, but it also allows for high availability and low latency.\nAnother example of weak consistency is a gaming platform where users can play online multiplayer games. When a user plays a game, their actions are immediately visible to other players in the same data center, but if there was a lag or temporary connection loss, the actions may not be seen by some of the users and the game will continue. This can lead to inconsistencies between different versions of the game state, but it also allows for a high level of availability and low latency.\nEventual consistency is a form of Weak Consistency. After an update is made to the data, it will be eventually visible to any subsequent read operations. The data is replicated in an asynchronous manner, ensuring that all copies of the data are eventually updated.\nIn an eventually consistent system, data is typically stored in multiple locations, and updates to that data are eventually propagated to all locations. This means that the system is highly available and has low latency, but it also means that there may be inconsistencies and conflicts between different versions of the data.\nAn example of eventual consistency is a social media platform where users can post updates, comments, and messages. The platform is designed for high availability and low latency, so the data is stored in multiple data centers around the world. When a user posts an update, the update is immediately visible to other users in the same data center, but it may take some time for the update to propagate to other data centers. This means that some users may see the update while others may not, depending on which data center they are connected to. This can lead to inconsistencies between different versions of the data, but it also allows for a high level of availability and low latency.\nIn conclusion, consistency patterns play a crucial role in distributed systems, and the choice of which pattern to use will depend on the specific requirements of the application or system. Each pattern has its own advantages and disadvantages, and each is more suitable for different use cases. Weak consistency is suitable for systems that require high availability and low latency, strong consistency is suitable for systems that require high data integrity, and eventual consistency is suitable for systems that require both high availability and high data integrity.\nCommunity created roadmaps, articles, resources and journeys to help you choose your path and grow in your career."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bc59de3d-1612-44c8-8f95-6f3b5408c269>","<urn:uuid:cbd2ff61-c21a-4c8c-843e-26c19db78711>"],"error":null}
{"question":"Can you list the key considerations for device usage policies in businesses alongside the main approaches for reducing environmental impact in digital industries?","answer":"Key considerations for device usage policies include: 1) Types of information that can be accessed/stored, 2) Device ownership (company vs personal), 3) Required security measures, 4) Rules about device syncing with work computers, and 5) Policies on device sharing between employees. For reducing environmental impact in digital industries, the main approaches are: 1) Using renewable energy sources like solar and wind power, 2) Developing energy-efficient equipment, 3) Implementing proof-of-stake consensus mechanisms instead of proof-of-work, and 4) Carbon offsetting through investment in sustainable projects.","context":["Five Key Questions for Your Wireless Device Usage Policy\nProtect your small business from liability, security risk, and noncompliance by creating a few simple rules for employees and their smartphones\nTake a poll of your employees. How many of them carry a smartphone in their pockets? How many are using them—or want to use them—to read and send work emails, text with colleagues, and even access cloud-based business applications? Because so many people now use these remarkable handheld computers to get so much done, small companies are being forced to figure out how they fit into their networks. And that means developing a usage policy for wireless handheld devices that your employees use for work.\nThe very first element your policy should cover is whether or not you allow employees to connect to your business network with their personal devices, like smartphones and tablets. If you want to let them check their work email, use your cloud-based apps, and use your other productivity tools on their devices, then you’ll need to figure out the detailed specifics of what data and applications will be allowed on those devices—and how they can be used when connected and not connected to your network.\nA wireless device usage policy is similar to an acceptable use policy (AUP) for your network. This post can help you write an AUP for your small business.\nCrafting a workable wireless device policy\nYou must address a number of questions in a wireless handheld usage policy. For example, who can use their wireless devices on your network? What kind of information is allowed? How much control do you retain over personal devices? Answers to these questions must be covered in your policy to protect your company from potential liability, security risks, and becoming noncompliant with mandated data privacy requirements. Here are the critical questions to consider:\n1. What types of information can be accessed or stored on employees’ wireless devices? This boils down to business-critical and confidential information vs. non-critical or even public information. If you’ll allow users to access files that are sensitive, then you’ll have to implement stricter controls. Remember, so much important data is sent through email and stored in the cloud, it’s probably safest to expect that everyone using their smartphone will at some point access sensitive information.\n2. Who owns the wireless devices? If your company decides to provide users with smartphones, then you own them and can dictate their use. However, if you allow employees to use their own devices, you can exert less control over them. Still, you can stipulate certain conditions—namely, adhering to the rules you set in your wireless device policy—that users must follow to be allowed to access your network , applications, and data with their personal devices.\n3. What security measures must they take on their devices? This may be the most important condition you set. You can require people to set passcodes and actively run mobile security software, like anti-virus and self-destruct applications, on their smartphones and tablets. You can also ask that the devices be checked against these mandated security techniques before users are given the OK to connect to your network.\n4. Can employees sync their handhelds with their work computers? When you sync your smartphone to your computer, you can easily transfer private data from one device to the other. This can pose a security risk if someone steals sensitive data and then leaks or gives it away, say to a competitor. Also, this is a path for malware and viruses to come into the company.\n5. Can people share their devices? If some employees have different access rights to company data, you probably do not want them sharing their handhelds with each other. Any need-to-know classifications your company has given certain employees will help structure this part of your policy.\nEnforcing this policy\nWhen you write your policy, make the conditions and consequences specific, but don’t name particular devices. By keeping the policy generic to “wireless handheld devices,” for instance, you can be sure that your wireless device usage policy will apply to any new technologies that make their way into your office.\nWhat do you allow employees to access on your small business network with their smartphones and tablets?","As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b4b8084c-d177-4f39-8c00-e5f4c7131211>","<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>"],"error":null}
{"question":"Why do the control systems differ so drastically between basic toy drones like the Aura and professional systems like the Avartek Boxer? What makes their piloting approaches so different?","answer":"The Aura drone uses a simplified glove controller system designed for accessibility, where hand gestures control the drone's movement - tilting your hand up/down and left/right controls direction, and a thumb button controls altitude and flips. This is specifically designed to be easy to learn, allowing even 6-year-olds to fly within minutes. In contrast, the Avartek Boxer employs a professional-grade control system, using a Cube Orange flight controller and Futaba 14-channel remote controller, integrated with sophisticated features like a hybrid power management unit. This complex system is necessary to manage its advanced capabilities, including long-duration flights and professional payload operations.","context":["Many of us drool over powerful high-flying camera drones or superfast racing quads, but for adults or kids just starting out, a far cheaper toy drone is far more practical.\nMade to zip around your living room or backyard for a few minutes at a time, toy drones are a good way to practice your piloting skills without worrying about watching something $500 or much, much more hit the ground or disappear into the trees. They're also a lot of fun to fly -- whether or not you're into RC toys. And, because the tech inside is getting smaller and cheaper, you can find them with more advanced features like GPS for safer, easier piloting and cameras for first-person-view flying (FPV).\nThere are plenty to choose from, which is great, but it can also be a bit overwhelming if you've never shopped for a drone before. You canif you're not quite sure what you're looking for or familiarize yourself with some common terminology. Or, if you're just after some quick toy drone suggestions, just read on.\nKeep in mind when you're shopping that flight times on toy drones are typically around 5 to 8 minutes, so if you want to keep flying without charging first, go with a model that uses removable rechargeable batteries. Also, if this is your first toy drone, we recommend going with one that has replacement parts readily available.\nAura Drone with Glove Controller\nAbsolutely perfect for any age or skill level, the Aura uses hand gestures to control it. Strap the \"controller\" onto your hand and you tilt your hand up and down to fly it away from you and back again. Tilt to the left to go left and to the right to go right. Press and hold a button under your thumb and the same movements let you raise and lower it or flip it left and right. Level out your hand and it will hover in place. Also, with the props are completely protected, so it just bounces off walls or whatever if you drift off course.\nCurrently available for $80 (AU$200 or £100), flying the Aura is a bit like playing with a flying yo-yo. It's so easy to learn, too, that I had my 6-year-old flying it on his own in less than a minute. Which is good because the battery only lasts about 5 minutes, though it is removable and extras are inexpensive.\nRecommended for: If you want to fly a drone but get anxious at the thought of the sticks and buttons on a typical remote control.\nParrot Mambo FPV\nParrot's Mambo quadcopter has a set of pins on top that allow for attachments that include a small cannon, a grabber claw and now a 720p HD camera. The camera can record to a microSD card, but more importantly it lets you pilot by FPV or first-person view by streaming to your phone.\nParrot's minidrones are designed to make flight easy, especially when flying indoors. However, its autopilot technologies are not something you really want when racing. To that end, Parrot lets you change to a Drift mode that disables the drone's horizontal stabilization and a Racing mode that completely disconnects the autopilot for full manual flight. Plus, diving into the settings lets you adjust all of its directional speeds, so you can learn to FPV race at your own pace.\nThe $150 bundle (AU$230 or £125) includes the quad and attachable camera, Parrot's Flypad controller and Cockpitglasses 2 headset for use with a phone running the FreeFlight Mini app. Bonus: The minidrone supports Tynker and Swift Playgrounds coding platforms designed to teach STEM skills to kids.\nRecommended for: Taking the sting (and expense) out of learning to fly a racing drone.\nPowerUp FPV Paper Airplane\nPowerUp has made several app-controlled paper airplane systems like its newest, the Dart, a powered paper airplane that does flips and rolls. The PowerUp FPV is its first to add a camera for live-streaming video to your phone, which you can place in a VR headset like Google Cardboard. Not only to you get a pilot's view from the plane, but you can control it just by tilting your head.\nBecause it's a plane, it can turn on a dime or start flying sideways like a quadcopter, so you're going to want a lot of space to fly this. But if you've here's your chance. It's $140 for the kit (AU$170 or £100) which includes everything you need except a phone, but right now you can get it for $100.\nRecommended for: Anyone who ever dreamed of actually flying on a paper airplane or just wants something different that'll let you prove people wrong when they say, \"There's no way that thing'll stay in the air.\"\nAt around $40 (AU$50 or £35) the 5XC is one of the least expensive toy drones with a 720p camera. This thing feels pretty cheap and the camera is basically toy-quality, as you might expect for the price, but it flies surprisingly well and can take quite a lot of crashing. It won't hold its altitude on its own, but that makes it great for learning how to actually control the drone's throttle.\nBattery life comes in at about 7 to 10 minutes, but extra batteries as well as replacement parts are easy to come by, and the manual even gives you an assembly breakdown. It's available at Amazon.\nRecommend for: Learning how to pilot -- and repair -- a quadcopter. This is a simple, ready-to-fly toy with a camera that flies better than its price suggests.\nSky Viper V2450GPS\nOne of the technologies that makes pricier drones more stable and easier to fly is GPS. With it, a drone can use satellites to help it know where it is in space, so it can just stop and hover in place if you let go of the sticks. It can also allow the drone to return to where it took off from with the press of a button or if its battery starts to get too low. And that's exactly what you're getting with the Sky Viper V2450GPS.\nThe 720p HD camera drone sells for less than $150 ( AU$179 or £100) that can live stream to your phone, capture photos or video to a microSD card and gives you the stability and safety of GPS. Its battery will get you around 10 minutes in the air and extras are about $8 each.\nRecommended for: Beginners or more advanced pilots who want the stability and convenience of GPS in a quick and nimble quadcopter with a camera at a rock-bottom price.","Avartek Boxer – Advanced Hybrid Power Technology\nAvartek Boxer Hybrid is a gasoline powered unmanned aerial system (UAS, UAV, RPAS or drone). The main feature of Avartek Boxer is the hybrid system that provides all the flight energy by transforming gasoline into electric thru its onboard power generator for much longer flight times while maintaining the heavy payloads of large LiPo powered drones.\nInstead of carrying heavy batteries the Avartek Boxer Hybrid is designed around a liquid cooled mini generator power plant that transforms gasoline into electrical power. Currently the Boxer generator is capable of delivering 4 kW of energy which translates to 5 kg of real payload and 2 hour flight times. This quadruples the flight mission lengths and provides easy on field operations due to the lack of battery management and charging problems.\nThe Boxer design is fully custom. The airframe was designed by Avartek in Solidworks and all parts are manufactured in Finland. Only motors, escs, the flight controller and other electronics are out sourced.\nThe whole copter is designed around the hybrid generator’s capabilities of producing 10x more energy than similar weight in LiPo batteries. 4 kg of gasoline provides 2 hours of flight time at MTOW 25kg, while a typical LiPo battery in this size of a drone of 11kg would only provide 30 minutes. 4 kg of gasoline (5 litres) equals 40 kg of LiPo batteries. On field operations greatly benefit from the very quick duty cycle of hybrid drone by offering easy refills instead of on battery management and recharging arrangements.\nAvartek Boxer’s Pixhawk flight controller provides an easy to use system with unparalleled options for customisation both from flight controller, sensor and add on hardware as well from the software perspective. Pixhawk 2 is 100% open system and compliant with latest technologies.\nTypical Payload Options\nAvartek offers ready made payload setups. Safe, quick mounting and fully suspensed – ready to mount the sensor, IMU and auxilaries, such as onboard PC and telemetry/cloud datalinks. In Boxer typically auxilaries are mounted inside the airframe while the IMU is securely attached to the sensor.\nQuicklocks provide quick mounting points to the hardware rail underneat the airframe. This provides quick access to the payload and allows for quick payload swaps in the field. The long payload rails and heavy lift capability makes the Boxer an ultimate multi sensor UAV. Boxer features a large deck that can accommodate payload auxiliary sensors such as hyper spectral camera irradiance sensors. Payload GPS antenna can be attached to the antenna tower. Cabling can be pulled thru the body with customisable deck covers.\nOptional retracting landing gear provides unrestricted field of view for most payloads and the front mounted FPV camera safety and video recordings of flown missions.\nFlight Controller: Cube Orange\nRemote Controller: Futaba 14ch\nESCs: T-Motor Flame 60A\nMotors: Avartek Boxer\nPower System: Avartek\nBattery: Lipo 12S 5Ah\nHybrid Power Management Unit: 4 kW\nFuel tank size: 5 litre\nFlight duration: @25kg TOW approx. 120 minutes\nFrame: Carbon fiber designed and manufactured by Avartek"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:7f83c27c-de87-4556-8421-dd47e2b859c1>","<urn:uuid:a83f4190-c299-4ac8-b1cb-f4b5899ebf4a>"],"error":null}
{"question":"What factors determine membrane fluidity in cells, and how has this been demonstrated experimentally?","answer":"Membrane fluidity is determined by multiple factors: the length of fatty acid tails, temperature, cholesterol content, and the degree of fatty acid saturation. Longer fatty acid tails decrease fluidity, while higher temperatures increase it. Cholesterol helps moderate fluidity by restricting fatty acid tail motion. The ratio of saturated to unsaturated fatty acids is crucial - unsaturated fatty acids have 'kinks' that prevent tight packing and maintain fluidity at lower temperatures. These properties have been experimentally verified through various methods, particularly fluorescent recovery after photobleaching (FRAP) experiments, which have demonstrated and quantitatively measured the two-dimensional movement of membrane components. Additionally, membrane dynamics have been confirmed through cell fusion experiments using Sendai virus to create heterokaryons from human and mouse cells.","context":["Introduction to Lipids\nThe cell is composed of two distinctive environments: the hydrophilic aqueous cytoplasm and the hydrophobic lipid membranes. The lipid environment is defined by the family of molecules that are characterized by their hydrophobic nature and their common metabolic origin. Three members of the lipid family of molecules will be discussed in this course: fats (triacylglcerol), phospholipids, and steroids.\nThe Structure of Lipids\nLipid molecules are slightly soluble to insoluble in water. Lipids are hydrophobic because the molecules consist of long, 16-18 carbon, hydrocarbon backbones with only a small amount of oxygen containing groups. Lipids serve many functions in organisms. They are the major components of waxes, pigments, steroid hormones, and cell membranes. Fats, steroids, and phospholipids are very important to the functioning of membranes in cells and will be the focus of this tutorial.\nFats (triacylglycerols, triglycerides)\nFats are synthesized from two different classes of molecules: fatty acids attached to the alcohol glycerol. The fatty acids are generally, 16-22 carbons long, unbranched hydrocarbons that terminate with a single carboxyl functional group. The fatty acid can be of two types: saturated and unsaturated. Saturated fatty acids have no carbon-carbon double bonds (they are saturated with hydrogen) while the unsaturated fatty acids have one to three double bonds along the backbone carbon chain. These double bonds introduce \"kinks\" in the carbon chain which have important consequences on the fluid nature of lipid membranes. Unsaturated fatty acids have lower melting points than saturated fatty acids.\nTo construct a fat, or triacylglycerol, three fatty acid molecules are attached to the glycerol through an ester bond between the carboxyl group of the fatty acids and the three alcohol groups of a glycerol molecule. This is another example of a condensation reaction that results in formation of an ester in this case and the release of a water molecule. A fat molecule can be composed of one, two, or three different types of fatty acids each of which can be saturated or unsaturated.\nAn unsaturated fat has at least one unsaturated fatty acid whereas a saturated fat has none. Because the double bonds of the unsaturated fatty acids introduce kinks in the hydrocarbon backbone, unsaturated fats will not pack into a regular structure and thus remain fluid at lower temperatures. A saturated fat will pack well and be a solid a low temperatures.\nFats are mainly energy storage and insulating molecules. Per gram, fats contain twice as much energy as carbohydrates. Layers of fat also surround the vital organs of animals to cushion them, and layers of fat under the skin of animals provide insulation.\nPhospholipids contain only two fatty acids attached to a glycerol head. This occurs by a condensation reaction similar to the one discussed above. The third alcohol of the glycerol forms an ester bond through reaction with phosphoric acid. This is another example of a condensation reaction between an acid and an alcohol with the release of water. As a triprotic acid (i.e it has three acidic functions on the phorphorus atom) the phosphate group attached to the glycerol has the potential to form ester links with a variety of other molecules such as carbohydrates, choline, inositol and amino acids. The phosphate group along with the glycerol group make the head of the phospholipid hydrophilic, whereas the fatty acid tail is hydrophobic. Thus phospholipids are amphipathic:a molecule with a polar end and a hydrophobic end. When phospholipids are in an aqueous solution they will self assemble into micelles or bilayers, structures that exclude water molecules from the hydrophobic tails while keeping the hydrophilic head in contact with the aqueous solution. View the animation that demonstrates the formation of micelles and bilayers.\nPhospholipids serve a major function in the cells of all organisms: they form the phospholipid membranes that surround the cell and intracellular organelles such as the mitochondria. The cell membrane is a fluid, semi-permeable bilayer that separates the cell's contents from the environment, see animation below. The membrane is fluid at physiological temperatures and allows cells to change shape due to physical constraints or changing cellular volumes. The phospholipid membrane allows free diffusion of some small molecules such as oxygen, carbon dioxide, and small hydrocarbons, but not charged ions, polar molecules or other larger molecules such as glucose. This semi-permeable nature of the membrane allows the cell to maintain the composition of the cytoplasm independent of the external environment.\nThe steroids are a family of lipids based on a molecule with four fused carbon rings. This family includes many hormones and cholesterol. Cholesterol is a component of the cell membrane in animals and functions to moderate membrane fluidity because it restricts the motion of the fatty acid tails.\nReview of Lipids\nUse the following animation to review the discussion of lipids.\nThe Structures of the Cell Membrane\nFluid Quality of Membranes\nThe cell membrane must be a dynamic structure if the cell is to grow and respond to environmental changes. To keep the membrane fluid at physiological temperatures the cell alters the composition of the phospholipids. The right ratio of saturated to unsaturated fatty acids keeps the membrane fluid at any temperature conducive to life. For example winter wheat responds to decreasing temperatures by increasing the amount of unsaturated fatty acids in cell membranes. In animal cells cholesterol helps to prevent the packing of fatty acid tails and thus lowers the requirement of unsaturated fatty acids. This helps maintain the fluid nature of the cell membrane without it becoming too liquid at body temperature. The fluidity of the membrane is demonstrated in the following animation. The lipids in the membrane are in random bulk flow moving about 22 µm (micrometers) per second. Phospholipids freely move in the same layer of the membrane and rarely flip to the other layer. Flipping of phospholipids from one layer to the other rarely occurs because flipping requires the hydrophilic head to pass through the hydrophobic region of the bilayer.\nThe Mosaic Quality of Membranes\nBecause the cell membrane is only semipermeable, the cell needs a way to communicate with other cells and exchange nutrients with the extracellular space. These roles are primarily filled by proteins. Membrane proteins are classified into two major categories, integral proteins and peripheral proteins. Integral membrane proteins are those proteins that are embedded in the lipid bilayer and are generally characterized by their solubility in non-polar, hydrophobic solvents. Transmembrane proteins are examples of integral proteins with hydrophobic regions that completely span the hydrophobic interior of the membrane. The parts of the protein exposed to the interior and exterior of the cell are hydrophilic. Integral proteins can serve as pores that selectively allow ions or nutrients into the cell. They also transmit signals into and out of the cell. Unlike integral proteins that span the membrane, peripheral proteins reside on only one side of the membrane and are often attached to integral proteins. Some peripheral proteins serve as anchor points for the cytoskeleton or extracellular fibers. Proteins are much larger than lipids and move more slowly. Some move in seemingly directed manner while others drift.\nThe extracellular surface of the cell membrane is decorated with carbohydrate groups attached to lipids and proteins. Carbohydrates are added to lipids and proteins by a process called glycosylation, and are called glycolipids or glycoproteins. These short carbohydrates, or oligosaccharides, are usually chains of 15 or fewer sugar molecules. Oligosaccharides give a cell identity (i.e., distinguishing self from non-self) and are the distinguishing factor in human blood types and transplant rejection.\nMembranes are Asymmetric\nAs discussed above and seen in the picture, the cell membrane is asymmetric. The extracellular face of the membrane is in contact with the extracellular matrix. The extracellular side of the membrane contains oligosaccharides that distinguish the cell as self. It also contains the end of integral proteins that interact with signals from other cells and sense the extracellular environment. The inner membrane is in contact the contents of the cell. This side of the membrane anchors to the cytoskeleton and contains the end of integral proteins that relay signals received on the external side.\nSummary: Membranes as Mosaics of Structure and Function\nThe biological membrane is a collage of many different proteins embedded in the fluid matrix of the lipid bilayer. The lipid bilayer is the main fabric of the membrane, and its structure creates a semi-permeable membrane. The hydrophobic core impedes the diffusion of hydrophilic structures, such as ions and polar molecules but allows hydrophobic molecules, which can dissolve in the membrane, to cross it with ease. Proteins determine most of the membrane's specific functions. The plasma membrane and the membranes of the various organelles each have unique collections of proteins. For example, to date more than 50 kinds of proteins have been found in the plasma membrane of red blood cells.","What experiment supports the fluid mosaic model?\nAn important experiment that provided evidence supporting fluid and dynamic biological was performed by Frye and Edidin. They used Sendai virus to force human and mouse cells to fuse and form a heterokaryon.\nWhat experimental evidence supports the fluid mosaic model of biomembranes?\n18. What experimental evidence supports the fluid mosaic model of biomembranes? Ans: Results from fluorescent recovery after photobleaching (FRAP) experiments have demonstrated the two-dimensional movement of membrane components and allow quantitative measurement of the extent ofmembrane fluidity.\nWhat is the evidence for membrane fluidity?\nIf unsaturated fatty acids are compressed, the “kinks” in their tails push adjacent phospholipid molecules away, which helps maintain fluidity in the membrane. The ratio of saturated and unsaturated fatty acids determines the fluidity in the membrane at cold temperatures.\nWhy is the fluid mosaic model accepted?\nWe therefore don’t know for sure exactly what’s going on, however, the Fluid Mosaic Model is generally accepted as describing how membranes are arranged. … The Fluid Mosaic Model states that membranes are composed of a Phospholipid Bilayer with various protein molecules floating around within it.\nWhat is the fluid mosaic model of membranes?\nThe fluid mosaic model describes the cell membrane as a tapestry of several types of molecules (phospholipids, cholesterols, and proteins) that are constantly moving. This movement helps the cell membrane maintain its role as a barrier between the inside and outside of the cell environments.\nWhat is fluid mosaic model class 11?\nFluid mosaic model of cell membrane was proposed by Singer and Nicolson. According to Fluid mosaic model, the quasi-fluid nature of lipid enables lateral movement of proteins within the overall bilayer, and the ability to move within the membrane is measured as its fluidity.\nWhat would increase membrane fluidity?\nMembrane fluidity can be affected by a number of factors. One way to increase membrane fluidity is to heat up the membrane. Lipids acquire thermal energy when they are heated up; energetic lipids move around more, arranging and rearranging randomly, making the membrane more fluid.\nWhat happens to membrane permeability below 0?\nGenerally, increasing the temperature increases membrane permeability. At temperatures below 0 oC the phospholipids in the membrane don’t have much energy and so they can’t move much, which means that they’re closely packed together and the membrane is rigid.\nWhat factors increase membrane fluidity?\nNow, let’s take a look at the factors that influence membrane fluidity!\n- Factor #1: The length of the fatty acid tail. The length of the fatty acid tail impacts the fluidity of the membrane. …\n- Factor #2: Temperature. …\n- Factor #3: Cholesterol content of the bilayer. …\n- Factor #4: The degree of saturation of fatty acids tails.\nWhat makes the phospholipid bilayer fluid?\nCholesterol: The cholesterol molecules are randomly distributed across the phospholipid bilayer, helping the bilayer stay fluid in different environmental conditions. … Saturated and unsaturated fatty acids: Fatty acids are what make up the phospholipid tails.\nWho and how fluid mosaic model was confirmed?\nIn 1972 the Fluid—Mosaic Membrane Model of membrane structure was proposed based on thermodynamic principals of organization of membrane lipids and proteins and available evidence of asymmetry and lateral mobility within the membrane matrix [S. J. Singer and G. L. Nicolson, Science 175 (1972) 720–731]."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cff79b9a-4f78-4d27-a632-770f99f40692>","<urn:uuid:b4023228-7cfb-40e4-9104-f7669257499b>"],"error":null}
{"question":"How does the Arabic word 'rihlah' uniquely capture the relationship between travel writing and the journey itself, and how does this connect to historical Islamic traditions?","answer":"The Arabic word 'rihlah' uniquely captures this relationship by meaning both the book about a journey and the journey itself, treating them as an inseparable whole. This concept emerged from an Islamic tradition that began in the ninth century during the time of the Abbasid caliphs, following Mohammed's instruction to his followers to 'seek knowledge, even if you have to go to China.'","context":["St Augustine’s clichéd adage, “the world is a book, and those who do not travel read only a page,” has a close equivalent in China, where it is said that a journey of a thousand miles is more instructive than reading ten thousand scrolls. Both statements evoke the close equivalence between reading and travelling, between a book and a journey, but it is perhaps the Arabic word rihlah that sums up the connection best. A rihlah is a book about a journey as well as the journey itself, as if the two were a single, inseparable whole. When Ibn Batutta called his book Rihlah, he was continuing an Islamic tradition that took shape in the ninth century, in the time of the Abbasid caliphs, two hundred years after the prophet Mohammed instructed his followers to “seek knowledge, even if you have to go to China.”\nBatutta was a near contemporary of Marco Polo, and their travelogues as well as their journeys are similar in a variety of ways. In some cases, and within their respective cultures, Batutta and Polo brought home the first factual reports of mythical lands, and both men made passing on knowledge their top priority, even if they dabbled in myth-making on the side. Their books are travelogues, guides and histories rolled into one, without enough of any genre’s depths to satisfy readers of our time. They fail as travelogues because they are not intimate, as histories because they do not corroborate facts, and as guides because they are not systematic – because they are corrupted by the idiosyncrasies of singular expeditions. Both literature and travel have moved on dramatically since Batutta and Polo. Travel is easy and consumable en masse. Literature has turned inwards, to examine our personal lives. History has also changed; it has been shaped by the fragmentation and specialisation that penetrate every aspect of our lives, but travel writing is still a genre that allows writers to indulge in a mixture of reportage that others rarely allow, and the link between reading and travelling remains.\nThat link is curiosity, to want to see life through another person’s eyes. On their own, travelling or reading will only take you a part of the way, but together they can give you some sense of the whole. Graham Greene thought that the Indian novelist RK Narayan had offered him “a second home,” because without his writing, Greene “could never have known what it is like to be Indian.” He was grateful to Narayan – he “wakes in me a spring of gratitude,” wrote Greene – and I am similarly grateful to the writers listed below, because they have lent me their eyes and allowed me to see the places I pass through, however fleetingly, as home.\nTravel Books about India\nMaximum City: Bombay Lost and Found\nBorn in Gujarat, Suketu Mehta moved to Mumbai as a child. He moved again in his late teens, to the US, and it is his dislocation that Mehta grapples with in Maximum City, after he returns to his childhood home with his wife and children, to make a life. Mehta’s book is a portrait of the Mumbai tourists do not see, of its underbelly and its elite, groups that are not always easy to tell apart. Not satisfied by the journalist’s casual encounter, he builds close connections with his subjects – with gangland assassins, transvestite beer-bar dancers, slum dwellers, Bollywood stars, Bollywood hopefuls, Marathi nationalists and Jain diamond merchants who have relinquished everything to pursue spiritual purity. He allows himself to be sucked into their worlds, to which he becomes a superlative guide, able to immerse himself entirely – in a way no complete outsider ever could – and able to see out at the same time, so that he can translate his experiences for us, the reader without Indian – or Mumbaikar – eyes.\n|Maximum City||Amazon US||Amazon UK|\nCity of Djinns: A Year in Delhi\nWilliam Dalrymple started his travel writing career in 1989 with In Xanadu. He was a student, reading history at Cambridge’s Trinity College, but his book, which describes a journey in the footsteps of Marco Polo, is a classic: an addition to the canon of overland travelogues that showed-up Paul Theroux’s Great Railway Bazaar, when I read it, by being about a journey that was as hopelessly, hilariously difficult as it was frivolous. Dalrymple passed through seven countries – Israel, Cyprus, Turkey, Syria, Iran, Pakistan and China – on his way to the Kublai Khan’s capital Shangdu, and would return to some of them in his later writing, but it was India that swallowed him up, like it did Claire and I. After graduating, he set up a home in Delhi, where he has lived on and off ever since, and it is this transition that City of Djinns describes.\nIt is impossible to choose just one of William Dalrymple’s books about India, and I won’t try to here. He has written histories of Britain’s early adventures on the Subcontinent and his travel journalism about the region has been collected into two books. I have only read two of these – White Mughals and The Age of Kali – and the only reason that City of Djinns is in bold here, at the top, is because it ties his work as an historian and travel writer together, by being both a search for Delhi’s Mughal roots and a home within the Indian capital.\n|City of Djinns||Amazon US||Amazon UK|\nOther Books: Novels, Guides, Histories, Journalism, Polemics\nSide by Side:\nBeing Indian and In Spite of the Gods\nPavan K. Varma and Edward Luce\nWhen I taught English in Shanghai, I would occasionally describe an event that unfolded while I was in Delhi to my students. Reliance, a typically versatile Indian corporation, had recently opened a chain of green grocers that they called Reliance Fresh. The shops were orderly and air-conditioned, with fruit and vegetables clearly priced and separately shelved – a world apart, in other words, from the vendors who laid out their produce on canvas at markets or on the street. They were cheaper too, because Reliance could bring its full organisational depth to the problem of getting goods to market: refrigerated trucks were sent straight to farms, cutting out an assortment of middle men. The farmers got a better price and the quality of the produce was better too, because it didn’t have to be loaded and unloaded, in the sun, on its way from A to B to C to D. A win-win situation you might think, unless you were a street vendor or middle man, and the two groups made a prompt show of their anger. They went on strike and gathered outside branches of Reliance Fresh to hurl abuse – and sometimes stones – at its customers. Reliance was forced to shutter its shops and everybody had to go back to paying more for dirty, overripe fruit and vegetables. When the pandemonium died down, it reopened them, but the stores had been restocked and repositioned: they were now convenience stores, which would not compete directly with India’s existing agricultural supply chain.\nMy students were all a part of China’s upwardly mobile middle class, but many had simpler beginnings, in villages, and they had arrived at the symbol of their country’s progress – Shanghai, where the skyscrapers of Liujiazui twinkled outside my classroom window – through their own hard work and the sacrifices made by their parents. I told them the story because I thought it would help them understand what made China and India different. The middle men and market vendors would not have gotten away with their band in China, my students agreed, because the government wouldn’t let them. It’d call in the army if it had to, and progress would be made.\nWhat, you might wonder, does this have to do with Being Indian and In Spite of the Gods? Both books are full of examples like mine, because they point at the contradictions that make India so fascinating and frustrating. The books have similar subtitles – “The truth about why the 21st century will be India’s” and “The strange rise of modern India” respectively – but come at the problem from different perspectives. Varma is an Indian diplomat. His book is a polemic, written in the course of a posting to Cyprus. Luce was the South Asia bureau chief for the Financial Times. His book is a collection of journalistic essays, and it moves easily between travelogue and economic analysis.\nI read Being Indian during my first trip to the country, in 2006. I dog-eared it and scribbled in its margins because, like no other book I had found, it explained where India was going, which was at least as interesting as where it had been. I read In Spite of the Gods in Shanghai, in 2010, after it was recommended to me by the Daily Telegraph’s China bureau chief, who had just moved from Delhi to Beijing, and I carried it with me on my second trip to the country because it was an almost encyclopaedic catalogue of India’s potential as well as its problems. I will read them both again in the future, side by side, probably when I get back to the boxes of books I have sent home, because there is more in both of them than can be absorbed in a single reading, just as there is more – much more – in India than can be absorbed in a single visit.\n|Being Indian||Amazon US||Amazon UK|\n|In Spite of the Gods||Amazon US||Amazon UK|\nKaren Armstrong is a modern-day syncretist, fascinated by the threads that tie all religions together. She has written best-selling histories of Judaism, Christianity and Islam; in Buddha, she puts the life of Gautama Siddhartha in the context of the Axial Age, when the market economy of new towns was transforming the Old World. Socrates, Plato and Aristotle, Elijah, Isaiah and Jeremiah, Zoroaster, Confucius and Daoism’s mythical founding figure Laozi: all were products of the Axial Age, like the Buddha, who Armstrong thinks emerged out of a sea change in the relationship between individuals, society and nature.\nBuddha is a biography, in the Penguin Lives series, but it is not a life story of cross-referenced fact. Gautama Siddhartha probably did live – in the fifth or fourth century BCE – but was considered a legend until evidence of a historical Buddha was dug up during the British Raj, and the details of his life are either lost or obscured by myth. Armstrong reaches for the Axial Age as a result, to put the Buddha in historical context, and her portrait comes mostly from Buddhist scripture, with its mixture of allegorical and mundane events. The book is a useful tool anyway, because it is concise and well written. In India, it puts the emergence of Buddhism and the Upanishads side-by-side. In Southeast Asia, it helps makes sense of Buddhist symbolism, which has largely been drawn from the Buddha’s life.\n|Buddha||Amazon US||Amazon UK|"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:30a999ad-47c1-4dc8-9aed-d04581f28e86>"],"error":null}
{"question":"Compare aerial surveillance: Civil War balloons vs modern drones?","answer":"Civil War balloons required tethering near command posts or specialized vessels like the USS George Washington Parke Custis for deployment, and relied on telegraph communication to relay intelligence. They could reach heights of 3500 feet and provided basic visual reconnaissance and mapping capabilities. In contrast, modern UAVs offer significantly enhanced capabilities with high-definition cameras, GPS systems, and autonomous navigation. They can operate at various altitudes from one meter to 4-5 kilometers, provide real-time data transmission up to 10 kilometers away, and incorporate advanced technologies like thermal infrared sensors and spectrum analyzers. While Civil War balloons were primarily limited to military applications, modern drones serve diverse roles across industries including environmental monitoring, agricultural inspection, and disaster response.","context":["I started my Air Force career as an imagery intelligence analyst (also known as a “1N1X1” or a “squint”). It was my job to examine images taken by any of a plethora of “overhead” military platforms (U2 spy planes, as one example) and determine what was going on in them. American intelligence collection capability is truly amazing, and the origins of airborne intelligence collection were simultaneously humble and revolutionary.\nThe value of airborne intelligence gathering was pretty well evident in the earliest days of aviation. Only months following the first human’s ascension in a hot air balloon into the sky near Versailles in 1783, the French incorporated a balloon unit into its military. However, despite the notable contributions made during successive European battles, by 1802 the aérostiers were retired.[i] So by the time the US Civil War erupted in 1861, manned balloons had remained almost entirely outside of military operations for the better part of six decades.\nAt the outset of the conflict, experienced balloonists (called “aeronauts”) like Thaddeus Lowe, John La Mountain, and John Wise petitioned the US government for the opportunity to serve the country with the unique capabilities afforded by their aerial platforms. And while La Mountain was able to establish balloon operations in beleaguered Fort Monroe, Virginia, Lowe distinguished himself from his professional competitors by demonstrating the unique capabilities offered by aerial intelligence directly to President Lincoln. On 16 June, 1861, Lowe ascended in a balloon which was tethered near the White House. He then, using a telegraph machine in the basket, telegraphed what he saw directly to Lincoln’s office.\n“This point of observation commands an area near fifty miles in diameter. The city with its girdle of encampments presents a superb scene,” reported Lowe in what could be described, using today’s military concepts, as an exercise intelligence report.[ii] Lincoln was impressed, and appointed Lowe head of the Union’s new Balloon Corps.\nFrom the earliest days of the war through early 1863, the Balloon Corps demonstrated the unique capability afforded by aerial intelligence. An early example of this is actually on display at the National Museum of the United States Air Force, in Dayton, Ohio. Set up in the front of the Early Years Gallery is a hand-drawn map. In April 1861, Union forces at Fort Monroe in Virginia were isolated thanks to Virginia’s secession. John La Mountain managed to get his equipment and a single balloon into the old fort, which was bracing for a possible assault similar to Fort Sumter. Major General Benjamin Butler, in command at Monroe at the time, needed intelligence, and La Mountain was the man to get it for him. On 10 August, La Mountain ascended in his balloon to 3500 feet. From that vantage, he was able to identify troop camps and naval activity. He also provided General Butler with this map, possibly the first example of aerial intelligence mapping.[iii]\nThe use of these military balloons not only advanced military intelligence collection capabilities, but the Union army was forced to devise new technologies to deploy the assets in the field. Thaddeus Lowe created a mobile hydrogen gas generator, as well as directed the conversion of a Navy vessel into a specialized balloon deployment asset. This vessel, the USS George Washington Parke Custis – a coal barge- was fitted with the special hydrogen generator, and the deck was cleared to allow for balloon inflation. This gave the Union the ability to tow the balloon along the Potomac and adjacent waterways, expanding the range and flexibility of aerial intelligence collection.[iv]\nArguably then, the first military aviation platforms commissioned by the US Army were intelligence collection platforms. Yes, they were used for artillery spotting, but one of the primary drivers for President Lincoln to approve the creation of a Balloon Corps was the promise of real-time intelligence collection and transmission to commanders on the ground. It was not uncommon for an officer (at times, the commander) to ascend with Lowe to get a sense of the land and enemy disposition. The Confederates were vexed by the balloons, and tried to destroy them whenever they were observed rising.\nPhotographs would not be used with balloons, although some experiments of aerial photography (using kites and balloons) had been conducted by civilians around this time. However, the methodical use of professional intelligence gathering by specially trained aeronauts during the US Civil War is clear milestone (if not the first milestone) in the evolution of American aerial intelligence capabilities. A whole new dimension of warfare was emerging.\n[i] I’m not going to pretend that my research into French ballooning goes beyond the reading of a few secondary sources at this point. Charles M. Evans, in War of the Aeronauts, gives a brief overview of the earliest days of ballooning as he lays the ground work for his in-depth telling the use of balloons in the US Civil War. But I found a fascinating and concise article on the subject in All the Year Round, a British periodical and literary journal edited by none other than Charles Dickens. All the Year Round, Volume 1; Volume 21 (27 Feb, 1869) pp297-299.\n[ii] Lincoln, Abraham. Abraham Lincoln papers: Series 1. General Correspondence. 1833 to 1916: Thaddeus S. C. Lowe to Abraham Lincoln, Sunday,Telegram from balloon. 1861. Manuscript/Mixed Material. https://www.loc.gov/item/mal1031300/.\n[iii] Evans, Charles M, War of the Aeronauts, a History of Ballooning (Stackpole Books, Mechanichsville, PA, 2002), 96-98.\n[iv] U.S. AIRCRAFT CARRIERS: THE FORERUNNERS, NavSource Online Naval History (https://www.navsource.org/archives/02/forerunners/cv-forerunners.htm), Last update: 17 September 2006, accessed 23 Sep 2018.","UAVs are widely used in industries such as police, urban management, agriculture, geology, meteorology, electricity, emergency rescue and disaster relief, and video shooting.\n1. Application field: power inspection\nWorking principle: UAVs equipped with high-definition digital cameras and cameras and GPS positioning systems can locate autonomous cruises along the power grid, transmit and shoot images in real time, and monitor personnel can watch and control them synchronously on the computer. Recommended reason: The traditional manual power line inspection method is used, which is difficult and inefficient. The front-line power inspectors occasionally encounter the danger of being \"pushed by dogs\" and \"bitten by snakes\". The drone has realized electronic, information-based and intelligent inspection, which improves the work efficiency of power line inspection, the level of emergency rescue and the reliability of power supply. In emergency situations such as flash floods and earthquake disasters, drones can conduct surveys and emergency investigations on potential dangers of the line, such as the fall of the tower base, without being affected by the road conditions at all. It can also detect the blind spots of the human eye, which is very helpful for quickly restoring the power supply.\n2. Application field: agricultural insurance\nWorking principle: UAVs integrated with high-definition digital cameras, spectrum analyzers, thermal infrared sensors and other devices are used to fly on farmland to accurately measure the planting area of the insured plot, and the collected data can be used to evaluate crop risks and insurance rates. , and can determine the damage of the affected farmland. In addition, the drone inspection also realizes the monitoring of crops. Reasons for recommendation: Natural disasters occur frequently, and in the face of the situation of no harvest, agricultural insurance is sometimes a life-saving straw for farmers, but due to the difficulty in claim settlement, it makes people more suffering. The application of drones in the field of agricultural insurance can not only ensure the accuracy of loss assessment and the high efficiency of claims settlement, but also monitor the normal growth of crops and help farmers carry out targeted measures to reduce risks and losses.\n3. Application field: environmental protection work\nPrinciple: The application of drones in the field of environmental protection can be roughly divided into three types. 1. Environmental monitoring: observe the conditions of air, soil, vegetation and water quality, and can also quickly track and monitor the development of sudden environmental pollution incidents in real time; 2. Environmental law enforcement: Environmental monitoring departments use drones equipped with collection and analysis equipment to Cruise in a specific area, monitor the exhaust gas and wastewater discharge of enterprises and factories, and find pollution sources; 3. Environmental governance: use the soft-wing drones carrying catalysts and meteorological detection devices to spray in the air, and the working principle of drones to spread pesticides In the same way, eliminate smog in a certain area. Recommended reason: UAVs carry out aerial photography, with strong durability, and can also use far-infrared night photography and other modes to achieve all-weather aerial monitoring, and UAV law enforcement is not limited by space and terrain. It has strong timeliness, good mobility, and a wide range of inspections, especially in the Beijing-Tianjin-Hebei region with severe smog, which enables law enforcement officers to investigate the source of pollution in time and reduce the degree of smog pollution to a certain extent.\n4. Application field: film and television drama shooting\nWorking principle: The drone is equipped with a high-definition camera. In the case of wireless remote control, according to the shooting requirements of the program, it can shoot from the air under the control of remote control. Recommended reason: UAV realizes high-definition real-time transmission, and its distance can be as long as 5 kilometers, while the standard-definition transmission distance is as long as 10 kilometers; the drone is flexible and maneuverable, as low as one meter and as high as four or five kilometers. Chasing cars, raising and lowering, rotating left and right, and even shooting against the belly of a horse, etc., greatly reduces the cost of shooting.\n5. Application field: the problem of right confirmation\nHow it works: From the territorial disputes between the two countries to the confirmation of rural land rights, drones can be used for aerial photography. Recommended reason: Take the dispute over the Diaoyu Islands as an example, the drone is flexible and maneuverable, and it can record the small actions that Japan has carried out around the Diaoyu Islands in my country without dispatching a single soldier. In fact, the issue of border confirmation within some countries also involves different races. UAVs are dispatched to collect border data, which effectively avoids potential social conflicts.\n6. Application field: street view work\nPrinciple: Use a drone with a shooting device to carry out large-scale aerial photography to achieve the effect of aerial view. Recommended reason: Google and Tencent Street View are both \"Out\". The street view cars are pressing down on the road again and again, and maybe one day they will take pictures of your and my faces, but the drone is very different. Now, the street view pictures it shoots not only have a bird's-eye view of the world, but also have a little artistic flavor. Don't forget that in areas covered by clouds and fog all the year round, when remote sensing satellites are not enough, drones can charge into battle.\n7. Application field: express delivery work\nPrinciple: The drone can realize the distribution of goods of the size below the shoe box package. Just enter the GPS address of the recipient into the system, and the drone can take off.\n8. Application field: post-disaster rescue\nHow it works: UAVs equipped with high-definition cameras are used to take aerial photos of disaster-stricken areas to provide first-hand the latest images. Recommended reason: The drone moves quickly. It takes only 7 minutes from take-off to landing and has completed aerial photography of 100,000 square kilometers. It is of great significance for the post-disaster rescue work that races against time. In addition, drones ensure the safety of rescue work. Through aerial photography, those dangerous areas that may have landslides are avoided. It will provide reasonable allocation of rescue forces, determination of key areas for disaster relief, selection of safe rescue routes, and site selection for post-disaster reconstruction. Valuable reference. In addition, drones can monitor the situation in disaster-stricken areas in real-time and in all directions to prevent secondary disasters. In fact, there are more than eight popular application areas for UAVs, and there are more than ten and twenty. There are also huge advantages such as Jinying UAV surveying and mapping.\n9. Application field: remote sensing mapping\nHow it works: First of all, remote sensing means remote sensing. Broadly speaking, it means that you do not go to the target area and use remote control technology to inquire about local conditions. In a narrow sense, it is satellite pictures and aerial pictures. Remote sensing for surveying and mapping is the use of remote sensing technology to perform calculations on a computer and achieve the purpose of surveying and mapping.\nUAVs are widely used, low cost, high efficiency; no risk of casualties; strong survivability, good maneuverability, and easy use. They play an extremely important role in modern warfare and have broader prospects in the civilian field.\nThe reconnaissance aircraft is used to complete battlefield reconnaissance and surveillance, positioning and calibration, damage assessment, electronic warfare, etc.; it can also be used for civilian purposes, such as border patrol, nuclear radiation detection, aerial photography, aerial prospecting, disaster monitoring, traffic patrol, public security monitoring, etc. Target drones can be used as targets for artillery and missiles."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:9bb55d44-576a-4e63-a02c-3f3ad07573ac>","<urn:uuid:950ba8c6-27cf-4736-b09e-8cf89c4e6e18>"],"error":null}
{"question":"What are the fundamental principles of fascia's role in body mechanics and how do restrictions in fascial tissue impact overall physical function? Please provide a detailed breakdown of both the normal state and pathological conditions.","answer":"In a healthy state, fascia is a relaxed, stretchy, and movable connective tissue that spreads three-dimensionally throughout the body. It functions to provide support, protection, separation, cellular respiration, elimination, fluid flow, immune system function, and helps the body resist mechanical stresses. The fascia forms an interconnected web-like structure covering muscles, bones, nerves, organs, and vessels. When restrictions occur due to trauma, inflammation, or surgeries, the fascia loses its pliability, which can cause severe pain, pressure, and reduced range of motion. These restrictions can affect flexibility and stability, limit mobility, cause postural distortion, impair cellular nutrition, and lead to various dysfunctions. Due to the interconnected nature of fascial tissue, a restriction in one area can impact other distant parts of the body.","context":["Any traumatic force to the head can cause restrictions in the dural fascia, as well as the superficial and deep fascia of the head and neck. Restrictions in this area can affect the whole body. Myofascial Release is a technique for the evaluation and treatment of pain and dysfunction. The technique is gentle, and the goal is to restore mobility in fascia and to soften connective tissue that has become rigid. It works very well as part of a comprehensive physical therapy program including therapeutic exercises and modalities.\nFascia is a loose, irregular connective tissue that spreads three-dimensionally throughout the body. It covers the muscles, bones, nerves, organs, and vessels. It consists of four layers:\n• Subcutaneous - continuous layer over the entire body between the skin and deep fascia\n• Deep - series of sheets and bands that hold muscles and other structures in place\n• Subserous - lies between the fascia and serous membranes lining the body cavities\n• Deepest - within dura mater of the craniosacral system (cranium, spine, sacrum)\nThe fascia can not be seen on x-rays, CT scans or EMGs. Its functions include support, protection, separation, cellular respiration, elimination, fluid flow, immune system function, and allowing the body to resist mechanical stresses. All structures of the body can be affected when fascial restrictions occur.\nFascia is a system in the body that looks like a spider’s web. It is densely woven over and in every muscle, bone, nerve, arteries/veins, and all of our internal organs. The fascial system can be thought of as one continuous structure that twines throughout our body, interconnecting everything. This provides strength to the argument that everything in our bodies are connected—when one thing is out of whack, multiple things can be affected.\nFascia in a healthy state is relaxed, stretchy, and movable. When one experiences physical trauma, fascia loses its pliability. Trauma, inflammation, and surgeries create restriction in the fascia that can produce incredible amounts of pain, pressure, and range of motion loss within the body. Fascial restrictions affect our flexibility and stability, impeding our every-day activities. The restrictions can be caused by physical trauma or injury, poor posture or structural imbalance, and inflammation or infection. Restrictions can cause limited mobility, postural distortion, poor cellular nutrition, pain, and other dysfunctions. Because of the interconnectedness of the tissue, restriction in one part of the body can affect other distant parts. The fascia also stores memory of past traumatic events. Psychosomatic adaptations may occur, which may cause one to avoid positions which are associated with pain.\nYour therapist will evaluate your body visually and by palpating (feeling) tissue texture. Gentle pressure is applied by slowly pushing, pulling, and stretching the skin. Fascia has the characteristic of thixotrophy, which means it can change from a more solid to a more gel-like state with movement and increase in temperature. Some techniques are:\n• Sustained Stretch - gentle pressure into the direction of restriction, usually with a sliding motion that stretches the tissue.\n• Skin Rolling - gently picking up and pulling skin away from underlying structures. This stretches subcutaneous fascia, breaks cross-links, and makes tissues more pliable."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:435d7435-f2b2-45fa-9f9a-8e77610c9087>"],"error":null}
{"question":"How do you freeze fresh potatoes to keep them usable for up to a year?","answer":"To freeze fresh potatoes for up to one year, first ensure they are uniform in size (small potatoes can stay whole). Blanch them by putting them in boiling water for 4-7 minutes depending on their size, then immediately transfer to ice water to stop cooking. Let them cool completely, peel them, and place in a freezer bag. Less starchy varieties like Red Bliss and New Potatoes work better than starchy ones like Russet and Idaho. You don't need to thaw them before cooking later.","context":["Stop letting good food go bad! Get storage savvy and check out this list of foods that you may not know you could freeze for later use. Just remember to freeze items right away in order to maintain maximum freshness, and mark each bag/container with the contents and freeze date.\nFreeze Up to 1 Year\nYou probably know that cooked potatoes freeze well, but what if you want to store potatoes longterm and still have your choice of cooking options? Blanch before you freeze! This allows potatoes to remain in a mostly uncooked state until you're ready to boil, roast, fry, or whatever you'd like to do with them later.\nLess starchy (i.e. waxy) potatoes such as Red Bliss and New Potatoes are preferred for this method over starchy potatoes like Russet and Idaho. For more information on freezing uncooked potatoes, visit this article.\nFirst, make sure they are uniform in size (small potatoes can be left whole). Then blanch by dropping into boiling water for 4 to 7 minutes depending on the size of the potatoes or potato pieces, then transfer right away to ice water to stop the cooking process.\nLet them cool completely, peel, place in a freezer bag and freeze up to one year. No need to thaw them out before cooking.\nBlanching is standard for freezing uncooked vegetables in general, as this allows them to retain both their color and nutrient content while frozen.\nFreeze Between 1 - 3 Months\nSome types of sandwiches (with low-moisture fillings) can be assembled in advance, frozen, and then thawed out in the fridge the day before eating them. Really handy for preparing lunches for weeks (even months!) ahead of time.\nFillings that freeze well:\n- Deli meats\n- Hard cheeses\n- Tuna salad\n- Chicken salad\n- Nut butters (minus the jam/jelly)\n- Egg salad, as hard-boiled egg whites become rubbery\n- Moisture-rich sandwich fillers like tomatoes, lettuce, pickles, and sprouts which will make the bread soggy and/or get wilted. Add these in after thawing.\n- Jellies or jams\nWrap whole sandwiches as airtight as possible (without smushing it!) with plastic cling wrap.\nThen place inside freezer bags. The day before you'd like to eat your sandwich, place it into the refrigerator and allow to thaw overnight.\nFresh Citrus Slices\nFreeze Up to 6 Months\nIf you've got a citrus tree, you probably end up with an overabundance of fruit. Instead of spending loads of time juicing them, slice them up to use for drinks and garnishes later.\nSimply place slices in between pieces of freezer/wax/parchment paper (so they're easy to separate), stack in a freezer-safe container and use as needed. You can add frozen citrus slices directly into drinks, a drink pitcher, or punch bowl. To use as a garnish, place in the refrigerator to thaw for a couple hours or at room temperature for about 30 minutes before using.\nFreeze Up To 6 Months\nHard cheeses such as gouda, cheddar, and Swiss stand up well to freezing. Soft cheeses may be frozen as well for shorter periods of time (a couple weeks to a month), but there may be a significant change in texture.\nWrap well in plastic wrap, place in freezer bags and freeze for up to six months. Allow to thaw completely in refrigerator around 24 hours before eating.\nFreeze Up to 2 months\nIt can be difficult to judge how much whipping cream you need for topping desserts and other applications, especially when it's in a liquid state. No need to worry about waste, though, because you can easily freeze any extra whipped cream for later use.\nPlace individual dollops of whipped cream on a freezer/wax/parchment-lined baking sheet or roasting pan. Place entire sheet in the freezer just long enough for the dollops to harden (for easy handling).\nFinally, transfer the frozen dollops into a freezer bag or freezer-safe container.\nIf using to top a dessert, you can place frozen dollops directly on top of the dessert and let sit at room temperature for about 20 minutes, or in the refrigerator for a couple of hours. You can also place frozen dollops directly on top of hot cocoa, if you don't mind it cooling down your drink a bit. If you'd rather keep your cocoa piping hot, let the whipped cream thaw in the fridge for a couple hours, or around 20 minutes at room temperature before using.\nFreeze Up to 3 Months\nIt's surprising how many people let milk go to waste when it can easily be frozen and stored. The preferred method is in mason jars for their freezer-safe and preservation qualities and because they have a freeze line on them which makes it easy to know how much space to leave within the container to allow for expansion of the frozen liquid.\nFill a mason jar to the freeze line or if using another freezer-safe container, fill up to 3/4 full. Let defrost 24 hours in the refrigerator before using.\nAs with all frozen food items, don't forget to mark the freeze date on the container."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3a2bbfca-6509-4828-a2f1-5305a1e8c795>"],"error":null}
{"question":"Which aircraft achieved better passenger comfort: the RRE's modified Vickers Viscount or the Lockheed Super Constellation?","answer":"The Lockheed Super Constellation offered superior passenger comfort features. While the Vickers Viscount had a pressurized fuselage and quiet, comfortable working environment for crew and equipment operators, the Model 1049 Super Constellation boasted additional refinements specifically designed for passenger comfort, including air conditioning, reclining seats, and extra lavatories. These features were considered unheard-of at the time of its introduction in 1951.","context":["The Vickers Viscount seen here flying over Malvern on November 18 1968 was an aircraft from a large fleet of trials aircraft operated by RRE’s Aircraft Department. At the time of the photo, RRE’s aircraft were based at Pershore airfield (nowadays often wrongly called Throckmorton airfield). This airfield was the home of the establishment’s Aircraft Department between September 1957 and July 1977, when its rundown to closure was completed. The Department previously had control of Defford airfield from late May 1942 (when its aircraft were transferred to Worcestershire from their previous Dorset base) until September 1957.\nThis Viscount was one of a fleet of three Type 838 Viscount airliners bought new by Ghana Airways in September 1961 and used by that airline until January 1965. At that time, RRE had a particular requirement for a Viscount to be added to its fleet. RRE’s controlling government ministry was then the UK Ministry of Technology (MinTech). The aircraft was available for sale and it was purchased for RRE, by MinTech, and delivered to RRE Pershore in late January 1965. On arrival, it lost is Ghanaian civilian identity (9G-AAV), and then, like the other RRE aircraft, it became part of the very large MinTech Controller of Aircraft fleet, it acquired a military identity (XT661).\nRRE’s requirement for a Viscount arose from the establishment’s need to replace its existing Handley Page Hastings ECM (electronic countermeasures – also called ‘radar jamming’) aircraft (identify TG514). As a type, the Hastings had been in service as a utility military transport with the RAF since 1948, and eight of them modified into specialised radar trials aircraft gave good service to RRE between 1953 and 1974. By the 1960s it was an obsolescent type, and the Viscount was assessed as suitable for modification into a replacement for TG514. Compared with the 1940s design of the Hastings, the Viscount had a generally much improved performance, its pressurised fuselage enabling operation at higher altitudes: the airliner standard interior offered a quiet and comfortable working environment for the flight deck crew and for the ‘boffins’ operating the radar equipment now installed in the cabin previously used by passengers.\nLike scores of other other aircraft modified for radar trials use by RRE and its predecessors, the conversion of XT661 from a passenger airliner into a powerful flying jamming system was undertaken by the Aircraft Department. While the aircraft’s structural modifications were designed, manufactured and installed at RRE Pershore, the electronic design and manufacture of the jamming system was undertaken at RRE’s main site at Malvern. Modification of the aircraft was a complex job. It was required to have antennas in both its nose and tail for the radar jammer transmitter, but the nose was already the location for the aircraft’s weather radar (important for flight safety). This problem was solved by the design and manufacture of a pod, attached to the under surface of the fuselage, to house the antennas. Redesign and upgrading of the aircraft’s electrical system was also required as the electrical system could not supply sufficient electrical power for the powerful transmitters.\nECM is a technology used by all military forces to negate, or to mislead, the radar systems of their adversaries. In contemporary times, ECM has developed into a very sophisticated electronic science. The equipment in XT661 was more basic. Its basic function was to test the sensitivity of UK military radar equipments to simulated enemy jamming. In this role, the aircraft could be flown versus all types of radar systems – e.g. the ground-based air defence radar network, the air defence radars of Royal Navy ships, Army and RN guided missile system radars, airborne interception radars of fighter aircraft. This very wide requirements base required that the signals transmitted could be selected to be on a number of widely differing radar frequencies and this, in turn, required duplication of the radar transmitters and other equipment in the aircraft.\n© MRATHS 2016","How the Constellation Became the Star of the Skies\nIn 1939, the top brass of the Lockheed Corporation—president Robert Gross, chief engineer Hall Hibbard, and chief research engineer Kelly Johnson—scheduled a key meeting with a VIP, a man with deep pockets who had recently shown an interest in buying not just one or a handful of new planes but a fleet of them.\nThe customer’s request had been ambitious. He hoped to hire Lockheed to design a revolutionary aircraft capable of comfortably shuttling 20 passengers and 6,000 pounds of cargo across the United States, offering commercial aviation’s first coast-to-coast, non-stop service.\nBut the Lockheed team had come to express even grander ambitions. They wanted to build the company’s first large transport, one that “would carry more people farther and faster than ever before, and economically enough to broaden the acceptance of flying as an alternative to train, ship and automobile,” said Johnson.\nIn the years to come, the plane would be named the Constellation—Connie for short—and be flown by airlines around the world, as well as the U.S. military over the ensuing three decades. Eventually, it would be remembered as an enduring symbol, the epitome of grace in propeller-driven aircraft. But at that moment in 1939 in Los Angeles, the Lockheed Corporation was focused on winning over one customer and one customer only. His name was Howard Hughes.\nThe Secret Weapon\nHaving purchased a majority stake in TWA airlines earlier that year, Hughes saw the Constellation as his secret weapon in stealing market share from his competitors. He treated the project with all the subterfuge that secret weapons require. Not only did he demand total secrecy, but also specified that Lockheed could not sell the aircraft to any other transcontinental airline until TWA had received 35 of them.\nHughes outlined the initial performance specifications, but it was Lockheed that would design the sleek, distinctive, now-iconic aircraft. It was a critical turning point for Lockheed. As Hibbard said, “Up to that time we were sort of ‘small-time guys,’ but when we got to the Constellation we had to be ‘big time guys’ … We had to be right and we had to be good.”\nBeing good meant introducing new features previously unseen on passenger planes. The Constellation would offer the first hydraulically boosted power controls, aviation’s equivalent of power steering. It would be faster than most World War II fighters at 350 mph. And, using award-winning technology pioneered by Lockheed a few years earlier, it would feature a pressurized cabin for 44 passengers that allowed the plane to fly faster and above 90 percent of weather disturbances, what Constellation regulars would come to call smooth sailing.\nA Record Breaker\nIn fact, Lockheed’s design was so good, the U.S. military, readying for war, saw its potential as a transport for troops and supplies in Europe and took over production in 1942.\nThe first official flight test for a Constellation, sheathed in olive green paint and redesignated C-69, came early the next year. It was a plane equally beautiful in form as well as function. First flight went so well that five more flights were performed the first day. Hughes went about publicizing the Constellation the best way he knew how: by breaking a transcontinental speed record on a Burbank to Washington, D.C., flight in April 1944. The Connie averaged 331 mph, flying nonstop in six hours, 57 minutes, and 51 seconds on this flight. After setting the record, that aircraft was returned to the military and during service testing at Wright Field, Ohio, Orville Wright, who had made the first powered flight, made his last flight, serving as copilot on a test run.\nPushing the Limits\nWhen the war ended, TWA bought back all the C-69s it could from the government; conversions were made and the Constellation entered commercial service in February 1946.\nAs the Connie was designed to change the face of commercial flight—it was as experimental, in some ways, as the early commercial aircraft of the late 1920s—there were issues during its infancy. Lockheed was flexing the limits of piston-engine technology, and the engine’s complex design required maintenance, and sometimes replacement parts, at rates that would be considered unacceptable with the forthcoming introduction of jet airplanes.\nBy 1951, the much-beloved Model 1049 Super Constellation was unveiled, boasting unheard-of refinements, such as air conditioning, reclining seats, and extra lavatories. It was a plane ahead of its time, at least twice as fuel efficient as the industry’s first jets and as efficient as many of today’s modern aircraft.\nA Return to War\nWhile only 13 Constellations were built during World War II—Lockheed would be asked, instead, to focus on the P-38—the Army, Air Force, and Navy had recognized the plane’s versatility. By 1948, the Navy was calling in orders for Connies to act as long-range patrol aircraft , nicknamed Po-Boys from the PO-1 designation then in use. In time, Constellations would be used for everything from rescue missions and VIP transports to airborne early warning missions and the mapping of the earth’s magnetic field.\nIts area of distinction, however, was clearly airborne command and control and early warning. During the Vietnam War, Connies were flown in elliptical orbit near enemy territory to collect and transmit information on air activity. Constellations were also the first planes to carry rotating radomes, saucer-shaped domes used to protect radar antennas, a technology that is still in use with modern aircraft controlling the skies over the Middle East and with US Customs and Border Protection P-3s running drug interception missions in the Caribbean today.\nPresident Eisenhower was a big fan of the Connie, and his personal presidential plane, the only VC-121E built, was the first to bear the now-recognized moniker “Air Force One” when the president was onboard. All told, the U.S. military bought nearly 40 percent of all the Constellations ever manufactured, using them over nearly three decades, with aircraft serving well into the 1970s.\nA New Age Beckons\nSources and Additional Reading\n- Air Mobility Command Museum. “C-121 Constellation.” http://amcmuseum.org/exhibits_and_planes/c-121.php, accessed May 17, 2012.\n- Barlett, Donald L. and James B. Steele. Howard Hughes: His Life & Madness. New York: W.W. Norton & Company, 2004.\n- Boyne, Walter. Beyond the Horizons: The Lockheed Story. New York: St. Martin’s Griffin, 1999.\n- “Howard Hughes.” Chasing the Sun. PBS. http://www.pbs.org/kcet/chasingthesun/innovators/hhughes.html, accessed May 17, 2012.\n- Johnson, Clarence L. Kelly: More than My Share of It All. Washington, D.C.: Smithsonian Books, 1985.\n- “Today’s Mark of Distinction” (advertisement), Life. May 31, 1954.\n- Transport & Environment. “Report Shows Fifty-Year Failure of Aviation Industry to Improve Fuel Efficiency.” http://www.transportenvironment.org/press/report-shows-fifty-year-failure-aviation-industry-improve-fuel-efficiency, accessed May 17, 2012.\n- U.S. Centennial of Flight Commission. “The Hughes Companies.” http://www.centennialofflight.gov/essay/Aerospace/Hughes/Aero44.htm, accessed May 17, 2012.\n- Wilkinson, Stephan. “The Legendary Lockheed Constellation.” Aviation History. http://www.historynet.com/the-legendary-lockheed-constellation.htm, accessed May 17, 2012.\n- Yenne, Bill. Classic American Airliners. St. Paul, MN: Zenith Press, 2005."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d51e4a10-34c6-4b4e-96de-bb84d3829235>","<urn:uuid:550d6e44-3fca-4f8a-9564-fd31d7e21d04>"],"error":null}
{"question":"How does one diagnose Major Depressive Disorder (MDD), and what are step-by-step criteria that doctors look for? Can you explain process with examples such as symptoms?","answer":"To diagnose MDD, a patient must present with at least two primary symptoms: either depressed mood or inability to experience pleasure (anhedonia), plus at least 4 other symptoms that persist for at least 2 weeks. These additional symptoms include: significant weight changes, sleep disturbances, fatigue, decreased concentration ability, psychomotor changes, feelings of worthlessness/guilt, and thoughts of death/suicide. Importantly, these symptoms must cause significant distress or functional impairment and cannot be due to medication, medical conditions, or bereavement. Doctors may use tools like the PHQ9 questionnaire in primary care settings to screen for depression. The diagnosis can be complicated as patients may try to hide their depression due to social stigmas and might present with primarily somatic complaints.","context":["Flashcards in Psych - Depression Deck (14):\nIt is a brain disorder that affects mood, cognition, and physical well-being, and results in the deterioration of function, productivity, and interpersonal relationships; despite what colloquial language may suggest, it is not a transient mood state and is different from normal grief\nEpidemiology of Depression\nAffects 6.7% of adults (15 million) a year\nLifetime prevalence is 16.2%\nFemale : Male ratio = 2 : 1\nLeading cause of disability from all mental illnesses for people between the ages of 15-44 in US/Canada\nPrimary cause of suicide!\nDiagnosis of MDD\nFor a patient to be diagnosed with MDD, they must present with DEPRESSED MOOD or the INABILITY TO EXPERIENCE PLEASURE (anhedonia) and at least 4 other symptoms for at least 2 weeks --> significant weight loss/gain, sleep disturbances (too much OR not enough), fatigue, decreased ability to concentrate, psychomotor retardation or agitation, feelings of worthlessness/excessive guilt, and recurrent thoughts of death or suicidal ideation\nThese symptoms CANNOT BE DUE TO A MEDICATION, MEDICAL CONDITION, or BEREAVEMENT\nMUST cause significant DISTRESS OR FUNCTIONAL IMPAIRMENT\nEpisodic illness that may or may not have an apparent trigger\nThe episodes are co-morbid with substance abuse, anxiety disorders or others; COGNITIVE DYSFUNCTION is a significant feature - pseudo dementia in severe cases!\nThe AGE of first onset is in teenage years, especially in females.\nPatients may present with SOMATIC complaints but MAY TRY TO HIDE THEIR DEPRESSION due to social stigmas - makes diagnosis difficult!\nPHQ9 is a questionnaire that can be used to screen for depression in the primary care setting\nSUBTYPES of MDD\nPSYCHOTIC Depression - psychotic symptoms, usually delusions are present in addition to symptoms of depression; psychotic symptoms are NOT present between episodes of depression!!\nATYPICAL DEPRESSION -- when a patient experiences increased sleep and appetite, extreme fatigue, and interpersonal sensitivity (more common in women and for SOME REASON RESPONDS BETTER TO MAO INHIBITORS than others!!!)\nMELANCHOLIA -- a severe form of depression characterized by worse symptoms in the morning (diurnal variation), lack of reactivity, and PROFOUND ANHEDONIA; has been associated with abnormalities in the HPA axis\nMEDD WITH SEASONAL PATTERN (Seasonal Affective Disorder) --> when the depressive symptoms occur during the FALL and WINTER and REMIT in the SPRING for 2 CONSECUTIVE YEARS without other changes in psychosocial variables --> responds WELL to therapy\nA milder form of chronic depression that lasts for 2 years with LITTLE OR NO REMISSION\nThese patients must have two or more of the following symptoms -- poor appetite/overeating, insomnia or hypersomnia, low energy, low self-esteem, poor concentration, or hopelessness\nThey must also be WITHOUT A MAJOR DEPRESSIVE EPISODE in the first two years -- if they do, it is likely that he patient has BOTH dysthymic and MDD!!\nDiff Dx for Depressive Disorders\nHave to rule these out before diagnosing MDD:\nDrug induced (corticosteroids, Beta blockers, alcohol)\nNeurologic Disorders (MS, Wilson's, Early dementia)\nInfection - AIDS in the CNS!\nOR other Psych disorders!\nMonoaminergic Hypothesis of Depression\nDepressive states are brought upon by LOW DA and 5HT activity --> DA is the KEY NT in pleasure pathways!\nMakes sense that if a patient has anhedonia, something is wrong with DA!\nOther hypothesis --> HPA hypothesis (depression is a chronic form of stress that interferes with the feedback loop!)\nMany genes contribute to susceptibility to depression and gene-environment interactions can trigger depressive episodes\nOne of the most studied is the SEROTONIN TRANSPORTER (5HTT) which is a gene with 27 variations -- the \"s\" allele is associated with neuroticism that correlates with a risk for depression --> 2 \"s\" alleles exhibits greater amygdala activation to fearful stimuli\nNeuroimaging in Depression\nStudies have shown that neural systems that regulate emotion and reward seeking are actually dysfunctional\nAMYGDALA, MEDIAL PREFRONTAL CORTEX, and VENTRAL STRIATUM --> all of these tend to respond to NEGATIVE EMOTIONAL STIMULI and in depressed patients, they show abnormally INCREASED activity!\nOther regions, associated with emotional REGULATION (dorsolateral prefrontal cortex and dorsal portions of the anterior cingulate cortex) have ABNORMALLY LOW activity in depressed patients\nFirst line treatments for depression?\nSSRI FIRST LINE!!!! But there are obviously other drugs -- SNRI, TCA, MAOI, etc.\nTreatments take a few weeks for therapeutic benefits to be seen (but side effects occur much earlier!!!)\nSequenced Treatment Alternatives to Relieve Depression\nGoal was to assess the effectiveness of treatments in patients with MDD\nStratified into 4 levels that tested different medications or medication combinations --> if patient was not symptom free at the first level (Citalopram, an SSRI) they were placed in the next one\nStudy found that ONLY 30% OF PATIENTS WERE IN REMISSION AFTER TREATMENT WITH CITALOPRAM --> At Level 2, only 25% of those patients responded, and at level 3, 30%\n***patients may require multiple treatment strategies before achieving remission!!!****\nCognitive Behavioral Therapy\nCBT works off the idea that thoughts and attitudes, and NOT external events create moods, so the treatment involves education of the patient, relaxation training, cognitive restructuring where the patients examine their thoughts and whether they make sense and then try to change them, all followed by behavior activation\nIntrapersonal therapy uses biopsychosocial conceptualization and frames depression as a medical illness occurring in a social context, addressing the effect that personal relationships can have on depression"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:33b26c59-7875-4c69-9ccd-e58225d8a63f>"],"error":null}
{"question":"How do modern preservation challenges for historic buildings compare between the National Trust's approach to the Farnsworth House and Wright's Prairie-style architecture, particularly regarding environmental integration?","answer":"The preservation approaches show different environmental considerations. For the Farnsworth House, preservation focuses on flood mitigation while maintaining historical integrity, requiring careful consideration of environmental impacts and avoiding solutions that might divert water to neighboring properties. The preservation team specifically chose a lift system that would be hidden from public view. In contrast, Wright's Prairie-style architecture, exemplified in buildings like the Robie House, was inherently designed to work with the environment, featuring long horizontal lines that invoke the prairie landscape and incorporating natural elements into the design. Wright's approach to environmental integration was proactive in the design phase, focusing on creating structures that worked harmoniously with their surroundings rather than requiring later adaptation to environmental challenges.","context":["Owners of buildings that are susceptible to flooding are currently evaluating what they can do to protect their properties. Here at the National Trust for Historic Preservation, we have been going through that process with our site that frequently floods, the Farnsworth House in Plano, Illinois.\nFarnsworth is a tricky case study because it was designed to handle some amount of flood waters—that is why it’s elevated on stilted legs. Farnsworth also isn’t your typical property: It’s a National Historic Landmark and arguably one of the most recognized modern buildings in the world. As a result, a lot of people are invested in what we, its owners, decide to do. Furthermore, we were not willing to abandon the site by relocating the building. Although the site had been altered, it is the location that Ludwig Mies van der Rohe selected, and we consider that relevancy worth saving. As a national preservation organization, we needed a sensitive preservation solution. Flood mitigation, we realized, is very similar to other interventions that are necessary for the safe continued use of historic buildings—like seismic retrofitting, ADA compliance, or building code compliance. We ultimately selected a lift system to elevate the Farnsworth building during floods. We are basically adding a basement that will hold the lift equipment below ground, out of view of the visiting public. When the site floods by more than four feet, we will be able to activate the lift and lock the building in place until the waters recede.\nBefore we settled on this solution, we spent five years working with engineers to consider many other options. In the process, we realized both that we had learned an awful lot about flood protection and that we had not seen other sources that compared the different options. Realizing that the information we had gathered could help other property owners, we applied for a National Center for Preservation Technology and Training grant to help us present the information to the public through an interactive website. These are some of the questions we are most frequently asked about our project.\nHow did you come up with a quiz format? And how does it work?\nWe set out to objectively compare flood mitigation approaches with the goal of prescribing the best solutions for different circumstances. While we started with a matrix, we quickly realized that many of the options had similar strengths and weaknesses, which made an interactive and graphically pleasing quiz more useful.\nThe quiz presents about the expected flood level, the amount of land, and the budget, and then directs users toward different mitigation options based on their answers. Users can also visit pages that describe these various options, providing a definition, advantages and disadvantages, important considerations, helpful links, and images of the solution in use.\nWhat systems did you compare?\nWe analyzed wet floodproofing, dry floodproofing, permanent floodwalls, temporary barrier systems, relocation, elevation by raising the grade, elevation on columns/piers, elevation by hydraulic lift, and elevation by buoyancy.\nWhat are the major flood mitigation issues related to preservation?\nOne question that preservationists always ask themselves is, how much of the intervention should be visible? At many properties, adding visible safety or protection features, such as smoke detectors, is considered perfectly acceptable. For flood protection, such options include retractable flood gates or barrier walls, which can easily be incorporated into the landscape.\nPotential environmental ramifications are another important consideration. We don’t want to recommend solutions that save one property by diverting water toward neighboring resources. Adding fill can also be environmentally tricky because flooding typically first occurs on low land that was filled before there were environmental regulations against it.\nOther considerations include getting neighborhood buy-in—especially in urban environments—as well as mobilization time and staff capacity.\nWhat did you learn while doing this research?\nThe amount and flow of the floodwater determines the solution—minor flooding is much easier to deal with through inexpensive solutions. If the water rises more than around six feet, some of the barriers don’t work. If the water rises more than 15 feet, relocation is the only permanent option.\nHow did this study change your views about sea-level rise and sustainability?\nFlood protection is expensive, and while we know that the need for it is growing, specifics are difficult to predict. I expect that the country won’t fully deal with this issue until it becomes an economic imperative. For preservationists, it’s important to reorient the conversation away from the precious and toward the realistic.\nAshley Wilson, AIA, is the Graham Gund Architect in the Historic Sites department of the National Trust for Historic Preservation.#Flood#DisasterResponse#DisasterRelief#Farnsworth#NationalTreasure#HistoricSites","Each week we pay homage to a select “Original Creator” — an iconic artist from days gone by whose work influences and informs today’s creators. These are artists who were innovative and revolutionary in their fields. Bold visionaries and radicals, groundbreaking frontiersmen and women who inspired and informed culture as we know it today. This week: Frank Lloyd Wright.\nWidely considered one of the most esteemed American architects, Frank Lloyd Wright (1867-1959) designed over 1,000 projects over the course of his prolific career, yielding over 500 completed works, including personal homes, offices, churches, schools, skyscrapers, hotels, and museums. He was also one of the first architects to incorporate his design aesthetic into every detail of his buildings, conceiving complementary interiors, built-in furniture, light fixtures and stained glass—employing himself as an architect of space, not just of structure. It is believed that playing with Froebel Gifts (geometric blocks that could be arranged in various 3D compositions) as a young child greatly influenced his work, as his designs are known for their geometric clarity and structure.\nWright never received formal architecture schooling, and and even turned down an offer to study for free at École des Beaux-Arts in Paris because he thought the classical education he would receive would conflict with his vision for modern architecture. He was a pioneering student in the fields of Organic, Usonian, and the Prairie schools of architecture designing for a servant-less, domesticated American culture that incorporated more open floor plans and “workspaces” into his homes and buildings. Some consider his “open” design technique to be a metaphor for the openness of American political and social life.\nDespite multiple marriages and a semi-tumultuous personal life, Wright was very successful in his professional career, honored with an Honorary Doctorate of Fine Arts from the University of Wisconsin-Madison in 1955, and recognized in 1991 by the American Institute of Architects as “the greatest American architect of all time,” among other numerous awards and achievements. In addition to influencing modern designers like Mies van der Rohe and post-modernists like Frank Gehry, Wright was a writer, educator, and an active dealer of Japanese art. Here we look at five of his most significant architectural designs.\nFrederick C. Robie House (1908): Chicago, Illinois\nThe Frederick C. Robie house, a U.S. National Historic Landmark, is known as the greatest example of Wright’s Prairie styled homes, a type of architecture that is considered most uniquely American, and is recognizable by long horizontal lines thought to invoke the prairie landscape. The home features cantilevered roof eves, the usage of Roman bricks, four fireplaces, 174 art glass windows and door panels in 29 different designs. In 1956, The Architectural Record selected the Robie House as “one of the seven most notable residences ever built in America.”\nEnnis House (1924): Los Angeles, California\nThe Ennis House was originally designed as a residential home, modeled after the designs of ancient Roman temples, and constructed out of pre-cast concrete blocks. It’s most impressive qualities are the engraved details on the exterior blocks, making it a popular location for numerous movies, fashion shoots, music videos, and TV shows including Blade Runner, Buffy the Vampire Slayer, The Karate Kid, Part III, and Mulholland Drive.\nFallingwater (1935): Mill Run, Pennsylvania\nFallingwater is perhaps Wright’s most famous residential home, partly built over a waterfall, and originally designed for the Kaufmann family before becoming a museum in 1964. The home was designed as a nature retreat for the Kaufmann’s, invoked design elements of Japanese architecture, and utilized a nearby quarry’s rocks for the exterior walls. The building is surrounded by broad balconies that open up to the outside, and notable features include a staircase that leads down from the living room to the stream that is accessed by movable glass panes. Fallingwater is listed among Smithsonian magazine’s 28 Places To Visit Before You Die, and for good reason.\nJohnson Wax Building (1936): Racine, Wisconsin\nThe Johnson Wax Building, the world headquarters of S.C. Johnson & Son, is one of Wright’s most notable commercial works, featuring over 200 different types of curved red bricks seen on the interior and exterior. Wright utilized Pyrex glass tubing in the interior to let in soft light, and the overall building structure is an example of streamlined design.\nThe free-flowing “Great Room” in the interior of the building features innovative columns that Wright coined as “lily pads.” After demonstrating that a single column could support 60 tons of weight, Wright was given his building permit.\nSolomon R. Guggenheim Museum (1959): New York, New York\nThe Guggenheim art museum, located on the Upper East Side in Manhattan, NY is home to collections of Impressionist, Post-Impressionist, early Modern and contemporary artwork. The pristine white structure is considered one of Wright’s last major works, and the interior mimics the inside of a seashell. Wright originally designed the structure with the idea that patrons would take the elevator to the top and then view the artwork as they descended the spiral ramp, though currently the museum displays the artwork directing patrons to ascend the ramp instead of starting at the top.\nFrank Lloyd Wright’s homes and buildings can still be seen strewn across America, concentrated most heavily around Buffalo, NY; Chicago, IL; Los Angeles, CA; and Scottsdale, AZ. Most of them continue to be preserved as U.S. National Landmarks, and certain places are open to the general public. Wright’s designs completely changed the way American homes are built and structured, and he was the first architect to focus on how the interior space reflected the exterior skeleton of his buildings. Wright believed in designing something as a unit, whether it be the marriage of the interior and exterior layouts, or the geometric patterns and building blocks that made up his designs, which is echoed in the practice of many generative architects today.\nInstead of copying European architecture, he chose to be inspired by existing elements of nature, invoking Japanese influences as seen in Fallingwater, as well as building architecture that flowed with the structures’ natural surroundings, most notably in his Prairie style residences throughout rural America. He introduced floor plans that were more open and flowing, and were focused around the hearth, which he believed the “heart” of the home. His designs were also some of the first to utilize an abundance of natural light, evident in his frequent rows of windows, and through the intricate skylights featured in his more commercial work.\nHis designs of Fallingwater and the Guggenheim have become so ingrained in popular culture that they’ve recently been made into Lego sets, introducing his style and innovations to the younger generations, as well as inspiring the builder in all of us. Frank Lloyd Wright’s work can be viewed currently in the Frank Lloyd Wright: Organic Architecture for the 21st Century exhibition at the Milwaukee Art Museum, through May 15, 2011."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:18fb4658-c241-4141-a7f8-a308a206b48d>","<urn:uuid:57a32901-3bdf-4fa2-adde-657eedfac2d1>"],"error":null}
{"question":"What's the difference between SpaceX and NanoRacks in terms of their main space activities?","answer":"SpaceX focuses on developing orbital launch vehicles, with their initial Falcon 1 requiring four attempts to succeed, while NanoRacks specializes in International Space Station utilization, having delivered over 550 payloads to the ISS and launched over 180 small satellites. NanoRacks is also currently building the first commercial Airlock for the ISS.","context":["Startups Rocket To The Front Of The Space Race\nCapitalizing on the falling cost of launch services and the miniaturization of satellites, a new breed of startups are gearing up for the space race under the moniker of NewSpace.\nAt a recent conference in San Jose, Calif., leading figures in this NewSpace movement (a term used to describe the startups that stand in the shadows of space industry giants such as Boeing and Lockheed-Martin) discussed their novel approaches to spaceflight and exploration and discovery.\nWhile many in NewSpace have their sights set on lofty dreams of planetary exploration or asteroid mining, the first steps to getting off the planet are completely down to Earth. NewSpace is clearly a decade old but remains a nascent industry.\nThe first day keynote speaker was the newly appointed deputy administrator of that National Aeronautics and Space Administration, Dava Newman (known for her work at MIT developing next generation agile, light-weight spacesuits). Newman kicked off a day of presentations that placed NewSpace in the framework of a space exploration industry which is now 50 plus years beyond Sputnik — humanity’s first satellite.\nIn the US, space exploration has been dominated by the government through NASA. In part, this was driven initially by a space race and the development of modern electronics.\nNewSpace represents a new race, driven by the dreams of a young generation of engineers and enabled by private investors.\nWhile NewSpace companies are pursing several different potential markets, asteroid mining is clearly one of the big prizes in the nascent NewSpace market. Deep Space Industries (DSI) from Texas, for instance, is vying to be a leader in the future asteroid mining industry.\nA small half mile wide metallic asteroid holds more iron, precious and rare metals than has been extracted from the Earth throughout all human history. The most accessible near-Earth asteroids possess sufficient raw materials to support a human population of a 100 billion or more. — DSI Chief Scientist John Lewis.\nAnother critical piece of the NewSpace industry is determining new approaches to launching payloads to space. Lowering the cost to lift a pound (or kilogram) to space is vital for the industry’s success.\nPrograms like DARPA’s Airborne Launch Assist Space Access (ALASA) eliminates the need for a first stage. It’s an alternative approach that was described by Mitchell Burnside Clapp, the manager of the Tactical Office at Defense Advanced Research Projects Agency (DARPA) at the conference.\nUsing an F-15 (ALASA project) to loft and accelerate a small single-stage launcher to orbit, DARPA’s solution is one of several alternatives that use a single stage to orbit. Burnside using the F-15 provides approximately 3000 feet/sec of delta-V, which amounts to a 10% cost saving.\nAdditionally, ALASA can provide a quick turnaround launch system for small payloads. Rather than taking several months (or even years) small payloads, often experimental in nature, could see orbit within a few months of being scheduled. Clapp and DARPA are hoping that demand will lead to lower prices for both DoD and private sector payloads.\nDemand is the key to success for these alternative launch systems. It’s the driving force behind other piggyback systems like Virgin Galactic’s Launcherone. There, demand comes from another project under development by Sir Richard Branson’s Virgin Group — OneWeb, the group’s attempt to provide global internet connectivity.\nWilliam Pomerantz , Vice President for Special Projects at Virgin Galactic described the LauncherOne approach which will use the WhiteKnightTwo aircraft to reach 50,000 feet.\n“Being able to launch 10 times in 10 days will be a game changer,” said Sean Mahoney, CEO of Masten Space Systems. For him and others, taking on the challenges with significant risks is worth the effort.\nThe emergence of the new space industry can be traced to two different events in the early 2000s. The creation of the Ansari XPrize led to the development of the first privately developed sub-orbital flight by Burt Rutan in 2004 was one inflection point for the industry. And, even earlier, Elon Musk’s launch of SpaceX in 2002, when he became turned off by Russian launch operators, and decided to do-it-himself, building a privately developed orbital launch vehicle.\nThere are no overnight billionaires in this industry. Elon Musk was saved by friends as SpaceX’s Falcon 1 took four attempts to succeed. As one panelist stated, “the entrepreneurs most likely to succeed in NewSpace industry are operating on $5/hr salaries.”\nAt the same time, leading figures chosen as conference panelists warned hopeful business types and engineers to think twice about developing another launch vehicle. There are other areas in the new space industry with greater potential now, they said.\nThe San Francisco Bay Area has a broad array of start-ups which includes Planet Labs, NanoRacks, Spire Global (formerly Nanosatisfi), Google’s SkyBox Imaging,Elysium Space and more. Their pursuits are wide ranging from Elysium placing cremated remains into low-Earth orbit, SpaceVR developing virtual reality imaging platforms, Mountain View concern Made in Space, Inc. offering rugged 3D printers, to Planet Labs, Spire and Skybox offering various cubesat solutions for Earth observations.\nWhile some innovations propelling the NewSpace industry forward are brand new, predicated on the latest and most cutting-edge scientific research, the old saws of the tech industry — like Moore’s Law — are foundational components of NewSpace.\nThe increasing speed of computing electronics, accompanying advances in programming and also reduced size and power demands has reduced weight, power and volume of the avionics to control a space vehicle and also reduced the size of common payload instruments such as imagers and spectrometers. Cubesats, nanosatellites with a unit size of 10 centimeters cubed are made possible by Moore’s Law.\nAdditionally, the wealth developed by Silicon Valley ventures has been instrumental. Its responsible for saving SpaceX to making numerous startups possible.\nNewSpace consists primarily of young generations of engineers and entrepreneurs. The overall space industry has an average age close to 50 and the term “graybeards” was commonly used in panel discussions. Older engineers of the space industry have become senior advisers or have taken management roles to help guide the start-ups.\nJames Muncy, a qualified graybeard of NewSpace, presented a retrospective on the industry. His advocacy group, Polispace, is representative of the pivotal role of senior engineers and managers in NewSpace. He is also a co-founder of the Space Frontier Foundation.\nThough the industry shows promise, and has captured the imagination of engineers and investors alike, the future of NewSpace stands is still unclear. Historically, the tech bubble of 2000 and the Great Recession of 2008 dried up resources and led to failures in the space industry — in the matter of a few weeks. A sudden economic downturn today could likewise fell many of the existing companies. And with a continued dependence on NASA, political changes could also dramatically alter the prospects of the industry.\nThere is a love-hate relationship with NASA. Regulations and a chauvinism within NASA has been a hindrance but the agency is altering course and taken a more proactive role in NewSpace, according to industry participants.\nStill, there are programs supporting NewSpace like the Center for the Advancement of Science in Space (CASIS), Small Business Innovation Research (SBIR) program, the Small Business Technology Transfer (STTR) program and the Space Act Agreement (SAA) partnerships. The latter provides NewSpace startups with access to NASA technology, facilities and engineering experience. CASIS has provided NanoRacks access to ISS and an on-going program of releasing NewSpace cubesats from their portals in the space station.","NanoRacks Chief Executive Officer Jeffrey Manber Receives Pioneer of NewSpace Award\nJune 29, 2017 – San Francisco, CA – NanoRacks is proud to announce that CEO Jeffrey Manber has been awarded with the Pioneer in NewSpace Award from the Space Frontier Foundation (SFF).\nThe Pioneer in NewSpace Award is given to an individual who was an early leader and pioneer in the creation of the NewSpace industry, and who has left a lasting legacy – either through their own personal investment or in the creation of an early NewSpace business enterprise.\n“Throughout his career, Jeffrey Manber has been a tireless advocate for what we now call NewSpace,“ says SFF Chairman of the Board, Jeff Fiege. “He worked to overcome the obstacles to enable the human development of space before most of today’s NewSpace companies even existed. It is truly an honor to officially add him to the list of NewSpace Pioneers.”\nJeff received this award for his history both his establishing and working in the development of NewSpace and commercial space – from his time at the Department of Commerce Office of Space Commercialization, to being CEO of MirCorp, now to leading one of the top companies for space station utilization.\n“It is pretty cool to have my career in commercial space honored at the NewSpace 2017 conference by the Space Frontier Foundation,” says Jeffrey Manber. “Whether at NanoRacks or before, I’ve tried to make space just another place to do business. And slowly, slowly, we are getting there, thanks to organizations like the Space Frontier Foundation.” Adds Manber, “At NanoRacks, we look forward to continuing our work in further commercializing access to space and making space a destination for anyone with a passion for exploration.”\nJeff has a long-standing career in commercial space utilization. He served as the Advisor to the Chairman of PanAmSat, the first privately owned international satellite venture, ending the monopoly on international satellite communications. He co-developed the first Wall Street Fund dedicated solely to commercial space and helped create the Office of Space Commerce at the U.S. Department of Commerce in the Reagan Administration. He then served at CEO of MirCorp, and remains the only person to-date to have marketed a private space station. Under MirCorp, Jeff signed media and entertainment deals with space tourist Dennis Tito, Survivor television producer Mark Burnett, and moved producer James Cameron.\nToday, NanoRacks is one of leaders in commercial utilization of the International Space Station. NanoRacks has delivered over 550 payloads to the International Space Station, including the launch and deployment of over 180 small satellites. NanoRacks is currently building the first-ever commercial Airlock to be mounted to the ISS and has also been awarded funding from the NASA NextSTEP Phase II Award to study the reuse of the upper stages of launch vehicles as commercial habitats.\nNanoRacks LLC was formed in 2009 to provide commercial hardware and services for the U.S. National Laboratory onboard the International Space Station via a Space Act Agreement with NASA. NanoRacks’ main office is in Houston, Texas, right alongside the NASA Johnson Space Center. The Business Development office is in Washington, DC. Additional offices are located in Silicon Valley, California and Leiden, Netherlands.\nIn July 2015, NanoRacks signed a teaming agreement with Blue Origin to offer integration services on their New Shepard space vehicle. NanoRacks, along with partners at ULA and Space Systems Loral was also recently selected by NASA to participate in the NextSTEPs Phase II program to develop commercial habitation systems in low-Earth orbit and beyond.\nAs of June 2017, over 550 payloads have been launched to the International Space Station via NanoRacks services, and our customer base includes the European Space Agency (ESA) the German Space Agency (DLR,) the American space agency (NASA,) US Government Agencies, Planet Labs, Millennium Space Systems, Space Florida, NCESSE, Virgin Galactic, pharmaceutical drug companies, and organizations in Vietnam, UK, Romania and Israel.\nAbout the Space Frontier Foundation\nFounded in 1988, the Space Frontier Foundation is a non-profit organization dedicated to opening the space frontier to human settlement as rapidly as possible by unleashing the power of free enterprise. Our goals include protecting the Earth’s fragile biosphere and creating a more prosperous life for each generation by using the unlimited energy and material resources of space. The Foundation is composed of a diverse, multinational array of space activists, expert scientists and engineers, media and political professionals, entrepreneurs, and passionate citizens."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:af18bbf6-fe43-43ef-9de6-8586cf287255>","<urn:uuid:229d8c1a-63a6-4138-8ec3-b47a9d79aa0f>"],"error":null}
{"question":"What's the key difference between player influence in blackjack and the house edge effect - can other players' mistakes really impact your odds like the house advantage does?","answer":"Other players' mistakes cannot impact your odds in blackjack, unlike the house edge which has a mathematical impact. Even when players make poor decisions like hitting on 16 against a dealer's 5 or splitting tens, it mathematically evens out in the long run and doesn't affect other players' results. In contrast, the house edge is a defined mathematical advantage that guarantees the casino will profit over time - for example, blackjack has a house edge of 0.5-1% that consistently works in the casino's favor regardless of how other players perform.","context":["In this new series on the Myths of Blackjack, I’m starting with the most common myth surrounding the game.\nThe conversation usually goes like this…\nInterested Player: So, you play blackjack, huh?\nKen: Yeah, I’ve played a lot of blackjack over the years.\nInterested Player: You know what really drives me crazy about blackjack? …\nInterested Player: You sit down and get a good game going, and then some idiot sits down at third base and starts messing up the cards. What do you do about that?\nOK, I’ll start with a fact…\nOther players have no appreciable affect on your results.\nThat’s right… Johnny Clueless from Buffalo who sat down at your table had nothing to do with your losing streak.\nNow, if you already knew this to be true, you probably know what happens next in the conversation. Trying to explain that other players can’t screw up your results invariably leads to that blank stare. You know the one. It’s where you can almost see them thinking: “This Kenny guy doesn’t know squat about blackjack! How did he ever make any money?!”.\nGenerally, I don’t even bother trying to dispute their notion. Instead I’ll just nod my head as if these kinds of players bother me too, and change the subject as soon as I can.\nAs penance for all those times, let me make a concerted effort to explain why this is a myth. Even those of you who don’t need enlightening might find some ammo for your own rebuttals here too.\nThere are actually a whole group of possible complaints about Johnny Clueless. We’ll address them one by one.\nPeople jumping in and out of the game can’t “mess up the cards.”\nSo you’ve been winning a few hands, and when Johnny Clueless jumps in mid-shoe and adds an extra hand to the deal, the dealer starts killing everyone. It must be his fault, right? Well, no. Cause and effect is a tricky thing, especially in games where randomness is a factor. Our brains are evolved to look for patterns in causality, and that makes us see patterns and causes everywhere, even when they don’t really exist. There wasn’t anything magic about the number of spots that was already in play before he added a hand. There was certainly no guarantee that you would continue to win if he didn’t enter the game. He’s just a convenient scapegoat for our brains to blame as a cause.\nThe problem here really stems from the related myth that there are “hot tables” and “cold tables” in the casino. If you have won the last ten hands in a row, you would be accurate in saying that the table has been hot, but that tells you absolutely nothing about the next ten hands to come. But of course, if Johnny sits down and you start losing, you know who will get the blame. There’s no such thing as a hot table, only a table that has been hot.\nThere is no magic about a particular number of spots in play causing a winning streak, or ending one. Sometimes you’ll win and sometimes you’ll lose. That’s gambling!\nOther player’s strategy mistakes cannot hurt you.\nNow we’re on to the part of the myth claiming that unless all the players at the table play a solid basic strategy, none of the players will be able to win. I am always amused that most of the players who cling to this idea actually have no idea what the correct basic strategy is, but they are quite sure that the new guy at the table is playing badly and costing everyone.\nBut seriously, this is total bull. At my table, I don’t care how awful the other players are. In fact, I love to see bad players. They are the reason that blackjack is still a viable game for skilled players. Without a steady supply of uninformed masses, the casinos couldn’t offer a game like blackjack. If everyone played well, the game would make such small profits that the floor space would be converted to something else. But, I digress…\nYes, I’m telling you that even the guy that splits tens, hits on hard 16 when the dealer has a 5 up, and sometimes stands on a hand like (Ace,3) because he “has a feeling” cannot hurt your results. Sometimes his awful plays will cost the whole table, but other times his wacky plays will save the table. In the long run, it all just evens out. He can’t hurt you. So relax! Remember… Sometimes you’ll win and sometimes you’ll lose. That’s gambling!\nAnd no, there is no such thing as “taking the dealer’s bust card.”\nThis is probably the most common thing that drives uninformed players crazy. When Johnny Clueless is sitting at third base and decides to hit his hard 14 against the dealer’s 5, you can rest assured that everyone at the table will roll their eyes when Johnny busts and the dealer makes a hand. He “took the dealer’s bust card.” Well, yeah, maybe he did this particular time. But since you don’t know what the order of the undealt cards is beforehand, you can’t say that he wasn’t going to save the table instead.\nThis is such a strongly defended bit of mythology I’m going to dive a little deeper into the details. Now I know that many of the people who believe this nonsense can’t be bothered with details, but I am going to make an effort anyway.\nLet’s create a completely arbitrary, and impossibly simple situation… The dealer has a 5 up, and let’s also assume that his hole card is a Ten. You stand on your hard 12, and now the play is up to Johnny. We’ll say that there are exactly 4 cards left in the shoe, and somehow we know that the remaining cards are two sixes, and two Tens, although we don’t know the order.\nJohnny looks at his hard 16 and says “I’ve got a feeling”, and motions for a hit. Now we know that Johnny is going to bust with either a six or a ten. But what has he just done to you? Before we see the card, we don’t know. More importantly, before we see the card, it is correct to say that there is absolutely no effect on your result.\nHalf the time, Johnny will bust with a ten, and he did indeed take away one of the dealer’s possible bust cards. What’s left in the shoe after that is one ten, and two sixes. That means that 2/3rds of the time you will lose now because the dealer has 2 out of 3 chances to make a 21. Johnny sure worked you over, right?\nWell, the other half the time, Johnny will bust by drawing a six instead, leaving one six and two tens in the shoe. Now he’s done you a big favor, and you’ll lose only 1/3rd of the time.\nHere’s the part that you need to follow…\nThe chances of Johnny drawing a ten, and you subsequently losing to a dealer 21 is: 50% X 2/3 (That works out to 1/3 total, or expressed differently: 2/6.)\nThe chances of Johnny drawing a six, and you subsequently losing to a dealer 21 is: 50% X 1/3 (That works out to 1/6 total.)\nAdd these up (2/6 + 1/6) = (3/6) = (1/2)\nWell, look at that. Our overall chance of losing when Johnny takes a card is… 1/2.\nOur overall chance of losing when Johnny does not take a card is… 1/2.\nThis is not some evil coincidence. It works exactly the same way no matter how many cards are in the deck, and how complicated the math would be to verify it. It’s a mathematical fact… Johnny taking a card will help you exactly as much as it will hurt you on average. It all evens out in the long run.\nSo relax. Let Johnny play however he wants. He can’t hurt your expected win or loss.\nAnd after all, sometimes you’ll win and sometimes you’ll lose. That’s gambling!","It’s essential to comprehend the idea of the house edge whether you like to play roulette, baccarat, blackjack, or any other casino game.\nWhat is the house advantage in casinos? How does it work? Why does it matter? All these questions and more will be answered in this article. Let’s dive into the world of house edge and see how it affects your time at the casino.\nHouse Edge: Definition\nFundamentally, the house edge refers to a mathematical benefit that the casino possesses in relation to the players. This concept denotes the proportion of every bet that the casino anticipates to get as profit over an extended period. In a nutshell, it is the casino’s guaranteed return on investment or return on capital.\nThis principle universally applies to every casino game and constitutes a vital element of their overall design. An example that deviates from this principle is Poker, played exclusively between individuals and hence has no house edge. But in poker, the house also takes rake, so they nonetheless profit from the game.\nThe House Edge: How Is It Determined?\nThe house edge measures the casino’s advantage in a game and is based on several variables, such as the odds, probabilities, and rules involved. But before anything else, you must ensure that the casino you plan to wager in is licensed and offers various games. For instance, Florida has a robust land-based casino sector, but residents are prohibited from using any online gambling services. Please view more here for more information regarding gambling in Florida.\nLet’s examine the mathematics behind the house edge for a few familiar casino games:\nRoulette’s House Edge\nPlayers are captivated by roulette’s exhilarating wheel spin and the prospect of huge payouts. Roulette’s house edge varies from game to game. The house edge is roughly 2.7% when playing European roulette with a single zero.\nHowever, because of a second zero in American roulette, the house edge is roughly 5.26 %. These numbers are the average over a long period, so keep that in mind.\nBaccarat’s House Edge\nAmong casino games, baccarat is well-known for its timeless reputation as one of the most classic and easy to play. Compared to other casino games, baccarat’s house edge is one of the lowest, making it a popular pick among gamblers.\nBaccarat’s house edge typically falls between 1% and 1.5%, though this does vary per stake. The house edge is slightly smaller when wagering on the banker than on the participant or a tie.\nBlackjack’s House Edge\nPlayers of the popular card game Blackjack would do well to familiarize themselves with the house edge at the table. Depending on the player’s approach and the particular game regulations, blackjack’s house edge might change.\nBlackjack has a house edge of between 0.5 and 1% typically. With proper blackjack strategy, skilled gamers can even bring the house edge down to 0.5% or less.\nWhat Makes the House Edge Important?\nEvery gambler should be aware of the house advantage because it has a direct bearing on their long-term success or failure at the casino. With the house edge in place, the casino is virtually guaranteed to profit over the long run instead of the players.\nAlthough there is a chance for players to win big in the short run, the house edge ensures that the house will always win overall.\nWhat Effect Does the House Advantage Have on Players?\nThe house advantage guarantees a profit for the casino, but that doesn’t imply the players have to lose. Remember that the house advantage is determined over a long time frame, typically thousands of games.\nThe thrill and suspense of gambling comes from the fact that, in the near term, players stand an equal chance of winning and losing.\nHow to Reduce the House’s Advantage\nPlayers can nevertheless reduce the influence of the house edge on their bankroll by using various methods, even though the house edge is a defined mathematical concept. Consider the following:\nKnowing the Rules of the Game\nLearning the game’s rules is a good starting point for reducing the casino’s advantage. Rules for each game are unique, and even little changes can have a big impact on the house’s advantage.\nLearning the ins and outs of the game, such as any side bets or optional wagers, will help you make well-informed selections.\nMastering the Best Tactics\nAnother strategy to lower the house’s advantage is to use the best playing techniques possible. Numerous casino games have developed tactics that have been shown to lower the house edge or raise the participant’s chances of winning.\nIn blackjack, for instance, fundamental strategy charts detail the optimal moves to perform in response to your hand and the dealer’s upcard. The odds of success will increase dramatically if you implement these techniques.\nWhen trying to reduce the effect of the house edge, proper bankroll management is essential. Establish and maintain a limit for your wagering expenditures. Always know your limits and never chase losses.\nBeing prudent with your bankroll allows you to play longer and increases your chances of winning.\nAny casino player must comprehend the idea of the house edge. It symbolizes the edge in statistics that the casino has over the participants in every game.\nGamers can improve their chances of winning and have a more satisfying gaming experience by being aware of the house’s advantage and using techniques to reduce its effects. Always remember the house edge, whether you’re gambling at a physical casino or on a web-based gambling site."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2b22586a-5656-4a44-8c7e-54a52f7f7334>","<urn:uuid:ea09c84f-0999-44e7-89ab-f40605f79e2a>"],"error":null}