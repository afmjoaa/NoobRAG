{"question":"How do the tourist and cultural attractions differ between Tokyo's urban subcultural spaces and Barcelona's modernist architectural sites?","answer":"Tokyo and Barcelona offer distinctly different cultural attractions. Tokyo's cultural landscape is characterized by various subcultures that create their own unique spaces across the city, with these subcultures shaping and being shaped by their spatial environments. In contrast, Barcelona's cultural attractions are predominantly centered around architectural landmarks, particularly Gaudí's works including the Sagrada Familia, Casa Batlló, Casa Milà, and Park Guell. The city also features formal cultural institutions like the Picasso Museum, the National Museum of Catalan Art, and Barcelona's Contemporary Art Museum, showing a more institutionalized approach to cultural attractions compared to Tokyo's subcultural spaces.","context":["This research project brings together social scientists, architects, and urban designers to mutually investigate current social and spatial conditions in Tokyo. Professionals from different fields will contribute their individual perspectives and will stimulate one another’s work through an intellectual and analytical exchange. The convergence of disciplines is aimed to produce a unique body of knowledge of social dynamics in the complex and diverse spatial condition of Tokyo. The purpose of this project is to provide opportunities to deepen participants’ understanding of fundamental issues in the study of Japan through substantive discussion and exchange of views from an interdisciplinary perspective, and to disseminate the conclusions in a volume of essays and an exhibition.\nGood Neighborhood Form | Masami Kobayashi and Teddy Kofman\nTokyo is known as a polycentric city, consisting of multiple individual clusters of activity and density. Each of those clusters forms a neighborhood, an autonomous micro-cosmos with a unique socio spatial identity. This study examines three such neighborhoods in Tokyo which exemplify a live-work-play balance that appears to be desirable to many of the city’s residents and visitors: Koenji, Shimokitazawa and Yanesen. These neighborhoods are undergoing transformative processes which allow them to evolve, while maintaining their a multi-generational, community-based orientation. This study aims to identify these processes and their spatial manifestation. By analyzing these neighborhoods individually, and comparing them to one another, this study attempts to identify prototypical relationships and hierarchies between local formal and non-formal components which form the living conditions of these urban environments.\nRevisiting the Hinterland of Tokyo | Manuel Tardits\nA study of urban patterns and building typologies in Tokyo’s urban fringe or hinterland. The latter term names the large suburban realm, located outside the core urban fabric of the city. By city one means both Tokyo, Kawasaki, Yokohama and Chiba which constitute the main cities within the Tokyo metropolitan area or Shuto ken. This hinterland or fringe is delineated by a certain similarity of urban forms, density, zoning, land cost, etc. With the heavy demographic and economic evolutions of present Japan, this large area enhances the most dramatic changes that are occurring in term of shrinkage and urban expansion. Such a study shows a comprehensive view of the future change of the urban metropolis.\nStreet with no name | Davisi Boontharm\nThis research is part of my ongoing project of mapping and investigation of local urban narratives in residential neighborhoods which I am intimately acquainted. Using my Sketch and Script method of combined recording, annotation, analysis and reflection, I focus at residential parts of Tokyo where I live and, by creating visual and textual documentation which does not shy away from expression of my subjectivity, open those spaces, as lived entities, to discussion. My focus on urban ordinaries of Tokyo starts from on my own everyday life and lived experiences, from my innermost “self”, and expands towards my home, street and broader spaces of the neighborhood. The production of textual and visual documentations will be displayed to engage with residents and public to create discussion, introspection and comments.\nTransition of Physical and Social Environment| Hiroyuki Sasaki\nThis study aims to examine the transition of physical and social environments of communities of Onden in Harajyuku-Jingumae district, and to explore how the transformation of physical and social environments of the communities have been affecting each other in the period between Tokyo Olympics in 1964 and 2020.\nUrban Subcultures | Kaichiro Morikawa\nSubcultures are common in Tokyo, each having its own space across the city. Kaichiro will study how\nvarious sub cultures co-exist in Tokyo, how they shape their spatial environment, and in turn are\nshaped by them.\nSpaces Where Eyes Never Meet| Darko Radović\nThis project explores places and practices of everyday life in several residential precincts of Tokyo. The focus is on quality of life within traditional, low-rise high-density urban fabric, and within the increasingly present high-rise high-density precincts. Those two distinct spatial projections of society represent profoundly contradictory sets of values and development paradigms. While the former remains within an established, authentic urbanity which stimulates Lefebvrian “plurality, coexistence and simultaneity in the urban of patterns, ways of living urban life”, the latter is an imported paradigm of inauthenticity, embraced by global (including Japanese) financial and political elites. The push towards the Olympic Games 2020 stimulates insatiable appetites for profit, with no care for the day after.\nPublic Participation in Design | Mark Mulligan\nUsing Shibuya station as a case study, Mark will investigate how resident/stakeholder participation\nis shaping the city environment. He will attempt to identify what outcomes in the final design result\nwere a result of public participation, what would have been the alternative, and how common is\npublic participation in design in Tokyo.\nUrban Sociology | Shunya Yoshimi\nThis sociological urban study is part of a continuous research. It includes the examination of social\ndynamics between individuals and groups in the context of the spaces they occupy.","Barcelona City Breaks\nBarcelona is the heart and motor of the autonomous region of Catalonia in the northeast corner of Spain and is the second biggest city in the country. With almost four million people, Barcelona has one of the largest metropolitan atreas of any Mediterranean city.\nWith its extraordinary mix of architecture, cosmopolitan flair and joie de vivre, Barcelona is without doubt one of the top European city break destinations. from the impressive modernist architecture to the elegant boulevards of the Eixample and the narrow streets and plazas of the old city to the quirky museums and great nightlife, it is no surprise that the Catalan capital has captivated the hearts of so many visitors.\nGaudi's magnificent Sagrada Familia (a work in progress!), Parc Guell, the Olympic Stadium, the old Jewish Quarter and the marvellous market, \"La Boqueria\" off Las Ramblas are just a few of the wonderful sights to enjoy in this gorgeous city.\nPlease Note: A government tax may be payable on arrival in a city destination – it cannot be prepaid in advance. This is a nominal charge that is subject to change.\nDid You Know...?\nBarcelona's famous beachfront was actually created for the 1992 Olympics.\nLa Sagrada Familia has taken longer than the Egyptian Pyramids to build.\nBarcelona is home to the largest football stadium in Europe.\nTop Reasons to Visit Barcelona\n- Stunning Architecture\n- Inspiring Arts & Culture\n- Local Hotspots & City Sights to Explore\nWandering through the city you’ll see many pieces of Gaudí’s impeccable art, or architecture clearly influenced by his unique style. Ranging from la Sagrada Familia, a one of a kind cathedral, to Casa Batlló and Casa Milà, all the way to Park Guell, Gaudí’s work is ubiquitous in Barcelona.\nThe Gothic quarter is one of the oldest and prettiest districts in Barcelona. Its home to the gothic Cathedral and the gothic Church, Santa Maria del Pi. Stroll around the cobblestone streets and you’ll find great little cafes and vintage shops to browse. Take a break at one of the many plazas in the area.\nPerhaps the city’s most iconic street, La Rambla is a must-see for visitors in Barcelona. The promenade runs down from Placa de Catalunya to the Columbus Monument at the waterfront and is filled with shopping stands, street performers, living statues, and more.\nPalau de la Música Catalana\nThis brilliant concert hall is a UNESCO-listed heritage site with a gorgeous interior. You can visit for a tour, or check their website to see if there are any great shows on during your holiday.\nInspiring Arts & Culture\nBarcelona is full of so many fantastic museums that it can be overwhelming. Our top five include: the Picasso Museum, Poble Espanyol Village, the National Museum of Catalan Art, Barcelona’s Contemporary Art Museum, and the Chocolate Museum. Pick what sounds best to you and get lost in the spectacular exhibitions!\nMontjuïc Castle, a 17th century fortress, sits atop formidable Montjuïc Hill. You can either walk up the hill (passing the magic fountain) or take the funicular from the Parallel Station (metro) up to the top of the hill. Enjoy stunning views of the harbour and the city.\nMagic Fountain of Montjuïc\nSet in between the National Art Museum of Catalonia and the Plaza España, the Magic Fountain is the best free experience in Barcelona. Late in the evenings, the huge fountain in the centre of the square lights up in a magical musical show. Check online for the timetable.\nThe perfect day trip from Barcelona, go back in time at the magnificent Monastery Santa Maria de Montserrat. Located on a mountain peak, the views are outstanding. The surrounding nature offers a great contrast to the busy city.\nLocal Hotspots & City Sights to Explore\nMercado La Boqueria\nThis famous market is located just off Las Ramblas. Buy fresh fruits and veggies, get some seafood, or just wander along the bustling alleys. Visiting a local market is a great way to connect with the culture of the city.\nIdeally located on the Mediterranean Sea, Barcelona has beautiful beaches to explore. The mild climate keeps it nice and warm, and perfect for a swim! The Barceloneta area has all the most beautiful beaches.\nCamp Nou Stadium\nThis enormous venue is the biggest football stadium in Europe. Get tickets for a match and you’ll witness a-once-in-a-lifetime experience. There is also an FC Barcelona Museum here, or you can tour the stadium on the Camp Nou Experience.\nBarcelona Airport Transfers & Weather\nAirport: Barcelona El Prat (BCN)\nDistance: Approx. 14km from city centre\nThe metro into the city centre departs roughly every 7 minutes. There is a stop inside Terminal 1 and a stop outside Terminal 2, next to the train station. The cost per journey is approximately €4.50.\nThe bus runs to Plaça de Catalunya o Plaça d'España. The trip takes approximately 35 minutes and departs every 5 to 10 minutes. The bus station is located outside the terminal, and the journey costs approximately €5.90.\nThe train into the city centre takes approximately 20 to 25 minutes and departs about every 30 minutes. The train station is located in Terminal 2, and there is a free shuttle bus (approx. 10 minutes journey) from Terminal 1.\nA private transfer into the city centre would take approximately 30 minutes and would cost about €46pp for a return trip based on two people sharing."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7918d83e-cd9e-4204-9dc5-d7355934f906>","<urn:uuid:283c8e81-a59c-44df-875f-0271d96946bf>"],"error":null}
{"question":"Which publisher companies released these books - Y Lolfa and Cambridge University Press?","answer":"Y Lolfa published 'Hard Men of Rugby', while Cambridge University Press published 'Russia's Cotton Workers and the New Economic Policy'.","context":["Author: Luke Upton\nBrand: Y Lolfa\nNumber Of Pages: 192\nRelease Date: 16-10-2020\nDetails: Product Description\nThe true stories behind 20 of the toughest players to ever play the game, from pre-WWI firebrands to modern-day YouTube sensations. They are shocking, gruesome, often very funny and sometimes tragic, but what unites these men is their total commitment to the sport. Irrespective of size, reputation or opposition, they never took a step back, and many were as lively off the pitch as they were fiery on it. In our era of citing commissioners, slow-motion replays and trial by social media, some of their actions are hard to believe. Featuring exclusive interviews with some of the players themselves, insights from former teammates and a foreword from refereeing legend Nigel Owens, if you love the characters that make rugby great, then this is the book for you.\nTable of contents: Foreword by Nigel Owens Introduction Paddy Mayne Brian Lima Wayne Shelford Bobby Windsor Colin Meads Jerry Collins Norm Hadley David Bedell-Sivright Scott Gibbs Gérard Cholley Trevor Brennan Wade Dooley Bakkies Botha Tomás Lavanini Jacques Burger Armand Vaquerin Martin Johnson Brian Thomas Sébastien Chabal Weary Dunlop Author acknowledgements Bibliography Notes\nIt was safer and certainly more fun to read about the Hard Men of Rugby than to play against them! It's like having a few laughs over beers with them in the bar post-match, rather than what would have taken place on the pitch! - Al Charron, Former Canada Captain & World Rugby Hall of Fame inducteeI couldn't put the book down. It was great to learn more about some of the hardest men in rugby folklore. And I had the misfortune to play against some of these legends! -\nBernard Jackman, Former Ireland & Leinster player, coach and commentator\nA cracking read, with some great stories about some guys I played with and against. Plus many more I am relieved I never had to face.- Lee Byrne, Former Wales & Lions player and punditI laughed, winced and even found myself moved by this lively and fun rundown of rugby hardmen.-\nBen Mercer, Former rugby polayer & author of best-selling 'Fringes: Life on the edge of professional rugby'\nFresh, entertaining, well-researched and full of really fun nuggets. - Robbie Owen / Squidge, Youtuber, podcaster & writer\nIt's a privilege to be asked by Luke to write the foreword to this lively and engaging book. It's a great selection of players, some of whom I know personally. Others were far less familiar and it was fun getting to know them a little better. --Nigel Owens, Rugby World Cup Referee\nI've battled against some of those profiled and heard some amazing stories about many of the others, so I am honoured to be included in\nHard Men of Rugby. --Bakkies Botha, Former South Africa Player and World Cup Winner\nLuke has uncovered some fantastic stories about some of rugby's most renowned characters, and in doing so has brought them vividly to life. --Ross Harries, Broadcast Journalist and Author\nAbout the Author\nBorn and bred in South Wales, Luke Upton's first job was selling match-day lottery tickets for Swansea RFC. He now lives in London, where after working in the sports industry for five years, he is now a business journalist. He co-runs @NotGavHenson, the spoof rugby account on Twitter now regularly amusing over 41,000 followers, and is the author of hilarious satirical rugby novel 'Absolutely Huge'.\nPackage Dimensions: 8.4 x 5.4 x 0.7 inches","Russia's Cotton Workers and the New Economic Policy\nShop-Floor Culture and State Policy, 1921-1929\nBy (author) Chris Ward\nNormal Price: $235.00\nYour Price: $211.50 AUD, inc. GST\nShipping: $7.95 per order\nYou Save: $23.50! (10% off normal price)\nPlus...earn $10.58 in Boomerang Bucks\nAvailability: Available to Backorder, No Due Date for Supply\nRussia's Cotton Workers and the New Economic Policy by Chris Ward\nBook DescriptionIn Russia's Cotton Workers and the New Economic Policy Chris Ward uses a wide range of published and unpublished Soviet sources to examine key aspects of life on the shop floor of the Russian cotton mill in the 1920s. He reveals the existence of a complex world of work which grew out of the interaction between the experience of industrialisation in late nineteenth- and early twentieth-century Russia and the mechanisation of the cotton industry in Britain in the late eighteenth and early nineteenth centuries. The author explores the manner in which a 'mill culture' emerged from these developments and demonstrates that by the 1920s this culture was often very resistant to change. Russia's Cotton Workers and the New Economic Policy provides a realistic understanding of the relationship between worker, state policy and technology in Russia in the 1920s.\nBuy Russia's Cotton Workers and the New Economic Policy book by Chris Ward from Australia's Online Bookstore, Boomerang Books.\nBook DetailsISBN: 9780521345804\n(228mm x 152mm x 22mm)\nImprint: Cambridge University Press\nPublisher: Cambridge University Press\nPublish Date: 30-Mar-1990\nCountry of Publication: United Kingdom\nBooks By Author Chris Ward\nGenius of Paul Morphy, Paperback (April 2016)\nIn this book Chris Ward critically examines Morphy's style, strengths and weaknesses- the first time that a contemporary Grandmaster has so systematically appraised Morphy's games in the context of the modern understanding of chess.\n1 Group Bomber Command, Hardback (June 2014)\nA comprehensive and detailed operational history of 1 Group Bomber Command. The fifth release in a series that has already explored the records of 3,4,5 and 6 Groups.\n4 Group Bomber Command, Hardback (June 2012)» View all books by Chris Ward\nThe book contains individual squadron statistics, their commanding officers, stations and aircraft losses. It provides a detailed reference for one of the RAFs most important operational groups.\n» Have you read this book? We'd like to know what you think about it - write a review about Russia's Cotton Workers and the New Economic Policy book by Chris Ward and you'll earn 50c in Boomerang Bucks loyalty dollars (you must be a member - it's free to sign up!)\nBestselling Books: Our Current Bestsellers | Australia's Hottest 1000 Books | Bestselling Fiction | Bestselling Crime Mysteries and Thrillers | Bestselling Non Fiction Books | Bestselling Sport Books | Bestselling Gardening and Handicrafts Books | Bestselling Biographies | Bestselling Food and Drink | Bestselling History | Bestselling Travel Books | Bestselling School Textbooks & Study Guides | Bestselling Children's General Non-Fiction | Bestselling Young Adult Fiction | Bestselling Children's Fiction | Bestselling Picture Books | Top 100 US Bestsellers\nPhone: 1300 36 33 32 (9am-5pm Mon-Fri AEST) - International: +61 2 9960 7998 - Online Form\nAddress: Boomerang Books, 878 Military Road, Mosman Junction, NSW, 2088\n© 2003-2017. All Rights Reserved. Eclipse Commerce Pty Ltd - ACN: 122 110 687 - ABN: 49 122 110 687\nFor every $20 you spend on books, you will receive $1 in Boomerang Bucks loyalty dollars. You can use your Boomerang Bucks as a credit towards a future purchase from Boomerang Books. Note that you must be a Member (free to sign up) and that conditions do apply."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:825430cf-bb43-41f9-8794-43afe91d00a7>","<urn:uuid:481dd420-3959-4be2-9d5b-552f14f1f6be>"],"error":null}
{"question":"How do the maintenance requirements compare between a regular propeller aircraft and the Taurus Electro G2 electric motorglider?","answer":"Regular propeller aircraft require extensive maintenance including pre-flight inspections for physical damage, regular cleaning, paint care, static and dynamic balancing, and mandatory overhauls at TBO limits. In contrast, the Taurus Electro G2's powertrain is virtually care-free, only requiring battery recharging every 90 days, cleaning of the controller cooling duct, and checking/tightening of the motor's main bearing every 10 hours of operation.","context":["At Hartzell Propeller, one question we often hear from aircraft owners and operators is, “How can I extend my propeller’s service life?”\nWhile aircraft ownership will never be an inexpensive endeavor, there are some proactive steps you can take to avoid pricey repairs, especially when it comes to your propeller. Regular inspections and preventative maintenance tasks can go a long way in increasing your propeller’s operational life, helping you save time and money down the road.\nTake a look at these five ways to protect your prop and help extend its operational life:\nPerform regular inspections\nRegular pre-flight inspections of your propeller will allow you to catch emerging issues before they turn into more significant problems. Before each flight, give your propeller a thorough inspection for physical damage that’s visible to the naked eye, including paint damage, nicks, and gouges. Look for any signs of external corrosion visible on the blades. Then, check the spinner and spinner assembly for cracks and loose or missing hardware. Finally, look for grease or oil leakage in the interior of the spinner. While some oil or grease is normal, it shouldn’t be trailing down the blades. Watch this video for a more in-depth explanation of how to perform a visual inspection of a composite Hartzell propeller.\nKeep it clean\nThe propeller is one of the most highly stressed components on an aircraft, so give it some well-deserved TLC after every flight. When cleaning your propeller, you should never use a pressure washer as it can remove paint and damage internal components. Instead, wash the propeller blades with a soft cloth and a simple solution of dish soap and water after every flight to remove dirt and bugs. Point the blades in the “down” position and wipe toward the ground to prevent water from entering the hub.\nCare for the paint\nPaint is about more than ramp appeal—it’s a protective layer that can add significant life to your propeller. Be sure to check for chipping or flaking in your paint during pre-flight inspections. While you can do minor paint touch-ups on your own, having a full dress and paint done by a professional is best for major paint damage.\nFor more substantial paint problems, such as surface corrosion, repainting the propeller blades is necessary. If you have a Hartzell propeller, you can send it to the Hartzell Service Center or one of our Recommended Service Facilities around the world to have it repainted in its original paint scheme.\nKeep your propeller balanced\nA static balance should be performed any time the propeller is removed from the airplane and is routinely performed during overhaul services. Dynamic balancing, on the other hand, is a specialized service that’s performed when the entire propeller and engine assembly is running. A dynamic balance measures the vibration magnitude of the propeller and engine, allowing propeller technicians to identify where to correct imbalances. The process helps to reduce cabin vibration and usually results in a smoother, more comfortable flight. Although dynamic balancing typically involves an additional service charge, it’s a smart move to protect the health and longevity of the propeller as well as the engine, airframe, and avionics.\nFollow the manufacturer’s published TBO limits\nThere are several steps you can take to help extend the life of your propeller, but there inevitably comes a time when your prop will need to be overhauled or potentially replaced. Flying an aircraft with a propeller that’s beyond its intended service life is inadvisable and may result in unsafe operating conditions.\nPay close attention to the propeller manufacturer’s published TBO limits and send your propeller to a certified prop shop when it’s time for an overhaul. These limits are intended to protect your safety, because like any other mechanical device on your aircraft, propellers require periodic maintenance and inspection. Complying with the manufacturer’s published overhaul limits will ensure that any issues developing within your propeller are identified early, ideally while they are still repairable.","So, what does an owner think ?\nHere are a few comments from owners Terry and Kim.\nOur Taurus is the first G2.5 and our solar trailer is the first G2 in the USA.\nThe Taurus is based at our home airfield, Boerne Stage Airfield (5C1), Boerne, TX. The airfield is about 20 miles north of San Antonio.\nAfter 17 flights and 19 hours, Kim, my wife, and I have confirmed our Taurus Electro is the best motorglider for our soaring profile.\n- The cockpit is the most comfortable, roomy, easy to get into and out of, and has the most awesome visibility of all the 28 different gliders we have flown.\n- The conventional landing gear means we can taxi to and from the runway by ourselves. Since we live on a mostly power airfield and are retired, we can blend in with the powered aircraft and also fly on any day of the week without the help of anyone in our soaring club.\n- The hottest day we have flown on so far has been 94F. With my wife and I in the glider, we were airborne in about 600 ft and climbed 500-550 fpm. The Taurus climbs so good that, if the motor fails at any altitude, we will always be able to land into the wind on the active runway.\n- The Taurus is a very nimble glider in all three axis. The flight controls are well balanced, very crisp, with light stick forces.\nThe Taurus Electro G2 was the first electric 2-seat aeroplane in serial production available on the market. It offers complete freedom and independence thanks to the retractable electric engine, dual retractable main landing gear, excellent gliding performance, inexpensive maintenance and a well ventilated spacious cockpit. Furthermore, Pipistrel believes it is the only truly useful electric aircraft out there, because the electric drive is applied to the glider airframe, where battery capacity is not a limiting factor in performance/endurance. Taurus Electro G2 represents a leap forward in performance, safety, functionality and user friendliness.\nCan electric perform better than conventional? Absolutely!\nFor the first time electric power outperforms its gasoline-powered counterpart – the Taurus. Taurus Electro G2 can use a shorter runway, climbs faster and is performs much better than the gasoline-powered version when it comes to high altitude operations. All this is possible thanks to the specially-developed emission-free Pipistrel’s 40kW electric power-train.\nThe tailor-developed Lithium-technology batteries come in two configurations, capable of launching the aeroplane to 1200 m (4000 ft) or 2000 m (6500 ft) respectively. They are placed in self-contained boxes, monitored constantly by the super-precise Pipistrel battery management system (BMS), compete with data-logging and battery health forecasting.\nThe propulsion motor weighs an impressive 11 kg (rather than 16 kg) and generates 10 kW more power, resulting in a total of 40 kW. Due to this 33% increase in power and 40% decrease in weight we developed a whole new propeller, which has proven to be more efficient than the version flying on the Taurus Electro Prototype.\nThe motor peaks at 40 kW for take-off and allows continuous climbing at 30 kW power. It is controlled by a specially developed power inverter/controller and governed by the cockpit ESYS-MAN instrument. All components are networked via CAN-bus, feature proprietary multi-layer protection logic and produce a true throttle-by-wire experience.\nThe result is a full featured maintenance-free electric powertrain, that can be retrofitted into existing gasoline powered Taurus gliders and will be offered for integration into third party platforms as well. With Pipistrel, it is all about safety, more performance and fewer emissions.\nThe Taurus Electro G2 conforms with EU microlight standards as well as FAA ASTM standards for the airframe. Pipistrel is working with authorities to obtain full certification for the electric engine systems.\nThe Taurus Electro G2 made its first public appearance at AERO 2011 Friedrichshafen, where it won the Berblinger prize registered as D-METD. Numerous other certifications, including France, South Africa and USA were completed by the end of 2011.\nAlso at AERO 2011, Pipistrel is unveiling another World’s first – the concept of Flying For Free. Pipistrel developed the Solar Trailer, which can charge-up the Taurus Electro G2 in as little as 5 hours absolutely free of charge and with zero emissions! Furthermore, when the Taurus Electro G2 is stored in the trailer during the week of bad weather, it will still be charged and ready to fly by the weekend. The Solar Trailer and Taurus Electro G2 are perfect companions and demonstrate how it is possible to fly for free, quietly and with absolutely zero emissions, with today’s technology! This changes everything, again.\nMore than just a touch of Innovation!\nWith the Taurus Electro G2 we are introducing a World’s first – a full set of on-board networked avionics providing for a fly-by-wire powertrain management with built-in multi-layer protection logic. Let us tell you that this represents a great improvement over the system used in the Taurus Electro prototype, where everything was handled by the pilot.\nState-of-the-art battery system that uses information from the past to see into the future.\nThe first element of this networked system is the state-of-the-art hybrid battery management system which was developed entirely in house to be able to function with tighter tolerances than commercially available systems, yielding better performance and longer battery life. The Battery Management system monitors the batteries, which were specially developed to be used in the Taurus Electro G2.\nThese batteries represent the absolute pinnacle of today’s battery technology, combining low weight, high power and high energy density to levels that seemed impossible as recently as 2009. The batteries are placed in aluminum boxes with dedicated power and signal connectors. Each battery cell’s performance is monitored, temperature measured and future performance predicted.\nThe system is able to forecast when a battery cell is not performing and signals the need for a premature replacement. All parameters are also logged in the on-board flight data recorder. The four battery boxes are removable and replaceable.\nThe second element is the power inverter/motor controller. It’s location has been moved into the aircraft’s fuselage, it has become more efficient and features self-protection logic against current spikes, over temperatures and other potential abnormal situations. In other words, the controller is not only driving the motor, but also providing for the low-level protection of both itself, the batteries and the motor.\nIntroducing the ESYS-MON cockpit interface instrument!\nThe most noticeable addition to the networked system is the color-display cockpit interface instrument. The screen is really bright, in fact brighter than most displays out there, and is readable in the strongest of sunshine! It indicates the drive mode and important parameters to the pilot and provides the interface for engine retraction and extension. Everything is operated via two (2) toggle switches and a rotatable knob. The first toggle switch is the power on/off switch and does exactly that – powers up the motor controller.\nThe second toggle switch is the motor position selector »up/down« i.e. extended or retracted. This process is fully automated – the propeller is positioned and held in place while the motor extends or retracts. The pilot only selects the desired mode with the toggle switch. The rotary encoder acts as the throttle selector.\nThe beauty comes from within…ESYS-MON is much more than just a display.\nDue to the nature of electric propulsion system, we decided to employ the computational power of this cockpit interface instrument and gave it the role of the master on board computer. This means it not only displays data to the pilot, but also »talks« to every other powertrain component on board via CAN bus and makes everything sing together. It is able to detect overheating of individual components and reduce power gradually in order to maximize the climb potential with the given system status. Not only that, it provides systems diagnostics and all necessary warnings to the pilot.\nCombining all elements, we have developed an aircraft which offers more performance and an almost care-free use of this revolutionary technology.\nTaurus Electro is made in highest technology composites (epoxy resin, glass fibre, carbon fibre, kevlar fibre and honeycomb structures). The airfoil used on wings is ORL 170, (F. Orlando).\n|Model TAURUS ELECTRO G2|\n|Motor||High performance synchronus 3-phase electric outrunner with permanent magnets|\n|Power||40 kW for takeoff, 30 kW contionus|\n|Propeller||2 blade Pipistrel 1650 mm diam special for Taurus Electro G2|\n|Wing span||14.97 m|\n|Wing area||12.33 m2|\n|Rudder area||0.9 m2|\n|Horizontal tail area||1.36 m2|\n|Positive flaps||5 deg, 9 deg, 18 deg|\n|Negative flaps||-5 deg|\n|Center of gravity||23% – 41%|\n|Empty weight (includes standard battery!)||306 kg|\n|Minimum pilot weight||60 kg|\n|Maximum total pilots weight||220 kg|\n|Max take off weight (MTOW)||550 kg|\n|Weight of std battery system||42 kg|\n|Weight of optional battery system||59 kg|\n|Stall with flaps||63 km/h|\n|Stall without flaps||71 km/h|\n|Manoeuvring speed||163 km/h|\n|Max. speed with flaps extended||130 km/h|\n|Max. speed with airbrakes extended||225 km/h (extend at or below 160 km/h)|\n|Max. speed with powerplant extended||160 km/h|\n|Min.sink speed||94 km/h|\n|Max. sink with airbrakes||6.0 m/sec @ 100 km/h|\n|Best glide||1: 41|\n|Best glide ratio speed||107 km/h|\n|Best glide at 150 km/h||1: 33|\n|Best glide at 180 km/h||1: 23|\n|Max towing speed||150 km/h|\n|45°- 45° roll time||3.9 sec|\n|Take off run MTOW||160 m|\n|Take off over 15 m MTOW||245 m|\n|Best climb speed||100 km/h|\n|Max climb rate (MTOW)||3.1 m/sec|\n|Relative climb (elevation independent!)||1100 m (4000 ft) / 2000 m (6000 ft)|\n|Max load factor permitted (x1,8)||+5.3g -2.65g|\n|Max load factor tested||+ 7.2g – 7.2g|\nPipistrel reserves the right to revise the above data whenever occasioned by product improvement, government/authority regulations or other good cause. The design basis follow the strictest EASA CS-22, CS-VLA and CS-23 (sections), as well as rules applicable to ultralight/microlight aircraft (LTF-UL 2003, etc.)\nI wish to know more about the batteries used in Taurus Electro G2\nBattery type is a special-made Li-poly battery, 10 Ah capacity per cell, 25 C discharge rate. The system includes 4 boxes where the batteries are located, the BMS also. The standard battery configuration is 128 cells, optional are 192 cells. They fit in the same 4 boxes in both cases.\nThe basic option gives you total capacity of 4.75 kWh, from which it is sensible not to use more than 80% due to battery cell life. Effectively you end up with 3.8 kWh of useful energy. This version fully meets European microlight standards regarding the empty weight! The battery pack weighs 10.5 kg per box, there are 4 boxes, totaling at 42 kg.\nThe optional pack adds capacity to reach 7.10 kWh total, again by taking 80% »sensible discharge level« you are effectively at 5.7 kWh of useful energy. This battery pack weighs 13.9 kg per box, there are also 4 boxes on board, totaling at 55.6 kg.\nWhat is the endurance of the Taurus Electro G2 in real life?\nIn terms of endurance the following margins apply for the basic battery pack:\n20 kW power output: 11 min 30 sec\n30 kW power output: 7 min 40 sec\n40 kW power output: 5 min 40 sec (theoretical, expected is 1 min on 40 kW, then reduced power)\nThe data may change because of ambient temperature. 80% sensible discharge level is taken into account.\nIn terms of endurance the following margins apply for the optional battery pack:\n20 kW power output: 17 min 10 sec\n30 kW power output: 11 min 20 sec\n40 kW power output: 8 min 35 sec (theoretical, expected is 1 min on 40 kW, then reduce power)\nThe data may change because of ambient temperature. 80% sensible discharge level is taken into account. Please note that in horizontal flight only 7 kW is needed, so theoretical endurance reaches 1 hour.\nWhat are the variables influencing the top-of-climb capability?\nThere are a lot of factors for this, from cockpit load, runway condition (how much energy you burn for the taxi & take-off), ambient temperature, thermal properties of different components, controller parameters, etc. Ambient temperature is the most important factor of all.\nWhat maintenance is required for the powertrain?\nThe maintenance is virtually care-free!\nBattery system takes care of itself but needs to be recharged to full charge at least once every 90 days to keep them »healthy«.\nController maintenance: nothing, just clean the cooling duct.\nMotor maintenance: check main bearing for axial free play and tighten main bearing every 10 hours of motor operation.\nIs it mandatory to wait a long time until engine is hot before start?\nAbsolutely not. The colder – the better for the engine. It will not be recommended to apply full power however if the batteries are below 5 deg. Celsius.\nHow does the Propeller stop and the engine retract? Is it all fully automatic?\nYes. Fully automatic systems include the brake, which is all electric, and positioning via a magnetic 16-bit hall-sonde encoder. When the propeller is correctly positioned, it can retract. The stopping, positioning and retraction of the propeller work flawlessly at the press of a single button.\nIs engine and controller cooling adequate even in the summer time Australian conditions?\nThe cooling is proving to be sufficient. In any case, there is a protection logic built in which will slowly reduce the power on the system if it will be picking up temperature too fast (considers also temperature gradient, not just limit temperatures!)\nIs it possible to retract the engine right after stopping it, or is it mandatory to wait while the batteries cool down?\nBatteries essentially do not become over 50 deg.C hot. It’s not necessary to cool them down – you can retract immediately at any time, mid-flight or on the ground!\nIs it possible to extract and restart the engine mid-flight?\nOf course. As with the retraction, it is all automatic.\nIs there a recommendation »don’t take-off when the remaining power is under a certain percent« ?\nThe system will not allow you to do that. If less than 3 minutes of battery endurance is indicated, it will not go to take-off power and it will produce a warning.\nHow long does it need to charge at 220V?\n3.5 hours for the standard battery configuration, 5 hours for the optional configuration. This is when the batteries are completely empty! You can monitor all this via the ESYS-MAN instrument. Charging is also possible form the Pipistrel’s Solar Trailer.\nIs charging with 380 is recommended?\nNo, the charger is a single-phase 220V or 110V.\nIs there any built-in safety in case of too high temperature or controller dysfunctions ?\nThere is a multi-layer logic in place. The controller takes care of itself. In case of too high temperature it will first reduce power (up to 5%) and then switch itself off in case of severe over-heating. BUT BEFORE THIS OCCURS THE FOLLOWING WILL HAPPEN:\nWe have an on-board computer now. It measures not only the temperatures of all components of the system (motor, controller, 4 temperature probes per battery box etc), but also a bunch of other parameters and has the limit temperatures as well as limit temperature gradients programmed inside. For example, if the motor is heating up more than a certain amount of degrees-per-minute, it will reduce power to track the maximum permitted temperature gradient (slope), in order not to reach the limit temperature at all. The same goes for the controller, as well as for the batteries. Manual override is possible, of course. There are warnings which display on the screen, too.\nWill parachute remain usable in case of battery overheat / fire ?\nThanks to the super-precise Battery Management System, which was specially developed by Pipistrel just for Taurus Electro G2, battery issues are extremely unlikely. Furthermore, the batteries are placed in self-contained metal boxes in the fuselage. In event of an overheat/fire, the parachute remains fully functional.\nHow is throttle control executed?\nThe system uses throttle-by-wire concept. The throttle input is received at the ESYS-MAN, filtered with protection logic and the reference for the RPM is then sent over to the motor controller via CAN bus. It is all very elaborate, not via a simple potentiometer as it is common with other aircraft.\nDo you need Service or Support for your Pipistrel products ?\nManuals for an aircraft you are servicing ? Spare Parts ? Airworthiness Information ?\nGo to the Pipistrel Cloud https://cloud.pipistrel.si/ for the following information\n- IPC – Internet part catalog\n- Technical Publications Portal\n- Electro Portal\n- Pipistrel Product Registration\n- Employment Portal for Job Applications\nIf you need more information then please contact Pipistrel USA here including as many details as possible"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bbc5cef9-2c38-4887-9c80-1352cff69a64>","<urn:uuid:53d3d082-08d6-476a-a7f4-020c47ed94c6>"],"error":null}
{"question":"How will future observations of Earth and Neptune Trojan asteroids compare in terms of detection challenges and stability?","answer":"The detection and observation challenges differ significantly between Earth and Neptune Trojans. For Earth Trojans, they are particularly difficult to observe because they appear close to the Sun in the sky during most of their orbits. However, new efforts are underway to improve detection, including the expanded coverage by the Catalina Sky Survey and planned observations by the Vera C. Rubin Observatory. For Neptune, there are currently 17 known Trojan asteroids, and numerical simulations show it is difficult to get Trojans that survive over the lifetime of the Solar System. Neptune's Trojans require specific conditions for stability, including a slow planetary migration over 150 Myr to achieve the observed orbital inclinations.","context":["Hi! Today I will tell you about a study recently accepted for publication in Astronomy & Astrophysics, by Rodney Gomes and David Nesvorný, on the survival of the asteroids which precede and follow Neptune on its orbit.\nThe coorbital resonance\nIn the Solar System, the mean motion resonances are ubiquitous. When the orbital frequencies of two bodies are commensurate, interesting phenomena might happen: they could have a more stable orbit, or they could experience a permanent forcing which raise their eccentricity and / or inclination, and in some cases could result in an ejection. A resonance has particularly strong effects on a small body which orbit resonates with the one of a large planet. This is for instance how the giant planets shaped the asteroid belt.\nHere, we deal with the coorbital resonance, which is a very specific and interesting case. This happens between two bodies which have on average the same orbital frequencies, and the perturbations associated result in some zones of stability. In particular, there are five equilibrium positions for the coorbital restricted 3-body problem, i.e. if we consider the Sun, a planet, here Neptune, and a small body. These equilibriums are known as Lagrangian points, and the most remarkable of them are denoted L4 and L5. They precede and follow the planet at an angular distance of 60°, and are stable equilibriums. As a consequence, they are likely to accumulate several small bodies, and this is verified by the observations, which have detected asteroids which coorbit with Jupiter, Uranus and Neptune.\nAt this time, 6,288 of these objects have been detected for Jupiter, 1 for Uranus, and 17 for Neptune.\nThe planetary migration\nSince 2004 and the first version of the Nice model, the giant planets are assumed to have formed closer to the Sun than they are now, and have migrated to their current orbit. The reason for this migration is that they were form in a large proto-planetary disk, full of planetesimals which drove migration. The asteroids are some of these planetesimals. This raises the following question: could the coorbital (or Lagrangian) asteroids survive this migration?\nLong-term numerical integrations\nAddressing this problem requires long-term and intensive numerical simulations. The issue is this: you need to simulate the evolution of the Solar System over 4.5 Gyr. For that, you write down the gravity equations ruling the motion of the planets and the planetesimals (these are many objects… the authors considered 60,000 of them), and you propagate them numerically.\nTo propagate them, you start from a given position and velocity of each of your bodies (initial conditions), and the equations give you the time-derivative at this point. You then use it to extrapolate the trajectory in the time, and you reiterate…\nOf course, this algorithm does not give exact results. To lower the error, you should take a small time-step, but a too small time-step requires more iterations, and at each iteration you add an error due to the internal accuracy of the computer. To make your life easier, numerical integrators have been developed to improve the accuracy for a given time-step. In this study, the authors use two very well-known tools, SWIFT and MERCURY, dedicated to the integration of the motion of the planets and asteroids.\nIn this paper\nThe authors show it is difficult to get Trojans of Neptune that survive over the lifetime of the Solar System. In a first numerical integration, they do get captures, but none of them survive. Then they consider planetesimals which are very close to the observed Trojan, and they get some captures.\nSomething interesting is that they show that the orbital inclination of these Trojans can be excited during the migration process. For that, the migration should be slow enough, i.e. over 150 Myr, while previous studies, which assumed a migration ten times faster, did not excite the inclinations up to observed values.\nEven if it is now accepted that the planets have migrated, several competing scenarios exist (Nice, Nice 2, Grand Tack,…) and some are probably to come, just because there are many ranges of initial conditions which are possible, many possible assumptions on the initial state of the proto-planetary nebula… and these scenarios should of course impact the capture of Trojans of the giant planets.\n- The study, Gomes R. & Nesvorný D., 2016, Neptune trojan formation during planetary instability and migration, Astronomy and Astrophysics, in press\n- The web site of David Nesvorný\n- The page of Rodney Gomes on ResearchGate\n- The integrator SWIFT\n- The Minor Planet Center, which provides an up-to-date catalog of the known asteroids","A recently discovered asteroid appears to be an Earth Trojan, orbiting a gravitationally stable area with only one other known occupant.\nEarth has a second Trojan asteroid sharing its orbit, reports amateur Tony Dunn on the Minor Planet Mailing List. The asteroid, dubbed 2020 XL5, is a few hundred meters across and its orbit is tied to a gravitationally stable region ahead of Earth in its orbit.\nTrojans are asteroids gravitationally locked to stable Lagrange points either 60° ahead (L4) or behind (L5) the planets in their orbits around the Sun. 2020 XL5 was found around the L4 point. Massive Jupiter has more than 9,000 Trojans. In theory, Trojan orbits would be stable around every planet except Saturn, where Jupiter’s gravity pulls them away. So far, Trojans have been found sharing orbits — at least temporarily — with Neptune, Uranus, Mars, Venus, and Earth.\nEarth Trojans are hard to find because during most of their orbits, they appear close to the Sun in the sky. Not only that, but the gravitational resonance does not hold them in lockstep at 60° ahead and behind of the Earth, explains Dunn. Instead, the objects trace paths around the L4 and L5 points, which are themselves moving as Earth orbits the Sun.\nNASA's Wide-field Infrared Survey Explorer spacecraft spotted the first Earth Trojan, 2010 TK7, also locked to the L4 point, in October 2010 when it scanned the infrared sky 90° from the Sun. Two other observers recovered it a few months later with the Canada-France-Hawaii Telescope. It's slightly smaller than 2020 XL5.\nThe orbits of our two Trojans are best visualized along with that of Earth’s and, in the case of 2020 XL5, the orbits of all the inner planets. When viewed relative to Earth, 2010 TK7 drifts between a spot close to Earth to the L3 point on the other side of the Sun from Earth, but it doesn’t pass through the L4 point. The orbit of 2020 XL5 ranges more widely, drifting inward to inside Venus’s orbit and outward almost to Mars.\nThe wide-ranging orbit shows “[2020 XL5] is almost certainly a garden-variety bit of rock that went [close to] Venus and got perturbed into an orbit with a period very close to one year,” says Bill Gray of Project Pluto.\nAldo Vitagliano, a retired Italian chemist and author of the Solex orbital software, said on the MPML that the orbit should remain stable for 2,000 to 4,000 years, but gravitational tugs would eventually move it to another orbit. So far 2020 XL5 has only been observed for only a few weeks, and amateur astronomer Sam Deen says we may have to wait until November or December until more observations can be made to pin down its orbit.\nThe first Earth trojan, 2010 TK7, comes within 20 million kilometers (12 million miles) to Earth every few hundred years; it is currently drifting away. Models show its orbit is stable enough to stay in a one-to-one resonance with Earth for about a quarter million years. While there are Earth Trojan orbits that are stable for the life of the solar system, no objects have been found occupying them.\nTwo spacecraft on their way to visit near-Earth objects searched Trojan regions in 2017, but NASA's Osiris-Rex found nothing at L4 and the Japanese Hayabusa 2 found nothing at L5. However, the observations were not definitive, and in 2019 Renu Malhotra (University of Arizona) wrote that the Earth could still have up to several hundred Trojans at least a few hundred meters in diameter, amounting to several percent of the some 10,000 near-Earth objects of that size.\nA population of Earth Trojans should have survived since the planet formed if its orbit hasn’t changed since then, she says. Their existence — or lack thereof — has other implications, too. Searching for ancient Trojans could help explain why the leading hemisphere of the Moon has about 70% more young craters than the trailing side, a difference current models can’t explain. Earth Trojans slowly escaping from their orbits might account for the extra young craters.\nNow, Malhotra says, astronomers are stepping up their search for Earth Trojans. The Catalina Sky Survey has expanded the area it covers, and a group at the Vera C. Rubin Observatory is also planning observations once that observatory comes online in a year or two."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:08fdf3c5-a8c7-4f64-aed8-5c7c9f1bca7e>","<urn:uuid:5a77b3c0-d3ef-4007-b7ba-c1ea26621926>"],"error":null}
{"question":"¿Como empiezan rocky planets like Earth their formation process?","answer":"According to theories, rocky planets like Earth start their formation process as microscopic bits of dust that are tinier than a grain of sand. These dust particles then accumulate and grow larger over time to eventually form planets.","context":["Rocky planets like Earth start out as microscopic bits of dust tinier than a grain of sand, or so theories predict.\nAstronomers using the National Science Foundation’s (NSF) Green Bank Telescope (GBT) have discovered that filaments of star-forming gas near the Orion Nebula may be brimming with pebble-size particles — planetary building blocks 100 to 1,000 times larger than the dust grains typically found around protostars. If confirmed, these dense ribbons of rocky material may well represent a new, mid-size class of interstellar particles that could help jump-start planet formation.\n“The large dust grains seen by the GBT would suggest that at least some protostars may arise in a more nurturing environment for planets,” said Scott Schnee, an astronomer with the National Radio Astronomy Observatory (NRAO) in Charlottesville, Virginia. “After all, if you want to build a house, it’s best to start with bricks rather than gravel, and something similar can be said for planet formation.”\nThe new GBT observations extend across the northern portion of the Orion Molecular Cloud Complex, a star-forming region that includes the famed Orion Nebula. The star-forming material in the section studied by the GBT, called OMC-2/3, has condensed into long, dust-rich filaments. The filaments are dotted with many dense knots known as cores. Some of the cores are just starting to coalesce while others have begun to form protostars — the first early concentrations of dust and gas along the path to star formation. Astronomers speculate that in the next 100,000 to 1 million years, this area will likely evolve into a new star cluster. The OMC-2/3 region is located approximately 1,500 light-years from Earth and is roughly 10 light-years long.\nBased on earlier maps of this region made with the IRAM 30 meter radio telescope in Spain, the astronomers expected to find a certain brightness to the dust emission when they observed the filaments at slightly longer wavelengths with the GBT.\nInstead, the GBT discovered that the area was shining much brighter than expected in millimeter-wavelength light.\n“This means that the material in this region has different properties than would be expected for normal interstellar dust,” noted Schnee. “In particular, since the particles are more efficient than expected at emitting at millimeter wavelengths, the grains are very likely to be at least a millimeter, and possibly as large as a centimeter across, or roughly the size of a small Lego-style building block.”\nThough incredibly small compared to even the most modest of asteroids, dust grains on the order of a few millimeters to a centimeter are incredibly large for such young star-forming regions. Due to the unique environment in the Orion Molecular Cloud Complex, the researchers propose two intriguing theories for their origin.\nThe first is that the filaments themselves helped the dust grains grow to such unusual proportions. These regions, compared to molecular clouds in general, have lower temperatures, higher densities, and lower velocities — all of which would encourage grain growth.\nThe second scenario is that the rocky particles originally grew inside a previous generation of cores or perhaps even protoplanetary disks. The material could then have escaped back into the surrounding molecular cloud rather than becoming part of the original newly forming star system.\n“Rather than typical interstellar dust, these researchers appear to have detected vast streamers of gravel — essentially a long and winding road in space,” said NRAO astronomer Jay Lockman, who was not involved in these observations. “We’ve known about dust specks and we have known that there are things the size of asteroids and planets, but if we can confirm these results it would add a new population of rocky particles to interstellar space.”\nThe most recent data were taken with the Green Bank Telescope’s high frequency imaging camera, MUSTANG. These data were compared with earlier studies as well as temperature estimates obtain from observations of ammonia molecules in the clouds.\n“Though our results suggest the presence of unexpectedly large dust grains, measuring the mass of dust is not a straightforward process and there could be other explanations for the bright signature we detected in the emission from the Orion Molecular Cloud,” concluded Brian Mason, an astronomer at the NRAO and co-author on the paper. “Our team continues to study this fascinating area. Since it contains one of the highest concentrations of protostars of any nearby molecular cloud it will continue to excite the curiosity of astronomers.”\nA paper detailing these results is accepted for publication in the Monthly Notices of the Royal Astronomical Society.\nThe GBT is the world’s largest fully steerable radio telescope. Its location in the National Radio Quiet Zone and the West Virginia Radio Astronomy Zone protects the incredibly sensitive telescope from unwanted radio interference.\nLater this year, the GBT will receive two new, more advanced high frequency cameras: MUSTANG-1.5, the even-more-sensitive successor to MUSTANG, and ARGUS, a camera designed for mapping the distribution of organic molecules in space."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:102cecf9-0919-40e5-a974-230c05ea39e2>"],"error":null}
{"question":"What is the relationship between Russia's changing role since 2010 and environmental security challenges in shaping NATO's future strategy? 俄罗斯自2010年以来角色的变化与环境安全挑战如何共同塑造北约未来战略？","answer":"Russia's changed behavior since 2010, marked by the illegal annexation of Crimea and nuclear threats against Western neighbors, represents a direct military challenge to NATO, while environmental security presents a different type of threat multiplier that requires adaptation in military operations and humanitarian response capabilities. While Russia's actions demanded immediate military responses and changes to NATO's defensive posture, environmental challenges are driving longer-term changes in how NATO conducts missions, including increased focus on humanitarian assistance, disaster relief, and adaptation of military installations and equipment to extreme weather conditions. Both challenges are reshaping NATO's strategic thinking, but while the Russian threat primarily requires traditional military responses, environmental challenges necessitate a broader transformation of NATO's capabilities and partnerships.","context":["NATO: Towards a new Strategic Concept\nThe new threats and challenges the Euro-Atlantic area is facing pose the question of whether it is the right moment for a new Strategic Concept to be developed.\nThe Strategic Concept is NATO’s authoritative strategic document, which describes the Alliance’s goals, challenges and means to meet its security goals and requirements. Strategic Concepts are updated more or less every 10 years to take account of changes in the global security landscape and to ensure that the Alliance is prepared to fulfil its tasks. This article briefly analyses the changed threats and challenges NATO faces since the publication of the current Strategic Concept (2010) and argues why it should develop a new document anytime soon.\nThe current Strategic Concept\nThe Strategic Concept published at the Lisbon summit in 2010 reflects the experiences and lessons learnt from 9/11 and the fight against Islamic terrorism, especially the NATO-led ISAF mission in Afghanistan. The three NATO essential core tasks underlined in it, are the principles of collective defence, crisis management and cooperative security. Plus, it emphasizes solidarity between allies, the importance of transatlantic consultation and the need to engage in reform processes. Among the threats to be faced by the Alliance from 2010 onwards, the Strategic Concept includes the proliferation of ballistic missiles and nuclear weapons, terrorism, cyber-attacks and various environmental problems. When promoting security, a key element is cooperation, which must take place at different levels and areas, such as at the reinforcement of arms control, through disarmament and non-proliferation efforts and by enhancing partnerships with different actors and in different regions of the world, among others.\nWhat’s new since 2010?\nToday, seven years after the current Strategic Concept was published, there are several main reasons to argue that the security environment has changed. The first of them is Russia’s changed role and behaviour. In 2010, NATO aimed for a true strategic partnership with Russia. Some of the members from Eastern Europe took a critical stance on Russia, but the majority agreed on the idea of a common European security order that included it. However, with the illegal annexation of Crimea in 2014, Putin did not only alter the basis for such a cooperative relation but turned against the European security order itself. It is also important to note that Russia’s changed behaviour does not only apply to Ukraine. Nuclear threats against Western neighbours, test flights with nuclear bomber aircrafts or President Putin’s announcement that Russia could overrun the Baltic States within a matter of days have contributed to the crisis in Eastern Europe as well.\nAnother reason why the security environment has changed is the instability in the Middle East and North of Africa (MENA) region. Countries that are usually defined as belonging to MENA include Afghanistan, Algeria, Egypt, Iraq, Jordan, Syria or Yemen, among others. Given continuous civil wars and conflicts taking place in the North of Africa and the Middle East, the rush of migrants has been increasing since 2010. Plus, major cities in the Euro-Atlantic area have suffered terrorist attacks from the so called “Islamic State” (IS) of Iraq and the Levant. NATO works very hard to provide stability on the Eastern flank, but it must also address the needs of member states on the Southern flank in order not to get obsolete, and to be able to fulfil its primary task, guaranteeing security for its members. In this context of MENA, it is argued that an article 5 threat would be possible in a scenario of a hypothetical devastating attack by IS against, for example, a NATO member such as Turkey (if the consequences were analogous to a military aggression).\nIt is also important to consider the European Union’s new situation after the United Kingdom’s decision to leave. The UK still wants to be involved in Euro-Atlantic security affairs –being a properly functioning NATO central to May’s view of British security-, and so does the EU, which has now lost its strongest military power. For this reason, the European Union would be more willing than ever before to cooperate with NATO. The EU has already started to catch up on security matters, for instance with the creation of a defence fund to streamline military research, development and procurement. This and other actions would make the European Union a more attractive partner for NATO.\nImportant also is the election of Donald Trump as the new President of the United States. After his dubious and polemic declarations during the electoral campaign and his first months in the presidency, Trump is implementing all US commitments to NATO, and he has even asked the Congress to increase the budget of the European Reassurance Initiative (supporting the US security presence in Europe) from $3.4 billion to $4.8 billion. It can thus be said that, so far and in spite of his unpredictability and discontent with the current state of the 2% initiative, Trump’s administration is one of continuity when it comes to security policy, as was his predecessor’s.\nFurther challenges to be considered are developments in the Asia-Pacific region, such as the economic and military growth of China or the presence of nuclear powers, such as North Korea, India or Pakistan. In a region where also the United States is exerting its influence, the potential for violent conflict is real.\nIs it the right time for a new Strategic Concept?\nAll the above mentioned are, in addition to issues that should be reflected on a new Strategic Concept, reasons to actually write and publish a new document. However, there are some reasons to believe that this might not be the best time to do so. The first of them is that there is always a risk with getting a document started. Once the drafting begins, the old document must be replaced by a new one, otherwise NATO’s credibility and unity would be doubted. Secondly, the relationship between member states and the trust between them is particularly complicated at the moment. This is mainly due to some allies’ lack of faith and trust on Trump’s USA, the different strategic priorities of member states and the already existing East-South gap, Turkey’s recent developments, etc. This kind of uncertainty is argued not to create the circumstances in which NATO member states could agree on a new strategy. The main argument here is that a debate around a new Strategic Concept would increase tensions, and it would weaken allied solidarity instead of strengthening it. The final major counterargument to a new Strategic Concept is that since the Alliance already reacted to major changes at the Wales and Warsaw summits, through the publication of elaborate declarations and working papers, the revision of the Strategic Concept is not as urgent as it might seem, given that these documents can for now be used as strategic guidelines.\nNew Strategic Concept- better than do nothing?\nThe arguments fearing a new Strategic Concept are right and important to take into account. However, in the light of such changes and new challenges, it seems essential to adapt NATO’s strategic foundations to the new situation. Avoiding a debate for the sake of avoiding disagreement or looking weak should not stop an organization that has often been characterised as the institutionalization of transatlantic dispute, and which still has survived for almost 70 years now. Developing a new Strategic Concept is argued to be better than not doing anything because inaction can also be harmful for NATO, since it would send a signal of insecurity and self-doubt. In this context, it would be desirable to have a policy debate on the future tasks of the Alliance that would lead to a generally accepted new Strategic Concept. The North Atlantic Council recently announced that Secretary General Jens Stoltenberg will continue at the head of NATO until 2020, which means he will be in charge of leading the transition towards a new Strategic Concept, if this is to take place in the next few years.\nAbout the author: Rosario Soler is a Bachelor student of International Studies at Universidad Carlos III de Madrid and was a Research Assistant Intern at the IIR.","Image courtesy of Lisa Ferdinando/DVIDS.\nEnvironmental change1 is increasingly recognised as one of the major factors that will shape the global security environment. According to most experts, rising global temperatures will lead to rising sea levels and cause more extreme weather events, such as storms, flooding, droughts and wildfires.2 The firestorms that engulfed parts of Australia in late 2019 and early 2020, burning an area the size of Belgium and Denmark combined, and severely decimating that continent’s wildlife, were a stark reminder of the force of these changes.\nWhile the causal relationship between environmental change and conflict is difficult to establish, there have been arguably several conflicts where environmental change has acted as a trigger, notably Darfur and Somalia. Even the beginning of the Arab Spring has been related to environmental change: unrest erupted because of increasing food prices, which in turn were the result of several bad harvests attributed to climate change.3 In general, there is a widely held assumption that environmental change could lead to food and water shortages, pandemic diseases, mass migration, and humanitarian disasters.\nEnvironmental changes will also influence the way in which military forces conduct their missions. For example, the military could be called upon more often to provide humanitarian assistance and disaster relief. Other consequences for the military are the vulnerability of coastal installations to rising sea levels and the impact of floods, wildfires, and more extreme temperatures on military exercises and supply chains. Finally, as climatic changes open up regions hitherto largely closed to human activity (e.g. the Artic) the military will need to operate in these challenging environments.\nNATO is not the first responder to climate change. This role is played by other international bodies, in particular those who can set limits on CO2 emissions. However, as the premier transatlantic security and defence organization, NATO would seem an appropriate forum for discussing the security implications of environmental change. After all, NATO offers a seamless continuum of political consultation and decision-making, military planning and military implementation.\nIn addition, through its various committees and agencies NATO is covering many non-military, security-related subjects that range from intelligence sharing to civil emergency planning, and from environmental to medical issues. Moreover, as a democratic Alliance NATO is answerable to public concerns, in particular if these concerns have a genuine security dimension.\nNATO’s environmental security “acquis”\nIn NATO’s 2010 Strategic Concept, Allies for the first time acknowledged that “[k]ey environmental and resource constraints, including health risks, climate change, water scarcity and increasing energy needs will further shape the future security environment and have the potential to significantly affect NATO planning and operations”.4 This formulation already hinted at two distinct dimensions of environmental security: as a “threat multiplier” that generates new security challenges or aggravates existing ones, and as a phenomenon that impacts on the nature and conduct of NATO’s military operations.5 Both dimensions lead to different conclusions. While the first puts the onus on prevention and mitigation, the second puts the emphasis on (military) adaptation. Given NATO’s nature as a political-military alliance, its emphasis will inevitably lie on adaptation. However, as a closer look at NATO’s environmental security activities reveals, the Alliance’s agenda also encompasses some preventive elements. Three major areas can be identified: Strategic Awareness, the Military Dimension, and Cooperative Security.\nStrategic Analysis. The security implications of environmental change in terms of climate-induced geopolitical, economic and military shifts are already dealt with in Allied consultations and intelligence-sharing. Moreover, certain NATO Committees in the Civil Emergency Planning domain have been dealing with, inter alia, extreme weather conditions, pandemics, and disaster relief. Also, the Secretary General’s Policy Planning Unit and the Emerging Security Challenges Division are promoting a dialogue with climate experts. NATO’s Allied Command Transformation is conducting work related to the security implications of environmental change through its regular Strategic Foresight Analysis and the Framework for Future Alliance Operations. Finally, the NATO Strategic Direction-South Hub is also undertaking studies on environmental change, notably with a focus on the African continent.\nPolicies and Standards on Environmental Protection. Since the 1970s NATO developed environmental protection guidelines and standards, resulting in an overarching policy (MC469, agreed in 2003). The implementation of this policy is supported by a number of NATO standards, for example on waste management, water treatment and best practice for camps. Furthermore, NATO’s “Policy on Power Generation for Deployed Force Infrastructure” has a strong focus on saving fossil fuel. The rationale of these efforts is both military and political: mission success requires ensuring the public acceptance of deployed NATO forces in the host country. This requires these forces to demonstrate a genuine commitment to the well-being of the local population, including by protecting their environment.\nEducation, Training, and Exercises. The need to prepare for operations in a changing environment is also reflected in NATO’s education and training efforts. Some training tools already exist, for example the Norway-based NATO Centre of Excellence (CoE) for Cold Weather Operations. ln addition to CoEs, national education and training facilities as well as the NATO School in Oberammergau are conducting environmental protection courses that sensitise soldiers and civilians with the stringent requirements for operating deployed camps, protecting cultural property, etc. NATO’s education and training efforts are constantly being adapted to changing needs and requirements. If considered politically and/or militarily desirable, training courses could be devised that focus specifically on climate-related threats and responses.\nLikewise, NATO’s Euro-Atlantic Disaster Response Coordination Centre (EADRCC) is regularly conducting consequence management field exercises involving dozens of Allies and partner countries. The scenarios, some of which are based on environmental challenges faced by the host nations, are designed to strengthen the ability of teams from different nations to cooperate across a wide range of relief operations. These include urban search and rescue, emergency medical teams, as well as detection, protection and decontamination teams. Compared with traditional military exercises, they are smaller (up to 2,000 personnel) and performed in close cooperation with the United Nations Office for the Coordination of Humanitarian Affairs (UN OCHA), which retains the primary role in the coordination of international disaster relief operations.\nThe military dimension\nOperational Planning and Defence Procurement. As Allies acknowledged in the Strategic Concept, climate change and other developments could impact NATO’s operational planning. The Pentagon has repeatedly noted that rising sea levels may impact the execution of amphibious landings; changing temperatures lengthening the arid season could impact operation timing windows; and the increased frequency of extreme weather could limit surveillance and reconnaissance measures.6 Environmental change can also affect military bases (e.g. a rise in the sea level could render important hubs like Diego Garcia unusable). More hostile climatic conditions could also affect both the service life and the maintenance requirements of military equipment (for example, adding dust filters to protect the engines of military vehicles will limit their range and performance). Finally, changing climatic conditions could also lead some Allies to invest in a different force structure, e.g. emphasising helicopters, coast guard vessels and amphibious vehicles over other equipment.\nEnergy Efficiency in the Military. The high fuel demand of combat forces can diminish their performance, increase their vulnerability, and may require the diverting of combat forces to protect supply lines. Hence, in-creased energy efficiency could offer benefits in terms of combat power and agility. NATO’s own work in this regard focuses on reducing the consumption of fossil fuel in deployed force infrastructure (i.e. camps), resulting in more autonomy, a lesser logistical burden and a smaller environmental footprint.7 In early 2014, Allies agreed the “Green Defence Framework”, which sought to bring various internal work strands in NATO closer together in order to create synergies and improve NATO’s “green” profile.8 In addition, questions related to national energy efficiency measures have been included in NATO’s Defence Planning Capability Survey. All these measures are undertaken with military effectiveness in mind. However, there is widespread agreement that a reduced energy footprint of certain military activities offers the additional advantage of demonstrating to a broader public that the military is not indifferent to environmental concerns.\nConsequence Management. While humanitarian relief missions are not considered a core task of NATO, the Alliance has been involved in such missions on several occasions, starting in 1991 to protect the Kurdish population in Northern lraq (“Provide Comfort”); in the aftermath of a severe earthquake in Pakistan in 2005 (where NATO delivered almost 3,500 tons of relief supplies); and after Hurricane Katrina hit the US in 2005. The Euro-Atlantic Disaster Response Coordination Centre also coordinated national assistance on many other occasions (floods, mudflows, wildfires) in Allied and partner countries (e.g. Albania, Bulgaria, Turkey, Algeria, Moldova). Noticeably, thus far, the NATO Response Force, which had initially been conceived as the flagship of NATO’s military transformation, has mostly been deployed in humanitarian relief efforts.\nEnvironmental security as a partnership tool. The partnership dimensions of addressing environmental security risks are several-fold. First, as environmental change affects many partner countries, they will be interested in scientific cooperation with NATO on mitigation measures, but also on consequence management, training and education. Past examples of such scientific cooperation encompassed measures on flood control in Ukraine and preventing desertification in Egypt. Second, helping partner countries build the capacity and resilience to better manage environmental impacts could become a legitimate element in NATO’s Defence Capacity Building approach. Finally, as humanitarian relief oper-ations involve many other international organizations and NGOs, they provide a strong rationale for deepening NATO’s ties with all prospective actors, thereby underlining NATO’s inclusive and comprehensive approach to security.\nScientific Cooperation. NATO’s Science for Peace and Security (SPS) Programme has a long tradition of addressing environmental concerns. Its predecessor, the “Committee on the Challenges of Modern Society”, looked at the challenge of environmental degradation as far back as the late 1960s. Today, SPS brings together scientists from Allied and partner countries, has supported numerous workshops and multi-year projects linked to security issues arising from key environmental and resource constraints, as well as disaster forecasting and the prevention of natural catastrophes, and defence-related environmental issues. Allies’ interest in supporting projects on environmental security has waned in the recent past. Still, SPS remains an important resource for NATO to support workshops and training courses with military and civilian experts addressing geophysical, meteorological and atmospheric/ space-weather phenomena that have an effect on military capabilities.\nThe delicate path ahead\nAs this non-exhaustive list of NATO’s activities demonstrates, the Alliance is already coping with the consequences of environmental change on various levels. Hence, giving this dossier greater emphasis and visibility appears feasible. This could entail, first and foremost, revisiting the aforementioned “Green Defence Framework”, seeking to operationalise its various suggestions in areas such as energy efficiency measures and the sharing of best practices, and perhaps even considering “the applicability of ‘green’ standards and principles across the NATO HQ, NATO Command Structure and NATO Agencies, and […] the applicability of setting up ‘green’ accounting and benchmarks to measure progress”.9 Allies could also support more scientific research projects through NATO’s Science for Peace and Security Programme, seek to elevate the role of environmental security in its dialogue and cooperation with partner countries, enhance NATO’s presence at climate-related events, and initiate a more robust public diplomacy effort. However, simply producing more public statements on the importance of environmental security will not be enough. Successfully raising NATO’s visibility in this domain will depend on a number of important factors.\nFirst, as environmental change touches upon many Allied sensitivities, great care needs to be taken to establish and sustain consensus on this subject. One reason why the 2010 Strategic Concept did not lead to a stronger focus on environmental security was the hesitation of many Allies to become engaged in a subject that could upset the balance of interest in certain regions (e.g. the High North) or risks degenerating into a controversial debate about national environmental or energy policies. In the same vein, Allies’ interest in using the SPS Programme to support scientific projects related to environmental security proved rather uneven in the past, with some Allies questioning whether NATO was the appropriate framework for sponsoring such efforts. In short, without proper handling, these diverging views could quickly re-surface. Hence, all Allies must be reassured that a more visible NATO role in climate and environmental issues will not be detrimental to their national (security) interests.\nA plausible narrative\nSecond, given NATO’s nature as a political-military Alliance, for any NATO narrative on environmental security to be credible, it should be focussed predominantly on consequence management and less on prevention – where NATO’s role is comparatively small. While this may clash with the public sense of urgency about the need to slow down or even arrest environmental change, NATO’s major contribution to security remains in employing its military competence as a “force for good”, be it through deterring major war or offering humanitarian assistance after natural disasters. This contribution to international peace and stability is what makes NATO unique. Hence, any attempt to give NATO more visibility in the environmental field must take care not to send conflicting messages. With public expectations set on mitigation and prevention, Allies must not allow a view to take hold that they have concluded that mitigation efforts will fail and that NATO therefore had to prepare for the worst. At the same time, neither should they give the impression that a stronger focus by NATO on environmental security means moving away from its core business of military deterrence and defence.\nThird, given the highly emotional and at times outright apocalyptic nature of the current debate on environmental change, Allies must resist any temptation to cater to the wilder shores of this debate. After all, irrespective of public expectations regarding reduced greenhouse emissions, the military (notably air forces) will remain a major polluter.10 Stressing NATO’s “greening” efforts is a genuinely positive message, all the more so as it would tie in with the logic of other initiatives, such as the EU’s new “Green Deal”, which aims for the Union to become climate-neutral by 2050. However, its military focus will not allow NATO to formulate similarly ambitious goals. Militaries will have to focus on operational effectiveness, with environmental concerns playing a growing, yet secondary role. Overselling NATO’s contribution to environmental security would run the risk of breeding disappointment or even outright resentment, with hardcore climate activists denouncing such efforts as mere political window-dressing. In short, in discussing environmental security NATO must not be apologetic. Environmental and military security are both of existential importance; they must not be pitted against each other.\nConclusion: setting the stage for a more visible role in environmental security\nThe international discussion on environmental change is now becoming a legitimate part of the security debate. While NATO is not going to be a major factor in the environmental debate, its elevated role in international security demands that it be more than merely a dispassionate observer of that debate. If NATO can demonstrate that in implementing its core mission of deterrence and defence it is conscious of environmental concerns, and that its national militaries have understood the need to make their own contribution, the stage could be set for a more visible role in environmental security. Even if modest, such a more visible role in environmental security would help align NATO with a challenge that a growing number of people are regarding as a major security concern.\nThis article was published under a Creative Common “Attribution-Non Commercial-NoDerivs” Licence. (CC BY-NC-ND)\n1 Although the term “climate change” is more commonly used, it is politically charged, as it is closely connected with the question of whether it is a man-made phenomenon. Hence, this paper mostly uses the term “environmental change”, as it is also more comprehensive.\n2 See International Military Council on Climate and Security, The World Climate and Security Report 2020, February 2020.\n3 See S. Johnstone and J. Mazo, “Global Warming and the Arab Spring”, Survival, April/May 2011, pp.11-17. 4 NATO’s Strategic Concept, 2010, para.15.\n5 See also T. H. Lippert, NATO, climate change and international security: a risk governance approach, Palgrave MacMillan, 2019.\n6 See the numerous examples listed in: Office of the Under Secretary of Defense for Acquisition and Sustainment, “Report on Effects of a Changing Climate to the Department of Defense”, Washington, DC, January 2019.\n7 For a selection of documents and other information on Allies’ and NATO’s work on “smart energy” see the “Smart Energy LibGuide” at http://www.natolibguides.info/smartenergy\n8 Green Defence Framework, February 2014, http://www.natolibguides. info/ld.php?content_id=25285072. Given the novelty of some of the issues addressed in that paper, its rather hesitant tone should not come as a surprise. However, it was Russia’s annexation of Crimea in March 2014 and NATO’s subsequent re-emphasis on collective defence that prevented a more systematic pursuit of the paper’s various innovative elements.\n9 Green Defence Framework, 2014, para.10.\n10 For a typical example see T. Lorincz, “NATO is a threat to the climate”, Ricochet, 29 December 2019.\nAbout the Author\nMichael Rühle is Head, Energy Security Section, in the Emerging Security Challenges Division in NATO’s International Staff.\nFor more information on issues and events that shape our world, please visit the CSS website."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b5e9d007-760b-4575-bea7-d4bd46032941>","<urn:uuid:c393af76-3013-49d5-919b-39a6c9f5c59f>"],"error":null}
{"question":"How does the hierarchical structure of Latin American regional organizations compare to the UN's internal hierarchy of norms in terms of institutional formality and rule-following?","answer":"The UN has a clearly defined hierarchical structure of norms, starting with the Charter at the top, followed by resolutions, staff regulations, rules, bulletins and administrative instructions. In contrast, Latin American regional organizations exhibit low degrees of institutionalization and supranationalism. They typically don't establish hard legal rules and function more as forums for discussion rather than rule-setting institutions. While Latin American countries are capable of following international rules (as shown in their compliance with UN decisions), their regional organizations tend to operate more flexibly, expecting member states to stay within broad ideological parameters rather than strictly following formal rules.","context":["Since the 1960s, Latin America has been characterized as a region with several organizations focused on regional integration. Notwithstanding, this characteristic shows that, when compared to the process of European integration, Latin America lags behind issues such as supranationalism and institutionalism. These two concepts are central to the neo-functionalism, commonly used in research on regional integration. If Latin American regional integration has a low degree of supranationalism and institutionalization, why do regional organizations carry a considerable weight in political debates and are highly politicized?\nTo answer this question, professors Jochen Kleinschmidt, Universidad del Rosario/Colombia, and Pablo Gallego Pérez, Universidad EAFIT/Colombia, propose an analysis from the sociological systems theory and the possible impacts for the future of regionalism in Latin America. Their article entitled Differentiation Theory and the Ontologies of Regionalism in Latin America was published in the issue 1/2017 (Volume 60 – N. 1) of Revista Brasileira de Política Internacional and the authors gave an interview to Guilherme Frizzera is a PhD candidate in International Relations at the University of Brasilia and member of the editorial team of RBPI.\nOne of the criticisms of Latin American regionalism is the concentration of power in the hands of presidents. This characteristic has been present in the regional organizations since former presidents have been chosen to occupy positions of general secretariat or also the model of rotating presidency of the institutions. How much does a technical and internal bureaucracy in organizations impact on the low degree of supranationality or institutional deepening?\nPablo Gallego: Having former presidents take roles in international organizations such as UNASUR or OAS certainly creates some added visibility or prestige for those organizations. In the absence of structured technocratic bureaucracies in Latin American regional organizations, leadership by experienced political operators with large personal networks probably increases their relevance. Without an institutional power base, and considering the presidentialist tendencies of Latin American politics, it may enable them to become involved in multilateral discussions in ways that otherwise would be impossible. On the other hand, of course, those personalistic tendencies may inhibit the development of stronger, institutionalized, more transparent bureaucracies.\nJochen Kleinschmidt: The criticisms made of Latin American presidentialism, ejecutivismo, and related phenomena certainly have their points. Yet, we should not forget that other highly presidentialist systems, such as that of France, have been well able to participate in supranational integration projects. I would argue that the lack of powerful institutions is mainly the result of a lack of economic interdependence between Latin American countries, of a lack of mutual trust, at least in some cases, and perhaps of the absence of shared perceptions regarding security issues. With these factors, of course Latin American regionalism can at times become a playing field for presidents, and sometimes a springboard for their personal ambitions or ideological projects. As we discuss in our article, this is well explained by conventional theories on regionalism.\nThe lack of supranationality and institutional deepening of larger or classical organizations (eg: UNASUR, MERCOSUR and CAN) could be an induction factor for subregional projects (eg: Pacific Alliance, ALBA)?\nPablo Gallego: It’s a plausible hypothesis. We can draw a parallel to how failure of the Doha rounds in the World Trade Organization actually led several countries, including Latin American countries, to pursue regional trade agreements like FTAs or regional economic alliances like Pacific Alliance. The failure of a large structure like that WTO – and in our specific case, UNASUR or OAS – could lead countries to pursue their specific interests within smaller, potentially more efficient projects.\nJochen Kleinschmidt: Of course, the hypothesis implies that those smaller organizations should then display a momentum towards institutional deepening. This has not been the case so far. I think the cause for the emergence of smaller projects, such as the Pacific Alliance or ALBA, rather lies in their strong ideological convergence. As we argue in our article, instead of providing supranational structures to regulate economic and other forms of interdependence, Latin American regionalism has evolved into a platform for achieving transnational constitutional legitimation for different answers to the problem of societal inclusion. You could say that overall, Mercosur has pursued a more classical, protectionist modernization approach, ALBA a radical, anti-hegemonic model, and the Pacific Alliance an orthodox liberal one. These models obviously are highly politicized, and therefore tend to express themselves in politically more homogenous communities rather than in politically diverse structures such as UNASUR.\nOrganizations go beyond a place that facilitates communication and practices among its members. They also create norms and rules, generating both a process of socialization and embarrassment. The degree of internalization of these rules and regulations are low in Latin America or are different for each organization?\nPablo Gallego: Many Latin American organizations, like the Pacific Alliance or ALBA, are not created to establish hard legal rules. Rather, they propose a general normative outline of how progress might be achieved and then take tentative steps to work in the desired direction, which are generally organized in an intergovernmental fashion. The larger organizations, like CELAC or UNASUR, are generally designed as fora, not as rule-setting institutions. The irony is that Latin American countries, when dealing with the UN, for example, those same countries are often willing and able to internalize any decision made within a global setting. A similar dynamic can be observed when regional organizations organize transnational intra-elite cooperation, for example, within the judicial sector.\nJochen Kleinschmidt: To take up that argument – the problem is not that Latin American countries are unwilling or incapable of following rules generated on an international level. It is rather that the specifically Latin American organizations tend not to follow that supranational, EU-type model. The expectation seems to be that member states – for example of Mercosur or the Pacific Alliance – should stay within the broad ideological parameters associated with that organization, and then behave in a basically cooperative fashion, but do so without necessarily following all the rules to the letter. I think we do see some indicators that when countries leave the general ideological consensus of an organization, their membership also becomes problematic in some way, if you consider the cases of Venezuela and Paraguay in Mercosur. The flirtations between Argentina and the Pacific Alliance after the election of Macri point in a similar direction. In general, we should, to some degree, expect a flexible geometry of regionalist projects, which will sometimes follow ideological currents within Latin American states.\nThe neo-functionalism provides a sufficiently adequate explanation for the project of European integration, but that would not be enough to explain regionalism in Latin America. How could Latin American regionalism contribute to a new theoretical and methodological perspective for regional integration literature?\nPablo Gallego: If anything, the Latin American case points toward a need to reconsider what states are looking for in regionalist projects. Its regional organizations differ from those of other regions in that they prioritize mutual legitimation over material interdependence. Critics might say that those efforts serve as “promotional ads” for politicians of each country to show their efforts, more so than actually using these groups for actually getting something done. However, as we argue in the article, some of the organization could serve a very real need for generating societal legitimation in countries that are undergoing rapid societal transformations.\nJochen Kleinschmidt: Already some time ago, authors like Amitav Acharya pointed to the need to understand the different regionalisms of the world on their own terms, and not as attempts simply to copy the European Union. I think our research underlines that need, and points to some real theoretical possibilities to conceptualize regionalisms that diverge from the neo-functionalist archetype. Moreover, as the European Union model as well as the traditional liberal ideas underlying FTAs such as NAFTA are losing their intellectual hegemony, I think there will be greater interest in alternative conceptualizations of how regions become meaningful in global politics. The Latin American case might be a poster child of this development precisely because of its rich history of regionalist projects that cannot be conceptualized in a neo-functionalist framework. And world society theory may provide a suitable theoretical setting for future research that goes beyond integrationism, and enables interesting cross-regional comparisons.\nRead the article\nKleinschmidt, Jochen, & Gallego Pérez, Pablo. (2017). Differentiation theory and the ontologies of regionalism in Latin America. Revista Brasileira de Política Internacional, 60(1), e017. Epub October 23, 2017.https://dx.doi.org/10.1590/0034-73292017001018\nAbout the authors\nJochen Kleinschmidt – Universidad del Rosario,Facultad de Ciencias Políticas, Bogota, Colombia (firstname.lastname@example.org);\nPablo Gallego Pérez – Universidad EAFIT, Departamento de Negocios Internacionales, Medellin, Colombia (email@example.com);\nGuilherme Frizzera is a PhD candidate in International Relations at the University of Brasilia and member of the editorial team of RBPI.","What is the hierarchy of norms in the UN legal order?\nLast Updated: Oct 15, 2019\nA \"hierachy of norms\" refers to the order or importance in which a norm is considered within a legal system. For matters internal to the organization, the hierarchy of norms in the UN has been set out in certain judgments of the Dispute Tribunal as follows:\n- Charter of the United Nations\n- Resolutions and decisions\n- Staff regulations\n- Staff rules\n- Secretary-General's bulletins\n- Administrative instructions\nFrom Villamoran v. Secretary-General of the United Nations (UNDT/2011/056), paragraph 29:\nAt the top of the hierarchy of the Organization’s internal legislation is the Charter of the United Nations, followed by resolutions of the General Assembly, staff regulations, staff rules, Secretary-General’s bulletins, and administrative instructions (see Hastings UNDT/2009/030, affirmed in Hastings 2011-UNAT-109; Amar UNDT/2011/040). Information circulars, office guidelines, manuals, and memoranda are at the very bottom of this hierarchy and lack the legal authority vested in properly promulgated administrative issuances.\nLegal opinions of the Secretariat and Advisory Opinions of the International Court of Justice may also provide information on the interpretation of legal matters within the UN.\n- The UN Juridical Yearbook includes, among other useful information:\n- A general review of the legal activities of the UN\n- Selected decisions of the administrative tribunals of the UN\n- Selected legal opinions of the secretariats of the UN\n- Selected judicial decisions on questions relating to the UN from both international and national tribunals\nMore broadly, Schermers and Blokker, in International Institutional Law, 5th rev. ed, 2011, section 1145, state:\nThe constitution sets the pattern for the legal order of the international organization. Further rules develop from the operation of its organs. The power of these organs to take decisions stems from the constitution. From this common source, a hierarchy of the various legal rules is developed, and a single legal order thus created. The validity of each rule will depend on the constitution, the basis of the legal order.\nIn the larger context of the international community and its legal order, the role of the Charter in public international law in general is a topic of scholarly debate. Many books, articles, courses and lectures concern the position of the UN Charter within the hierarchy (or hierarchies) of norms in public international law.\nSome key terms are: hierarchy of norms, conflict of norms, fragmentation of international law, unity of international law, interpretation, jus cogens, obligations, international system, international legal order, sources of international law\nSome key provisions are:\n- Article 38 of the Statute of the International Court of Justice, concerns sources of international law\n- Article 103 of the Charter, concerns conflict between the Charter and other treaty obligations\n- Article 30 of the Vienna Convention on the Law of Treaties, concerns Article 103 of the Charter and other treaties\n- Articles 31-33 of the Vienna Convention on the Law of Treaties, concern interpretation of treaties\nNational and international courts and tribunals may be called upon to interpret treaty provisions\n- For example, see discussions of the Kadi cases in the European Union courts, which concerned, in part, the relationship between obligations of UN Member States under Chapter VII of the Charter and their obligations under other elements of the international legal order, such as EU regulations.\nDisclaimer: answers are prepared by library staff using resources available at the time of writing. This site may include links and references to third-party databases, websites, books and articles, this does not imply endorsement by the United Nations."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:14ab325a-19f5-41ec-acfa-09ff2cb03614>","<urn:uuid:4421b097-29d4-44a8-9f06-7a8421db0919>"],"error":null}
{"question":"What are two main groups most vulnerable to the negative effects of climate change?","answer":"The elderly and young children are the populations that will bear the most burden of climate change, due to their weak immune systems. The poor are also particularly vulnerable as they are unable to put mitigation strategies in place to protect themselves from the negative effects of climate change.","context":["Policy Brief on Climate Change Name Institution Instructor Course Date Policy Brief on Climate Change Background Information Climate is one of the major factors that affect the health of all individuals across the globe\nPolicy Brief on Climate Change\nPolicy Brief on Climate Change\nClimate is one of the major factors that affect the health of all individuals across the globe. Climate can be described as the weather patterns in a given area, which have been identified over a long time (Burch, Harris, & University of Toronto Press, 2016). Different parts of the continent have different types of climate. When climate changes to the extreme, it will affect the human health, as extreme heat will increase the transmission of certain diseases while extreme cold will also affect the human health negatively (Burch, Harris, & University of Toronto Press, 2016). Some of the consequences of climate change include flooding and drought, which all will affect individual’s health negatively. Climate change also affects the basic needs necessary for the well-being of all individuals, which may include food, water, and the air which are important for the daily survival of all living creatures (Burch, Harris, ; University of Toronto Press, 2016).\nWhen these basic needs are affected, the human health also bears the consequences (Black, 2013). The issue of climate change must be considered and addressed due to the negative impacts it has on all humans across the globe. Climate change is a health issue that every nation should be keen to address and measures to protect the environment from a climate change should be implemented (Black, 2013). The major cause of climate change is human activities and all government and authorities should put measures to protect the environment from human activities that may have negative effects on the weather (Black, 2013). The populations that will bear the most burden of climate change are the elderly and the young children due to their weak immune systems and the poor who are unable to put mitigation strategies to protect themselves from the negative effects of climate change (Black, 2013).\nThe extreme weather changes may even lead to death or injury for the poor population, as they are unable to protect themselves from the events that may be caused by extreme weather changes (Burch, Harris, ; University of Toronto Press, 2016). The United States has enacted measures at the national, state, and local levels to address the issue of climate change in the country. This will ensure that the nation addresses any arising threats to the climate at all levels by ensuring coordination and support between the levels (Burch, Harris, ; University of Toronto Press, 2016).\nPolicies are made for all levels considering the risks at the specific levels and mitigation strategies are implemented to prevent any negative human activities that may lead to the negative effects of climate change (Black, 2013). This is important as state and local representatives are allowed to participate and air their opinions, as they are the ones responsible in enforcing and implementing any policies regarding climate protection (Black, 2013). The constituencies and the states are diverse and have different weather patterns and so it is necessary to involve them in policy making for the betterment of the nation (Black, 2013).\nClimate change has affected the whole globe with rising temperatures which are negatively affecting all population across the globe and this is due to human activities which pollute the environment affecting its normal functioning and this has led to disasters in some parts of the world that are unable to implement preventive strategies to protect their citizens (Hanusch, 2018). Following these negative effects caused by climate change, all nations should put protective policies on their resources to prevent human activities from exploiting these resources, which has led to the troubles related to climate change (Hanusch, 2018). These human activities have led to the degrading of certain regions exposing them to the negative effects of extreme weather effects. These negative effects have affected the human health and this has a negative outcome for any country as it tries to reverse the negative side effects caused by these extreme weather changes (Hanusch, 2018).\nMost governments spend a large part of their budget in solving these negative situations and addressing the health issues of its citizens caused by the climate changes (Hanusch, 2018). Climate change also has a negative impact on agriculture, which is important for any country as agriculture provides food for its citizens, and if this is affected, the health status of all individuals will be impacted as well as the motion’s economic budget in its provision of the basic need to its citizens. Due to this, nations must address the issue of climate change at all levels to ensure that the world is stable from any negative effects of climate change (Hanusch, 2018).\nSuggestions for Addressing the Issue (Solutions) – (a) Including Necessary Stakeholders (Government Officials, Administrator); and (b) Include Budget or Funding Considerations, If Applicable\nClimate change can be addressed by educating the public about the negative effects of climate change, which are caused by human activities (Reid, 2014). Education and training programs should be planned and implemented across all countries across the globe since climate change is a global concern and a single nation cannot be successful in mitigating the issue alone (Reid, 2014). These programs will help the public to be aware of their activities and be on the watch out for any activities that may lead to the negative effects (Reid, 2014).\nThe United States government should help its citizens in adapting to new weather changes and strategies to help the nation in times of disasters should be planned (Reid, 2014). Weather changes should be monitored to avoid unexpected weather changes, which can negatively impact a nation (Reid, 2014). The government and administrators should implement strict measures on environment protection against pollution, which largely contributes to climate change (Reid, 2014). A budget should also be planned to address emergencies and to support organizations, which help protect the environment and enforcing policies and laws regarding environment protection (Reid, 2014).\nImpact of climate change on the Health Care Delivery System\nThe health care system aims at providing quality healthcare to the citizens of the United States (Maslin, 2014). With the negative effects of climate change, the number of patients will increase impacting the healthcare delivery system negatively as it has to provide health care services to its population regardless the number and this will negatively impact the quality of healthcare services due to the limited resources in healthcare facilities (Maslin, 2014). Due to the climate change affecting food and water security, health care delivery system will have more diverse issues, which will involve patients without food and other basic needs, which will affect the goals and objectives of the health care delivery system of bettering the health status of the citizens of the country (Maslin, 2014).\nNegative effects that occur due to climate change, which may include floods and storms, may affect healthcare facilities affecting their power and other basic equipment, which can worsen the situation due to the difficult of administering healthcare services to the citizens (Maslin, 2014). Due to this, health care facilities need to put more resources in their preparation for natural disasters brought about by climate changes and educate health care practitioners on what procedures to undertake in times of disasters (Maslin, 2014).\nBlack, B. (2013). Climate change: An encyclopedia of science and history. Santa Barbara, California: ABC-CLIO.\nBurch, R. W., Harris, S. E., & University of Toronto Press. (2016). Understanding climate change: Science, policy, and practice. Toronto: University of Toronto Press.\nHanusch, F. (2018). Democracy and climate change. London: Routledge, Taylor, & Francis Group.\nMaslin, M. (2014). Climate change: A very short introduction. Oxford, United Kingdom: Oxford University Press.\nReid, H. (2014). Climate change and human development. London: Zed Books."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:59855dfe-fbad-4e8d-b422-86a7bb9ce2eb>"],"error":null}
{"question":"How does mechanical stress influence cell behavior in fibrosis, and what pulmonary consequences can result from this condition?","answer":"Mechanical stress affects cells through tensional homeostasis, which regulates essential cellular features like shape, motility, proliferation, and differentiation. The MRTF-A/SRF pathway acts as a biosensor for tensional homeostasis in cells. When these mechanical processes are disrupted in conditions like fibrosis, they can lead to serious pulmonary consequences including poor lung function, increased risk of infections, and decreased exercise tolerance. These patients require monitoring through pulmonary function testing, including diffusing capacity of carbon monoxide and ratio of residual volume to total lung capacity measurements.","context":["Dr Maryse Bailly\nMy group uses a range of cellular models to study physiological and pathological cell behaviour regulated by actin dynamics in the context of tissue contraction and scarring. The common long-term goal of these studies is to identify basic molecular mechanisms to uncover new specific targets to modulate scarring and fibrosis in a range of diseases as well as within cell repair and regeneration processes. The lab most recent interests broadly fall under two areas:\n1) Mechanisms of fibroblast-mediated contraction in wound healing and fibrosis:\nTissue contraction and scarring processes play a part in the pathogenesis or failure of treatment of virtually every major blinding disease, and are also a major cause of morbidity in the whole human body. Yet, very little is know about the molecular mechanisms involved and there are no treatment available for ocular scarring besides toxic anti-cancer drugs with limited use. My group has developed over the last 10 years in vitro and ex vivo models that allow the study tissue contraction mechanisms within pseudo-physiological 3D “tissue-like” environments. Focusing on ocular diseases, the group has used these models to characterise the biology of the contraction process and the molecular pathways underlying conjunctiva contraction, identifying novel targets such as the Rac1 small GTPase to develop promising anti-scarring agents. More recently we have extended this work to specific eye diseases with a major fibrotic component such as Floppy Eye Lid Syndrome (FES), Thyroid Eye Disease (TED) and recurrent trachomatous trichiasis (TT), where the disease mechanisms are unknown and no good treatment is available. These studies have identified a specific molecular signature underlying the fibrotic disease phenotype, and demonstrated that the diseased cells display specific alterations in their mechanical properties that underlie they ability to contract tissues and promote scarring.\n2) Tensional homeostasis, mechanical stress and modulation of the SRF/MRTF-A pathway in fibrosis:\nMechanical sensing is a fundamental process that regulates essential cellular features such as shape, motility, proliferation or differentiation. Deregulation of the tissue tensional homeostasis in stromal cells is at the basis of most stress-induced pathological remodelling, such as cardiac hypertrophy, fibrosis and scarring. We have recently identified Myocardin-Related Transcription Factor-A (MRTF-A), the major co-activator for the Serum Response Factor (SRF) transcription factor, as a biosensor for tensional homeostasis in cells. Our goal is now to identify the mechanisms by which cells measure their internal tension and what actually determines the intrinsic tensional homeostasis level in individual cells, and the role of the MRTF-A/SRF pathway in the process.\nActin dynamics in cell motility and tissue mechani\nBlinding diseases (genetics and mechanisms)\nI teach a number of lectures at UCL, broadly around cytoskeleton, models and imaging.\nANAT3050- Actin cytoskeleton and Intermediate Filaments\nCELL2006- CELL2008: Models organisms and techniques\nMSc Biology of Vision- Ocular Cell Biology: Cytoskeleton and Models\nProf Alison Hardcastle; Prof Mike Cheetham; Professor Sir Peng Khaw; Prof Philip Beales; Prof Andrew Forge; Prof Tony Moore; Prof Steve Wilson; Prof Andrew Webster; Prof Susan Povey\n- Li H, Ezra DG, Burton MJ, Bailly M (2013). Doxycycline prevents matrix remodeling and contraction by trichiasis-derived conjunctival fibroblasts.. Invest Ophthalmol Vis Sci, , - . doi:10.1167/iovs.13-11787\n- Anandagoda N, Ezra DG, Cheema U, Bailly M, Brown RA (2012). Hyaluronan hydration generates three-dimensional meso-scale structure in engineered collagen tissues.. J R Soc Interface, 9(75), 2680 - 2687. doi:10.1098/rsif.2012.0164\n- Terry SJ, Elbediwy A, Zihni C, Harris A, Bailly M, Charras G, Balda M, Matter K (2012). Stimulation of cortical myosin phosphorylation by p114RhoGEF drives cell migration and tumor cell invasion. PLoS One, In press, - .\n- Ezra DG, Krell J, Rose GE, Bailly M, Stebbing J, Castellano L (2012). Transcriptome-level microarray expression profiling implicates IGF-1 and Wnt signalling dysregulation in the pathogenesis of thyroid-associated orbitopathy.. J Clin Pathol, 65(7), 608 - 613. doi:10.1136/jclinpath-2012-200719\n- Tovell VE, Chau CY, Khaw PT, Bailly M (2012). Rac1 inhibition prevents tissue contraction and MMP mediated matrix remodeling in the conjunctiva.. Invest Ophthalmol Vis Sci, 53(8), 4682 - 4691. doi:10.1167/iovs.11-8577\n- Nola S, Daigaku R, Smolarczyk K, Carstens M, Martin-Martin B, Longmore G, Bailly M, Braga VM (2011). Ajuba is required for Rac activation and maintenance of E-cadherin adhesion.. J Cell Biol, 195(5), 855 - 871. doi:10.1083/jcb.201107162\n- Martin-Martin B, Tovell V, Dahlmann-Noor AH, Khaw PT, Bailly M (2011). The effect of MMP inhibitor GM6001 on early fibroblast-mediated collagen matrix contraction is correlated to a decrease in cell protrusive activity.. Eur J Cell Biol, 90(1), 26 - 36. doi:10.1016/j.ejcb.2010.09.008\n- McGee KM, Vartiainen MK, Khaw PT, Treisman R, Bailly M (2011). Nuclear transport of the serum response factor coactivator MRTF-A is downregulated at tensional homeostasis.. EMBO Rep, 12(9), 963 - 970. doi:10.1038/embor.2011.141\n- Ezra DG, Ellis JS, Gaughan C, Beaconsfield M, Collin R, Bunce C, Bailly M, Luthert P (2011). Changes in tarsal plate fibrillar collagens and elastic fibre phenotype in floppy eyelid syndrome. Clinical and Experimental Ophthalmology, , - .\n- Tovell VE, Dahlmann-Noor AH, Khaw PT, Bailly M (2011). Advancing the treatment of conjunctival scarring: a novel ex vivo model.. Arch Ophthalmol, 129(5), 619 - 627. doi:10.1001/archophthalmol.2011.91\n- Ezra DG, Ellis JS, Beaconsfield M, Collin R, Bailly M (2010). Changes in fibroblast mechanostat set point and mechanosensitivity: an adaptive response to mechanical stress in floppy eyelid syndrome.. Invest Ophthalmol Vis Sci, 51(8), 3853 - 3863. doi:10.1167/iovs.09-4724\n- Brooks SP, Coccia M, Tang HR, Kanuga N, Machesky LM, Bailly M, Cheetham ME, Hardcastle AJ (2010). The Nance-Horan Syndrome protein encodes a functional WAVE Homology Domain (WHD) and is important for co-ordinating actin remodelling and maintaining cell morphology. Hum.Mol.Genet., , - .\n- Dahlmann-Noor AH, Cosgrave E, Lowe S, Bailly M, Vivian AJ (2009). Brimonidine and apraclonidine as vasoconstrictors in adjustable strabismus surgery. J.AAPOS., 13(2), 123 - 126.\n- Hayes MJ, Shao DM, Grieve A, Levine T, Bailly M, Moss SE (2009). Annexin A2 at the interface between F-actin and membranes enriched in phosphatidylinositol 4,5,-bisphosphate. Biochim.Biophys.Acta, 1793(6), 1086 - 1095.\n- Hayes MJ, Shao DM, Grieve A, Levine T, Bailly M, Moss SE (2008). Annexin A2 at the interface between F-actin and membranes enriched in phosphatidylinositol 4,5,-bisphosphate. Biochim.Biophys.Acta, , - .\n- Saleem MA, Zavadil J, Bailly M, McGee K, Witherden IR, Pavenstadt H, Hsu H, Sanday J, Satchell SC, Lennon R, Ni L, Bottinger EP, Mundel P, Mathieson PW (2008). The molecular and functional phenotype of glomerular podocytes reveals key features of contractile smooth muscle cells. Am.J.Physiol Renal Physiol, 295(4), F959 - F970.\n- Brooks SP, Coccia M, Kanuga N, Bailly M, Cheetham ME, Hardcastle AJ (2008). The Nance-Horan Syndrome protein (NHS) is a member of a new protein family and is localised at epithelial cell junctions..\n- Dahlmann-Noor AH, Martin-Martin B, Eastwood M, Khaw PT, Bailly M (2007). Dynamic protrusive cell behaviour generates force and drives early matrix contraction by fibroblasts. Experimental Cell Research, , - . doi:10.1016/j.yexcr.2007.07.040\n- Bailly M (2007). Moving away from death: when capsase-11 meets cofilin.. Nature Cell Biology, 9, 245 - 246. doi:10.1038/ncb0307-245\n- Hayes MJ, Shao DM, Bailly M, Moss SE (2006). Regulation of actin dynamics by annexin 2. EMBO J, 25(9), 1816 - 1826. doi:10.1038/sj.emboj.761078\n- Bailly M (2006). Protrusive activity as the driving force behind growth factor-mediated tissue contraction.\n- Hayes MJ, Shao D, Bailly M, Moss SE (2006). Regulation of actin dynamics by annexin 2. The EMBO Journal, 25(9), 1816 - 1826. doi:10.1038/sj.emboj.7601078\n- Shao D, Forge A, Munro PMG, and Bailly M (2006). Arp2/3 complex-mediated actin polymerisation occurs on specific pre-existing networks in cells and requires spatial restriction to sustain functional lamellipod extension. Cell Motility and the Cytoskeleton, 63(7), 395 - 414. doi:10.1002/cm.20131\n- Dahlmann AH, Mireskandari K, Cambrey AD, Bailly M, Khaw PT (2005). Current and future prospects for the prevention of ocular fibrosis. OPHTHALMOL CLIN NORTH AM, 18(4), 539 - 559.\n- Zhang J, Betson M, Erasmus J, Zeikos K, Bailly M, Cramer LP, Braga VM (2005). Actin at cell-cell junctions is composed of two dynamic and functional populations. Journal of Cell Science, 118(Pt 23), 5549 - 5562. doi:10.1242/jcs.02639\n- Hayes MJ, Merrifield CJ, Shao D, Ayala-Sanmartin J, Schorey CD, Levine TP, Proust J, Curran J, Bailly M, Moss SE (2004). Annexin 2 binding to phosphatidylinositol 4,5-bisphosphate on endocytic vesicles is regulated by the stress response pathway.. Journal of Biological Chemistry, 279(14), 14157 - 14164.\n- Ahir A, Cammer M, Condeelis J, Moss SE, Khaw PT, Bailly M (2004). Sequential cell mediated contractile activity and MMP-mediated physical and chemical matrix remodelling define two distinct phases in tissue contraction.\n- Bailly M (2004). Ena/VASP family: new partners, bigger enigma.. Developmental Cell, 7(4), 462 - 463. doi:10.1016/j.devcel.2004.09.008\n- DesMarais V, Macaluso F, Condeelis J, Bailly M (2004). Synergistic interaction between the Arp2/3 complex and cofilin drives stimulated lamellipod extension.. Journal of cell science, 117(16), 3499 - 3510. doi:10.1242/jcs.01211\n- Toda M, Dawson M, Nakamura T, Munro PM, Richardson RM, Bailly M, Ono SJ (2004). Impact of engagement of FcepsilonRI and CC chemokine receptor 1 on mast cell activation and motility.. Journal of Biological Chemistry, 279(46), 48443 - 48448. doi:10.1074/jbc.M408725200\n- Bailly M, Jones GE (2003). Polarised migration: cofilin holds the front. Current Biology, 13(4), R128 - R130. doi:10.1016/S0960-9822(03)00072-1\n- Bailly M (2003). Connecting cell adhesion to the actin polymerization machinery: vinculin as the missing link?. Trends in Cell Biology, 13(4), 163 - 165.\n- Ahir A, Moss SE, Khaw PT, Bailly M (2003). Cell motility and extracellular matrix remodelling during cell mediated collagen contraction - The effect of matrix metalloproteinase modulation.\n- DesMarais VM, Lorenz M, Condeelis J, Bailly M (2002). Contribution of Arp2/3 to the increase of F-actin at the leading edge of rapidly extending lamellipodia.\n- Bailly M (2002). Postdocs don't need reality to hit so hard. Nature, 415(6867), 13 - .\n- Bailly M, Condeelis J (2002). Cell motility: insights from the backstage. Nature Cell Biology, 4(12), E292 - E294.\n- Ichetovkin I, Bailly M, Zebda N, Chan A, Condeelis J (2001). Cofilin and Arp2/3 synergize in the generation of barbed ends in stimulated actin polymerization. MOL BIOL CELL, 12, 290A - 290A.\n- Condeelis JS, Wyckoff JB, Bailly M, Pestell R, Lawrence D, Backer J, Segall JE (2001). Lamellipodia in invasion. Seminars in Cancer Biology, 11(2), 119 - 128.\n- DesMarais VMO, Ichetovkin I, Bailly M, Chan A, Condeelis J, Hitchcock-DeGregori S (2001). Dynamics of Arp2/3, cofilin, and tropomyosin localization and function at the leading edge. MOL BIOL CELL, 12, 423A - 423A.\n- Bailly M, Ichetovkin I, Grant W, Zebda N, Machesky LM, Segall JE, Condeelis J (2001). The F-actin side binding activity of the Arp2/3 complex is essential for actin nucleation and lamellipod extension. Current Biology, 11(8), 620 - 625. doi:10.1016/S0960-9822(01)00152-X\n- (2000). Phosphorylation of ADF/Cofilin abolishes EFG-induced actin nucleation at the leading edge and subsequent lamellipod extension. The Journal of Cell Biology, 151(5), 1119 - 1128.\n- (2000). Role of cofilin in epidermal growth factor-stimulated actin polymerization and lamellipod protrusion. The Journal of Cell Biology, 148(3), 531 - 542.\n- (2000). Epidermal growth factor receptor distribution during chemotactic responses. Molecular Biology of the Cell, 11(11), 3873 - 3883.","Late Disease and Treatment Effects of Childhood LCH\nOrthopedic problems from lesions of the spine, femur, tibia, or humerus may be seen in 20% of patients. These problems include vertebral collapse or instability of the spine that may lead to scoliosis, and facial or limb asymmetry.\nDiffuse pulmonary disease may result in poor lung function with higher risk for infections and decreased exercise tolerance. These patients should be followed with pulmonary function testing, including the diffusing capacity of carbon monoxide and ratio of residual volume to total lung capacity.\nLiver disease may lead to sclerosing cholangitis, which rarely responds to any treatment other than liver transplantation.\nDental problems characterized by loss of teeth have been significant for some patients, usually related to overly aggressive dental surgery.\nBone marrow failure secondary to LCH or from therapy is rare and is associated with a higher risk of malignancy. Patients with LCH have a higher-than-normal risk of developing secondary cancers.[12,13] Leukemia (usually acute myeloid) occurs after treatment, as does lymphoblastic lymphoma. Concurrent LCH/malignancy has been reported in a few patients, and some patients have had their malignancy first, followed by development of LCH. Three patients with T-cell acute lymphoblastic leukemia (T-ALL) and aggressive LCH, for which the two disorders had shared markers of clonality, have been reported.[14,15] One study reported two cases in which clonality with the same T-cell receptor gamma genotype was found. The authors of this study emphasized the plasticity of lymphocytes developing into Langerhans cells. In the second study, one patient with LCH after T-ALL who had the same T-cell receptor gene rearrangements and activating mutations of the NOTCH1 gene was described.\nAn association between solid tumors and LCH has also been reported. Solid tumors associated with LCH include retinoblastoma, brain tumors, hepatocellular carcinoma, and Ewing sarcoma.\n- Lau LM, Stuurman K, Weitzman S: Skeletal Langerhans cell histiocytosis in children: permanent consequences and health-related quality of life in long-term survivors. Pediatr Blood Cancer 50 (3): 607-12, 2008.\n- Haupt R, Nanduri V, Calevo MG, et al.: Permanent consequences in Langerhans cell histiocytosis patients: a pilot study from the Histiocyte Society-Late Effects Study Group. Pediatr Blood Cancer 42 (5): 438-44, 2004.\n- Donadieu J, Rolon MA, Pion I, et al.: Incidence of growth hormone deficiency in pediatric-onset Langerhans cell histiocytosis: efficacy and safety of growth hormone treatment. J Clin Endocrinol Metab 89 (2): 604-9, 2004.\n- Komp DM: Long-term sequelae of histiocytosis X. Am J Pediatr Hematol Oncol 3 (2): 163-8, 1981.\n- Willis B, Ablin A, Weinberg V, et al.: Disease course and late sequelae of Langerhans' cell histiocytosis: 25-year experience at the University of California, San Francisco. J Clin Oncol 14 (7): 2073-82, 1996.\n- Nanduri V, Tatevossian R, Sirimanna T: High incidence of hearing loss in long-term survivors of multisystem Langerhans cell histiocytosis. Pediatr Blood Cancer 54 (3): 449-53, 2010.\n- Nanduri VR, Lillywhite L, Chapman C, et al.: Cognitive outcome of long-term survivors of multisystem langerhans cell histiocytosis: a single-institution, cross-sectional study. J Clin Oncol 21 (15): 2961-7, 2003.\n- Mittheisz E, Seidl R, Prayer D, et al.: Central nervous system-related permanent consequences in patients with Langerhans cell histiocytosis. Pediatr Blood Cancer 48 (1): 50-6, 2007.\n- Bernstrand C, Cederlund K, Henter JI: Pulmonary function testing and pulmonary Langerhans cell histiocytosis. Pediatr Blood Cancer 49 (3): 323-8, 2007.\n- Braier J, Ciocca M, Latella A, et al.: Cholestasis, sclerosing cholangitis, and liver transplantation in Langerhans cell Histiocytosis. Med Pediatr Oncol 38 (3): 178-82, 2002.\n- Guimarães LF, Dias PF, Janini ME, et al.: Langerhans cell histiocytosis: impact on the permanent dentition after an 8-year follow-up. J Dent Child (Chic) 75 (1): 64-8, 2008 Jan-Apr.\n- Egeler RM, Neglia JP, Puccetti DM, et al.: Association of Langerhans cell histiocytosis with malignant neoplasms. Cancer 71 (3): 865-73, 1993.\n- Egeler RM, Neglia JP, Aricò M, et al.: The relation of Langerhans cell histiocytosis to acute leukemia, lymphomas, and other solid tumors. The LCH-Malignancy Study Group of the Histiocyte Society. Hematol Oncol Clin North Am 12 (2): 369-78, 1998.\n- Feldman AL, Berthold F, Arceci RJ, et al.: Clonal relationship between precursor T-lymphoblastic leukaemia/lymphoma and Langerhans-cell histiocytosis. Lancet Oncol 6 (6): 435-7, 2005.\n- Rodig SJ, Payne EG, Degar BA, et al.: Aggressive Langerhans cell histiocytosis following T-ALL: clonally related neoplasms with persistent expression of constitutively active NOTCH1. Am J Hematol 83 (2): 116-21, 2008."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8184f830-70e6-46d7-8f4a-adee64384555>","<urn:uuid:ffc90185-7c04-4fa6-aa76-c74e777f6d43>"],"error":null}
{"question":"As a parent of a young child: how do child psychologists help in schools, and what rights do parents have regarding their child's mental health treatment in educational settings?","answer":"Child psychologists help in schools by working with educators and administrators to evaluate and revise learning environments for children with special needs. They may also supervise social workers and help monitor children's behavior and progress in the classroom. Regarding parental rights, under the Individuals with Disabilities Education Act (IDEA), parents have the right to request appropriate accommodations related to their child's diagnosis. Parents can also open lines of communication with school personnel about their child's anxiety or other mental health conditions, ask them to monitor classroom behavior changes, and have them communicate directly with doctors or therapists about the child's progress.","context":["Share This Article\nYou May Also Like\nCareers in Child Psychology are Not All Child’s Play\nLearn how child psychology is really a hybrid of family, school and developmental psychology.\nCommonly understood as a distinct subfield within the psychology profession, child psychology more accurately is a subset of family therapy, school psychology, developmental psychology and other fields of psychology that work with children as a part of their practice and incorporate child or adolescent psychology strategies into their treatment plans.\nChild psychologists help kids find balance and become well-adjusted individuals, working well with their peers, their families and in school.\nChild Psychology in Practice\nGenetics and physical growth inherently play roles in a child’s development. In addition to these factors, child psychology considers aspects that influence a child’s mental, social and emotional development, such as the following:\n- Family and ethnic culture\n- Gender roles\n- Personality development\n- Sexual development\nOf course, a child’s experience also impacts growth. Circumstances like divorce, death of a close family member, relocating to a new home, switching schools and other family situations all have the potential to seriously affect a child’s life.\nAs part of their job, child psychologists perform a variety of clinical, research and administrative tasks. Some common duties involved in child psychology practice include:\n- Meeting with children and their family members to observe and diagnose concerns about a child’s mental or emotional state.\n- Collaborating with parents, teachers, pediatricians or other clinicians involved in a case to implement child psychology treatment plans.\n- Working with elementary educators and school administrators to evaluate and revise learning environments to suit children with special needs.\n- Teaching students at the college or university level.\n- Researching disorders common among children and adolescents, and devising new treatment strategies.\n- Performing tasks related to running an independent practice, such as bookkeeping, billing, and managing insurance issues.\nChild psychology professionals work in a variety of settings, some of which include the following:\n- Colleges and universities\n- Health clinics\n- Outreach programs\n- Private practice\nDepending on the setting, they might put in evening and weekend hours to accommodate the schedules of children and their parents. Some child psychologists supervise less senior psychology staff members or social workers; in universities, child psychology professors usually mentor graduate-level students training in a child psychology specialty.\nEducation & Training\nWorking as a child psychologist in independent practice or research typically requires a doctoral degree, either a PhD or PsyD. Along with a doctorate, child psychologists must pass a licensing exam in order to treat children. Child psychology professionals working in elementary and secondary schools may be subject to additional requirements, depending on the state. To maintain or renew licensure, practitioners usually need to complete a certain number of hours of continuing education over a specified time period.\nRewards of the Career\nClearly, child psychology offers significant challenges and inherently delivers immeasurable rewards. Providing children with the support and understanding they need in difficult times or in overcoming developmental issues can make a world of difference to their well-being as children and to their resilience and perspective as adults.","- Anxiety Medication Guide for Parents, AACAP.org, written by ADAA member John Walkup, MD\n- 2020 Children's Mental Health Report by Child Mind Institute (ADAA Partner)\n- 2019 Children's Mental Health Report by Child Mind Institute (ADAA Partner)\n- Anxiety in Teens is Rising - What's Going On? HealthyChildren.org\nTaking your child to a doctor for a mental health problem is as important as visiting a doctor for an ear infection or broken arm. Finding a health professional that you and your child can work with—and who makes you both feel comfortable—is critical.\nAnxiety disorders in children are treatable, and they can be treated by a wide range of mental health professionals who have training in scientifically proven treatments. Psychiatrists and nurse practitioners can prescribe medication. Psychologists, social workers, and counselors are more likely to have cognitive-behavioral treatment, or CBT, training. Learn more about treatment.\nA therapist should be willing to answer any questions you may have about methods, training, and fees during a consultation. Bring a list of your child’s symptoms to discuss, and consider asking these questions:\n- What training and experience do you have in treating anxiety disorders?\n- Are you qualified to provide cognitive-behavioral therapy (CBT)?\n- What is your basic approach to treatment?\n- Can you prescribe medication or refer me to someone who can, if that proves necessary?\n- How long is the course of treatment?\n- How frequent are treatment sessions and how long do they last?\n- Do you include family members in therapy?\n- What is your fee schedule, and do you have a sliding scale for varying financial circumstances?\n- What kinds of health insurance do you accept?\nIf a therapist is reluctant to answer your questions, or if you or your child does not feel comfortable, see someone else.\n- Pay attention to your child’s feelings.\n- Stay calm when your child becomes anxious about a situation or event.\n- Recognize and praise small accomplishments.\n- Don’t punish mistakes or lack of progress.\n- Be flexible and try to maintain a normal routine.\n- Modify expectations during stressful periods.\n- Plan for transitions. (For example, allow extra time in the morning if getting to school is difficult.)\nYour child’s anxiety disorder may affect success at school. If an anxiety disorder is causing your child to struggle academically or socially, the first step is to talk to the teacher, principal, or counselor about your concerns. School personnel will likely recognize some symptoms or manifestations of your child’s anxiety at school, but they may not realize they are caused by an anxiety disorder, or how they can help. Use your child’s diagnosis to open lines of communication.\nTalk about any accommodations that may help your child succeed in the classroom. You have the right under the Individuals with Disabilities Education Act (IDEA) to request appropriate accommodations related to your child’s diagnosis. Also ask them to monitor changes and behavior in the classroom so you can inform your doctor of any progress or problems, or ask them to speak to the doctor or therapist directly.\nADHD, behavior problems, anxiety, and depression are the most commonly diagnosed mental disorders in children in the United States*\n- 9.4% of children aged 2-17 years (approximately 6.1 million) have received an ADHD diagnosis.\n- 7.4% of children aged 3-17 years (approximately 4.5 million) have a diagnosed behavior problem.\n- 7.1% of children aged 3-17 years (approximately 4.4 million) have diagnosed with anxiety.\n- 3.2% of children aged 3-17 years (approximately 1.9 million) have diagnosed with depression.\n* These statistics are taken from the CDC.gov website Additional anxiety/depression statistics can be found on this website.\n- School Refusal\n- Protests, Racism, And Our Children: Helping Kids Cope\n- 4 At-Home Ways to Support Your Child's Anxiety\n- Is My Child Depressed?\n- All About Parent-Child Interaction Therapy (PCIT)\n- What Parents Can Do to Help Young Children and Teens Deal with Stress and Anxiety, and Other Negative Feelings\n- Residential Treatment for Youth with OCD: Answers to Your Top 3 Questions\n- Worried About Sending Your Kids Back to School?\n- Three Ways to Help Kids Manage Emotions During Covid\n- Protests, Racism and our Children - Helping Kids Cope -\n- The Pandemic Parenting Guide: How to Improve Your Child/Teen's (and your own) Emotional Well-Being in Times of COVID-19\n- Parents - Being \"Good Enough\" Right Now is Ok\n- How to Talk to Your Anxious Child or Teen About Coronavirus\n- Back to School Anxiety\n- Blips and Bumps Happen\n- Childhood Depression\n- Is Your Child Scared By Halloween? A Cognitive Behavioral Therapist Offers Help\n- Separation Anxiety - What Parents Should Know\n- Three Syrian Refugee Children on the Streets of Istanbul\n- Watch, Ask and Listen: How to Tell if Your Child or Teen is Anxious or Depressed\n- When Reassurance is Hurting Your Child More Than Helping\n- Where are Mommy and Daddy?! The Traumatic Impact of Separating Families\n- Building Resilience: Help your Child Adapt and Thrive\n- Helping Kids with Anxiety & Depression: Strategies for Parents & Caregivers\n- Does Your Child Have Anxiety or OCD?\n- Collaborating with Pediatricians: Tools & Techniques to Enhance Relationships & Care Coordination with Pediatricians in your Community\n- Empowering Families in the Face of Pediatric Acute Onset Neuropsychiatric Syndrome (PANS)\n- Diagnosing and Treating ADHD and Comorbidity Conditions in Preschoolers\n- How to Best Understand and Address Selective Mutism in Younger Children, Tweens, and Teens\n- Addressing Cognitive Dimensions of Academic & School Anxiety\n- Clinical Kung Fu: Managing Anger in Children and Teens with Anxiety Disorders\n- Is My Kid’s Therapy Helping? Plus 7 Steps to Take if It’s Not, Everyday Health, Jill Emanuele, PhD, 2022\n- What to know about school anxiety, Medical News Today\nADAA supports KidGuard and their mission to provide child protection relating to risks arising from the internet and technology."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d84d0730-093e-409a-88fc-f5e8ec80453b>","<urn:uuid:d46ab4c5-589a-4802-82e0-2ca4316acae7>"],"error":null}
{"question":"Need clarity on high caffeine risks: How do the medical documents define 'excessive' caffeine amounts for osteoporosis risk vs what's considered 'too much'? Break it down.","answer":"The documents define caffeine intake thresholds differently: one document lists 'excessive caffeine consumption' as a risk factor without specifying an amount, while the other document specifically defines 'too much' caffeine as more than three cups of tea, coffee, or equivalent daily when discussing osteoporosis risk factors.","context":["Bone Density Scan (cont.)\nCatherine Burt Driver, MD\nCatherine Burt Driver, MD, is board certified in internal medicine and rheumatology by the American Board of Internal Medicine. Dr. Driver is a member of the American College of Rheumatology. She currently is in active practice in the field of rheumatology in Mission Viejo, Calif., where she is a partner in Mission Internal Medical Group.\nWilliam C. Shiel Jr., MD, FACP, FACR\nDr. Shiel received a Bachelor of Science degree with honors from the University of Notre Dame. There he was involved in research in radiation biology and received the Huisking Scholarship. After graduating from St. Louis University School of Medicine, he completed his Internal Medicine residency and Rheumatology fellowship at the University of California, Irvine. He is board-certified in Internal Medicine and Rheumatology.\nIn this Article\n- Bone density scan facts\n- What is osteoporosis?\n- How does osteoporosis occur?\n- What is bone mineral density (BMD)?\n- Why is BMD measurement important?\n- What is the relationship between BMD and fracture risk?\n- Who should have BMD testing?\n- How is BMD measured?\n- What are other methods of measuring BMD?\n- How often should DXA scans be repeated to monitor treatment?\n- What is the cost of DXA?\n- What about the accuracy of BMD testing in the doctor's office using smaller equipment?\n- Find a local Rheumatologist in your town\nWhat is the relationship between BMD and fracture risk?\nIn patients with low bone mass at the hip or the spine (the two areas traditionally measured with DXA [formerly referred to as DEXA] scanning), there is a two- to threefold increase in the incidence of any osteoporotic fracture. In other words, low bone density at the measured areas of the spine and hip can even predict future osteoporotic fractures at other parts of the body besides the spine and hip. In subjects with a BMD in the osteoporosis range, there is approximately a five times increase in the occurrence of osteoporotic fractures.\nWho should have BMD testing?\nBMD testing is recommended for all women over the age of 65. Additionally, postmenopausal women under 65 years who have risk factors for osteoporosis other than menopause (these include a previous history of fractures, low body weight, cigarette smoking, and a family history of fractures) should be tested. Finally, men or women with strong risk factors as listed below should discuss the benefit of DXA scanning with their doctor to see if testing is indicated.\nThe following are potential risk factors for osteoporosis that might suggest the need for DXA scanning:\n- Personal history of fracture as an adult\n- History of fracture in first-degree relative\n- Low body weight or thin body stature\n- Advanced age\n- Current cigarette smoking\n- Use of corticosteroid therapy for more than three months\n- Impaired vision\n- Estrogen deficiency at early age\n- Poor health/frailty\n- Recent falls\n- Lifelong low calcium intake\n- Low physical activity\n- Alcohol intake of more than two drinks/day\n- Thyroid disease\n- Rheumatoid arthritis\n- Excessive caffeine consumption\n- Use of oral contraceptive (birth control pills)\nNext: How is BMD measured?\nViewers share their comments\nGet tips and advances in treatment.","Late effects: Bone heath & osteoporosis\nDuring childhood and into young adulthood, bone formation usually occurs faster than bone loss, causing bones to grow and become stronger. As a person gets older, the process of bone removal gradually overtakes bone formation, and bones slowly lose strength as part of the normal ageing process.\nOsteoporosis: A silent disease\nOsteoporosis (or osteopenia) is a disorder resulting from too little new bone formation or too much bone loss, causing bones to become weak. Most people do not have symptoms, especially in the early stages.\nAs bones become weaker, fractures may occur after minimal trauma, such as a fall. Osteoporosis may occur in any bone, but most commonly affects the wrists, hips and leg bones. The vertebrae of the spine often collapse, leading to loss of height, spinal curvature, and chronic pain.\nHow is osteoporosis diagnosed?\nAlthough osteoporosis may be suspected based on a person's symptoms and risk factors, the diagnosis is made by measuring bone density with special x-ray techniques called DEXA or bone density scans. These scans do not expose patients to large amounts of radiation, and generally take less than 20 minutes to perform.\nPeople who have osteoporosis should discuss treatment options with their doctor. Medications, such as calcium and vitamin D supplements and bisphosphonates may be appropriate for the treatment of low bone density. In addition, if you have low levels of male or female hormones, you may also benefit from hormone replacement therapy.\nWhat are the risk factors for osteoporosis?\nOsteoporosis is more common in people with the following characteristics:\n- Female (especially after menopause)\n- Family history of osteoporosis\n- Caucasian or Asian race\n- Small, thin frame\n- Older age\nThe following factors may also increase the risk of osteoporosis:\n- Cigarette smoking\n- Inadequate amounts of dietary calcium\n- Low vitamin D levels\n- Lack of physical activity\n- Too much caffeine (more than three cups of tea, coffee or equivalent daily)\n- Too much alcohol (more than two standard drinks per day)\nThe following cancer treatment factors may also increase the risk of osteoporosis:\n- A history of treatment with:\n- Corticosteroids (such as prednisone and dexamethasone)\n- Radiation to weight-bearing bones (legs, hips, spine)\n- Conditions resulting from prior treatment, including:\n- Low levels of female or male hormones\n- High levels of thyroid hormone\n- Chronic graft-versus-host disease requiring prolonged therapy with corticosteroids\n- Prolonged periods of inactivity (bed rest)\n- Other medical treatments, including:\n- Certain anticonvulsants (phenytoin and barbiturates)\n- Aluminium-containing antacids\n- High doses of heparin (used to prevent blood clots), especially with prolonged use\n- Cholestyramine (used to control blood cholesterol)\nMany of the medications on this list are essential treatments for certain medical conditions. If you are taking any of these medications, do not change your dosage or stop taking your medication without consulting with your doctor.\nWhat can I do to reduce the risk of osteoporosis?\nFortunately, there are many things you can do to reduce the risk of osteoporosis.\n- Regular weight-bearing exercise (such as brisk walking, dancing, tennis and jogging) is ideal to develop and maintain healthy bones. Bicycling and swimming are excellent exercises for general fitness, but as these are not weight-bearing exercises, they may be less effective in building strong bones. If you have problems with your heart, or have painful bones or joints, be sure to check with your doctor before starting any new exercise program.\n- A diet high in calcium also is important in preventing osteoporosis. Most doctors recommend 1000-1500 mg a day, which means a diet rich in dairy products (milk, cheese, yoghurt) and leafy green vegetables. Talking with a dietician may help you design a healthy diet. Over-the-counter calcium supplements also may be useful. If you are lactose intolerant, you may drink a calcium-rich soy milk or lactase treated cows milk.\n- Vitamin D is needed in order to absorb calcium. The skin makes this vitamin naturally when exposed to sunlight. Many dairy products also contain vitamin D. In general, you should not take more than 800 units of Vitamin D per day. Taking too much vitamin D may be harmful, so it’s best to check with your doctor before taking any vitamin D supplements.\n- Avoid smoking\n- Limit alcohol and caffeine intake\nWhat screening is recommended?\nAfter reviewing your treatment history and risk factors, your doctor can advise you regarding the need for bone density testing. Follow up scans may be needed for ongoing monitoring of bone density in some patients."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:584b34ae-de1c-49ef-9486-70585a64f1ce>","<urn:uuid:977f72f1-8c41-4278-b993-e5d7ad28dd69>"],"error":null}
{"question":"How can immigrant artists adapt to creative challenges during crises? Can you explain how mental health services are currently supporting immigrant communities during difficult times? 🎨","answer":"Immigrant artists adapt to creative challenges through various approaches, as seen in examples like 2Fik who transforms his immigration experience into art exploring gender, religion, and society, and Ximena Huizi who explores displacement through theatre. For mental health support, organizations provide culturally competent services - for instance, the Hmong Cultural Center of Butte County blends Western and traditional cultural treatments for trauma and stress, while organizations like FIRM have seen a 300-400% increase in clients seeking immigration-related services since 2017, offering multilayered culturally and linguistically competent support to help improve mental health outcomes.","context":["What's Online features a curated selection of Koffler.Digital artist projects and publications, Koffler Gallery exhibitions and online gallery publications, features on recent community-engaged programs, as well as podcasts of past Books & Ideas events, talks and discussions.\nThis week, the Koffler joins JIAS Toronto\n, Canada’s only Jewish agency dedicated solely to the settlement of immigrants and refugees, in the presentation of the panel discussion Artists Discuss Creativity & Crisis\n. We have invited a dynamic group of artists to share their perspectives, both through the lens of their immigration journey and in relation to the new and difficult challenges posed by the COVID-19 era of social isolation.\nLeading up to the live online panel this Thursday, May 28 at 12 PM\n, we invite you to explore the artistic practices of our three guest speakers: 2Fik, Ximena Huizi, and Igor Portnoi.\nVisit our Instagram page to see more photos from each participating artist.\nWe are happy to (re)introduce a dear friend of the Koffler, an artist whose major solo exhibition, His and Other Stories\n, we were proud to present in spring 2017: 2Fik!\nBorn in Paris to a Berber Muslim family, 2Fik grew up in Morocco and then France, before moving to Montreal in 2003. His immigration experience compelled him to confront his relationship to gender, religion, politics and society – a process from which he derives much of his artistic inspiration. In most of his work, 2Fik takes on the roles of artistic director, photographer, stylist and model to capture himself portraying various characters he then edits into single, composite images. To read about 2Fik’s previous projects, head over to our Gallery Publications page and enjoy the digital catalogue\ndocumenting his past exhibition.\n2Fik is currently in the midst of developing a new and ambitious trans-disciplinary project combining visual, digital and live artistic elements to examine the performance of the self on dating apps. Set to premier in 2021, Romance ain’t dead, 2Fik!\npushes the limits of self-representation through photography, reflecting on the construction of one’s image and presence in virtual contexts. Creating and embodying 100 distinct characters, each accompanied by individual profiles and online activity, 2Fik underlines the tensions between virtuality and reality to reveal the paradoxical nature of the self-as-performance inherent to dating apps. To find out more, follow 2Fik\non social media, or visit his website at 2fikornot2fik.com\nImage: 2Fik, His and Other Stories\nThe Koffler had the pleasure of working with Ximena Huizi in the fall 2016 where they performed in our co-production of Like Mother, Like Daughter\nwith Why Not Theatre\n. They also participated in the Koffler Gallery’s panel discussion, Yonder Perspectives: Inclusion Through Artistic Practices\nBorn in Caracas, Venezuela, Ximena is a theatre artist whose multidisciplinary work explores the complex issues of displacement, dualities and translations. In 2017, they directed a production of Nuestra Señora de las Nubes\n(written by Aristides Vargas) and approached the work as a way of investigating “stories about our home(s) in a way that doesn’t apologize for their language, their poetry, their magic.”\n(2017), Ximena created an immersive theatre experience; and Tell Me What It’s Called\n(2018) was an experiment in devised creation, presented as a part of Why Not Theatre’s Riser Project.\nMost recently, Ximena created Grappling With Emptiness\n(2020), a short video art piece commissioned by Convergence Theatre\nas part of their Covid Confessions series. Here Ximena received the anonymous audio of a woman sharing her experiences of emptiness and isolation, which she used as the track for a shadow object and video exploration. Watch the video here\nImage: Design Christine Urquhart, created/performed by Sandra Castillo, Ximena Huizi.\nBorn in Siberia, Igor Portnoi is a graduate of the State Academy for Theatrical Arts in Moscow, where he performed the leading roles in the numerous musical theatre productions, including Judas in Jesus Christ Superstar\n. In 1992, Igor made his new home in Canada. His Canadian credits include performances in The Phantom of The Opera\nin Vancouver and the workshop of the musical Ragtime\nin Toronto, in addition to appearances on the CBC. He can be heard on the recording of Songs from the Musical Ragtime\nand two solo albums Russians on Broadway\nand Let’s get together\nIgor has spent much of his career performing internationally in classic musical theatre productions including Zorro\n, The Sound of Music\n, Mamma Mia\nand Beauty and the Beast\n, in Moscow and St. Petersburg. And in December of 2002, Igor received a Green Card from the US Government as recognition of his extraordinary talent. This unique international experience has given Igor the opportunity to perform in both English and in his mother tongue: “It’s interesting that I tend to lose my Russian accent when singing and performing lyrics in English. Music and lyrics are like another language entirely.”\nVisit Igor's website at igor-portnoi.com","The Lifesaving Care of Culturally and Linguistically Competent Mental Health Services\nWhile Minority Mental Health Awareness Month has ended, SEARAC continues to urge policymakers to improve the cultural and linguistic competence of California’s mental health delivery system.\nSince November 2004,the Mental Health Services Act (MHSA) has imposed a 1% income tax on individuals with an income of over $1 million, and as a result, California has funneled more funding than ever before to improving and increasing access to mental health services. According to the California Department of Health Care Services, approximately $1.8 billion was deposited into the Mental Health Services Fund (MHSF) in the fiscal year 2016-2017, with the amount projected to gradually increase to $2.2 billion by the 2018-2019 fiscal year.\nHowever, even after almost 15 years since the act’s implementation, Southeast Asian Americans (SEAAs), communities of color, and LGBTQ+ community continue to face barriers in accessing and utilizing mental health services. In fact, Asian American and Pacific Islander adults have been shown to be the least likely racial group to access specialty mental health services even once, with just 2% of the population obtaining care.\nCulturally and linguistically competent mental health services are critical in ensuring access for SEAAs, communities of color, and LGBTQ+ community. For these reasons, SEARAC took on the effort to co-sponsor AB 512 – Cultural Competence in Mental Health.\nSoutheast Asian American Mental Health Awareness Spotlight:\nThe limiting state definition of a threshold language has left minority Southeast Asian American groups like the Hmong and Iu-Mien communities to experience even greater cultural and linguistic barriers to accessing mental health services. Due to the lack of translated materials and interpretation available in doctor’s offices, the Hmong Cultural Center of Butte County (HCCBC) and Iu-Mien Community Services (IMCS) have risen to fill in existing gaps to ensure their community has a fighting chance.\nDespite being in the top three languages most spoken in Butte County, Hmong isn’t considered a threshold language, and as a result, CA counties are not required to translate materials or have interpreters available in Hmong. HCCBC is one of few organizations providing culturally and linguistically competent mental health services to the Hmong community north of Sacramento, CA.. Through its innovative programming blending Western and traditional cultural treatment, HCCBC has been able to support the mental health needs of Hmong elders experiencing trauma, stress, anxiety, isolation, stigmatization, and depression.\nMary Lor, an active participant in HCCBC’s Zoosiab program shares the power of culturally and linguistically competent mental health services.\nMary shares: “I was referred from Butte County Department of Behavioral Health to the Zoosiab program and I feel that the program has done a great deal in educating me in ways to cope with my stress, raise mental health awareness, learn life skills, and etc. The services at HCCBC are important, because I do not feel judged [here]. The recreational group that I attend is comfortable and like no other [support] group because the staff speaks my native language and respect who I am. Since participating in the Zoosiab program, I have increased my mental wellbeing and reduced my stress and depression… Without the Zoosiab program, I don’t think my mental wellbeing would be as good because I lack the resources and services to navigate mental health services due to language barriers and mental health stigma.”\nSimilarly, the Iu-Mien community faces a similar struggle by not meeting the threshold standards in Sacramento County, despite Sacramento having one of the country’s largest populations of Iu-Mien. IMCS is the only non-profit organization specifically serving the Iu-Mien community, providing an array of intergenerational and culturally responsive programs and social services to address depression, stress and isolation.\nPian Va Saelee is a participant of IMCS’ Healthy Village Senior Group, a program that connects Iu-Mien community members in their early 40s to 80s to their community.\nPian shares: “This program is very important to me because it gives me an opportunity to meet with my friends weekly to share stories, foods, and information to help us live better. I don’t feel alone like I do a lot of time at home by myself. When I arrive at the group, I am accepted and not judged. The staff is very supportive and helpful. I love to see them because they look like my children and they respect me. … I don’t know what to do and where to go if IMCS no longer provides this service. I would probably have to spend more time alone at home.”\nCulturally and linguistically competent mental health services are lifesaving, but mental health services in California have not been doing enough to serve the SEAA community, communities of color and LGBTQ+ community. To read more community stories on the need for culturally and linguistically competent mental health, please visit our California health and aging news.\nTAKE ACTION: Support our culturally and linguistically competent mental health by telling your Senator to support AB 512\nCommunity members have shared with us the need for culturally and linguistically competent mental health, and we have been listening. CA AB 512, to improve cultural competence in mental health, has moved forward and passed out of the CA Senate Health Committee. AB 512 will now move to the Senate Appropriations Committee for a fiscal analysis.\nContact your CA senator today to advocate for culturally and linguistically competent mental health services and support AB 512.\nTake Action: CA Residents, Tell Us Your Mental Health Stories\nCelebrate Asian Pacific American Heritage Month and Mental Health Awareness Month by Demanding Culturally Competent Mental Health Services for AAPIs\nDue to cultural stigma and the lack of culturally and linguistically competent mental health services, many Asian Americans live with undiagnosed and untreated mental health disorders. The California Department of Health Care Services reports that in fiscal year 2016-2017, AAPI adults are the least likely racial group to access specialty mental health services even once, with just 2% of the population accessing care. Southeast Asian Americans, in particular, experience higher rates of post-traumatic stress disorder, depression, anxiety and other mental health challenges, compared to the general population.\nMental Health Awareness Spotlight\nFresno Interdenominational Refugee Ministries (FIRM) has seen a 300-400% increasein clients seeking immigration-related services since the inauguration of President Donald Trump in 2017.1 One such community member is Silath Saopadith, 55, who immigrated to the United States more than 12 years ago and has struggled to obtain his citizenship due to the lack of culturally and linguistically competent services. Mr. Saopadith shares the impact of FIRM’s multilayered culturally and linguistically competent services to support obtaining his citizenship and improving his mental health:\n“When there is someone that looks like me, I am able to open up a bit more. I’ve been in the states for 12 years, and I’ve never been able to open up about my past or ask anyone for help. Before coming to FIRM, I had a high stress level, [was] afraid of deportation, [and had] language barriers, so everything was working against me. After a few years coming to FIRM, the staff was able to help me reduce those symptoms and also make me feel safe. … I come to FIRM two or three times a week for help. Without their support, I don’t know where I would be.”\nCulturally and linguistically competent mental health services are lifesaving, but mental health services in California have not been doing enough to serve our SEAA community.\nSupport our sponsored mental health CA bill by telling us your mental health story\n1. FIRM is a faith-based nonprofit organization that provides culturally and linguistically competent education, naturalization, health navigation, and mental health services to refugees of SEAA, Slavic, and African origin.\nSEARAC Co-Sponsors CA Legislation to Improve Cultural Competence in Mental Health\nSoutheast Asian Americans experience high rates of PTSD, depression, and anxiety\nSACRAMENTO, CA – SEARAC has taken on the effort to co-sponsor California Assembly Bill (AB) 512 – Cultural Competence in Mental Health alongside the California Pan-Ethnic Health Network, Steinberg Institute, Latino Coalition for a Healthy California, and California LGBTQ Health and Human Services Network. AB 512 will improve county cultural competence plans and utilize both evidence-based and community-defined practices to address the mental health conditions of California’s diversity."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:fbbdc0f1-aab7-4726-b27c-f98e627f799b>","<urn:uuid:3fa4945c-1a40-4d49-9752-df22ebf29798>"],"error":null}
{"question":"How fast could the Bloch MB.170 fly at its maximum speed?","answer":"The Bloch MB.170 had a maximum speed of 530 km/h (329 mph) when flying at an altitude of 5,200 meters (17,060 ft).","context":["|Manufacturer||Société des Avions Marcel Bloch|\n|First flight||July 1939 (M.174)|\n|Introduction||March 1940 (M.174)|\n|Primary users||French Air Force\nThe Bloch MB.170 and its family of derivatives were French reconnaissance bombers designed and built shortly before World War II. They were, by far, the best aircraft of this type available to the Armée de l'Air at the outbreak of the war, with speed and maneuverability that allowed them to evade interception by the German fighters of the time. Although the aircraft could have been in service by 1937, debate over what role to give the aircraft delayed deliveries until 1940. Too few in number to make any measurable impact on the Battle of France, they continued in service with the Vichy forces after the armistice. The MB.174 will also be remembered as the aircraft flown by Antoine de Saint-Exupéry, author of The Little Prince during the campaign. His work Pilote de Guerre - translated as Flight to Arras and published in 1942 - is based on a 1940 reconnaissance mission in this type of aircraft.\nDesign and development\nIn 1936, the Ministry for the Air initiated a programme of modernisation of French aviation which included a request concerning a two- or three-seat multi-role aircraft that could be used as a light-bomber or attack aircraft or for reconnaissance. A design team at the former Bloch factory at Courbevoie (which had recently become part of the nationalised SNCASO), led by Henri Deplante, proposed the MB.170, a twin-engined, low-winged cantilever monoplane.\nThe first prototype, the MB 170 AB2-A3 No.01, equipped as a two-seat attack bomber or a three-seat reconnaissance aircraft, made its maiden flight on 15 February 1938. It was powered by two 720 kW (970 hp) Gnome-Rhône 14N radial engines and was armed with a 20 mm Hispano-Suiza cannon in the nose, two 7.5 mm MAC 1934 machine guns in the wing, with another machine gun flexibly mounted in the rear cockpit, with a ventral cupola housing either a rearward firing machine gun or a camera. The second prototype, the MB 170 B3 No.2 was a dedicated three seat bomber, with the ventral cupola housing the camera removed, a revised canopy and larger tail fins.\nAfter many modifications it became the definitive MB.174 version. After the 50th example was delivered in May 1940, the MB.175 succeeded the MB.174 on the assembly lines in full flow. This version, a dedicated bomber, had a redesigned bomb bay capable of carrying bombs of 100–200 kg (220-440 lb), where the MB.174 was limited to 50 kg (110 lb) bombs. The MB.175's fuselage was lengthened and widened to accommodate this greater capacity, but only 25 specimens were delivered before France's defeat. They were eventually used in the same reconnaissance units as the MB.174s. The MB.176 was a version with Pratt & Whitney R-1830 radial engines but which proved to have poorer performance than the MB.175. It was ordered into production in order to ease demand on the French engine manufacturers.\nThe Bloch MB.174 flew for the first time in July 1939 and entered in active service in March 1940. It was issued to strategic reconnaissance units where it replaced the Potez 637 that had proved too vulnerable during the Phoney war. Its first operational mission was flown by the famed pilot and writer, Cap. Antoine de Saint-Exupéry, of Groupe de Reconnaissance II/33, on 29 March 1940. The Bloch 174 appeared extremely effective in these missions as its speed and maneuverability at altitude allowed it to escape from most modern Luftwaffe fighters. Only 3 examples were lost to enemy fire during the Battle of France. However, like the majority of the modern equipment of the Armée de l'Air during the campaign, they arrived too late and in insufficient numbers. At the time of the armistice, most surviving MB.174s and 175s had been evacuated to North Africa. A few were recovered by the Germans and then used for pilot training. During the Vichy government rule on the French empire, MB.174s frequently flew over Gibraltar to monitor the British fleet.\nIn March 1941, German engineers used engines taken from MB.175s (as well as other captured aircraft) to propel the Messerschmitt Me 323 cargo aircraft, some of which actually flew with parts taken from already complete MB.175s.\nAfter Operation Torch, as French forces split from Vichy to side with the Allies, remaining examples of the MB.170 line flew their final combat missions during the battle of Tunisia. They were replaced by reconnaissance variants of the P-38 Lightning, and used as transports and target tugs.\n- MB.170.01 - The first prototype, equipped as reconnaissance aircraft, powered by Gnome-Rhône 14N-06 engines.\n- MB.170.02 - The second prototype, equipped as light bomber, powered by Gnome-Rhône 14N-06 engines.\n- MB.174.01 - The original MB.174 prototype, powered by Gnome-Rhône 14N-49 engines.\n- MB.174A.3 - Original production version, powered by Gnome-Rhône 14N-49 engines.. 56 built\n- MB.175.01 - The original MB.175 prototype, powered by Gnome-Rhône 14N-48 engines.\n- MB.175B.3 - Second production version. 23 built, plus 56 unarmed aircraft for the Luftwaffe, powered by Gnome-Rhône 14N-48 engines.\n- Post-war torpedo bomber version for the Aeronavale. 80 built.\n- MB.176.01 - The original MB.176 prototype, powered by two Pratt & Whitney R-1830 SC 3-G Twin Wasp engines.\n- MB.176B.3 - Production version. 5 built\n- Single prototype, powered by two Hispano-Suiza 12Y-31 inline engines.\n- Further development , construction halted by arrival of German forces.\n- Luftwaffe (captured)\nData from War Planes of the Second World War: Volume Seven Bombers and Reconnaissance Aircraft\n- Crew: Three\n- Length: 12.23 m (40 ft 1 1⁄2 in)\n- Wingspan: 17.92 m (58 ft 9 1⁄2 in)\n- Height: 3.55 m (11 ft 7 3⁄4 in)\n- Wing area: 38.0 m² (409 ft²)\n- Empty weight: 5,612 kg (12,346 lb)\n- Loaded weight: 7,175 kg (15,784 lb)\n- Powerplant: 2 × Gnome-Rhône 14N-20/21 14-cylinder radial engines, 772 kW (1,035 hp) each\n- Maximum speed: 530 km/h (286 knots, 329 mph) at 5,200 m (17,060 ft)\n- Cruise speed: 460 km/h (249 knots, 286 mph) at 4,000 m (13,120 ft)\n- Range: 1,650 km (890 nmi, 1,025 mi)\n- Service ceiling: 11,000 m (36,090 ft)\n- Climb to 8,000 m (26,250 ft): 11 min\n- Bombs: 400 kg (880 lb) of bombs - usually 8 × 50 kg (110 lb) bombs\n- Aircraft of comparable role, configuration and era\n- Related lists\n- List of aircraft of World War II\n- List of aircraft of the French Air Force during World War II\n- List of bomber aircraft\n|Wikimedia Commons has media related to Bloch aircraft.|\n- Green 1967, p. 107.\n- Breffort and Jouineau 2004, p. 46.\n- Green 1967, pp. 107–108.\n- Green 1967, pp. 108–109.\n- Green 1967, p. 114.\n- Breffort, Dominique; Jouineau, André (2004). French Aircraft 1939–1942: Fighters, Bombers, Reconnaissance and Observation Types: Volume 1: From Amiot to Curtiss. Paris: Histoire & Collections. ISBN 2-915239-23-1.\n- Green, William (1967). War Planes of the Second World War: Volume Seven Bombers and Reconnaissance Aircraft. London: Macdonald.\n- Dassault official home page"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:85bf0fc7-dc91-400a-9f61-dcf80e6ed958>"],"error":null}
{"question":"What is remarkable about the 'black box' flight recorder's invention and its technical capabilities in modern aviation?","answer":"The 'black box' flight recorder was invented by Dr David Warren in Melbourne in 1958. Modern black boxes consist of two units: the Cockpit Voice Recorder (CVR) which records radio transmissions, pilot voices, and engine noises, and the Flight Data Recorder (FDR) which monitors parameters like altitude, airspeed, and heading. These devices are installed in the most crash-survivable part of aircraft, usually the tail section, and are equipped with an Underwater Locator Beacon that can transmit from depths up to 14,000 feet. The recorders must withstand impacts of 3400 Gs, temperatures of 1100 degrees Celsius for 30 minutes, and water pressure at 20,000 feet depth.","context":["1838 PRE-PAID POSTAGE\nFirst pre-paid letter-sheets were issued by the New South\nWales Post Office.\n1843 GRAIN STRIPPER\nJohn Ridley and John Bull of South Australia developed the\nworld's first grain stripper that cut the crop, removed and\nplaced the grain into bins.\n1850 REFRIGERATION PLANT\nFirst mechanical refrigeration plant was built by Geelong\npublisher, James Harrison.\n1876 STUMP-JUMP PLOUGH\nRobert and Clarence Bowyer Smith developed a plough which\ncould jump over stumps and stones, enabling newly-cleared\nland to be cultivated.\n1882 STRIPPER HARVESTER\nThe first stripper harvester was conceived by Hugh Victor\nMcKay of Drummartin, Victoria.\nHenry Sutton's telephane, which transferred signals along\ntelegraph lines, was the forerunner of the television.\n1889 ELECTRIC DRILL\nArthur James Arnot patented the world's first electric drill\n1897 DIFFERENTIAL GEARS\nDavid Shearer of South Australia built a steam car with differential\ninside left rear wheel hub.\nThe teleprinter for recording messages onto a tape was invented\nby Donald Murray of Sydney.\n1903 FROTH FLOTATION PROCESS\nThe process of separating minerals from rock by flotation\nwas developed by Charles Potter and Guillaume Delprat of New\n1905 THRUST BEARING\nAnthony Mitchell invented the tilt-pad thrust bearing which\nrevolutionised thrust technology.\n1906 FEATURE FILM\nThe world's first feature length film, The Story of the Kelly\nGang, was a little over an hour long.\nThe key technology for the invention of xerography was developed\nby Professor O U Vonwiller at The University of Sydney.\n1913 AUTOMATIC TOTALISATOR\nInvented by George Julius, the tote automised betting at horse\nAspro was developed by Melbourne pharmacist, George Nicholas,\nand experimenter, Henry Woolf Shmith.\n1924 CAR RADIO\nThe first car radio was fitted to an Australian car built\nby Kellys Motors in New South Wales.\n1925 ELECTRIC RECORD CHANGING SALONOLA\nThe stepped centre spindle of the invention by Tasmanian Eric\nWaterworth was later used in record changers worldwide.\n1927 PEDAL WIRELESS\nThe pedal-operated generator, connected to a wireless, was\ninvented by Alfred Traeger.\n1928 FLYING DOCTOR SERVICE\nDr Kenyon Welsh and pilot Arthur Affleck began operating the\nfirst flying doctor service out of Cloncurry, Queensland.\n1934 UTILITY VEHICLE\nThe utility vehicle, with a front like a car and a rear like\na truck was designed by Lewis Brandt at the Ford Motor Company\nin Geelong, Victoria.\nGeorge Shepherd invented the dome shaped castors which replaced\ntraditional pivoted wheel castors.\n1952 ATOMIC ABSORPTION SPECTROPHOTOMETER\nThe complex analytical instrument used in chemical analysis\nwas invented by Sir Alan Walsh of the CSIRO.\n1957 TROUSERS WITH PERMANENT CREASES\nThe process for producing permanently creased fabric was invented\nby Dr Arthur Farnworth of the CSIRO.\n1958 'BLACK BOX' FLIGHT RECORDER\nThe 'black box' voice and instrument data recorder was invented\nby Dr David Warren in Melbourne.\nROUND-THE-WORLD AIRLINE SERVICE\nThe first round-the-world airline service was inaugurated\n1965 INFLATABLE AIRCRAFT ESCAPE SLIDE\nThe inflatable aircraft escape slide which doubles as a raft\nwas invented by Jack Grant of Qantas.\n1967 PUNCTURE-PROOF TYRES\nPuncture-proof tyres made of hollow rubber segments bolted\nonto a steel rim, were invented by Alan Burns.\n1970 VARIABLE RATIO RACK AND PINION\nThe variable ratio rack and pinion steering in motor vehicles\nwas invented by Australian engineer, Arthur Bishop.\nLASER BEAM LIGHT HOUSE\nThe world's first laser beam lighthouse was at Point Danger,\nNew South Wales.\n1972 ORBITAL INTERNAL COMBUSTION ENGINE\nThe orbital combustion process engine was invented by engineer\nRalph Sarich of Perth, Western Australia.\n1978 SPIRALLY WOUND PLASTIC PIPES\nThe production of large diameter pipes from spirally wound\ninterlocking plastic strips was invented by South Australian\nengineer Bill Menzel.\n1979 BIONIC EAR\nThe cochlear implant was invented by Professor Graeme Clark\nof The University of Melbourne.\nThe tiny camera used in sports broadcasts was developed by\nAustralian engineer, Geoff Healey.\nCOOL LIGHTWEIGHT WOOL FABRICS\nThe technique for spinning lightweight wool was invented at\n1984 FROZEN EMBRYO BABY\nThe first frozen embryo baby was born at the Queen Victoria\nMedical Centre in Melbourne.\nWORLD'S MOST EFFICIENT SOLAR CELLS\nDr Stuart Wenham and Professor Martin Green from The University\nof Sydney invented the world's most efficient solar cells.\nCONTINUOUS SELF-CLEANING MICROFILTRATION\nA group of engineers and scientists led by Dr Doug Ford invented\nthe continuous self-cleaning microfiltration.\n1985 SPINNING CONE FLAVOUR RECOVERY\nThe system of capturing flavours and aromas normally lost\nduring food processing was invented by Dr Don Casimir at the\nWORLD'S BEST LIGHTNING PROTECTION\nThe first real advancement in lightning protection technology\nsince Franklin's sharp rods and the Faraday cage, was the\nDynasphere invented by Rick Gumley of Tasmania.\nORAL VACCINE FOR BRONCHITIS\nOral vaccine to prevent bronchitis was developed by Professor\nRobert Clancy at the University of Newcastle.\n1986 GENE SHEARS\nThe discovery of gene shears was made by CSIRO scientists,\nWayne Gerlach and Jim Haseloff.\n1987 TWO-MINUTE AIDS TEST\nThe world's fastest AIDS test was developed at The University\nCOMPUTERISED CANCER DETECTION\nResearch by Dr Bevan Reid lead to the invention of a computerised\ndevice which reliably detects cancerous and pre-cancerous\nWAVE PIERCING CATAMARAN\nSydney naval architect Phillip Hercus designed the first wave\n1988 SPLIT-CYCLE INTERNAL COMBUSTION\nThe split-cycle internal combustion engine was invented by\nengineer, Rick Mayne in Queensland.\nCONTROLLED RELEASE ORAL MORPHINE CAPSULES\nThe world's first controlled release oral morphine capsules\nwere developed by a South Australian company.\nThe first non-chemical biological control agent was invented\nat The University of Adelaide.\n1989 HYDROMETALLURGICAL COPPER PROCESS\nThe first economic hydrometallurgical process to produce base\nmetals without toxic fumes was invented by Peter Everett of\n1990 READING MACHINE FOR THE BLIND\nThe world's first reading machine for the blind was invented\nby Milan Hudecek of Melbourne, Victoria.\n1991 BIO-DEGRADABLE MARINE DEGREASER\nThe world's first bio-degradable marine degreaser was developed\nby the CSIRO and the Australian Analytical Laboratories.\n1992 MULTI-FOCAL CONTACT LENS\nThe world's first multi-focal contact lens was invented by\noptical research scientist, Stephen Newman in Queensland.\n1993 UNDERWATER PC\nThe world's first underwater computer with a five-button hand-held\nkeypad was developed by Bruce Macdonald at the Australian\nInstitute of Marine Science.\n1994 NON-INVASIVE TB TEST\nThe world's first reliable non-invasive TB test, that avoids\npossible adverse reactions, was developed by scientists in\nThe world's most sophisticated optical anti-counterfeiting\ntechnology was developed by the CSIRO.\nDue to an unresolved dispute\nwith the Australian Trade Commission (Austrade),\nwho copied and adopted as their own certain material from\nTomorrow's World, the Australian Initiative, and published\nthe material in their Australia Open for Business website,\nwithout remorse or recompense, access\nby Australian Government servers to this online edition\nhas been blocked indefinitely.","Cockpit Voice Recorders (CVR) and Flight Data Recorders\nLarge commercial aircraft and some smaller commercial,\ncorporate, and private aircraft are required by the FAA to be equipped\nwith two \"black boxes\" that record information about a flight. Both recorders\nare installed to help reconstruct the events leading to an aircraft accident.\nOne of these, the Cockpit Voice Recorder (CVR), records radio transmissions\nand sounds in the cockpit, such as the pilot's voices and engine noises.\nThe other, the Flight Data Recorder (FDR), monitors parameters\nsuch as altitude, airspeed and heading. The older analog units use one-quarter\ninch magnetic tape as a storage medium and the newer ones use digital\ntechnology and memory chips. Both recorders are installed in the most\ncrash survivable part of the aircraft, usually the tail section.\nEach recorder is equipped with an Underwater Locator\nBeacon (ULB) to assist in locating in the event of an overwater accident.\nThe device called a \"pinger\", is activated when the recorder is immersed\nin water. It transmits an acoustical signal on 37.5 KHz that can be\ndetected with a special receiver. The beacon can transmit from depths\ndown to 14,000 feet.\nFollowing an accident, both recorders are immediately\nremoved from the accident site and transported to NTSB headquarters\nin Washington D.C. for processing. Using sophisticated computer and\naudio equipment, the information stored on the recorders is extracted\nand translated into an understandable format. The Investigator-in-Charge\nuses this information as one of many tools to help the Safety Board\ndetermine the Probable Cause of the accident.\nThe Cockpit Voice Recorder\nThe CVR records the flight crew's voices, as well as\nother sounds inside the cockpit. The recorder's \"cockpit area microphone\"\nis usually located on the overhead instrument panel between the two\npilots. Sounds of interest to an investigator could be engine noise,\nstall warnings, landing gear extension and retraction, and other clicks\nand pops. From these sounds, parameters such as engine rpm, system failures,\nspeed, and the time at which certain events occur can often be determined.\nCommunications with Air Traffic Control, automated radio weather briefings,\nand conversation between the pilots and ground or cabin crew are also\nA CVR committee usually consisting of members from the\nNTSB, FAA, operator of the aircraft, manufacturer of the airplane, manufacturer\nof the engines, and the pilots union, is formed to listen to the recording.\nThis committee creates a written transcript of the tape to be used during\nthe investigation. FAA air traffic control tapes with their associated\ntime codes are used to help determine the local standard time of one\nor more events during the accident sequence. These times are applied\nto the transcript using a computer process which provides a local time\nfor every event on the transcript. More precise timing for critical\nevents can be obtained using a digital spectrum analyzer. This transcript\ncontains all pertinent portions of the recording and can be released\nto the public at the time of the Safety Board's public hearing.\nThe CVR recordings are treated differently than the\nother factual information obtained in an accident investigation. Due\nto the highly sensitive nature of the verbal communications inside the\ncockpit, Congress has required that the Safety Board not release any\npart of a CVR tape recording. Because of this sensitivity, a high degree\nof security is provided for the CVR tape and its transcript. The content\nand timing of release of the written transcript are strictly regulated:\nunder federal law, transcripts of pertinent portions of cockpit voice\nrecordings are released at a Safety Board public hearing on the accident\nor, if no hearing is held, when a majority of the factual reports are\nTime recorded .......................... 30 min continuous,\n2 hours for solid state digital units\nNumber of channels ................. 4\nImpact tolerance ...................... 3400 Gs /6.5ms\nFire resistance .......................... 1100 deg C /30 min\nWater pressure resistance ........ submerged 20,000 ft\nUnderwater locator beacon ...... 37.5 KHz\nBattery: 6yr shelf life\n30 day operation"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:87ab6307-d58c-4ba5-b237-39cb41c6685d>","<urn:uuid:f26ff42f-6179-4c53-90ee-05c41d924874>"],"error":null}
{"question":"How does proper gravel extraction protect water resources, and what broader water quality issues arise from mining activities?","answer":"Proper gravel extraction techniques include maintaining undisturbed buffers, avoiding mining deeper than water elevation, and keeping equipment away from stream banks. Gravel washing should occur far from channels to prevent warm, silty water from entering streams. For mining in general, water quality is severely impacted - mining can contaminate surface water and groundwater through chemicals from mining processes. In gold mining specifically, while small-scale panning has little effect on water bodies, large-scale mining can have tremendous negative effects on water quality. Additionally, any materials stored in channels risk spills during high flows that could contaminate water resources.","context":["Mining Gravel and Protecting Streams\nOwners of streamside land have always faced flooding and erosion due to natural events, but damages caused by in-stream mining should not be an additional threat to personal property.\nMore than 500 in-stream mining sites exist in Missouri. Considering the potential harm any one gravel mining operation can cause, it is important that gravel mining occur only with proper safeguards.\nMissouri's streams are vulnerable to huge problems caused by gravel mining. These waterways have formed within their valleys over thousands of years. During this period, the stream meandered across its flood plain. Because this movement of the stream channel happens very slowly over many years, natural erosion occurs at a rate to which the stream can adjust.\nThe characteristics of a given stream are not formed on hot summer days when there is little water flowing in the channel. Instead, our streams are carved out during periods when the stream channel is flowing at least half full. During high flow, the water has enough power to cause changes.\nStreams are a series of alternating deep areas called pools and shallow areas called riffles. Riffles are high areas of the stream bottom that control the slope of the stream. It's because riffles help control the slope of the stream that they work to control erosion.\nGenerally speaking, erosion occurs on outside bends and silt, sand and gravel deposition occurs on the inside bends. This process produces the sinewy characteristics that we are accustomed to seeing as we float, fish or swim in our favorite stream.\nGravel mining often results in piles or pits that, during high water events, can rapidly increase the erosive process in streams.\nAlthough many of our streams have plenty of gravel to spare, we also have many streams which cannot afford to have gravel removed from them. To ensure you are mining from a stream that has ample gravel, look for some key characteristics. Streams with excess gravel generally have gravel bars with little or no vegetation growing on them. The rocks are generally small (less than three inches in diameter) and are loosely packed.\nGravel bars that are vegetated or where the rocks are tightly packed (not easily loosened when kicked) are characteristic of healthy streams with just the right amount of gravel. These should not be disturbed.\nWhen practical, rock aggregate should be acquired from non-stream sources such as nearby rock quarries, but if you must remove sand or gravel from a stream or its flood plain, at least do it in ways that will not damage your property or your neighbor's property.\nWithout a doubt, the worst damage caused by gravel mining is the extensive erosion that results when the mining operation makes the slope of the stream channel steeper than it was before mining. When a stream channel is made steeper, the water flows faster; and the faster it flows, the more power it has to cause erosion. In these situations, the bottom of the stream channel erodes away first, and then the stream banks fall in and wash away.\nIf the gravel mine is a pit dug in the stream channel and the miner continues to take gravel over a long period of time, the erosion can move far upstream and cause extensive property damage to many streamside landowners. Because of these potential problems, gravel should never be mined deeper than the water elevation at the time of removal. If the stream channel is dry, mining should not be deeper than the elevation of the stream bottom at the site. If you don't mine too deep, the natural slope of the channel will not steepen, and the risk of serious erosion damage to neighbors will be reduced.\nGravel should never be removed from riffles because breaking up the riffle threatens channel stability and important habitats for fish and other aquatic life.\nGravel miners need to get their equipment through the stream corridor and into the stream channel, but the wooded corridor along the stream protects the waterways and, itself, needs to be protected. This forested area slows erosion, filters excess nutrients and sediments, baffles powerful flood flows and provides important habitat for aquatic and terrestrial plants and animals. It also protects the waterway from our other activities in the flood plain and watershed and keeps stream temperatures cooler with its shade.\nWhen mining gravel, maintain an undisturbed buffer at least 50 feet wide at the top of the high bank and that extends for the length of the excavation site. Construct access points from the high bank into the channel to minimize erosion of both the access road and stream banks. Make sure you replant areas disturbed for access once excavation is complete.\nIt's also important to leave a 20-foot wide buffer from which mining equipment is restricted between the mined area and stream bank vegetation. Keeping mining equipment out of this area assures that the stream will not change its course as a result of the mining. It will also prevent eroded material from causing siltation problems downstream and will protect the roots of plants that prevent bank erosion.\nWhen gravel mining, never relocate or straighten stream channels. Doing so can cause instability that leads to excessive erosion of the streambed and banks.\nOnce mining is finished, unused material should be returned to the removal site and smoothed to mimic the original contours of the bar. Unused material should not be stockpiled in the channel, placed against the stream banks or deposited in a stream side wetland. Stockpiling material in the channel obstructs high stream flow, which can increase stream bank erosion. Pushing material against stream banks may actually increase erosion at the site. Remember, any sand, gravel, silt or other sediment eroded from one place will deposit somewhere else and may cause you or your neighbor serious problems.\nWater quality is always a primary concern, so fuel, oil and other wastes should not be stored in the channel. Sudden high flows from rain storms may cause spills.\nFish, macro-invertebrates and other stream creatures rely heavily on the stream bottom to maintain healthy populations. The stream bottom provides food, spawning habitat, a place to protect eggs, nursery habitat and shelter from predators. It only makes sense that gravel mining can be harmful to these stream dwellers.\nThe smallest particles of sand and soil carried by a stream are called \"fines.\" This is what makes a stream look muddy after a heavy rain. Normally, these fines settle as the water level in the stream falls after a rain and no harm comes from their presence. If we do something to cause an excessive amount of fines, then the water remains muddy and the bottom may become covered with this silt, causing many problems for fish and other aquatic life. We can prevent some of the risk by avoiding mining during the spawning season.\nA common practice in gravel mining is to wash the gravel to remove these fines. Sand and gravel washing as well as gravel crushing and sorting should occur far enough away from the channel so that the warm, stagnant, silty wash water cannot enter the stream. This will protect water quality and prevent sedimentation (silting) of important stream bed habitats.\nUsing proper techniques and safeguards, it's possible to excavate sand and gravel without increasing erosion and with little or no damage to important habitats of aquatic plants and animals. If you have questions about proper ways to excavate sand and gravel from stream channels contact the Missouri Department of Conservation for guidance and a free brochure.","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7606786b-9f45-4c3e-b13b-06f71147c7ba>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"As an environmental consultant, I'm interested in understanding how emergency environmental permits are processed and what constitutes proper environmental due diligence. Can you explain the emergency permit process and standards for environmental assessments?","answer":"Emergency permits are issued when circumstances pose an immediate threat to public health, safety, or substantial property. The Department of State Lands can provide rapid approval after verifying that the situation requires prompt action and the proposed work is the minimal amount necessary to address the threat. For standard environmental due diligence, a Phase I ESA is typically the starting point, consisting of public records review, site visits, interviews with owners and officials, and a professional assessment of contamination risk. However, a Phase I ESA alone may not be sufficient - additional methods often needed include Phase II environmental site assessments (soil/groundwater testing), environmental compliance audits, asbestos surveys, wetland surveys, and computer database searches for historical environmental issues and Superfund site involvement.","context":["Virtually all business transactions involve some level of environmental risk. The key is to identify all of the potential risks and collect sufficient information about them early in the due diligence period of a transaction. This proactive approach to environmental due diligence will help the buyer determine whether the risks are acceptable in light of the overall transaction and develop a strategy for managing them, both in the contract negotiations prior to acquisition and after the transaction is complete.\nHow much and to what extent businesses should conduct environmental due diligence typically depends on the nature of the transaction and the anticipated use of the property after purchase.\nBelow are five tips for buyers to consider when determining the appropriate level of environmental due diligence that should be performed in business transactions.\nAlthough the tips are geared toward buyers, sellers should also consider them so that they can better anticipate the types and level of due diligence that a buyer may perform (and understand their reasons for doing so). Identifying and correcting environmental problems in advance of the transaction may help prevent buyer demands for purchase price reductions or substantial escrows to cover potential environmental liabilities.\nThe most common types of environmental liability and costs associated with businesses and their properties include:\nWhen purchasing a property with no operations or structures from an unrelated third party, the environmental concerns may be limited to contamination on or emanating from the property. A Phase I environmental site assessment (Phase I ESA) that does not identify any environmental contamination concerns may be sufficient in that context. This scenario, however, is rare.\nA buyer planning to redevelop a property with existing structures will be concerned about the potential for added costs and delays associated with hazardous materials in the structure and conditions that may require special permits or design modifications. Purchasers of property who will continue the same operations will be concerned about the company’s compliance with environmental laws and permits and whether substantial expenditures may be necessary for the operations to achieve or maintain compliance. Stock purchasers and, as discussed in Tip 4, some asset purchasers will be concerned about all of these potential liabilities, including legacy environmental liabilities.\nA Phase I ESA is a non-intrusive investigation into past and current uses of a property to evaluate the potential existence of hazardous substance or petroleum contamination on or migrating from the property.\nA Phase I ESA generally consists of the following activities by an environmental professional:\n(i) a review of publicly available records;\n(ii) a site visit;\n(iii) interviews of owners, occupants and government officials; and\n(iv) a report describing the results of the investigation and the environmental professional’s opinion as to whether there is specific evidence of, or reason to believe, that the property is contaminated.\nAlthough it may contain general information about the property and facility operations, a Phase I ESA does not specifically address whether the operations comply with environmental laws and permits or whether there are site conditions that could affect the buyer’s development plans. A Phase I ESA also does not address the company’s potential liability for off-site waste disposal or legacy environmental liabilities.\nGiven that a Phase I ESA does not address many of environmental liabilities that a buyer may be concerned about, what other environmental due diligence methods are available? Below are a few of the most common additional due diligence methods.\nPhase II Environmental Site Assessments – testing of soil/groundwater to determine whether the property is, in fact, impacted.\nEnvironmental compliance audits – evaluation of the facility operations to determine whether all required permits have been obtained and the operations comply with the permits and environmental regulations.\nAsbestos surveys – identification and testing of building materials suspected to contain asbestos (Building materials that contain asbestos must be removed by a licensed asbestos contractor prior to any renovation or demolition activity.)\nWetland surveys – identification of wetlands subject to federal or state regulation in connection with construction and development activities\nComputer database searches – third-party database search services may be used to research the company’s former properties and businesses as well as determine whether it has been named a potentially responsible party (PRP) at Superfund sites\nFollowing a stock transaction, the surviving corporation will obtain all of the liabilities and obligations of the merging corporation. That’s why environmental due diligence in a stock deal should be comprehensive and include not only current properties and operations of the company, but also its former properties and operations, as well as any businesses that have been acquired/divested.\nBuyers often assume that if the transaction is structured as an asset deal rather than a stock deal, they need not worry about environmental liabilities that are unrelated to the purchased assets. While the general common law rule is that asset purchasers are not liable for debts and obligations of the seller corporation, there are four exceptions:\n(a) The purchaser expressly or impliedly agrees to assume seller’s liabilities;\n(b) The transaction is a de facto merger;\n(d) The purchaser is a “mere continuation” of the seller; or\n(e) The transaction is a fraudulent effort to escape liability.\nMany federal and state courts have found, applying especially the second and third exceptions, that a purchaser of substantially all of the assets of a company will assume the company’s environmental liabilities, whether or not they are associated with the purchased assets.\nRegardless of the exceptions applied, court decisions on successor liability for asset purchasers are made on a case-by-case basis and tend to involve fact-intensive analyses. The following factors have been relied on in finding that successor liability applies to asset purchasers.\nEach of these factors should serve as potential red flags to the buyer that its asset purchase may make it a successor of the seller — one who is now responsible for all of the seller’s environmental liabilities. If one or more of these factors are present following a transaction, the buyer should investigate the gamut of seller’s potential environmental liabilities, including its compliance history and that of its former properties, businesses and operations.\nLast, but not least, consult with an environmental attorney early in the transaction. Be sure to provide the attorney with all available details about the proposed structure of the transaction and the plans for the property and operations after the closing. The attorney can assist in developing a strategy and timetable for completing the appropriate environmental due diligence. Often, an attorney can also advise on steps that can be taken before and after the transaction to minimize or manage environmental risk.\nAlthough we would like to hear from you, we cannot represent you until we know that doing so will not create a conflict of interest. Also, we cannot treat unsolicited information as confidential. Accordingly, please do not send us any information about any matter that may involve you until you receive a written statement from us that we represent you (an â€˜engagement letterâ€™).\nBy clicking the â€˜ACCEPTâ€™ button, you agree that we may review any information you transmit to us. You recognize that our review of your information, even if you submitted it in a good faith effort to retain us, and, further, even if you consider it confidential, does not preclude us from representing another client directly adverse to you, even in a matter where that information could and will be used against you. Please click the â€˜ACCEPTâ€™ button if you understand and accept the foregoing statement and wish to proceed.","The Department of State Lands issues two types of permits and authorizations:\n- Removal-fill permits for removal or fill activity in waterways and wetlands\n- Proprietary waterway authorizations for use of state-owned waterways\nRemoval-fill activity on state-owned waterways requires both.\nThe information on this webpage pertains to removal-fill permits only. For information on obtaining a waterway authorization (for a dock, floating home or marine industrial facility for example), go to the Use of State-Owned Waterways page.\nOregon's Removal-Fill Law (ORS 196.795-990) requires people who plan to remove or fill material in wetlands or waterways to obtain a permit from the Department of State Lands. This permit is broadly referred to as the “Removal-Fill Permit.” The law applies to all landowners, whether private individuals or public agencies.\nThe purpose of the law, enacted in 1967, is to ensure protection and the best use of Oregon’s water resources for home, commercial, wildlife habitat, public navigation, fishing and recreational uses.\nIn most cases, a permit is required if an activity will involve filling or removing 50 cubic yards or more of material in a wetland or waterway. For activities in state-designated Essential Salmonid Habitat, State Scenic Waterways and compensatory mitigation sites, a permit is required for any amount of removal or fill.\nRemoval-Fill Guide: Detailed information on process, timelines and requirements\nThe Removal-Fill Guide is designed to help applicants understand the process, timelines and other important topics related to the Department of State Lands' administration of Oregon's Removal-Fill Law. It is organized into nine chapters:\n- Chapter 1: Working with the Aquatic Resource Management Program\n- Chapter 2: When is a Permit Required?\n- Chapter 3: What Activities are Exempt?\n- Chapter 4: Planning Ahead\n- Chapter 5: How to Apply for a Permit\n- Chapter 6: Processing the Removal-Fill Permit Application\n- Chapter 7: Emergency Permits\n- Chapter 8: Compensatory Mitigation Planning for Wetlands and Tidal Waters\n- Chapter 9: Monitoring the Compensatory Wetland Mitigation Site\nThe guide addresses existing laws and rules governing removal-fill activities in Oregon, and provides practical tips for complying with Department of State Lands' regulations. It explains agency practices, but does not take the place of or override regulations. The reader is cautioned to consult agency regulations first, and to rely on this guide to help understand those regulations and complete permit applications. Consultation with agency staff early in the project's development is strongly encouraged.\nRemoval-Fill Guide December 2016\nTo navigate the document more easily, be sure to use “bookmarks” in Adobe Reader.\nPlease note: Guide updates are made on a periodic basis. If you print or save this guide to your own computer, be sure to periodically check this website to make sure you are using the most recent version of the guide. The current version date is posted on the lower left corner of the Guide cover page.\nWorking with the Aquatic Resource Management Program\nThe Department of State Lands is organized into three programs: Aquatic Resource Management, Business Operations and Support Services, and Common School Fund Property. In addition, DSL oversees the South Slough National Estuarine Research Reserve in Charleston in partnership with the National Oceanic and Atmospheric Administration (NOAA).\nThe removal-fill permit program is administered by the Aquatic Resource Management Program (ARM) whose mission is to conserve and protect waters of the state, including wetlands. The program provides a variety of services, including wetland determinations, wetland delineation report review, responding to wetland land use notices, conducting pre-application meetings, and providing assistance for removal-fill permit application processing.\nThe Aquatic Resource Management Program is organized in five regional teams: Metro, Northwest, Midwest, Southwest and Eastern. Each team is comprised of:\n- Aquatic Resource Coordinator responsible for removal-fill permitting activities.\n- Jurisdictional Coordinator responsible for preparing wetland determinations and reviewing wetland delineation reports.\n- Proprietary Coordinator responsible for authorizing activities on state-owned waterways.\nRemoval-Fill Guide: Chapter 1 – Working with the Aquatic Resource Management Program\nWhen is a permit required?\nOregon's Removal-Fill Law (ORS 196.795-990) requires any person who plans to \"remove or fill\" material within \"waters of the state\" to obtain a permit from the Department of State Lands. Removal means taking rock, gravel, sand, silt, other inorganic substances, and large woody debris from the bed or banks of a waterway, or their movement by artificial means within the bed or banks, including channel relocation. Fill means the deposit by artificial means of any material (organic or inorganic) at any one location in the bed or banks. Waters of the state include wetlands on private and public land.\nTypes of \"waters of the state\" and jurisdictional limits:\n- Pacific Ocean: extreme low tide to 3 miles out\n- Tidal Bays and Estuaries: highest measured tide or upper edge of wetland\n- Perennial Streams, Lakes and Ponds: to ordinary high water\n- Intermittent Streams: to ordinary high water\n- Wetlands: wetland boundary as determined by delineation report\n- Artificial Ponds and Ditches: ordinary high water\n- Artificial Wetlands: wetland boundary\n- Reservoirs: normal operating pool level or upper edge of adjacent wetland\nFor most waters, a permit is required if a project will involve 50 cubic yards of fill and/or removal (cumulative) within the jurisdictional boundary. For activities in designated Essential Salmonid Habitat waters, State Scenic Waterways and designated compensatory mitigation sites, a permit is required for any amount of removal or fill. Removal is calculated on an annual basis. Fill is calculated on a cumulative basis.\nRemoval-Fill Guide: Chapter 2 - When is a Permit Required?\nSome activities in waters of the state are exempt from permit requirements of the Removal-Fill Law. Unless otherwise stated, the exemptions do not apply to State Scenic Waterways.\nEach exemption has specific side boards and limitations. It is important to thoroughly review the exemptions before assuming an activity is exempt. You are strongly encouraged to contact an Aquatic Resource Coordinator for help in understanding these exemptions.\nExempt activities include:\n- State Forest Management Practices\n- Fills for Construction, Operation and Maintenance of Hydroelectric Dams and Water Diversion Structures\n- Navigational Servitude (Maintenance of the Navigational Channel)\n- Maintenance or Reconstruction of Water Control Structures\n- Maintenance or Emergency Reconstruction of Roads and Transportation Structures\n- Prospecting and Non-Motorized Activities within Essential Salmon Habitat and State Scenic Waterways\n- Fish Passage and Fish Screening Structures in Essential Indigenous Anadromous Salmonid Habitat (ESH)\n- Change in Point of Diversion for Surface Water\n- Certain Removal of Large Wood Debris\n- Certain Voluntary Habitat Restoration Activities\n- Agricultural Exemptions\n- Special Situations: Railroads, Tribal Lands and Environmental Remedial Actions\nRemoval-Fill Guide: Chapter 3 – What Activities are Exempt?\nA well-planned project will result in an easier and faster permitting process. Important steps in the planning process include:\nIdentifying regulated waters on the project site. Early identification of regulated waters and their jurisdictional boundaries is essential for informed project planning. National and local wetland maps are helpful tools for early identification of wetlands, but they are not conclusive. While lakes and rivers are easily identifiable, regulated intermittent streams, channelized streams, ditches and ponds can be more difficult, and require additional investigation.\nRetaining professional consultant services. Most projects involving wetlands and waterways require the technical expertise of wetland or environmental consultants to determine wetland boundaries, prepare functional assessments and develop mitigation plans.\nExploring alternatives to avoid and minimize impacts. Applications for removal fill permits must include a consideration of alternative sites, designs and construction methods showing that the proposed project is the practicable alternative with the least impact to wetlands and waterways.\nPlanning to mitigate for unavoidable impacts. If some impacts to wetlands or waterways are unavoidable, the applicant must propose mitigation to replace the functions and values lost as a result of the project.\nPre-design permit scoping. Most projects in wetlands and waterways require approval from several local, state and federal agencies. Early identification of all the permits and their requirements is essential to avoid costly redesign and project delays.\nPre-application meetings. The Department offers pre-application meetings to assist applicants in planning ahead for a smooth permitting process.\nRemoval-Fill Guide: Chapter 4 – Planning Ahead\nHow to apply for a permit\nThere are four types of permits available to conduct work in wetlands and waterways:\nIndividual Permits (IPs) are issued for projects that:\n- Have more than minimal adverse effects to waterways and wetlands\n- Are more complicated and often involve more than one removal-fill activity\n- May involve a substantial mitigation obligation\n- Do not qualify for any of the General Authorizations or General Permits\nIP applicants use the Joint Permit Application. The processing timeline is up to 120 days.\nGeneral Authorizations (GAs) are an expedited process for nine specific types of removal-fill activities that have minimal adverse effects on wetlands and waterways:\n- Certain Minimal Disturbance Activities within Essential Salmon Habitat (ESH)\n- Piling Placement and Removal within ESH\n- Temporary Impacts to Non-Tidal Wetlands\n- Waterway Bank Stabilization\n- Certain Transportation-Related Activities\n- Removing and Disposing of Sediment Behind Tidegates and within Hydraulically Closed Perimeters\n- Waterway Habitat Restoration\n- Wetland Ecosystem Restoration\n- Recreational Placer Mining\nGA applicants use the General Authorization Notification Form. The processing timeline is up to 30 days.\nGeneral Permits (GPs) authorize a group of activities that are substantially similar in nature, recurring or ongoing, and have predictable effects and outcomes. There are currently five GPs available for use by the public:\n- Transportation-Related Structures\n- Minor Removal-Fill Impacts to Certain Non-Tidal Wetlands\n- Impacts to Vernal Pool Wetlands and Other Waters of the State in Jackson County, Oregon\n- Maintaining Drainage to Protect Agricultural Land\n- Navigational Access Maintenance Dredging\nGP applicants use the Joint Permit Application. The processing timeline is up to 40 days for most GPs.\nEmergency Permits (EPs) are rapid-approval authorizations for emergencies that pose a direct threat to human health, safety or substantial property, and where prompt removal-fill action is required to address the threat.\nEP applicants use the Emergency Permit Application. Approval is given as quickly as possible in emergency situations.\nRemoval-Fill Guide: Chapter 5 – How to Apply for a Permit\nProcessing the application\nGeneral Authorization Notifications\nThe Department reviews the GA notification within 30 days of receipt to confirm that it is complete and eligible.\nIndividual Permit and General Permit Applications\nStep 1: Application Completeness Review: (up to 30 days for IPs/up to 15 days for GPs) The application is reviewed for completeness and applicable permit type. A completeness review letter is sent to the applicant documenting the review. If the application is deemed incomplete, a new complete application is required.\nStep 2: Public Review Period: (30 days for IPs/15 days for GPs) If the application is complete, the public review period is initiated. A notice is sent to other agencies, adjacent property owners and other parties inviting comment on the application.\nStep 3: Final Review: (up to 60 days for IPs/up to 10 days for GPs) Comments relevant to the decision-making process are considered. The applicant is invited to address relevant comments and any unresolved technical issues by providing additional information or revising the project.\nStep 4: Permit Decision: The entire record is evaluated against the criteria for permit issuance and a permit is approved or denied. If more time is needed to address issues, the applicant may request an extension of the decision deadline.\nPermit Renewal and Transfer\nPermits may be issued for up to five years and may be renewed upon request. Before an IP expires, DSL will notify the permittee of the opportunity to renew the permit. Most GAs are issued for three years and are not renewable. Most GPs are issued for five years and are not renewable.\nModifying the Permit\nModification of an Individual Permit or a General Permit may be requested by the permittee or initiated by the Department.\nSpecial Permit Situations\nBy law, projects governed by the Corrections Facilities Siting Authority, Environmental Quality Commission pertaining to solid waste landfills, Energy Facility Siting Council and the Economic Recovery Review Council follow a removal-fill permit process that is different than the standard IP process.\nA permit or authorization decision may be appealed by the applicant or third parties that are “aggrieved” or “adversely affected” by the authorization decision. Applicants may appeal an incompleteness determination. Appeals are adjudicated through the contested case hearing process.\nRemoval-Fill Guide: Chapter 6 – Processing the Removal-Fill Permit Application\nWithin the context of the Removal-Fill Law, an emergency is a natural or human-caused circumstance that poses an immediate threat to public health, safety or substantial property, including cropland. If the actions necessary to alleviate the threat involve 50 cubic yards or more of removal or fill below a waterway’s ordinary high water elevation or in wetlands (or any amount of removal or fill in designated Essential Salmon Habitat (ESH) or a State Scenic Waterway (SSW) a DSL permit is required and may be authorized as an expedited emergency permit (EP).\nDSL applies all of the following considerations to assess whether an activity in wetlands or a waterway is eligible for an Emergency Permit (EP):\n- Does the nature of the threat allow enough time to obtain some other form of permit or is prompt action required to reduce or eliminate the threat?\n- Does the emergency pose a direct threat to public health or safety or substantial property, including but not limited to a dwelling, transportation structure, farm or cropland?\n- Is the proposed project the minimal amount necessary to reduce or eliminate the threat and does it minimize, to the extent practicable, adverse impacts to wetlands and waterways?\nProcedure for Obtaining an Emergency Permit\nStep 1: The applicant must collect information including location; nature and cause of the threat; the condition of the waterway; and what action is necessary to alleviate the immediate threat. Remedial actions must be limited to what is necessary to make repairs or prevent irreparable harm, injury or damage to persons or property.\nStep 2: Contact DSL to initiate the energy permit process.\n- During Business Hours: 503-986-5200 (west of the Cascades) 541-388-6112 (east of the Cascades)\n- After Business Hours: Oregon Emergency Response: 1-800-452-0311\nStep 3: Submit the energy permit application materials as directed by the DSL Aquatic Resource Coordinator and as time allows. If time allows, DSL may conduct a site visit or ask another designated agency to do so.\nStep 4: Qualifying activities will be issued a written permit as soon as key information is provided. DSL can issue a verbal approval in advance of the written approval where it is necessary to protect public health, safety or property.\nAfter the emergency, DSL staff may visit the site upon completion of the emergency work, and may require that the project be modified after the initial emergency work is completed or require mitigation to compensate for any impacts to the affected wetland or waterway. A subsequent permit may be required to conduct remedial work.\nRemoval-Fill Guide: Chapter 7 – Emergency Permits\nYou may wish to hire a private consultant who specializes in wetlands and waterways regulation to assist you with conducting a wetland delineation and/or removal-fill permit application and mitigation plan. Information to help you hire a qualified wetland consultant may be found in the Wetlands in Oregon fact sheet.\nMany wetland professionals working in Oregon are certified as a Professional Wetland Scientist (PWS) or Wetland Professional in Training (WPIT) by the Society of Wetland Scientists’ Professional Certification Program. To be certified as a PWS or WPIT, a person must meet education and experience requirements, and adhere to a code of ethics and professional practice.\nRemoval-Fill Guide: Chapter 4 – Planning Ahead\nTopics of interest"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:a358cb66-9381-4558-b730-1c30b51f4331>","<urn:uuid:5a7427fc-16c6-434d-a8cb-6c7c5371b660>"],"error":null}
{"question":"How do Voyager 1 and 2 help us understand the solar system's boundary, and what communication infrastructure supports their mission?","answer":"Voyager 1 and 2 have helped identify a distinct boundary where the solar system ends and interstellar space begins, crossing the heliopause at similar distances (121.6 and 119 astronomical units respectively). The spacecraft communicate through NASA's Deep Space Network, which consists of huge satellite dishes at three locations worldwide - near Barstow (California), Madrid (Spain), and Canberra (Australia). These sites are positioned 120 degrees apart in longitude to maintain continuous coverage as spacecraft move across the sky.","context":["NASA probe provides insight on solar system's border with interstellar space\nNASA probe provides insight on solar system's border with interstellar space\nNASA Voyager 2 Data Reveals There is a 'Distinct Boundary' to Solar System\n06 November, 2019, 19:55\nThese five studies describe Voyager 2's journey through its transition through the heliopause (part of the solar system exposed to particles and ions of deep space) and heliosheath (the region of the heliosphere beyond the termination shock) to what lies beyond in the universe.\n\"In a historical sense, the old idea that the solar wind will just be gradually whittled away as you go further into interstellar space is simply not true\", said Don Gurnett, co-author of a second plasma density paper and a professor emeritus of physics and astronomy at The University of Iowa in the US. Then Voyager 1 flies past star Gliese 445, at 17.6 light years from Earth.\nResearchers from the University of Iowa have been carrying out this research.\nComparing what Voyager 2 learned with the data available from the Voyager 1 crossing provides interesting similarities and contrasts, Stone said. Together, these findings will help scientists picture this cosmic shoreline, \"where the environment created by the [Earth's sun] ends and the vast ocean of interstellar space begins\".\nGreater than 4 many years after starting its epic journey, NASA'sVoyager 2 spacecraft has crossed the elusive boundary that marks the sting of the Solar's realm and the beginning of interstellar area, scientists have introduced. This alignment was discovered by Voyager 1, and has now been confirmed by the Voyager 2data.\nThe two probes' heliopause crossings occurred at similar distances from the Sun: For Voyager 1, it was at 121.6 astronomical units, and for Voyager 2, it was 119 astronomical units (one AU equals the average distance from Earth to the Sun). Two spacecrafts had different paths: Voyager 2 didn't rush to the edge of Solar System, it explored Uranus and Neptune during planetary flybys first. The data also supports into a debate about the shape of the heliosphere, which some models sought to be spherical and others more like a wind sock, with a long tail floating out behind.\nOur bubble protects the solar system from this \"interstellar wind\", and from cosmic radiation which could wreak havoc on our DNA.\nThe Iowa scientists say the heliosheath has varied thickness, based on data showing Voyager 1 sailed 10 AU farther than its twin to reach the heliopause. After over 40 years, both are functioning well enough to measure cosmic rays from the Sun as well as from interstellar space; the properties of nearby charged particles; the local magnetic field; and, in the case of Voyager 2, the energy of the local plasma. 2019. Voyager 2 plasma observations of the heliopause and interstellar medium. \"But now that both Voyagers are exploring interstellar space, we have made a small, but significant, step towards reaching the nearest stars\". \"Two people go up to an elephant with a microscope, and they come up with two different measurements. It does not surprise me that a sharp boundary forms\". An accompanying perspective suggests that Voyager 2 is going to be the last spacecraft to cross this boundary for at least 25 years.\nIn some ways, what Voyager 2 experienced was surprisingly different from what Voyager 1 found when it passed into interstellar space in 2012. In that space, the sting of our solar system's bubble, solar winds meet a movement of interstellar wind and fold back on themselves. With Voyager 1, scientists had only one sample of these magnetic fields and couldn't say for sure whether the apparent alignment was characteristic of the entire exterior region or just a coincidence. Voyager 2 also sent out signals that hint that the plasma outside the heliosphere could be compressed as it is slightly warmer, though it is unclear what is causing the compression.\nHowever, the width of the layer seen by Voyager 2 matches the width of a so-called stagnation region detected by Voyager 1. \"They're in their own orbits around the galaxy for five billion years or longer\". They will likely not run into anything and could very well outlast planet Earth.\nThis is Minnesota's best season in a long time, so losing him would have been extremely tough for the program. They are now 8-0 and ranked 13th in the country before Saturday's very important game against Penn State.\nJackie Bradley Jr . was one of the three finalists for the AL center field Gold Glove, but he failed to win his second straight. With the win, Kiermaier tied third baseman Evan Longoria (2009, 2010, 2017) for the most in franchise history.\nIt seems that Prince William saw his mom's personality in Kate Middleton's family as he feels very at home with them. She may be Britain's future Queen, but Kate Middleton needs a night off just like the rest of us.\nThe Jordanian police spokesman told A Jazeera that the attacker is under investigation to determine the reason for the attack. The attack was claimed by the Islamic State group (IS) and 10 people were eventually convicted of carrying it out.\nDetroit battled throughout the entire matchup, but oftentimes these games can come down to the final drive or the final play. The Lions will most likely continue to run their offense through Stafford as their rushing attack has been abysmal all year.\nBe that as it may, it is the first time that police used live fire in Baghdad since resumed their protests on Friday 25 October. The announcements, however, have failed to appease determined, who appear determined to keep pressure on the government.\nOnly in August, HP said Chief Executive Dion Weisler was leaving because of a \"family health matter\" it didn't disclose. Xerox is doing extremely well this year, since its shares are up 84% on a year-over-year basis.\nBy focusing light onto the microphone's diaphragm, they were able to make it vibrate as if it was hit by sound. Most smart speakers , and other smart devices have a microphone so that they can hear the user's command.\nSigning a star like Williams would help them kick off life in the Northern Hemisphere's top rugby league circuit with a bang. He appeared 73 occasions for Canterbury and 45 occasions for Sydney within the Nationwide Rugby League in Australia.\nOverwatch 2 Release Date, Rumors, and News\nA lighting artist who worked on the Overwatch 2 cinematic tweeted that the \"PR stuff out of our control\" affected the hype. Everything from Overwatch will be carried over to Overwatch 2 , regardless of when players make the decision to upgrade.\nFirst test for new-look Windies | Local Sports\nPollard will be returning to the ODIs after three years since he last played against Pakistan in the United Arab Emirates in 2016. Afghanistan will test West Indies with its experienced spin attack led by captain Khan, Mohammad Nabi and Mujeeb Ur Rahman .\nPro-Beijing lawmaker in Hong Kong stabbed while campaigning\nHo told reporters after an initial treatment that the stab left a minor 2-centimeter-deep wound, blocked by his rib cage. Afterwards, Mr Ho reportedly said the attackers were \" defending their home and people \", but denied any involvement.\nOhio St, LSU, Alabama, Penn St top CFP rankings\nThe Bruins have won three straight games after a bad start and are still in control of their Pac-12 South championship hopes. In that effort, Minnesota could lend a hand with an upset win over Penn State, although UM needs much more help than that.\nCanadian regulators seize and shut down an Exchange\nLegal representatives of the exchange say customers will be informed of the exchange's status in the next 30 to 60 days. Einstein is just the latest crypto company that appears to have to con its clients out of their hard-earned money.\nTakeaways from Ukraine testimony\nThe statement was also supposed to commit to investigating possible attempts to meddle in the 2016 U.S election. The security aid was approved by Congress to help Ukraine curb Russian-backed separatists in eastern Ukraine.\nIraq security kill Baghdad protesters with live rounds\nIt was organized in defiance of the Iraqi military, which attempted to clamp down on the protests by imposing a nightly curfew. Under ex-dictator Saddam Hussein, rallies that were not exuberantly supporting him or his Baathist government were banned.\nClimate crisis: 11,000 scientists warn of ‘untold suffering’\nWithout directly naming the US, Macron said he \"deplores the choices made by others\" as he sat next to Xi following the talks. More than 11,000 climate change scientists have come together to issue an urgent request: Quit having so many babies.","NASA's New Horizons spacecraft is delivering amazing images of Pluto, but receiving them from 3 billion miles (4.8 billion kilometers) away is no easy feat.\nThe strength of the radio signals, the time it takes the signal to travel back and forth and the speed of the data flow all present challenges, but it's all in a day's work for NASA's Deep Space Network. Think of it as the long-distance phone company for the solar system and beyond.\nThe DSN consists of a network of huge satellite dishes spread across three sites -- near Barstow in California; near Madrid in Spain; and near Canberra in Australia. Those locations are about 120 degrees in longitude apart to give wide coverage of the skies, so before a spacecraft is lost by one antenna, another one can pick it up.\nThe sensitive antennas work alone or in groups and communicate with about 30 space probes each month, said Jeff Osman, contact technical manager for the Deep Space Network, in an interview.\nFor New Horizons, the first images of its closest Pluto fly-by will be received by 70-meter antennas at the Madrid and Barstow sites, he said.\nData is coming down at a speed of approximately 1200 bits per second -- about as fast as a dial-up Internet modem in the early Nineties -- and it takes 4.5 hours for the signal to travel the distance from New Horizons to Earth.\nThis slow speed, coupled with the mission's science needs, which mean the spacecraft is not always pointing towards Earth, explains why the images from the closest point of the fly-by won't be received on Earth until more than a day after they were taken. It also explains why video from the edge of the solar system won't be possible any time soon.\nIn the most crucial phase of the Pluto fly-by on Monday evening, during a series of radio experiments, the Deep Space Network had six dishes pointed toward New Horizons. Some acted as back-ups in case of problems sending command signals to the craft, which travel away from Earth at the even slower rate of 500bps.\nThe Deep Space Network isn't confined to NASA craft. Because of its sensitivity, it also plays a supporting role to international missions, such as those launched by the European Space Agency and Japan's space agency.\nAt any one time, the DSN is talking to between 12 and 15 craft. A real-time view of this setup can be found on the Web at DSN Now. The page details which antenna is talking to which space probe, and even the data rate and frequency in use.\nMost spacecraft use a portion of the X-band at 8.4-8.5GHz, which is set aside globally for deep space communications. Because the signals coming back to Earth are so weak, agencies like NASA need a dedicated frequency band to avoid interference from terrestrial sources. Noise is also part of the reason space agencies are now eyeing even higher frequencies, around 32GHz, for future generations of craft.\nIf you keep a watch on the DSN Now page, you might see the DSN in communication with Voyager 1. That craft launched in 1977 and is now 25 billion miles from our planet -- the farthest a man-made object has ever traveled.\nThat's about a 36-hour radio round trip.\n\"We send a signal to Voyager one day, and we come in the next day for the answer,\" Osman said.\nMartyn Williams covers mobile telecoms, Silicon Valley and general technology breaking news for The IDG News Service. Follow Martyn on Twitter at @martyn_williams. Martyn's e-mail address is firstname.lastname@example.org\nJoin the CIO Australia group on LinkedIn. The group is open to CIOs, IT Directors, COOs, CTOs and senior IT managers."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:01c6865a-9da0-41ad-a80c-b6ff094a465e>","<urn:uuid:0373c788-60e7-4a03-97f5-b8c433116319>"],"error":null}
{"question":"What is the scientific reason behind fireflies' synchronized light show?","answer":"Scientists are still not sure why fireflies synchronize their flashes. However, they have discovered that the insects' blinks serve two purposes: attracting mates and warning predators. Laboratory experiments showed that bats took twice as long to learn fireflies tasted bad when their lights were covered with black paint.","context":["Summer brings the heat — and in some cases a lot of it, as those who suffered through record-breaking heat waves in Europe and South Asia in June can attest. But the season also ushers in long days filled with plenty of possibilities for outdoor fun. Parks fill with picnickers. Mountain trails fill with hikers. And beaches and pools swarm with swimmers trying to beat the heat.\nHere’s what science has to tell us about some of our favorite summer activities.\nFrolicking in forests\nThere’s hiking, and then there’s shinrin-yoku, the Japanese practice of “forest bathing.” Those who forest bathe walk slowly and breathe deeply — almost like a form of meditation. Developed in the 1980s, forest bathing has been gaining in popularity in the United States over the last few years. At least three books about the practice were published in the United States in 2018. Spending time among trees may improve health, from boosting the immune system to lowering blood pressure, the thinking goes. Some small studies have indicated possible benefits, an analysis published in 2017 in the International Journal of Environmental Research and Public Health, found. Research is ongoing.\nIn the forests of Tennessee’s Great Smoky Mountains, spectators gather each summer to watch fireflies put on a dazzling synchronized light show. Scientists still aren’t sure why those flashes sync up. One lingering mystery that has been solved, though, is why the insects’ behinds blink in the first place. Besides attracting mates, the light is also a warning to predators, scientists have found. In laboratory experiments, bats took about twice as long to learn that fireflies tasted bad when the bugs’ lights were covered by black paint. The findings confirmed a theory first proposed by entomologists in 1882.\nDo you know where your summer tan comes from? The sun’s ultraviolet rays are responsible for most of it, but UV light from nearby stars and other galaxies contributes a tiny bit — less than one-billionth of 1 percent of it, to be exact. Even light from the Big Bang contributes roughly 0.001 percent, astronomers have calculated.\nIf you’re working on perfecting that tan, sit poolside every other day — your skin will get darker and avoid some damage. That’s because skin produces melanin, a protective pigment, in 48-hour cycles, researchers say. Sunbathing day-after-day can disrupt this cycle and leave skin vulnerable to ultraviolet rays — and sunburn. And of course, don’t forget the sunscreen.\nWords of warning\nAnimals have all sorts of ways to beat the heat, from blowflies cooling their drool to toucans boosting blood flow to their big beaks. We humans often tend to flock to swimming pools and beaches to cool off. Before diving in, though, here are some rules of thumb to ensure a safer swim:\nDon’t pee in the pool. The chlorine in pool water reacts with nitrogen in urine to produce a toxic chemical called cyanogen chloride. This chemical, which acts like a tear gas, is classified as an agent of chemical warfare. Thankfully, it would probably take a lot of chlorine — way more than is allowed in swimming pools — and a lot of urine to conjure up harmful levels of cyanogen chloride. Still, mixing urine and chlorine produces other chemicals that in small amounts can irritate airways.\nDon’t drink the pool water. Besides often containing chlorine and pee, pool water can be rife with fecal matter and bacteria. Nearly 500 outbreaks of bacterial infections linked to pool water were reported in 46 states and Puerto Rico from 2000 to 2014, the last year for which data are available from the U.S. Centers for Disease Control and Prevention. Those tainted basins included hotel pools and hot tubs, water parks and other recreational facilities including public parks. The outbreaks resulted in more than 27,000 illnesses and eight deaths.\nDon’t ingest lake water, either. There’s a very small chance that killer amoebas could make their way up your nose and into your brain. Naegleria fowleri is commonly known as the brain-eating amoeba, but the nickname is something of a misnomer, scientists contend. The single-celled organism, which thrives in warm freshwater, doesn’t actually eat swimmers’ brains. Instead, N. fowleri triggers inflammation — the body’s immune response to invaders — leading to brain swelling and death. Cases are rare, though. Just 141 people died in the United States from 1962 to 2018, the CDC reports. The key to survival is a quick diagnosis, anti-amoeba drugs and reducing brain swelling, scientists say."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:adcb5c7c-781c-4d45-87d0-c922744c5cc4>"],"error":null}
{"question":"How does the resolution and imaging precision compare between the new ultrasound technique and MRI when it comes to detecting cancerous tissues?","answer":"The new ultrasound technology offers superior resolution, producing images with five to 10 times the resolution of traditional methods and can show blood flow with 0.05mm precision by tracking tiny bubbles in the bloodstream. MRI, while having greater natural contrast than standard ultrasound, works differently by mapping hydrogen distribution in the body and can distinguish fine variations in tissues deep within the body. Both technologies are effective at identifying tumors, with MRI being particularly good at spotting and distinguishing diseased tissues early in their development, while the new ultrasound technology specifically helps map the blood vessel networks that enable cancerous tumors to grow.","context":["A new ultrasound technology that could pick up far more cases of cancer and cut the need for biopsies has been developed by scientists.\nAt present, ultrasounds can help identify potential problems with key organs but are not sensitive enough to detect cancer.\nNow a team at Heriot-Watt University in Edinburgh say they have made a major breakthrough and can produce images with five to 10 times the resolution of traditional methods.\nThe new technique allows organs and blood flow to be scanned in super-resolution for the first time, with NHS trials on people starting at Edinburgh’s Western General Hospital in December.\nThe process works by injecting tiny bubbles into the bloodstream and scanning organs so that the blood flow can be shown with 0.05mm precision.\nThe patient need only stay still for a few minutes while they are scanned – allowing images to be produced in reasonable time.\nComputer technology is able to track these bubbles to produce images which have a greater resolution than anything in use at the moment.\nBy looking at blood vessels and flow, experts are able to map the networks that are enabling cancerous tumours to grow.\nDr Vassilis Sboros, from Heriot-Watt University, who led the research, said: “What we can see is all these bubbles one by one – we see dots in the image.\n“By joining the dots we end up with a picture that has much more detail and a lot more specific information.\n“At the moment we can detect a few cancers with ultrasound but our new technique increases the confidence with which we can be sure whether something looks cancerous.\n“We now need to do clinical trials on humans, but we may well be able to pick up cancers, such as pancreatic cancer and liver cancer, far earlier.”\nDr Sboros said the new technique will not require hospitals to upgrade their current equipment.\nHe said: “The limitations of current ultrasound images mean more expensive techniques like MRI are often employed for diagnosis and treatment.\n“MRI doesn’t provide clinicians with more detail but it has generally provided better results than other methods.\n“However, in the prostate for example, biopsy has to be performed as a separate procedure which is more expensive for the hospital and can be both disruptive and distressing for the patient.\n“Due to the super-resolution capability of our new images, we anticipate that the ability of the medical staff to pinpoint, diagnose and treat a range of cancers will be greatly enhanced.\n“We will work to establish the usefulness of our method in the upcoming clinical study.\n“We hope that further research will help expand this method to other applications in cardiovascular disease, diabetes, liver disease and transplant rejection and one day biopsies may not be necessary.”\nA study of the technique published in the Journal of Investigative Radiology showed that images of prostate cancer could be created.\nProfessor Alan McNeill, consultant urological surgeon at the Western General Hospital in Edinburgh said: “Prostate cancer is an increasing problem for our society.\n“Whilst we have a number of methods for detecting it, these don’t always provide us with the important information that we need regarding who has cancer that needs to be treated and who doesn’t.\n“A method that maps the blood flow of the tumour accurately could well provide new information about the disease state that allows us to better identify those men who need urgent treatment and those who don’t.\n“It is exciting that we will be the first hospital in the world that will assess this method with patients.”","Magnetic resonance imaging\nMagnetic resonance imaging (MRI) is one of the newest diagnostic medical imaging technologies that uses strong magnets and pulses of radio waves to manipulate the natural magnetic properties in the body to generate a visible image. In the field of mental health, an MRI scan may be used when a patient seeks medical help for symptoms that could possibly be caused by a brain tumor. These symptoms may include headaches, emotional abnormalities, or intellectual or memory problems. In these cases, an MRI scan may be performed to \"rule out\" a tumor, so that other tests can be performed in order to establish an accurate diagnosis .\nMRI was developed in the 1980s. Its technology has been developed for use in magnetic resonance angiography (MRA), magnetic resonance spectroscopy (MRS), and, more recently, magnetic resonance cholangiopancreatography (MRCP). MRA was developed to study blood flow, whereas MRS can identify the chemical composition of diseased tissue and produce color images of brain function. MRCP is evolving into a non-invasive potential alternative for the diagnostic procedure endoscopic retrograde cholangiopancreatography (ERCP).\nDETAIL. MRI creates precise images of the body based on the varying proportions of magnetic elements in different tissues. Very minor fluctuations in chemical composition can be determined. MRI images have greater natural contrast than standard x rays, computed tomography scan (CT scan), or ultrasound, all of which depend on the differing physical properties of tissues. This sensitivity allows MRI to distinguish fine variations in tissues deep within the body. It is also particularly useful for spotting and distinguishing diseased tissues (tumors and other lesions) early in their development. Often, doctors prescribe an MRI scan to investigate more fully earlier findings of other imaging techniques.\nSCOPE. The entire body can be scanned, from head to toe and from the skin to the deepest recesses of the brain. Moreover, MRI scans are not obstructed by bone, gas, or body waste, which can hinder other imaging techniques. (Although the scans can be degraded by motion such as breathing, heartbeat, and bowel activity.) The MRI process produces cross-sectional images of the body that are as sharp in the middle as on the edges, even of the brain through the skull. A close series of these twodimensional images can provide a three-dimensional view of the targeted area. Along with images from the cross-sectional plane, the MRI can also provide images sagitally (from one side of the body to the other, from left to right for example), allowing for a better three-dimensional interpretation, which is sometimes very important for planning a surgical approach.\nSAFETY. MRI does not depend on potentially harmful ionizing radiation, as do standard x ray and computed tomography scans. There are no known risks specific to the procedure, other than for people who might have metal objects in their bodies.\nDespite its many advantages, MRI is not routinely used because it is a somewhat complex and costly procedure. MRI requires large, expensive, and complicated equipment, a highly trained operator, and a doctor specializing in radiology. Generally, MRI is prescribed only when serious symptoms or negative results from other tests indicate a need. Many times another test is appropriate for the type of diagnosis needed.\nDoctors may prescribe an MRI scan of different areas of the body.\nBRAIN AND HEAD. MRI technology was developed because of the need for brain imaging. It is one of the few imaging tools that can see through bone (the skull) and deliver high-quality pictures of the brain's delicate soft tissue structures. MRI may be needed for patients with symptoms of a brain tumor, stroke , or infection (like meningitis). MRI may also be needed when cognitive or psychological symptoms suggest brain disease (like Alzheimer's or Huntington's diseases, or multiple sclerosis), or when developmental retardation suggests a birth defect. MRI can also provide pictures of the sinuses and other areas of the head beneath the face. In adult and pediatric patients, MRI may be better able to detect abnormalities than compared to computed tomography scanning.\nSPINE. Spinal problems can create a host of seemingly unrelated symptoms. MRI is particularly useful for identifying and evaluating degenerated or herniated spinal discs. It can also be used to determine the condition of nerve tissue within the spinal cord.\nJOINT. MRI scanning is most commonly used to diagnose and assess joint problems. MRI can provide clear images of the bone, cartilage, ligament, and tendon that comprise a joint. MRI can be used to diagnose joint injuries due to sports, advancing age, or arthritis. MRI can also be used to diagnose shoulder problems, such as a torn rotator cuff. MRI can also detect the presence of an otherwise hidden tumor or infection in a joint, and can be used to diagnose the nature of developmental joint abnormalities in children.\nSKELETON. The properties of MRI that allow it to see through the skull also allow it to view the inside of bones. Accordingly, it can be used to detect bone cancer, inspect the marrow for leukemia and other diseases, assess bone loss (osteoporosis), and examine complex fractures.\nHEART AND CIRCULATION. MRI technology can be used to evaluate the circulatory system. The heart and blood flow provides a good natural contrast medium that allows structures of the heart to be clearly distinguished.\nTHE REST OF THE BODY. Whereas computed tomography and ultrasound scans satisfy most chest, abdominal, and general body imaging needs, MRI may be needed in certain circumstances to provide better pictures or when repeated scanning is required. The progress of some therapies, like liver cancer therapy, needs to be monitored, and the effect of repeated x-ray exposure is a concern.\nMRI scans and metal\nMRI scanning should not be used when there is the potential for an interaction between the strong MRI magnet and metal objects that might be embedded in a patient's body. The force of magnetic attraction on certain types of metal objects (including surgical steel) could move them within the body and cause serious injury. Metal may be embedded in a person's body for several reasons.\nMEDICAL. People with implanted cardiac pacemakers, metal aneurysm clips, or who have broken bones repaired with metal pins, screws, rods, or plates must tell their radiologist prior to having an MRI scan. In some cases (like a metal rod in a reconstructed leg), the difficulty may be overcome.\nINJURY. Patients must tell their doctor if they have bullet fragments or other metal pieces in their body from old wounds. The suspected presence of metal, whether from an old or recent wound, should be confirmed before scanning.\nOCCUPATIONAL. People with significant work exposure to metal particles (working with a metal grinder, for example) should discuss this with their doctor and radiologist. The patient may need prescan testing—usually a single, regular x ray of the eyes to see if any metal is present.\nChemical agents designed to improve the picture or allow for the imaging of blood or other fluid flow during MRA may be injected. In rare cases, patients may be allergic to, or intolerant of, these agents, and these patients should not receive them. If these chemical agents are to be used, patients should discuss any concerns they have with their doctor and radiologist.\nThe potential side effects of magnetic and electric fields on human health remain a source of debate. In particular, the possible effects on an unborn baby are not well known. Any woman who is, or may be, pregnant, should carefully discuss this issue with her doctor and radiologist before undergoing a scan.\nAs with all medical imaging techniques, obesity greatly interferes with the quality of MRI.\nIn essence, MRI produces a map of hydrogen distribution in the body. Hydrogen is the simplest element known, the most abundant in biological tissue, and one that can be magnetized. It will align itself within a strong magnetic field, like the needle of a compass. The earth's magnetic field is not strong enough to keep a person's hydrogen atoms pointing in the same direction, but the superconducting magnet of an MRI machine can. This comprises the magnetic part of MRI.\nOnce a patient's hydrogen atoms have been aligned in the magnet, pulses of very specific radio wave frequencies are used to knock them back out of alignment. The hydrogen atoms alternately absorb and emit radio wave energy, vibrating back and forth between their resting (magnetized) state and their agitated (radio pulse) state. This comprises the resonance part of MRI.\nThe MRI equipment records the duration, strength, and source location of the signals emitted by the atoms as they relax and translates the data into an image on a television monitor. The state of hydrogen in diseased tissue differs from healthy tissue of the same type, making MRI particularly good at identifying tumors and other lesions.\nA single MRI exposure produces a two-dimensional image of a slice through the entire target area. A series of these image slices closely spaced (usually less than half an inch) makes a virtual three-dimensional view of the area.\nRegardless of the exact type of MRI planned, or area of the body targeted, the procedure involved is basically the same. In a special MRI suite, the patient lies down on a narrow table and is made as comfortable as possible. Transmitters are positioned on the body and the table moves into a long tube that houses the magnet. The tube is as long as an average adult lying down, and is open at both ends. Once the area to be examined has been properly positioned, a radio pulse is applied. Then a twodimensional image corresponding to one slice through the area is made. The table then moves a fraction of an inch and the next image is made. Each image exposure takes several seconds and the entire exam will last anywhere from 30 to 90 minutes. During this time, the patient must remain still as movement can distort the pictures produced.\nDepending on the area to be imaged, the radio-wave transmitters will be positioned in different locations.\n- • For the head and neck, a helmet-like covering is worn on the head.\n- • For the spine, chest, and abdomen, the patient will be lying on the transmitters.\n- • For the knee, shoulder, or other joint, the transmitters will be applied directly to the joint.\nAdditional probes will monitor vital signs (like pulse, respiration, etc.) throughout the test.\nThe procedure is somewhat noisy and can feel confining to many patients. As the patient moves through the tube, the patient hears a thumping sound. Sometimes, music is supplied via earphones to drown out the noise. Some patients may become anxious or feel claustrophobic while in the small, enclosed tube. Patients may be reassured to know that throughout the study, they can communicate with medical personnel through an intercom-like system.\nRecently, open MRIs have become available. Instead of a tube open only at the ends, an open MRI also has opening at the sides. Open MRIs are preferable for patients who have a fear of closed spaces and become anxious in traditional MRI machines. Open MRIs can also better accommodate obese patients, and allow parents to accompany their children during testing.\nIf the chest or abdomen is to be imaged, the patient will be asked to hold his to her breath as each exposure is made. Other instructions may be given to the patient as needed. In many cases, the entire examination will be performed by an MRI operator who is not a doctor. However, the supervising radiologist should be available to consult as necessary during the exam, and will view and interpret the results sometime later.\nMagnetic resonance spectroscopy (MRS) is different from MRI because MRS uses a continuous band of radio wave frequencies to excite hydrogen atoms in a variety of chemical compounds other than water. These compounds absorb and emit radio energy at characteristic frequencies, or spectra, which can be used to identify them. Generally, a color image is created by assigning a color to each distinctive spectral emission. This comprises the spectroscopy part of MRS. MRS is still experimental and is available only in a few research centers.\nDoctors primarily use MRS to study the brain and disorders like epilepsy, Alzheimer's disease , brain tumors, and the effects of drugs on brain growth and metabolism. The technique is also useful in evaluating metabolic disorders of the muscles and nervous system.\nMagnetic resonance angiography (MRA) is another variation on standard MRI. MRA, like other types of angiography, looks specifically at fluid flow within the blood (vascular) system, but does so without the injection of dyes or radioactive tracers. Standard MRI cannot make a good picture of flowing blood, but MRA uses specific radio pulse sequences to capture usable signals. The technique is generally used in combination with MRI to obtain images that show both vascular structure and flow within the brain and head in cases of stroke, or when a blood clot or aneurysm is suspected.\nMRI technology is also being applied in the evaluation of the pancreatic and biliary ducts in a new study called magnetic resonance cholangiopancreatography (MRCP). MRCP produces images similar to that of endoscopic retrograde cholangiopancreatography (ERCP), but in a non-invasive manner. Because MRCP is new and still very expensive, it is not readily available in most hospitals and imaging centers.\nIn some cases (such as for MRI brain scanning or MRA), a chemical designed to increase image contrast may be given immediately before the exam. If a patient suffers from anxiety or claustrophobia, drugs may be given to help the patient relax.\nThe patient must remove all metal objects (watches, jewelry, eye glasses, hair clips, etc.). Any magnetized objects (like credit and bank machine cards, audio tapes, etc.) should be kept far away from the MRI equipment because they can be erased. The patient cannot bring any personal items such as a wallet or keys into the MRI machine. The patient may be asked to wear clothing without metal snaps, buckles, or zippers, unless a medical gown is worn during the procedure. The patient may be asked not to use hair spray, hair gel, or cosmetics that could interfere with the scan.\nNo aftercare is necessary, unless the patient received medication or had a reaction to a contrast agent. Normally, patients can immediately return to their daily activities. If the exam reveals a serious condition that requires more testing or treatment, appropriate information and counseling will be needed.\nMRI poses no known health risks to the patient and produces no physical side effects. Again, the potential effects of MRI on an unborn baby are not well known. Any woman who is, or may be, pregnant, should carefully discuss this issue with her doctor and radiologist before undergoing a scan.\nA normal MRI, MRA, MRS, or MRCP result is one that shows the patient's physical condition to fall within normal ranges for the target area scanned.\nGenerally, MRI is prescribed only when serious symptoms or negative results from other tests indicate a need. There often exists strong evidence of a condition that the scan is designed to detect and assess. Thus, the results will often be abnormal, confirming the earlier diagnosis. At that point, further testing and appropriate medical treatment is needed. For example, if the MRI indicates the presence of a brain tumor, an MRS may be prescribed to determine the type of tumor so that aggressive treatment can begin immediately without the need for a surgical biospy.\nFaulkner, William H. Tech's Guide to MRI: Basic Physics, Instrumentation and Quality Control. Malden: Blackwell Science, 2001.\nFischbach, F. T. A Manual of Laboratory and Diagnostic Tests. 6th Edition. Philadelphia: Lippincott, 1999.\nGoldman, L., and Claude Bennett, eds. Cecil Textbook of Medicine. 21st Edition. Philadelphia: W. B. Saunders, 2000: pp 977–970.\nKevles, Bettyann Holtzmann. Naked to the Bone: Medical Imaging in the Twentieth Century. New Brunswick, NJ: Rutgers University Press, 1997.\nRoth, Carolyn K. Tech's Guide to MRI: Imaging Procedures, Patient Care and Safety. Malden: Blackwell Science,2001.\nZaret, Barry L., and others, eds. The Patient's Guide to Medical Tests. Boston: Houghton Mifflin Company,1997.\nCarr-Locke, D., and others, \"Technology Status Evaluation: Magnetic Resonance Cholangiopancreatography.\" Gastrointestinal Endoscopy (June 1999): 858–61.\nAmerican College of Radiology. 1891 Preston White Drive, Reston, VA 22091. (800) ACR-LINE. <http://www.acr.org> .\nAmerican Society of Radiologic Technologists. 15000 Central Avenue SE, Albuquerque, NM 87123–3917. (505) 298–4500. <http://www.asrt.org> .\nKurt Richard Sternlof Laith Farid Gulli, M.D."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:874940a6-cf3e-474a-841b-9a0e72b5f0aa>","<urn:uuid:c23f8c45-ce0d-4a42-bd0a-3f015563ddc7>"],"error":null}
{"question":"How does water affect the formation of both the karst system in Gnaraloo and the valleys in High Tatras? Comment l'eau affecte-t-elle ces deux régions?","answer":"Water shapes these regions in different ways. In Gnaraloo, acidic rainwater dissolves the limestone bedrock, creating a complex karst system with underground drainage networks, caves, and aquifers. This process occurs when mildly acidic rain collects carbon dioxide and forms carbonic acid, which eats away at the bedrock fractures. In the High Tatras, the valleys were formed by glaciers that 'mortised' the eastern faces, as evidenced in the crotch of Ostra where the west side remained untouched by glaciers while the east faces were shaped by them.","context":["Name: Sedielková kopa (sk), Siodelko (pl), Szedilkó (hun), Sedilko (ger)\nHigh Tatras in Slovakia. It´s picturesque mountain with superb views of western ridges of High Tatras and especially Krivan looks very impressively from this peak. It offers nice hikes and a bit of skialpinism. Sedielkova kopa is a grassy and patulous peak consisting of its main - south summit, grassy summit plateau of Sedielkova plan and the north monticule Kopa nad Sedielkom. It is connected to the rest of the crotch of Ostra by the saddle Sedielkovy priechod (1950 m). First another peak of the crotch is Ostra veza (2129 m).\nSedielkova kopa towers above two valleys - valley Vazecka dolina - its part valley Dolina suchej vody - is to the west and north and valley Furkotska dolina is to the east. It has two long shoulders - one from the main summit to the SSE and the other to the SW. Between the two shoulders there is small valley called Zlomisko, through what flows creek Zlomiskovy potok. In the elevation of apprimately 1450 meters this creek cross the red marked hiking trail to the tarn Jamske pleso, from where you may summit Kriván. From Jamske pleso you may also ascend Sedielkova kopa, but it is illegal due to presence of strict nature reserve. For more info see red tape section and this map. White areas within magenta dashed borders are places, where there is forbidden movement also for climbers. This route to Sedielkova kopa fall within this area.\nSedielkova kopa means in english \"Saddlous heap\" and in print it was first used by Karel Koristka, who was the professor of Prague polytechnics, in 1864 in Gothe in Germany. First it was described by wrong name \"Furkot\" (maybe according to its position up from Furkotska valley) by Albert Sydow in 1830. The first conquerer remained unknown. First to be on the summit may have been shepherds, hunters or miners. In winter was the first summiter Peter Havas Hornicek, who stood on the summit in january 1906. Except of dense fields of knee timber and of its SE ridge it is easily accesible by walk-up. But there leads no marked hiking trail to the summit. For more info see Red Tape section.\nMassif maps and picture description\nFor seeing maps presenting hiking trails to approach Sedielkova kopa see external maps linked from Getting there section.\nGetting There and routesthis map you see your transport routes from Poprad to Strbske pleso. For train or bus schedule see this link.\nDespite some 3 decades ago there led marked hiking trail to the summit of Sedielkova kopa, it was cancelled due to nature preservation. Now no marked hiking trail leads to the summit of Sedielkova kopa.\nOn Strbske pleso you may start your hike to Sedielkova kopa as well as to many other neighbouring summits. You´ve got two possibilities how to get closer to the Sedielkova kopa. Both you see clearly on this map. You may join the red marked hiking trail towards Jamske pleso (30 minutes), but at first junction follow yellow marked hiking trail to the valley Furkotska dolina (30 minutes) and from another junction follow the yellow route only for 15 more minutes, than you are to go west, cross the creek Furkotsky potok and ascend easily the saddle Sedielkovy priechod (1950 m), which takes another 30 minutes. From there you go easily through the Kopa pred Sedielkom and meadow on periglacially formed Sedielkova plan to the main summit of Sedielkova kopa. From the saddle it longs 15 minutes. The first, who followed this route is unknown. In summer - maybe first known - Alfred Martin ascended it on th 19th semptember 1907. In winter this route absolved first Gyula Komarnicki on the 22nd april 1912.\nThe other possibility how to approach Sedielkova kopa from Strbske pleso, is to follow blue marked hiking trail towards the chalet Chata pod Soliskom (1830 m). It will take 90 minutes. About the chalet see also the Camping section. From the chalet follow blue marks slightly down to the valley Furkotska dolina (15 minutes) and from the junction with the yellow mark you continue as was described above. The only parts of UIAA grade higher than 0 is SE rib of Sedielkova kopa. It has parts of grade I UIAA.\nSecurity and deaths\nIn High Tatras most fatal accidents (75%) fall within the subjective mistakes. When going to mountains be healthy, be careful and do not overestimate your abilities! From objective dangers there is the greatest lightning shot in summer and avalanches in winter. According to the statistics of the Tatras mountain rescue team to the date one man died on Sedielkova kopa. His name was JUDr. Jan Jamnicky (* 1908 Jasenova; + 4th august 1972 near the summit of Sedielkova kopa). R.I.P. He was national artist and actor. The cause of death was falling in senselessness.\nRed Tapenational park. Non climbers are not allowed to leave the marked hiking trails. Also climbers and skialpinists are restricted - it is officially forbidden to go up in terrain less than III grade UIAA. Skialpinists have some places, where they are allowed to do their activities, but Sedielkova kopa doesn´t belong to them. Anyway, Sedielkova kopa is often summited in all seasons. Skialpinists also downhill its slopes.\nFor things not to be so simple, as is Sedielkova kopa over two valleys, so differs the red tape. The west and north valley Vazecka dolina - and its part Dolina Suchej vody - is due to protecting chamoix strictly protected nature reserve. This means nobody except from national park rangers and scientists should ascend or descend Sedielkova kopa to that side. The side of valley Furkotska dolina is less illegal. If you see this map, it will approve, what was just mentioned. White areas within magenta dashed line are forbidden also for climbers. As you see at that map, the SE shoulder and N ridge of Sedielkova kopa is the border of the strict nature reserve. Ascend from Furkotska valley is thus less, but still in most cases illegal.\nCurrently it is discussed the reason of so strict restrictions and the regulations may be set not so strict, but the actual state of affairs is very strict, as I described it above. This regulation means that anybody you may be, you are restricted to summit Sedielkova kopa. For example Sedielkova kopa was suggested to be legal skialpinism area. It was the governmental council for saviour and re-examinating of Tatras (after the hurricane - \"kalamita\") who came with that claim. Recently nobody seems to be trying to fight for renewing the marked hiking trail to the summit of Sedielkova kopa.\nThe prohibition is the reason I´ve yet never summited it. I respect the law and I have in respect the rule of law. If laws are bad, we might try to change the state of affairs.\nDue to the presence of national park, camping is strictly restricted. Only allowed are climbers, but only in urge situations. The nearest place to spend the night is the chalet chata pod Soliskom, where you may spend the night for 300 slovak crowns - approximately 10 euro. To the chalet leads also ski-lift from Strbske pleso that operates also in summer. You may spend your night also in hotels in the village Strbske pleso or in neighbouring villages. The nearest campsite is FICC Eurocamp in Tatranska Lomnica, some 20 kilometers away. The other campsite is autocamp Rackova dolina, that is as far from Sedielkova kopa, as is Eurocamp Tatranska Lomnica.\nIn the first half of the 20th century there was one chalet directly in the valley Furkotska dolina, at its beginning. It was finished in 1936, was named Furkotska chalet and the owner was Lonek. This is, why it was called also Lonek´s chalet. In 1948 it was nationalised. In 1951 it was renamed after captiain Raso, former chief of local partisans during Slovak national uprising against fascism. In 1956 in was completely burn down. It was never renewed.\nActual conditions and security:\n-Actual webcam shots from Strbske pleso is here.\n-Tatra weather forecast: http://www.hory.sk/\n-Tatra mountain rescue urgent phone line: 18 300\n-In case mountain rescue rescues you, you will have to pay for it unless you have special mountain insurance. It costs 20 Slovak crowns (70 eurocents) a day.\n-Current cautions about moutain conditions you find here.\n-Actual avalanche situation is presented here.\nExternal photos and trip reports\nSummit photos and panoramas\n-A summit photo with copyright of Stanislav Klaučo from www.hory.sk:\n-view of Krivan from Sedielkova kopa\n-summit view over Kratka and Kozi hrb - here you see how differs the crotch of Ostra - the west side remained untouched by glaciers, but east faces were mortised by \"hungry\" glaciers.\nViews over the summit\nWinter views of photographer Havran:\n-1. a winter view of the summit from upper part of valley Furkotska dolina\n-2. a winter photo of Sedielkova kopa from the upper part of Furkotska dolina with Ostra veza and Kozia stena on the right.\n-3. an alpinist ascending Sedielkova kopa in winter.\nOutcomes of scientific work on Sedielkova kopa\n-Impact of climatic changes on flora in Tatras.","Gnaraloo features a magnificent geological and dynamic landscape, including its very own Gnargoo range.\nThe South Girilia Plateau, which stretches from Exmouth south to Gnaraloo, is considered an area of Environmental Management Priority, as it is a region of high conservation value (Department of the Environment, Water, Heritage and the Arts, Commonwealth, 2008).\nThe landscape on the Gnaraloo area is beautiful in its stark contrasts. It is characterised by the striking blue and greens of the Indian Ocean and the Ningaloo Reef, bordered by an area of relic linear dune plateau, with good biodiversity levels. Heading inland are vast red aeolian sand fields sparsely covered with spinifex (Spinifex squarrosus) grassland, before encountering the Gnargoo range. Underlaying all of this is an extensive karst system characterised by limestone bedrock. A unique feature of this area of coastline is the presence of tertiary karst systems as well as incised river channels that are directly linked to the coast. The climate along the coast varies between arid and semi-desert to subtropical conditions, with frequent annual cyclonic activity.\nThe Ningaloo coastline has principle characteristics of a tertiary karst environment, which is unparalleled in Australia, making the area of great importance to natural heritage (Department of the Environment, Water, Heritage and the Arts, Commonwealth, 2008). Tertiary karst environments form when the tides of the ocean interact with the hydrological network in the karst system, mixing the ground water and seawater. This increased salinity further dissolves the limestone bedrock, and combined with the flow created by the tidal pumping, creates interconnecting tubes between pre-existing cavities within the system. The presence of a tertiary karst system in Australia is very rare and there are no comparable systems within Australia (Department of the Environment, Water, Heritage and the Arts, Commonwealth, 2008).\nIn the Gnaraloo area, the predominant limestone bedrock results in the presence of aquifers, groundwater resources and incised river channels, which form a direct link between the Indian Ocean and the significant inland Lake MacLeod wetland system via subterranean channels (Department of the Environment, Water, Heritage and the Arts, Commonwealth, 2008).\nKarst is a three-dimensional geological feature which is formed over time by the action of water dissolving soluble layers of bedrock. The bedrock usually consists of carbonate rocks such as dolomite, limestone, marble or less commonly gypsum (Waele et al. 2008). These formations occur when rain gains acidity in the atmosphere through the collection of carbon dioxide. Once it reaches the ground surface, the mildly acidic rain begins to drain through the surface and the acidity can be amplified by collecting more carbon dioxide, creating carbonic acid (H2C03). As the acidic water drains through the bedrock, it begins to eat away at the fractures, causing them to enlarge. These fractures then begin to form an underground drainage system, which in turn increases the flow of water through the bedrock and begins to create the unique underground karst system. The results of karstification can vary greatly, producing small and large features both underground and on the surface.\nAbove ground, the features of a karst system can vary anywhere from small flutes, karren, runnels, grikes and clints, all the way through to sinkholes (dolines), vertical shafts, disappearing streams and reappearing springs (Waele et al. 2008). Below the surface extends a large underground drainage system, including features such as aquifers as well as an expanse of caves and cavern systems.\nIn areas where a karst system is present, there is a lack of surface water as it quickly drains through the crevices. In addition to this, the high permeability of karst systems means that underground water supplies are likely to be contaminated, as water travelling through the bedrock travels at such a fast rate that it does not undergo the same filtering processes as ground water would in other areas. These features make living on karst systems particularly difficult. Often the ground water, which in other areas would be heavily relied upon, cannot be used and there is a risk of sinkholes forming and underground caverns collapsing. This means that human activities in karst areas need to be monitored, especially waste disposal, freshwater supply, construction and mining (Waele et al. 2008).\nThe Giralia Range district includes the anticline structures of the Giralia Range, Rough Range, Gnargoo Range and numerous small folds adjacent to Lake MacLeod (Western Australian Planning Commission, 2004). The geological history of the district follows similar patterns as the Cape Range district in Exmouth with marine sedimentation, tectonic stress and the uplift and exposure of tertiary sediment.\nVery little is currently known about the Gnargoo Range, which is located in the southern Carnarvon basin, extending approximately 500m below Cretaceous strata. Gnargoo exists on the Gascoyne Platform, which extends from Kalbarri to Coral Bay, while the range itself stretches from north of the Gnaraloo Homestead towards Warroora Station (Western Australian Planning Commission, 2004).\nAllen. A. (1993). Outline of the geology and hydrogeology of Cape Range, Carnarvon Basin, Western Australia. Hydrology of Cape Range, 25-38.\nDepartment of the Environment, Water, Heritage and the Arts. (2008). Australian Heritage Database, Places for Decision, Class: Natural, Ningaloo Coast National Heritage Place.\nGlikson, A. & Uysal, I. 2013, “Geophysical and structural criteria for the identification of buried impact structures, with reference to Australia”, Earth-Science Reviews, 125, 114-122\nIasky, R. & Glikson, A. (2005). Gnargoo: A possible 75 km-diameter post-early Permian – pre- Cretaceous buried impact structure, Carnarvon Basin, Western Australia. Australian Journal of Earth Sciences, 52, 575-86.\nWaele, J. Plan, L. Audra, P. (2009). Recent developments in surface and subsurface karst geomorphology: An introduction. Geomorphology, 106, 1–8.\nWestern Australian Planning Commission. (2004). Ningaloo Coast Regional Strategy Carnarvon to Exmouth. August.\nThe information was compiled by Karen Hattingh, Simone Bosshard, Tessa Concannon, Tess DeSerisy, Heather Shipp and Megan Soulsby, 2017/18\nWithin the karst ecosystems, stygofaunal and troglofaunal species yield information about evolution of life on earth.\nGnaraloo is home to a diverse and natural ecosystem. Its landscape is classified as having high environmental sensitivity."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:11c86ae3-3b51-4cd6-ab5e-3e0b0a856f66>","<urn:uuid:98bc946e-6547-42da-ba7d-067359cce009>"],"error":null}
{"question":"How did critics respond to The Greatest Game Ever Played compared to Jurassic Park?","answer":"Both films received positive critical reception. The Greatest Game Ever Played received generally positive reviews with 63% positive ratings on Rotten Tomatoes and praise from critics like Roger Ebert who gave it three out of four stars. Jurassic Park was critically acclaimed and went on to win three Academy Awards for Best Visual Effects, Best Sound Mixing, and Best Sound Editing.","context":["The Greatest Game Ever Played\nThe Greatest Game Ever Played is a 2005 biographical sports film based on the early life of golf champion Francis Ouimet. The film was directed by Bill Paxton, and was his last film as a director. Shia LaBeouf plays the role of Ouimet. The film's screenplay was adapted by Mark Frost from his book, The Greatest Game Ever Played: Harry Vardon, Francis Ouimet, and the Birth of Modern Golf. It was shot in Montreal, Quebec, Canada, with the Kanawaki Golf Club, in Kahnawake, Quebec, the site of the golf sequences.\n|The Greatest Game Ever Played|\n|Directed by||Bill Paxton|\n|Produced by||David Blocker|\n|Screenplay by||Mark Frost|\n|Based on||The Greatest Game Ever Played: A True Story|\nby Mark Frost\n|Music by||Brian Tyler|\n|Edited by||Elliot Graham|\n|Distributed by||Buena Vista Pictures|\n|September 30, 2005|\n|Box office||$15.4 million|\nSet mainly in 1913, the film is about Francis Ouimet, the first amateur to win the U.S. Open. Amateur golf in that era was then a sport only for the wealthy, and Ouimet came from an immigrant family that was part of the working class. Ouimet watches an exhibition by legendary British golf pro Harry Vardon (Stephen Dillane) as a 7-year-old boy, and becomes very interested in golf. He begins as a caddie at The Country Club, a posh enclave located across the street from his home in suburban Brookline, Massachusetts, while making friends with the other caddies. He works on his own golf game at every chance, and gradually accumulates his own set of clubs. Francis practices putting at night in his room. He wins the Massachusetts Schoolboy Championship.\nOne day, a Club member, Mr. Hastings, asks Ouimet to play with him over The Country Club course, where caddies have almost no access of their own, and he shoots a fine round of 81 despite a 9 on one hole. His talent, composure, and good manners earn admirers and interest. With the help of Mr. Hastings and the Club Caddiemaster, Francis gets a chance to play in an upcoming tournament, the U.S. Amateur, the local qualifying for which is to be held at the very same Country Club course. However, his father Arthur (Elias Koteas) tells his son to quit golf and get a \"real job\". Ouimet needs $50 for the entry fee, and so agrees to get a real job and never play golf again if he could not qualify; his father lends him the money. On the 18th, Francis faces a three-foot putt that would secure him a spot in the championship, but he looks over and his father is watching. Ouimet is distracted, misses and falls one stroke short of qualifying for the championship proper.\nWith much jeer from the rich folk, Ouimet, now 20, fulfills his promise to his dad and works at a sporting goods shop, while continuing to live at home. After some time with his golf forgotten, Ouimet is still at the bottom of the working class. But one day, the president of the United States Golf Association enters the store and personally invites him to play in the upcoming U.S. Open. After some maneuvering and consideration from his employer, Ouimet secures entry. His father informs Ouimet that he must find his own place to live after the tournament and Ouimet agrees to this arrangement. However, his mother has been supportive of his golf from the start. She admonishes Ouimet's father for not recognizing Ouimet's talent and that he now has a chance to demonstrate it in an important tournament.\nOuimet competes in the 1913 U.S. Open that takes place at The Country Club in Brookline, Massachusetts, the familiar course located across the street from his home. The favorites are British champions Vardon and Ted Ray, who are accompanied by the snobbish Lord Northcliffe (Peter Firth), and the reigning U.S. Open champion, John McDermott. Northcliffe looks to see that either Vardon or Ray wins the Open, to affirm British dominance over the Americans in golf, and also to prove that only gentlemen were able champions. Ouimet competes with his 10-year old friend, Eddie Lowery (Josh Flitter), who skips school to caddie for Ouimet. After the first two rounds, Vardon and Ray have a seemingly comfortable lead, with McDermott unable to keep up. After some initial struggles, Ouimet rallies back and ends up tying with Vardon and Ray at the end of the fourth round, meaning that the three of them would compete in an 18-hole playoff to determine the champion. The night before, Northcliffe mocks Ouimet's social status to Vardon, who came from humble beginnings himself, and Vardon finally tells Northcliffe that he is going to try to win only for his own pride, not Britain's and that if Ouimet wins, it will be because of his own skill, not his background.\nThe playoff round commences, with all three competitors keeping it close until the final holes, where Ray fades out, and Ouimet ahead of Vardon by a stroke going into the final hole. Vardon finishes with a par, giving Ouimet the chance to clinch the win with a par himself. Seeing him become nervous before the final putt, Eddie calms him down, and Ouimet is able to make the putt and win the U.S. Open. As the crowd carries him and Eddie on their shoulders, they start to hand him money. Ouimet refuses it all, only accepting one bill from his now proud father. In the clubhouse, Vardon privately congratulates Ouimet and suggests that they should play a friendly round together in the future. Ouimet and Eddie then walk home, carrying the U.S. Open trophy.\nThe movie shows a dramatic finish in the playoff, with Ouimet sinking a putt on the 18th hole to win the Championship by a single stroke. In reality, Ouimet finished birdie-par on 17 and 18 to Vardon's bogey-double bogey to end the playoff five strokes clear of Vardon and six ahead of Ray. The movie also shows the playoff as being in fair weather, and moves the rain to the third round. In the movie the historical 17th hole plays as a \"dog leg right\" when in fact at Brookline Country Club is played as a \"dog leg left\".\n- Shia LaBeouf as Francis Ouimet\n- Stephen Dillane as Harry Vardon\n- Peter Firth as Lord Northcliffe\n- Elias Koteas as Arthur Ouimet\n- Luke Askew as Alec Campbell\n- Josh Flitter as Eddie Lowery\n- Peyton List as Sarah Wallis\n- Marnie McPhail as Mary Ouimet\n- Len Cariou as Stedman Comstock\n- Michael Sinelnikoff as Lord Bullock\n- Stephen Marcus as Ted Ray\n- Max Kasch as Freddie Wallis\n- Mike 'Nug' Nahrgang as Baritone\n- Walter Massey as William Howard Taft\n- Justin Ashforth as Ted Hastings\nThe film opened at #9 at the U.S. box office in its opening weekend grossing US$3,657,322.\nThe film received generally positive reviews, from golf fans and non fans of the sport alike. Roger Ebert gave it three of four stars, stating it gave the real history of the greatest golf match with a strong human element while showing the golf play in a \"gripping story\". He notes that he is \"not a golf fan but found (it) absorbing all the same... Paxton and his technicians have used every trick in the book to dramatize the flight and destination of the golf balls. We follow balls through the air, we watch them creep toward the green or stray into the rough, we get not only an eagle's-eye view but a club's-eye view and sometimes, I am convinced, a ball's-eye view.\" Larry King proclaimed it \"every bit as good as Seabiscuit.\"\nOn the review aggregation website Rotten Tomatoes, 63% of critics gave the film a positive \"fresh\" review. The website concludes, \"Despite all the underdog sports movie conventions, the likable cast and lush production values make The Greatest Game Ever Played a solid and uplifting tale.\"\nAwards and accoladesEdit\n|2006||Nominated||ESPY Awards||Best Sports Movie|\n|2006||Nominated||Satellite Awards||Best Youth DVD|\n|2006||Nominated||Young Artist Awards||Best Performance in a Feature Film - Supporting Young Actor||Josh Flitter|\nThe film has been released on DVD and UMD by Walt Disney Home Entertainment. Special features include two \"making of\" documentaries with cast and crew members, plus a rare 1963 interview with the real Francis Ouimet on WGBH, the Boston public television station, at Brookline, Massachusetts golf course where the 1913 U.S. Open took place. It was released on Blu-ray Disc in 2009, and again as a DVD/Blu-ray combo pack in 2011.\n- The Greatest Game Ever Played - Box Office Data, Movie News, Cast Information. The Numbers. Retrieved July 3, 2009.\n- Tridiatech. \"The Kanawaki Golf Club - Home Page\". www.kanawakigolf.com.\n- Ebert, Roger. \"The Greatest Game Ever Played Movie Review (2005) - Roger Ebert\". www.rogerebert.com.\n- \"The Greatest Game Ever Played\".","Universal Studios Inc. is an American motion picture studio. Universal is a subsidiary of NBCUniversal which, in turn, is owned by Comcast. Universal is known as one of the six major American film studios. Its production studios are at 100 Universal City Plaza Drive in Universal City, California. Distribution and other corporate offices are in New York City.\nFounded in 1912 by Carl Laemmle, Mark Dintenfass, Charles Baumann, Adam Kessel, Pat Powers, William Swanson, David Horsley, and Jules Brulatour, it is the oldest movie studio in the United States of America. It is also the fourth oldest in the world that is still in continuous production. On May 11, 2004, the controlling stake in the company was sold by Vivendi Universal to General Electric, parent of NBC. The resulting media super-conglomerate was renamed NBC Universal, while Universal Studios Inc. remained the name of the production subsidiary. In addition to owning a sizable film library spanning the earliest decades of cinema to more contemporary works, it also owns a sizable collection of TV shows through its subsidiary NBCUniversal Television Distribution. It also acquired rights to several prominent filmmakers’ works originally released by other studios through its subsidiaries over the years.\nFour of Universal Studios’ films–three of which being directed by Steven Spielberg—Jaws (1975), E.T. The Extra-Terrestrial (1982), Jurassic Park (1993), and Despicable Me 2 (2013)—achieved box office records. Jaws, E.T. and Jurassic Park each held the title of highest-grossing film in history with Jurassic Park claiming the title from E.T.\n- All Quiet on the Western Front\n- The Mummy\n- To Kill a Mockingbird\n- American Graffiti\n- The Sting\n- E.T. The Extra Terrestrial\n- The Deer Hunter\n- Jurassic Park\n- Schindler’s List\n- Apollo 13\n- The Lost World: Jurassic Park\n- Jurassic Park ///\nEven before publication, Spielberg learned of the novel in October 1989 while he and Crichton were discussing a screenplay that would become the television series ER. Spielberg in turn requested that Universal buy the rights to the novel adaptation. Before the book was published, Crichton demanded a non-negotiable fee of $1.5 million as well as a substantial percentage of the gross. Several directors and studios bid for the rights, but Universal eventually acquired them in May 1990 for Spielberg. Universal paid Crichton a further $500,000 to adapt his own novel, which he had finished by the time Spielberg was filming Hook. After completing Hook, Spielberg wanted to film Schindler’s List. Music Corporation of America (then Universal Pictures’ parent company) president Sid Sheinberg gave a green light to the film on one condition: that Spielberg make Jurassic Park first. Spielberg later said, “He knew that once I had directed Schindler I wouldn’t be able to do Jurassic Park.”\nFilm production lasted from August 24-November 30, 1992, wrapping up approximately two weeks ahead of schedule and under the 63 million dollar budget. John Williams completed and conducted the score from February to March of 1993. George Lucas supervised the post-production implementation of digital effects for the film which were completed by April. Jurassic Park was completed and ready for its June 11th release on May 28, 1993.\nPromotion for Jurassic Park totaled at 65 million dollars. Jurassic Park ran in worldwide theaters twice, totaling over one billion in revenue–the 17th such film to attain that milestone–$1,029,153,882. Jurassic Park was the highest-grossing film in history from 1993-1997, making 914 million dollars, but was topped by Titanic in 1997. Jurassic Park is now the 13th high-grossing film in cinema history. The critically acclaimed film by Universal was awarded three Oscars or Academy Awards, for Best Visual Effects, Best Sound Mixing and Best Sound Editing. Its success prompted Universal to commission two sequel films.\nThe Lost World: Jurassic Park\nDevelopment for The Lost World began immediately following the release of Jurassic Park in 1993. Michael Crichton was pressured by fans for a sequel novel. Having never written a sequel, he initially refused, until it prompted Steven Spielberg himself to request one. After the book was published in 1995, pre-production began the same year with a budget of 73 million. Filming on the sequel commenced in September 1996 and lasted until December 20th.\nILM created and inserted visual effects for the picture in the winter and spring months of 1997. John Williams completed his score within that time frame as well.\nThe movie premiered on May 23, 1997.It took $92.6 million for the four-day Memorial Day holiday in the U.S., which was the biggest opening weekend at the time. Likewise, it took the record for highest single-day box office take of $26,083,950 on May 25–achieved during that four-day weekend run–a record held until May 1999. It also became the fastest film to pass the $100 million mark, achieving the feat in just six days, again stemming from the massive opening. Total American gross of $229,086,679 and $389,552,320 internationally, the film ended up grossing $618,638,999 worldwide, second only to Titanic that year. The movie was nominated for one Academy Award for visual effects.\nJurassic Park III\nJurassic Park III began initial prep in August 1999, after being greenlit by Universal. The film spent a year in pre-production, finishing that time period with a teaser attached to the Pokemon: 2000 movie, a plan to feature Spinosaurus, Pterosaurs and new Velociraptors and an unfinished script. The production was plagued with an unclear and oft-changed and ‘made up on the go’ script. The result was a production that suffered from a flat story and misuse of sets originally planned to be bigger set pieces. Filming started on August 30, 2000 and concluded in the winter months. Post-production lasted until the summer of 2001. Jurassic Park III had the largest budget of the trilogy at 93 million.\nJurassic Park III had the smallest gross of the three, it earned $181,171,875 in the United States and $368,780,809 worldwide. It was negatively received and was nominated for no major awards at the Academy Awards."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e3c8144e-6e02-47b5-be20-95dde1c6863c>","<urn:uuid:599aac46-6698-4c03-9cbc-8f3d0a650cdb>"],"error":null}
{"question":"Can SAG mills and ball mills achieve the same particle size reduction? I need specific size examples!","answer":"While both types of mills are used for size reduction, they typically operate at different size ranges. SAG mills handle feed material of 200-350mm and reduce it to a few millimeters in the first grinding stage. Ball mills, on the other hand, are used for finer grinding, capable of producing much smaller particles down to micron sizes - for example, particles can be reduced to sub-micron levels or even nanometer size range with proper configuration.","context":["Mining Sidenor,In mining industry, one of the main processes that take place after the ore extraction and first crushing, is the grinding. During primary or secondary grinding, the ore size can be further reduced, so that minerals are distinguished from ore. The most usual grinding mills are the SAG Semi Autogenous Grinding mills and the Ball mills.\nAutogenous Grinding - Mining Fundamentals,Autogenous Grinding - Mining Fundamentals. Download PDF Copy; Request Quote; Written by AZoMining Apr 24 20 4. Autogenous grinding is a process of grinding ore in a rotating cylinder, using large pieces of the same ore. Autogenous mills operate mechanically like ball mills; however, the media used for grinding differs. Simply put, in autogenous\nAutogenous mill, Ag Mill, Semi-autogenous Mill, Ball Mill , . Simplify the crushing and grinding process; feeding particle size is 200-350mm, and after the first autogenous grinding, the product particle size reaches below a few millimeters. 2. Autogenous mill is suitable for ore with a high content of mud and water; small area taken, low investment.\nAutogenous Mill Appli ion Mining Industry - SPM Instrument , An autogenous mill is a mission critical appli ion found in the concentrator section of a mine. The mill& 39;s primary task is to grind ore into a suitable size for the next .\nAutogenousGrinding Equipment For Ore Processing,autogenous mill for chrome ore high frequency. autogenous grinding equipment for ore processing. Autogenous and Semiautogenous mills The AG/SAG mills are utilized to grind run of mine rock or primary crusher of processing, or an immediate size for further grinding in a ball mill, pebble mill, should be determined during testing of the ore, common flow sheets include: Mill motor gear drives can\nAutogenousGrinding & Semi Autogenous Grinding Circuits,Table of Contents Autogenous Grinding CircuitsSAG vs Ball Mill AdvantagesAG & SAG Mill Grinding Compared – Which is BestTesting for Product SizePrimary Autogenous Grinding Concentrators Recent Trends and DevelopmentsAutogenous Mill TypesPrimary Autogenous MillsWet Versus Dry Primary GrindingBasic Plant FlowsheetBasic Primary Grinding Flowsheets Size reduction is the most expensive operation\nSemi-autogenous grinding (SAG) mill liner design and . , Aug 2007 . Mokken, A.H., 978, “Progress in run-of-mine: (autogenous) milling as introduced and subsequently developed in the gold mines of the Union .\nEco-efficient and cost-effective process design for ,Mining – and especially minerals processing – routes for different ores base metals, iron ore, bauxite, platinum, etc. vary significantly, and the energy requirements and the opportunities for reduced energy consumption are also different. The major benefit of fully autogenous grinding is the elimination of steel grinding media costs\nDevelopment of a two-stage dynamic autogenous grinding mill , The article discusses some of the nearest future trends of the development of crushing and grinding equipment for nonmetal mineral mining. These trends are .\nAMIT 35: Lesson 6 Grinding Circuit – Mining Mill Operator ,Grinding takes place in more “open” space which makes the retention time longer and adjustable compared to crushers. Theoretical size reduction and power ranges for different grinding mills image: 35-6- AG/SAG Mills Autogenous Grinding AG Mill. Wet or dry; Primary, coarse grinding up to 400 mm feed size Grinding media is grinding feed\nAG Autogenous Grinding - 9 Metallurgist , 29 Mar 20 9 . In a autogenous mill the ore is fed directly into the mill from either the primary crusher or the mine itself. The size of the rock will be between four .\nMechanisms in the autogenous mill and their mathematical . , pilot run-of-mine autogenous milling installation of Cobar Mines pty Ltd,. Cobar, New South Wales, consisting of a grate-discharge. Hardinge. & 39;Cas- cade& 39; mill ( .\nDifference Between Sag Mill vs Ball Mill - mech4study,SAG is the abbreviated form for Semi-Autogenous Grinding Mill. This type of Mill is used for grinding large fragments into small pieces. Pieces are then used for further processing. The SAG mills are generally used in pre-processing of any type of material in grinding process. SAG mills are also known as first stage grinders.\nEFFECT OF AUTOGENOUS AND BALL MILL GRINDING ON SULFIDE ,T - EFFECT OF AUTOGENOUS AND BALL MILL GRINDING ON SULFIDE FLOTATION. AU - Iwasaki, I. AU - Reid, K. J. AU - Lex, H. A. AU - Smith, K. A. PY - 983/ / . Y - 983/ / . N2 - The effects of grinding on the floatabilities of copper-nickel sulfides from Duluth gabbro were investigated.\nAutogenous Grinding - Mining Fundamentals - AZoMining.com , 24 Apr 20 4 . Autogenous grinding is a process of grinding ore in a rotating cylinder using large pieces of the same ore. Autogenous mills operate .\nAutogenous Archives - Canadian Mining JournalCanadian ,Canadian Mining Journal provides information on new Canadian mining and exploration trends, technologies, mining operations, corporate developments and industry events. Related Publi ions. Glacier Media The Northern Miner June Warren Energy Daily Oil Bulletin MINING.COM\nGlossary of Mining Terms - SEC,Autogenousgrinding - The process of grinding ore in a rotating cylinder using large pieces of the ore instead of conventional steel balls or rods. B. Back - The ceiling or roof of an underground opening. Backfill - Waste material used to fill the void created by mining an orebody.\nAutogenousGrinding - 9 metallurgist.com,Autogenousgrinding is being carried out, throughout the world, either in mills grinding in water or solution or in dry mills using air classifi ion to remove the finished material. The wet grinding, particularly, can be divided into: Primary autogenous grinding, or “run-of-mine” grinding.\nMacPherson Autogenous Grindability Test | Mining | SGS , Mine. SGS has unrivaled expertise in conducting bench-scale tests, such as the MacPherson Autogenous Grindability Test to design power-efficient grinding .\nAG Autogenous Grinding - Mineral Processing & Metallurgy,Metallurgical ContentIntermediate Autogenous GrindingAutogenous Grinding ExplainedTest ConditionsGrinding Characteristics of Short Mill Versus Long MillObservations and Recommendations The third type of mill that I mentioned was an AUTOGENOUS MILL, this type of mill uses a completely different type of grinding media; the rock itself In conventional milling, the ore is crushed to, plus or minus\nautogenous mills mediaautogenous mills mining,Autogenous Mills Mediaautogenous Mills Mining. Autogenous mills mediaautogenous mills mining.Difference between sag mill vs ball mill mech4study.Sag is the abbreviated form for semi autogenous grinding mill.This type of mill is used for grinding large fragments into small pieces.Pieces are then used for further processing.The sag mills are generally used in pre processing of.\nWet grinding plants > Grinding plants > Mineral Processing ,The Industrial Solutions business area of thyssenkrupp is a leading partner for the engineering, construction and service of industrial plants and systems. Based on more than 200 years of experience we supply tailored, turnkey plants and components for customers in the chemical, fertilizer, cement, mining and steel industries.\nAdvanced Technology of Semi Autogenous Grinding Mill ,In order to improve the quality of autogenous mills products and reduce grinding cost, semi autogenous grinding mill manufacturer invests great manpower, material resources, and financial resources. This paper will take mining group as an example to introduce the advanced technology of semi autogenous grinding mill manufacturer.\n「mining mini autogenous mill for sale」,autogenous mill effective mining equipment mill. Small Mining Equipment ball Mill For Limestone,Barite,Silica Sand Hot Sale In For Sale,Used Grinding Machine Ball Mill For Copper Ore Cement from Mine Mill and/or rod milling; Primary crushing and autogenous/semiautogenous grinding Classifiers, Flotation Machines, Grinding Mills, Quick Lime Systems, Rotary.\nAutogenousGrinding Example,Mining control in the open pit must be exercised so as not to deliver too much silicified lime and hornfels in a given, time. By Jorge 20 8- 2- T 7:50:47-05:00 November 29th, 20 8 egories: Grinding Tags: 976 Comments Off on Autogenous Grinding Example\nWhen it Comes to Mining, Bigger is Best PTE,An ABC circuit consists of an autogenous grinding AG mill, ball mill and crusher. An SABC circuit consists of a semi-autogenous grinding SAG mill, ball mill, and crusher. A ball mill is a slightly inclined, horizontal rotating cylinder, partially filled with ceramic balls, flint pebbles or stainless steel balls, that grinds material to the\nAutogenous Grinding | Article about Autogenous Grinding by The . , “Autogenous Grinding Picks up Speed.” Mining Engineering, 970, vol. 22, no. 9. V. A. PEROV. The Great Soviet Encyclopedia, 3rd Edition ( 970- 979) .\nAutogenous Archives - Canadian Mining JournalCanadian ,Business Description: The brand is rooted in more than 50 years of mining technology and services. We help customers improve operations, increase profitability, and reduce risks with a range of crushing and grinding solutions, life cycle services, predictive maintenance tools, and parts support from a network of over 80 service centers.\nBall Mill, Grinding Mill, Ball Mill Manufacturers, Ball ,Autogenous Mill Introduction : A wet autogenous grinding mill that materials as grinding media Particle Size : 200-350mm Improvement : High-efficiency autogenous mill can realize the second and third stage crushing and screening, and part or all crushing and grinding of rod mill or ball mill. Low power consumption, no dust\nGrinding Iron Ore in a Wet Autogenous Mill,The Cretaceous plant at the Hill Annex mine of the Jones & Laughlin Steel Corp. was designed with a wet autogenous mill for grinding low grade iron ore so that it could be concentrated by spirals and flotation cells. The plant, on the west end of the Minnesota’s Mesabi Range, went into operation in June 96 , and preliminary operating and metallurgical information describing the performance\nMacPherson Autogenous Grindability Test Mining SGS,The MacPerson Autogenous Grindability Test determines the MacPherson Correlated Autogenous Work Index*. This can be used in conjunction with the Bond Rod and Ball Mill Work Indices to determine power requirements, and to suggest circuit configurations for autogenous grinding AG and semi-autogenous grinding SAG circuits.\nAutogenous Grinding Information - Mine Engineer.Com , Photo of Autogenous Mill at a copper mine. Autogenous Mills operate, mechanically, similar to the ball mill. They differ in the media they use to break or grind the .\nAutogenous and Semi-Autogenous Mills | FL , Your FL subscriptions. ×. Industry of interest: * Cement Mining. Topics of interest:*. Industry trends: Discover newsletter. Innovations and solutions: Products .\nAutogenous and Semi-Autogenous Mills - ScienceDirect , These are known as semi-autogenous grinding (SAG) mills. In the mining industry all of these types of mills are in use. The disintegration and size reduction of .\n20 5082 Morrell Method-GMG-ICE-v0 -r0 Morrell method for ,AutogenousGrinding AG , Ball mill, Bond Work Index Wi , Comminution circuit, High Pressure Grinding Rolls ods to survey and sample grinding circuits for deter-mining energy efficiency. Montreal, QC: Global Mining Guidelines Group. SMC Testing Pty Ltd. 20 5 . The SMC Test is the most\nAutogenous and Semi-Autogenous Mills - ScienceDirect,It can, therefore, be concluded that autogenous grinding was not suitable with this particular gold ore, but semi-autogenous operation with about 8% ball charge was the preferred option. Based on these test results, a 6.7 m diameter × 2. m length SAG mill and a 3.3 m × 6. m ball mill were installed for commercial operation in closed circuit\nChina Autogenous Mill - China Autogenous Mill, Mining Machine,Autogenous Mill Introduction A wet grinder which uses material itself as grinding medium. feed size 200-350 mm Appli ion Generally used in the rough grinding operation. Improvement Can achieve two and three stages of crushing and screening. Advantages: Autogenous Mill is a new type of grinding equipment which has functions of crushing and grinding.\nAutogenous Grinding & Semi Autogenous Grinding Circuits , 8 Apr 2020 . Autogenous Grinding Circuits; SAG vs Ball Mill Advantages; AG & SAG Mill . With some ores, it is possible to send mine-run ore directly to the .\nINTERNATIONAL AUTOGENOUS GRINDING . - Isamill , High pressure grinding roll, stirred mill, energy, comminution, flowsheet development. INTRODUCTION. The mining industry will be faced with new challenges in .\nAutogenousGrinding Mill Type Of Cascade- JUMBO Mining machine,Surface Modifi ion Of Pyrite During. Absence of grinding steel media autogenous grinding ag of pyrite was performed using slightly larger pyrite pieces as grinding media in a l inert nylamid mill that was sealed during the grinding process on the whole for these tests we used 200 grams of d 80 27 cm pyrite ore 200 grams of d 80 023 cm particles and 025 l solution\nAutogenousgrinding mills AG mills - ,Autogenous mills, or AG mills, can accomplish the same size reduction work as two or three stages of crushing and screening. Often used in grinding at modern mineral processing plants, AG mills reduce the material directly to the desired final size or prepare it for the following grinding stages.\nmining gyrasphere autogenous mill operating principle,Jul 22, 20 40 83;32;Sag mill is a semi autogenous grinding acronym. Sag mills are used in the initial stages of mining. These are used for grinding the ore medium for further processing of it. Though these are autogenous mills, they make use of ceramic or metal balls for assisting in the process of grinding. 3. Sag Mill 4. Working Principle 5.\nAutogenous and semiautogenous mills > Wet grinding plants . , Autogenous (AG) and semiautogenous mills (SAG) are used for the coarse . The wet grinding plant for gold ore consists of an SAG mill and a ball mill. . to fit various types of machines for several appli ions like mining technologies, .\nSemi Autogenous and Autogenous Grinding Mills Market . , AG and SAG mills are extensively used in the mining industry for extracting minerals such as metals, oil shale, limestone, rock salt, coal, gemstones, dimension .\n- grinding mill of refractories industries\n- mining ore grinding ball mill wet ball mill used in ore dres\n- hot sale high efficiency grinding mill\n- grinding mill for sale cheap\n- ball mill grinding dalam pengolahan knn\n- htc concrete grinding for sale\n- local grinding machine in vietnam\n- pedestal grinding machine dry 220 volt 50 hertz\n- carbonatom grinding stone\n- immage of mechanical grinding ball mill machine\n- malaysia grinding ball mill or raymond mill\n- mirror fine grinding machine\n- concrete grinding vertical\n- mining dressing equipment grinding ball mill machine\n- grinding methods in 3 stone wet grinder\n- grinding ball mill indian lab\n- limestone grinding affter\n- dotco ro series vertical pencil grinding ball mill\n- gravel aggregate grinding mill manufacturer\n- grinding attachments crown granite crushing crusher\n- vibrating mechanism grinding\n- hammer crusher for mine crushing plant\n- china manufacturer for making stone crushing plant\n- kromite grinding mirror\n- anton mill road north end car park andover\n- wet beneficiation ball mill epic\n- jute mills corporation how to make a gold crusher\n- second hand milling machines for sale uk\n- grinding mills nano\n- ball mill starting method\n- grinding process discover\n- cost to crushing aggregate in kalimantan\n- ballast crushing and screening plant stone crusher machine\n- bauxite wet ball mill bestmine\n- determining horsepower of motor for ball mill\n- flow chat of ball mill machines\n- thoriated ball mill machine\n- various wet ball mill for mineral processing\n- construction of concrete crushing recycling in mercedes\n- animation of separator in ball mill","mesh vs. microns. mesh size refers to the number of openings in one inch of screen. for example, a 4-mesh screen means that there are four squares within one inch of the screen. As the mesh size increases, the particle size decreases to create a finer material. mesh size is not a precise measurement of particle size, but it is a fair estimation.super orion ball mill s.o. specifications load cell technology results in precision product level control. high consistency of the product quality demands exact measurement of the amount of product in the mill and because of this, an optional accessory offered for our ball mills is the load cell system which permits precision control of the product level.ball mill particle size ball mill particle size advantages of ball mills It produces very fine powder It is suitable for milling toxic materials since it can be used in a completely enclosed form. has a wide application. It can be used for continuous operation.\nparticle size distribution of grinding mill products. the particle size distribution was observed after 20, 40, 60, 80, 100, 150, 200 and 300 ball mill revolutions.ball mills give a controlled final grind and produce flotation feed of a uniform size. ball mills tumble iron or steel balls with the ore. the balls are initially 510 cm diameter but gradually wear away as grinding of the ore proceeds. the feed to ball mills is typically vol.-% ore and 25% steel.is micron particle size achievable in ball mills modification and change In ball mill. china supplier of methods of modification and change In ball milltrade show has always been a barometer of market development, it is learned, methods of modific. mining equipment for ore surface processing.\nball mill is a type of grinder used to grind or blend materials for use in mineral dressing processes, paints, pyrotechnics, ceramics, and selective laser sintering.it works on the principle of impact and attrition: size reduction is done by impact as the balls drop from near the top of the shell. ball mill consists of a hollow cylindrical shell rotating about its axis.ball mill nano cost of ball grinding mill to produce nano size particle. grinding mill to produce less than micron particle size.is micron particle size achievable in ball mills a ball mill with a classifier will produce a fine product but the particle size when reduced to microns a mesh product has 643 000ball mill nano of ball grinding mill to produce nano size particle.grinding mill to produce less than micron particle size.is micron particle size achievable in ball mills a ball mill with a classifier will produce a fine product but the particle size when reduced to microns a mesh product has 643 000.\nIn a pin mill, the particle size is controlled by three parameters: feed rate, rotor speed and airflow. In the past, a particle size with a of microns was only possible. now particle sizes down to a of less than 3.5 microns can be achieved, with a in the range of 1.5 microns. how ball mills air classifying mills or jet mills can be suitable for ultra-fine grinding and when you want to mill your foods to sub-micron levels we can help you with ball mill solutions. and when you are trying to reduce the particle size of a solid in suspension in a liquid, you bead mills are one of the most effective methods for processing fine particles into sub-micron or even nanometer size range. wide variety of designs exist to adapt to the different viscosities, material characteristics and targeted particle sizes of dispersions.\nsep 29, 2015 jet milling was carried out using a hosokawa alpine spiral jet mill. the stress mode that effects size reduction in the air jet mill is mainly impact, by way of particleparticle and particlewall collisions.the collision energy is created by the high speed flow of compressed air .compressed air injection and grinding pressures of bar and bar ball mills micron particle size.jan 22, 2015 milling is the third and final stage of comminution after crushing which involves the reduction of solid particle sizes to micron size level.the size reduction is achieved by a combination of abrasive and impact forces between the particles and the milling media as well as the walls of the milllaboratory ball mill for particle size micron grinding mill design amp ball mill manufacturer 911 metallurgist jan 2018 all grinding mill amp ball mill manufacturers understand the object of the grinding the choice of mill design depends on the particle size distribution in the feed and product with minus 500 microns can be obtained in an economical manner.\nball mills micron particle size in anguilla. ball milling material milling jet milling aveka,in ball milling the desired particle size is achieved by controlling the time applied energy and the size and density of the grinding media the optimal milling occurs at a critical speed ball mills can operate in either a wet or dry state while milling ball mills micron particle size. particle size for ball mill grind ball mill grinding and particle size separation mobile ball mill grind particle size analysis for gold oreball mills tumble iron or steel balls with the ore the balls are initially 510 cm diameter but gradually wear away as grinding of the ore proceeds the feed to ball mills dry basis is typically vol ore and steel the particle size Of ball mill ra-warnecke.de. comparing ball and vertical mills performance An figure particle size distributions from ball mill and vertical mill circuits since the automatic sampling system from concentrator II takes samples only from the combined streams on the regrinding circuit a series of manual sampling campaigns were conducted along the year of 2012 to make this study\nsize reduction & milling from labpilot to production requirements, hosokawa micron powder systems offers a complete line of size reduction technologies. whether you call it size reduction, micronization, milling, pulverizing, jet milling, air milling, or simply grinding, we can help.planetary ball mill up to micron ball mills micron particle size fdp balmoral ball mill operation process In fact, in the same application a vertimill has less footprint than a ball operating is an energy efficient grinding tend to grind more efficiently than for example ball mills with feeds as coarse as mm to products finer than provides up to a higher hammer crusher micron size product hammer crusher micron size product microns mesh hammer mill in madagascar. mean particle size in microns or match hammer to 300 microns. hammer mill for very fine products in uniform with micron dietary particle size of 700 microns is a good goal fineness on and 212 micron sand washer, ball mill, vsi\nany of the vke mills can be run wet or dry, and configured in multiples with a rod equipped rougher mill feeding two fine grind mills equipped with steel balls, ceramic or any type of media that can be used to grind material down to sub micron particle size. our vibrokinetic energy mill is designed for ultra fine pulverization of minerals and ball mill grinding particle size distribution. ball mills micron particle size. ball mill optimization mill grinding particle size distribution case mill test from vdz congress 2009 cement plant in europe chamber good size reduction efficiency chamber micron shown results that grinding has stopped midway through the chambadvantages of ball mills It produces very fine powder It is suitable for milling toxic materials since it can be used in a completely enclosed form. has a wide application. It can be used for continuous operation.\ngrinding machine for micron size particle modification and change In ball mill. china supplier of methods of modification and change In ball milltrade show has always been a barometer of market development, it is learned, methods of modific. mining equipment for ore surface processing.particle sizes for carborex grit size to micron comparison chart. this grit chart is designed to give a general guideline for correlating fepa, ansi this single unit is capable of milling materials up to a hardness of mohs and enables accurate and highly controllable particle sizes to be achieved, typically with narrow size distribution. range of mill sizes is available for throughputs ranging typically from 0.g to tonne per hour with average particle sizes as low as to microns.\nIn ball milling, the desired particle size is achieved by controlling the time, applied energy, and the size and density of the grinding media. the optimal milling occurs at a critical speed. ball mills can operate in either a wet or dry state. while milling without any added liquid is commonplace, adding water or other liquids can produce the can planetary ball mill grind particles to smaller than 0.1 micron? yes, the size of particle can be smaller than 0.m. We have customers using our planetary ball mills grinding particles to submicrons. how to tell the size distribution of the particles? the particle size distribution will be in microns.the effect of wood particle size distribution on the reducing the particle size of WF to get a smaller, and more uniform of size can be accomplished with a ball mill machine. besides particle size reduction, ball mills are also widely used for mixing, dispersing, and blending.\nball milling and particle size modification and change In ball mill. china supplier of methods of modification and change In ball milltrade show has always been a barometer of market development, it is learned, methods of modific. mining equipment for ore surface processing.ball mills steel ball mills amp lined ball mills particle size reduction of materials in a ball mill with the presence of metallic balls or other media dates back to the late the basic construction of a ball mill is a cylindrical container with journals at its axis line ball mill ouitput size schittlisofficech. Is micron particle size To participate in the etallurgist forums, be sure to join & login use add new topic to ask a new questiondiscussion about grinding. OR select a topic that interests you. use add reply to replyparticipate in a topicdiscussion using add reply allows you to attach images or pdf files and provide a more complete input. use add comment to comment on someone elses\nfeb 10, 2016 mill and before implementing the project, we would like to gather as much information regarding the actual operation of this mill during cement grinding and the quality of the product in terms of the finenessparticle size distribution and how does it compare with the product of a ball mill relative to these parameters."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8003ba16-4e54-4d32-b3ef-1cc8beff3c27>","<urn:uuid:b4ce9574-b6b1-4de8-b4e5-93714cfb17cf>"],"error":null}
{"question":"What does it mean to 'play the board' in poker? Please provide a clear explanation.","answer":"Playing the board in flop games means that your best five-card hand consists of all five community cards. In this situation, none of your hole cards are used in making your final hand because the five cards on the board make up the strongest possible combination.","context":["For many years of existence of poker settled definite terminology, which is used by playes in the game. If you want to feel yourself naturally and to speak with another players on the same level, you must know and understand the main terminology. The list of often used terms is given below:\nA game in which players are playing a lot of pots. Actions: Checking/Betting/Raising\nA player who is still involved in the current game.\nA style of the game distinguished by the aggressive betting decisions of the player. The aggressive player will rather raise and re-raise than call, check or fold.\nBetting all the chips in a single betting round.\nMoney placed in the pot before the hand is begun.\nWhen a hand is beaten by a lucky draw.\nCatching both the turn and river card to make a drawing hand. For instance, let’s imagine you have Ah-10h as your pocket cards. The flop brings Kd-6c-5h. You bet and are called. The turn is the Jh, which everybody checks, and then the river is the Qh. You've made a \"backdoor\" nut flush.\nThe term used to call the re-raise from a player who originally called.\nMeans the amount of money managed by the player.\nThe term used to mention the fact of placing the chip into the pot.\nThe odds you get as a result of evaluating the number of callers to a raise.\nBet the Pot\nWhen a player bets the same amount as there is in the pot.\nThe amount fixed by the table limits, which is placed by the player sitting in the second position, clockwise from the dealer, before any cards are dealt. (Players joining a game in progress must post a Big Blind, but may do so from any position.)\nA starting hand that contains an A-K cards.\nThe obligatory forced bet(s) that must be made by the two players sitting directly to the dealer's left which will start the action on the first round of betting. The blinds are posted before any cards are dealt. (A 'Blind' bet is one that is made without looking at your cards.) Traditionally there are two Blinds: Small Blind and Big Blind, the amount of them is set by the table limits. For instance, in the game with 2/4 limits, the amount of the Small Blind will be equal to 2, and the amount of the Big Blind will be equal to 4, correspondingly. So, remember, the Small Blind is always a half of the Big Blind.\nThe situation when a player raises without looking at his cards.\nTo make other players believe that you are having a better hand than you might have have, by means of aggressive betting and raising.\nThe cards that are dealt face-up in a poker game for all players to see. In Texas Poker, five cards are dealt face-up in the center of the table.\nThe situation when a player uses the lowest card on the flop to make a pair with one of his own cards.\nThe term is used to call a pair of Aces.\nThe term is also known as the dealer button. It is a small round disk that is moved from player to player in a clockwise direction following each hand. That action is performed to theoretically indicate the dealer of each hand.\nThe term is used to determine the minimum amount of money required by a player to sit down in a particular poker game. Other words, the player “buys” his place at the table.\nCorrespond to the bet amount. In other words it is an answer to the previous raise or bet.\nTo call both a bet and a previous player’s raise.\nThe situation when a player decides to take the last kind of raise allowed per round of betting. There are 4 possible amounts of allowed raises, Cap is the last one.\nTo leave the game and convert chips into cash.\nWhen it's a player's turn to act and there has been no action in front of him and he decides not to place any bet, he 'checks.' Other words, the player checks the next actions at the table.\nThe term describing the situation when a player first checks and then raises in a single betting round.\nThe situation when all the players fold to the blinds before the flop. After that the blinds are returned to the players, who posted them and move on to the next hand. The term is also known as “chopping the blinds”\nA person who handles the cards, gives out the pots, and monitors the game.\nThe term used to determine a player who is the last to act in a betting round. The term is also known as the position ‘on the button”.\nOne of the four playing card suits. These are the red color cards represented in a form of tiles.\nA tool to protect online players in case their internet connection is lost while involved in a pot.\nHand that is possible to be played, but tend to lose against similar non-straight or flush hands. Example: A2 is being dominated by the other hands with an Ace, which might have a better second card apart from an Ace. Dominated hand loses or draws without improvement.\nStrong hands that are not only good, but also have lots of room for improvement.\nHole cards, or cards that are dealt face down to the player.\nA hand that is one or more cards away from the winning combination. The term is usually applied to a potential straight or flush. The missing card may come on any of the next rounds.\nThe situation when a player folds.\nA pair of Twos.\nPosition at the playing table during the betting round, where the player must act before most of the other players at the table. Two positions located to the left of the Blinds are considered to be “early positions”.\nThe term used to determine the fifth card among the other community cards to be revealed and placed on the board. Also known as the River.\nIn Hold'em, the first three community cards that are dealt face-up in the center of the table, before the betting round begin. The 'flop' also indicates the second round of betting.\nAny five cards of the same suit.\nWhen a player has four cards in his hand, all of the same suit and is hoping to draw a fifth card in the next rounds to make a Flush.\nTo throw your cards away when it's your turn to act. When the player decides not to play with his current cards he folds.\nA required bet that starts the action on the first round of a poker hand. This term is also used to determine an obligatory bet a player has to make in order to join the game or return to the game he had temporary left.\nFour of a Kind\nFour cards of the same number or face value ('quads').\nThe term used to determine the fourth card that is revealed and placed on the board among the other community cards. Also known as the Turn.\nAny three cards of the same number or face value, plus any other two cards of the same number or face value.\nA situation when a player is 1 card away from a straight and there are two betting rounds left to complete it. It means that he has 4 out of 5 cards required to form a straight.\nA player's best five cards.\nThe situation when only two players remaining in the game and are involved in the pot.\nOne of the four playing card suits. Red suit, represented in a form of a heart.\nIn order to decide the first dealer in the flop tournaments, each user is dealt a single card and the player with the highest card (based on the card and the suit order - of spades, hearts, diamonds & clubs) becomes the dealer. The second definition is a hand which ranking is determined only by the value of the highest card in the hand. In case any combination was formed at the table, the player with the high card in the hand wins.\nA game that requires a large amount of money to wager.\nGames where the pot is spited into High and Low ones for the corresponding hands.\nAlso known as Texas Hold'em. The poker game, where the players get two down cards and five community cards.\nThe term used typically in the community cards poker. These are individual face-down cards dealt to the player so that no other players are entitled to see them. Also knows as “pocket cards”.\nFour cards that require another one between the top and the bottom card to complete a straight. In other words – the middle card required to complete a straight. Example: a player has 5, 6, 8, 9 – to complete a straight, a player needs the 7. Players who managed to catch this card make an Inside Straight.\nA prize fund awarded to a player who meets a set of predetermined requirements. For example, some casinos will give a jackpot to someone who gets four-of-a-kind or higher and loses.\nThe highest unpaired side card in a player’s hand. For example, in 4 of a Kind combination, 4 cards out of 5 cards in the hand are involved in the combination; the remaining card is called a kicker. In Three of a Kind situation – two kickers are remaining.\nAn invitation to make a raise.\nThe term used when a player doesn’t want to raise, call or fold. The term is also known as Check.\nA Term used to call the position on a betting round where the player must act after most of the other players have acted. Usually is considered to be two positions next to the right of the button.\nA game that has fixed minimum and maximum betting limits, along with a specified number of raises allowed.\nA hand that can still win the pot.\nA game where the amounts wagered are small.\nThe center pot. The pot, which all the active players have contributed into and all of them are eligible to win it. Any other bets are placed in a side pot(s) are contested among the remaining players. For example: This occurs when a player goes all-in with a smaller amount of money than other players remaining in the pot.\nA pair composed with a pocket card and the second-highest card out of three cards revealed on the flop; second pair.\nThe position at the poker table between the early position and the late one. The fifth, sixth and seventh players to the left of the button are having the middle position. Any position between the first one-third part of the table and the final third of players to act.\nThis term has several meanings. First is the discarded card or cards thrown away and kept in a pile aside. It is also a pile of cards that are no longer in play. And the other meaning is not revealing your cards after another player has won.\nThe minimum amount of money which will allow you to start a game.\nA game where players can bet as much as they like (as long as they have it in front of them) on any round of betting.\nThe best possible hand during any point of the game. A hand that cannot be beaten.\nThe probability of making a hand vs. the probability of not making a hand.\nCards of different suits.\nOn the Button\nTo be on the button means to be the last player to act in a betting round. The term is also recognized as the Dealer's Position. The latest position at the table is called “on the button”.\nFour consecutive cards whereby one additional (consecutive) card is needed to make a Straight. There are two possible card ranks that will complete a straight; as opposed to a one-ended straight draw or an inside straight draw.\nA card that is dealt face-up.\nA pair that has been dealt face-up.\nA disc placed in front of a player who wishes to sit out a hand, but remain in the game and simply observe the process.\nThe number of cards left in the deck that theoretically can improve your hand.\nThe term used to call pocket pair of cards which are higher than any of the cards on the board.\nTwo cards of the same face or number value.\nMeans to fold.\nTerm used when a player decided to raise or re-raise another player's bet.\nPlaying the Board\nIn flop games, when your best five card hand consists of all five community cards.\nThe initial cards dealt to the player. The term is also known as the down cards or the hole cards.\nThe two cards dealt to you at the beginning of a Hold'em hand that no one else is entitled to see.\nAnother name for a pair of Aces in the pocket.\nThis term is used to determine the actual place where a player is located in relation to the dealer. Position therefore establishes the player's turn in the betting rounds.\nIf you miss the blind, and then you must post - add as many chips as were in the blind itself.\nThe money or chips in the center of a table that players try to win.\nThis is a game where the maximum bet can be equal to the amount of money in the pot.\nMeans to fold a hand.\nAnother name of Four of a Kind combination.\nTo raise means to increase the previous bet.\nThe certain amount of chips taken from the pot by the card room in a form of compensation for hosting the game is called Rake.\nMeans the placement of the cards value starting from the best to the worst basing on the face value of cards regardless of their suits.\nWhen a player physically knocks on the table indicating that means he/she has checked.\nAnalysis of a player based on how he plays, mannerisms, and talks performed in order to see the broad picture of the opponent and create the gaming strategy based on personal examinations.\nThe amount of money a player pays to add a fixed number of chips to his/her stack in a tournament.\nTo raise after a raise.\nAny \"live\" non-tournament game played for real money.\nThis is the last card given in all games.\nRound of Betting\nThis is when players have the opportunity to bet, check or raise. Each round of betting ends when the last bet or raise has been called.\nThis is an Ace high straight (A-K-Q-J-10) all cards of the same suit. It is the best possible hand in poker.\nA mini-tournament winning in which will give a player an entry into a larger tournament.\nA waiting list. A player can put his or her name on this list if there are no players at the table at which they are willing to play.\nIn flop games, when you pair the second highest card on the board.\nMeans having a pocket pair that matches one of the cards on the board. In other words Three of Kind formed by two pocket cards of the player and one out of the community cards on the board.\nAt the end of the final betting round, it's when all active players turn their cards face-up to see who has won the pot.\nA separate pot(s) which is contested by remaining active players when one or more players are all-in.\nThe amount put in the pot by the person immediately to the left of the dealer 'button' prior to the cards being dealt.\n1. Describes the portions of the pot divided between several winners.\n2. The situation when both players do not want to show cards in the showdown and decide to split the pot between them.\n3. A showdown involving multiple hands of equal rank; a tie.\nA pile of chips.\nFive consecutive cards of any suit.\nFive consecutive cards of the same suit.\nThe limits put on the blinds/ante, bets, and raises in any particular game.\nA characteristic of a playing card. The card being either of Clubs, Diamonds, Hearts, or Spades.\nThe surface on which poker is played.\nThis is also the name for Hold'em, the most popular form of poker played with two face-down cards dealt to each player, and 5 community face-up cards on the board.\nThree of a Kind\nThree cards of the same number or face value ('triplet').\nEither a player who doesn't play many pots, or a game that doesn't have much action.\nIn flop games, when the player pairs one of his down cards with the highest card on board.\nTop Two Pair\nHaving both your pocket cards matching the highest two cards on the board.\nIn flop games, this is the fourth dealt card. It is the third round of betting.\nA hand consisting of two different pairs.\nA pocket pair of lower value than the lowest card on the board.\nA card that is dealt face-up.\nA term used to call a losing hand.\nWorld Series of Poker. The biggest poker tournament played annually in Las Vegas."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0b9fdd07-e1ea-4a6f-8479-7d4d3b7c0352>"],"error":null}
{"question":"How does the threat intelligence lifecycle work, and why is it more complex than just consuming threat feeds?","answer":"The threat intelligence lifecycle consists of 4 major stages: Collection (ingesting IOCs from multiple sources), Normalization (converting data into a standardized format), Analysis (combining tactical and technical intelligence through deduplication, enrichment, correlation, and confidence scoring), and Dissemination/Actioning (delivering intelligence to internal security tools and external partners). Like cooking requires more than just buying ingredients, effective threat intelligence requires proper processing through all these stages to produce actionable intelligence that improves security operations.","context":["“Threat Intelligence is all about consuming feeds”, said no security professional ever. This is like saying that to cook a great meal all you have to do is buy the ingredients. While that is a good start, you still need to know how to properly combine the ingredients and prepare them in a way that leads to a finished and delicious meal. Acquiring and consuming threat intelligence feeds is an important step to take, but much like buying ingredients for a meal, it is only the first part of the process. In the end, threat intelligence is only valuable and useful to an organization if it is relevant and actionable, leading to improvements in overall security operations.\nSince consuming threat intelligence is only the beginning, it's helpful to look at the entire process. The threat intelligence lifecycle can be broken down into 4 major stages: Collection, Normalization, Analysis, Dissemination, and Actioning.\nThreat intelligence Collection\nThe first stage in the threat intelligence lifecycle is the ingestion of threat intelligence which includes indicators of compromise (IOCs) from multiple threat intelligence sources such as commercial feeds, open-source feeds,, industry peers, vendors, regulatory bodies, etc. These feeds are often ingested in both structured and unstructured formats depending on their source. Unstructured threat data usually includes emails, reports, and blogs while Structured threat data includes intelligence in STIX 2.1, STIX 1.0, etc. formats.\nThreat intelligence Normalization\nThe second stage in the threat intelligence lifecycle is to normalize multi-sourced threat intelligence into a single format such as STIX. Normalization is a prerequisite for threat data correlation and analysis. It converts the entire ingested threat data into a structured and standardized format that can be more easily managed and disseminated to deployed security technologies or shared with industry peers, vendors, or sharing community (ISAC/ISAO) members.\nThreat intelligence Analysis\nThe key objective of threat intelligence analysis is to combine tactical and technical intelligence to produce actionable intelligence. The analysis stage involves several key functions such as deduplication, enrichment, correlation, and confidence scoring. Threat intelligence needs to be enriched by combining information from various trusted sources and correlating threat indicators to prioritize them based on several parameters such as the geographical location, industry sector, previous incidents, and other contextual factors. To move towards a proactive cybersecurity approach, threats need to be detected in their early stages by using tactical threat intelligence. Tactical threat intelligence plays a crucial role by highlighting a threat actor’s tactics, techniques, and procedures (TTPs). The MITRE ATT&CK framework\nwhich is a database of all TTPs used by threat actors in real-world campaigns helps security teams to better identify attacker footprints and prioritize threat response.\nIntelligence Dissemination and Actioning\nThe enriched and analyzed threat intelligence derived from the previous stages now needs to be actioned upon to bring to its logical conclusion of helping out with proactive mitigation. The actioning includes dissemination of threat intelligence into the internally deployed security tools and technologies and sharing with internal teams and external partners. Internal - The enriched threat intelligence is now delivered to multiple internal teams in the SOC, such as the incident response team, threat hunting team, VAPT teams, etc. The enriched threat intelligence is then utilized to inform decisions and initiate actions in security tools such as Firewalls for blocking/unblocking a particular IOC, removing an IOC from the watchlist, blocking phishing attempts, quarantining an infected device, etc.\nExternal - The enriched threat intelligence should also be disseminated to external teams such as trusted sharing community (ISACs/ISAOs) members, industry peers, subsidiaries, and external vendors.\nSeeing the entire lifecycle, it is easier to see how each step is crucial in producing the end result. A threat intelligence platform (TIP), like CTIX\n, performs a variety of functions such as intelligence collection, normalization, enrichment, analysis, and dissemination by leveraging automation to increase the speed and efficiency of analysts while also improving accuracy. If you are a part of a smaller team or just looking to get started with actionable intel, then CTIX Lite\nmight be a better fit for you and your budget."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:31165d2f-638a-49fb-878d-4c5edea15083>"],"error":null}
{"question":"what will be timeline of brain damage in future cardiac arrest cases without cpr help?","answer":"The timeline of brain damage in cardiac arrest cases is as follows: From 0 to 4 minutes, brain damage is unlikely to develop. From 4 to 6 minutes, there is a possibility of brain damage. From 6 to 10 minutes, there is a high probability of brain damage. After 10 minutes and over, brain damage is probable.","context":["Updated Feb 2019\nCpr facts and stats published and released annually by various organizations and publications in the health and safety space. According to recent stats, more than 70% SCA or Sudden Cardiac Arrests occur at home or similar private settings.95% of Sudden Cardiac Arrest victims die prior to even reaching the hospital. Out of all these numbers, only 6% survive cardiac arrest. Therefore, getting trained and acquainted with the basics of CPR and learning how to perform CPR will help you save life of a loved one.\nAn unresponsive victim with no signs of breathing and pulse is considered to be in a cardiac arrest. Providing CPR in the initial minutes can help circulate blood that contains oxygen to the victim’s brain and other vital organs.\nKEY CPR FACTS AND STATISTICS\n- More than 350,000 out-of-hospital cardiac arrests occur in the United States per year, out of which 70% happen inside homes\n- 90 percent of people who suffer cardiac arrest die prior to reaching a hospital or medical care facility.\n- Effective CPR provided by a bystander in the first few minutes of cardiac arrest can increase the chances of survival by 2x or 3x.\n- According to AHA, the bystander should push the chest at a rate of 100 to 120 compressions per minute.\n- Less than 20 percent Americans are equipped to perform CPR during a medical emergency situation.( AHA Study)\n- CPR aids in maintaining vital flow of blood that to the brain and heart. It also aids in increasing the duration of electric shock provided via a defibrillator, thereby, making the process more effective.\n- If a bystander does not perform CPR, the survival chances of a victim will decrease 7% in every single minute of delay.\n- Within 5 – 6 minutes after a victim has experienced cardiac arrest and within that time span, no CPR is performed, followed by defibrillation, the victim might further suffer from brain death crisis.Within 4 to 6 minutes after a victim has experienced cardiac arrest brain death and permanent death starts to occur\n- In addition to cardiac arrests, 7 million people, including children and adults suffer injuries every year in their homes or within similar environments. Use of CPR is commonly required in those medical emergencies.\n- As per studies, 45% heart attacks occur amongst people under 65 years of age.\n- As per AHA, 1 in 6 men and 1 in 8 women, above 45 years of age have had stroke or heart attack.\n- When providing mouth-to-mouth resuscitation , the victim receives oxygen concentration of 16 vs the 20 to 21 percent received by ambient air.\nMedical Emergencies where CPR can be used\n- Heart attack\n- Electric shock\n- Allergic reactions of severe nature\n- Drug overdose\nTimeline of CPR\n- 0 to 4 minutes, unlikely development of brain damage\n- 4 to 6 minutes, possibility of brain damage\n- 6 to 10 minutes, high probability of brain damage\n- 10 minutes and over, probable brain damage\nAHA or American Heart Association has been involved in providing education related to CPR only within the contours of public chest compression, in order to encourage more and more people to combat and respond to medical emergencies that require CPR and first aid performance.\nMost individuals or bystanders get apprehensive about medical emergencies and similar situations. They either fear handling a victim or lack the knowledge of giving CPR and responding effectively in such situations. For the first few minutes, a bystander can relax and gear up before responding to the victim because the victim has sustainable amount of oxygen in his/her blood within those few minutes. The most important thing to do is get the oxygenated blood circulated via the heart, brain and lungs of the victim, which is ensured when the bystander or rescuer starts giving chest compressions.\nWhy should you learn CPR skills ?\nCardiac arrests are not uncommon medical emergencies. They can occur at any time and any place. Even an individual who might appear to be healthy can suffer from a cardiac arrest or similar condition. Furthermore, cardiac arrest and heart attack are not same conditions. Sudden cardiac arrest does not occur out of any severe heart condition. It can occur out of electrical impulses, severe injuries etc. Therefore, it will be helpful for not only you but others around you, to learn at least the basics of CPR and techniques of performing the same on victims, irrespective of gender or age.\nVital facts of choking\n- According to AHA, over 90% deaths due to foreign objects occur amongst children below the age 5 years, while 65% is amongst infants.\n- Liquids are amongst the most common causes of choking, especially in infants.\n- 19% causes of choking are due to candies, while 65% are due to hard candies.\n- 160 children within the age 14 years, in 2000, died because of respiratory tract obstruction due to ingestion or inhalation of foreign bodies.\n- In 2001, about 11% children were treated for choking emergencies.\nWhat percentage of people are saved by CPR ?\n- According to a study by AHA (PDF)\n4% to 16% of patients who received immediate CPR were eventually discharged from the hospital. This number includes people who were already at the hospital during the cardiac arrest\n- According to The new England journal of medicine, About 18% of people aged above 65+ who received CPR at the hospital survived and were discharged"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f629bf04-58c9-4ee2-93e0-76f5779cfd5f>"],"error":null}
{"question":"As a project manager interested in innovation, can you compare how environmental factors influence both business CSFs and muscle tissue engineering outcomes?","answer":"In both cases, environmental factors play a crucial role in success. For business CSFs, the external environment (political, economic, social, and technological factors) largely determines their design and must be carefully considered even if they can't always be influenced. Similarly, in muscle tissue engineering, the environment or 'nature's incubator' (the body) is crucial for further development of the implanted tissue. The implants are not fully functional at the time of implantation but require the body's environment for proper development and integration, showing how environmental factors are critical to success in both fields.","context":["Critical Success Factors\nThis article offers a practical explanation of Critical Success Factors (CSF). After reading, you’ll understand the basics of this powerful strategy tool.\nWhat are Critical Success Factors?\nCritical Success Factors, or CSFs, are indicators for opportunities, activities or conditions required to achieve an objective within a project or mission. Critical Success Factors (CSF) differ per organisation and reflect current and future objectives.\nWhether it concerns a bar, an insurance agency or contractor, it’s essential that the course of action is coordinated with those aspects that help the organisation fulfil its mission. These key variables often have a huge impact on the degree to which a company is successful and effective in reaching strategic goals within the mission and are crucial in gaining a competitive advantage.\nCritical Success Factors (CSF) are therefore of vital importance for the success of an organisation. They can be created for a specific department within the organisation, for the organisation as a whole, but they are always directly linked to the company’s strategy and are created by higher management.\nThe concept of CSFs was developed and introduced by D. Ronald Daniel, on behalf of McKinsey & Co, in 1961. A decade later, John F. Rockart refined and popularised the concept. Ever since then, the concept has been widely applied to guide organisations in developing and implementing strategies and projects.\nRelationship Critical Success Factors (CSF) and Strategic Planning\nJohn F. Rockart emphasised that CSFs are intended to expose critical points in the organisation, particularly with regard to management. He believed that CSFs improve an organisation’s development and increase the value of procedures by revealing criteria that can hinder the achievement or failure of a specific organisational goal.\nRockart also recognised that CSFs are essential in the strategic planning process. According to him, it’s important that a company’s characteristics are marked to gain a competitive advantage.\nDespite the fact that CSFs don’t provide a concrete contribution to the strategy’s progress, they do provide a significant contribution to the planning procedure of the strategy. When the CSFs are combined with a complete strategic planning method, they function as elements that are vital to an organisation’s success.\nSources Critical Success Factors (CSF)\nCritical Success Factors (CSF) arise from five important sources or areas that influence an organisation. These areas differ from each other, given that different situations lead to different CSFs. Rockart and Bullen have written about the following five most important sources of CSF:\nIndustry Critical Success Factors (CSF)\nThese factors are dependent upon the specific industry characteristics. It’s important that the organisation continues to monitor these factors to be able to compete in the market. For instance, a chemical company demands specific technology and a clothing producer absolutely requires cotton. These Critical Success Factors (CSF) may influence all competitors within a specific industry, but could also affect individual organisations.\nCompetitive Strategy and Industry Position CSF\nNot all companies in a specific industry have the same Critical Success Factors (CSF). The current position and development phase impact which Critical Success Factors (CSF) are created, as well as the available means and capacities. In addition to an organisation’s total value, the demographic and other factors, each management will create different Critical Success Factors (CSF).\nEnvironmental Factors Critical Success Factors (CSF)\nThe external environment of an organisation largely determines the design of the Critical Success Factors (CSF). A PEST analysis can be used to analyse this external environment. These political, economic, social and technological factors create CSFs for every company. The organisation isn’t always able to influence these macro-environmental factors, but these must certainly be considered. Managers who work in production, for instance, must be able to guarantee quality and keep sufficient stock.\nManagement Critical Success Factors (CSF)\nIndividual or relatively small aspects within organisations may also lead to new CSFs. When certain responsibilities within a management position are considered to be crucial for an organisation’s performance as a whole, this must be closely monitored and measured.\nTemporary Factors Critical Success Factors (CSF)\nTemporary factors are linked to short-term situations. Although these factors can be important, they are usually not long-lasting. Temporary or one-time factors are often the result of a certain event. When an organisation expands into a new market, for instance on another continent, the CSF may concern expanding and recruiting new capable management.\nThings that are measured are carried out more often than things that aren’t measured. Each Critical Success Factor (CSF) must be measurable and linked, or be related to a specific company goal.\nDeveloping and Measuring Critical Success Factors (CSF)\nThere are several factors and principles that help in developing CSFs.\nFirst, the organisation and its environment must be visualised. This may be done after the five primary sources of CSFs have been analysed, as described above. Knowledge of the competition and their strategy is a key factor in this process. Sources for analysing the external environment include: news articles, trade associations, information on buyers and suppliers, analyst reports and analyses of the financial overviews.\nSubsequently, develop critical success factors that can be measured with observable differences. It’s not absolutely necessary that these are measured quantitatively; writing CSFs in observable terms is sufficient.\nA good CSF starts with an action verb and describes what’s important in as few words as possible. Examples of action verbs include: inspecting, monitoring, analysing, determining, etc. An example of a CSF that starts with an action verb is: ‘monitor supply and demand and future trends’. Such CSFs are presented as an activity.\nCSFs can also be presented as a requirement, for instance: ‘after the sales target of product A has been achieved, further analysis will lead to concrete requirements in the development of product B’.\nSubsequently, determine a way to measure whether the Critical Success Factors (CSF) are achieved. When the CSFs are fulfilled, chances are that the general strategy will be successful. Take the right precautions to ensure that projects are carefully measured. Let’s say that the general strategy is to double the size of the organisation.\nA related CSF is then growth within the current customer base. If this CSF doesn’t perform well, the strategic goal is compromised. Regularly follow and monitor each CSF.\nThe frequency of this depends on the strategy. A CSF linked to the budget or revenue can be kept track of each quarter. In case of a CSF connected to recruiting and hiring of new personnel, monitoring twice a year is enough.\nCritical Success Factors examples\nBelow are several examples of Critical Success Factors (CSF). Some might be irrelevant in certain industries, whereas other industries additional CSFs should be added.\n- Increase customer loyalty\n- Prevent price wars\n- Invest in rising markets\n- Respond to changing customer needs and wishes\n- Analyse and understand the capacity, potency and strategy of the competition\n- Develop new technological tools to boost the production process\nNow it’s your turn\nWhat do you think? Are you familiar with the explanation of Critical Success Factors (CSF)? How do you think you can apply CSFs within your organisation or do you already use them? What do you think are essential critical success factors within the industry you’re interested in?\nShare your experience and knowledge in the comments box below.\n- Rockart, J. F. (1979). Chief executives define their own data needs. Harvard business review, 57(2), 81-93.\n- Hong, K. K., & Kim, Y. G. (2002). The critical success factors for ERP implementation: an organizational fit perspective. Information & management, 40(1), 25-40.\n- Leidecker, J. K., & Bruno, A. V. (1984). Identifying and using critical success factors. Long range planning, 17(1), 23-32.\nHow to cite this article:\nJanse, B. (2019). Critical Success Factors. Retrieved [insert date] from ToolsHero: https://www.toolshero.com/strategy/critical-success-factors/\nAdd a link to this page on your website:\n<a href=”https://www.toolshero.com/strategy/critical-success-factors/”>ToolsHero: Critical Success Factors</a>\nWe are sorry that this post was not useful for you!\nLet us improve this post!\nTell us how we can improve this post?","\"While the body has a capacity to repair small defects in skeletal muscle, the only option for larger defects is to surgically move muscle from one part of the body to another. This is like robbing Peter to pay Paul,\" said George Christ, Ph.D., a professor at Wake Forest Baptist Medical Center's Institute for Regenerative Medicine. \"Rather than moving existing muscle, our aim is to help the body grow new muscle.\"\nIn the current issue of Tissue Engineering Part A, Christ and team build on their prior work and report their second round of experiments showing that placing cells derived from muscle tissue on a strip of biocompatible material – and then \"exercising\" the strip in the lab – results in a muscle-like implant that can prompt muscle regeneration and significant functional recovery. The researchers hope the treatment can one day help patients with muscle defects ranging from cleft lip and palate to those caused by traumatic injuries or surgery.\nFor the study, small samples of muscle tissue from rats and mice were processed to extract cells, which were then multiplied in the lab. The cells, at a rate of 1 million per square centimeter, were placed onto strips of a natural biological material. The material, derived from pig bladder with all cells removed, is known to be compatible with the body.\nNext, the strips were placed in a computer-controlled device that slowly expands and contracts – essentially \"educating\" the implants on how to perform in the body. This cyclic stretching and relaxation occurred three times per minute for the first five minutes of each hour for about a week. In the current study, the scientists tried several different protocols, such as adding more cells to the strips during the exercise process.\nThe next step was implanting the strips in mice with about half of a large muscle in the back (latissimus dorsi) removed to create functional impairment. While the strips are \"muscle-like\" at the time of implantation, they are not yet functional. Implantation in the body – sometimes referred to as \"nature's incubator\" – prompts further development.\nThe goal of the project was to speed up the body's natural recovery process as well as prompt the development of new muscle tissue. The scientists compared four groups of mice. One group received no surgical repair. The other groups received implants prepared in one of three ways: one was not exercised before implantation, one was exercised for five to seven days, and one had extra cells added midway through the exercise process. The results showed that exercising the implants made a significant difference in both muscle development and function.\n\"The implant that wasn't exercised, or pre-conditioned, was able to accelerate the repair process, but recovery then stopped,\" said Christ. \"On the other hand, when you exercise the implant, there is a more prolonged and extensive functional recovery. Through exercising the implant, you can increase both the rate and the magnitude of the recovery.\"\nA variety of laboratory tests were used to measure results. A test of muscle force at two months, for example, showed that animals who received the implants with extra cells added had a threefold increase in absolute force compared to animals whose muscle damage was not repaired. The force-producing capacity of muscle is what determines the ability to perform everyday tasks.\n\"If these same results were repeated in humans, the recovery in function would clearly be considered significant,\" said Christ. \"Within two months after implantation, the force generated by the repaired muscle is 70 percent that of native tissue, compared to 30 percent in animals that didn't receive repair.\"\nThe results also showed that new muscle tissue developed both in the implant, as well as in the area where the implant and native tissue met, suggesting that the implant works by accelerating the body's natural healing response, as well as by prompting the growth of new muscle tissue.\nThe researchers hope to evaluate the treatment in patients who need additional surgery for cleft lip and palate, a relatively common birth defect where there is a gap in muscle tissue required for normal facial development. These children commonly undergo multiple surgeries that involve moving muscle from one location to another or stretching existing muscle tissue to cover the tissue gap. The implant used in the current research is almost exactly the size required for these surgeries.\n\"As a surgeon I am excited about the advances in tissue-engineered muscle repair, which have been very promising and exciting potential in the surgical correction of both functional and cosmetic deformities in cleft lip and cleft palate\" said Phillip N. Freeman, M.D., D.M.D., associate professor of Oral and Maxillofacial Surgery at the University of Texas Health Science Center at Houston. \"Current technology does not address the inadequate muscle volume or function that is necessary for complete correction in 20 percent to 30 percent of cases. With this innovative technology there is the potential to make significant advances in more complete corrections of cleft lip and cleft palate patients.\"\nThe technology was originally developed under the Armed Forces Institute of Regenerative Medicine (AFIRM) with funding from the Department of Defense and the National Institutes of Health. The sponsor of the current research was the Telemedicine & Advanced Technology Research Center. A longer-term goal is to use the implant – in combination with other tissue-engineered implants and technologies being developed as part of AFIRM -- to treat the severe head and facial injuries sustained by military personnel. For example, AFIRM-sponsored projects under way to engineer bone, skin and nerve may one day be combined to make a \"composite\" tissue.\nCo-researchers were Benjamin T. Corona, Ph.D., lead author, Masood A. Machingal, Ph.D., Tracy Criswell, Ph.D., Manasi Vadhavkar, MD., Ashley C. Dannahower, Christopher Bergman, and Weixin Zhao, M.D., all from Wake Forest Baptist.\nMedia Contacts: Karen Richardson, firstname.lastname@example.org, (336) 716-4453 or Main Number (336) 716-4587.\nAbout the Wake Forest Institute for Regenerative Medicine\nThe Wake Forest Institute for Regenerative Medicine is dedicated to the discovery, development and clinical translation of regenerative medicine technologies. The institute has used biomaterials alone, cell therapies, and engineered tissues and organs for the treatment of patients with injury or disease. Institute scientists were the first in the world to engineer a replacement organ in the laboratory that was successfully implanted in patients. The Institute is based at Wake Forest Baptist Medical Center, a fully integrated academic medical center located in Winston-Salem, North Carolina. The institution comprises the medical education and research components of Wake Forest School of Medicine, the integrated clinical structure and consumer brand Wake Forest Baptist Health, which includes North Carolina Baptist Hospital and Brenner Children's Hospital, the commercialization of research discoveries through the Piedmont Triad Research Park, as well a network of affiliated community based hospitals, physician practices, outpatient services and other medical facilities.\nKaren Richardson | EurekAlert!\nResearch shows black plastics could create renewable energy\n17.07.2019 | Swansea University\nA new material for the battery of the future, made in UCLouvain\n17.07.2019 | Université catholique de Louvain\nAdjusting the thermal conductivity of materials is one of the challenges nanoscience is currently facing. Together with colleagues from the Netherlands and Spain, researchers from the University of Basel have shown that the atomic vibrations that determine heat generation in nanowires can be controlled through the arrangement of atoms alone. The scientists will publish the results shortly in the journal Nano Letters.\nIn the electronics and computer industry, components are becoming ever smaller and more powerful. However, there are problems with the heat generation. It is...\nScientists have visualised the electronic structure in a microelectronic device for the first time, opening up opportunities for finely-tuned high performance electronic devices.\nPhysicists from the University of Warwick and the University of Washington have developed a technique to measure the energy and momentum of electrons in...\nScientists at the University Würzburg and University Hospital of Würzburg found that megakaryocytes act as “bouncers” and thus modulate bone marrow niche properties and cell migration dynamics. The study was published in July in the Journal “Haematologica”.\nHematopoiesis is the process of forming blood cells, which occurs predominantly in the bone marrow. The bone marrow produces all types of blood cells: red...\nFor some phenomena in quantum many-body physics several competing theories exist. But which of them describes a quantum phenomenon best? A team of researchers from the Technical University of Munich (TUM) and Harvard University in the United States has now successfully deployed artificial neural networks for image analysis of quantum systems.\nIs that a dog or a cat? Such a classification is a prime example of machine learning: artificial neural networks can be trained to analyze images by looking...\nAn international research group led by scientists from the University of Bayreuth has produced a previously unknown material: Rhenium nitride pernitride. Thanks to combining properties that were previously considered incompatible, it looks set to become highly attractive for technological applications. Indeed, it is a super-hard metallic conductor that can withstand extremely high pressures like a diamond. A process now developed in Bayreuth opens up the possibility of producing rhenium nitride pernitride and other technologically interesting materials in sufficiently large quantity for their properties characterisation. The new findings are presented in \"Nature Communications\".\nThe possibility of finding a compound that was metallically conductive, super-hard, and ultra-incompressible was long considered unlikely in science. It was...\n24.06.2019 | Event News\n29.04.2019 | Event News\n17.04.2019 | Event News\n19.07.2019 | Physics and Astronomy\n19.07.2019 | Physics and Astronomy\n19.07.2019 | Earth Sciences"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:328decf0-7577-43ef-bacd-30eb0f0dcab0>","<urn:uuid:75f67b3d-2780-435c-9834-ee8a15a58018>"],"error":null}
{"question":"In the past, ¿why was una small standard deviation importante for aircraft landing distances compared to normal probability distribution? ⚠️","answer":"A small standard deviation was important for aircraft landing distances because it ensured consistent and accurate landing distances, which was crucial for safety. If landing distances varied too much (had a large standard deviation), aircraft could either undershoot the runway (land short) or overshoot it (not stop before the end), potentially resulting in injuries or fatalities. This differs from normal probability distribution, which assumes that large samples will naturally create a bell-shaped curve with values spread out from the mean, as the goal with landing distances was specifically to maintain consistency rather than natural variation.","context":["This week, we are covering an important concept that is widely used in a variety of fields: the standard deviation. The standard deviation is a measure of how close together a set of data is grouped. Kozak (2014) stated:\n“In general a “small” standard deviation means the data is close together (more consistent) and a “large” standard deviation means the data is spread out (less consistent). Sometimes you want consistent data and sometimes you don’t. As an example if you are making bolts, you want to lengths to be very consistent so you want a small standard deviation. If you are administering a test to see who can be a pilot, you want a large standard deviation so you can tell who are the good pilots and who are the bad ones.” (p. 89).\nExample Situations Involving Pilots\nA large standard deviation tells us that there is a lot of variability in the scores; that is, the distribution of scores is spread out and not clustered around the mean. As Kozak (2104) has stated, when assessing potential pilots, we may want a large standard deviation, so that we can differentiate between candidates; that is, we can determine who would be good pilots and those who would not be good pilots. Since a large standard deviation tells us that there is a lot of variability in the scores, candidates, who would be good pilots, would have scores far above the mean, while those who would not be good pilots would have scores far below the mean. Thus, only the “top” candidates would be selected to be pilots.\nA small standard deviation tells us that there is not a lot of variability in a distribution of scores; that is, the scores are very consistent (similar) and close to the mean. Using our pilot example, a small standard deviation is desirable, when considering aircraft landing distances. If there is not enough distance when landing, the aircraft could undershoot the runway; that is, land short of the runway. On the other hand, if there is too much distance, the aircraft can overshoot the runway; that is, the aircraft does not stop before the end of the runway. Both undershooting and overshooting runways could result in injuries and/or fatalities. Thus, it is important that pilots have consistent accurate landing distances. Specifically, over a number of landings, the distances would, on average, be appropriate to the runway length with little variation among the distances.Now think about how this might apply in your chosen field and answer both of the following questions:\nWhat is an example of when you would want consistent data and, therefore, a small standard deviation?\nWhat is an example of when you might want a large standard deviation? That is, data that is more spread out?\nDelivering a high-quality product at a reasonable price is not enough anymore.\nThat’s why we have developed 5 beneficial guarantees that will make your experience with our service enjoyable, easy, and safe.\nYou have to be 100% sure of the quality of your product to give a money-back guarantee. This describes us perfectly. Make sure that this guarantee is totally transparent.Read more\nEach paper is composed from scratch, according to your instructions. It is then checked by our plagiarism-detection software. There is no gap where plagiarism could squeeze in.Read more\nThanks to our free revisions, there is no way for you to be unsatisfied. We will work on your paper until you are completely happy with the result.Read more\nYour email is safe, as we store it according to international data protection rules. Your bank details are secure, as we use only reliable payment systems.Read more\nBy sending us your money, you buy the service we provide. Check out our terms and conditions if you prefer business talks to be laid out in official language.Read more","What is Normal Probability Distribution?\nIn order to qualify as a standard normal probability distribution, the distribution must meet certain requirements, such as having a mean of zero. Any random variable that has a normal distribution will follow certain rules which can be used to calculate the standard deviation or probability of a specific outcome. Normal probability distribution is an incredibly useful statistical tool.\nBell Shaped Curve\nWhen random variables with a normal distribution are graphed, they result in a bell shaped curve. The bell shaped curve shows that the farther a value is from the mean, the less likely it is to occur. The lowest values of x have a low probability, but that probability increases as the value of x increases. The probability peaks at the mean, and descends steadily, so that the highest values of x are incredibly unlikely as well. The bell shaped curve is a mirror image, so that the right side of the mean looks identical to the left.\nEverything is Normal\nIn statistics, there is the assumption that if the sample is large enough, it will end up with a normal distribution. How large the sample should be depends on the demographics of the group and the random variable being tested. The value of the mean may change, but as long as there are a few people who are extreme, and the majority cluster around the average, the probability distribution will have a bell shaped curve. In other words, they naturally create the bell shaped curve.\nAn Example of Normality\nCollege students, as a general rule, have an above average IQ. If the IQs of the average population were graphed, it would result in a typical bell shaped curve, with most college students clustered above the mean. However, if college students were placed in their own category, their IQ scores would create a normal distribution as well. The mean IQ for college students would be higher, and the standard deviation would be smaller, but overall there would be some incredibly smart students, some students well below average, and most students would fall right in the middle.\nArea of a Bell Shaped Curve\nWhen working with a standard normal probability distribution, parts of the bell shaped curve are shaded in to indicate that they meet the requirements for a successful trial. In other words, the shaded area indicates p. The total area under the curve is said to equal one. The area also equals the combined probability of all possible results (p +q). Because of this, the probability of an event is simply equal to the shaded area. If the shaded part under the bell curve has an area equal to 0.95, the desired event has a probability of 95%.\nUsing Z Scores\nThe z score simply indicates how many standard deviations away from the mean something is. A positive z score refers to a value that is so many standard deviations above the mean, whereas a negative z score will lie below the mean. Z scores with the same number but opposite sign will be equidistant from the mean. Z scores are helpful since each z score represents a specific probability. There are tables available which compile all z scores and their corresponding probability, but many people find it easier to use computer programs such as STAQTDISK, Minitab, or Excel.\nItís assumed that a large enough sample will produce a normal probability distribution. This assumption is helpful because properties of a normal distribution help statisticians make a number of calculations. The distribution makes probability very easy to calculate and also helps to determine the standard deviation thanks to the z score."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ea87fe92-9104-46ca-918f-c067637a7e1b>","<urn:uuid:fe48d3f7-5c88-4888-9808-6d0c802c5478>"],"error":null}
{"question":"¿Cuál es el molecular weight de IL-15RA y CD72? Please compare their kDa!","answer":"IL-15RA has a calculated molecular weight of 39 kDa, while CD72 is a 39-43 kDa type II membrane glycoprotein.","context":["|Application ||WB, E|\n|Other Accession||NP_001243694, 377520151|\n|Reactivity||Human, Mouse, Rat|\n|Calculated MW||39 kDa|\n|Application Notes||IL-15RA antibody can be used for detection of IL-15RA by Western blot at 1 - 2 µg/mL.|\n|Target/Specificity||IL15RA; Multiple isoforms of IL-15RA are known to exist.|\n|Reconstitution & Storage||IL-15RA antibody can be stored at 4℃ for three months and -20℃, stable for up to one year.|\n|Precautions||IL-15RA Antibody is for research use only and not for use in diagnostic or therapeutic procedures.|\n|Function||High-affinity receptor for interleukin-15. Can signal both in cis and trans where IL15R from one subset of cells presents IL15 to neighboring IL2RG-expressing cells. Expression of different isoforms may alter or interfere with signal transduction. Isoform 5, isoform 6, isoform 7 and isoform 8 do not bind IL15. Signal transduction involves SYK.|\n|Cellular Location||Membrane; Single-pass type I membrane protein Nucleus membrane; Single-pass type I membrane protein. Note=Mainly found associated with the nuclear membrane Isoform 6: Endoplasmic reticulum membrane; Single-pass type I membrane protein. Golgi apparatus membrane; Single-pass type I membrane protein. Cytoplasmic vesicle membrane; Single-pass type I membrane protein. Membrane; Single-pass type I membrane protein. Note=Isoform 5, isoform 6, isoform 7 and isoform 8 are associated with endoplasmic reticulum, Golgi and cytoplasmic vesicles, but not with the nuclear membrane Isoform 8: Endoplasmic reticulum membrane; Single-pass type I membrane protein. Golgi apparatus membrane; Single-pass type I membrane protein. Cytoplasmic vesicle membrane; Single-pass type I membrane protein. Membrane; Single-pass type I membrane protein. Note=Isoform 5, isoform 6, isoform 7 and isoform 8 are associated with endoplasmic reticulum, Golgi and cytoplasmic vesicles, but not with the nuclear membrane|\n|Tissue Location||Isoform 1, isoform 3, isoform 4, isoform 5, isoform 6, isoform 7, isoform 8 and isoform 9 are widely expressed. Expressed in fetal brain with higher expression in the hippocampus and cerebellum than in cortex and thalamus. Higher levels of soluble sIL-15RA form in comparison with membrane-bound forms is present in all brain structures|\nThousands of laboratories across the world have published research that depended on the performance of antibodies from Abgent to advance their research. Check out links to articles that cite our products in major peer-reviewed journals, organized by research category.\nfirstname.lastname@example.org, and receive a free \"I Love Antibodies\" mug.\nProvided below are standard protocols that you may find useful for product applications.\nIL-15RA Antibody: Interleukin 15 (IL-15RA) is a cytokine receptor that specifically binds IL-15 with high affinity. IL-15 regulates T and natural killer cell activation and proliferation. The IL-15 and IL-2 receptors share two subunits, IL-2R beta and IL-2R gamma, and IL-15RA is structurally related to IL-2RA, an additional IL-2-specific alpha subunit necessary for high affinity IL-2 binding. Unlike IL-2RA, IL-15RA is capable of binding IL-15 with high affinity independent of the other subunits. IL-15RA is thought to enhance cell proliferation and expression of apoptosis inhibitor Bcl-xL and Bcl-2.\nGiri JG, Kumaki S, Ahdieh M, et al. Identification and cloning of a novel IL-15 binding protein that is structurally related to the alpha chain of the IL-2 receptor. EMBO J. 1995; 14:3654-63.\nBodnar A, Nizsaloczki E, Mocsar G, et al. A biophysical approach to IL-2 and IL-15 receptor function: localization, conformation and interactions. Immunol. Lett. 2008; 116:117-25.\nLorenz HM, Hieronymus T, Grunke M, et al. Differential role for IL-2 and IL-15 in the inhibition of apoptosis in short-term activated human lymphocytes. Scand. J. Immunol. 1997; 45:660-9.\nIf you have used an Abgent product and would like to share how it has performed, please click on the \"Submit Review\" button and provide the requested information. Our staff will examine and post your review and contact you if needed.\nIf you have any additional inquiries please email technical services at email@example.com.","- 3F3 (See other available formats)\n- VI CD72.1\n- Other Names\n- Lyb-2, Ly-19.2, Ly-32.2\n- Mouse IgG2b, κ\n- Ave. Rating\n- Submit a Review\n- Product Citations\n|Cat #||Size||Price||Quantity Avail.||Save|\nCD72 is a 39-43 kD type II membrane glycoprotein. It is a disulfide-linked homodimer belonging to C-type lectin family. CD72 is a pan-B cell marker expressed on pre-pre-B cells throughout B cell differentiation with the exception of plasma cells. It is also expressed on follicular dendritic cells, splenic red pulp macrophages (but not on peripheral blood monocytes), and liver Kupffer cells. CD72, a negative coreceptor of B cells, contains immunoreceptor tyrosine-based inhibitory motifs in the cytoplasmic domain which has been shown to recruit the tyrosine phosphatase SHP-1. Ligation of CD72 with its ligand regulates CD72 tyrosine dephosphorylation and SHP-1 dissociation to promote B cell activation and proliferation. CD100 and CD5 have been shown to be CD72 ligands. The CD100-CD72 interaction plays a role in maintenance of B cell homeostasis.Product Details\n- Antibody Type\n- Host Species\n- Human lymphocytes\n- Phosphate-buffered solution, pH 7.2, containing 0.09% sodium azide and 1 mM EDTA.\n- The antibody was purified by chromatography and conjugated with TotalSeq™-A oligomer under optimal conditions. The solution is free of unconjugated DNA and unconjugated antibody.\n- 0.5 mg/ml\n- Storage & Handling\n- The antibody solution should be stored undiluted between 2°C and 8°C. Do not freeze.\nPG - Quality tested\n- Recommended Usage\nEach lot of this antibody is quality control tested by immunofluorescent staining with flow cytometric analysis and the oligomer sequence is confirmed by sequencing. For Proteogenomics TotalSeq™-A analysis, the suggested use of this reagent is ≤ 1.0 µg per million cells in 100 µl volume. It is recommended that the reagent be titrated for optimal performance for each application.\nTo maximize performance, centrifuge the antibody dilution (1.0 µg of antibody in 100 µl of staining buffer for every 1 million cells) before adding to the cells at 14,000xg at 2 - 8°C for 10 minutes. Carefully pipette out the liquid avoiding the bottom of the tube and add to the cell suspension.\nBuyer is solely responsible for determining whether Buyer has all intellectual property rights that are necessary for Buyer's intended uses of the BioLegend TotalSeq™ products. For example, for any technology platform Buyer uses with TotalSeq™, it is Buyer's sole responsibility to determine whether it has all necessary third party intellectual property rights to use that platform and TotalSeq™ with that platform.\n- Application Notes\nAdditional reported applications (for the relevant formats) include: immunoprecipitation.\n- Additional Product Notes\nTotalSeq™ reagents are designed to profile protein levels at a single cell level following an optimized protocol similar to the CITE-seq workflow. A compatible single cell device (e.g. 10x Genomics Chromium System and Reagents) and sequencer (e.g Illumina analyzers) are required. Please contact technical support for more information, or visit biolegend.com/totalseq.\nThe TotalSeq™-A barcode sequence associated with clone 3F3 is CAGTCGTGGTAGATA.\nThe flanking sequences are CCTTGGCACCCGAGAATTCCA, and the poly A tail, BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA*A*A, where B represents either C, G, or T, and * indicates a phosphorothioated bond, to prevent nuclease degradation.\nThe full oligomer sequence for this product, with the specific barcode in brackets is CCTTGGCACCCGAGAATTCCA [CAGTCGTGGTAGATA]BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA*A*A.\n(PubMed link indicates BioLegend citation)\n- Knapp W, et al. Eds. 1989. Leucocyte Typing IV. Oxford University Press. New York.\nAB_2783189 (BioLegend Cat. No. 316205)\n- Type II membrane glycoprotein, C-type lectin family, 39-43 kD, homodimer\nPre-pre-B cells throughout B cells differentiation (but not on plasma cells), follicular dendritic cells, splenic red pulp macrophages, liver Kupffer cells\n- negative regulation of B cell receptor signaling\n- Ligand Receptor\n- CD5, CD100/Sema4D\n- Cell Type\n- B cells, Dendritic cells\n- Biology Area\n- Molecular Family\n- CD Molecules\n- Antigen References\n1. Knapp W, et al. Eds. 1989. Leucocyte Typing IV. Oxford University Press. New York.\n2. Schwarting T, et al. 1992. Am. J. Hematol. 41:151.\n3. Wu HJ and S. Bondata. 2002. Immunol. Res. 25:155.\n4. Kumanogoh A, et al. 2000. Immunity 13:621.\n5. Parnes JR and C. Pan. 2000. Immunol. Rev. 176:75.\n6. Kumanogoh A, et al. 2005. Intnl. Immunol. 17:1277.\n- Gene ID\n- 971 View all products for this Gene ID\n- View information about CD72 on UniProt.org\nCompare Data Across All Formats\nThis data display is provided for general comparisons between formats.\nYour actual data may vary due to variations in samples, target cells, instruments and their settings, staining conditions, and other factors.\nIf you need assistance with selecting the best format contact our expert technical support team."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:2140a573-5b80-444b-b389-df03a6a893b4>","<urn:uuid:2d741be3-58c8-4e11-98f7-851c189c9129>"],"error":null}
{"question":"How can teachers effectively monitor and track students' progress in primary school? Need some concrete tips! 🤔","answer":"Teachers can effectively monitor pupil progress through several methods: conducting regular observations of children at work, using formative assessment, and maintaining close monitoring of pupil progress. Additionally, teachers should focus on understanding how children learn, observe their time on task, and implement appropriate assessment strategies to track their development.","context":["Developing Advanced Primary Teaching Skills\nBy Denis Hayes\nRoutledge – 2012 – 232 pages\nDo you believe that continuous improvement in teaching is essential?\nDo you wish to enhance your understanding of how children learn?\nAre you eager to become a well-informed professional?\nFrom the author of the hugely respected Foundations of Primary Teaching, this advanced textbook explores the essential elements of teaching and learning and the process of becoming a caring and competent teacher. It introduces a wide range of education issues, challenges and requirements with the intention of promoting advanced classroom practice, both for individuals and within teams. The book offers insights, ideas, hints and thought-provoking education topics for individual reflection and team discussion.\nWith a focus on understanding the teaching and learning processes and the factors that impact upon providing a high quality education for every pupil, this book discusses in detail key learning skills, dilemmas and challenges for primary teachers and themes in continuing professional development. It covers issues in teaching and learning including:\nIncluding action points, hints and challenges, this book will be of interest to trainee teachers, postgraduates, experienced qualified teachers, deputy head teachers and head teachers who wish to be more consistently effective and make a positive impact on the lives of children in their primary classroom.\n'This has been a most useful and inspiring book. As a classroom teacher, I have found it to be superb for refreshing my thought about teaching, challenging me and developing my practice'. Peter Sainsbury, Deputy Head, Winterbourne Earls Primary C of E School\nIntroduction Prelude Change in Education Excellence in teaching Effective communication Meaningful connection with pupils Closely monitoring pupil progress Skilful pedagogy SECTION ONE. Key Learning Skills… 1.Understanding key concepts 2. Learning across subject boundaries 3. Developing social skills 4. Working collaboratively 5. Criteria for sorting and classifying 6. Planning and sequencing 7. Recognising cause and effect 8. Adapting to different circumstances 9. Speaking confidently 10. Being imaginative and creative 11. Making valid judgements 12. Developing an enquiring mind SECTION TWO. Teaching and Learning Issues… Part A. Child Development and Education 1. The nature-nurture debate 2. The impact of pre-school education 3. Motivation and pupil background 4. Developing parental involvement 5. Adults as effective role models 6. Emotional development 7. Emotional intelligence, children and learning 8. Children’s happiness 9. Children’s moral development 10. Alternative forms of education Part B. Pupil learning 1. Learning and concept mapping 2. Cognition and Bloom’s Taxonomy 3. Motivation and Maslow’s Hierarchy 4. Pupil learning styles 5. Learning through auditory means 6. Questions and higher level thinking 7. Pupil ability and capability 8. Gifted and talented pupils and accelerated learning 9. Memory and learning difficulties 10. Television as a learning tool Part C. Classroom practice 21. Ability grouping 22. Alternative education and Steiner-Waldorf 23. Maintaining discipline 24. The use of rewards and incentives 25. Pupil time on task 26. Boys and girls in school 27. Raising boys’ achievement 28. Use of formative assessment 29. The value of speaking and listening 30. Learning to read Part D. The teacher 31. Teachers as educational agents 32. Gender, teachers and teaching 33. The consequences of labelling children 34. Implementation of the ECM agenda 35. Attitudes towards computers 36. Reflective and self-reflexive practice 37. Shaping professional development 38. Persevering as a teacher 39. Being observed teaching 40. Coping with setbacks as a teacher SECTION THREE. Professional Themes Education purpose and policy 1. The purpose of education 2. Education policy decisions Children Understanding children 4. Pupil attendance in school 5. Developing a child’s sense of belonging 6. Developing positive attitudes in children 7. Adult responses to children 8. Pupils’ moral perspectives during primary school 9. Labelling children with special educational needs Child development 10. Child development and learning 11. Forms of intelligence 12. Forms of knowledge 13. Combining knowledge, skills and understanding 14. Working atmosphere and learning climate 15. The impact of an objectives-driven agenda 16. Personalised and individualised learning 17. Competitiveness between pupils 18. Benefits and disadvantages of homework 19. Parental involvement in learning Teachers and teaching 20. Attributes of good teachers 21. Teachers’ beliefs about teaching and learning 22. Observing children at work 23. Questions asked by teachers 24. New teachers’ rite of passage 25. Accountability and autonomy in teaching 26. Differentiating for learning 27. Effective public speaking 28. Fostering creativity in teaching and learning 29. Reflective practice and teaching 30. Teaching methods and cooperative learning 31. Teachers’ duty of care 32. Professional learning communities 33. Achieving peace of mind as a teacher Discipline and behaviour 34. Pupil compliance 35. Dealing with indiscipline 36. Coping with restless pupils 37. Managing disruptive behaviour Curriculum Practice 38. Negotiated meaning for effective learning 39. Effective learning environments 40. Targets for learning 41. Encouraging children to write 42. The educational value of play and drama 43. Organising large space lessons 44. Transition from primary to secondary school SECTION FOUR. Challenges for Teachers… 1: Maintaining a calm working environment 2: Protecting your own wellbeing 3: Fitting it all in 4: Starting with a new class 5: Achieving a balanced teaching approach 6: Teaching mixed-ability, multi-age classes POSTSCRIPT. Letter to a newly qualified teacher\nDenis Hayes is an education writer and speaker. He was Professor of Primary Education at the Faculty of Education, University of Plymouth after spending seventeen years teaching across primary, middle and secondary schools. He has authored Foundations of Primary Teaching, The Guided Reader to Teaching and Learning and Encyclopedia of Primary Education, all published by Routledge."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:99171d2e-0bd2-4669-804e-e02d4249e08f>"],"error":null}
{"question":"What are the primary industries that utilize bare metal servers for secure data handling and processing?","answer":"The primary industries using bare metal servers for secure data handling are fintech for financial documents, healthcare institutions for medical records, and e-commerce sites, social media platforms, and streaming services for handling confidential data such as credit card numbers and social security details.","context":["Bare metal servers present a powerful solution for individuals or businesses seeking unmatched performance and reliability. These servers—unlike shared hosting services—are exclusively allocated to a single user or tenant, providing dedicated resources for tasks that require top-tier performance, customization, and dependability. In this in-depth exploration of bare metal servers, we’ll explore their function, benefits, and potential applications, helping you decide whether this robust computing solution is the right fit for your business, project, or client needs.\nA bare metal server is a physical server that is accessed via the cloud. It offers a unique degree of control to a single user over the equipment, unlike shared services. When using a bare metal server, the client has complete management over the hardware resources and server’s load. This results in the following key features:\nThese are just a few of bare metal’s key features. In the next section, let’s take a closer look at how a bare metal server works and explore its benefits in more detail.\nBare metal servers operate by granting direct access to the underlying physical hardware; they eliminate unnecessary layers and complexities regarding server access, resulting in a more efficient system. Here’s a breakdown of a bare metal server’s operation.\nUsers have the flexibility to choose their preferred operating system (OS) and install it directly on the bare metal server. This eliminates the need for virtualization software and provides greater control and customization options. Additionally, keeping the operating system up to date is crucial for optimal server performance as it helps protect data and defend against external threats.\nResource allocation on a bare metal server gives users full control over computing resources. After the operating system is installed, users can customize the allocation of CPU cores, assign the desired amount of RAM, and determine the storage capacity based on their specific requirements.\nThis level of control enables optimized utilization of server resources, allowing users to allocate the right amount of computing power, memory, and storage to different applications and workloads. Users can ensure efficient performance and responsiveness for their specific tasks by tailoring the resource allocation to their needs, maximizing the server’s capabilities.\nThe process of hardware provisioning involves assigning and configuring specific physical components of a bare metal server, such as the CPU, RAM, storage, and network interfaces, for exclusive use by the server. This ensures that the user has complete access to the allocated resources and can make the most of them.\nIf you own a website, you’re probably familiar with shared hosting servers, where users share resources to save costs. However, there are downsides. Shared hosting is limited in terms of performance and reliability because resources are distributed among multiple users, leading to potential slowdowns during peak usage or when other users’ needs increase. As such, businesses or individuals with resource-intensive applications, growing traffic, or high-performance requirements might encounter performance problems when using shared hosting.\nEnter bare metal servers. A bare metal server is a physical server that is exclusively allocated to a single user or tenant. You have your own private space, where you can enjoy all the resources and benefits of having an entire server just for yourself. Unlike shared hosting servers, bare metal servers are not shared with other users, providing dedicated resources for enhanced performance and reliability—this means that bare metal is a serious upgrade for serious users. Bare metal is perfect for tasks that need top-notch performance, reliability, and customization.\nWhile there are similarities between bare metal and dedicated servers—such as being dedicated to a single user and not sharing resources with other users—bare metal servers use more advanced technology in terms of hardware. The main difference lies in how they operate. A bare metal server offers direct access to the hardware and allows for greater customization, while a dedicated server operates within a virtualized environment, sacrificing performance for the benefits of scalability and resource sharing. Dedicated servers can be challenging to operate. On the other hand, bare metal servers operate seamlessly with the assistance of a software called a hypervisor.\nThis means that a bare metal server is a significantly more powerful solution compared to a dedicated server. A dedicated server can get the job done for certain use cases, but it is worth noting that a bare metal server gives you complete control. It offers superior hardware and greater flexibility for customization to suit your particular requirements to your business.\nBare metal servers provide flexible usage options to meet diverse needs. However, the question arises: Who should consider using a bare metal server? Is it suitable for everyone? To help you find the answer, we have compiled a list of who can benefit from using bare metal servers with real examples from across industries.\nIf you’re a business owner and your online service requires significant processing power and memory to handle demanding workloads like high-traffic websites, gaming servers, or video streaming platforms, it is worth considering using bare metal servers. These servers are designed to provide the necessary resources and performance capabilities to support such intensive tasks.\nBased on the Digital 2023: Global Overview Report, it is observed that the latest findings show that popular streaming services like Netflix and Disney+ have gained significant traction, capturing over 45% of the total TV viewing time among internet users of working age. This represents an increase of 10% since Q3 2019, indicating that users are now spending more than 1.5 hours per day on streaming services and online TV.\nIn order to meet the rising expectations of users and stay competitive in the media industry, content providers need to ensure fast and high-quality content delivery. This is particularly important when facing competition from established platforms like Netflix. To achieve this, having an infrastructure with sufficient bandwidth is crucial; this is where bare metal servers come into play. These servers offer the capability to deliver content at high speeds, ensuring a seamless and enjoyable user experience. Gcore offers an exclusive infrastructure tailored for streaming video, which can be combined with Gcore Bare Metal for a top-of-the-line streaming solution.\nIn the high-stakes world of fintech, bare metal servers provide the robust security measures required for the safe handling and storage of sensitive financial documents. Similarly, healthcare institutions such as hospitals can use the advanced security features of bare metal servers to protect vital medical records. E-commerce sites, social media platforms, and streaming services continuously handle large volumes of confidential data, such as credit card numbers or social security details.\nTo ensure the protection of sensitive data, bare metal is a wise choice. Some bare metal server providers, like Gcore, offer built-in defenses against common security threats like DDoS attacks. It’s important to examine these security features available in bare metal servers. We’ll get into this in more detail in the next section.\nBy utilizing bare metal servers, gaming companies and their developers can meet the performance demands of modern games, ensuring fast processing speeds and low latency. This is especially crucial for AAA games that require high-performance capabilities.\nAdditionally, in multiplayer gaming, low latency is essential to provide a seamless and immersive experience for players. Bare metal servers contribute to low latency by reducing network latency, minimizing delays, and enhancing overall gameplay quality. Bare metal servers also help to ensure the security of user data, which is critical in the gaming industry where personal information and payment details are handled.\nWith 2023’s focus on AI, numerous companies—from tech giants to individual entrepreneurs—are jumping on board to create an impact and take advantage of this growing industry. However, these types of projects require substantial processing capabilities to handle complex algorithms and large datasets. By utilizing bare metal servers, developers can access dedicated hardware resources, such as powerful CPUs and ample RAM, which are crucial for running resource-intensive AI and machine learning workloads.\nIn addition, the dedicated nature of bare metal servers ensures enhanced data security and privacy. AI and machine learning applications often deal with sensitive data, such as customer information or proprietary datasets. Developers can isolate their workloads on bare metal and maintain complete control over security measures, reducing the risk of unauthorized access or data breaches.\nHere’s a helpful table that summarizes the various use cases of bare metal servers by industry or sector:\n|Sector/industry||Use cases||Real-life example|\n|Technology||Hosting high performance applications Virtualization platforms Big data workloads AI/Machine learning||Hosting a high-traffic website Creating an AI tool Running a data analytics platform|\n|Finance||Secure and high-performance applications Banking systems, including financial transaction processing Data management||Hosting a banking system Processing financial transactions securely|\n|Healthcare||Electronic health records Healthcare applications Data security and privacy Medical research||Hosting an electronic health record system Running medical research simulations|\n|Gaming and media entertainment||Online gaming servers Media streaming platforms Content distribution||Multiplayer game servers like Minecraft and Counter-Strike Media streaming platforms such as Netflix and Apple TV|\n|E-commerce||Hosting online platforms Managing high-traffic websites Processing secure transactions||Online marketplaces such as Amazon and Alibaba Processing customer orders securely|\n|Research and data science||Complex simulations Large dataset analysis Scientific research Machine learning training||Running scientific simulations Training machine learning models|\nIt’s important to keep in mind that the way bare metal servers are used can differ depending on specific business needs and requirements. Additionally, there may be other industries that use bare metal servers that are not included in our list.\nAfter learning about the different use cases that can benefit from using a bare metal server, you might be interested in acquiring one for either business or personal use. Let’s discuss the essential factors you should consider before purchasing.\nSuppose you’re considering using a bare metal server for your business or project. In that case, there are several necessary factors to consider in order to make an informed decision when purchasing. Let’s take a closer look at them:\nCost is a significant factor to consider when purchasing a bare metal server, and here’s why. First, you need to find a server that fits your budget. Setting a budget and exploring different pricing options allows you to choose a server that offers excellent value without breaking the bank. Secondly, considering pricing allows you to compare the costs and benefits of different service providers. You can evaluate features, performance, and support to find the best deal for your budget.\nTaking pricing and your budget into account helps you plan for the future. You can budget for the initial purchase and any ongoing expenses, like maintenance and future upgrades. You should make a purchase confident that you can afford the server and any additional services you need—not only now, but for the foreseeable future.\nUnderstanding the resources available on a server helps you make sure that it can handle your specific capacity and power requirements. Different applications and workloads have different demands for CPU, RAM, storage and networking capabilities.\nPerformance is vital in delivering a smooth and great experience for your apps and services, but purchasing a bare metal server is a significant investment. That is why it becomes crucial to carefully assess which resources you actually need to achieve your performance goals, and purchase accordingly. By carefully evaluating the resources and performance of a bare metal server, you can ensure that it aligns with your specific needs, delivers the desired performance level, and provides the scalability required for future growth.\nIt is essential to take note of the service provider’s support and maintenance services. Reliable support means that any issues or technical difficulties can be promptly addressed and resolved, minimizing downtime and disruptions to your operations. Regular maintenance and updates are also essential to keep your server running smoothly and securely, protecting your data and ensuring optimal performance. So, choosing a provider that offers comprehensive support and maintenance services can provide you with peace of mind and minimize potential risks and challenges associated with server management.\nWith all of these considerations in mind, let’s explore how Gcore’s bare metal server can benefit businesses and various sectors with its reliable bare metal service.\nAt Gcore, we recognize the importance of balancing affordability, effectiveness, security, and high performance when it comes to servers. That’s why our clients can benefit from our range of cloud services, including our bare metal servers. We prioritize value for money, performance, and top-of-the-line support to ensure a seamless experience for your business.\nLet’s take a look at the advantages of Gcore Bare Metal:\nLearn more about the features, benefits and pricing about Gcore Bare Metal.\nA bare metal server is a powerful and versatile hosting solution for businesses and individuals seeking high performance, security, and control over their infrastructure. Its use cases span various industries, including online services, media streaming, gaming, and AI. When buying bare metal servers, it’s crucial to take into account the cost, necessary resources, and available support.\nHere at Gcore, we understand the importance of bridging affordability, security, and high performance. With our bare metal server offering, we prioritize cost-efficiency, performance, and reliable support. Our global platform ensures protection against DDoS attacks and provides a production-ready environment with guaranteed uptime and 24/7 support.\nInterested in trying out Gcore bare metal service for your business or big project? Want to learn more about how bare metal works? Talk to our experts today. We’re ready to help and hear from you!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:24821e27-4130-4ae4-8500-28cb85396aaf>"],"error":null}
{"question":"How does synthetic aperture radar achieve high resolution imaging? レーダーによる高解像度撮影はどのように達成されますか？","answer":"Synthetic Aperture Radar achieves high resolution through several mechanisms. It operates either simultaneously or sequentially, using focused array resolution. The system employs motion compensation and range-gate drifting techniques. It operates in multiple modes including real-beam mapping, strip mapping, and spotlighting. The process requires specific waveform restrictions and processing throughputs. The system can implement synthetic aperture 'monopulse' concepts. Unlike optical imaging, it creates synthetic aperture rather than relying on real aperture, overcoming real beam limitations.","context":["Radar Systems Design and Engineering Training\n|Commitment||4 days, 7-8 hours a day.|\n|User Ratings||Average User Rating 4.8 See what learners said|\n|Delivery Options||Instructor-Led Onsite, Online, and Classroom Live|\nThis Radar Systems Design and Engineering Training covers radar functionality, architecture, and performance. Fundamental radar issues such as transmitter stability, antenna pattern, clutter, jamming, propagation, target cross section, dynamic range, receiver noise, receiver architecture, waveforms, processing, and target detection are treated in detail within the unifying context of the radar range equation and examined within the contexts of surface and airborne radar platforms and their respective applications.\nAdvanced topics such as pulse compression, electronically steered arrays, and active phased arrays are covered, together with the related issues of failure compensation and autocalibration. The fundamentals of multi-target tracking principles are covered, and detailed examples of surface and airborne radars are presented. This Radar Systems Design and Engineering Training course is designed for engineers and engineering managers who wish to understand how surface and airborne radar systems work, and to familiarize themselves with pertinent design issues and the current technological frontiers.\nRadar Systems Design and Engineering Training cover the following topics:\n- Radar Systems Design and Engineering: Introduction. Radar systems examples\n- Radar Systems Design and Engineering: CW Radar, Doppler, and Receiver Architecture\n- Radar Systems Design and Engineering: Radio Waves Propagation\n- Radar Systems Design and Engineering: Radar Clutter and Detection in Clutter\n- Radar Systems Design and Engineering: Electronically Scanned Radar Systems\n- And More…\n- 4 days of Radar Systems Design and Engineering Training with an expert instructor\n- Radar Systems Design and Engineering Training Course Guide\n- Certificate of Completion\n- 100% Satisfaction Guarantee\n- Radar Systems Design and Engineering Training – https://www.wiley.com/\n- Radar Systems Design and Engineering Training – https://www.packtpub.com/\n- Radar Systems Design and Engineering Training – https://store.logicaloperations.com/\n- Radar Systems Design and Engineering Training – https://us.artechhouse.com/\n- Radar Systems Design and Engineering Training – https://www.amazon.com/\nUpon completing this Radar Systems Design and Engineering course, learners will be able to meet these objectives:\n- What are radar subsystems?\n- How to calculate radar performance.\n- Key functions, issues, and requirements.\n- How different requirements make radars different.\n- Operating in different modes & environments.\n- ESA and AESA radars: what are these technologies, how do they work, what drives them, and what new issues do they bring?\n- Issues unique to multifunction, phased array, and radars.\n- State-of-the-art waveforms and waveform processing.\n- How airborne radars differ from surface radars.\n- Today’s requirements, technologies & designs.\n- We can adapt this Radar Systems Design and Engineering Training course to your group’s background and work requirements at little to no added cost.\n- If you are familiar with some aspects of this Radar Systems Design and Engineering course, we can omit or shorten their discussion.\n- We can adjust the emphasis placed on the various topics or build the Radar Systems Design and Engineering Training around the mix of technologies of interest to you (including technologies other than those included in this outline).\n- If your background is nontechnical, we can exclude the more technical topics, include the topics that may be of special interest to you (e.g., as a manager or policy-maker), and present the Radar Systems Design and Engineering course in a manner understandable to lay audiences.\nThe target audience for this Radar Systems Design and Engineering course:\n- Technical managers\n- Logistics and support\nRadar Systems Design and Engineering Training\nThe knowledge and skills that a learner must have before attending this Radar Systems Design and Engineering course are:\n- Basic technical knowledge\nPART I: RADAR AND PHENOMENOLOGY FUNDAMENTALS\n- Introduction. Radar systems examples. Radar ranging principles, frequencies, architecture, measurements, displays, and parameters. Radar range equation; radar waveforms; antenna patterns, types, and parameters.\n- Noise in Receiving Systems and Detection Principles. Noise sources; statistical properties. Radar range equation; false alarm and detection probability; and pulse integration schemes. Radar cross-section; stealth; fluctuating targets; stochastic models; detection of fluctuating targets.\n- CW Radar, Doppler, and Receiver Architecture. Basic properties; CW and high PRF relationships; dynamic range, stability; isolation requirements, techniques, and devices; superheterodyne receivers; in-phase and quadrature receivers; signal spectrum; spectral broadening; matched filtering; Doppler filtering; Spectral modulation; CW ranging; and measurement accuracy. Radar Systems Design and Engineering Training\n- Radio Waves Propagation. The pattern propagation factor; interference (multipath,) and diffraction; refraction; standard refractivity; the 4/3 Earth approximation; sub-refractivity; super refractivity; trapping; propagation ducts; littoral propagation; propagation modeling; attenuation.\n- Radar Clutter and Detection in Clutter. Volume, surface, and discrete clutter, deleterious clutter effects on radar performance, clutter characteristics, effects of platform velocity, distributed sea clutter, and sea spikes, terrain clutter, grazing angle vs. depression angle characterization, volume clutter, birds, Constant False Alarm Rate (CFAR) thresholding, editing CFAR, and Clutter Maps.\nPART 2: CLUTTER PROCESSING, WAVEFORM, AND WAVEFORM PROCESSING\n- Clutter Filtering Principles. Signal-to-clutter ratio; signal and clutter separation techniques; range and Doppler techniques; principles of filtering; transmitter stability and filtering; pulse Doppler and MTI; MTD; blind speeds and blind ranges; staggered MTI; analog and digital filtering; notch shaping; gains and losses. Performance measures: clutter attenuation, improvement factor, subcluster visibility, and cancellation ratio. Improvement factor limitation sources; stability noise sources; composite errors; types of MTI.\n- Radar Waveforms. The time-bandwidth concept. Pulse compression; Performance measures; Code families; Matched and mismatched filters. Optimal codes and code families: multiple constraints. Performance in the time and frequency domains; Mismatched filters and their applications; Orthogonal and quasi-orthogonal codes; Multiple-Input- Multiple-Output (MIMO) radar; MIMO waveforms and MIMO antenna patterns. Radar Systems Design and Engineering Training\nPART 3: ESA, AESA, AND RELATED TOPICS\n- Electronically Scanned Radar Systems. Fundamental concepts, directivity and gain, elements and arrays, near and far field radiation, element factor and array factor, illumination function and Fourier transform relations, beamwidth approximations, array tapers, and sidelobes, electrical dimension, and errors, array bandwidth, steering mechanisms, grating lobes, phase monopulse, beam broadening, examples.\n- Active Phased Array Radar Systems. What are solid-state active arrays (SSAA), what advantages do they provide, emerging requirements that call for SSAA (or AESA), SSAA issues at T/R module, array, and system levels, digital arrays, and future direction?\n- Multiple Simultaneous Beams. Why multiple beams, independently steered beams vs. clustered beams, alternative organization of clustered beams and their implications, quantization lobes in clustered beams arrangements, and design options to mitigate them.\nPART 4: APPLICATIONS\n- Surface Radar. Principal functions and characteristics, nearness and extent of clutter, effects of anomalous propagation, the stressing factors of dynamic range, signal stability, time, and coverage requirements, transportation requirements and their implications, sensitivity time control in classical radar, the increasing role of bird/angel clutter and its effects on radar design, firm track initiation and the scan-back mechanism, antenna pattern techniques used to obtain partial relief.\n- Airborne Radar. Frequency selection; Platform motion effects; iso-ranges and iso-Dopplers; antenna pattern effects; clutter; reflection point; altitude line. The role of medium and high PRFs in lookdown modes; the three PRF regimes; range and Doppler ambiguities; velocity search modes, TACCAR and DPCA.)\n- Synthetic Aperture Radar. Principles of high resolution, radar vs. optical imaging, real vs. synthetic aperture, real beam limitations, simultaneous vs. sequential operation, derivations of focused array resolution, unfocused arrays, motion compensation, range-gate drifting, synthetic aperture modes: real-beam mapping, strip mapping, and spotlighting, waveform restrictions, processing throughputs, synthetic aperture ‘monopulse’ concepts.\n- Multiple Target Tracking. Definition of Basic terms. Track Initiation: Methodology for initiating new tracks; Recursive and batch algorithms; Sizing of gates for track initiation. M out of N processing. State Estimation & Filtering: Basic filtering theory. Least-squares filter and Kalman filter. Adaptive filtering and multiple model methods. Use of suboptimal filters such as table look-up and constant gain. Correlation & Association: Correlation tests and gates; Association algorithms; Probabilistic data association and multiple hypothesis algorithms."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2dcd9187-4e5e-45a1-b201-c410952b8a47>"],"error":null}
{"question":"What's the difference in how Arctic sea ice decline affects phytoplankton versus multi-year ice patterns? Could you please provide examples of these changes?","answer":"The declining Arctic sea ice affects phytoplankton and multi-year ice patterns differently. For phytoplankton, earlier ice melt disrupts their life cycle - they normally grow under the ice each spring as sunlight increases, but the earlier melting disturbs this pattern, resulting in less food for the entire Arctic food chain. Regarding multi-year ice, there has been a dramatic decline - the proportion of Arctic winter ice that is multi-year ice has dropped from around 70% in the 1980s to about 20% today. This is significant because multi-year ice has a higher albedo (reflects more heat) than single-year ice, contributing to further warming and ice loss.","context":["Whales and Changing Sea Ice\nWhat Scientists Think\n1978, scientists have used satellites to track sea ice and other factors\nin the Arctic. This long-term data has let them observe dramatic changes\nthey could not have seen in a short time period. Here are some of their\nfindings and theories.\nSea Ice is Melting!\n- In the\npast few years, Arctic sea ice has melted unusually early in the spring.\nIn fact, in the 2004-2005 winter season, it began to melt earlier than\nin any other winter on record. In the gray whale's feeding and breeding\ngrounds in the Bering sea, it's melting about three weeks earlier than\nice coverage and thickness has dropped steadily during the last few\ndecades. Now the minimum cover is the lowest ever measured\nsince sea ice cover records were kept.\nthe past few decades, annual average temperatures in the Arctic have\nincreased at almost twice the rate as in the rest of the world.\nice is light-colored, so it reflects most of the sun's energy\n(heat) back into space. But as the sea ice melts, the darker water absorbs\nmuch of this energy. This warming water melts the ice even faster!\nsay that global climate change is the main cause of\nthe warming of the Bering Sea. (Another is weaker cold north winds that\nblow across the sea.)\nactivity, such as burning fossil fuels, is a key cause of global\nwarming. It releases tons of carbon dioxide and other \"greenhouse\ngases\" into the air. These cause the Earth to trap more of the\nsun's warmth than it otherwise would.\nCould it Affect Gray Whales?\nnorth Bering Sea is one of the world's richest feeding grounds for whales,\nwalruses, and sea birds. Scientists only know some of the ways\nin which climate change affects marine life. After all, it's a complex system.\nEverything is linked to everything else! Here are some of their observations.\nWhat other ideas did you come up with?\neach photo to enlarge.\nAn adult gray whale will swallow about 77 tons of food\nin the Arctic. That's a LOT of amphipods, which are only a few\nfood available — Microscopic sea plants called phytoplankton\nare the starting link in the Arctic sea food\nchain. Each spring as sunlight hours increase, they start to make food\nand grow under the ice. They grow so fast that they crowd each other\nout by June. As they suffocate and die, they fall to the muddy bottom.\nThis creates a nutrient-rich food layer. Bottom-dwelling animals such\nas amphipods — the main food for gray\nwhales — feed on that layer. But as ice melts earlier and water\nwarms, the phytoplankton's life cycle is disrupted. The result? Less\nfood for every animal in the food chain!\ncompetition — Near-freezing water in\nthe northern Bering Sea once kept bottom-feeding fish like halibut and\nflounder farther south in warmer waters. But as sea floor temperatures\nhave risen, the fish have moved farther north. There they compete for\nfood with larger animals, such as walrus, sea ducks, and whales.\nmoving north — Researchers say that\ngray whales are adjusting to the changes by heading further north. There\nthey feed in colder waters of the Chukchi Sea. But those shallow waters\nonly go so far! Once whales get to the deep waters off the continental\nshelf, they won't be able to find food. (Other animals, such as bearded\nseals, may not be able to adjust enough to survive.)\nthe gray whales shift northward, they move near the territory of the\nbowhead whale, which feeds offshore. Alaskan natives hunt and eat the\nbowhead. They are concerned that the more aggressive gray whale may\ninterfere with the quieter bowhead. Hunters are noticing\nthat many gray whales are sticking around all winter rather\nthan making the journey south!\nDeeper: You Be the Judge!\nRead these articles. As you do, highlight, underline, or keep lists of\nof changes in sea ice and climate\nabout why these changes are happening (use one color for facts and another\nabout how these changes affect gray whales (use one color for facts\nand another for theories/opinions)\nopinion on what you think we could do to slow down these changes.","New data confirms Arctic ice trends: sea ice being lost at a rate of five days per decade\n4 March 2014\nThe melt season across the Arctic is getting longer by five days per decade, according to new research from a team including Prof Julienne Stroeve (Professor of Polar Observation and Modelling at UCL Earth Sciences). New analysis of satellite data shows the Arctic Ocean absorbing ever more of the sun’s energy in summer, leading to an ever later appearance of sea ice in the autumn. In some regions, autumn freeze-up is occurring up to 11 days per decade later than it used to.\nThe research, published in a forthcoming issue of the journal Geophysical Research Letters, has implications for tracking climate change, as well as having practical applications for shipping and the resource industry in the arctic regions.\n“The extent of sea ice in the Arctic has been declining for the last four decades,” says Julienne Stroeve, “and the timing of when melt begins and ends has a large impact on the amount if ice lost each summer. With the Arctic region becoming more accessible for long periods of time, there is a growing need for improved prediction of when the ice retreats and reforms in winter .”\nWhile temperatures have been increasing during all calendar months, trends in melt onset are considerably smaller than that of autumn freeze-up. Nevertheless, the timing of melt onset strongly influences how much of the Sun’s energy gets absorbed by the ice and sea. This in turn is affected by how reflective the surface is. Highly reflective surfaces, such as ice, are said to have a high albedo, as they reflect most of the incoming heat back into space. Less reflective surfaces like liquid water have a low albedo, and absorb most of the heat that is directed at them.\nThis means that even a small change in the extent of sea ice in spring can lead to vastly more heat being absorbed over the summer, leading to substantially later onset of ice in the autumn. There is also a second effect, in that multi-year ice (which survives through the summer without melting) has a higher albedo than single-year ice that only covers the sea in winter. Since the 1980s, the proportion of the Arctic winter ice that is made up of multi-year ice has dropped from around 70% to about 20% today, so the changes are quite substantial\nThese feedback effects, in which small changes in atmospheric temperature and sea ice lead to large changes in heat absorption, was what the team set out to study.\nStroeve’s team analysed satellite imagery of the Arctic region, dating back over 30 years. The data breaks down the whole region into 25x25km squares, and the team analysed the albedo of each of these for each month for which they had data. This allowed them to update trends and add an extra 6 years onto the most recent analysis of its kind. The new data continues the trend towards longer ice-free periods previously observed.\n“The headline figure of five days per decade hides a lot of variability. From year to year, the onset and freeze-up of sea ice can vary by about a week. There are also strong variations in the total length of the melt season from region to region: up to 13 days per decade in the Chukchi Sea, while in one, the Sea of Okhotsk, the melt season is actually getting shorter.”\nThe amounts of energy involved in these changes are enormous – hundreds of megajoules of extra energy accumulated in every square metre of sea. This is equivalent to several times the energy released by the atom bomb at Hiroshima for every square kilometre of the Arctic ocean.\nFor organisations such as oil drillers operating in the Arctic region, a sophisticated understanding of when the sea will freeze up is essential. For climate scientists, this type of study helps them better understand the feedback mechanisms inherent in the Arctic climate. The results from this study are closely in line with previous work and therefore give added confidence that models of the complex Arctic climate are broadly correct.\nCorrection: The first paragraph of this article initially referred to the 'ice-free season' rather than the 'melt season'. This error has now been corrected.\n- The research appears in a paper entitled \"Changes in Arctic melt season and implications for sea ice loss\", to be published in a forthcoming issue of the journal Geophysical Research Letters. An online pre-print is available now on the journal website.\n- Julienne Stroeve is a recent appointment to UCL Earth Sciences, joining the department from the National Snow and Ice Data Centre in Colorado, USA.\n- Article in Geophysical Research Letters\n- UCL Earth Sciences\n- National Snow & Ice Data Center\n- Centre for Polar Observation and Modelling\nSatellite view of sea ice\nThis image may be reproduced freely as it is in the public domain\nMap of changing melt seasons in the Arctic\nThis image is available under a Creative Commons Attribution Non-commercial No-derivatives licence. It may additionally be reproduced for the purpose of news reporting.\nUCL Earth Sciences and National Snow & Ice Data Center\nUCL Faculty of Mathematical and Physical Sciences\n020 7679 7964"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:faa60553-4650-4b3c-b1ad-9f6a096cf546>","<urn:uuid:4e5fe65b-7034-402d-9d2d-0a09720497c5>"],"error":null}
{"question":"What are the main functions of tree roots in different species, and how do they vary based on environmental conditions? Compare root systems like taproots vs horizontal roots.","answer":"Tree roots perform both anchoring and absorbing functions, with distinct patterns varying by species and environment. Some species like pine have deep-penetrating taproots, while others like spruce have platelike systems that spread horizontally just below the surface. In wet sites, particularly in the tropics, trees may develop buttress roots that are thickened vertically, sometimes seen on elm trees as well. Additionally, some species like spruce and poplar can develop adventitious roots that sprout from the lower trunk in response to burial by peat, silt on river floodplains, or windblown sand.","context":["Values People have always been impressed by trees, by their massiveness and majesty, by the sound of wind in their branches, and by their visual beauty. Legend and folklore suggest attitudes of awe and reverence: YEWS, symbols of eternity; BIRCHES, holy trees; LARCHES, guardians against enchantment.\nTreesTrees are single-stemmed, perennial, woody plants taller than 3 m and exceeding 8 cm in diameter at breast height; shrubs are multistemmed and smaller. These definitions are somewhat arbitrary, since many species (eg, willow, alder, cherry, maple) can grow as trees or shrubs, depending on the environment. Counting the 30-odd shrubs that assume tree form under favourable conditions, there are about 140 native Canadian trees. The largest and oldest grow in the Pacific temperate rain FOREST. Douglas fir is an imposing example and, although it does not reach the size of redwoods or the age of bristlecone pines, specimens 90 m tall, 5 m in basal diameter and older than 1000 years have been reported. Canada's tallest reported tree (95 m) is a Stika spruce, the \"Carmanah Giant\" in the Carmanah Valley on the west coast of Vancouver Island. Canada's tallest Douglas fir, 94.3 m in height and 8.07 m in circumference, stands near Coquitlam, BC.\nPeople have always been impressed by trees, by their massiveness and majesty, by the sound of wind in their branches, and by their visual beauty. Legend and folklore suggest attitudes of awe and reverence: YEWS, symbols of eternity; BIRCHES, holy trees; LARCHES, guardians against enchantment. The Greeks gave trees spirits (dryads), attributing religious significance to them, as did the druids, who conferred on forest groves and oak woods a sacred, precommercial value now, unfortunately, lost.\nToday, trees are valued for their products: pulpwood, sawtimber, poles, plywood, particle board, paper, cork, rubber, gums, tannin, pharmaceuticals, fruits, nuts and syrups. Indirect benefits include soil stabilization and prevention of EROSION, windbreaks, sound barriers and air purification.\nApart from a few large, single-stemmed FERNS, trees are classified as gymnosperms and angiosperms. Gymnosperms (CONIFERS), with scalelike or needlelike leaves, appear first in the FOSSIL record (Carboniferous period, 353-300 million years ago) and, by early Mesozoic times (Triassic, 250-205.7 million years ago), dominated Earth's vegetation. Later in the Mesozoic, during the Cretaceous period (144.2-65 million years ago), broadleaf angiosperms evolved to become the more important group, perhaps profiting from their close relationships with INSECT pollinators and with larger animals which spread their fruits. Angiosperms also developed the ability to reproduce vegetatively by sprouting, an advantage which is shared by few conifers.\nConifers tend to concentrate growth in a central trunk from which many small branches are offset, producing a conical crown. They are usually evergreen, an adaptation fitting them for difficult environments by allowing internal recycling of nutrients from old to new foliage. Broadleaf trees tend to have rounded crowns because side branches grow just as well as main stems, which may fork repeatedly. They are typically deciduous, and grow on more fertile soils and in more moderate climates. There are numerous exceptions: some conifers (eg, larches, CYPRESSES, dawn redwood) are deciduous; some pines have relatively hard wood; some broadleaf trees (eg, POPLARS) have soft wood; others are evergreen, especially in subtropical and tropical climates. The only native Canadian broadleaf evergreen is the red-barked arbutus of southwestern BC.\nTree roots perform both anchoring and absorbing functions. Like the tops, they are distinctive according to species and environment. They may penetrate deeply (taproots of PINE) or spread horizontally just below the surface (platelike system of SPRUCE). Buttress roots, thickened vertically, characterize wet sites, particularly in the tropics, although they are sometimes seen on ELM. Adventitious roots, sprouting from the lower trunk (as on spruce and poplar), are a response to burial by accumulations of peat, silt on river floodplains, or windblown sand.\nIn climates that vary seasonally, the \"growth ring\" is a characteristic anatomical feature of trees. Regenerative, meristematic cells (cambium and cork cambium) sheathe the living trunk, branches and roots just under the bark, annually forming layers of phloem and corky cells to the outside (bark) and xylem cells to the inside (wood). Both bark and wood thicken with age. Products of photosynthesis and various other biochemicals are transported by phloem; water is transported chiefly by active xylem in sapwood surrounding older, darker, nonfunctional heartwood.\nThe water-conducting efficiency of xylem cells is a function of their size, controlled by growth regulators released from the tree's growing tips. When shoot growth begins in spring, cambium produces large-diameter cells. Later, in summer, as growth slows and stops, wood-cell diameters decrease. Therefore, a cross-section of trunk, root or branch shows concentric \"growth rings\" outlined by the contrast between the small, dense, latewood cells of one year and larger, lighter earlywood cells of the next. An uncritical count of a tree's growth rings may overestimate its true age, since extra flushes of growth in a year can be triggered by weather changes or defoliation, forming false annual rings.\nRing size reflects growing conditions. Where precipitation is the limiting factor, sensitive trees record wet and dry years in wide and narrow rings. Where heat is limiting, ring sizes mirror sequences of warm and cold summers.\nTrees were eliminated by ice-age GLACIERS which covered most of Canada. Deglaciation began about 18 000 years ago, allowing the migration of plants to newly exposed soils. Species with small, winged seeds travelled fastest and farthest, and the boreal zone was filled by spruce, pine, larch, FIR, poplar and birch. Trees that migrated more slowly or were less stress tolerant came later, and they now characterize more favourable environments: southern BC, with numerous conifers; and southern Ontario, with an even greater variety of broadleaf deciduous trees.\nAltitudinal and latitudinal distributions are in part related to wood anatomy: small-diameter xylem cells of boreal species (spruce, fir, ASPEN, ALDER, WILLOW, birch) are less prone to freezing damage than larger cells of \"ring porous\" southern hardwoods (OAK, hickory, WALNUT).\nEach wide-ranging tree species includes locally adapted varieties. Although they seem similar, white spruces from the territories, Newfoundland and southern Manitoba are genetically different and respond differently when planted together. Successful planting is best accomplished, therefore, by using seed from trees native to the area. This variation within species means that the preservation of native trees, as well as that of other plants and animals, in all their genetic diversity, requires the protection of many large, widely distributed, natural areas as ecological preserves.\nSee also individual tree entries."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d0dcabe5-b955-4906-a2ff-bab89be208dc>"],"error":null}
{"question":"What's the size difference between the existing LHC tunnel and the cancelled Superconducting Super Collider tunnel?","answer":"The Large Hadron Collider (LHC) has a 27 km circumference tunnel, while the partially constructed Superconducting Super Collider (SSC) tunnel in Texas was designed to be significantly larger at 87 km in circumference. About 46% of the SSC tunnel was already bored before the project was cancelled.","context":["Mark Thiessen/National Geographic Society\nWhen it comes to shutting down the most powerful atom smasher ever built, it’s not simply a question of pressing the off switch.\nIn the French-Swiss countryside on the far side of Geneva, staff at the Cern particle physics laboratory are taking steps to wind down the Large Hadron Collider. After the latest run of experiments ends next month, the huge superconducting magnets that line the LHC’s 27km-long tunnel must be warmed up, slowly and gently, from -271 Celsius to room temperature. Only then can engineers descend into the tunnel to begin their work.\nThe machine that last year helped scientists snare the elusive Higgs boson – or a convincing subatomic impostor – faces a two-year shutdown while engineers perform repairs that are needed for the collider to ramp up to its maximum energy in 2015 and beyond. The work will beef up electrical connections in the machine that were identified as weak spots after an incident four years ago that knocked the collider out for more than a year…\nThe particle accelerator, which reveals new physics at work by crashing together the innards of atoms at close to the speed of light, fills a circular, subterranean tunnel a staggering eight kilometres in diameter. Physicists will not sit around idle while the collider is down. There is far more to know about the new Higgs-like particle, and clues to its identity are probably hidden in the piles of raw data the scientists have already gathered, but have had too little time to analyse.\nBut the LHC was always more than a Higgs hunting machine. There are other mysteries of the universe that it may shed light on. What is the dark matter that clumps invisibly around galaxies? Why are we made of matter, and not antimatter? And why is gravity such a weak force in nature? “We’re only a tiny way into the LHC programme,” says Pippa Wells, a physicist who works on the LHC’s 7,000-tonne Atlas detector. “There’s a long way to go yet…”\nThe search for dark matter on Earth has failed to reveal what it is made of, but the LHC may be able to make the substance. If the particles that constitute it are light enough, they could be thrown out from the collisions inside the LHC. While they would zip through the collider’s detectors unseen, they would carry energy and momentum with them. Scientists could then infer their creation by totting up the energy and momentum of all the particles produced in a collision, and looking for signs of the missing energy and momentum.\nOne theory, called supersymmetry, proposes that the universe is made from twice as many varieties of particles as we now understand. The lightest of these particles is a candidate for dark matter…\nAnother big mystery the Large Hadron Collider may help crack is why we are made of matter instead of antimatter. The big bang should have flung equal amounts of matter and antimatter into the early universe, but today almost all we see is made of matter. What happened at the dawn of time to give matter the upper hand?\nThe question is central to the work of scientists on the LHCb detector. Collisions inside LHCb produce vast numbers of particles called beauty quarks, and their antimatter counterparts, both of which were common in the aftermath of the big bang. Through studying their behaviour, scientists hope to understand why nature seems to prefer matter over antimatter…\nExtra dimensions may separate us from realms of space we are completely oblivious to. “There could be a whole universe full of galaxies and stars and civilisations and newspapers that we didn’t know about,” says Parker. “That would be a big deal.”\nRead the whole article. Set your imagination free into science that moves faster than the speed of light.","By Michael Banks\nFollowing the closure of Fermilab’s 1 TeV Tevatron particle collider near Chicago in 2011 – and with no similar facility being planned to replace it in the US – many physicists in the country felt not surprisingly concerned that America was losing its place at the “energy frontier”. That baton had already passed to the CERN particle-physics lab near Geneva when its Large Hadron Collider (LHC) fired up in 2008, and with collisions set to restart there next year at 13 TeV, the US’s day looked certain to have passed.\nIndeed, as we first reported three weeks ago, researchers meeting in Geneva last week discussed plans that would keep Europe at the energy frontier for decades to come with options for an LHC successor – a machine that would be even bigger and bolder that the 27 km-circumference LHC.\nOne new design tabled at the conference would involve creating a huge 80–100 km tunnel near Geneva that would house a new collider to study the Higgs boson in great detail. In the future, this tunnel could then be used to search for new particles by colliding protons at 100 TeV – much greater than the LHC’s 13 TeV.\nHowever, a group of US physicists from Texas A&M University and Michigan State University is now proposing to wrestle back the energy frontier by constructing a huge accelerator in the US.\nIn a paper posted on the arXiv preprint server today, the researchers outline plans to use the partly constructed tunnel of the axed Superconducting Super Collider (SSC) just outside Dallas, Texas. Conceived in 1983, the SSC was to be the next big particle collider with a circumference of 87 km and a maximum collision energy of 40 TeV. But 10 years later the all-American project was cancelled, largely on grounds of cost, leaving a few buildings on the surface as well as tens of kilometres of tunnels deep underground.\nMost of the cost of a new collider would be in excavating the tunnel, but the researchers claim that around 46% of the SSC tunnel has been already bored and some facilities built, such as the linear accelerator that feeds particles into the collider. This would then make it much cheaper than the CERN proposal.\nThe physicists point out that if the SSC tunnel were finished off, it could be home to a 240 GeV “Higgs factory” that would collide electrons with positrons to study the new boson in unprecedented detail.\nBut their plans don’t stop there. The researchers say that given its “soft consolidated cretaceous rock”, the site in Texas is an “ideal medium for large-bore tunnelling”. This same location could be home to a 270 km-circumference particle collider that could then host a 100 TeV proton–proton machine. The SSC tunnel would be used as an injector into the new 270 km tunnel. The authors add that in future the collider could even be upgraded to a 300 TeV machine.\nThere’s nothing like thinking big!\nBut if the plans do go ahead, there may be some clearing up to do first. During the 2011 March Meeting of the American Physical Society in Dallas, a group of “rogue” physicists (see image above) took a break from the gruelling conference schedule to break into the SSC’s derelict site. They found that the tunnels are well below the water table and are therefore flooded, while many unopened crates containing electronic equipment are just lying around.\nYet the authors seem serious about their plans and have submitted the document to a subpanel of Department of Energy’s High Energy Physics Advisory Panel, and are planning to discuss the proposal at a workshop at Fermilab in July."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:3829f9fc-f647-417d-93a1-b33d4a3d510b>","<urn:uuid:61f675e8-bbe4-4131-98a4-a3340c152dd7>"],"error":null}
{"question":"As someone studying Ohio's legal history: What's the main difference between how State v. Swift and State v. Tenace changed the state's legal landscape in their respective areas? Can anyone compare these landmark cases?","answer":"State v. Swift and State v. Tenace made significantly different changes to Ohio's legal landscape. Swift established procedural requirements for collecting court fines, requiring specific hearings before and after incarceration for non-payment, with mandatory steps including allowing offenders to present evidence about their ability to pay and requiring an immediate hearing on the first court day following arrest. In contrast, State v. Tenace represented a groundbreaking shift in death penalty jurisprudence - it was the first time in over 200 death penalty decisions that the Ohio Supreme Court reversed a death sentence based on a defendant's troubled background alone, expanding beyond previous considerations that were limited to cases involving serious mental illness.","context":["The Cost of Crime\nPatching Ohio’s leaky court-fine collections\nBy William Tinch(1)\nA judge I clerked for in Franklin, Ohio, asked me to research an issue that has long frustrated city officials. Every week, people are summoned to court for various reasons. No matter the purpose, once judgment is levied, guilty parties are usually ordered to pay a fine and court costs at minimum. Reasonable people recognize that court orders are not optional.\nSome people, however, do not pay and get away with it, and several factors make it extremely difficult to force them to reach for their wallets, according to the Ohio Revised Code (O.R.C.) and Ohio case law.\nIn State v. Twitty,(2) Yolanda Twitty was incarcerated for not paying $271 in fines. Twitty touches on many issues implicated by O.R.C. § 2947.14.(3) It particularly focuses on “jail-time credit” toward satisfying various fines.(4) Section A of the statute defines what type of fine can be substituted for jail time. The referenced fine is only imposed as a sentence or part of a sentence; it does not include court costs. The Twitty court explained the difference in treatment.\nThe purposes of incarceration and the imposition of a fine are both to punish the offender. Therefore, it is logical to provide for the substitution of one for the other. The imposition of court costs is to reimburse the State for its expenses, not to punish the offender. Therefore, jail time is not a proper substitute for the payment of costs. Incarceration does not reduce or ameliorate the State’s expenses, but instead imposes an additional financial burden upon the State, in the form of the expenses of the additional incarceration.(5)\nA magistrate can only order the offender to jail or workhouse(6) if he determines the offender has the “ability to pay,” but refuses.(7) Finally, the statute says that the hearing on whether the offender has the ability to pay “shall be conducted at the time of sentencing.”(8)\nThe Twitty court held that it is the trial court’s duty to keep “separate accounting of the court costs and fine amounts and payments, and to credit jail-time to the fine portion . . . [to satisfy any court costs] the trial court must rely upon methods of collection for civil judgments.”(9) Traditional collection methods for civil judgments vary—filing liens against real property, judicial orders to garnish wages or seize property, turning judgments over to collections agencies, etc.\nSo what does it mean to have the “ability to pay?” The offender has the right to counsel, testify, and present evidence of her ability to pay. If the magistrate finds that the offender has the ability to pay, he has to support his findings by facts in the judgment entry, which considers the offender’s income, assets, and debts, as presented by the offender.(10) This reliance on the offender’s testimony seems absurd; fortunately, a 2010 Ohio Supreme Court decision limited that reliance.\nIn State v. Plummer,(11) appellant Jeremy Plummer was indicted in five separate cases between October 2008 and March 2009, and on April 4, 2009, he entered five guilty pleas.(12) In May 2009, following a sentencing hearing and its review of a pre-sentence investigation (PSI) report, the trial court sentenced Plummer to 30 months in prison and ordered him to pay court costs and $8,560 in restitution.\nThe Plummer court discussed other ways to determine one’s ability to pay. The court emphasized that a trial court’s use of PSIs in making this determination can be quite useful, as PSIs often contain personal and financial information about the offender.(13) In Plummer’s case, the PSI did not detail his assets, but it did contain his age, education, family status, health, drug history, and prior employment.(14) Thus, the court derived Plummer’s ability to pay from several factors: he was 23 at his sentencing, he left school in 12th grade, he was in good health, and he had made $9-10 per hour. He would also be 26 upon his prison release. Nothing in the record indicated he would be unable to obtain work after his release.(15)\nThe hearing process to determine one’s ability to pay is cumbersome. In State v. Swift,(16) the appellant, Mark Swift, failed to appear at his first arraignment, and the trial court then issued several warrants for Swift’s arrest due to his continued non-payment of fines and court costs.(17) O.R.C. § 2947.14(C) requires that certain steps be taken before and after the offender’s incarceration for failure to pay a fine.\nA full reading of the statute indicates that a hearing may be held at the initial sentencing, but is required only when a trial court decides to incarcerate the offender, according to the Swift court.(18) At the hearing, the offender may be represented by counsel and present evidence concerning his ability to pay the fine.(19) A court’s determination of the offender’s ability to pay must be based on evidence of his income, assets, and debt, “as presented by the offender.”(20) The court may issue an arrest warrant if the offender does not pay after the hearing. “The court must (emphasis added) then afford the offender another hearing ‘on the first regularly scheduled court day following the arrest,’ unless waived by the offender.”(21)\nEven if the court follows the cumbersome collection procedure, additional hurdles often exacerbate the situation. Franklin prosecutor Steve Runge summarized the problem: “If someone out of town is arrested and assessed fines and court costs but fails to pay them, it will often cause more time and money—on the part of the City—than the amount owed by the offender.”\nThe sentencing court is left with two options: It can take an officer off his beat to retrieve the offender or it can ask the city in which the offender resides to get him. The sentencing court is reluctant to remove an officer from his beat to retrieve a non-paying offender because of the time and money involved. Complicating matters, the offender’s home city, which did not issue the citation, often does not wish to be involved for the same reasons.\nTypically, the statute of limitations passes, enabling the offender to avoid paying court costs and fines. To prevent this injustice, courts have to make a concerted effort to hold the initial hearing provided in the statute and to incorporate PSI reports in evaluating the offender’s ability to pay.\nThese actions will not absolutely guarantee collection of unpaid fines and court costs, but their implementation significantly increases that likelihood. Incarceration and enforcing civil judgments are expensive and tedious, but justice requires their implementation; otherwise, the double standard will persist, and court orders will only apply to conscientious citizens.\n(1) Tinch, Class of 2011, clerked for Franklin, Ohio, Municipal Court Judge\nRupert E. Ruppert in summer 2010.\n(2) State v. Twitty, Montgomery App. No 23080, 2009-Ohio-5600.\n(3) Hearing on ability to pay fine.\n(4) Twitty at P 28.\n(5) Id. at P 35.\n(6) A house of correction.\n(7) O.R.C. § 2947.14(A).\n(9) Twitty at P 36.\n(10) O.R.C. § 2947.14(B).\n(11) State v. Plummer, 2010-Ohio-849.\n(12) Plummer at P 2, 3.\n(13) Id. at P 33, 34.\n(14) Id. at P 36.\n(16) State v. Swift, Montgomery App. No 20543, 2005-Ohio-1595.\n(17) Swift at P 1, 2, 3, 4, 5, 6.\n(18) Id. at P 17.\n(19) Id. at P 19.","Ohio ACLU Wins State Supreme Court Ruling on Death Penalty Eligibility\nFOR IMMEDIATE RELEASE\nDefendants with Horrific Backgrounds May Be Exempt from Execution\nCOLUMBUS, OH – The American Civil Liberties Union of Ohio today welcomed a landmark ruling by the Ohio Supreme Court saying that a jury cannot discount the background of a defendant when it decides whether to use the death penalty. In the case of State v. Tenace, the Court for the first time reversed a death sentence based on the extraordinarily troubled life of the defendant.\n“This is a first,” said ACLU of Ohio Legal Director Jeffrey Gamso. “Never before, in well over two hundred death penalty decisions, has the court acknowledged what the law requires: that a defendant’s history, character, and background can be so horrific that a death sentence cannot fairly be imposed.”\nAccording to the United States Constitution and Ohio’s death penalty statute, if a death penalty defendant has experienced an extraordinarily damaging childhood, these facts must be considered during sentencing. The State Supreme Court upheld that standard in yesterday’s opinion by Justice Judith Ann Lanzinger.\nPreviously, the Ohio Supreme Court had limited that law, applying it only to cases in which the defendant suffered from serious mental illnesses. The court’s decision yesterday recognized that the law demanded more, and that traumatic childhood alone could justify a sentence of life in prison rather than the death penalty.\n“The court did not try to excuse the actions of Troy Matthew Tenace, but took into consideration the truly horrific life he had,” Gamso noted. “As a child, he was taught by his parents to use drugs and commit crimes. He was a victim of terrible abuse. His brother and sister are both in prison.”\nIn her opinion, Justice Lanzinger called Tenace “doomed.” Another Justice noted that if there was ever a case in which the childhood of a defendant should be considered during sentencing, this case was it.\n“Hopefully, this opens the door for others who have been failed by society to receive some sort of recognition for the tremendous obstacles in their life,” Gamso said. “This is not about excusing or minimizing their crimes, but showing compassion and fairness and recognizing that we are the products of our pasts.”\nGamso, who has represented Tenace since 1994, was joined by Dayton attorney Gary Crim in arguing the case before the Ohio Supreme Court.\nEvery month, you'll receive regular roundups of the most important civil rights and civil liberties developments. Remember: a well-informed citizenry is the best defense against tyranny.\nThe latest in Smart Justice\nThe American Civil Liberties Union is a nonprofit organization whose mission is to defend and preserve the individual rights and liberties guaranteed to every person in this country by the Constitution and laws of the United States of America.\nLearn More About Smart Justice\nThe ACLU Campaign for Smart Justice is an unprecedented, multiyear effort to reduce the U.S. jail and prison population by 50% and to challenge racism in the criminal legal system."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:835dec7a-5bb8-4ed8-885c-e6efb90ddce0>","<urn:uuid:e8e918f9-fbe5-4a3b-8325-ddfe7f5f26e9>"],"error":null}
{"question":"Hola, estoy investigando sobre el compuesto globosuxanthone A. ¿Qué actividad biológica presenta contra células cancerosas?","answer":"Globosuxanthone A (compound 3) showed cytotoxic activity against two human cancer cell lines: it inhibited cell proliferation in HCT-15 (colon adenocarcinoma) with an IC50 value of 10.7 μM and Jurkat cells (T-cell leukemia) with an IC50 value of 2.3 μM.","context":["- freely available\nMar. Drugs 2012, 10(12), 2691-2697; doi:10.3390/md10122691\nPublished: 27 November 2012\nAbstract: 1-Hydroxy-10-methoxy-dibenz[b, e]oxepin-6,11-dione (1) was obtained from the culture broth of a marine-derived fungus, Beauveria bassiana TPU942, isolated from a marine sponge collected at Iriomote Island in Okinawa, together with two known compounds, chrysazin (2) and globosuxanthone A (3). The structure of 1 was elucidated on the basis of its spectroscopic data (HREIMS, 1D and 2D NMR experiments including 1H–1H COSY, HMQC and HMBC spectra). Dibenz[b, e]oxepines are rare in nature, and only six natural products have been reported. Therefore, compound 1 is the seventh natural product in this class. Compounds 2 and 3 showed an antifungal activity against Candida albicans, and 3 inhibited the cell growth against two human cancer cell lines, HCT-15 (colon) and Jurkat (T-cell lymphoma). Compound 1 did not show an apparent activity in the same bioassays.\nMicroorganisms from marine environments, especially marine-derived fungus, have proven to be an attractive source of new bioactive secondary metabolites [1,2,3,4,5]. Many of the metabolites possess unique chemical structures and interesting biological activities, such as cytotoxicity against cancer cell lines, antimicrobial activity, inhibition of several enzyme activities and so on [1,2,3,4,5].\nIn the course of our studies on bioactive components produced by marine-derived fungi isolated from tropical and sub-tropical coral reefs, we found that the EtOAc extract of the culture broth from a marine-derived fungus, Beauveria bassiana TPU942, isolated from a marine sponge collected at Iriomote Island, Okinawa Prefecture, Japan, showed cytotoxicity against a human T-cell lymphoma Jurkat cells. Bioassay-guided separation from the EtOAc extract led to the isolation of a new dibenz[b, e]oxepine derivative (1) and two known anthraquinone and xanthone derivatives, chrysazin (2) and globosuxanthone A (3). The structure of compound 1 was elucidated as 1-hydroxy-10-methoxy-dibenz[b, e]oxepin-6,11-dione (Figure 1) by the analysis of its spectroscopic data. Natural products possessing a dibenz[b, e]oxepine structure are quite rare, and only six compounds have been reported as natural products in scientific journals [6,7,8,9]. Antifungal and cytotoxic activities were exhibited by two known compounds (2 and 3).\nWe describe herein the isolation and structure elucidation of compound 1 and biological activities of isolated compounds.\n2. Results and Discussion\nThe producing fungus, B. bassiana TPU942, was isolated from a piece of an unidentified marine sponge collected at Iriomote Island and cultured as described in Experimental Section. The culture broth and the EtOAc extract of strain TPU942 showed cytotoxicity in the screening bioassay against Jurkat cells and was separated into seven fractions (Fr. 1–Fr. 7) using a silica gel column. Compounds 1 and 2 were isolated from Fr. 1 (eluted with 100% CHCl3) and 3 from Fr. 2 (CHCl3–MeOH = 100:1) by HPLC (ODS).\nCompound 2 was identified as chrysazin , a ubiquitous anthraquinone derivative isolated from several organisms, from the spectroscopic data. The structure of compound 3 was assigned on the basis of its spectral data and comparison of these data with those of the reported values for globosuxanthone A . Globosuxanthone A (3) was originally isolated from Chaetominum globosum, and a cytotoxicity against seven human solid tumor cell lines, accumulation of cells at either G2/M or S phase and induction of apoptosis have been described .\nCompound 1 showed a molecular ion peak at m/z 270 [M]+ in the EIMS, and the molecular formula C15H10O5, 11 degrees of unsaturation, was determined from HREIMS [m/z 270.0531 [M]+, Δ +0.3 mmu] and NMR data. IR absorptions of 1 at 1735 and 3435 cm−1 suggested the presence of carbonyl and hydroxyl groups in the molecule. The 13C NMR spectrum showed 15 resolved signals, which were classified into one oxygenated methyl, six sp2 methine, three sp2 quaternary, three oxygenated sp2 quaternary and two carbonyl carbons by the analysis of 1D and 2D NMR spectra (Table 1). The 1H NMR spectrum displayed 10 proton signals, and two signals at δ 4.03 and 12.2 were assigned as a hydroxy proton (1-OH) and methoxy protons (10-OMe), respectively (Table 1). The connectivity of carbons and protons was established by the HMQC correlations.\n|Table 1. 13C (100 MHz) and 1H NMR (400 MHz) data for compound 1 (CDCl3).|\n|No.||δC||δH (J in Hz)||HMBC|\n|1||161.8||-||1, 2, 11a|\n|2||111.0||6.82 dd (8.3, 1.0)||1, 4|\n|3||137.1||7.61 t (8.5)||1, 4a|\n|4||106.9||6.94 dd (8.3, 1.0)||2, 4a|\n|7||119.5||7.56 dd (8.3, 1.0)||6, 9, 10a|\n|8||135.0||7.77 dd (8.5, 7.1)||6, 6a|\n|9||122.7||7.33 dd (7.3, 1.0)||10, 10a|\n|1-OH||-||12.2 s||1, 2, 11a|\nThe presence of two 1,2,3-trisubstituted aromatic rings was assigned by the 1H–1H COSY and HMBC correlations of the 1H NMR signals at δ 6.82 (H-2), 7.61 (H-3), 6.94 (H-4), 7.56 (H-7), 7.77 (H-8) and 7.33 (H-9) (Figure 2). HMBC correlations were observed from 1-OH (δ 12.2) to C-1 (δ 161.8), C-2 (δ 111.0) and C-11a (δ 109.0) and from 10-OMe (δ 4.03) to C-10 (δ 169.5). Therefore, an OH group was attached at the C-1 position and an OMe group at C-10. An HMBC correlation from H-7 (δ 7.56) to C-6 (δ 156.1) and from H-8 to C-6 (4J), and the 13C chemical shift of C-4a (δ 155.8), an oxygenated sp2 carbon, revealed that C-4a and C-6a were connected via C-6 by an ester bond. Therefore, one remaining carbonyl group (δ 181.0, C-11) was inevitably attached between C-10a and C-11a [6,7,8,9]. This assignment was supported by an HMBC correlation from H-7 to C-11 (4J). Thus, the structure of compound 1 was assigned as 1-hydroxy-10-methoxy-dibenz[b, e]oxepin-6,11-dione.\nDibenz[b, e]oxepines are rare metabolites, and, thus far, only six natural products (4–9, Figure 3) have been reported [6,7,8,9]. Compound 1 is, therefore, the seventh example of this class of natural products. 13C and 1H NMR data for 1 were very similar to those of the reported values for 5 .\nCompounds 1–3 were tested for their antimicrobial activity against four test microorganisms (fungus Mucor hiemalis, yeast Candida albicans, Gram-positive bacterium Staphylococcus aureus and Gram-negative bacterium Escherichia coli) using the paper disk method and for their cytotoxicity against two human cancer cell lines (colon adenocarcinoma HCT-15 and T-cell leukemia Jurkat cells) using the MTT method  (Table 2). Compounds 2 and 3 showed inhibition zone of 7 mm against C. albicans at the concentration of 10 μg/disk. Compound 3 inhibited the cell proliferation against HCT-15 and Jurkat cells with IC50 values of 10.7 and 2.3 μM, respectively. On the other hand, compound 1 did not show apparent activity in these bioassays. Further study on biological activity of compound 1 is now in progress.\n|Table 2. Biological activities of compound 1–3.|\n|Compound||IC50 (μM)||Inhibition zone (mm)|\n|Cytotoxicity||Antimicrobial activity (10 μg/disk)|\n|HCT-15||Jurkat||S. aureus||E. coli||C. albicans||M. hiemalis|\n|1||>||30||>||30||– a||– a||– a||– a|\n|2||>||30||>||30||– a||– a||7||– a|\n|3||10.7||2.3||– a||– a||7||– a|\na Not active.\n3. Experimental Section\nEI mass spectra were obtained by a JEOL JMS-MS 700 mass spectrometer (Tokyo, Japan). 1H and 13C NMR spectra were recorded on a JEOL JNM-AL-400 NMR spectrometer (400 MHz for 1H and 100 MHz for 13C) in DMSO-d6 (δH 2.49, δC 39.5) or CDCl3 (δH 7.26, δC 77.0). UV spectra were measured on a Hitach U-3310 UV-Visible spectrophotometer (Tokyo, Japan) and IR spectra on a PerkinElmer Spectrum One Fourier transform infrared spectrometer (Waltham, MA, USA). Preparative HPLC was carried out with a Hitachi L-6200 system.\nFetal bovine serum (FBS) and other culture materials were purchased from Invitrogen (Carlsbad, CA, USA). 3-(4,5-Dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide (MTT) was purchased from Sigma-Aldrich (St. Louis, MO, USA). All other chemicals and organic solvents were purchased from Wako Pure Chemical Industries Ltd. (Osaka, Japan).\n3.3. Fermentation and Isolation\nThree pieces of an unidentified marine sponge collected in the coral reef of Iriomote Island in Okinawa, Japan were incubated on a 1/2 PDA plate (Difco Laboratories, Detroit, MI, USA). The strain TPU942 was grown from the sponge body and inoculated into a slant (1/10 YSA). The fungus was identified as Beauveria bassiana by the comparison of 217 bp ITS1 rDNA sequence (100% match).\nA slant culture of strain TPU942 grown on 1/10 YSA (0.020% yeast extract, 0.10% soluble starch, and 1.5% agar; dissolved in 90% sea water and adjusted to pH 6.0 before sterilization) was inoculated into a 500-mL Erlenmeyer flask containing 100 mL of the seed medium (2.0% glucose, 0.50% polypeptone, 0.050% MgSO4·7H2O, 0.20% yeast extract, 0.10% KH2PO4 and 0.10% agar; adjusted to pH 6.0 before sterilization). The flask was shaken reciprocally for three days at 27 °C to obtain the seed culture, which was then transferred to the production medium (3.0% sucrose, 3.0% soluble starch, 1.0% malt extract, 0.30% Ebios (Asahi Food & Healthcare Co. Ltd., Tokyo, Japan), 0.50% KH2PO4 and 0.050% MgSO4·7H2O; adjusted to pH 6.0 before sterilization). The production culture was carried out at 27 °C for seven days under the agitation condition. The seven-day-old whole broth (2.0 L) was extracted with 2.0 L of acetone. The extract was filtered and concentrated to remove acetone, and the aqueous solution was extracted with ethyl acetate. The EtOAc extract was dried over Na2SO4 and concentrated in vacuo to dryness to yield a red brown material (1088.3 mg), and the residue was suspended in CHCl3 and adsorbed on a silica gel column (100 g). The silica gel column was eluted stepwise with each 500 mL of CHCl3, a mixture (v/v) of CHCl3–CH3OH (10:1, 5:1 and 1:1) and CH3OH into seven fractions (Fr. 1–Fr. 7). An active Fr. 1 (CHCl3 eluate) was concentrated in vacuo to dryness to give a brown oil (112.9 mg). A portion (40 mg) of Fr. 1 was purified by a preparative HPLC [column; PEGASIL ODS (Senshu Scientific. Co. Ltd. Tokyo, Japan), 10 × 250 mm; solvent, 90% CH3OH; detection, UV at 254 nm; flow rate, 2.0 mL/min] to give compounds 1 (eluted at 11.0 min) and 2 (eluted at 17.8 min) as a pale yellow solid (3.0 mg) and an orange solid (24.1 mg), respectively. The second active Fr. 2 (CHCl3-CH3OH = 100:1) was concentrated to yield a brown oil (368.9 mg), and 50 mg of residue was purified by a preparative HPLC (same conditions as Fr. 1) to yield compound 3 (eluted at 7.6 min) as a white solid (4.7 mg).\n1-Hydroxy-10-methoxy-dibenz[b, e]oxepin-6,11-dione (1): obtained as a pale yellow solid; UV λmax (MeOH) nm (ε): 230 (39200), 254 (36900), 282 (10800), 300 (10200), 363 (3600); IR νmax (KBr) cm−1: 3435, 1735, 1650, 1604, 1490 1290; HREIMS (m/z) found: 270.0531, calcd: 270.0528 [M]+ for C15H10O5; 1H and 13C NMR data, see Table 1.\nChrysazin (2): obtained as an orange solid; EIMS (m/z): 240 [M]+; 1H NMR (CDCl3) δ 7.26 (2H, dd, J = 8.5, 1.2 Hz), 7.65 (2H, t, J = 8.0 Hz), 7.78 (2H, dd, J = 7.5, 1.2 Hz), 12.0 (2H, s, 2 × OH); 13C NMR (CDCl3) δ 115.8, 120.0, 124.6, 133.5, 137.2, 162.5, 181.5, 193.0.\nGlobosuxanthone A (3): obtained as a white solid; EIMS (m/z): 304 [M]+; 1H NMR (DMSO-d6) δ 4.28 (1H, d, J = 4.4 Hz), 6.49 (1H, d, J = 10.1 Hz), 6.61 (1H, dd, J = 9.9, 4.1 Hz), 6.82 (1H, d, J = 8.2 Hz), 7.07 (1H, d, J = 8.7 Hz), 7.66 (1H, t, J = 8.2 Hz), 12.4 (1H, s, OH); 13C NMR (DMSO-d6) δ 51.7, 71.4, 75.0, 107.3, 110.1, 111.2, 114.6, 119.6, 136.0, 141.3, 154.8, 159.7, 159.9, 171.6, 180.6.\n3.4. Antimicrobial Assay\nThe growth inhibitory activity was examined by the paper disk method against Mucor hiemalis IAM 6088 (fungus), Candida albicans IFM 4954 (yeast), Staphylococcus aureus IAM 12544T (Gram-positive bacterium) and Escherichia coli IAM 12119T (Gram-negative bacterium) as test microorganisms.\n3.5. Cytotoxicity Assay\nHCT-15 and Jurkat cells were obtained from the Center for Biomedical Research, Institute of Development, Aging and Cancer, Tohoku University (Miyagi, Japan). The cell lines were cultured in RPMI-1640 medium. The medium was supplemented with 10% fetal bovine serum, 100 units/mL penicillin, and 100 μg/mL streptomycin. Exponentially growing cells, cultured in a humidified chamber at 37 °C containing 5.0% CO2, were used for experiments.\nCytotoxic activity was evaluated using the colorimetric MTT assay . HCT-15 (1.0 × 104 cells in 100 μL) or Jurkat cells (2.0 × 104 cells in 100 μL) were added to each well of a 96-well plastic plate (Corning Inc., Corning, NY, USA). A sample (1.0 μL in CH3OH) was added to each well to make the final concentration from 0 to 30 μM, and the cells were incubated for 48 hr at 37 °C. MTT (10 μL of 5.5 mg/mL stock solution), and a cell lysate solution (90 μL, 40% N, N-dimethylformamide, 20% sodium dodecyl sulfate, 2.0% CH3COOH and 0.030% HCl) were added to each well and the plate was shaken thoroughly by agitation at room temperature overnight. The optical density of each well was measured at 570 nm using an MTP-500 microplate reader (Corona Electric Co., LTD., Ibaraki, Japan).\nA new 1-hydroxy-10-methoxy-dibenz[b, e]oxepine-6,11-dione (1) and two known compounds, chrysazin (2) and globosuxanthone A (3), were obtained from a marine-derived fungus, B. bassiana, strain TPU942, isolated from a marine sponge collected in Iriomote Island, Okinawa. Bioactivities observed by the extract of culture broth were reproduced by two known compounds, but compound 1 was not active in the same bioassays. Compound 1 had a rare dibenz[b, e]oxepine structure, and only six natural products have thus far been reported.\nThis work was supported in part by a Grant-in-aid for Scientific Research (21603012) from the Ministry of Education, Culture, Sports, Science, and Technology (MEXT) of Japan to MN. We are grateful to Center for Biomedical Research, Institute of Development, Aging and Cancer, Tohoku University for kindly providing human cancer cell lines. We express our thanks to T. Matsuki and S. Sato for measuring mass spectra.\n- Samples Availability: Not available.\n- Faulkner, D.J. Marine natural products. Nat. Prod. Rep. 2002, 19, 1–48. 11902436\n- Blunt, J.W.; Copp, B.R.; Munro, M.H.G.; Northcote, P.T.; Prinsep, M.R. Marine natural products. Nat. Prod. Rep. 2012, 29, 144–222, doi:10.1039/c2np00090c.\n- Robert, A.H. Marine natural products. Annu. Rep. Prog. Chem. 2012, 108, 131–146, doi:10.1039/c2oc90007f.\n- Mostafa, E.R.; Rainer, E. Secondary metabolites of fungi from marine habitats. Nat. Prod. Rep. 2011, 28, 290–344, doi:10.1039/c0np00061b. 21229157\n- Namikoshi, M. Biologically Active Natural Products from Marine Fungi. In Biomaterials from Aquatic and Terrestrial Organism; Fingerman, M., Nagabhushanam, R., Eds.; Science Publishers: Enfield, NH, USA, 2006; pp. 285–359.\n- Fujimoto, H.; Inagaki, M.; Satoh, Y.; Yoshida, E.; Yamazaki, M. Monoamine oxidase-inhibitory components from an ascomycete, Coniochaeta tetraspora. Chem. Phram. Bull. 1996, 44, 1090–1092, doi:10.1248/cpb.44.1090.\n- Carvalhoa, M.R.; de Almeida Barbosab, L.C.; de Queiroz, J.H.; Howarthc, O.W. Novel lactones from Aspergillus versicolor. Tetrahedron Lett. 2001, 42, 809–811, doi:10.1016/S0040-4039(00)02154-7.\n- Rosario, M.A.; Rodrigues, F.E.; Moitinho, M.L.; Santos, L.S. Biologically active polyketides produced by Penicillium janthinellum isolated as an endophytic fungus from fruits of Melia azedarach. J. Braz. Chem. Soc. 2005, 16, 280–283, doi:10.1590/S0103-50532005000200023.\n- Huang, H.; Li, Q.; Feng, X.; Chen, B.; Wang, J.; Liu, L.; She, Z.; Lin, Y. Structural elucidation and NMR assignments of four aromatic lactones from amangrove endophytic fungus (No. GX4-1B). Magn. Reson. Chem. 2010, 48, 496–499. 20474030\n- Muhlemann, H. Anthrachinons and anthrachinon glycosides. Pharm. Acta Helv. 1952, 27, 9–10. 14941668\n- Wijeratne, E.M.; Turbyville, T.J.; Fritz, A.; Whitesell, L.; Gunatilaka, A.A. A new dihydroxanthenone from a plant-associated strain of the fungus Chaetomium globosum demonstrates anticancer activity. Bioorg. Med. Chem. 2006, 14, 7917–7923, doi:10.1016/j.bmc.2006.07.048.\n- Mosmann, T. Rapid colorimetric assay for cellular growth and survival: Application to proliferation and cytotoxicity assays. J. Immunol. Methods 1983, 65, 55–63, doi:10.1016/0022-1759(83)90303-4.\n© 2012 by the authors; licensee MDPI, Basel, Switzerland. This article is an open-access article distributed under the terms and conditions of the Creative Commons Attribution license (http://creativecommons.org/licenses/by/3.0/)."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:250211d1-7b85-411e-8898-80449fe88d92>"],"error":null}
{"question":"How to use social media platforms for political satire in Africa today?","answer":"Social media platforms like Twitter, Facebook and YouTube have fundamentally changed how political satire works in Africa, especially in repressive contexts. These platforms are now filled with satirical writing and pictures from both professional comedians and ordinary citizens who creatively reinterpret daily politics. This represents a shift from the past, where satirists were often dependent on media editors, TV producers, and theatre managers. Social media has made it possible for many voices to act as modern-day court jesters rather than just a select few.","context":["When you write about Africa, make sure to always include sad and starving characters, advises Kenyan author Binyavanga Wainana in his famously ironic essay “How to write about Africa”, which takes aim at Western prejudices. He adds, “Avoid having the African characters laugh, or struggle to educate their kids, or just make do in mundane circumstances”.\nIn the same way that everyday laughter has been excluded from all-too-familiar depictions of the continent, African humour and satire as a form of social and political engagement remains underexplored. Although it may not always enjoy the same recognition as in other regions around the world, Africa does have a longstanding satirical tradition. The role of the praise singer, such as the imbongi in southern Africa or the griot in parts of West Africa, has been compared to that of the ancient Greek comedy playwright or the medieval court jester: a figure who entertains but also speaks truth to power by using humour to present views that would otherwise not be tolerated. Today, South African Trevor Noah’s stint as host of The Daily Show, the political cartoons of East Africa’s Godfrey Mwampembwa (otherwise known as Gado), and internationally successful Nollywood films show that satire is alive and vibrant across the continent.\nHowever, African satire has never had an easy existence. In many parts of the continent, criminal charges – or at least the threat of them – for insult and defamation are commonplace and have a negative effect on free expression. Even in South Africa, probably the continent’s most vibrant democracy, satire faces serious backlash from those in power. For example, President Jacob Zuma sued political cartoonist Zapiro for defamation, although charges were eventually dropped; the satirical news website hayibo.com had to shut down, in part due to a lack of advertising from corporates who wanted to play it safe with government; and Higher Education and Training Minister Blade Nzimande called for an “insult law” following the exhibition of a painting by local artist Brett Murray that depicted Zuma with his genitals exposed.\nJesters, at once powerful and fragile, have always been subject to punishment when the royal sense of humour failed. However, as Samm Farai Monro from Zimbabwe’s satirical Zambezi News puts it, “you can’t kill satire because you can’t kill people’s sense of humour and their sense of justice”. The popularity of Zambezi News is itself a notable example of how social media has fundamentally changed the playing field, especially in repressive political contexts. Platforms like Twitter, Facebook and YouTube explode with satirical writing and pictures as professional comedians and ordinary citizens alike creatively reinterpret the politics of the day. The court jester is not a lone figure anymore; today there are many.\nSatire may not necessarily change politics but, as most articles in this edition illustrate, it is a powerful tool to undermine propaganda, expose abuses of power, and ridicule cultural and social taboos. As with any form of power, however, satire comes with its own dangers and dark sides. As Sisonke Msimang reminds us: “At its worst … satire can degenerate into crude racism and sexism and amplify the biases of those who use it to mock others.” Her contribution, along with Rebecca Davis’ article, unearths some of satire’s internal power dynamics and hierarchies shaped by colonialism, racism and patriarchy. It is not only political rulers who control access to the public, and censorship can work in subtle ways. Prior to the social media, many satirists worked at the whim of media editors, TV producers, and theatre managers. Since opportunities, provided or denied, can decide the fate of an artist, much remains to be discovered in the hidden histories of African satire."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:80f9eb11-b2c7-4353-8ca2-067480df3a82>"],"error":null}
{"question":"How do bacteriophages and antibodies differ in their approach to fighting harmful bacteria like C. difficile and superbugs?","answer":"Bacteriophages and antibodies represent different approaches to fighting harmful bacteria. Bacteriophages are naturally occurring viruses that specifically target bacteria, as demonstrated with C. difficile where they reduce bacterial cells and toxin production without affecting other gut bacteria. In contrast, antibodies are proteins produced by the body or synthesized in laboratories that bind to specific disease-causing agents. While bacteriophages can insert their DNA into bacterial chromosomes (lysogeny), antibodies work by binding to their targets with high specificity. However, antibodies face challenges in manufacturing as they can be fragile proteins whose structure can be disrupted during production, leading to aggregation and loss of activity.","context":["Bacteriophages battle superbugs\nInstitute of Food Research microbiologists in France are reinvigorating a way of battling C. difficile infections that they hope will help overcome the growing problem of antibiotic resistant superbugs in hospitals.\nOur digestive system is home to trillions of bacteria, which are crucial to our overall health, through helping us digest food and battling potentially harmful microbes. When we take antibiotics to combat bacterial infections these beneficial bacteria can also be killed off, leaving us at risk of infection by harmful bacteria. Clostridium difficile is one of these harmful bacteria and is the leading cause of hospital infections in England and Wales. Although the number of C. difficile infections is dropping, treating them is becoming harder as it becomes more resistant to antibiotics. New ways of controlling C. difficile infections are desperately needed to replace ineffective antibiotics, and bacteriophages are one such technology being investigated.\nBacteriophages are naturally occurring viruses that target bacteria. Bacteriophage therapy is not a new idea; it was being developed not long after their discovery at the start of the 20th century. However, after the discovery of penicillin and other antibiotics the research into the use of phages was abandoned in the West but its application continues in Eastern Europe, particularly in Georgia. Now that the bacterial resistance to antibiotics is becoming such a large problem, there is renewed interest in developing bacteriophage therapy.\nFor use as a therapy, the bacteriophage must not affect any of the hundreds of different bacterial species that make up a healthy human microbiota. Researchers at the IFR had previously discovered and isolated a bacteriophage that specifically targets C. difficile. The new study, published in the journal Anaerobe, looked to assess how effective using this phage might be in combatting C. difficile infections.\nThe researchers used a model of the human colon, set up to mimic that of an elderly person in hospital. Antibiotics were given in the same way as in hospital, disrupting the normal balance of bacteria and allowing C. difficile to establish itself, and then produce the toxins that make C. difficile infections so dangerous.\nThe study showed that the administration of a specific bacteriophage significantly reduced the number of C. difficile cells and also the amount of toxin produced, without significantly affecting the other members of the gut microbiota. This suggests that bacteriophages could have great potential for use to combat C. difficile infections in hospital settings.\nThe phage wasn’t, however, able to kill off all of the C. difficile cells. This was because this bacteriophage, like many others, is able to insert its own DNA into the bacterial chromosome – a process known as lysogeny. This makes the bacteria resistant to further bacteriophage attack.\nInterestingly, in some cases the lysogeny seemed to prevent the C. difficile cells from producing the toxin. So although all of the cells aren’t killed, those that survive are a lot less dangerous. This may give clinicians more time to get C. difficile infections under control.\nThis bacteriophage shows considerable promise as a new therapeutic agent to control C. difficile infections in hospitals, with potential to provide a new weapon that is desperately needed in the battle against superbugs.\nThis research was carried out by Dr Emma Meader as part of her PhD studentship at the IFR, under the supervision of Dr Arjan Narbad. This article is one of a series highlighting the work of IFR’s excellent PhD students who received their Doctorates at UEA’s congregation ceremony in July 2013.\nSource: Institute of Food Research","‘Directing’ evolution to identify potential drugs\nScientists have developed a technique that could significantly reduce the time to discover potential new antibody-based drugs to treat disease.\nAntibodies are produced by the body in response to the presence of a disease-causing agent. They can also be synthesised in the laboratory to mimic natural antibodies and are used to treat a number of diseases.\nAntibody therapies can be highly effective, but challenges can arise when promising candidate antibodies are produced at a large scale. Stresses encountered during manufacturing can disrupt the structure of these fragile proteins leading to aggregation and loss of activity. This in turn prevents them from being made into a therapeutic.\nNew research from an eight-year collaboration between scientists at Leeds and the biopharmaceutical company AstraZeneca has resulted in a technique that allows fragments of antibodies to be screened for susceptibility to aggregation caused by structure disruption much earlier in the drug discovery process.\nThe approach is described by the researchers in a paper in the journal Nature Communications, published today.\nDr David Brockwell, Associate Professor in the Astbury Centre for Structural Molecular Biology at the University, led the research. He said: \"Antibody therapeutics have revolutionised medicine. They can be designed to bind to almost any target and are highly specific.\n\"But a significant problem has been the failure rate of candidates upon manufacturing at industrial scale. This often only emerges at a very late stage in the development process – these drugs are failing at the last hurdle.\n\"But our research is turning that problem on its head.\"\nWhen it comes to developing an antibody drug, scientists are not restricted to a single protein sequence. Fortunately, there is often an array of similar antibodies with the same ability of locking or binding tightly onto a disease-causing agent. That gives researchers a range of proteins to screen, to determine which are more likely to progress through the development process.\nThe collaboration demonstrates the power of industry and academia working together to tackle what has been one of the major roadblocks preventing the efficient and rapid development of these powerful therapeutic molecules.\nProfessor Sheena Radford, FRS, Director of the Astbury Centre, said: \"The collaboration that has existed between the team of scientists within the University of Leeds and AstraZeneca demonstrates the power of industry and academia working together to tackle what has been one of the major roadblocks preventing the efficient and rapid development of these powerful therapeutic molecules.\"\nHow target proteins are screened\nThe target proteins are cloned into the centre of an enzyme that breaks down antibiotics in the bacterium E.coli. This enables the scientists to directly link antibiotic resistance of the bacteria to how aggregation-prone the antibody fragment is.\nA simple readout - bacterial growth on an agar plate containing antibiotic – gives an indication of whether the target protein will survive the manufacturing process. If the antibody proteins are susceptible to stress, they will unfold or clump together, become inactive, and the antibiotic will kill the bacteria. But if the protein chain is more stable, the bacteria thrives and will display antimicrobial resistance and will grow in the presence of the antibiotic.\nThe scientists harvest the bacteria that have survived and identify the cloned protein sequence. That indicates which protein sequences to take forward in the development pipeline. The whole cycle takes about a month and increases the chances of success.\nBut the process can go a step further, using the idea of directed evolution.\nScientists use the idea of natural selection where mutations or changes take place in the proteins, sometimes making them more stable. Directed evolution could generate new better performing sequences that, at the current time, cannot even be imagined, let alone designed and manufactured.\nHow does this method work? Like Darwin’s natural selection, evolutionary pressure in this case is applied by the antibiotic and selects for the survival of bacteria that produce the protein variants that do not aggregate.\nThe protein sequences hosted in the bacterial cells that have shown resistance are harvested and their genes sequenced and scored, to select the best performing sequences. After a quick check to ensure that the new antibody sequences still retain their excellent binding capability for the original disease-causing target, they can be taken forward for further development.\nProfessor Radford said: \"There is tremendous excitement about this approach. We are letting evolutionary selection change the sequence of the proteins for us and that might make some of them more useful as drug therapies. Importantly for industry, nature does the hard-work – obviating the need for so called rational engineering which is time- and resource-intensive.\n\"As we do this, we will be putting the sequence information we gather into a database. As the database gets bigger, it may well be possible with artificial intelligence and machine learning to be able to identify the patterns in protein sequences that tell us that a protein can be scaled up for pharmaceutical production without needing any experiments. That is our next challenge and one we are tackling right now.\"\nThe screening system we have developed here is a great example of industry and academia working together to solve important challenges in the development of potential new medicines.\nDr David Lowe, who led the work at AstraZeneca, said: \"The screening system that we have developed here is a great example of industry and academia working together to solve important challenges in the development of potential new medicines.\n\"By combining AstraZeneca’s antibody discovery and screening expertise, together with the Astbury Centre’s world-leading knowledge of protein structure and aggregation, we have produced a high throughput method for rapidly evolving proteins with better biophysical properties that has the potential for wide scientific applicability.\"\nThe research was funded by AstraZeneca, Innovate UK, the Biotechnology and Biological Sciences Research Council and the Wellcome Trust.\nFor further information or to talk to a member of the research team, please contact the media relations at the University of Leeds via email@example.com.\nThe paper, \"An in vivo platform to select and evolve aggregation-resistant proteins\", is published in Nature Communications, today, https://doi.org/10.1038/s41467-020-15667-1"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:54bab2c4-889a-4ff4-9a17-33ecbd33b35d>","<urn:uuid:df9e7146-9db5-4004-bb1e-17c78dc09420>"],"error":null}
{"question":"How many native languages were there in North America before Europeans arrived?","answer":"Prior to sustained contact with Europeans, there were as many as five hundred distinct tribal languages in North America. Today, only around 180 remain, and this number continues to decrease rapidly. These languages belong to various world language families.","context":["Everything You Wanted to Know About Indians But Were Afraid to Ask\nPublication Year: 2012\nPublished by: Minnesota Historical Society Press\nHalf Title Page\nDownload PDF (180.2 KB)\n2nd Half title page\nDownload PDF (26.4 KB)\nDownload PDF (57.2 KB)\nIndians. They are so often imagined, but so infrequently well\nI grew up in a borderland. My family moved a couple times, but we usually lived on or near the Leech Lake Reservation in northern Minnesota. I went to school in the nearby town of Bemidji with plenty of other native kids and many more whites...\nDownload PDF (94.4 KB)\nWhat terms are most appropriate for talking about\nNorth America’s first people?\nThe word Indian comes from a mistake: on his first voyage to the Americas, Columbus thought the Caribbean was the Indian Ocean and the people there were Indians. The use of the word and assumptions around it are well documented in Columbus’s\nDownload PDF (710.4 KB)\nHow many Indians were in North and South America\nThe shortest and most honest answer to this question is that nobody knows for sure. Genomic and archaeological research is starting to give us more accurate information about how many groupings of people there were and the size of the communities...\nReligion, Culture & Identity\nDownload PDF (614.2 KB)\nWhy do Indians have long hair?\nThere are around five hundred distinct Indian tribes in North America, and their cultural beliefs are diverse. For many Native Americans, hair was viewed as a symbol of spiritual health and strength. Leonard Moose, an Ojibwe elder from Mille Lacs, said...\nDownload PDF (1.6 MB)\nWhat is a powwow?\nThe word powwow is actually derived from a term for spiritual leader in the Narragansett and Massachusett languages but was later misapplied to many types of ceremonial and secular events. Although Ojibwe drum ceremonies and traditional...\nDownload PDF (145.6 KB)\nHow many tribal languages are spoken in North America?\nThere may have been as many as five hundred distinct tribal languages in North America prior to sustained contact with Europeans. There are now around 180, but the number is shrinking quickly. All world languages are members of families, such...\nDownload PDF (1.2 MB)\nWhat is sovereignty?\nSovereignty means supreme and independent authority over a geographic area. Indian nations are sovereign because they have such power and control over reservations. Tribal sovereignty is the basis for most fundamentally different legal and political...\nDownload PDF (332.9 KB)\nDo Indians get a break on taxes, and if so, why?\nSome Indians do get a break on some taxes—but of course, it’s complicated. All Indians, whether they are enrolled members or not, must pay federal income tax. All Indians must also pay property taxes in the county or municipality in which they own...\nDownload PDF (230.1 KB)\nWhat were federal residential boarding schools?\nOne of the most pernicious dimensions of the war on Indian culture was the residential boarding school system.1 Beginning in the late nineteenth century, missionary, military, and government officials advocated for the removal of Indian children...\nPerspectives: Coming to Terms and Future Directions\nDownload PDF (210.0 KB)\nWhy are Indians so often imagined rather than\nPart of the story is simple math. American Indians are a very small percentage of the global population and even a small percentage of the U.S. population. In some parts of the country, one is likely to run into an Indian. But for most Americans...\nConclusion: Finding Ways to Make a Difference\nDownload PDF (298.6 KB)\nHow can I help?\nSometimes the brambled racial borderland of my youth seems as impenetrable as it ever was. Indians remain imagined more than they are understood. Public and political backlash against Indian casinos and treaty rights is still obvious. Indians are still...\nDownload PDF (45.0 KB)\nMy time devoted to this project was in part enabled by support\nfrom the American Philosophical Society, the Bush Leadership\nFellows Program, and the John Simon Guggenheim Foundation.\nThank you to Michael Meuers for suggesting the title for this book and the lecture series that inspired it and for his leadership in promoting bilingual signage in Bemidji along with Rachelle...\nDownload PDF (62.9 KB)\nDownload PDF (85.5 KB)\nDownload PDF (84.3 KB)\nDownload PDF (39.2 KB)\nPage Count: 188\nIllustrations: 20 b&w photos\nPublication Year: 2012"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:6522f3b7-8d37-49f8-9113-09240215e74a>"],"error":null}
{"question":"As an environmental engineer, I'm curious to compare the bacterial processes used in home septic systems versus modern municipal wastewater facilities - how do they differ in handling nitrogen and phosphorus?","answer":"In home septic systems, bacteria simply break down sewage and wastewater, with end products still containing nutrients and bacteria. Municipal facilities, however, use sophisticated biological processes: for nitrogen removal, they employ specific nitrifying bacteria in a two-step process (converting ammonia to nitrite, then to nitrate) followed by denitrification to convert it to nitrogen gas. For phosphorus, they use Enhanced Biological Phosphorus Removal (EBPR) with special Phosphorus Accumulating Organisms (PAOs) that store excess phosphate in their cell mass, which is then removed with waste sludge. This makes municipal systems more effective at removing these nutrients than basic septic systems.","context":["How septic systems work\nA septic system processes all the wastewater from your house. There are two stages:\n- A double-chambered concrete tank, which has baffles to prevent raw waste from flowing into the second chamber\n- A system of water-permeable pipes called the tile bed. Aeration may also be added between the first and second stages.\nBacteria in the system break down sewage and wastewater. Undigested solids settle in the bottom of the tank as sludge. Lighter solids float to the top as scum. Liquid containing dissolved materials is taken from between these two layers and flows continuously and evenly into the tile bed. As the wastewater works its way from the tiles through the bed it is treated one last time, before being discharged into the water table. At every stage, bacteria are at work, digesting the material. End products of the system still contain nutrients, bacteria and chemicals.\nProblems with septic systems\nThe tank should be pumped out regularly or the sludge/scum can be drawn into the tile bed, eventually overloading the system. After time, this can lead to partially treated wastewater appearing directly on the ground surface. Sewage will contaminate the soil and the water supply, including your well or a neighbour’s.\nThe same situation can happen if too much water is dumped into the tank. Excess household chemicals, soaps and detergents can also kill the bacterial action in the septic tank and affect its ability to process waste.\n- The Rideau Valley Conservation Authority’s Septic Smart manual\n- The Ministry of Environment's Care and Feeding of Your Septic Tank\n- The Landowner Resource Centre, 613-692-3571 ext.1136\n- New septic systems: contact the Ottawa Septic System Office at 613-692-3571 or toll free at 1-800-267-3504 ext.1129 to get a sewage system permit.\nSigns of trouble\n- Grass over the tile bed is unusually green or spongy to walk on\n- Plumbing takes longer to drain\n- You can smell sewage\n- Grey or black liquids surface in yards\n- A test of your or a neighbour's well water shows contamination\nSeptic system dos\n- Know where the tank is located and keep a maintenance record\n- Make sure you hire a licensed septic tank servicing company for regular inspections and that they take care not to damage the inlet or outlet baffles or tees during pumping\n- Get the tank pumped to remove the accumulated scum and sludge. Pumping intervals should be based on regular inspections (including measurement of scum and sludge levels in your tank).\n- Plant grass over the leaching field; it will help prevent erosion and absorb excess water.\n- Divert surface runoff water from roofs, patios, driveways and other areas away from the leaching field\n- Conserve water to avoid overloading the system\nSeptic system don’ts\n- Use your toilet as a trash can\n- Use more soap or detergents than you need\n- Install a garbage disposal without checking whether your septic tank can handle the added volume\n- Poison your septic system and the groundwater by pouring harmful chemicals and cleaners such as chlorine bleach, toilet bowl cleaners, borax and drain openers down the drain\n- Drive over or park cars, trucks or heavy equipment on the tile bed\n- Plant trees or shrubbery in or near the tile bed, because the roots will grow into the lines and plug them\n- Pave the tile bed with concrete or asphalt\n- Drain your water softener backwashes into the septic tank. Use a class-2 leaching pit (dry well) or the sump hole in your basement.\n- Add \"starters\" or \"conditioners\"; some will interfere with normal operations; others (particularly degreasers) contain cancer-causing substances that could contaminate the groundwater\nNEVER flush these items into the septic tank (they cannot be broken down by bacteria or will destroy the bacterial action):\n- Loose hair\n- Cigarette butts\n- Coffee grounds\n- Fat, grease or oil\n- Dental floss\n- Paper towels\n- Disposable diapers\n- Sanitary napkins, tampons or condoms\n- Kitty litter\n- Gauze bandages\nNEVER flush chemicals into the septic tank (they could contaminate surface and groundwater):\n- Waste oils\n- Photographic solutions\n- Pesticides or herbicides","Waste Water Treatment\nModular “Plug & Play” Waste Water Treatment Solutions\nWaste Water Treatment Overview\nThe principal objective of wastewater treatment is generally to allow human and industrial effluents to be disposed of without danger to human health or unacceptable damage to the natural environment. The three basic types of wastewater are classified by source:\n- Industrial Wastewater\n- Domestic Wastewater\n- Storm Wastewater\nThe composition of industrial wastewater varies with the type of industrial process discharging the wastewater. Some industrial wastewater can be readily treated in conventional wastewater treatment plants. Other more contaminated sources may have to be pre-treated to remove or limit any process specific contaminants to acceptable levels before being accepted at a municipal treatment facility.\nDomestic wastewater comes from normal day to day activities occurring in homes, business and institutions. It is composed of a variety of organic and inorganic substances. Organic substances consist of molecules that are based on carbon and include fecal matter, detergents, soaps, fats, greases and food particles. Domestic wastewater is readily treatable in publically owned treatment facilities, which have been specifically designed to treat domestic wastewater in accordance with prescribed water quality regulations and standards.\nStorm water is also readily treated in these municipal facilities as it is relatively low in contaminants. Too much storm water however can overload these facilities and also dilute the sewage so that biological treatment processes are compromised. Most modern municipal wastewater system designs, channel domestic and storm sewage in separate distribution lines.\nMeasured and Regulated Constituents of Wastewater\nBiochemical Oxygen Demand\nOne of the most widely measured characteristics of wastewater is Biochemical Oxygen Demand or BOD. It is defined as the amount of oxygen aerobic microorganisms must consume to breakdown the organic material present in the wastewater. BOD is determined by measuring the quantity of oxygen consumed by microorganisms during a five-day period. For example, a regulated effluent discharge limit of say 20 ml BOD/l, means that the concentration of oxygen in a water sample diminishes by no more than 20 mg/l over 5 days. Also referred to as BOD5, it is the most common measure of the biodegradable organic material content of wastewater. BOD of effluent is tightly regulated, because effluent high in BOD can deplete oxygen in receiving lakes and rivers, causing fish kills and ecosystem imbalances.\nTotal Suspended Solids\nMost waste water contains large quantities of undissolved organic and inorganic materials. Referred to as Total Suspended Solids or TSS, these solids are problematic because most are in the form of fine particles, which not only support BOD but also plug up or clog septic systems and mechanical wastewater treatment equipment. TSS are removed from waste streams by various means such as screening, granular filtration, and induced settling/flotation schemes. TSS values are expressed as mg TSS/liter.\nOne of the major concerns regarding constituents in wastewater effluent is the concentration of nutrient compounds, particularly nitrogen and phosphorus. When released into receiving waters these nutrients concentrate and stimulate the excessive growth of algae and other aquatic plants, leading to decreased oxygen levels that can lead to hypoxic conditions. Hypoxia occurs when dissolved oxygen concentrations fall below the level necessary to sustain most animal life; generally considered to occur at levels below 2mg/l.\nNitrogen is present in many forms in the waste water stream. Most nitrogen excreted by humans is in the form of organic nitrogen (dead cell material, proteins, amino acids) and urea. Ammonia (NH3) is the primary form of nitrogen in influent entering treatment facilities. Nitrogen removal is accomplished by the biological processes of nitrification and denitrification, which convert ammonia (NH3) into gaseous nitrogen ( N2 ), an inert gas suitable for release into the atmosphere.\nNitrification is a two-step aerobic process facilitated by two different species of nitrifying bacteria. The oxidation of ammonia (NH3) to nitrite (NO2−) occurs first followed by the oxidation of nitrite (NO2−) to nitrate (NO3−). Then the NO3− is reduced to N2 through anaerobic denitrification. A wide array of bacterial populations are employed in the denitrification process.\nThe average phosphorus content of most sewage is estimated at around 10 mg/liter. Synthetic detergents are the largest source followed by human waste in the form of urine and feces. While legislated requirements for more environmentally friendly detergent formulations have reduced the amount of phosphorus entering the waste stream, the contribution from human waste remains constant. Phosphorus removal can be achieved by chemical precipitation or biologically through the enhanced biological phosphorus removal process (EBPR).\nIn chemical precipitation soluble phosphorus is transformed to a solid that settles and can be removed with the sludge. Several different metal salt additives are commonly used to chemically precipitate phosphorus, they include : Ferric Chloride (FeCl3), Ferrous Chloride (FeCl4), Ferrous Sulfate (FeSO4) and Aluminum Sulfate (alum) ( Al4 (SO4) 3 ) .\nEnhanced Biological Phosphorus Removal ( EBPR) is a process that uses alternating anaerobic and aerobic zones to provide an environment that encourages the growth of Phosphorus\nAccumulating Organisms (PAO). PAOs store excess polyphosphate in their cell mass and captured phosphorus is removed with the waste sludge. Essentially EBPR relies on the selection and proliferation of microbial populations capable of sequestering phosphates in greater amounts than would normally be required to sustain their growth. The resulting sludge high in phosphate can be used as fertilizer or disposed of in a conventional land fill.\nReview of Basic Wastewater Treatment Processes\nThe most basic form of wastewater treatment involves the removal of contaminants by allowing or inducing them to either settle to the bottom or to float to the top of reservoirs or clarification tanks for collection and removal. Pollutants are then subjected to biological treatment, either in natural settling ponds or in bio-reactors specifically designed to optimize certain microbial populations and metabolic processes in order to remove or chemically transform the targeted contaminants.\nWastewater flowing into modern treatment facilities generally passes through five common stages:\n- Influent collection & delivery\n- Primary treatment\n- Secondary treatment\n- Tertiary treatment\n- Disinfection & Effluent Discharge\nInfluent Collection & Delivery\nInfluent as the name implies, is wastewater flowing into a wastewater treatment facility. It contains all of the water, debris, and waste that entered the collection system feeding the facility.\nPrimary treatment involves basic processes to remove suspended solid waste from influent and to reduce biochemical oxygen demand (BOD) – the amount of oxygen microorganisms must consume to breakdown the organic material present in the wastewater. First, influent is passed through a series of raked bar screens to mechanically remove large objects such as bottles, plastic materials, pieces of wood, trash and other forms of suspended solid waste. The water is then passed through a grit removal system to take out smaller inorganic particles like sand and gravel. Finally the water flows into large primary clarification tanks or clarifiers, where suspended organic solids either settle by gravity or float to the surface as scum and grease. The floating scum is skimmed off, the settled solids, known as primary sludge, is removed and the primary effluent moves onto the next stage for secondary treatment. Primary treatment can reduce BOD by 20 to 30 percent and suspended solids by up to 60 percent.\nSecondary treatment employs aerobic biological treatment processes to remove biodegradable organic matter from the primary influent. Aerobic biological treatment is performed in the presence of oxygen by microorganisms (principally bacteria) that metabolize the organic matter in the wastewater, thereby producing more microorganisms and inorganic end-products (principally CO2, NH3, and H2O). Several aerobic biological processes are used for secondary treatment differing primarily in the manner in which oxygen is supplied to the microorganisms and in the rate at which organisms metabolize the organic matter.\nSecondary treatment systems are classified as fixed-film or suspended-growth systems. Fixed-film or attached growth systems, include trickling filters, bio-towers, and rotating biological contactors, where the biomass grows on media and the sewage passes over its surface. Fixed-film systems have been further perfected with the advent of Moving Bed Biofilm Reactors (MBBR). MBBR systems are the treatment method of choice here at Sapphire water.\nSuspended-growth systems include conventional activated sludge processes (also aerated lagoons and aerobic digestion), where the waste flows around and through the free-floating microorganisms, gathering into biological flocs that settle out of the wastewater. The settled flocs retain the microorganisms, meaning they can be recycled for further treatment.\nTypically secondary treatment systems can remove up to 85 percent of BOD and total suspended solids.\nThe purpose of tertiary treatment also referred to as effluent polishing, is to provide a final treatment stage to further improve the effluent quality before it is discharged to the receiving environment. Specifically, tertiary treatment targets remaining bioavailable organics (contributors to BOD), nutrients ( nitrogen and phosphorus ) and toxins ( pesticides, solvents, petroleum, metals – lead, cadmium, mercury). Treatment methods vary and include granular filtration, biological processes such as nitrification and denitrification, enhanced biological phosphorus removal ( EBPR), chemical precipitation, coagulation, flocculation and toxin specific treatment strategies. All in all, tertiary treatment can remove up to 99 percent of all impurities from sewage, but it is a very expensive process.\nDisinfection & Effluent Discharge\nThe purpose of disinfection is to destroy or inactivate pathogenic bacteria, viruses and parasites remaining in treated wastewater before it is discharged back into the environment in order to prevent the spread of waterborne diseases.\nVarious monitoring systems have been established that use the presence of indicator bacteria to gage the effectiveness of disinfection systems. The most commonly used indicators are total and faecal coliforms, E. coli and faecal streptococci. The presence of the indicator organisms in numbers greater than a specific target level suggests an increased probability of the presence of pathogenic organisms. The Water Environment Federation reported in 1996 that high levels of coliform indicators have been detected in 88% of the waterborne disease outbreaks in North America.\nChlorine is the most widely used disinfectant for wastewater, followed by ozone and ultraviolet radiation. Chlorine kills microorganisms by destroying cellular material. This chemical can be applied to wastewater as a gas, a liquid or in a solid form similar to swimming pool disinfection chemicals. However, any free (uncombined) chlorine remaining in the water, even at low concentrations, is highly toxic to aquatic life. Therefore, removal of even trace amounts of free chlorine is often needed to protect fish and aquatic life.\nOzone is produced from oxygen exposed to a high voltage current. Ozone is very effective at destroying viruses and bacteria and changes back to oxygen rapidly without leaving harmful by products. Ozone is a relatively expensive approach to disinfection due to high energy costs.\nUltra violet (UV) disinfection occurs when electromagnetic energy in the form of light in the UV spectrum penetrates the cell wall of exposed microorganisms. The UV radiation degrades the ability of the microorganisms to survive by damaging their genetic material. UV disinfection is a physical treatment process that leaves no chemical traces.\nEmerging Trends in Water and Wastewater Treatment\nThe overarching trends driving global demand for improved water and wastewater treatment systems are: population growth, increasing water scarcity, aging infrastructure along with the associated funding gap and the enactment of stricter water quality regulations to address rising concerns over the effects of inadequately treated water on human health and the environment.\nWith a large portion of existing water treatment systems reaching the end of their service lives many countries are faced with the urgent need to fund the replacement of this aging infrastructure. The situation is further exacerbated as new more stringent water quality regulations, render many middle aged treatment facilities incapable of compliance without major upgrades and overhauls.\nAs a result of this situation we are seeing the emergence of smaller more economical decentralized water and wastewater treatment systems capable of meeting current and future water quality standards. Often these decentralized systems are being funded in part by Public Private Partnership (3P or PPP ) finance schemes in an effort to bridge the infrastructure funding gap. From a technology stand point these modular packaged systems are generally scalable to accommodate future growth and rely heavily on advanced biological treatment technologies as opposed to chemical treatment options. (Decentralized WWTP Video)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b1169090-c2ab-42ed-be08-069a0bba7d82>","<urn:uuid:5c51ae6f-0bf1-4201-950d-42b398ba57da>"],"error":null}
{"question":"What are the common signs of daytime sleepiness, and what essential nutrients should be monitored for maintaining good health?","answer":"Common signs of daytime sleepiness can be observed in various situations, such as sitting and reading, watching TV, being inactive in public places like theaters or meetings, riding as a passenger in a car for an hour, lying down to rest in the afternoon, and sitting quietly after lunch. Regarding essential nutrients for health maintenance, several key elements need monitoring: protein from sources like lean meats, eggs, and legumes; calcium from dairy products and dark leafy greens; iron from dark leafy greens and lean red meats; fiber from whole grains and fruits like berries and apples; folate from dark leafy greens and oranges; Vitamin C from whole fruits; and Vitamin D from fortified juice, eggs, and milk.","context":["Sleeping Soundly: Understanding and Treating Sleep Disorders\nDoes your partner's snoring keep you awake? Are you worried that your child seems to be struggling for the next breath at night? Perhaps you find falling asleep difficult or you doze off at inappropriate times? Sleeping Soundly is intended to help you to fully understand sleep problems and to provide solutions so that you and your family can enjoy a great night's rest every night.\nWhat people are saying - Write a review\nWe haven't found any reviews in the usual places.\nDreams and dreaming\nSleeprelated internet sites\nThe function of sleep\nabnormal airway resistance syndrome alcohol antidepressants anxiety awake bedtime benzodiazepines body bruxism cataplexy cause insomnia cent Chapter child chronic fatigue syndrome Ciguatera common daytime sleepiness delayed sleep phase difficulty initiating disturbed breathing dose driving effect emotional epilepsy example eye movements fall asleep feeling Figure hormone Hospital 03 Hospital Sleep Unit important insomnia light limb movement disorder lucid dreams maintaining sleep medical conditions melatonin milligrams modafinil mouthguards muscle naps narcolepsy narcoleptic narcoleptic syndrome night nighttime non-REM sleep obstructive sleep apnoea occur overnight sleep study oxygen parents patients periodic limb movement person psychiatric illness rapid eye movement recognised REM sleep reported restless legs risk shift side-effects sleep and wake sleep apnoea sleep deprivation sleep diary sleep disorders Sleep paralysis sleep phase syndrome sleep quality sleep starts Sleep Unit 07 sleep walking sleep-wake sleepiness tendency sleeping tablets stimulant medications stopping breathing surgery symptoms treatment usually wake function\nPage 79 - On such occasions, the instructions are to be followed afterward when you intend to go to sleep. 3. If you find yourself unable to fall asleep, get up and go into another room. Stay up as long as you wish and then return to the bedroom to sleep: Although we do not want you to watch the clock, we want you to get out of bed if you do not fall asleep immediately. Remember the goal is to associate your bed with falling asleep quickly!\nPage 21 - How likely are you to doze off or fall asleep in the following situations, in contrast to feeling just tired? This refers to your usual way of life in recent times. Even if you have not done some of these things recently, try to work out how they would have affected you.\nPage 21 - Situation Chance of dozing Sitting and reading Watching TV Sitting, inactive in a public place (eg, a theater or a meeting) As a passenger in a car for an hour without a break Lying down to rest in the afternoon when circumstances permit Sitting and talking to someone Sitting quietly after a lunch without alcohol In a car, while stopped for a few minutes in traffic Source: From Johns,25 p.\nPage 79 - Step 3. Do this as often as is necessary throughout the night. 5. Set your alarm and get up at the same time every morning irrespective of how much sleep you got during the night. This will help your body acquire a consistent sleep rhythm. 6. Do not nap during the day.\nPage 34 - ... is calculated by dividing the weight (in kilograms) by the square of the height (in meters).\nPage 80 - People who feel angry and frustrated because they cannot sleep should not try harder and harder to fall asleep but should turn on the light and do something different.\nPage 15 - Early to bed early to rise Make a man healthy, wealthy and wise.\nPage 189 - Akerstedt, T., Torsvall, L. and Gillberg, M. 1982 'Sleepiness and shift work: field studies\nPage 189 - Chronic fatigue syndrome: chronic ciguatera poisoning as a differential diagnosis' Medical Journal of Australia vol.\nPage 177 - American Sleep Disorders Association 1610 14th St NW, Suite 300 Rochester, MN 55901...","Eating for two may well be one of the most enjoyable aspects of pregnancy. Indulging in a little extra hummus or peanut butter or, let’s face it, ice cream or a few potato chips, here and there is a definite pregnancy perk. As long as you do your best to maintain a predominantly healthy diet and refrain, whenever possible, from gaining too much weight, you should be fine to indulge in a craving every once in a while. Aside from not over-doing the sweets and other foods that don’t offer a whole lot of nutritional value, and barring any dietary restrictions due to a medical condition, there are relatively few foods that mothers-to-be are advised to stay away from. However, there are a few precautions that you should take seriously regarding your nutrition and the health of your unborn baby. Don’t worry. The list isn’t very long and it isn’t all that restrictive. It is, however, important that you become familiar with it.\nFoods to Avoid During Pregnancy\n- Fish high in mercury. This includes tuna, swordfish, tilefish, king mackerel, shark, marlin, orange roughy, grouper, Chilean sea bass, bluefish, halibut, and sablefish. Generally speaking, the larger the fish, the more likely it will have a high mercury count.\n- Raw meat and fish. Eating raw or undercooked meat and fish increases the risk of contracting food-borne illnesses that are dangerous to both you and your baby.\n- Raw eggs can possibly transmit salmonella although pasteurization does help to reduce this risk. It’s best to avoid them just the same.\n- Soft cheeses such as brie, camembert, feta, gorgonzola, and certain queso are not considered safe for consumption during pregnancy.\n- Unpasteurized milk.\n- Pate carries the possibility of the presence of listeria.\n- Excess caffeine can possibly lead to dehydration and even miscarriage. Limit your daily intake to your physician’s suggestions.\n- Alcohol has been associated with birth defects.\n- Unwashed produce sometimes harbors bacteria.\n- Raw sprouts pose potential health hazards to pregnant women and babies due to possible bacterial contamination.\nWhile this list may seem a little daunting at first it’s clear that there are many other healthy and tasty alternatives that will satisfy your cravings and keep you and your baby safe. The American Pregnancy Association advises mothers-to-be that “The food we eat on a daily basis affects how our bodies work, how we heal and grow, and how we maintain energy and strength for years to come. It also determines the basic nutritional health that our children are born with, and provides a model for their eating habits during childhood and beyond.” That reminder should make it easier for us to choose to adopt a healthful nutrition plan during pregnancy and hopefully for the rest of our lives.\nThe Mayo Clinic states that “during pregnancy, the basic principles of healthy eating remain the same…However, a few nutrients in a pregnancy diet deserve special attention.”\nKey Nutritional Needs to Keep in Mind\n- This includes sources such as lean meats, fully cooked fish or seafood, eggs, cottage cheese, nuts and legumes.\n- Calcium derived from dairy products and certain dark, leafy greens as well as salmon.\n- Iron sourced from dark, leafy greens, lean red meats, beans, fortified cereals or bread or possibly a supplement prescribed by your physician.\n- Fiber from whole grains and certain fruits such as berries and apples.\n- Folate from dark, leafy greens, oranges, peanuts, and certain fortified breads and cereals or a supplement prescribed by your physician.\n- Vitamin C derived from whole fruits.\n- Vitamin D from fortified juice, eggs, and milk.\nA diet rich in whole foods, particularly vegetables and fruits, as well as whole grains, nuts, legumes, lean cuts of meat, healthy varieties of fish and sufficient amounts of water are important nutritional standards to abide by for optimal health. Doing your best to maintain a healthy nutritional standard will help to ensure the health and safety of you and your baby."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4771e92c-22d7-4e05-ab81-2f5958033536>","<urn:uuid:fee1878e-a7cd-4eb4-a97d-5e267947e361>"],"error":null}
{"question":"大家好！我正在研究AI系统的可解释性，想问问神经网络和基于案例的解释方法各有什么优缺点啊？🤔","answer":"Neural networks and example-based explanation methods each have distinct characteristics. Neural networks excel at deriving meaning from complex and non-linear data, with successful applications in areas like autonomous driving, speech recognition, and computer vision. However, their major limitation is their lack of interpretability - data scientists can only feed inputs and watch outputs without understanding the intermediate process. Example-based explanations, on the other hand, are more interpretable as they explain predictions by selecting specific instances from the dataset. They work particularly well with structured data like images or texts where instances can be represented in a humanly understandable way. Example-based methods help humans construct mental models of both the machine learning model and the underlying data distribution. However, they can be challenging to apply with tabular data that has hundreds or thousands of less structured features.","context":["Chapter 7 Example-based Explanations\nExample-based explanation methods select particular instances of the dataset to explain the behavior of machine learning models or to explain the underlying data distribution.\nKeywords: example-based explanations, case-based reasoning (CBR), solving by analogy\nExample-based explanations are mostly model-agnostic, because they make any machine learning model more interpretable. The difference with model-agnostic methods is that the example-based explanation methods explain a model by selecting instances of the dataset and not by creating summaries of features (such as feature importance or partial dependence). Example-based explanations only make sense if we can represent an instance of the data in a humanly understandable way. This works well for images, because we can view them directly. In general, example-based methods work well if the feature values of an instance carry more context, meaning the data has a structure, like images or texts do. It’s more challenging to represent tabular data in a meaningful way, because an instance can consist of hundreds or thousands of (less structured) features. Listing all feature values to describe an instance is usually not useful. It works well if there are only a handful of features or if we have a way to summarize an instance.\nExample-based explanations help humans construct mental models of the machine learning model and the data the machine learning model has been trained on. It especially helps to understand complex data distributions. But what do I mean by example-based explanations? We often use them in our jobs and daily lives. Let’s start with some examples 35:\nA physician sees a patient with an unusual cough and a mild fever. The patient’s symptoms remind her of another patient she had years ago with similar symptoms. She suspects that her current patient could have the same disease and she takes a blood sample to test for this specific disease.\nA data scientist is working on a new project for one of his clients: Analysis of the risk factors that lead to the failure of production machines for keyboards. The data scientist remembers a similar project he worked on and reuses parts of the code from the old project because he thinks the client wants the same analysis.\nA kitten sits on the window ledge of a burning and uninhabited house. The fire department has already arrived and one of the firefighters ponders for a second whether he can risk going into the building to save the kitten. He remembers similar cases in his life as a firefighter: Old wooden houses that have been burning slowly for some time were often unstable and eventually collapsed. Because of the similarity of this case, he decides not to enter, because the risk of the house collapsing is too great. Fortunately, the kitty jumps out of the window, lands safely and nobody is harmed in the fire (Happy end!).\nThese stories illustrate how we humans think in examples or analogies. The blueprint of example-based explanations is: Thing B is similar to thing A and A caused Y, so I predict that B will cause Y as well. Implicitly, some machine learning approaches work example-based. Decision trees partition the data into nodes based on the similarities of the data points in the features that are important for predicting the target. A decision tree gets the prediction for a new data instance by finding the instances that are similar (= in the same terminal node) and returning the average of the outcomes of those instances as the prediction. The k-nearest neighbours (knn) model works explicitly with example-based predictions. For a new instance, a knn model locates the k nearest neighbours (e.g. the k=3 closest instances) and returns the average of the outcomes of those neighbours as a prediction. The prediction of a knn can be explained by returning the k neighbours, which - again - is only meaningful if we have a good way to represent a single instance.\nThe chapters in this part cover the following example-based interpretability methods:\n- Counterfactual instances tell us how an instance has to change to significantly change its prediction. By creating counterfactual instances, we learn about how the model makes its predictions and can explain individual predictions.\n- Adversarial examples are counterfactuals used to fool machine learning models. The emphasis is on flipping the prediction and not explaining it.\n- Prototypes and criticisms: Prototypes are a selection of representative instances from the data and criticisms are instances that are not well represented by those prototypes. 36\n- Influential instances are the training data points that were the most influential for the parameters of a prediction model or the predictions itself. Identifying and analysing influential instances helps to find problems with the data, debug the model and understand the model’s behavior better.\n- k-nearest neighbours model: An (interpretable) machine learning model based on examples.\nAamodt, A., & Plaza, E. (1994). Case-based reasoning : Foundational issues, methodological variations, and system approaches. AI Communications, 7(1), 39–59.↩\nKim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! criticism for interpretability.” Advances in Neural Information Processing Systems. 2016.↩","Neural network or artificial neural network is one of the frequently used buzzwords in analytics these days. Neural network is a machine learning technique which enables a computer to learn from the observational data. Neural network in computing is inspired by the way biological nervous system process information.\nBiological neural networks consist of interconnected neurons with dendrites that receive inputs. Based on these inputs, they produce an output through an axon to another neuron.\nThe term “neural network” is derived from the work of a neuroscientist, Warren S. McCulloch and Walter Pitts, a logician, who developed the first conceptual model of an artificial neural network. In their work, they describe the concept of a neuron, a single cell living in a network of cells that receives inputs, processes those inputs, and generates an output.\nIn the computing world, neural networks are organized on layers made up of interconnected nodes which contain an activation function. These patterns are presented to the network through the input layer which further communicates it to one or more hidden layers. The hidden layers perform all the processing and pass the outcome to the output layer.\nNeural networks are typically used to derive meaning from complex and non-linear data, detect and extract patterns which cannot be noticed by the human brain. Here are some of the standard applications of neural network used these days.\nThese applications fall into different types of neural networks such as convolutional neural network, recurrent neural networks, and feed-forward neural networks. The first one is more used in image recognition as it uses a mathematical process known as convolution to analyze images in non-literal ways.\nLet’s understand neural network in R with a dataset. The dataset consists of 724 observations and 7 variables.\n“Companies.Changed” , “Experience.Score”, “Test.Score”, “Interview.Score”, “Qualification.Index”, “age”, “Status”\nThe following codes runs the network classifying ‘Status’ as a function of several independent varaibles. Status refers to recruitment with two variables: Selected and Rejected. To go ahead, we first need to install “neuralnet” package\nNow, removing NA from the data\n> temp <-na.omit(temp)\n> dim(temp) # 724 rows and 7 columns)\n 724 7\n> y<-( temp$Status )\n# Assigning levels in the Status Column\n# Now converting the factors into numeric\n> y<-as.numeric (as.character (y))\n> y <-as.data.frame(y)\nRemoving the existing Status column and adding the new one Y\n> temp$ Status <-NULL\n> temp <-cbind(temp ,y)\n> temp <-scale( temp )\n> set.seed (100)\n> n=nrow( temp )\nThe dataset will be split up in a subset used for training the neural network and another set used for testing. As the ordering of the dataset is completely random, we do not have to extract random rows and can just take the first x rows.\n> train <- sample (1:n, 500, FALSE)\n> f<- Status ~ Companies.Changed+Experience.Score+Test.Score+Interview.Score+Qualification.Index+age\nNow we’ll build a neural network with 3 hidden nodes. We will Train the neural network with backpropagation. Backpropagation refers to the backward propagation of error.\n> fit <- neuralnet (f, data = temp [train ,], hidden =3, algorithm = \"rprop+\")\nPlotting the neural network\n> plot(fit, intercept = FALSE ,show.weights = TRUE)\nThe above plot gives you an understanding of all the six input layers, three hidden layers, and the output layer.\n> z<-z[, -7]\nThe compute function is applied for computing the outputs based on the independent variables as inputs from the dataset.\nNow, let’s predict on testdata (-train)\n> pred <- compute (fit, z[-train,])\n> sign(pred$net.result )\nNow let's create a simple confusion matrix:\n> table(sign(pred$net.result),sign( temp[-train ,7]))\n-1 108 20\n1 36 60\nHere, the prediction accuracy is 75%\nI hope the above example helped you understand how neural networks tune themselves to find the right answer on their own, increasing the accuracy of the predictions. Please note that the acceptable level of accuracy is considered to be over 80%. Unlike any other technique, neural networks also have certain limitations. One of the major limitation is that the data scientist or analyst has no other role than to feed the input and watch it train and gives the output. One of the article mentions that “with backpropagation, you almost don't know what you're doing\".\nHowever, if we just ignore the negatives, the neural network has huge application and is a promising and practical form of machine learning. In the recent times, the best-performing artificial-intelligence systems in areas such as autonomous driving, speech recognition, computer vision, and automatic translation are all aided by neural networks. Only time will tell, how this field will emerge and offer intelligent solutions to problems we still have not thought of."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9e8a5ef8-4755-4930-adc4-c0faa5747ead>","<urn:uuid:ff84b967-06ce-4f8d-a2af-f2190894f5eb>"],"error":null}
{"question":"¿Cómo se puede determinar la jerarquía de masas de los neutrinos? ¡Necesito entender este proceso paso a paso! 🤔","answer":"The determination of neutrino mass hierarchy involves several steps and measurements. First, the KamLAND experiment showed that the difference between two of the three mass states is small. The MINOS experiment indicated that the third state is at least five times smaller or larger. The Daya Bay experiment measured the magnitude of mass splitting (|Δm2ee|) at (2.59±0.20) x 10-3 eV2. By precisely measuring both this value and the complementary effective mass splitting (Δm2μμ) from muon neutrinos, scientists can calculate the two mass-squared differences (Δm232 and Δm231) among the three mass states.","context":["Scientists from the Daya Bay Neutrino Experiment have announced the latest results, including high-precision measurement of subatomic shape shifting and new results on differences among neutrino masses.\nThe international Daya Bay Collaboration has announced new results about the transformations of neutrinos – elusive, ghostlike particles that carry invaluable clues about the makeup of the early universe. The latest findings include the collaboration’s first data on how neutrino oscillation – in which neutrinos mix and change into other “flavors,” or types, as they travel – varies with neutrino energy, allowing the measurement of a key difference in neutrino masses known as mass splitting.\n“Understanding the subtle details of neutrino oscillations and other properties of these shape-shifting particles may help resolve some of the deepest mysteries of our universe,” said Jim Siegrist, Associate Director of Science for High Energy Physics at the U.S. Department of Energy (DOE), the primary funder of U.S. participation in Daya Bay.\nU.S. scientists have played essential roles in planning and running of the Daya Bay experiment, which is aimed at filling in the details of neutrino oscillations and mass hierarchy that will give scientists new ways to test for violations of fundamental symmetries. For example, if scientists detect differences in the way neutrinos and antineutrinos oscillate that are beyond expectations, it would be a sign of charge-parity (CP) violation, one of the necessary conditions that resulted in the predominance of matter over antimatter in the early universe. The new results from the Daya Bay experiment about mass-splitting represent an important step towards understanding how neutrinos relate to the structure of our universe today.\n“Mass splitting represents the frequency of neutrino oscillation,” says Kam-Biu Luk of the U.S. Department of Energy’s Lawrence Berkeley National Laboratory (Berkeley Lab), the Daya Bay Collaboration’s Co-spokesperson, who identified the ideal site for the experiment. “Mixing angles, another measure of oscillation, represent the amplitude. Both are crucial for understanding the nature of neutrinos.” Luk is a senior scientist in Berkeley Lab’s Physics Division and a professor of physics at the University of California (UC) Berkeley.\nThe Daya Bay Collaboration, which includes more than 200 scientists from six regions and countries, is led in the U.S. by DOE’s Berkeley Lab and Brookhaven National Laboratory (BNL). The Daya Bay Experiment is located close to the Daya Bay and Ling Ao nuclear power plants in China, 55 kilometers northeast of Hong Kong. The latest results from the Daya Bay Collaboration will be announced at the XVth International Workshop on Neutrino Factories, Super Beams and Beta Beams in Beijing, China.\n“These new precision measurements are a great indication that our efforts will pay off with a deeper understanding of the structure of matter and the evolution of the universe – including why we have a universe made of matter at all,” says Steve Kettell, a Senior Scientist at BNL and U.S. Daya Bay Chief Scientist.\nU.S. contributions to the Daya Bay experiment include coordinating detector engineering; perfecting the recipe for the liquid used to track neutrinos in the Daya Bay detectors; overseeing the photo-detector systems used to observe neutrino interactions and muons; building the liquid-holding acrylic vessels and the detector-filling and automated calibration systems; constructing the muon veto system; developing essential software and data analysis techniques; and managing the overall project.\nMeasuring neutrino mass and flavors\nNeutrinos come in three “flavors” (electron, muon, and tau) and each of these exists as a mixture of three masses. Measuring oscillations of neutrinos from one flavor to another gives scientists information on the probability of each flavor occupying each mass state (the mixing angles) and the differences between these masses (mass splitting).\nDaya Bay measures neutrino oscillation with electron neutrinos – actually antineutrinos, essentially the same as neutrinos for the purpose of these kinds of measurements. Millions of quadrillions of them are created every second by six powerful reactors. As they travel up to two kilometers to underground detectors, some seem to disappear.\nThe missing neutrinos don’t vanish; instead they have transformed, changing flavors and becoming invisible to the detectors. The rate at which they transform is the basis for measuring the mixing angle, and the mass splitting is determined by studying how the rate of transformation depends on the neutrino energy.\nDaya Bay’s first results were announced in March 2012 and established the unexpectedly large value of the mixing angle theta one-three, the last of three long-sought neutrino mixing angles. The new results from Daya Bay put the precise number for that mixing angle at sin22 Θ13=0.090 plus or minus 0.009. The improvement in precision is a result of having more data to analyze and having the additional measurements of how the oscillation process varies with neutrino energy.\nThe energy-dependence measurements also open a window to the new analysis that will help scientists tease out the miniscule differences among the three masses. From the KamLAND experiment in Japan, they already know that the difference, or “split,” between two of the three mass states is small. They believe, based on the MINOS experiment at Fermilab, that the third state is at least five times smaller or five times larger. Daya Bay scientists have now measured the magnitude of that mass splitting, |Δm2ee|, to be (2.59±0.20) x 10-3 eV2.\nThe result establishes that the electron neutrino has all three mass states and is consistent with that from muon neutrinos measured by MINOS. Precision measurement of the energy dependence should further the goal of establishing a “hierarchy,” or ranking, of the three mass states for each neutrino flavor.\nMINOS, and the Super-K and T2K experiments in Japan, have previously determined the complementary effective mass splitting (Δm2μμ) using muon neutrinos. Precise measurement of these two effective mass splittings would allow calculations of the two mass-squared differences (Δm232 and Δm231) among the three mass states. KamLAND and solar neutrino experiments have previously measured the mass-squared difference Δm221 by observing the disappearance of electron antineutrinos from reactors about 100 miles from the detector and the disappearance of neutrinos from the sun.\nUC Berkeley and Berkeley Lab’s Bill Edwards, Daya Bay’s U.S. Project and Operations Manager, says, “The ability to measure these subtle effects with greater and greater precision is a testament to the scientific and engineering team that designed and built this exceptional experiment.”\nU.S. scientists are also laying the groundwork for a future neutrino project, the Long-Baseline Neutrino Experiment (LBNE). This experiment would use high intensity accelerators at Fermi National Accelerator Laboratory to produce high-energy muon neutrinos and aim them at detectors 1,300 kilometers away in South Dakota, a distance from neutrino source to detector needed to observe the transformations of high-energy muon neutrinos. LBNE would detect the appearance of the other two flavors at the far-away detector in addition to the disappearance of one flavor of neutrino as evidence of oscillation. The combined results from LBNE and other global neutrino experiments will give scientists new ways to test for violations of fundamental symmetries, and open other avenues to understanding the structure of the universe today.\n- For more about the international Daya Bay Collaboration go here\n- Read the news release on the Collaboration’s first results here\n- View a slide show of the construction of the experiment here\n- For more about CP violation go here\nImages: Roy Kaltschmidt"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_language_proficiency_implied","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:49700568-45c2-40da-b6ef-cf1133877014>"],"error":null}
{"question":"How did a Maori journalist learn Mandarin during their time in Hong Kong?","answer":"The Maori journalist learned Mandarin by strenuously studying Mandarin textbooks and forcing themselves to speak the language while serving as a missionary in Hong Kong for 18 months. Despite being the hardest thing they had ever done, their effort to speak Mandarin helped Chinese people open their doors and hearts to them. The process demonstrated the Maori saying 'ko te reo te taikura ō te whakaaro marama' – language is the key to understanding.","context":["Shilo Kino, a Māori writer who wove her own ties with China, rejoices at the release of Rose Lu’s debut, All Who Live on Islands.\nI first met Rose Lu when she stood up to introduce herself at an Asia Leadership Network meeting earlier this year. We were all newbies, a group of under-30s chosen to be “young leaders” in an organisation working to strengthen Asia’s relationship with New Zealand.\nEach of us introduced ourselves and explained our connection to Asia. There were young business owners, CEOs, lawyers – you know, the kind of people who make you feel like you aren’t doing enough with your life.\nA Chinese girl wearing bold lipstick stood up. Her name was Rose. She said she had written a book based on her experiences of growing up as a Chinese immigrant in New Zealand and it was due out this year. A stunned silence and then a deafening round of applause – the loudest cheer coming from me. I was desperate to read this book.\nSo what’s my connection with China? I am tangata whenua – Ngāpuhi and Tainui – with no Chinese tupuna. I grew up in Waipu, a small town in Northland with a population of just over 1,500. The majority of the population was Pākehā and Māori, with the exception of one Chinese family who ran the local fish and chip shop. Magic Tasty was revolutionary, introducing the locals to fried rice and chicken chow mein. It was the first time most of us had tasted Chinese food and it was also the first time most of us had interacted with Chinese people.\nIn 2015, when I was 25, I left my job as a journalist for Fairfax Media and embarked on an 18 month volunteer mission for The Church of Jesus Christ of Latter-day Saints in Hong Kong.\nYou can only imagine how it felt for me, growing up in a small town like Waipu and then living in a place like Hong Kong, where trains travelled at the speed of light, massive buildings seemed to reach the clouds, and herds of people moved around in a frantic dash, speaking a language I could not understand.\nAs a missionary, I mostly interacted with and shared my faith with the people from Mainland China, which meant I had to learn Mandarin. Learning Mandarin was hard, the hardest thing I have ever done. There were many days I saw the language as simply a barrier.\nBut there is a saying in Māori, ko te reo te taikura ō te whakaaro marama – language is the key to understanding. As I strenuously studied my Mandarin textbooks, and forced myself to open my mouth and speak Mandarin, the Chinese people saw the effort I was making to understand them and so they opened their doors and their hearts. I began to love these people – not just as friends, but as family. I truly believe that with the embracing of language comes the embracing of culture.\nWhen I came home to Tauranga, I went to both the Mount Maunganui and Papamoa libraries, searching out fiction and nonfiction literature on Chinese people in New Zealand. I wanted to understand the Chinese experience in this country, read up on the history, and hear a different perspective – a perspective that can come only from Chinese people living here.\nWhat I found was a tiny section – about 10 books – right next to the section on Europe. The only local, as in New Zealand, books in it were by University of Auckland emeritus professor Manying Ip. I mean, I was ecstatic to find those. Ip has done amazing work on analysing the relationship between Chinese and Māori, something I am hugely interested in. But I was left wondering where the other Chinese Kiwi books were. Had I not looked hard enough? And why were Ip’s books lumped in between Amy Tan (The Joy Luck Club) and the story of some white American dude’s experience living in China?\nEnter Rose Lu, a 29-year-old software engineer now based in Wellington. She’s the daughter of Chinese immigrants who own a dairy and takeaway shop in Whanganui. In All Who Live On Islands, her debut essay collection, Lu writes with precision and grace about her grandmother inspecting peaches. She writes about working at the Mad Butcher and about very bad high school sex. About yoga and MSN and tramping in the Himalayas.\nAs Nina Mingya Powles noted in her launch speech, Lu incorporates Hanzi and Pinyin without italics throughout, writing in an endnote: “For those of us who have grown up with several languages, this signifier is meaningless.”\nPowles, a poet and writer, was born in Wellington and is of Chinese-Malaysian heritage. “I tried to imagine what it would have been like for me if this book had existed when I took my very first undergrad creative writing workshop when I was 19,” she said. “There had been Asian-NZ writers published long before, but I didn’t know yet how to find them. I would have been less afraid to write about my Asianness. I would have allowed myself to exist in that workshop space unapologetically as the only Asian in the room.”\nLu is a pioneer. That bold lipstick matches her bold voice in a whitewashed publishing industry. She is paving the way for a new generation of writers. But, she tells me, there’s still a long way to go.\n“I can name all of the other Chinese New Zealand authors, which is great, but there should be so many that I can’t do that. I can’t name all the Pākehā writers and I don’t even think I can name all the Māori writers.”\nI think back to that shelf packed with books on the Asian American experience. Why has it taken so long for our stories to emerge?\n“There’s this entire structure set up to help Asian American writers succeed and nothing like that really exists here in New Zealand,” Lu says. “It’s not that I don’t think Asian Kiwis are given opportunities, it’s just that there’s not that many opportunities in New Zealand.”\nShe also points out that migration patterns have an influence. “There was the gold rush generation [and] the fruit shop generation, and both those generations had to work so hard and probably didn’t have time to write books. With my generation, it’s been only in the last five years where we are getting to an age where the children of the people who migrated are entering their twenties and thirties.”\nIn All Who Live on Islands, Lu explores growing up in small rural towns where she and her family were often the only Chinese. She writes about prejudice, isolation, expectations, family dynamics. But it was the internalised racism, Lu says, that was the great silent battle.\n“When I was living in Auckland, it was easier to be passively Chinese because I had Asian friends and I felt a bit more protected. But I spent more time in places where there wasn’t a big Asian population and you do feel kind of exposed. A lot of my friends were Pākehā because I lived in Pākehā towns.\n“I think I had a lot of internalised racism because I didn’t really understand or appreciate my Chinese culture. I think that’s something I could have let go of, or worked through at an earlier age, if I knew it was okay to have those kind of feelings. We talk a lot about racism, but in terms of internal racism? That’s such a subtle thing.”\nLu’s relationship with her grandparents weaves throughout the essay collection and is perhaps the most endearing aspect of it. In the opening chapter, for example, we meet Kon-kon and Bu’uah and follow the couple as they go shopping with Lu at Pak’n’Save – or as they call it, “the poor people shop”.\n“Whenever I talk to people about my family, Pākehā are always surprised my grandparents live with us,” Lu says. “But this is a normal thing in my culture.”\nBut the central role of her grandparents in these essays goes far beyond a sweet family relationship. Lu is paying homage, and making us see the people we so often ignore. “[Older migrants] are a voiceless community because many of them are at an age where they are way too old to proficiently learn a language. They’re a big block of people here but there’s no one really talking about them and their lives. But for my family, we couldn’t have gotten here without the support of my grandparents. The amount they have given to us is invisible.”\nLu wrote All Who Live on Islands while studying a Masters of Arts in Creative Writing at the International Institute of Modern Letters, where she won the Creative Nonfiction Prize. She’d been working as a software developer for six years before starting the course, after swerving off the med school path that seemed set for her.\n“It just seemed impossible to me I could do something like writing; I didn’t even consider it as an option,” she says. “From a really young age, an idea that was really drilled into me was that work needed to be stable, dependable and skilled. You don’t have to necessarily like your work, but this is how my mum and dad got by. I chose engineering school because it seemed like the least worst thing I could be doing. I mean, I really like my job now so it turned out okay.”\nRose worked part-time while studying and writing – the book took upwards of 15 months. Her favourite essay in the collection is the final one, written for her younger brother Matthew. ‘The Tiger Cub’ is a story about exams and expectations and depression. “Some of the stuff I write about is big stuff I’ve been thinking for years and processing in the background,” Lu says.\nIn the acknowledgements, she jokes that the target audience for this book is “1.5 generation Chinese migrants who have grown up in the regions”.\nShe’s being self-deprecating, of course. This is one of the most buzzed-about local books of the year.\nFor some it will function as an eye-opener, Lu thinks. “I just know there will be people that will read this and it might be one of only a handful of books from an Asian writer they will read in a lifetime.”\nFor others, like me and Nina Mingya Powles, it means much more.\n“Reading these essays felt like I’d jumped into very cold water and come up laughing,” Powles said. “It felt like my heart had been opened.”\nAll Who Live on Islands, by Rose Lu (Victoria University Press, $30) is available at Unity Books.\nThe Spinoff Weekly compiles the best stories of the week – an essential guide to modern life in New Zealand, emailed out on Monday evenings."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f4622efa-5868-466d-98bf-bf535660e0d3>"],"error":null}
{"question":"How does the diffusion rate compare between water molecules and proteins when crossing capillary walls versus the transport of respiratory gases in the bloodstream?","answer":"Water molecules diffuse through capillary walls at a rate 80 times faster than plasma flow, while proteins have very limited permeability (about 1/1000 that of water) due to their large molecular size compared to the 6-7 nanometer pore width. In contrast, respiratory gases have different transport dynamics in the bloodstream: oxygen is 98.5% bound to hemoglobin with minimal direct dissolution, while carbon dioxide is more soluble and uses three transport methods - dissolution (5-7%), hemoglobin binding (10%), and bicarbonate conversion (85%).","context":["- Describe how oxygen is bound to hemoglobin and transported to body tissues\n- Explain how carbon dioxide is transported from body tissues to the lungs\nOnce the oxygen diffuses across the alveoli, it enters the bloodstream and is transported to the tissues where it is unloaded, and carbon dioxide diffuses out of the blood and into the alveoli to be expelled from the body. Although gas exchange is a continuous process, the oxygen and carbon dioxide are transported by different mechanisms.\nTransport of Oxygen in the Blood\nAlthough oxygen dissolves in blood, only a small amount of oxygen is transported this way. Only 1.5 percent of oxygen in the blood is dissolved directly into the blood itself. Most oxygen—98.5 percent—is bound to a protein called hemoglobin and carried to the tissues.\nHemoglobin, or Hb, is a protein molecule found in red blood cells (erythrocytes) made of four subunits: two alpha subunits and two beta subunits (Figure 39.19). Each subunit surrounds a central heme group that contains iron and binds one oxygen molecule, allowing each hemoglobin molecule to bind four oxygen molecules. Molecules with more oxygen bound to the heme groups are brighter red. As a result, oxygenated arterial blood where the Hb is carrying four oxygen molecules is bright red, while venous blood that is deoxygenated is darker red.\nIt is easier to bind a second and third oxygen molecule to Hb than the first molecule. This is because the hemoglobin molecule changes its shape, or conformation, as oxygen binds. The fourth oxygen is then more difficult to bind. The binding of oxygen to hemoglobin can be plotted as a function of the partial pressure of oxygen in the blood (x-axis) versus the relative Hb-oxygen saturation (y-axis). The resulting graph—an oxygen dissociation curve—is sigmoidal, or S-shaped (Figure 39.20). As the partial pressure of oxygen increases, the hemoglobin becomes increasingly saturated with oxygen.\nThe kidneys are responsible for removing excess H+ ions from the blood. If the kidneys fail, what would happen to blood pH and to hemoglobin affinity for oxygen?\nFactors That Affect Oxygen Binding\nThe oxygen-carrying capacity of hemoglobin determines how much oxygen is carried in the blood. In addition to , other environmental factors and diseases can affect oxygen carrying capacity and delivery.\nCarbon dioxide levels, blood pH, and body temperature affect oxygen-carrying capacity (Figure 39.20). When carbon dioxide is in the blood, it reacts with water to form bicarbonate and hydrogen ions (H+). As the level of carbon dioxide in the blood increases, more H+ is produced and the pH decreases. This increase in carbon dioxide and subsequent decrease in pH reduce the affinity of hemoglobin for oxygen. The oxygen dissociates from the Hb molecule, shifting the oxygen dissociation curve to the right. Therefore, more oxygen is needed to reach the same hemoglobin saturation level as when the pH was higher. A similar shift in the curve also results from an increase in body temperature. Increased temperature, such as from increased activity of skeletal muscle, causes the affinity of hemoglobin for oxygen to be reduced.\nDiseases like sickle cell anemia and thalassemia decrease the blood’s ability to deliver oxygen to tissues and its oxygen-carrying capacity. In sickle cell anemia, the shape of the red blood cell is crescent-shaped, elongated, and stiffened, reducing its ability to deliver oxygen (Figure 39.21). In this form, red blood cells cannot pass through the capillaries. This is painful when it occurs. Thalassemia is a rare genetic disease caused by a defect in either the alpha or the beta subunit of Hb. Patients with thalassemia produce a high number of red blood cells, but these cells have lower-than-normal levels of hemoglobin. Therefore, the oxygen-carrying capacity is diminished.\nTransport of Carbon Dioxide in the Blood\nCarbon dioxide molecules are transported in the blood from body tissues to the lungs by one of three methods: dissolution directly into the blood, binding to hemoglobin, or carried as a bicarbonate ion. Several properties of carbon dioxide in the blood affect its transport. First, carbon dioxide is more soluble in blood than oxygen. About 5 to 7 percent of all carbon dioxide is dissolved in the plasma. Second, carbon dioxide can bind to plasma proteins or can enter red blood cells and bind to hemoglobin. This form transports about 10 percent of the carbon dioxide. When carbon dioxide binds to hemoglobin, a molecule called carbaminohemoglobin is formed. Binding of carbon dioxide to hemoglobin is reversible. Therefore, when it reaches the lungs, the carbon dioxide can freely dissociate from the hemoglobin and be expelled from the body.\nThird, the majority of carbon dioxide molecules (85 percent) are carried as part of the bicarbonate buffer system. In this system, carbon dioxide diffuses into the red blood cells. Carbonic anhydrase (CA) within the red blood cells quickly converts the carbon dioxide into carbonic acid (H2CO3). Carbonic acid is an unstable intermediate molecule that immediately dissociates into bicarbonate ions and hydrogen (H+) ions. Since carbon dioxide is quickly converted into bicarbonate ions, this reaction allows for the continued uptake of carbon dioxide into the blood down its concentration gradient. It also results in the production of H+ ions. If too much H+ is produced, it can alter blood pH. However, hemoglobin binds to the free H+ ions and thus limits shifts in pH. The newly synthesized bicarbonate ion is transported out of the red blood cell into the liquid component of the blood in exchange for a chloride ion (Cl-); this is called the chloride shift. When the blood reaches the lungs, the bicarbonate ion is transported back into the red blood cell in exchange for the chloride ion. The H+ ion dissociates from the hemoglobin and binds to the bicarbonate ion. This produces the carbonic acid intermediate, which is converted back into carbon dioxide through the enzymatic action of CA. The carbon dioxide produced is expelled through the lungs during exhalation.\nThe benefit of the bicarbonate buffer system is that carbon dioxide is “soaked up” into the blood with little change to the pH of the system. This is important because it takes only a small change in the overall pH of the body for severe injury or death to result. The presence of this bicarbonate buffer system also allows for people to travel and live at high altitudes: When the partial pressure of oxygen and carbon dioxide change at high altitudes, the bicarbonate buffer system adjusts to regulate carbon dioxide while maintaining the correct pH in the body.\nCarbon Monoxide Poisoning\nWhile carbon dioxide can readily associate and dissociate from hemoglobin, other molecules such as carbon monoxide (CO) cannot. Carbon monoxide has a greater affinity for hemoglobin than oxygen. Therefore, when carbon monoxide is present, it binds to hemoglobin preferentially over oxygen. As a result, oxygen cannot bind to hemoglobin, so very little oxygen is transported through the body (Figure 39.22). Carbon monoxide is a colorless, odorless gas and is therefore difficult to detect. It is produced by gas-powered vehicles and tools. Carbon monoxide can cause headaches, confusion, and nausea; long-term exposure can cause brain damage or death. Administering 100 percent (pure) oxygen is the usual treatment for carbon monoxide poisoning. Administration of pure oxygen speeds up the separation of carbon monoxide from hemoglobin.","![if !IE]> <![endif]>\nExchange of Water, Nutrients, and Other Substances Between the Blood and Interstitial Fluid\nBy far the most important means by which substances are transferred between the plasma and the interstitial fluid is diffusion. Figure 16–3 demonstrates this process, showing that as the blood flows along the lumen of the capillary, tremendous numbers of water molecules and dissolved particles diffuse back and forth through the capillary wall, providing continual mixing between the interstitial fluid and the plasma.\nDiffusion results from thermal motion of the water molecules and dissolved substances in the fluid, thedifferent molecules and ions moving first in one direc-tion and then another, bouncing randomly in every direction.\nLipid-Soluble Substances Can Diffuse Directly Through the Cell Membranes of the Capillary Endothelium. If a substance islipid soluble, it can diffuse directly through the cell membranes of the capillary without having to go through the pores. Such substances include oxygen and carbon dioxide. Because these substances can per-meate all areas of the capillary membrane, their rates of transport through the capillary membrane are many times faster than the rates for lipid-insoluble substances, such as sodium ions and glucose that can go only through the pores.\nWater-Soluble, Non-Lipid-Soluble Substances Diffuse Only Through Intercellular “Pores” in the Capillary Membrane.\nMany substances needed by the tissues are soluble in water but cannot pass through the lipid membranes of the endothelial cells; such substances include watermolecules themselves, sodium ions, chloride ions, and glucose. Despite the fact that not more than 1/1000 ofthe surface area of the capillaries is represented by the intercellular clefts between the endothelial cells, the velocity of thermal molecular motion in the clefts is so great that even this small area is sufficient to allow tremendous diffusion of water and water-soluble sub-stances through these cleft-pores. To give one an idea of the rapidity with which these substances diffuse, therate at which water molecules diffuse through the cap-illary membrane is about 80 times as great as the rate at which plasma itself flows linearly along the capillary.That is, the water of the plasma is exchanged with the water of the interstitial fluid 80 times before the plasma can flow the entire distance through the capillary.\nEffect of Molecular Size on Passage Through the Pores.\nThe width of the capillary intercellular cleft-pores, 6 to 7 nanometers, is about 20 times the diame-ter of the water molecule, which is the smallest molecule that normally passes through the capillary pores. Conversely, the diameters of plasma protein molecules are slightly greater than the width of the pores. Other substances, such as sodium ions, chloride ions, glucose, and urea, have intermediate diameters. Therefore, the permeability of the capillary pores for different substances varies according to their molecu-lar diameters.\nTable 16–1 gives the relative permeabilities of the capillary pores in skeletal muscle for substances com-monly encountered, demonstrating, for instance, that the permeability for glucose molecules is 0.6 times that for water molecules, whereas the permeability for albumin molecules is very, very slight, only 1/1000 that for water molecules.\nA word of caution must be issued at this point. The capillaries in different tissues have extreme dif-ferences in their permeabilities. For instance, the mem-brane of the liver capillary sinusoids is so permeable that even plasma proteins pass freely through these walls, almost as easily as water and other substances. Also, the permeability of the renal glomerular mem-brane for water and electrolytes is about 500 times the permeability of the muscle capillaries, but this is not true for the plasma proteins; for these, the capillary permeabilities are very slight, as in other tissues and organs. When we study these different organs later in this text, it should become clear why some tissues—the liver, for instance—require greater degrees of capillary permeability than others to transfer tremendous amounts of nutrients between the blood and liver parenchymal cells, and the kidneys to allow filtration of large quantities of fluid for formation of urine.\nEffect of Concentration Difference on Net Rate of Diffusion Through the Capillary Membrane. The “net” rate of diffu-sion of a substance through any membrane is propor-tional to the concentration difference of the substance between the two sides of the membrane. That is, the greater the difference between the concentrations of any given substance on the two sides of the capillary membrane, the greater the net movement of the sub-stance in one direction through the membrane. For instance, the concentration of oxygen in capillary blood is normally greater than in the interstitial fluid. Therefore, large quantities of oxygen normally move from the blood toward the tissues. Conversely, the con-centration of carbon dioxide is greater in the tissues than in the blood, which causes excess carbon dioxide to move into the blood and to be carried away from the tissues.\nThe rates of diffusion through the capillary membranes of most nutritionally important substances are so great that only slight concentration differences suffice to cause more than adequate transport between the plasma and interstitial fluid. For instance, the con-centration of oxygen in the interstitial fluid immedi-ately outside the capillary is no more than a few per cent less than its concentration in the plasma of the blood, yet this slight difference causes enough oxygen to move from the blood into the interstitial spaces to provide all the oxygen required for tissue metabolism, often as much as several liters of oxygen per minute during very active states of the body.\nCopyright © 2018-2023 BrainKart.com; All Rights Reserved. Developed by Therithal info, Chennai."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c96c2764-bb5b-4b13-816d-f5ac6982ecc1>","<urn:uuid:09347250-bac0-40f1-a5c5-b09f3adf9f5a>"],"error":null}
{"question":"What is the difference between how the BEXUS II balloon and the Perlan II glider are launched and powered during flight?","answer":"The BEXUS II balloon is a zero pressure balloon that operates independently using its 10,000 m3 volume, while the Perlan II glider must be pulled into the air by a powered airplane and released at around 3,000 meters. After release, the glider relies on warm, rising air and mountain waves to gain altitude, while having no engine of its own. In contrast, the balloon does not require external aircraft assistance for launch or altitude gain.","context":["Description of the payload\nSometime before a balloon campaign starts at Esrange every year is launched a balloon to test their technique and logistics systems. The students at Kiruna's different space related educations are offered the opportunity to send their own designed and built experiments with the balloon. The students are from the space engineering programmes at the Department of Space Science and from the technical high school programs.\nStudents from different years and courses are involved in the project, which means that different groups of students meet and get a chance to work together on the basis of their respective areas of competence. It is the students themselves who lead, design and build the experiments and the instruments destined for the balloon launch.\nMost of the data to be collected during the balloon flight is used later in the courses.\nThis was the second flight of the project and involved the participation of students at Kiruna Space and Environment Campus togheter with pupils from Hjalmar Lundboms school and the space high school, Rymdgymnasiet and also the collaboration from the University of Copenhagen.\nDetails of the balloon flight and scientific outcome\nLaunch site: European Space Range, Kiruna, Sweden\nBalloon launched by: Swedish Space Corporation (SSC)\nBalloon manufacturer/size/composition: Zero Pressure Balloon 10.000 m3\nThe balloon was successfully launched at 11:57 UTC on Febraury 26th and reached a float altitude of 27,6 km at 13:30 utc.\nAfter a flight of near 5 hours due to east, the cut down command was given and the landing took place in the Northern Part of Finland near the Russian border.\nThe experiences performed in this second flight were:\nThe main plataform called BEXUS II provides support to the rest of experiences. It includes: high definition sampling of analogue signals, serial platform bus for handling of commands and data between ground station and the experiments, GPS-system localization and time, power switching, measuring and distribution, power supply with batteries and a ground station segment for monitoring and controling the BEUXS II platform and experiments.\nThe experiments to be performed were:\nVHI (verification of harddrive integrity) to evaluate the use of a hard drive to store data. This would then eliminate the use of a radio link for experiments that produces large amounts of data.\nChemical Power High Altitude Experiment to study the behavior of Fuel cells and Zinc-air batteries under different atmospheric conditions.\nGAMMA-RAY PROJECT that used a Geiger Mueller tube with a good time resolution to detect Gamma Ray Bursts.\nBASS (Bexus Active Stabilizer System) a platform which will stabilize a camera.\nThe project ”solar panels” to test how much power 4 different solar cells deliver depending on temperature, pressure and amount of light.\nNAC (Narrow Angle Camera) originally designed for an European satellite project to take pictures in flight by remote command.\nG-spot to measure the attitude of the balloon and its payload as well as the shocks during lift-off and landing with 8 accelerometers.\nHEAL (2-Hour Extreme Altitude Life-analysis) were organism called tardigrades will be sent up with the balloon to be exposed to the extreme environments that can be found along the journey to investigate how the environment has affected its DNA.\nTest of the E-Link system with maximum data load versus achieved flight range.\nExternal references and bibliographical sources","Powerless Aircraft Sets Altitude Record\nTwo pilots recently set a new altitude world record for gliders.\nA glider is an aircraft that does not have an engine. They are pulled into the air by powered airplanes and then released. They are designed to ride warm, rising air to gain altitude.\nThe Perlan II glider reach an altitude of 15,902 meters on September 3. The two pilots flew their aircraft above mountains in southern Argentina. The pilots beat the earlier world record for gliders by 441 meters.\nThe pilots hope the ability of their plane to reach the edge of outer space will influence young people to follow careers in science and engineering.\nReaching new heights\nThe glider was pulled into the air by a small plane. At about 3,000 meters, the plane released the connecting cable and started looking for strong updrafts of air, called the mountain waves, to take them higher.\nAccording to chief pilot Jim Payne it took eleven flights to reach the altitude record.\n\"Basically, we take steps of certain amount of altitude and airspeed, gather data, analyze that data and if those data conform to our theory and the model we have for the airplane - then we can safely go on to the next step.\"\nA glider flight at high altitude is very difficult. The glider has no engine to fight turbulence they may encounter.\nAlso, although the airspeed instrument shows the speed of only 80 to 90 kilometers per hour, the true airspeed can be much higher. At those high speeds, the wings may flutter to the point of breaking.\nAlso, it is very cold said pilot and project manager Morgan Sandercock.\n\"We have electric socks on the pilots, we literally could not fly without those electric socks, we would get frostbite. And heated vests and we're looking at upgrading our heating systems for next year because cold is the big issue that we've been dealing with this year.\"\nInvesting in the future\nThe Perlan II glider is made of special kind of light carbon fiber material. It also has improved mechanical properties. The company Airbus helped build the glider and transport it to Argentina.\nKen McKenzie, an employee with Airbus, says the company hopes the project will attract young people to science and technology.\n\"We want to actually grab the next group of pilots, engineers, mechanics, airport operators, flight attendants, basically kids that are in high school and university right now that may not have considered aerospace as a career and we want to inspire them.\"\nIn the future, the pilots hope to break the world altitude record for level flight of 25,929 meters, set in 1979 by the U.S. spy plane called the SR-71 Blackbird.\nThe pilots also plan to turn their aircraft into a non-polluting research vehicle for the stratosphere.\nI'm Phil Dierking.\n1.the world record 世界纪录\nHe has knocked 10 seconds off the world record.\n2.fiber material 纤维材料\nFiber material processing is an important branch of materials processing.\n3.right now 立刻；马上\nIf you have a problem with that, I want you to tell me right now.\n4.heating systems 加热系统\nNew irrigation and undersoil heating systems were also installed at the same time as the2003 work.\n1.The pilots hope the ability of their plane to reach the edge of outer space will influence young people to follow careers in science and engineering.\nouter space 外太空\nThey have been visited by creatures from outer space.\nExploring outer space is a challenge to mankind.\n2.And heated vests and we're looking at upgrading our heating systems for next year because cold is the big issue that we've been dealing with this year.\"\ndealing with 处理；应付\nWe came up against a great deal of resistance in dealing with the case.\nWe know we're dealing with someone with a different frame of reference.\n滑翔机是一种无引擎的飞行器 。动力飞机将滑翔机拉升到空中后释放 。滑翔机的设计旨在通过上升的热气流带动来增加高度 。\n9月3日，Perlan II滑翔机达到了15902米的飞行高度 。两名飞行员驾驶飞行器成功飞越了阿根廷南部的群山 。两名飞行员打破了滑翔机飞行高度441米的世界纪录 。\nPerlan II滑翔机由一架小型飞机拉升至上空 。待攀升至3000米后，飞机松开绳索并等待上升气流（地形波）来带动滑翔机继续攀升 。\n首席飞行员吉姆·佩恩（Jim Payne）称，滑翔机攀升11次后才到达如今的高度 。\n滑翔机很难高空飞行 。它没有发动机，无法应对可能遇到的不稳气流 。\n此外，尽管空速仪显示速度仅为每小时80至90公里，但是真正的空速会快得多 。在高速飞行时，滑翔机机翼可能会震颤至断裂 。\n飞行员兼项目负责人摩根·桑德尔柯克（Morgan Sandercock）补充称，飞行时的温度很低 。\n“我们的飞行员会穿电袜子，如果没有电袜子，根本就无法飞行，我们会被冻伤 。还有加热背心 。明年我们会升级加热系统，因为寒冷是我们今年需要解决的一大难题 。”\nPerlan II滑翔机由特殊的轻薄碳纤维材料制成 。其机械性能也得到改进 。空中客车公司制造了这一滑翔机并将其运往阿根廷 。\n空中客车的职员肯恩·麦肯齐（Ken McKenzie）表示，空客希望该项目能吸引年轻人关注科学技术领域 。"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7a9f2603-d4dc-4c97-9bc9-f5be1d6a01c0>","<urn:uuid:82c640fe-2e86-4f1a-b780-7ee752213589>"],"error":null}
{"question":"What are the primary medications used for treating dopamine-related conditions like Parkinson's disease and methamphetamine addiction, and what are their effects?","answer":"For Parkinson's disease, the main medications include Levodopa (combined with AADC blockers), which transforms into dopamine in the brain to supplement natural dopamine levels; dopamine agonists that directly stimulate dopamine receptors; and enzyme blockers (MAO-B and COMT inhibitors) that prevent dopamine breakdown. For methamphetamine addiction, while no medications are FDA-approved specifically for treatment, several drugs show promise in clinical trials. These include Buproprion, which may reduce meth use and cravings by affecting dopamine and norepinephrine levels; Naltrexone, which blocks meth-induced dopamine release; and medications like Modafinil and Mirtazapine, which when combined with behavioral therapy may help reduce meth use.","context":["Anti Parkinsonian drugs\nThis section of our med store is dedicated to medicinal products aimed to minimize the symptoms of Parkinson’s disease and Parkinsonism (the set of the manifestations typical for Parkinson’s and other conditions with similar manifestations).\nIt is a slowly progressing chronic neurologic disease mostly characteristic for people of elderly age. This is a degenerative illness of the extrapyramidal motor system induced by the accelerating destruction of neurons that release dopamine which is one of the most important neurotransmitters, primarily in the substantia nigra, and other areas of the central nervous system (CNS). The insufficient release of the substance induces to the activating impact of basal ganglia on the cortex of the brain. The main signs of the condition onset are: muscle stiffness; hypokinesia (the state of insufficient motor activity of the organism with the restriction of the tempo and volume of movements); tremor; and posture violation.\nUnfortunately, to date, the disease is impossible to be cured. It means that the manifestations can be minimized but not completely eliminated if you buy antiparkinsonian drugs.\nThe existing methods of medicinal and surgical treatment can significantly improve the quality of life of individuals with the disease and slow down the development of it.\nThe term Parkinsonism is a general notion used for a number of conditions and disease with the above-listed major symptoms. But the most notorious form of the disorder is Parkinson’s disease which is an idiopathic condition, i.e. the autonomous disease not associated with the genetic factors or other diseases.\nThe name of the condition was suggested by the French neurologist Jean-Martin Charcot as a tribute to the British physician and author of “The essay on the tremulous paralysis”, James Parkinson, whose work was not properly appreciated during his lifetime\nWhat are antiparkinsonian medications?\nThe medicinal formulations used for Parkinsonism are aimed at elevating the dopamine compound in the CNS as its low levels are the major cause of the symptoms. Dopamine along with acetylcholine, serotonin, and norepinephrine is the most important neurotransmitter. Its main effect is manifested in the striatum on predominantly postsynaptic D1 and D2 receptors. Re-seizure of dopamine in the presynaptic endings and its inactivation proceeds via dopamine transports.\nSubsequently, dopamine is destroyed by the action of monoamine oxidase type B and catechol-O-methyltransferase. The lack of dopamine leads to an overabundance of acetylcholine and a lack of serotonin and norepinephrine which results in a violation of coordinated movements or the onset of unintentional, involuntary movements.\nThe elevation dopamine level is made in several ways: by supplementary feeding to the brain of dopamine or substances acting as dopamine; a blockade of enzymes that promote the breakdown of the neurotransmitter (COMT-, MAO-B-, NMDA-blockers), and a lowering of the elevated levels of acetylcholine (anticholinergics).\nClassification of antiparkinsonian medicines\nWithin the framework of medical management of Parkinson's disease the following medicinal preparations are utilized:\n1. Levodopa along with aromatic L-amino acid-decarboxylase (AADC) blockers;\n2. Dopamine agonists (substances that promote the release of dopamine);\n3. Blockers of enzymes inducing the destruction of dopamine;\n4. Drugs that act on the manifestations of the disease are not through dopamine mechanisms: amantadine and anticholinergic medicines.\nLevodopa is a direct predecessor of dopamine. After entering the blood from the bowel, it then penetrates the CNS. In the striatal system of the brain, the preparation is perceived by the remaining dopaminergic nerve endings and, with the aid of decarboxylase, is transformed into dopamine. The substance formed this way supplements the dopamine produced by the organism and does not differ in its properties from the endogenous one.\nThe effective dosages of the preparation necessary lead to the onset of pronounced peripheral actions - nausea and elevated arterial pressure. Therefore, levodopa is utilized exclusively along with a peripheral decarboxylase blocker (benserazide or carbidopa). At our med store, we offer you to buy antiparkinsonian drugs based such as Carbidopa plus Levodopa. The medication has biological availability of around 100 percent. The max level in the blood is reached in 30-60 minutes.\nDopamine agonists render stimulating effect directly on dopamine receptors. They do not turn into active forms of dopamine, like Levodopa. All dopamine agonists exert their effect through the stimulation of D2 and D3 receptors, the drugs differ, however, from each other in their effect on other dopamine receptors, duration of action, and adverse reactions. Dopamine agonists are an important part of Parkinson's therapy. Tests have shown that if the therapy of Parkinsonism begins with the utilization of dopamine agonists or their combination with preparations with Levodopa in their composition, the development of dyskinesia and other motor complications occurs somewhat later. The subject of discussion is also the possible protective effect of dopamine agonists on dopamine-producing nerve cells. On the other hand, due to the simultaneous binding of peripheral dopamine receptors, dopamine agonists may cause more severe side effects than Levodopa, namely: orthostatic circulation disorder and nausea, less often leg edema, dopamine-induced psychoses, impulse control impairments, and fatigue.\n10 dopamine agonists (5 ergoline and 5 nonergolinic derivatives) are allowed on the market. Non-agroline preparations are first choice medicines, these are oral preparations for piribedil, pramipexole, and ropinirole, a transdermal patch rotigotine, and parenteral apomorphine. Ergoline derivatives due to their side effects (increased risk of pleuropulmonary, retroperitoneal and cardiac fibrosis) are second choice drugs. These include bromocriptine, cabergoline, alpha-dihydroergocryptine, lisuride, and pergolide.\nBlockers of enzymes involved in the destruction of dopamine\nPreparations of this group block enzymes that catalyze the metabolism of dopamine, thereby increasing its level. These include inhibitors MAO-B inhibitors.\nCatechol-O-methyltransferase inhibitors (COMT)\nIn this group, two preparations are known on the market: entacapone and tolcapone. It is possible to utilize them along with levodopa. At the end of 2003, a combined preparation containing levodopa, Carbidopa, and entacapone had been introduced onto the market.\nMonoamine Oxidase Type B Inhibitors (MAO-B)\nThe preparations of this group reduce the cerebral oxidant metabolism of dopamine and therefore elevate its level in the striatal (possibly also in the extrestrial) system. There are two drugs on the market with selective inhibition of B-type monoamine oxidase: selegiline is the first-generation medication; and rasagiline, a second-generation medication, the advantage of which is the rapid achievement of maximum plasma concentration within one to two hours after administration and the sufficiency of one intake a day.\nDrugs that act on the manifestations of the condition not through dopamine mechanisms\nThis group includes amantadine and budipin.\nAmantadine - its effect is due to incomplete blockade of NMDA receptors, which causes anticholinergic action. It Produced in the form of hydrochloride (for parenteral use) or sulfate (for oral use). It is utilized both in mono- and in combined therapy. Due to the ability to use it parenterally, it is used in the therapy of akinetic crises. But the medication has a serious adverse effect which is the causing of psychosis and confusion, especially in old and multimorbid (suffering from multiple diseases) individuals with dementia - which limits the use of the preparation in this group.\nBudipin has a mechanism similar to amantadine. In small uncontrolled studies, his positive effect on tremor was shown. But due to the risk of occurrence of the QT-interval elongation, periodic ECG monitoring is required.\nPreparations of this group are the very first in the group of antiparkinsonian. They have been used since 1860, then in the form of tinctures of belladonna. The market was released: biperiden, bornaprin, methixen and triexyphenidyl.\nAntiparkinsonian medications you can buy at RXShopMD\n- Sinemet, a combination product based on Carbidopa and Levodopa is an antiparkinsonian, dopaminergic medication that inhibits decarboxylase. It is used for the therapy of Parkinson's disease symptoms such as rigidity, tremor, dysphagia, sialorrhea, orthostatic instability of the body.\n- Trivastal based on Piribedil by Serdia Pharmaceuticals is an antiparkinsonian preparation, an agonist of dopamine receptors. The preparation renders stimulating effect on the dopamine receptors in the CNS, improves the blood inflow to brain tissues, their supply of oxygen, and cerebral metabolism, promotes the transfer of nerve impulses, raises the electrical activity of cortical neurons. The preparation is utilized for Parkinson’s disease and other degenerative conditions including dementia.\nPlease note that although we offer you to get to buy antiparkinsonian medications without Rx, it is still highly advised to follow the instructions of your doctor on the selection of the preparation and the therapy itself.","Crystal Meth Addiction Medications\nWhy Is It Hard to Break a Crystal Meth Addiction\nTreatment for Crystal Meth Addiction\nMedications That May Help\nMedication Use in Treatment Facilities\nMedications Can’t Substitute for Human Support\nThe Effects of Crystal Meth Addiction\nLearn More and Find Help\nMethamphetamine, often referred to as crystal meth, belongs to a broad class of drugs known as psychostimulants.\nCrystal meth has dominated media headlines because of its alarmingly addictive nature and its potentially destructive consequences for those who use it, as well as its impact on families and communities.\nMethamphetamine’s synthetically made chemical structure is much like other amphetamines – but is an even more potent central nervous system stimulant.\nThe high, or rush, elicited by methamphetamine use creates a rewarding sense of well-being due to the release of the neurotransmitter, dopamine – urging even first time users to repeat the experience time and again.\nWhy Is It So Hard to Break a Crystal Meth Addiction?\nWhy is it so hard to stop using crystal meth once you’ve started?\nPart of the reason is due to the primary neurotransmitter that gets released in the brain when using the drug: dopamine.\nDopamine – the Body’s Pleasure Chemical\nDopamine plays a role in personal motivation, pleasurable feelings and motor functions. The release of dopamine is thought to be responsible for the rewarding effects of many drugs that are abused. This surge in dopamine activity – combined with the drug’s relatively low cost – makes users of crystal meth highly susceptible to abuse, addiction and dependency.\nAs a result, crystal meth addiction can be a hard addiction to overcome. For both long-term and short-term users of crystal meth, the crash after using the drug can feel unbearable. The high release of dopamine means that after the meth rush, there is a relative depletion of active dopamine – leaving the user with intense cravings to go back to the drug and regain the high. For long-term users, brain chemistry can change so dramatically that even high doses of meth cannot release enough dopamine to produce desired levels of pleasure.\nTreatment for Crystal Meth Addiction\nIf you or someone you love struggles with crystal meth addiction, there is still hope. You can achieve complete recovery from crystal meth addiction through a comprehensive treatment plan.\nTreatment design will vary on an individual basis. However, commonly utilized treatment approaches for crystal meth addiction often include a combination of behavioral treatment strategies1:\n- Cognitive-behavioral therapy.\n- Contingency-management interventions (tangible rewards in exchange for maintaining sobriety).\n- Education for family members.\n- One-on-one counseling.\n- Drug tests.\n- 12-step support groups.\n- Support for participating in activities that are not related to drugs.\nSo Where Does Medication Fit In?\nThe FDA doesn’t currently approve of any specific medications for the treatment of crystal meth addiction.2\nSupportive medications, however, are meant to be used alongside therapy to help ease the intensity of the detoxification process – which typically includes a range of uncomfortable withdrawal symptoms. Withdrawal symptoms typically felt by meth users include3:\n- Intense desire for the drug.\nThe National Institute on Drug Abuse (NIDA) has made medication development for crystal meth addiction a priority, and NIDA’s National Drug Abuse Clinical Trials Network has made great strides to find medications that enhance the user’s ability to cope with meth withdrawal.4\nMedications That May Help Your Meth Addiction Recovery Efforts\nWhen withdrawal is supervised by professionals, you can usually receive medication treatment to soften the symptoms of both short-term detoxification and long-term psychological withdrawal. While meth is not currently FDA approved for treating meth addiction, research has been starting to show some specific medications that may become approved by the FDA in the near future for their abilities to help treat meth addiction.\nMedications Currently Under Study for Treating Crystal Meth Addiction\nTrial studies have been performed with many medications believed to possibly enhance long-term recovery success. However, more studies will be needed to confirm drug efficacy in the treatment of meth addiction, and each individual’s response to a given medication may somewhat vary. Some of the medications that have been studied so far include5:\nFor Reducing Meth Use:\n- Buproprion. This drug may reduce meth use in light meth users only.\n- Modafinil. This drug shows mixed results. One study has suggested that this drug – when combined with cognitive-behavioral therapy – may help reduce meth use. Other studies have not shown a lot of promise for this drug.\n- Naltrexone. More than one study have suggested that this drug has potential for reducing use and increasing abstinence of methamphetamine.\n- Mirtazapine. One study found that mirtazapine – alongside cognitive-behavioral therapy – was associated with significant reductions in meth use among a sample of men who have sex with men (MSM).\n- Topiramate. One study found topiramate to reduce overall meth use. Total abstinence from meth was not observed in conjunction with taking topiramate, however.\nFor Reducing Meth Cravings:\nA range of medications have been tested for reducing meth cravings, many of which showed no success in reducing meth cravings. Some of the medications that have shown more promise, however, include:\n- Dextroamphetamine. While this drug has not been shown to affect meth use, it has been shown to reduce meth cravings. Dextroamphetamine is itself an addictive stimulant, but is available as a prescription tablet – potentially facilitating closer medical vigilance and safer dosing while mitigate cravings.\n- Rivastigmine. Studies have suggested this drug might help reduce meth users’ desire for meth.\n- Buproprion. This drug has been correlated with reduced meth cravings.\n- Nicotine. Nicotine administration during meth withdrawal has show to reduce meth-seeking behavior in some individuals.\n- Naltrexone. Studies on naltrexone have appeared to reduce meth-seeking behavior – a possible indicator of its ability to reduce meth cravings.\nMedications that May Reduce Both Meth Use and Meth Cravings\nOut of the medications that have been studied so far, it appears that two of those drugs may show some promise in reducing both meth use as well as meth cravings:\n- Buproprion (including Wellbutrin). Available in several trade name formulations (including Wellbutrin and Zyban), buproprion is currently FDA-approved for major depression, seasonal affective disorder and smoking cessation. Buproprion may be one of the most publicized medications in aiding methamphetamine addiction. Buproprion’s mechanism of action is not completely understood, but it is believed to weakly inhibit uptake of norepinephrine and dopamine. This action results in increased amounts of norepinephrine and dopamine available in the body. As mentioned earlier, dopamine is one of the body’s primary “pleasure chemicals,” while norepinephrine is one of the body’s primary “fight or flight” chemicals. Buproprion’s efficacy in this capacity is reported to be pronounced only in light meth users.\n- Naltrexone. Naltrexone is currently FDA-approved for treating both alcohol and opioid drug dependence. It works as an opioid receptor antagonist – meaning that it competes with and blocks other drugs that would normally have an effect on opioid receptors. Naltrexone is believed to have some potential for helping with meth addiction by blocking meth-induced dopamine. More studies are still needed to evaluate naltrexone’s efficacy and role in treating meth addiction.5\nAlong with these promising medications, some researchers have also been working steadily on immunological treatments.6 This kind of treatment involves engineering antibodies to target methamphetamine in the bloodstream and bond to the molecules of meth. Researchers hope that this method could help halt meth overdose and other dangerous effects meth has on the brain and other organs. It would also negate the pleasurable rush of the drug.\nDiverting ADHD medications\nMedication Use in Treatment Facilities\nWhen you feel ready to really turn your life around and quit meth, you will want to begin considering the type of treatment facility you’d like to help you along your recovery journey. The two main types of treatment you will encounter are:\nOutpatient treatment is an option for some individuals who have less severe addictions and who don’t have any coexisting medical or mental health conditions. Many standard rehab facilities offer outpatient treatment services that let you intermittently check in with your healthcare provider for medications and therapy – while still being able to go home at night after treatment.\nWhile the intense symptoms of acute stimulant withdrawal may only last days to weeks, crystal meth is capable of creating such dramatic changes in brain chemistry over time that prolonged care is frequently beneficial for those seeking recovery. As a result, many crystal meth users maximize their chance of success by quitting their meth use under the watchful care found in an inpatient rehabilitation center.\nInpatient programs offer an immersive treatment environment, with access to medical care and support should it be required. Supportive medications can also be administered to help you navigate the unpleasant withdrawal period, potentially increasing the likelihood of long-term sobriety.\nInpatient Facility Types\nWhen you’ve decided to look into inpatient addiction treatment options, you’ll come across a few different types of treatment facilities, depending on your needs and budget:\n- Luxury rehab facilities offer residential addiction treatment alongside a wide range of desirable, high-end amenities to make your recovery process as comfortable as possible.\n- Executive rehab facilities provide private residential addiction treatment with accommodations and allowances that enable busy professionals to maintain an active involvement in their workplace throughout recovery.\n- Standard recovery programs also provide quality addiction treatment in a residential setting. While these facilities might not offer the luxuries of the aforementioned programs – they are generally the most affordable inpatient treatment option for those on more limited budgets.\nMedications Are No Substitute for Human Support\nAlthough medications have been proven to help with crystal meth addiction, they should be used in conjunction with – rather than as a substitute for – some of the primary meth treatment approaches that rely on human support. Group and individual counseling, cognitive-behavioral therapy, family involvement and addiction support groups all have their own substantial roles in contributing to one’s addiction recovery.\n- Group and individual counseling. Group and individual counseling are common and important aspects of most addiction treatment programs, as social support and one-on-one therapy both play important roles in successful recovery.\n- Cognitive-behavioral therapy (CBT). CBT should be an essential component to your recovery and is often incorporated into group and/or individual counseling. Although medications may aid in reducing cravings and may temporarily address other emotions involved in addiction, ongoing recovery requires resolving the root causes for your addiction and changing your mental and behavioral patterns. Cognitive-behavioral therapy can help you understand the triggers and situations that cause you to use meth and can help you learn how to cope with these cravings without meth. This type of therapy teaches real-life techniques for recovery.\n- Family involvement. Research has demonstrated that family involvement and support can improve the chances for a loved one to seek help and successfully comply to a treatment plan.7 Family members can offer invaluable support, often providing encouragement and motivation for the user to get sober. This support can lead the user towards a more dedicated recovery process.\n- Addiction support groups. Recovery success rates among those involved with a 12-step program tend to be higher than success rates among those who are not. Those in a support program are much less likely to relapse. Involvement in a 12-step group – such as Alcoholics Anonymous or Narcotics Anonymous – can provide you with peer support as well as spiritual growth. Although it may not be for everyone, a 12-step support group may provide just the support you need during your early days of recovery and beyond.\nOverall, there are many tools that can help in the process of recovering from methamphetamine addiction.\nMedication is one of these tools and can certainly aid you, or a loved one, in the recovery process. Although recovery from crystal meth can seem like a daunting ordeal, the long-term effects are considerably more daunting. With the help of medication, you can avoid some of the common pitfalls that can lead to relapse.\nThe Effects of Crystal Meth Addiction\nFor individuals addicted to crystal meth, the prospect of quitting the drug can be daunting and even terrifying. These feelings of overwhelming fear are understandable, however, as the idea of going through the withdrawal process can be overwhelming.\nBut before you run away from the meth recovery process out of fear, consider the consequences of not working through this fear. Continuing use of crystal meth puts you at risk for both short-term and long-term consequences – some of which can even be deadly.\nShort-term effects of crystal meth may include8:\n- Heightened vigilance or attention.\n- Increased activity and restlessness.\n- Increased breathing rate.\n- Rapid and irregular heartbeats.\n- Euphoric rush.\n- Decreased appetite.\n- Hyperthermia (increased body temperature).\nOne of the most obvious and hazardous long-term effects of crystal meth use is the potential to develop an addiction. As the functionality and molecular structure of your brain begin to change, the compulsion to persistently use the drug is strengthened. Since your body eventually becomes tolerant to meth over time, however, your regular drug dose will begin to lose its ability to produce the same pleasurable effects – spurring the need to take higher doses to achieve the same effects.\nIn addition to addiction and tolerance, other long-term effects from meth use include8:\n- Profoundly disrupted sleep patterns.\n- Mood disturbances.\n- Dental problems (“meth mouth”).\n- Auditory hallucinations and delusions.\nIt has been found that individuals who have experienced severe meth-induced psychosis might struggle with persistent psychotic features, even after achieving sobriety. It can take a brain quite a long time to readjust. Strokes and other sudden cardiovascular events may occur, even in the short term with methamphetamine abuse. However, the consequences of the neurologic damage that may result – including paralysis, loss of speech, cognitive decline and dementia, for example – may persist for much longer durations.\nFind Help for Your Meth Addiction\n- What treatments are effective for people who abuse methamphetamine? National Institute on Drug Abuse.\n- Addiction Medications. National Institute on Drug Abuse.\n- What are the long-term effects of methamphetamine abuse? National Institute on Drug Abuse.\n- Anderson, D. (2015) Narrative of discovery: in search of a medication to treat methamphetamine addiction. National Institute on Drug Abuse.\n- Courtney, K. E., Ray, L. A. (2014). Methamphetamine: an update on epidemiology, pharmacology, clinical phenomenology, and treatment literature. Drug Alcohol Depend, 0: 11-21.\n- Chen, Y. H., Chen, C. H. (2013). The development of antibody-based immunotherapy for methamphetamine abuse: immunization, and virus-mediated gene transfer approaches. Curr Gene Therapy, 13(1), 39-50.\n- Center for Substance Abuse Treatment. (2004). Chapter 1 substance abuse treatment and family therapy. Rockville, MD: Substance Abuse and Mental Health Services Administration.\n- Stimulants. Substance Abuse and Mental Health Services Administration."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_language_proficiency_implied","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7641e7cb-50e1-4925-a105-6db364f6a82f>","<urn:uuid:713824ab-451b-4856-b61b-be266ba1d46b>"],"error":null}
{"question":"As a systems architect, I'm curious: why are hyperstructures important in computer networks, and what are the key security risks they face?","answer":"Hyperstructures are important because they have wide applications in computer science and information systems, particularly in classification systems and network design. They provide a mathematical framework that can be applied to groups and help in designing efficient classification systems. Regarding security risks, these systems face several threats including viruses (malicious programs that can duplicate and delete data), computer worms (self-replicating programs that consume disk space), and man-in-the-middle attacks (where hackers interrupt traffic between users and networks to steal data). Additionally, they are vulnerable to phishing attempts and unauthorized access through rootkits that can modify system settings.","context":["- About this Journal ·\n- Abstracting and Indexing ·\n- Advance Access ·\n- Aims and Scope ·\n- Article Processing Charges ·\n- Articles in Press ·\n- Author Guidelines ·\n- Bibliographic Information ·\n- Citations to this Journal ·\n- Contact Information ·\n- Editorial Board ·\n- Editorial Workflow ·\n- Free eTOC Alerts ·\n- Publication Ethics ·\n- Reviewers Acknowledgment ·\n- Submit a Manuscript ·\n- Subscription Information ·\n- Table of Contents\nVolume 2011 (2011), Article ID 953124, 8 pages\nOn Hyperideals in Left Almost Semihypergroups\nDepartment of Mathematics & Computer Science, Faculty of Natural Sciences, University of Gjirokastra, Gjirokastra 6001, Albania\nReceived 7 June 2011; Accepted 11 July 2011\nAcademic Editors: A. V. Kelarev and A. Kiliçman\nCopyright © 2011 Kostaq Hila and Jani Dine. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nThis paper deals with a class of algebraic hyperstructures called left almost semihypergroups (LA-semihypergroups), which are a generalization of LA-semigroups and semihypergroups. We introduce the notion of LA-semihypergroup, the related notions of hyperideal, bi-hyperideal, and some properties of them are investigated. It is a useful nonassociative algebraic hyperstructure, midway between a hypergroupoid and a commutative hypersemigroup, with wide applications in the theory of flocks, and so forth. We define the topological space and study the topological structure of LA-semihypergroups using hyperideal theory. The topological spaces formation guarantee for the preservation of finite intersection and arbitrary union between the set of hyperideals and the open subsets of resultant topologies.\n1. Introduction and Preliminaries\nThe applications of mathematics in other disciplines, for example in informatics, play a key role, and they represent, in the last decades, one of the purposes of the study of the experts of hyperstructures theory all over the world. Hyperstructure theory was introduced in 1934 by a French mathematician Marty , at the 8th Congress of Scandinavian Mathematicians, where he defined hypergroups based on the notion of hyperoperation, began to analyze their properties, and applied them to groups. In the following decades and nowadays, a number of different hyperstructures are widely studied from the theoretical point of view and for their applications to many subjects of pure and applied mathematics and computer science by many mathematicians. In a classical algebraic structure, the composition of two elements is an element, while in an algebraic hyperstructure, the composition of two elements is a set. Some principal notions about hyperstructures and semihypergroups theory can be found in [1–7].\nThe Theory of ideals, in its modern form, is a contemporary development of mathematical knowledge to which mathematicians of today may justly point with pride. Ideal theory is important not only for the intrinsic interest and purity of its logical structure but because it is a necessary tool in many branches of mathematics and its applications such as in informatics, physics, and others. As an example of applications of the concept of an ideal in informatics, let us mention that ideals of algebraic structures have been used recently to design efficient classification systems, see [8–12].\nThe study of LA-semigroup as a generalization of commutative semigroup was initiated in 1972 by Kazim and Naseeruddin . They have introduced the concept of an LA-semigroup and have investigated some basic but important characteristics of this structure. They have generalized some useful results of semigroup theory. Since then, many papers on LA-semigroups appeared showing the importance of the concept and its applications [13–23]. In this paper, we generalize this notion introducing the notion of LA-semihypergroup which is a generalization of LA-semigroup and semihypergroup, proposing so a new kind of hyperstructure for further studying. It is a useful nonassociative algebraic hyperstructure, midway between a hypergroupoid and a commutative hypersemigroup, with wide applications in the theory of flocks etc. Although the hyperstructure is nonassociative and noncommutative, nevertheless, it possesses many interesting properties which we usually find in associative and commutative algebraic hyperstructures. A several properties of hyperideals of LA-semihypergroup are investigated. In this note, we define the topological space and study the topological structure of LA-semihypergroups using hyperideal theory. The topological spaces formation guarantee for the preservation of finite intersection and arbitrary union between the set of hyperideals and the open subsets of resultant topologies.\nRecall first the basic terms and definitions from the hyperstructure theory.\nDefinition 1.1. A map is called hyperoperation or join operation on the set , where is a nonempty set and denotes the set of all nonempty subsets of .\nDefinition 1.2. A hyperstructure is called the pair , where is a hyperoperation on the set .\nDefinition 1.3. A hyperstructure is called a semihypergroup if for all , , which means that\nIf and are nonempty subsets of , then\nDefinition 1.4. A nonempty subset of a semihypergroup is called a sub-semihypergroup of if , and is called in this case super-semihypergroup of .\nDefinition 1.5. Let be a semihypergroup. Then is called a hypergroup if it satisfies the reproduction axiom, for all , .\nDefinition 1.6. A hypergrupoid is called an LA-semihypergroup if, for all ,\nEvery LA-semihypergroup satisfies the medial law, that is, for all ,\nIn every LA-semihypergroup with left identity, the following law holds: for all .\nAn element in an LA-semihypergroup is called identity if . An element 0 in a semihypergroup is called zero element if . A subset of an LA-semihypergroup is called a right (left) hyperideal if and is called a hyperideal if it is two-sided hyperideal, and if is a left hyperideal of , then becomes a hyperideal of . By a bi-hyperideal of an LA-semihypergroup , we mean a sub-LA-semihypergroup of such that . It is easy to note that each right hyperideal is a bi-hyperideal. If has a left identity, then it is not hard to show that is a bi-hyperideal of and . If denotes the set of all idempotents subsets of with left identity , then forms a hypersemilattice structure, also if , then . The intersection of any set of bi-hyperideals of an LA-semihypergroup is either empty or a bi-hyperideal of . Also the intersection of prime bi-hyperideals of an LA-semihypergroup is a semiprime bi-hyperideal of .\n2. Main Results\nProposition 2.1. Let be an LA-semihypergroup with left identity, a left hyperideal, and a bi-hyperideal of . Then and are bi-hyperideals of .\nProof. Using the medial law (1.4), we get also Hence, is a bi-hyperideal of . we obtain also Hence, is a bi-hyperideal of .\nProposition 2.2. Let be an LA-semihypergroup with left identity and two bi-hyperideals of . Then is a bi-hyperideal of .\nProof. Using (1.4), we get\nBy the above, if and are nonempty, then and are connected bi-hyperideals. Proposition 2.1 leads us to an easy generalization, that is, if are bi-hyperideals of an LA-semihypergroup with left identity, then are bi-hyperideals of , consequently the set of bi-hyperideals forms an LA-semihypergroup.\nIf is an LA-semihypergroup with left identity , then and are bi-hyperideals of . It can be easily shown that , , and . Hence, this implies that and . Also, , , , , and (if is an idempotent), consequently . It is easy to show that .\nLemma 2.3. Let be an LA-semihypergroup with left identity, and let be an idempotent bi-hyperideal of . Then is a hyperideal of .\nProof. By the definition of LA-semihypergroup (1.3), we have and every right hyperideal in with left identity is left.\nLemma 2.4. Let be an LA-semihypergroup with left identity , and let be a proper bi-hyperideal of . Then .\nProof. Let us suppose that . Since , using (1.3), we have . It is impossible. So, .\nIt can be easily noted that .\nProposition 2.5. Let be an LA-semihypergroup with left identity, and let be bi-hyperideals of . Then the following statements are equivalent: (1)every bi-hyperideal is idempotent, (2),(3)the hyperideals of form a hypersemilattice , where .\nProof. (1)⇒(2). Using Lemma 2.3, it is easy to note that . Since implies , hence .\n(2)⇒(3). and . Similarly, associativity follows. Hence, is a hypersemilattice.\nA bi-hyperideal of an LA-semihypergroup is called a prime bi-hyperideal if implies either or for every bi-hyperideal and of . The set of bi-hyperideals of is totally ordered under the set inclusion if for all bi-hyperideals either or .\nTheorem 2.6. Let be an LA-semihypergroup with left identity. Every bi-hyperideal of is prime if and only if it is idempotent and the set of the bi-hyperideals of is totally ordered under the set inclusion.\nProof. Let us assume that every bi-hyperideal of is prime. Since is a hyperideal and so is prime which implies that , hence is idempotent. Since is a bi-hyperideal of (where and are bi-hyperideals of ) and so is prime. Now by Lemma 2.3, either or which further implies that either or . Hence, the set of bi-hyperideals of is totally ordered under set inclusion.\nConversely, let us assume that every bi-hyperideals of is idempotent and the set of bi-hyperideals of is totally ordered under set inclusion. Let and be the bi-hyperideals of with and without loss of generality assume that . Since is an idempotent, so implies that , and, hence, every bi-hyperideal of is prime.\nA bi-hyperideal of an LA-semihypergroup is called strongly irreducible bi-hyperideal if implies either or for every bi-hyperideal and of .\nTheorem 2.7. Let be an LA-semihypergroup with zero. Let be the set of all bi-hyperideals of , and the set of all strongly irreducible proper bi-hyperideals of , then forms a topology on the set , where and : Bi-hyperideal preserves finite intersection and arbitrary union between the set of bi-hyperideals of and open subsets of .\nProof. Since is a bi-hyperideal of and 0 belongs to every bi-hyperideal of , then , also which is the first axiom for the topology. Let , then , where is a bi-hyperideal of generated by . Let and , if , then and . Let us suppose , this implies that either or . It is impossible. Hence, which further implies that . Thus . Now if , then and . Thus and , therefore , which implies that . Hence is the topology on . Define : Bi-hyperideal by , then it is easy to note that preserves finite intersection and arbitrary union.\nA hyperideal of an LA-semihypergroup is called prime if implies that either or for all hyperideals and in .\nLet denotes the set of proper prime hyperideals of an LA-semihypergroup absorbing 0. For a hyperideal of , we define the sets and .\nTheorem 2.8. Let be an LA-semihypergroup with zero. The set constitutes a topology on the set .\nProof. Let , if , then and and . Let which implies that either or , which is impossible. Hence, . Similarly . The remaining proof follows from Theorem 2.7.\nThe assignment preserves finite intersection and arbitrary union between the hyperideal and their corresponding open subsets of .\nLet be a left hyperideal of an LA-semihypergroup . is called quasiprime if for left hyperideals of such that , we have or .\nTheorem 2.9. Let be an LA-semihypergroup with left identity . Then a left hyperideal of is quasiprime if and only if implies that either or .\nProof. Let be a left hyperideal of . Let us assume that , then\nHence, either or .\nConversely, let us assume that , where and are left hyperideal of such that . Then there exists such that . Now, by the hypothesis, we have for all . Since , so by hypothesis, for all , we obtain . This shows that is quasiprime.\nAn LA-semihypergroup is called an antirectangular if , for all . It is easy to see that . In the following results for an antirectangular LA-semihypergroup , .\nProposition 2.10. Let be an LA-semihypergroup. If are hyperideals of , then is a hyperideal.\nProof. Using (1.4), we have also which shows that is a hyperideal.\nConsequently, if are hyperideals of , then are hyperideals of and the set of hyperideals of form an antirectangular LA-semihypergroup.\nLemma 2.11. Let be an antirectangular LA-semihypergroup. Any subset of is left hyperideal if and only if it is right.\nIt is fact that . From the above lemma, we remark that every quasiprime hyperideal becomes prime in an antirectangular LA-semihypergroup.\nLemma 2.12. Let be an anti-rectangular LA-semihypergroup. If is a hyperideal of , then .\nProof. Let , then . Hence . Also, .\nAn hyperideal of an LA-semihypergroup is called an idempotent if . An LA-semihypergroup is said to be fully idempotent if every hyperideal of is idempotent.\nProposition 2.13. Let be an antirectangular LA-semihypergroup, and, be hyperideals of . Then the following statements are equivalent: (1) is fully idempotent, (2),(3)the hyperideals of form a hypersemilattice where .\nThe proof follows from Proposition 2.5.\nThe set of hyperideals of is totally ordered under set inclusion if for all hyperideals either or and denoted by hyperideal.\nTheorem 2.14. Let be an antirectangular LA-semihypergroup. Then every hyperideal of is prime if and only if it is idempotent and hyperideal is totally ordered under set inclusion.\nProof. The proof follows from Theorem 2.6.\nIn conclusion, let us mention that it would be interesting to investigate whether it is possible to apply hyperideals of hyperstructures to the construction of classification systems similar to those introduced in [8–12].\nThe authors are highly grateful to referees for their valuable comments and suggestions.\n- F. Marty, “Sur une generalization de la notion de group, 8th Congres Math,” Scandinaves, pp. 45–49, 1934.\n- P. Corsini, Prolegomena of Hypergroup Theory, Aviani Editore, 2nd edition, 1993.\n- P. Corsini and V. Leoreanu, Applications of Hyperstructure Theory, vol. 5 of Advances in Mathematics, Kluwer Academic Publishers, Dordrecht, The Netherlands, 2003.\n- B. Davvaz and V. Leoreanu-Fotea, Hyperring Theory and Applications, International Academic Press, 2007.\n- T. Vougiouklis, Hyperstructures and Their Representations, Hadronic Press, Palm Harbor, Fla, USA, 1994.\n- K. Hila, B. Davvaz, and K. Naka, “On quasi-hyperideals in semihypergroups,” Communications in Algebra. In press.\n- K Hila, B. Davvaz, and J. Dine, “Study on the structure of Γ-semihypergroups,” Communications in Algebra. In press.\n- A. V. Kelarev, J. L. Yearwood, and M. A. Mammadov, “A formula for multiple classifiers in data mining based on Brandt semigroups,” Semigroup Forum, vol. 78, no. 2, pp. 293–309, 2009.\n- A. V. Kelarev, J. L. Yearwood, and P. W. Vamplew, “A polynomial ring construction for the classification of data,” Bulletin of the Australian Mathematical Society, vol. 79, no. 2, pp. 213–225, 2009.\n- A. V. Kelarev, J. L. Yearwood, and P. Watters, “Rees matrix constructions for clustering of data,” Journal of the Australian Mathematical Society, vol. 87, no. 3, pp. 377–393, 2009.\n- A. V. Kelarev, J. L. Yearwood, P. Watters, X. Wu, J. H. Abawajy, and L. Pan, “Internet security applications of the Munn rings,” Semigroup Forum, vol. 81, no. 1, pp. 162–171, 2010.\n- A. V. Kelarev, J. L. Yearwood, and P. A. Watters, “Optimization of classifiers for data mining based on combinatorial semigroups,” Semigroup Forum, vol. 82, no. 2, pp. 242–251, 2011.\n- M. A. Kazim and M. Naseeruddin, “On almost semigroups,” The Aligarh Bulletin of Mathematics, vol. 2, pp. 1–7, 1972.\n- Q. Mushtaq and S. M. Yusuf, “On LA-semigroups,” The Aligarh Bulletin of Mathematics, vol. 8, pp. 65–70, 1978.\n- Q. Mushtaq and S. M. Yusuf, “On locally associative LA-semigroups,” The Journal of Natural Sciences and Mathematics, vol. 19, no. 1, pp. 57–62, 1979.\n- Q. Mushtaq, “Abelian groups defined by LA-semigroups,” Studia Scientiarum Mathematicarum Hungarica, vol. 18, no. 2–4, pp. 427–428, 1983.\n- Q. Mushtaq and Q. Iqbal, “Decomposition of a locally associative LA-semigroup,” Semigroup Forum, vol. 41, no. 2, pp. 155–164, 1990.\n- Q. Mushtaq and M. S. Kamran, “On left almost groups,” Proceedings of the Pakistan Academy of Sciences, vol. 33, no. 1-2, pp. 53–55, 1996.\n- P. V. Protić and N. Stevanović, “On Abel-Grassmann's groupoids,” in Proceedings of the Mathematical Conference in Priština, pp. 31–38.\n- Q. Mushtaq and M. S. Kamran, “On LA-semigroup with weak associative law,” Scientiffic Khyber, vol. 1, pp. 69–71, 1989.\n- Q. Mushtaq and M. Khan, “M-systems in LA-semigroups,” Southeast Asian Bulletin of Mathematics, vol. 33, no. 2, pp. 321–327, 2009.\n- Q. Mushtaq and M. Khan, “Topological structures on Abel-Grassman's grupoids,” arXiv:0904.1650v1, 2009.\n- P. Holgate, “Groupoids satisfying a simple invertive law,” The Mathematics Student, vol. 61, no. 1–4, pp. 101–106, 1992.","A network system refers to the interconnection of two or more computers to share resources (such as printers and scanners) to easily exchange and transmit information through cables or wireless connection that follows certain rules or protocols.\nThere are three common types of network, the Local Area Network (LAN), Wireless Local Area Network WLAN), and Wide Area Network (WAN).\n- Local Area Network (LAN) generally covers a limited geographic area like an internet café, school laboratory, or office. Computers are connected to a server or workstations. The server is configured so it can do its function without direct human intervention. Its function includes software hosting, file storage and retrieval, complete access control for the network resources, and many more. On the other hand, workstations need human users to interact and utilize network services, unlike servers.\n- Wireless Local Area Network (WLAN) is the same as LAN, but the connection on the network is wireless.\n- Wide Area Network (WAN) connects a larger geographic area such as a whole state, country, or even the world. This global network is made possible by connecting through the satellite. WAN is a complex network because it uses a variety of connection materials such as multiplexers, bridges, and routers to connect locally, then metropolitan networks for global connection like the internet.\nConnecting to any network will create an opening to potential risk and attacks on the security of computer systems. Thus, an effective and strict computer security system must be implemented.\nWhat are the main types of computer security?\nComputer security protects the information that is stored in a computer from anyone who wants to harm or steal it and prevents unauthorized access. Below are the main types of computer security:\n- Physical Security – setting a unique password to a computer. The password will be known to authorize the person only to assure the safety and confidentiality of the data being stored in that computer.\n- Network Security – the network has many entry points. Thus, network security requires defense methods such as firewalls. A firewall is a software device that monitors the traffic and prevents access to the network depending on security rules.\n- Executable security – installing an anti-virus. An anti-virus is a computer program that will try to identify, remove and prevent viruses from harming your computer system. A virus may delete, duplicate or even damage your whole computer system. So it is a must to have an installed and updated anti-virus.\nThe main goal of computer security is to ensure the confidentiality of the information to authorized persons, to maintain the integrity of the information (which means there are no deviations from the original information being stored), and to make information available to concerned persons only.\nWhat are the 3 basic internal security of computer system?\nAttacks on the computer system and networks are rampant these days; thus, a computer’s internal security system must be strictly implemented and observed. There is three basic internal security of a computer system: management security, operational security, and physical security.\n- Management security is referred to as administrative controls, which provide the guidance, rules, and procedures for implementing the overall security of the system.\n- Operational security is referred to as technical controls, which assure the effectiveness of your controls, such as access controls, authentication, and security topologies applied to networks and installed applications.\n- Physical security refers to the protection of personnel, data, hardware and etc., from physical attacks and threats that can harm, damage, disrupt the operation or even destroy and affect the confidentiality, integrity, or availability of data or the system.\nWhat are computer security risks?\nNowadays, it is complicated and unpredictable to know how or when an attack on your computer system will occur. Harming your computer system through physical damage and non-physical (viruses) may happen at any time, so it’s always better to be careful and avoid it from happening as much as possible.\nComputer security risk is an act that could damage the hardware and software of a computer unit. Following are examples of computer security risks:\n- Humans – the biggest security risk. In an organization, if your employee is not well trained about safety and good practices, misuse of a computer system may happen. It is critical to make sure that they are knowledgeable in terms of security, especially if that employee holds or is assigned in a very important and confidential task.\n- Bad backups – if your backup is corrupted or lost, you will face big trouble, especially if you don’t have any alternative copies. Always make sure that you properly back up the data and keep it in a safe place.\n- Not maintaining hardware or software – will give a potential entry to an attacker. It is always recommended to have regular check-ups of your hardware and software systems.\n- Not planning for a natural disaster and more.\nConnecting to a network or to the internet and installing software, your computer system is already introduced to risk. Today, almost all people need the internet to communicate and need to install new software and applications, we can never assure 100% security, but building and creating strong computer security helps alleviate the risk.\nWhat is the importance of computer security?\nComputer security will keep all information protected. Tightening the security of your computer will prevent unnecessary and harmful incidents that may result in information leaks and loss, and even hardware damages. Being aware and knowing the do’s and don’ts in terms of security will help you to sidestep unwanted occurrences.\nStrengthening computer security, guaranteed safety processing, storage of sensitive information, and more. It also provides confidentiality, integrity, and availability for all components of the computer system.\nComputer security allows your computer system to avoid the following:\n- Viruses – are malicious programs that will be installed into your computer without your knowledge. This program can duplicate, hide and even delete your data. It can also replicate itself and infect other installed software in the system and files.\n- Computer worm – a software program that replicates itself in greater volume and excessive speed and infects from one computer to another computer without the users’ knowledge. This program will use and consume users’ computer hard disk space.\n- Man-in-the-Middle Attacks (MITM) – a cybercrime that occurs when a visitor user uses an unsecured public Wi-Fi network. Hackers interrupt the traffic between the user and the network, and during that time, they can steal data by using malware to install the software.\n- Phishing – a cybercrime that involves someone pretending to be a legitimate and trusted institution over the telephone, email, or even text messages to trick possible targets into disclosing personal and confidential information (such as email, passwords, bank details and etc.). This may result in financial loss and stealing and using your identity.\n- Botnet – a network of connected computers that are conceded by hackers without the users’ knowledge. The infected individual computers are now called a ‘zombie computer,’ which now is the bot. It will be used for doing malicious activities on bigger-scale attacks such as DDoS.\n- Rootkit – malicious software designed to enable unauthorized access to a computer system or restricted network while hiding its existence. It allows a hacker to modify system settings and configurations and execute commands remotely.\n- Keylogger – a software monitoring that can track and record all keyboard strokes and activities without the users’ knowledge\nAdditional precautions to secure harmful incidents:\n- Do not leave your computer unattended\n- Do not visit websites without SSL certificates\n- Do not click links that you are not familiar\n- Be mindful of the information you are entering\nHere are some important reasons why computer security is important:\n- Protects your personal information\n- Protects your companies property\n- Stops unauthorized access\n- Prevents data leak and theft\n- Prevents viruses and malware\nHow do we maintain computer security?\nMaintaining your computer security allows you to avoid hacking and stealing attempts on your information and even your computer hardware. Computer hardware can be secured and protected through lockers, doors, confidential information, and unauthorized access to your system requires complex security strategies and practices. Below are some strategies and practices to maintain your computer security:\n- Backup your data regularly and store it in a protected place\n- Use a unique and strong password and change it regularly\n- Use a two-factor authentication\n- Use a firewall. Always activate your firewall because it acts as a security guard between your local network and the internet.\n- Keep all your installed software up to date.\n- Install reliable and trustworthy anti-virus software and malware protection\n- Be cautious of suspicious emails. Do not easily disclose personal information when asked. Always verify the reliability of all emails you receive.\n- Do not click any link provided or open any attachments, especially if it’s not familiar to you.\n- Make sure that the Wi-Fi is secure. Browse the web safely. Do not visit sites that offer prizes or promos.\n- Avoid pirated materials and cracked software. Always use and install software from a trusted source.\n- Do not leave your computer unit unattended.\n- Avoid using USB and other external devices unless you own it\n- Educate yourself with basic aspects of computer security and the latest security, as well as the evolving threats and ways to attack.\n- Always perform a daily or regular full system scan.\n- And more..\nA network system is critical in terms of security mainly because security threats are constantly evolving, and so as the security programs to defend the system against any threats and attacks continuously.\nThere are numerous threats and attacks that only wait for the opportunity to enter your system. However, understanding them and knowing how to avoid these threats is a key to preventing or defending your network and system from potential malicious activities.\nStrengthening your computer security system is always the best solution to avoid any harmful effects on your software, hardware, and network.\nJust visit www.thepractical.co.th for more information or any queries and concerns."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:06fc8131-ada6-49f3-a5c6-e1e70cf86bc0>","<urn:uuid:8cd50449-54a2-4538-995d-3016a79a4b22>"],"error":null}
{"question":"I need to get my eyes checked soon. How do modern digital preliminary testing methods compare with traditional eye examination techniques, and what comprehensive tests are included in current routine eye exams?","answer":"Modern digital preliminary testing has significantly expanded the scope of routine eye exams. The Stereo Optical digital Optec Plus efficiently tests for multiple conditions including loss of focusing flexibility, clarity of vision at distance and near, astigmatism, fusion, depth perception, color vision, contrast sensitivity, and horizontal peripheral vision. Traditional examination techniques are still maintained, including visual acuity tests using eye charts, refraction exams with a phoropter to determine eyeglass prescriptions, and slit-lamp exams using a microscope to examine eye structures in detail. Additionally, modern exams may include pupil dilation to better view internal eye structures, and specialized tests like corneal topography for mapping the eye's surface.","context":["Pioneering Technology by Optos – Daytona Plus\nOptos’ patented ultra-widefield digital scanning laser technology acquires images of the back of the eye that support the detection, diagnosis, analysis, documentation and management of ocular pathology and systemic disease that may first present in the periphery. These conditions may otherwise go undetected using traditional examination techniques and equipment. Simultaneous, non-contact, central pole-to-periphery views of up to 82% or 200 degrees of the retina are displayed in one single capture, compared to 45 degrees achieved with conventional methods.\nPlay the video below to find out about the new technology of Autofluorescence in the Daytona Plus. This has the potential to discover pathology earlier than other standard retinal cameras because it can detect stressed retinal cells.\nGlaucoma Screening Test GST from Haag Streit\nLess than one minute\nOur new Glaucoma Screening Test GST can distinguish between normal and abnormal visual fields in less than one minute. The test is purely qualitative and distinguishes between normal and abnormal visual fields by presenting stimuli three times at a brightness that patients with normal vision should see. If not seen, then the visual field is flagged as abnormal with high reliability and the patient can be further tested. Because the test is efficient, it opens doors for more routine visual field testing to ensure no pathology goes undetected. The patient-friendly Pulsar stimulus has been developed for early glaucoma detection and shows a short learning curve and low test-retest variability.\nThe sensitive Cluster Analysis groups visual field defects along nerve fibre bundles and combines high sensitivity with good specificity to detect early glaucomatous changes. Significant defects are highlighted and the cluster defects can be used for structural comparison.\nFor intuitive structural comparison\nCombining the results of both structure and function is key to obtaining a comprehensive assessment of the onset and progression of glaucoma. The Octopus Polar Analysis projects local visual field defects along the nerve fibers to the optic disk and displays them oriented as structural results.\nNew Pediatric Technology – Plusoptix\nAccent Eye Care now has the recently developed Plusoptix Automated Refractor used to detect most common vision disorders in children as early as possible.\nIn combination with other entry examination tests, Dr. Gong and her staff will be able to determine a valuable starting point for myopia, astigmatism and axis measurements (information used for prescribing eyewear).\nPlusoptix pediatric auto-refractors take binocular (both eyes simultaneously) or monocular readings from 1 meter (3.3 feet) away in less than one second and can even be used for strabismic (eye turn) or aphakic (lens removed from eye) patients.\nSD-OCT (Spectral Domain – Optical Coherence Tomography)\nOCT provides cross-sectional images of the retina through retinal imaging. We have the newest spectral domain (SD) technology which improves image quality and acquisition, providing a larger set of data points for accurate spatial correlations and mapping of individual retinal layers.\nThis new technology allows us to easily conduct an eye exam in Phoenix, AZ by offering complete retina, glaucoma, and keratoconus/anterior segment scanning as part of our advanced diagnostic testing. Because the streamlined user interfaces are fast and efficient, you will not be waiting long for your results.\nCorneal topography is a non-invasive medical imaging technique for mapping thousands of points of the corneal surface, the front of the eye. This can be crucial in determining the quality of vision for conditions such as keratoconus and pellucid marginal degeneration, which may exhibit corneal steepening before any biomicroscopic signs are evident. Topography also is invaluable when evaluating pre- and postsurgical patients, especially corneal transplants, radial keratotomy or LASIK. Postoperatively, topography can help follow the healing phase and assist with contact lens fitting, especially for hard to fit contacts.\nNew Technology for digital preliminary testing\nWith the newly developed Stereo Optical digital Optec Plus, Dr Gong’s office can efficiently test for: loss of focusing flexibility, clarity of vision at distance and near, astigmatism ( 2 prescriptions required, 90 degrees apart ), latent hyperopia (farsightedness), fusion (ability to use your eyes together), lateral and vertical phoria ( where the eyes rest horizontally and vertically), depth perception ( 3-D vision), Color vision tests, contrast sensitivity test ( your ability to tell the difference between the target and background) and Horizontal peripheral vision testing for the DMV paperwork: Stimuli at 45° nasal and 55°, 60°, 70°, 80°, 85°, 90° and 100° temporal.\nDigital retinal imaging is another non-invasive procedure used by our office to conduct that incorporates high-resolution imaging systems to take pictures of the nerve-rich lining of your eye that you use to see with, the retina. This helps us to detect and manage eye and health conditions such as glaucoma, diabetes, and macular degeneration.\nRetinal images provide a permanent and historical record of changes in your eye. Images can be compared side-by-side, year after year, to discover even subtle changes and help monitor your eye health.\nGene testing is a procedure to identify early or intermediate age-related macular degeneration risks. Gene testing recognizes those who will most likely develop advanced macular degeneration with vision loss.\nFor the test, a simple cheek swab is taken in Dr. Gong’s office, and is at no cost to you if you have medical insurance. The only charge would be for the office visit to review the results.\nMacular degeneration usually goes unnoticed in most people until they lose central vision in one eye. The loss of vision can happen rapidly and without warning; it may be detained but is typically irreversible. In order to prevent blindness from macular degeneration, early discovery and treatment is crucial.\nVisual Field Analyzer\nA visual field test in our AZ eye exam facility is designed to detect dysfunctions in central and peripheral vision which can be caused from various medical conditions such as glaucoma, stroke, brain tumors, and other neurological deficits.\nOur eye exam in Phoenix, AZ is performed by using the Visual Field Analyzer from Carl Zeiss, which utilizes Frequency Doubling Technology (FDT). FDT is proven technology used to target low redundancy ganglion cells; because these cells are sparsely populated, there is less compensation available to mask cell damage. By targeting these cells, the Humphrey FDT perimeters can detect early visual field loss.","Regular eye exams are an important part of good eye care. Many serious eye disorders have few or no symptoms, including conditions related to the retina and optic nerve. With a routine eye exam, your optometrist can diagnose these as well as the common refractive errors of nearsightedness, farsightedness and astigmatism. During your routine eye exam you may have one or several of the tests shown below.\nVirginia Mason offers eye exams in five western Washington locations: Federal Way, Issaquah, Kirkland, Lynnwood and Seattle. Learn more about Virginia Mason's optometrists or find information for scheduling an appointment.\nCommon Eye Exams\n- Visual Acuity Test\nThis common eye test assesses your reading and distance vision with the use of eye charts. Blurry vision is a common sign in many eye disorders and is quickly seen during this exam.\n- Visual Field Test\nThis simple test is performed to determine if any changes have occurred in your peripheral (side) vision, which is a common sign in glaucoma. A basic form of this test can be done by your doctor or technician. A more sensitive test is done with a device known as an automated perimeter.\nA procedure in which eye pressure is measured using one of several different methods. This is a very important test that is done during a routine eye exam.\n- Refraction Exam\nThis common test uses a machine called a phoropter that allows your doctor to determine the amount of refractive error (eyeglasses prescription). It determines whether you are farsighted (hyperopic) nearsighted (myopic) or have astigmatism.\n- Dilation of Pupils\nDuring this procedure your pupils are dilated or enlarged with eye drops so that your ophthalmologist or optometrist can better view the internal eye structures. Typically the dilation last four to six hours and you may be light sensitive or find it difficult to read during this period.\n- Slit-lamp Exam\nA slit lamp is a type of microscope that is typically used to exam the anterior segment of the eye. It allows your eye-care practitioner to examine structures such as the eyelids, cornea and iris in great detail. If your eyes have been dilated your doctor, with the addition of a specialized lens, may also use the slit-lamp to view structures inside the eye.\n- Potential Acuity Test\nIf it is uncertain to your eye surgeon what your best vision will be after cataract surgery, your acuity can be estimated using a Potential Acuity Meter.\nThis is generally used in cases of eye disease or trauma, when the surgeon is unable to obtain an adequate view of the retina because of a dense cataract.\n- Ocular Coherence Tomography (OCT)\nAn eye test called ocular coherence tomography (OCT) provides 3D imaging of your retina. This test gives your eye doctor a real-time view of the multiple layers of the retina.\nThis procedure can be performed with either a manual or computerized device and measures the curvatures of your cornea. This information can help your eye care professional fit contacts or diagnose certain corneal diseases."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7c3f77f1-83c5-4381-8656-97acf0eacd10>","<urn:uuid:a489e690-e7b4-416c-9f79-80fa4a435966>"],"error":null}
{"question":"I'm trying to understand gluten sensitivity better - what's the difference between Celiac disease and non-autoimmune gluten sensitivity, and what diet changes do these conditions require? 🤔","answer":"Celiac disease is an autoimmune condition where gluten triggers the production of autoantibodies against tissue transglutaminase and may cause intestinal damage. People with Celiac must permanently and strictly avoid all gluten, as even small amounts can trigger autoimmune responses. Non-autoimmune gluten sensitivity (also called Non-Celiac gluten sensitivity) involves symptoms triggered by gluten but without the autoantibodies or intestinal damage seen in Celiac disease. While people with non-autoimmune sensitivity should avoid gluten, there's still controversy over whether they need permanent, strict avoidance. For both conditions, a healthy gluten-free diet should focus on whole foods like vegetables, fruits, lean proteins, and alternative grains like quinoa and buckwheat, rather than processed gluten-free products which are often higher in fat and sugar and lower in nutrients.","context":["Celiac Disease and Thyroid Health\nPublished June 30 2014\nCeliac disease is an autoimmune condition that is triggered by gluten. This condition affects both adults and children, and while people with this condition can experience gastrointestinal symptoms such as bloating, gas, stomach pain, diarrhea or constipation, it’s common for others to experience extraintestinal symptoms. Some of these signs and symptoms include dermatitis herpetiformis, anemia, osteoporosis, weight loss, female infertility, neurological problems, and other health issues. As I’ll discuss in this article, there is a higher prevalence of Celiac disease in autoimmune thyroid conditions.\nLet’s begin by taking about why people develop Celiac disease. Just as is the case with most autoimmune conditions, Celiac disease is triggered by a combination of genetic and lifestyle factors. With regards to the genetics of Celiac Disease, more than 95% of patients with Celiac disease share the major histocompatibility complex II class human leukocyte antigen (HLA) DQ2 or DQ8 haplotype (1). As a result, if someone tests negative for these markers then there is a pretty good chance they don’t have Celiac disease, although not having these markers doesn’t completely rule this condition out, and of course there is the possibility that the person will have a non-autoimmune gluten sensitivity issue, which I’ll discuss later in this article.\nAdditional Testing For Celiac Disease\nIn addition to testing for the genetic markers of Celiac disease, one can also obtain a Celiac panel. Although a negative Celiac panel doesn’t always rule out Celiac Disease, obtain such a panel is usually a good place to start if someone suspects a gluten sensitivity issue. Some people get the gliadin antibodies tested, but keep in mind that it’s possible for these to be negative even if someone has Celiac Disease. If a problem with gluten is suspected you want to test the gliadin antibodies, the antibodies to tissue transglutaminase, and the anti-endomysial antibodies. It’s also a good idea to test both immunoglobulin A (IgA) and immunoglobulin G (IgG). The reason for this is because if one of these immunoglobulins happen to be depressed it can result in a false negative result.\nThe company Cyrex Labs has a comprehensive test for gluten called the Wheat/gluten proteome reactivity and autoimmunity panel. This measures one dozen of the markers associated with gluten, and tests the IgA and IgG for each one. Although it’s a great test, very rarely do I order it, as I usually just recommend for my patients to just avoid gluten. However, if someone really wants to know if they have an intolerance to gluten then this is a test to consider obtaining. This is a more comprehensive test than a Celiac panel, as in addition to measuring alpha gliadin antibodies it also measures additional antibodies against wheat proteins and peptides. In addition, whereas a Celiac panel will only measure transglutaminase-2 antibodies, the Cyrex Labs panel will also test for the antibodies against transglutaminase-3 and transglutaminase-6. This is important, as is it possible to test negative for the transglutaminase-2 antibodies, yet be positive for either the transglutaminase-3 or transglutaminase-6 antibodies.\nIf one of the antibodies come out positive then the doctor might want to perform a biopsy from the small intestine. The reason for this is because Celiac disease frequently shows damage to the villi of the small intestine. However, just because someone has a negative biopsy doesn’t conclude that they don’t have Celiac disease. If someone has Celiac disease yet has mild intestinal damage then it’s possible to have a negative biopsy. On the other hand, if a biopsy is performed and is positive, then a follow-up biopsy can be conducted at a later date to determine if the person responded well to a gluten free diet.\nSince Celiac disease can affect the absorption of some of the nutrients and can cause anemia, some other tests are important to obtain if one tests positive for this condition. This includes a complete blood count, a complete metabolic profile, an iron panel, as well as checking the vitamin B12 and vitamin D levels. Bone density can also be affected, and so a bone density scan might be a good idea in some cases.\nSo What Exactly Is Celiac Disease?\nI started out by mentioning that Celiac disease is an autoimmune condition that is triggered by gluten. But what actually happens from a physiological standpoint when someone has Celiac disease? In order to better understand the mechanisms behind Celiac disease, it’s probably a good idea to briefly define some of the common proteins and structures associated with this condition. I briefly mentioned gliadin earlier, as this is a protein of gluten. So when you measure the gliadin antibodies, these are the antibodies that your body produces against one of the gluten proteins. Earlier I mentioned how it’s possible for the gliadin antibodies to be negative, yet for someone to still have Celiac disease. The reverse is true as well, as it’s possible to have positive gliadin antibodies yet not have Celiac disease. I’ll elaborate on this shortly.\nTransglutaminase is a calcium-dependent enzyme that forms a complex with ingested gliadin and causes specific deamidation of gliadin (2), which in simple terms means that it modifies the protein structure of gliadin. It’s a complex process, but what’s important to understand is that autoantibodies against transglutaminase confirms that someone has Celiac disease, and thus will need to avoid gluten. So if you solely have gliadin antibodies then this isn’t considered an autoimmune process, and the reason for this is because gliadin is a protein of gluten, and isn’t part of your body. On the other hand, transglutaminase is an enzyme produced by your body, and so when there are antibodies against transglutaminase then this confirms the presence of autoimmunity. What about endomysium antibodies? The endomysium wraps around the muscle fibers, and so if you have positive anti-endomysial antibodies then this also confirms that you have an autoimmune component.\nSo without making this too difficult to understand, what essentially happens in someone with Celiac Disease is that someone consumes gluten, and the HLA-DQ2 and HLA-DQ8 antigen presenting cells I mentioned earlier present the gluten peptides to the T cells of our immune system, which results in immune system activation. In a case of “mistaken identity” the body will begin to produce antibodies against tissue transglutaminase, and anti-endomysium antibodies might also be present. When tissue transglutaminase-2 antibodies are present this will result in damage to the cells of the small intestine, and can lead to digestive symptoms, malabsorption, anemia, and some of the other problems I previously discussed. On the other hand, tissue transglutaminase-3 antibodies primarily affect the epidermis (the skin), whereas tissue transglutaminase-6 while affect the tissues of the nervous system.\nWhat Is Non-Autoimmune Gluten Sensitivity?\nNon-autoimmune gluten sensitivity, also known as Non-Celiac gluten sensitivity, is defined as a condition in which symptoms are triggered by gluten ingestion, in the absence of Celiac-specific antibodies and of classical Celiac villous atrophy, with variable Human Leukocyte Antigen (HLA) status and variable presence of first generation anti-gliadin antibodies (3). And so what this is saying is that this condition doesn’t necessarily cause damage to the villi, and there are no autoantibodies to tissue transglutaminase or anti-endomysial antibodies. However, in some cases there are positive gliadin antibodies. And the genetic markers will also be positive in some people.\nMany people with Non-Celiac gluten sensitivity will experience symptoms upon ingesting gluten. As I discussed in the beginning of this article, they might experience gastrointestinal symptoms, or they might experience extraintestinal symptoms, such as fatigue, brain fog, dermatitis, anemia, or muscle and joint pain. The pathophysiology of non-autoimmune gluten sensitivity is still controversial, and whereas an increase in intestinal permeability (a leaky gut) is common in people with Celiac disease who continue to consume gluten, it’s not known whether people who don’t have Celiac disease but are sensitive to gluten are at risk of developing a leaky gut.\nIs Eating A Small Amount of Gluten Okay?\nIf someone has Celiac disease then without question they need to strictly avoid gluten. Eating even a small amount can cause the development of the autoantibodies I discussed before, which in turn will lead to inflammation. I realize it’s not easy to completely avoid gluten, and what makes it even more challenging is that there are many hidden sources of gluten. Speaking of which, if you haven’t done so already I’d recommend reading my blog post entitled “Are You Really Gluten Free?”\nIf someone has a non-autoimmune gluten sensitivity problem should they still be strict when it comes to avoiding gluten? Well, if anyone has any type of gluten sensitivity I think it’s important to strictly avoid gluten. However, while someone with Celiac disease needs to avoid gluten on a permanent basis, there still is controversy over whether someone with a non-autoimmune gluten sensitivity should avoid gluten for the rest of their life. Some healthcare professionals will recommend for people with any type of gluten sensitivity to avoid gluten on a permanent basis. I’m still not sure about this, as while I think that everyone should minimize their consumption of gluten, even if they’re not sensitive to it, I’m not sure if everyone who has a gluten intolerance but doesn’t have Celiac disease needs to completely avoid gluten on a permanent basis. For example, if someone has developed a gluten sensitivity due to a leaky gut, upon healing the gut there is a chance that they might be able to tolerate gluten. But of course keep in mind that there is no good reason for eating gluten, and so even if someone can tolerate gluten it still is a good idea to minimize one’s consumption of it.\nCeliac Disease and Thyroid Autoimmunity\nWithout question there is a much higher prevalence of Celiac disease in people with autoimmune thyroid conditions. This is the case with both Hashimoto’s Thyroiditis and Graves’ Disease.\nCeliac Disease and Hashimoto’s Thyroiditis. One study involved 150 newly diagnosed patients with autoimmune thyroid disease, and the data suggested a significantly higher prevalence of Celiac disease in patients with autoimmune thyroid disease, in particular with Hashimoto’s Thyroiditis (4). A small study involving 27 adults with newly diagnosed Celiac disease showed that they had an increased risk of thyroid autoimmune conditions (5). This study showed that a gluten-free diet didn’t prevent the progression of the autoimmune thyroid condition during a follow-up of one year. So while some people with autoimmune thyroid conditions report a decrease in thyroid antibodies when they stop consuming gluten, this definitely isn’t the case with everyone. Another small study showed that a significant proportion of patients with Hashimoto’s Thyroiditis present signs of “potential” Celiac disease and of activated mucosal T cell immunity (6). Since Celiac disease seems to be more common in patients with hypothyroidism, a study looked to determine whether the absorption of levothyroxine was influenced by the presence of Celiac disease (7). The study showed that levothyroxine malabsorption likely occurs with hypothyroidism and untreated Celiac disease, and that absorption may improve after Celiac disease treatment. This is of course very important, as if someone with hypothyroidism is taking thyroid hormone and it doesn’t seem like it’s working well, it’s possible that they are having absorption problems which is responsible for this.\nCeliac disease and Graves’ Disease. One study looked to determine the prevalence of Celiac disease in people with Graves’ Disease. This involved 111 patients, and the study showed that the prevalence of Celiac disease in patients with Graves’ hyperthyroidism was 4.5% as compared with 0.9% in healthy controls (8). Another study involving 161 patients looked to screen for Celiac disease in patients with Graves’ Disease, and confirmed that patients with Graves’ disease are at a substantial risk of Celiac disease (9).\nCeliac Disease and Thyroid Autoimmunity in Children. Many children also have Celiac disease, and a few studies have shown a higher prevalence of thyroid autoimmunity in children with this condition (10) (11). This is very important, as if you have a child with Celiac disease, even if they aren’t experiencing any thyroid symptoms it probably would be a good idea to screen them for the presence of thyroid antibodies, especially anti-thyroglobulin antibodies and anti-thyroid peroxidase antibodies.\nSteps To Take After Being Diagnosed With Celiac Disease\nIf someone has been diagnosed with Celiac disease then they want to take the following steps:\n1. Avoid gluten…permanently. If someone has Celiac disease, eating even a small amount of gluten can result in the production of the autoantibodies mentioned earlier. As a result, if you have been diagnosed with Celiac disease you really do need to avoid eating gluten on a permanent basis.\n2. Consider avoiding other foods which cross react with gluten. This isn’t always necessary, although one does need to consider that eating other foods such as corn can cause problems in people with Celiac disease. Research has shown that the proteins from corn can cause a Celiac-like immune response due to similar or alternative pathogenic mechanisms to the proteins found in wheat (12). In other words, eating corn can cause a similar response as gluten. So if someone with Celiac disease completely eliminates gluten from their diet but still has overt symptoms, then they might be reacting to corn, or a different type of food.\n3. Conduct tests for anemia, nutrient deficiencies, and bone density. Because Celiac disease results in malabsorption of many nutrients it is a good idea to do some additional testing. For example, everyone with Celiac disease should receive a CBC and an iron panel. Serum testing for vitamin B12, folate, vitamin D, and magnesium also would be a good idea. Although the blood isn’t the best method of evaluating mild to moderate deficiencies of many nutrients, if someone has a severe nutrient deficiency this usually will show up on serum testing. Regardless of what the testing shows, everyone with Celiac disease should be on a good quality multivitamin with minerals. Due the malabsorption of nutrients people with Celiac disease might also have a decrease in bone density. As a result, in some cases having a bone density scan would be a good idea.\n4. Test for a leaky gut. If someone has Celiac disease then I would recommend either doing a test for intestinal permeability (a leaky gut), or just assuming that someone has a leaky gut and then doing things to help them repair the gut. Many people with Celiac disease who stop eating gluten don’t take measures to help repair the gut.\n5. Eat whole, healthy foods. This might seem obvious, but it is possible to be gluten free but still eat an unhealthy diet. And this what many people with Celiac disease do, as they avoid gluten, but eat a lot of “gluten free processed foods”. This includes gluten free cookies, gluten free cakes, gluten free potato chips, gluten free cereal, gluten free pasta, gluten free pizza, etc. I’m not saying that you can never eat these foods, but you of course want to eat mostly whole foods, and minimize the consumption of processed foods, even if they are gluten free.\nThere’s a lot more information relating to Celiac disease that I didn’t cover in this article. For example, there does seem to be a correlation between Celiac disease and irritable bowel syndrome (IBS). Other conditions such as autism and schizophrenia might also be related to gluten ingestion. If you visit pubmed.gov and do some research you will see that there is a correlation between Celiac disease and many chronic health conditions. And when this is the case, eliminating gluten usually will result in a dramatic improvement in the person’s health.\nIn summary, many people have Celiac disease, which is an autoimmune condition that is triggered by gluten. Celiac disease is more common in people with Graves’ Disease and Hashimoto’s Thyroiditis. Although a Celiac panel doesn’t always rule out Celiac disease, if someone is consuming gluten then it’s usually a good place to start. If someone has Celiac disease then they will need to avoid gluten on a permanent basis. They also will want to consider doing testing for anemia, nutrient deficiencies, and perhaps a bone density scan. Having a leaky gut is common for those people with Celiac disease, and so this is also something to consider. Finally, in addition to avoiding gluten, you ideally want to eat mostly whole, healthy foods, and minimize the consumption of processed gluten free foods.","Should You Go Gluten-Free? The Truth Behind The World’s Most Hated Protein\nDoes it have any benefit for people without celiac disease? Here’s what you need to know\nIn recent years, more and more people are declaring themselves “gluten free”, exasperating dining partners, waiters, and researchers alike. Gluten is a protein found in wheat and many other grains – meaning it’s found in many common Western foods such as bread and pasta.\nThe gluten-free diet can be a life saving treatment for people diagnosed with genuine medical illnesses. That’s a fact.\nBut does it make any difference for the people who worship it as a weight loss diet? Or as a way of “cleansing” their bodies?\nThe gluten-free diet has been around for more than half a century, when doctors first established a link between gluten and the symptoms of celiac disease, an autoimmune condition where gluten triggers intestinal damage.\nThere is no cure for celiac disease or other gluten sensitive conditions, but the gluten-free diet allows those diagnosed to live comparatively healthy, symptom free lives (albeit ones without things like wheat bread).\nBut when carb-cutting took centre stage for people looking to lose weight, it was inevitable that some people would take it to the extreme and adopt the gluten-free diet.\nAfter all, why cut out just wheat when you can also eliminate barley, rye, and other grains as well?\nDoes this strategy offer any advantage for the people who don’t have celiac disease?\nAccording to a 2015 study published in the British Journal of Nutrition, the answer is maybe, but not if you’re relying on gluten-free products to fill the new gaps in your diet.\nWhen Is a Gluten-Free Diet Healthy?\n“The gluten-free diet can be healthy if the focus is on whole foods: vegetables, fruits, whole grains, lean proteins, nonfat or low-fat dairy, seeds, nuts, legumes, beans,” says Nancee Jaffe, registered dietitian to the UCLA Digestive Health and Nutrition Clinic.\nBut, she emphasises, grains like whole wheat and barley offer health benefits as well. A gluten-reduced diet may be the best option.\nOne benefit: Reducing your intake of gluten means that you need to increase your intake of other foods. Because food manufacturers are now catering to the gluten-free demo, there’s a host of new, healthful products on the market.\nFor grains, work amaranth, buckwheat, millet, or quinoa, into your diet. Each variety has different vitamins and minerals that can help you round out your diet.\nRelated: Is Gluten Really That Evil?\nWhen Is a Gluten-Free Diet Unhealthy?\nIf you’re cutting out healthy grains like whole wheat and barley and supplementing your diet with processed gluten-free products, you may be doing more harm than good.\nGluten-free snacks like pretzels and cookies are often higher in fat and sugar and lower in fibre and protein than their gluten equivalents, according to new research. They are also often not enriched or fortified with key nutrients.\nThat means that you’re probably paying more money for less nutritious food: gluten-free products can cost up to 200 percent more than their gluten-containing counterparts, according to research from Nova Scotia.\nFocusing on whole foods is an excellent way to maintain a healthy diet, but think carefully about your overall health before you decide to cut out gluten altogether."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_language_proficiency_implied","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f9e8589a-4015-43f8-8118-c4859847c029>","<urn:uuid:dc71cbbb-cb72-47ca-a0db-c23de0e80d51>"],"error":null}