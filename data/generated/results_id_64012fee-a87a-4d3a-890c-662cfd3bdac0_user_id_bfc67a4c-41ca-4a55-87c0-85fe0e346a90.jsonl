{"question":"How do Planning and External Environment considerations complement each other in business operations?","answer":"Planning and External Environment considerations are interconnected as planning takes into account both internal and external factors while setting targets. Planning is continuous in nature and never ends while the organization exists, specifically because it must respond to various external environmental factors like economic conditions, technological changes, and socio-cultural factors. The planning function sets goals and chooses means to achieve them while considering the broader macro environment factors such as economic policies, political conditions, and demographic changes that affect business operations.","context":["Planning is deciding in advance what to do, how to do it, when to do it and who is to do it.\nOrganizing is the process of defining and grouping the activities of the organization.\nStaffing is the process of recruitment, selection, development, training, promotion etc.\nDirecting is a process to instruct, guide, communicate, inspire and motivate employees to achieve organizational objective\nCo-ordinating is a process to establish communicate, inspire and motivate employees to achieve organizational objectives.\nControlling measures the deviation of actual performance from the standard performance and takes corrective actions.\nThe main object is to set goals and choosing the means to achieve these goals.\nThe main objective is to identify and bring together all the resource.\nThe main objective is to appoint right type of people at the right positions and at the right time.\nThe main objective of giving direction to the subordinates is to get the things done in the right manner.\nThe main objective is ensure unity of efforts of the employees and smooth functioning of the organization.\nThe main objective is to ensure that the targets must be achieved as per the plans.\n3. Area of Function\nIt includes Setting objectives Identifying alternate course of action Select the best plan\nIt includes Identification of the activities Grouping of related activities.\nIt includes Recruitment Training Promotion Transfers, etc.\nIt includes Guiding Instruction Inspiring Motivating Communicating\nIt includes Top Level Management Middle Level Management Lower Level Management\nIt includes Fixation of standards Measurement of actual performance.\nInternal as well as external factors are considered while setting targets\ninternal and external factors are considered for making arrangement of resources\nStaffing is mostly concerned with Internal factors.\nDirecting is also concerned directly with Internal (human) factors.\nIt is concerned with Internal factor\nIt takes care of Internal as well as external factor for taking corrective action\nIt is the starting point of management Other functions depends of Planning\nIt following planning, resources are organized as per the Planning\nHuman Resources are needed for the organization so it follows Organizing.\nDirection is always needed. It follows organizing and staffing.\nIt is an important element of organizing It follows Planning.\nIt follows all other functions. It is an end function of management process.\nPlanning is done as per the resources required for achieving the targets.\nAll the resources needed are arranged i.e. men, machine, methods, material and money.\nIt is related with Human Resources.\nIt gives direction to the employees and regarding use of other resources.\nIt is related with Human Resources. It is an integrated effort.\nIt is related with all the resources as it helps to achieve targets\nThe targets set are analysed and then selected.\nAl the resources are arranged to achieve targets.\nIt includes proper allocation of human resources to achieve better results.\nProper direction to the employees is needed to achieve the targets.\nInternal Co-ordination among employees is must to achieve the desired results.\nActual Performance is compared with standard performance are taken to achieve targets.\nIt is continuous in nature. Planning will never come to an end till the organization exists.\nIt takes place till the time all the resources are collected\nIt is a continuous process as it deals with human beings.\nIt is a continuous process as directing is needed till the targets are achieved.\nIt is a process where only people are concerned so continuity is needed.\nIt is a process which starts once the actual target is achieved.\n9. Levels of Management\nTo level management is with related planning of activities.\nTop level management and middle level managers are related with organizing of resources.\nMiddle level management are related with recruitment, training, promotion, etc.\nMiddle level manager provides direction to the employees for achieving targets.\nAll the level i.e. top to middle to the lower co-ordination is required to achieve targets.\nTop level management and middle level managers are responsible for proper controlling of activities.","Components of International Business Environment\nThe business environment can be divided into two ways.\n(A) Internal Environment\n(B) External Environment\n(I) Micro Environment\n(II) Macro Environment\nInternal Environment: It includes all those factors which are with in the business itself and influence business. These are usually under the control of business. For example, objectives, policies, organisation structure, management, production method, etc.\nExternal Environment: It includes all those factors which are outside the business and business has no control over these factors. It is divided into two parts:\n(I) The Micro Environment of Business\nThe micro environment consists of the forces in the company’s immediate environment that affects the performance of the company. The micro factors may affect different firms in a particular industry in different ways.\nIt consists of following elements:\n(i) Customers: Customers have direct impact on the micro environment of business. The desires, preferences, attitudes and expectations of customers keep on changing and it impose a constant challenge to business.\n(ii) Suppliers: Suppliers are the most important force in the task environment of a business. For the smooth functioning of the business it is very important to have a reliable source of supply. Multiple source of supply helps to reduce the risk of unavailability or uncertainty of supply of raw material.\n(iii) Labour: In big organisations, where hundreds of workers are employed, the labour force is organized in the form of trade unions. The trade unions pressurise the management for the fulfillment of their demands like higher wages, better working conditions and bonus etc.\n(iv) Competitors: Competitors play a vital role in running the business enterprise. Business has to adjust its business activities according to the behaviour of the competitors. It is very necessary to know about the competitor’s strategies, policies and product features for other companies to secure its market share.\n(v) Regulating Agencies: The regulators include government departments and other organisations which monitor the activities of business. There are certain departments like income tax department, quality control department and other revenue departments and professional bodies like ICAI which prescribes certain standards and practices for the business in their respective areas.\n(II) The Macro Environment of Business\nThe macro environment of business includes activities which are uncontrollable and need proper attention on the part of a business enterprise. It refers to the general and overall environment within which an environment entity operates.\nIt consists of following elements:\n(i) Economic Environment: It refers to all those economic factors which have a bearing on the functioning of a business unit. The major macro-economic factors which have considerable influence on business are:\n· Economic Systems\n· Economic Planning\n· Economic Policies\n(b) Industrial Regulations\n(c) Business Laws\n(d) Import and Export Regulations, etc.\n· Economic Growth\n· Interest Rates\n· Economic Reforms\n· Currency Exchange Rates\n(ii) Political and Government Environment: Political environment constitutes all the factors related to government affairs such as the type of government in power, the ideology of ruling party, attitude of government towards different groups of societies. The businessman has to make changes in his organisation according to the changing factor of political environment. For example, in 1977 when Janata Party came in power they made the policy of sending back all the foreign companies. As a result, the Coca Cola and IBM companies had to close their businesses and leave the country.\n(iii) Socio-Cultural Environment: Socio-Cultural environment include all the social factors like people’s attitude, education system, their beliefs and values, culture, religion, ethical issues and social responsibility of business etc. All these factors have a great impact on activities of business enterprise. For instance, the chocolate boy ad of AXE Effect was banned by Information and Broadcasting Ministry on grounds of being offensive and vulgar.\n(iv) Technological Environment: It consists of new products, new techniques and new approaches to production, new methods and new equipments. In order to survive in today’s competitive world, a business has to adopt technological changes from time to time. If they will not do so, they will be out of market. For instance, in late 1990’s Pagers were very popular among the people, but then came the mobile phone revolution. The companies those were manufacturing pagers at that time, they have to shift towards mobile phones.\n(v) Demographic Environment: It includes:\n(a) Size, growth rate, age composition, sex etc. of population\n(b) Family size\n(c) Educational level\n(d) Economic stratification of population, etc.\nAll these demographic factors are relevant to business. These factors affect the demand for goods and services. For example, increase in the demand of baby products shows the increase in the birth rate in certain area.\n(vi) International Environment: Due to liberalisation, globalisation, now the Indian companies are competing with the foreign companies. It has been observed that major international developments have their impact on domestic market. Recent example is increase in fuel prices in Indian market because of rise in prices of crude oil at international level. In the same way, because of US recession, Indian stock market also faced downfall."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e927b115-d4c1-41c8-a904-f62e47a86a27>","<urn:uuid:cef7f424-66da-49a2-97d2-527c495ff046>"],"error":null}
{"question":"How does corrosion impact aircraft safety during night operations, and what are the notification requirements for structural hazards in aviation?","answer":"Corrosion can significantly compromise aircraft safety, particularly during challenging night operations. As demonstrated in the Cessna 150J accident, corrosion had a detrimental effect on the strength of an aluminum component (the flap track), significantly diminishing its structural integrity and life expectancy. This type of damage may go unnoticed despite regular inspections, as shown in this case where the corrosion wasn't detected during annual inspections. Regarding structural hazards, there are strict notification requirements. When installing new structures that could be obstacles to air navigation, operators must notify the Transport Canada Regional Office by completing an Aeronautical Assessment Form. They must also notify NAV CANADA's Land Use Department to assess potential impacts on flight procedures. Additionally, if there's a failure in obstacle marking or lighting, immediate notification through NAV CANADA Flight Information Centre is required, including detailed information such as structure type, location, height, and estimated repair time.","context":["Aviation Investigation Report A17O0209\nCollision with water\nCessna 150J, C-FHPU\nThe Transportation Safety Board of Canada (TSB) investigated this occurrence for the purpose of advancing transportation safety. It is not the function of the Board to assign fault or determine civil or criminal liability. This report is not created for use in the context of legal, disciplinary or other proceedings. See Ownership and use of content.\nHistory of the flight\nA privately registered Cessna 150J aircraft (serial number 15070602, registration C-FHPU) departed Brampton-Caledon Airport (CNC3), Ontario, at approximately 1951Footnote 1 for a night visual flight rules (VFR) cross-country training flight to Goderich Airport (CYGD), Ontario. Weather was suitable for the flight with light winds, good visibility, and few clouds.\nThere were 2 pilots on board the aircraft: the owner of the aircraft, who was seated in the left seat, and an instructor from the Brampton Flying Club, who was seated in the right seat.\nAfter departure, the aircraft climbed to an altitude of 3000 feet above sea level, where it remained for the cruise portion of the flight. The aircraft's position was recorded on radar until coverage was lost as the aircraft descended into CYGD. The last radar return, at 2036, showed the aircraft at 1800 feet above sea level, 0.8 nautical miles (nm) east of the airport.\nThe aircraft was observed flying westbound toward Lake Huron, then in a southeast direction toward the airport, followed by a rapid descent. Tall trees then blocked the aircraft from view, but the sound of the aircraft hitting the water was audible shortly thereafter.\nEmergency services were contacted and an extensive search was conducted. The aircraft was located at the bottom of Lake Huron, in 25 feet of water, approximately 0.6 nm from shore. The aircraft was destroyed by the impact forces. Both occupants were fatally injured.\nAircraft and wreckage information\nThe aircraft was manufactured in 1969 and had accumulated approximately 5896 hours total time since manufacture.\nThe aircraft wreckage was recovered and transported to the TSB regional facility in Richmond Hill, Ontario, where it was examined in further detail. Damage to the aircraft indicated that it struck the water in a steep (nearly vertical) nose-down attitude. The flaps were found in an asymmetric condition: the right wing flap was in the fully retracted (up) position, whereas the left wing flap was extended (down) more than its physical limit of 40°.\nThe aircraft systems were examined to the degree possible and all flight control surfaces were accounted for. Damage to the propeller was consistent with considerable power being produced at the time of impact.\nThe instruments were recovered and examined at the TSB Engineering Laboratory in Ottawa, Ontario. The examination of the turn coordinator, directional gyro, altimeter, and airspeed indicator did not yield any useful information; however, the vertical speed indicator was indicating a rate of descent in excess of 2000 feet per minute. The artificial horizon was tested, and it was determined that it was functional at the time of impact.\nExamination of wing flaps and associated components\nThe wing flaps on the Cessna 150J are electrically driven. The flap actuator assembly is located in the right wing and there are 2 drive pulleys (1 in each wing), which are interconnected by cables. The drive pulleys are connected to the flaps via push-pull rods. A comprehensive examination of the wing-flap system and associated components was conducted.\nThe left wing flap sustained damage to the inboard end as a result of the impact. The surfaces of the flap were otherwise undamaged.\nThe forward section of the left wing inboard aft flap track was fractured (Figure 1). As a result, the left flap inboard aft roller, which is normally positioned inside the associated flap track, was located outside of the broken flap track.\nThe outboard flap track was still intact and, as a result, it was possible for the left flap to be cocked in position, preventing it from travelling\nThe fractured surface of the inboard flap track was examined with a scanning electron microscope. It was determined that the damage was caused by corrosion, which gave the appearance of delamination (Figure 2). The corrosion had been present prior to the occurrence; however, when or how it began could not be determined. Corrosion has a detrimental effect on the strength of an aluminum component and can significantly diminish the structural integrity and life expectancy of an aircraft component.\nThe fracturing of the non-corroded portion of the flap track was caused by overstress from a combined shear and tension load. However, it could not be determined whether this happened before or during impact.\nThe arm attached to the flap drive pulley in the left wing was bent almost 90° from its original position as a result of tensile overload (Figure 3). It is possible that this damage occurred during the impact sequence. However, there was no corresponding damage to the left flap, which is connected to the drive pulley arm via a push-pull rod. It is also possible that the bending of the drive pulley arm occurred during operation of the flaps. If the left flap was unable to move, the arm may have been bent from the cable tension produced by the flap actuator.\nThe examination of the right wing flap, the flap actuator, and other components indicated that the right flap was fully retracted at the time of impact. The flap direct (down) interconnect cable that drives the left flap down in alignment with the right flap was found broken near the right wing root. An examination of the cable indicated that it had broken due to overstress; there was no sign of metal fatigue or corrosion.\nIt could not be determined with certainty when the cable broke. The force of the impact may have broken the cable; however, it is also possible that, if the left flap was cocked, extreme tension applied during operation of the flaps may have broken the cable.\nAircraft maintenance and inspection\nThe aircraft was being maintained under the pre-approved maintenance schedule for privately registered aircraft (Canadian Aviation Regulations [CARs] Standard 625, Appendix B, Part I). The Standard 625, Appendix B, Part I inspection must be completed at intervals not exceeding 12 months and includes tasks to inspect the aircraft for corrosion.\nThe last annual inspection of the aircraft was completed in October 2016; the aircraft had accumulated 32 hours of air time since that inspection. No anomalies or modifications related to the flaps had been recorded in any of the aircraft log books since the aircraft was imported into Canada in 1990.\nThe Cessna 150 Service Manual provides a supplemental inspection document detailing the following recommended intervals with respect to inspecting the flap tracks for corrosion:\n- For aircraft operating in areas where the corrosion severity is rated as mild to moderate, the initial inspection is to be completed after 20 years of operation, then every 10 years.Footnote 2\n- For aircraft operating in areas where the corrosion severity is rated as severe, the initial inspection is to be completed after the first 10 years, then every 5 years.Footnote 3\nThese recommended supplemental inspections are not mandatory for privately registered aircraft in Canada, and there was no record of any such inspections in the aircraft log books since the aircraft was imported to Canada.\nRecords indicate that both pilots were certified and qualified for the flight in accordance with existing regulations. There was nothing to indicate that their performance was degraded by physiological factors.\nThe pilot had received his private pilot licence approximately 5 weeks prior to the accident, and had approximately 142 hours total flight time. The occurrence flight was a night cross-country training flight, which is one of the requirements to obtain a night rating.\nThe instructor held a commercial pilot licence with Class 3 instructor rating and a night rating. She had been employed by the Brampton Flying Club since January 2017 and had approximately 700 hours total flight time, including 42 hours at night and 35 hours with reference to instruments. According to records, the instructor had flown to CYGD at night on 2 previous occasions, once in May 2017 and once in July 2017.\nNeither the pilot nor the instructor held an instrument rating.\nBrampton Flying Club\nThe Brampton Flying Club owns and operates a fleet of aircraft primarily used for flight training and rental. However, it also allows students to use their own aircraft for training.\nIn order for students to obtain realistic night flying experience, the Brampton Flying Club's Brampton Flight Centre Flight Training Operations Manual states the following:For the night x country requirement, two popular routes are to Muskoka and Goderich. Muskoka gives the student good experience of very little reference points and black hole effect and Goderich will require either a take-off or approach over water which will require some reference to instruments. Both routes take approximately two hours including a couple of circuits. The student should complete the planning for the x country and then fly the route with very little intervention.Footnote 4\nAt the time of the occurrence, the Brampton Flying Club did not provide its instructors with specific flight training to address the black-hole effect, or provide its non-instrument-rated instructors with proficiency training. In addition, instructors were not tested to ensure they were proficient at operating aircraft during black-hole approaches or at night with limited visual references. This type of flight training is not required by regulation.\nNo safety reports had been filed in the Brampton Flying Club safety management system regarding the black-hole effect or operating at night with limited visual reference.\nNight visual flight rules\nThere are many risks associated with night flying, given the poor visual cues, especially on takeoff and landing. Few or no visual references at night can lead to various illusions that cause spatial disorientation due to the lack of a discernible horizon. Night flying over featureless terrain, such as bodies of water or wooded terrain—called black-hole conditions—is particularly difficult.\nThe principle behind VFR flight is that the pilot uses visual cues outside the aircraft (e.g., the horizon or ground references) to determine the aircraft's attitude. Therefore, some basic requirements must be met when conducting VFR flight, no matter whether it is during the day or at night.\nAccording to CARs sections 602.114 and 602.115, an aircraft must be \"operated with visual reference to the surface,\"Footnote 5Footnote 6 regardless of whether it is operated in controlled or uncontrolled airspace. The CARs define surface as \"any ground or water, including the frozen surface thereof.\"Footnote 7 However, the CARs do not define \"visual reference to the surface,\" which has been widely interpreted by the industry to mean visual meteorological conditions.\nTherefore, a flight conducted over an area away from cultural lighting and where there is inadequate ambient illumination to clearly discern a horizon would not likely meet the requirements for operation under VFR (i.e., to continue flight solely by reference to the surface). Instead, such flights would require pilots to rely on their flight instruments to ensure safe operation of the aircraft.\nIn this occurrence, the pilots could expect to see lights from the town of Goderich, located approximately 1 nm south of the airport, and some cultural lighting (e.g., houses, traffic on the road) to the east and north of the airport. However, to the west of the airport, over Lake Huron, pilots would not generally see any cultural lighting.\nA TSB investigation reportFootnote 8 on a helicopter that crashed while departing under VFR at night from a remote airport with limited lighting raised the issue of a lack of clarity in the definition of what flight \"with visual reference to the surface\" means in practice. The TSB recommended that\nthe Department of Transport amend the regulations to clearly define the visual references (including lighting considerations and/or alternate means) required to reduce the risks associated with night visual flight rules flight.\nTSB Recommendation A16-08\nIn its response, Transport Canada indicated that it will take a two-fold approach to address this recommendation to reduce the risks associated with night VFR flights. In the short term, Transport Canada will conduct safety promotion and education activities, which will be followed by a regulatory amendment project.\nThe Board has assessed Transport Canada's response to Recommendation A16-08 as showing Satisfactory Intent.\nIn this accident, an in-flight flap asymmetry could not be ruled out. The broken flap track was heavily corroded, which weakened its structural integrity and life expectancy. The corrosion had been present for some time, but was not noticed in any of the annual inspections that followed standards set out in the CARs. The manufacturer recommends periodic supplemental corrosion inspections; however, these are not required by regulation, and none were documented as having been completed.\nAlthough both pilots had received some instrument training, neither of them had an instrument rating. Nonetheless, the aircraft was being operated at night in an area with limited visual reference to the surface. As detailed in TSB Recommendation A16-08, the CARs do not clearly define the visual references that are required in these situations.\nThis report concludes the Transportation Safety Board's investigation into this occurrence. The Board authorized the release of this report on . It was officially released on .","For a pilot to safely fly under visual flight rules, they need to be aware of structures that are obstacles to air navigation so they will have sufficient time to react and avoid them.\nTransport Canada requires that structures, which are obstacles to air navigation, are marked and/or lighted so they can be easily identified during the day and night. Some structures that may be obstacles include:\n- broadcast towers\n- catenaries (power transmission lines crossing a river or valley)\n- cooling towers, and\nOn this page:\nThis page includes information on ....\n- Canada’s rules regarding marking and lighting of obstacles\n- Who to notify when you install a new obstacle\n- How to report a failure of marking or lighting [for owners of obstacles].\nCanada’s rules for the marking and lighting of obstacles\nThe following rules are applicable to marking and lighting of structures that are obstacles to air navigation:\n- Part VI, Division III — Marking and Lighting of Obstacles to Air Navigation Sections 601.23 to 601.29 of the Canadian Aviation Regulations (CARs)\n- Standard 621 — Obstruction Marking and Lighting\nWho to notify when you plan to install new obstacle\n- If you plan to install a new structure that will be an obstacle to air navigation, you must notify the Transport Canada Regional Office by completing an Aeronautical Assessment Form (PDF, 792 KB) so they can assess the structure and your plans for construction.\nContact information for Transport Canada regional offices, can be found in Appendix A to Standard 621.\n- You should also notify NAV CANADA's Land Use Department and respond to subsequent requests. This is so they can assess whether your structure impacts flight procedures, as this can occur even if it has been determined to not be an obstacle by Transport Canada. NAV CANADA Land use forms are available at ....\n- With respect to the Navigation Protection Program (NPP), you should consult the Apply to the NPP webpage if you plan to construct, place, alter, remove or decommission a work in, on, over, under, through or across any navigable water in Canada.\nReporting a Failure of marking or lighting\n(A report of failure should be done by the owner of the obstacle)\nIf there is a failure of either marking or lighting a NOTAM is issued immediately through the NAV CANADA Flight Information Centre (FIC). Action to repair should be started as soon as possible.\nWhen reporting a failure, the following information should be provided. This enables later identification of the affected structure and review of the progress for repair.\n- The originator’s name, telephone number, email address\n- Name of the originator’s company\n- Name of company owning the obstacle\n- Obstacle site number or ticket number.\n- NAV CANADA Land Use # (if available)\n- The type of structure;\n- Location of structure (latitude and longitude)\n- Height of structure above ground level (AGL)\n- Overall height above mean sea level (AMSL)\n- An estimated return to service date.\nThe originator should subsequently notify NAV CANADA when the repair is complete.\nA NOTAM is intended to be temporary nature and short duration. The originator should advise NAV CANADA, if the repair will take more than 3 months.\nTo facilitate the NOTAM process, the originator should respond to subsequent NAV CANADA requests for more information."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:86128d1c-bf92-4367-be29-4de9e3fd94ae>","<urn:uuid:529dde59-0c08-4a51-a964-d3b0038bb13c>"],"error":null}
{"question":"How is climate defined in modern meteorology, and what makes the current climate change historically unprecedented?","answer":"Climate is defined by weather patterns - including precipitation, temperature, humidity and wind - averaged over a 30-year period. It gives us expectations about typical weather in a region. The current climate change is unprecedented in several ways: CO2 concentrations have reached record highs of 379 ppm compared to the historical range of 180-300 ppm over the past 650,000 years, and this increase has happened at an exceptionally rapid rate. Current temperatures are warmer than they've been for at least 500-1000 years. While the Earth has experienced similar temperature changes during glacial cycles (4-7°C), those changes occurred gradually over about 5,000 years, whereas current warming is happening much more rapidly.","context":["Check out this resource from NOAA to learn about the major climate zones in the continental United States.\nThere’s a lot of talk about climate change in the news, but it’s a topic that can be really hard to understand. What exactly is climate change? Put simply, climate change is a change in average weather patterns over a long period of time. However, people often confuse natural variations in weather with climate change. In order to understand the distinction, you have to understand the difference between weather and climate. Weather refers to atmospheric conditions like temperature, precipitation, and wind at a specific time and place, whereas climate refers to long-term trends of weather.\nLook outside. Is it sunny? Is it raining? Is it snowing? Is it windy? Whatever is happening right now is weather. Weather varies from day to day. Some days it might be sunny, other days in might be cloudy and rainy.\nA region’s climate is determined by weather patterns — patterns in precipitation, temperature, humidity and wind — usually averaged over a 30-year span. Climate gives us an idea of what the weather in a particular region might be. For example, if it’s winter in Miami, you don’t expect it to snow. That’s because Miami has a tropical climate with warm temperatures year-round and lots of rain. Likewise, if it’s winter in Maine, you don’t expect it to be 95 degrees Fahrenheit.\nTo determine changes in climate, you have to look at long-term trends and averages of past weather data. For example, past weather data from California’s Central Valley shows that this area is experiencing warmer temperatures and less fog compared to what would be normal variations in weather. This suggests a changing climate.\nClimate change is happening now and is already impacting the planet. With climate change, we are seeing shifts in regional climate. For example, spring temperatures in the Midwest arrive about two weeks earlier compared to the 1950s. Parts of the Northeast are experiencing an increase in heavy rain and snow events, and parts of the Southeast are having more days with temperatures above 95 degrees. These are all examples of changes to regional climates.\n- What does the term “weather” mean?\n- What is climate?\n- What does the term “climate change” mean?\n- Describe the climate where you live.\nThis post is part of our Clue into Climate collection of resources.\nThis explainer is featured in our Clue into Climate e-book series. Click on the tabs below to download our free e-books and subscribe to our iTunes U course. You can also visit our e-books page to view our other offerings.\nLearn about how climate change influences precipitation patterns and how it impacts our frozen landscape.\nInvestigate shifts in the distribution of plant and animal species due to climate change, and the effects of increased carbon dioxide emissions on the ocean.\nSee how communities are preparing for sea level rise and other impacts of climate change, and ways we can help minimize future climate change.","Frequently Asked Question 6.2\nIs the Current Climate Change Unusual Compared to Earlier Changes in Earth’s History?\nClimate has changed on all time scales throughout Earth’s history. Some aspects of the current climate change are not unusual, but others are. The concentration of CO2 in the atmosphere has reached a record high relative to more than the past half-million years, and has done so at an exceptionally fast rate. Current global temperatures are warmer than they have ever been during at least the past five centuries, probably even for more than a millennium. If warming continues unabated, the resulting climate change within this century would be extremely unusual in geological terms. Another unusual aspect of recent climate change is its cause: past climate changes were natural in origin (see FAQ 6.1), whereas most of the warming of the past 50 years is attributable to human activities.\nWhen comparing the current climate change to earlier, natural ones, three distinctions must be made. First, it must be clear which variable is being compared: is it greenhouse gas concentration or temperature (or some other climate parameter), and is it their absolute value or their rate of change? Second, local changes must not be confused with global changes. Local climate changes are often much larger than global ones, since local factors (e.g., changes in oceanic or atmospheric circulation) can shift the delivery of heat or moisture from one place to another and local feedbacks operate (e.g., sea ice feedback). Large changes in global mean temperature, in contrast, require some global forcing (such as a change in greenhouse gas concentration or solar activity). Third, it is necessary to distinguish between time scales. Climate changes over millions of years can be much larger and have different causes (e.g., continental drift) compared to climate changes on a centennial time scale.\nThe main reason for the current concern about climate change is the rise in atmospheric carbon dioxide (CO2) concentration (and some other greenhouse gases), which is very unusual for the Quaternary (about the last two million years). The concentration of CO2 is now known accurately for the past 650,000 years from antarctic ice cores. During this time, CO2 concentration varied between a low of 180 ppm during cold glacial times and a high of 300 ppm during warm interglacials. Over the past century, it rapidly increased well out of this range, and is now 379 ppm (see Chapter 2). For comparison, the approximately 80-ppm rise in CO2 concentration at the end of the past ice ages generally took over 5,000 years. Higher values than at present have only occurred many millions of years ago (see FAQ 6.1).\nTemperature is a more difficult variable to reconstruct than CO2 (a globally well-mixed gas), as it does not have the same value all over the globe, so that a single record (e.g., an ice core) is only of limited value. Local temperature fluctuations, even those over just a few decades, can be several degrees celsius, which is larger than the global warming signal of the past century of about 0.7°C.\nMore meaningful for global changes is an analysis of large-scale (global or hemispheric) averages, where much of the local variation averages out and variability is smaller. Sufficient coverage of instrumental records goes back only about 150 years. Further back in time, compilations of proxy data from tree rings, ice cores, etc., go back more than a thousand years with decreasing spatial coverage for earlier periods (see Section 6.5). While there are differences among those reconstructions and significant uncertainties remain, all published reconstructions find that temperatures were warm during medieval times, cooled to low values in the 17th, 18th and 19th centuries, and warmed rapidly after that. The medieval level of warmth is uncertain, but may have been reached again in the mid-20th century, only to have likely been exceeded since then. These conclusions are supported by climate modelling as well. Before 2,000 years ago, temperature variations have not been systematically compiled into large-scale averages, but they do not provide evidence for warmer-than-present global annual mean temperatures going back through the Holocene (the last 11,600 years; see Section 6.4). There are strong indications that a warmer climate, with greatly reduced global ice cover and higher sea level, prevailed until around 3 million years ago. Hence, current warmth appears unusual in the context of the past millennia, but not unusual on longer time scales for which changes in tectonic activity (which can drive natural, slow variations in greenhouse gas concentration) become relevant (see Box 6.1).\nA different matter is the current rate of warming. Are more rapid global climate changes recorded in proxy data? The largest temperature changes of the past million years are the glacial cycles, during which the global mean temperature changed by 4°C to 7°C between ice ages and warm interglacial periods (local changes were much larger, for example near the continental ice sheets). However, the data indicate that the global warming at the end of an ice age was a gradual process taking about 5,000 years (see Section 6.3). It is thus clear that the current rate of global climate change is much more rapid and very unusual in the context of past changes. The much-discussed abrupt climate shifts during glacial times (see Section 6.3) are not counter-examples, since they were probably due to changes in ocean heat transport, which would be unlikely to affect the global mean temperature.\nFurther back in time, beyond ice core data, the time resolution of sediment cores and other archives does not resolve changes as rapid as the present warming. Hence, although large climate changes have occurred in the past, there is no evidence that these took place at a faster rate than present warming. If projections of approximately 5°C warming in this century (the upper end of the range) are realised, then the Earth will have experienced about the same amount of global mean warming as it did at the end of the last ice age; there is no evidence that this rate of possible future global change was matched by any comparable global temperature increase of the last 50 million years."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:719c9f52-4eed-4088-8186-07b5b325298b>","<urn:uuid:3504e0be-1f20-4313-b40b-aa594b9e43e5>"],"error":null}
{"question":"Will pedestrian refuges on Bellasis Road and Barcelona's superblocks both help improve street safety in similar ways?","answer":"While both initiatives aim to improve street safety, they work differently. Bellasis Road's pedestrian refuge islands provide safe harbor in the median while crossing traffic, reducing pedestrian crashes by 39-46%. Barcelona's superblocks take a broader approach by restricting traffic to one-way systems with 10km/hr speed limits, making cars an afterthought to community life by converting road lanes into community spaces. Both demonstrate how street redesign can prioritize pedestrian safety, though through different mechanisms.","context":["Streets in the last century were primarily designed as a mode of transport for goods and services. This automobile-centric approach of street design stifled the human accessibility aspect, and eventually, cities forgot that every citizen is a pedestrian at some point in a day. But this scenario is changing, and cities globally are leading the movement to reinvest in their streets with a focus on pedestrian environment.\nMumbai is a city of pedestrians, with 51% of the total daily trips being walk trips, according to the April 2016 edition of the Comprehensive Mobility Plan of Greater Mumbai. Taking this into account, the Municipal Corporation of Greater Mumbai (MCGM) joined the global movement to improve pedestrian experience in August 2018, and announced the redesigning of Bellasis Road, a stretch between Nagpada and Mumbai Central Junction, in Byculla’s neighbourhood, to make it a ‘model street’. Following a series of consultations with stakeholders including institutions, vendors and the general public, the redesigning of this road is currently underway. The proposed new design complies with the national road safety standards, while safeguarding the vibrancy and local identity of the street.\nExisting road condition\nOriginally built in 1793, Bellasis Road is an 823-meter-long street, stretching between Nagpada Junction and Mumbai Central Station Junction. The street is an important east-west urban connector, with ground floor retail frontage along the corridor, which generates high pedestrian and vehicle volumes every day. The common issues observed on this road included lack of walkable footpaths, vehicular congestion, bottlenecks, multiple layers of street side parking and poor pedestrian crossing infrastructure.\nDuring a road safety audit conducted on Bellasis Road recently, a pedestrian environment data scan (PEDS) showed that most of the pedestrians use footpath despite its less-than-ideal condition. The haphazard placement of bus stops, utilities, shop extensions, vendors, planters, etc., reduced the walkable space on the footpath. Unorganized parking was another obstacle that every pedestrian had to negotiate. Along with the absence of safe pedestrian crossings, Bellasis Road was indeed every pedestrian’s nightmare.\nProposed Design and Design Elements\nThe proposed design of Bellasis Road included the following principles:\n- Maintaining contiguous vehicular traffic lanes: two lanes on each side\n- Creating multi-utility/parking zones all along the street\n- Introducing continuous footpaths\n- Introducing protected pedestrian crossings\n- Introducing ‘placemaking’ to manage the footpaths\nFollowing these principles in the new design, existing median alignment was retained and the effective carriageway of two lanes each side was maintained. The space left between the carriageway and footpath was further analyzed based on the adjoining land-use to determine the quality of pedestrian environment on the road. For example, at the stretch where schools are, it became waiting/pick-and-drop zone while at the mall, it became parking zone and at smaller retail shops it became two-wheeler parking zone.\nTowards the street frontage, a clear walking zone was identified to increase walkability and eliminate pedestrian-vehicular conflicts on the street. To facilitate this, shifting of certain above ground utility boxes, bus stops, etc., was recommended.\nThese proposed changes to the Bellasis Road design are based on three chief design elements: curb extensions, refuge islands and multi-utility zones. Modules for these elements were developed keeping the adjoining land-uses, pedestrian walkability and safety as a priority. How are these elements expected to rejuvenate the street?\n1. Curb Extensions (Bulb-Outs)\nA curb extension, known as a bulb-out, is a footpath extension at strategic locations to reduce pedestrian crossing distance and control their exposure to the moving vehicles. Curb extensions also increase the pedestrian and driver’s visibility towards each other and act as a traffic calming measure.\nOn Bellasis Road, at locations where street side parking obstructed the pedestrian crossings, curb extensions have been suggested. The space required for one such extension would be no more than the space taken by one parked car. At corners, curb expansions would allow extra space to introduce good quality place making and creating identity of the street.\nEvidence from Latin American cities show that the chance of vehicle collisions and pedestrian crashes decrease by 6% for every one-meter decrease of pedestrian crossing distance. Curb extensions also help to tighten intersection radii and encourage slower turning speeds. Introducing curb extensions result in 2-8 kmph speed reduction for residential collector roads, which are moderate-capacity roads that move traffic from local streets to arterial roads.\n2. Pedestrian Refuge Islands\nA pedestrian refuge island is a paved portion in the median of the street, where pedestrians can stop and wait while crossing. While refuge islands may be used on both wide and narrow streets, they are generally applied at loca¬tions where speeds and volumes make crossings prohibitive. In the case of Bellasis Road, median refuge islands will be used where pedestrians need safe harbor after crossing one direction of traffic and before starting the other.\nGlobally, providing raised medians or pedestrian refuge areas have reduced pedestrian crashes by 46% and 39% on marked and unmarked crosswalk locations, respectively.\n3. Multi-Utility Zones\nMulti-utility zone (MUZ) is the footpath area allocated for various utilities necessary for the functioning of a street, such as vendors, public seating, landscaping, bus bays, parking, utility boxes, streetlights, signage etc. Provision of MUZs ensure that adequate clear walking space is available while accommodating ancillary activities within the footpath area. MUZs are best suited for neighborhood streets with mixed land-use, where multiple activities are present.\nBellasis Road is known for its street side everyday informal activities. An organized space along the street for these activities will help to maintain the character of the street as well as smoothen the pedestrian and vehicular movement.\nStrategic design for safer roads\nSafe walkable streets are fundamental components for any city. Today, the issues faced by pedestrians in Bellasis Road are common to people in almost every urban street in India. Bellasis Road is an embodiment of a typical Indian street, with its varied land-uses generating a plethora of street activities, jostling shoulder-to shoulder with traffic. When designed strategically, these multiple elements can contribute to a safer environment for vulnerable road users, and help establish vibrant, mixed-use public spaces while ensuring smooth vehicular flow.\nThe Sustainable Cities program at WRI India’s Ross Center, was the knowledge and implementation partner of MCGM for the intersection redesign project, under Bloomberg Philanthropy’s Initiative for Global Road Safety.","The politics of space, culture, and placemaking for post-COVID place branding\nThe coronavirus lockdown has been full of revelations that may shape a new future for both rural places and the world’s cities. People are re-evaluating the importance of community - and what made a place attractive before may not be relevant now. Estate agents in the UK are reporting an avalanche of enquiries for more rural locations now that working from home is more acceptable and the need to be “commutable” is less valued, and we’re sure this is not a situation unique to Britain. The difficulties for our cities’ cultural attractions are far from over as many struggle to see a way to re-open in ways that fit with social distancing regulations. Easy access to public space has been the saving grace for many during this challenging time; and we are not really all in this together in the same way.\nThis won’t be the first pandemic to create long lasting changes in the design of public spaces. In 14th century Europe, the Bubonic Plague was the catalyst for a number of urban changes, including opening larger public spaces and redesigning to more away from cramped living quarters. Some five hundred years later, Victorian reformer William Farr noted that “a park in the East End of London would probably diminish the annual deaths by several thousand … and add several years to the lives of the entire population”.\nJust as tourism teams are looking to “reset” with a more sustainable approach, place leaders and economic development teams are reconsidering approaches to placemaking. There is a need to reinvent our public spaces to rebuild the attractiveness of places – and to reconnect communities that have been living in isolation for upwards of three months.\nCulture and community\nRecently, we’ve seen more organisations innovating and creating safe experiences for a post-COVID-19 world. The touring production of ‘The Phantom of the Opera’ in Seoul plays to a nearly full house each night thanks to a comprehensive tracking system and temperature sensors at the doors of the theatre. Other organisations are enforcing timed ticket slots and reduced numbers, such as the Rijksmuseum in Amsterdam.\nWe’re written before about the number of DMOs and EDOs who have pivoted to support their SMEs and cultural institutions and continuing to support these organisations who are innovating and evolving will help to preserve the heart and soul of our communities. But there’s more that needs to be done. According to Julian Tang, a virologist from England’s University of Leicester, “the likelihood of a successful infection occurring from an exposure outside is likely less because the sunlight may damage the virus as it passes through the air between people.” With social distancing easier to manage in an open space, and the risks of catching the virus diminished, public space could hold the key to opening up quickly and safely.\nBut how do we move indoor business outdoors?\nVilnius, capital of Lithuania, will be re-allocating much of their public space to struggling bar and restaurant owners – effectively turning the city into an open-air café. With eighteen of the city’s public spaces given over to the project, businesses will be able to resume business while still observing physical distancing rules. So far, more than 160 businesses have applied to be a part of the initiative.\nWe need to reinvent what our public spaces can do. Outdoor theatre and cinema need to move into the mainstream. Art exhibitions can be bought on to the streets. Disused public space can be reclaimed as pocket parks to help bring green spaces into densely urban areas. These changes will be pivotal to renewing the quality of life proposition that people are seeking.\nSustainable city infrastructure\nThere’s been a global explosion in cycling, with cities around the world transforming empty road lanes into cycle paths. Theoretically, most of these changes are temporary – they’re a short-term fix to the anxieties about public transport during a pandemic. But many councils are keen to make these changes more permanent. Walkability is of growing importance for people looking to relocate, and safer cycling spaces are a green alternative to driving, decreasing the strain on overtaxed public transport.\nThe Mayor of Athens, Kostas Bakoyannis, is one of several city leaders who has ambitious plans for urban rejuvenation – with plans to allocate 50,000 square metre of public space for cyclists and pedestrians: “We have this once-in-a-lifetime opportunity and are fast-forwarding all our public works[.] The goal is to liberate public space from cars and give it to people who want to walk and enjoy the city.” Meanwhile in Berlin, a city which has been known to take 10 years to build a bike lane opened up 14 miles in between three and ten days. These aren’t isolated phenomena either. New York. Bogota. Paris. Sydney. London. Cities around the world are reimagining how public space can be better used for residents.\nWe’re teetering on the edge of a sustainable transport revolution, but we need to be thoughtful in how we proceed. Traditionally, space is a luxury afforded to the wealthy. While improving our public spaces will be key to recovery, we need to ensure that we develop equally and don’t price out the local communities as these innovations make certain areas more desirable.\nPolitics of space\n“The pandemic has exposed the gulf between the space-poor and the space-rich to create a new politics of resentment, a coarsening of discourse,” Roger Boyes, Diplomatic Editor for the Times argues. Space has become a closely guarded commodity. Suddenly a private balcony is a godsend, and a garden? Luxury. The lockdown experience has been vastly different for those who have personal gardens, those who have easy access to public communal space, and those who are left without.\nIt’s also important to note that for all the vilification of urban density as a prime breeding ground for COVID-19, it isn’t the cause of pandemic. Or at least, not alone. In New York for example, the population dense (but economically wealthy) Manhattan is one of the least affected boroughs. Contrastingly, Queens & the Bronx (moderately dense but less well-off) have been some of the hardest hit. Phil Myrick at the Project for Public Spaces refers to the pattern as a “‘doughnut,’ with the downtown and gentrified inner neighbourhoods largely spared, and a crisis unfolding in distressed outer neighbourhoods.”\nIt would be reductive to say that this is purely due to access to public space. Lower income groups are more likely to be key workers out on the front line, with less ability to work remotely. But less access to public space also means that more people are being crowded into smaller locations. And those who have the additional struggle of living in food deserts need to travel further for their basic necessities – opening them up to greater risk.\nPlacemaker and author Jay Pitter has coined two terms. First, the dominant density – “designed by and for predominately white, middle-class urban dwellers living in high-priced condominiums within or adjacent to the city’s downtown core.” And then the forgotten densities: mobile park homes; favelas; tent cities; public housing; shanty towns.\nThere is a risk that by building better public spaces, by changing policy to support outdoor business ventures, you can price out the very people that you’re trying to support. We need to ensure equitable development to enable us to protect our forgotten densities from the next crisis. Moral and ethical reasons for helping people achieve their basic needs aside, it’s a weak spot in our pandemic defence.\nThe Superillas of Barcelona\nSo how do you add public space to an area without gentrifying it? Some of the answer is policy and zoning to ensure people can continue to live in your city affordably. Perhaps though, Barcelona’s superblocks could be a model for the future.\nTake the Poblenou superblock. By restricting traffic to a set of one-way systems and a 10km/hr speed limit, the superblock encourages through-traffic to move elsewhere. And by decreasing the cars on the roads to residential traffic, they were able to convert the unused lanes into community spaces – outside seating, play parks, and cafes. Cars have become an afterthought to community life.\nCrucially though, the Poblenou superblock is surrounded by a social housing project – protecting residents from rising house costs.\nWith fifteen more superblocks in progress and plans to build over 500 in the coming decades, the model has evolved to put the residents at the heart of the planning process. “There won’t be any surprises,” says Janet Sanz, Barcelona’s deputy mayor for ecology, urbanism, and mobility. “[The residents] will feel like it’s theirs from the first moment, because they co-designed it alongside City Hall.”\nPreparing for the new normal 2.0\nWe weren’t prepared for any of this - but we can prepare for what comes next. In the face of a recession, it’s easy to ask if time and money should be spent on reinventing our public spaces, but we could, we should and frankly, we have to. The pandemic has highlighted how inadequate access to open space is for many city dwellers, and how much this is disadvantaging already underprivileged groups. Being more creative in our reimaging of public space will be key to short-term recovery and will also drive longer-term place attractiveness by ensuring the health and resilience of our communities and by improving the quality of life for all residents.\nThis is too important an opportunity to be left solely with urban planning experts. City marketing boards, DMOs, EDOs, BID teams, and chambers of commerce who understand the circular connection between places that are great to live in, to work in, to visit, and to invest in can all play a pivotal role. There’s never been a better time to work together on a place brand and development strategy that engages the community, re-imagines a better future, and advocates for equitable development – to ensure that no one is left behind again."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:72a363d5-2a47-46ae-8942-c2e895874315>","<urn:uuid:7c551376-480a-48c2-ae17-015497b46728>"],"error":null}
{"question":"How do the modernization approaches differ between the M110A1 and M2010 rifle programs?","answer":"The M110A1 is part of a broader modernization effort that includes multiple small arms upgrades across the National Guard to match active-duty capabilities, including new weapons like the M3E1, M17 pistol, and M320A1 grenade launcher. The M2010 represents a different modernization approach, where the existing M24 rifle action was retained and upgraded with a new barrel, caliber (.300 Winchester Magnum), and chassis system, maintaining core components while significantly enhancing capabilities like extended range to 1200 meters and adding features such as night-vision compatibility and suppressor integration.","context":["FORT PICKETT, Va. – Virginia and Kentucky Army National Guard Soldiers assigned to the 116th Infantry Brigade Combat Team trained on and fielded new M110A1 squad designated marksman rifles July 12-15 at Fort Pickett.\nThe new weapon is a 7.62 mm rifle, which gives Soldiers greater range and accuracy than the standard M4 rifle. It makes use of an advanced targeting system and sound suppression and fills a distance gap between the M4 and larger-caliber sniper rifles.\nThose advantages make infantry Soldiers more lethal and allow them to engage the enemy at a greater distance, according to 1st. Lt. Matthew Arnold, the Virginia Army National Guard’s New Equipment Training / New Equipment Fielding state coordinator.\n“The M110A1 SDMR allows our Soldiers to be able to reach targets out to 800 meters and beyond, depending on the Soldier’s skill level,” said Arnold. “This basically allows the standard infantry squad to have more standoff between them and their target as well as increased time and space for maneuver as a result.”\nThirty Soldiers assigned to the Lynchburg-based 1st Battalion, 116th Infantry Regiment, the Winchester-based 3rd Battalion, 116th Infantry Regiment, the Portsmouth-based 2nd Battalion, 183rd Cavalry Regiment, the Fredericksburg-based 229th Brigade Engineer Battalion and the Kentucky National Guard’s 1st Battalion, 149th Infantry Regiment, participated in the training and fielding.\nThe event started with time in the classroom with civilian instructors from the U.S. Army Tank-automotive and Armaments Command, Materiel Fielding and Training Directorate, Soldier Lethality Team, learning the nuances and mechanics of the new weapon. From there, Soldiers took to the range to zero the rifles before firing at targets up to 800 meters away, taking turns behind the trigger and shot-spotting. Command Sgt. Maj. Ronald Smith, the VNG command sergeant major, and Sgt. Maj. Latane I. Gilliam, the 116th IBCT operations sergeant major, observed the live-fire range and took turns behind the new rifle.\nArnold said in the past, the higher-caliber and increased-range rifles were primarily used as weapons for snipers.\n“The main point is that this weapon, which was previously used as a sniper rifle, is now going to be in widespread use across our formation and the ways we use it will change depending on the situation,” said Arnold.\nThe M110A1 fielding was the most recent in a series of small arms fieldings across Virginia Army National Guard’s formations the past several months to bring its Soldiers up to speed with their active-duty Army counterparts. Other fieldings included the M3E1 Multi-purpose Anti-armor Anti-personnel Weapon System, a recoilless rifle that is replacing the AT-4, as well as the M17 pistol, replacing the M9, and the M320A1 grenade launcher, which replaces the M203.\nThose small arms fielding highlights the important role the NET/NEF facility plays in improving the Virginia Army National Guard’s warfighting capabilities.\n“It’s important to keep our Soldiers up to date with the latest weaponry because it can and will change how they fight,” said Arnold. “NET/NEF is the process that keeps us on par with the active component.”\nBy Mike Vrabel, Virginia National Guard","Warrior Maven Video Above: The Army is massively revving up its fleet of Bradley Fighting Vehicles\nWarrior Maven thought: What is the technology behind this weapon?\nBy Kyle Mizokami,The National Interest\nThe U.S. Army’s long-serving sniper rifle has its roots in one of the most iconic American hunting rifles ever produced. The M-24 Sniper Weapon System, standard issue among the Army’s sniper teams, is based on the fifty-six-year-old Remington 700 hunting rifle. When the U.S. Army decided to field a new, heavier-caliber sniper weapon, it again turned to the Remington 700 to produce the advanced M2010 rifle.\nThe Remington 700  is one of the most popular American firearms of the twentieth century. Introduced in 1956 as a affordable, relatively lightweight bolt action hunting rifle, the 700 line of rifles sold more than four million copies. The rifle is available in a more than two dozen calibers, from .17 Remington to .458 Winchester Magnum, and can bag game from squirrels to moose.\nIn the mid-1980s the U.S. Army decided it needed to replace the service’s existing sniper rifle, the M21 sniping rifle, with a new weapon. The M21, based on the M14 battle rifle, dated to the Vietnam War. Although semiautomatic, the M21 was less accurate and required more maintenance than most sniper rifles. The Army opened up a competition for a new rifle in November 1986 and picked a winner, Remington Arms, in July 1987.\nRecommended:Stealth vs. North Korea’s Air Defenses: Who Wins?\nRemington’s contestant, known as the M24 Sniper Weapon System , was a bolt-action rifle based on the Remington 700 hunting rifle. It was similar to the Marine Corps’ M40 sniper weapon, also based on the Remington hunting rifle. The rifle barrel was free-floated, only touching the rest of the rifle where it attaches to the Remington action, in order to prevent pressure from changing the point of impact. Remington also provided the weapon with a Harris bipod to support shooting from the prone position and cover.\nThe M24 was chambered in 7.62×51-millimeter (.308 Winchester), meant to use M118 7.62×51 Special Ball ammo and the M118’s 173-grain bullet. The rifle used a heavy, twenty-four-inch-long barrel that increased velocity (and thus distance), and could be fired repeatedly without heat affecting accuracy. Barrel twist, which imparts spin on the bullet and increases accuracy, was one complete turn for every eleven inches.\nThe optic is one of the most important features of a sniper rifle. The M24 was originally equipped with a Leupold M3A Ultra fixed power scope with ten-power magnification, and featured Redfield (and later OK Weber) fixed iron sights for use in emergencies. A fixed power scope, the M3A had fewer moving parts to break under field conditions. This was later replaced with a ten-power Leupold Mk. IV LR/T M1 scope.\nUnlike the M21, which was a semiautomatic rifle fed by a large twenty-round box magazine, the M24 was a bolt-action rifle fed by an internal five-round magazine. The shift away from semiautomatic to bolt action may have been controversial at the time, but it was the right decision. Bolt-action weapons, manually cycled by the user, are more reliable and far less prone to jamming. They were also more accurate than semiautomatic rifles at the time, although this is no longer necessarily true.\nThe M24/M118 Special Ball combination was a reasonable accurate combination, capable of shooting a minute of angle (one inch of deviation at one hundred yards). As one retired U.S. Army sniper put it , the M118 Special Ball round was capable of accuracy from .2 (1.5 inch groups at five hundred yards) to one minute of angle. At worst the M118 Special Ball round was capable of a ten-inch spread at a thousand yards—acceptable considering a human target’s chest is generally assumed to be twenty-three inches across.\nThe M24 performed well in Afghanistan and Iraq, with sniper teams capable of long-distance, long-term observation of target areas. On September 27, 2005, U.S. Army sniper team leader SSgt. Jim Gilliland shot an insurgent  with his M24 Sniper Weapon System at a range of 1,367 yards, or seven-tenths of a mile. At that distance, gravity caused the 7.62-millimeter M118 round to drop an amazing ninety-one feet vertically, a distance that Staff Sergeant Gilliland had to compensate for in order to make his shot.\nIn 2010, the U.S. Army selected a new sniper rifle, the M2010 Enhanced Sniper Rifle , to replace the M24. The M2010 is actually the same M24 Remington action, which in a move of great foresight the Army had demanded be technically a long-action, capable of someday moving to the heavier .300 Winchester Magnum caliber. The M24 action was rebarreled with a twenty-four-inch .300 WM barrel and bedded into a futuristic-looking aluminium chassis that provided a rock-steady firing platform. Although the M2010 has a detachable box magazine, it retains the same number of rounds (five) as the M24.\nThe selection of .300 Winchester Magnum extended the U.S. Army sniper’s range to 1200 meters—four hundred beyond that of the M24. As a result the M2010 ESR received an improved, longer-range optic, the Leupold Mark 4 M5A2 with variable 6.5- to twenty-power magnification. The Leupold scope can be fitted with the Knight’s Armament AN/PVS-29 or AN/PVS-30  night-vision scopes for night firing. Finally, a suppressor designed by Advanced Armament Company eliminates flash and significantly reduces the noise signature of the rifle, very useful features when a sniper is trying to conceal his or her position.\nThe Remington Model 700 is completely unrecognizable as the M2010 Enhanced Sniper Rifle, but the sleek, skeletal sniper weapon owes its existence to the classic hunting rifle found in homes across America. The ability to draw on a hunting weapon and turn it into not one but two sniper rifles is a testament to the Model 700’s excellent design.\nKyle Mizokami is a defense and national security writer based in San Francisco who has appeared in the Diplomat, Foreign Policy, War is Boring and the Daily Beast. In 2009 he cofounded the defense and security blog Japan Security Watch. You can follow him on Twitter:@KyleMizokami .\nMore Weapons and Technology - WARRIOR MAVEN (CLICK HERE)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:018306d9-0995-4662-94ab-1546f4195eed>","<urn:uuid:0521e8b9-18b2-4d56-97aa-65a360a53738>"],"error":null}
{"question":"How does scenario planning combine rational decision-making with strategic prioritization in modern business contexts?","answer":"Scenario planning combines rational decision-making through the rational loop, which carefully analyzes implications of decisions, with strategic prioritization as demonstrated in mature sectors. Companies use it both for creative thinking and as a means of strategic alignment. For example, a $10 billion packaged foods producer used scenario planning to adapt to changes in consumer tastes, retail relationships, and market dynamics, while ensuring decisions were based on rational analysis. The process involves constructing complex scenarios beyond simple two-by-two matrices to capture nuanced business environments, followed by workshops to identify and evaluate strategic needs that are both innovative and pragmatic.","context":["The implications for strategic management of the four loops\nThe implications for strategic management of the four loops\nThere are four loops that is part of integrated model of decision-making and control. The four loops include the rational; the overt politics; the covert politics; and the culture and cognition. The rational loop helps in making sure that strategic management would be based on rational thoughts and ideas. The rational loop focuses on the use of rationality in making decisions and controlling the use of strategies. The rational loop makes sure that all implications of any decision or strategies would be carefully analyzed before anything would be done. The overt politics loop creates a more open or divulge influence in strategic management. Overt politics are used by some to add their ideas to strategic management without being discreet about it. Overt politics focuses on the use of influence and connections to assert a strategic idea that one believe is the best thing for the company. Covert politics loop create a more secretive means to influence strategic management. This involves a member of an organization secretly influencing strategic management decisions in his/her favor. Covert politics loop is a more secretive way to influence decisions and strategies and this usually creates decisions that may be beneficial to one but not necessarily good for the company.\nThe culture and cognition loop makes use of culture and tradition to influence strategic management decision and to make strategies based on the culture of a place or a country. The culture and cognition loop makes use of ideas to change strategic management decision so that the firm will act according to the needs and demands of the environment. In culture and cognition loop, culture and cognition would serve as an important part in making decisions that will affect the firm and it would serve as a basis for the strategies the firm would use. The processual approach helps the firm maintain its goal of a simple strategy based on the four loops. The loops can affect but should not totally change the strategic management process of the firm. Through the processual approach companies can gradually use each of the four loops to make proper decisions for the firm. The first loop that should be used under the processual approach would be the culture and cognition loop. This loop would serve as an ethical foundation for the decision and strategy. The second loop would be the overt politics loop this would give some initial feedbacks and opinions about a decision or strategy; the covert politics would then come next to provide more ideas and opinions. The last loop would be the rational loop wherein everything would be combined to form the best strategy and decision. Strategic management is a process geared at detecting environmental threats and turning them into opportunities. Scenario-driven planning has evolved from humble beginnings and specific disciplines, such as strategic management and operational research, into a bandwagon that is attracting numerous researchers and practitioners (Kay 1995).\nThe demand for scenario-driven planning in business has been increasing for two reasons: first, there is abundant evidence that the strength of an economy can be declining or at least stagnating. Second, using scenarios as a strategic tool provides a handsome return on the investment it requires. The demand for strategic planning with scenarios is beginning to outstrip the supply of scenario analysts. To continue the introduction of scenario-driven strategies into the increasingly turbulent business environment, firms need more people capable of generating strategic change scenarios. Scenario-driven planning is a systematic approach to an increasingly important responsibility of general management: positioning today's business firm in a rapidly changing and complex global environment (Acar & Georgantzas 1995).Predicting a world in which technology and competitive patterns change at an unprecedented rate is hard enough. Moving ahead of it is simply larger than the extended talents and resources that are now available in any of the world's leading firms. The technology of scenario-driven planning constitutes a superior approach designing corporate and business strategies (Donaldson & O'Toole 2002).Firms can use it in formulating and analyzing strategic situations productively. Scenario-driven planning is not a panacea but can be very successful. Because of its multidisciplinary nature, it can help professionals and managers in a variety of applications, namely career planning, competitive analysis, crisis management, decision support systems (DSS), macroeconomic analysis, marketing, portfolio management, and product development. Although scenarios have been used mostly to forecast future corporate environments, scenario-driven planning is increasingly of interest to functional managers in diverse business areas. Managers should incorporate both in their mental models and in their plans, the same way that their competitors might move or respond to tactical moves. Their analysts should sit with them, encouraging them to articulate and to analyze the relationships among these important issues. In sessions with managers, analysts need to become good sparring partners. Attaining greater specificity will empower the managers for better insights. Then, the meaning of combined changes in the environment and in strategy will be understood. At implementation time, this will result in better communication and in a more analytically bent leadership style (Drejer 2002).\nAcar, W & Georgantzas, NC 1995, Scenario-driven planning:\nLearning to manage strategic uncertainty, Quorum Books,\nDonaldson, B & O'Toole, T 2002, Strategic market relationships:\nFrom strategy to implementation, John Wiley & Sons, New York\nDrejer, A 2002, Strategic management and core competencies:\nTheory and application, Quorum Books, Westport, CT.\nKay, JA 1995, Foundations of corporate success: How business\nstrategies add value. Oxford: Oxford University Press\ncomments powered by Disqus","Scenario Planning for Consumer Products Enterprise Strategy\nAcross diverse business sectors, many of FSG’s clients have effectively adopted scenario-based strategic planning as a tool to spur creative and innovative thinking. But equally impressive are the firms, often in mature sectors, which have embraced scenario planning primarily as a means of strategic prioritization and alignment, with break-out thinking a secondary priority. This was the case of a $10 billion packaged foods producer, which was struggling to adapt to changes in consumer tastes, retail relationships, government regulations, supply chain dynamics, media consumption, and weakening brand hegemony.\nThe senior leadership of the company saw the inherent value in a scenario-based approach for setting strategic direction. The firm, for decades an unchallenged industry leader, had been experiencing significant challenges to its market position. Emerging consumer preference for “natural” (and local, responsibly-sourced, etc.) products was one reason. But there were other impactful discontinuities as well, including the shift away from traditional media and advertising and the coincident rise of social media. Gradually, the firm was adapting to this new operating environment. But what were the next big waves or shifts that the firm could not afford to miss – and in fact wanted to be out in front of?\nFirst Order of Business: Forming a Core Team\nWith FSG’s guidance, the project sponsor assembled a core project team comprised of 12 individuals from across the organization. Among the departments represented were brands, marketing, sales, manufacturing, R&D, human capital, market research, finance and IT. As in other FSG scenario planning engagements, the core team was a strong and, in fact, critical partner. The core team would go on to contribute significantly to all phases of the project: trend analysis, driver identification, scenario scoping, scenario development, scenario workshop facilitation and synthesis of workshop results. Less tangibly but no less importantly, the core team helped build enthusiasm and credibility for the project around the organization. Their dedication and the long hours they committed sent a clear signal that the project was important and that the results would have enduring value.\nConstructing Complex and Challenging Scenarios\nConsiderable work goes into scoping what’s called “the scenario space.” It’s critical to get this right, and FSG has strong opinions on the topic. Some scenario consultants default to simple two-by-two scenario matrices, which essentially represent all variations (four) of two variables deemed most important to defining the range of future operating conditions. The problem with 2x2 matrices is that they tend to dummy down the scenario space. Moreover, the 2x2 approach constrains choice – you must do only the four scenarios identified, and there is no option for a fifth future operating-environment, even if the four seem confining. The resulting scenario worlds are easy to characterize—in fact, too easy—and seldom capture more complex and nuanced aspect of future business settings. In contrast, FSG uses three or four major drivers (what we call “dimensions”). This opens up the field of scenario vision, and helps the core team imagine more nuanced and counter-intuitive business environments. They also tend to be messier, a bit more challenging to understand, with contradictory elements, just like real life.\nAfter considerable analysis and discussion, FSG and the client core team identified a set of four unique scenario themes. As a set, the scenarios captured the range of market, competitive and operational challenges (as well as opportunities) that had surfaced in preliminary research and interviews with senior management and leadership. Leadership accepted the core team’s recommended set of four scenario worlds, but requested that select characteristics of other potential scenario worlds not chosen be integrated into the final set of four worlds. This is a useful and common option in scenario-selection tasks.\nWith leadership’s nod, the project team developed full 18-20 page scenario documents to render the worlds as vividly as possible, each one containing a fictional “day-in-the-life” narrative and more detailed descriptions of future operating conditions, everything from lifestyle to politics to business conditions to retail structure to the role of food in society.\nScenario Workshop to Identify Strategic Needs\nSome 60 senior-level personnel participated in the two-and-a-quarter day scenario-planning workshop facilitated by FSG and the client core team. Critically, all department heads plus all but the top two officers of the company participated in the workshop. Participants were divided into four groups. Each participant was assigned one of the four scenario worlds to explore. A workbook containing a common set of questions bounded the discussions and set up the key, final question for each group: Given the characteristics of your scenario world, what strategic needs must the company address to be prepared for the future?\nIn decades of evolving scenario-planning best practices, FSG has found that a scenario workshop endgame of identifying “strategic needs” is often preferable to trying to craft actual strategies. There are a number of reasons for this; a couple of main ones are the shortage of time in a typical workshop to forge meaningful strategy statements and the fact that the right subject matter experts are rarely present to shape credible strategy statements. Far better to identify the broad strategic needs in the workshop setting and leave the actual formation of corresponding strategies and actions to post-workshop implementation teams (made up of both core team members and subject matter experts).\nBy the close of the scenario workshop, each team had refined a list of strategic needs. Representatives of each scenario team presented the strategic needs to each of the other groups for consideration and evaluation (in the context of the other groups’ assigned scenarios). Facilitators collected evaluation “scores” and recorded detailed notes for offline synthesis.\nZeroing in on “Robust” Strategic Needs\nFSG and the Core Team worked intensely in the weeks following the workshop. The chief tasks were to review (and normalize) evaluation scores, score any strategic needs raised in the workshop but not as yet, evaluated; create a master spreadsheet containing the full list, sorted by score; and set a cut-off point to qualify high (and therefore potentially “robust”) strategic needs. Approximately two-thirds of the strategic needs made the cut.\nThe next step was to cluster the strategic needs by theme and affinity. This is an iterative process requiring intense and focused core team participation and insights. The project team wrestled with the wording of the strategic needs to ensure the descriptions and intent of each were clear.\nFinally, the project team bulked up the strategic needs statements with context descriptions (i.e., the future world characteristics giving rise to this need) and preliminary implementation considerations, some of which came directly out of the workshop and others from post-workshop core team deliberations. When this task was completed, the 22 strategic needs packages were ready to be briefed to senior management and firm leadership.\nFinal Evaluations and Action Steps\nFinal evaluation took pace in two phases. In the first, client-side project team representatives presented the 22 robust strategic needs to management, nearly all of whom had participated in the workshop and therefore had a context for understanding where the needs came from. Following questions and review, the group voted on which ones were most critical for the firm to initiate action on. Other project team members took detailed notes on the discussions. Ultimately, a group of eight strategic needs received significantly higher votes than the rest and were deemed high priority.\nIn the second and final round of evaluation, the strategic needs were presented to senior leadership. All 22 were discussed in at least cursory fashion, but the greatest amount of time was spent on the eight deemed high priority by the management group in the previous evaluation. With the backdrop of current company challenges and initiatives already underway, the senior leadership team debated the merits of the top vote getters. Ultimately, leadership decided on a subset of six strategic needs and formed teams to commence work on implementation.\nThe six action areas were:\n- Brand portfolio\n- Retail relationships\n- Ingredient sourcing\n- External affairs\n- Knowledge management\n- Consumer messaging and influence networks\nThe strategic needs that did not make the final high-priority cut fell into a few different categories. Importantly, a subset were considered, in effect, very important for the company to address, but in process already. For example, an entire set of high-scoring strategic needs in the human capital domain were handed over to human resources. The scenario-planning process helped validate HR’s strategic direction, while concretizing specific actions for consideration. Other strategic needs in the list of 22 were deemed relevant, but not urgent. Some were more tactical (e.g., cyber-security risk-management). The scenario process initiated discussions about emerging risks, challenges and issues that some scenario-planning participants believed the company was not addressing sufficiently.\nAs of this writing, teams are developing implementation plans for the high-priority strategic needs approved by leadership. There is energy and, in the client’s words, “excitement” driving progress.\nThe strategic needs that ended up being robust but not urgent do not disappear. They become part of the scenario-planning record and a source of ongoing consideration as internal priorities change and external market conditions evolve.\nScenario consultants always look for innovative solutions to arise from scenario-planning engagements, and this was the case with this packaged foods producer. A number of innovative ideas and approaches rose to the top and are being acted upon. But as valuable as the innovation outcomes were in this case, the company benefited as much from simply getting management and executives aligned and energized around core priorities and actions that were not particularly novel, some of which were already under consideration, but needed a spark to effectively ignite them. The scenario process was, in effect, a rallying point – a way to get executives on board and energized around a manageable set of strategic actions that promise to serve the company well, no matter how the future turns out."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9d173390-2bb1-45b9-aabe-8515e07c74fb>","<urn:uuid:6957f653-1180-4147-ad25-d13f106b3e21>"],"error":null}
{"question":"Do American and German coin grading systems use the same terminology?","answer":"Yes, German coins can be graded using the same terminology as American coins, from Poor to Mint State, despite differences in collecting culture. Both systems use similar descriptions for uncirculated coins, breaking them down into categories based on characteristics like luster, bagmarks, and surface quality. The main difference lies not in the terminology itself, but in how these grades are accepted and used in their respective markets, with Americans putting more emphasis on numerical grades and certification while Germans have traditionally focused less on third-party grading.","context":["why the AU designation if it isn't almost uncirculated\nDo you want the short answer, or the long answer?\nHere's the long answer. Scroll down to the bottom of the post for the short answer.\nOnce upon a time, back in the late Middle Ages, coins were graded much the same everywhere, and on a very simple scale: Poor, Fair and Good. Since the only coins being collected back then were ancients, this scale was sufficient, and the words meant pretty much what they mean in everyday conversation: a coin in Good condition was actually pretty good, given that it had been dug up after being buried for a thousand years.\nAs coin collecting became more popular and modern coins began to be collected, additional grades beyond \"Good\" had to be invented, especially with the introduction of modern machinery capable of creating coins of very high quality (just like new words to describe the purity of olive oil had to be invented as oil refining and filtering technologies advanced). This we acquired Fine, Very Fine and, early in the 20th century, Extremely Fine. Then there was Uncirculated, to describe a coin in mint-fresh condition. \"AU\" or \"aUnc\" is a relatively modern invention.\nUp until at least the mid-20th century, condition was not a particularly significant factor in assigning a value to a coin; rarity was much more important. The Sheldon scale was invented in 1949 as a simplified way to convert condition-to-value for a given coin: a VF-20 coin was worth one-third the price of an MS-60 coin, no matter what the coin actually was. This kept the Sheldon catalogue relatively uncluttered, since only one price was required to be listed for each type and variety: the theoretical \"basal-state\" (BS-1) price. Prices for higher grades could be calculated just by multiplying the BS price by the Sheldon number for the grade.\nTime marched on, and the demand for coins in the \"best possible grade\" grew. Prices for Unc coins escalated far beyond the usefulness of the Sheldon scale to calculate value. Thus, we have the complicated system of grading, with multiple levels, adjectives, and words that seem to mean the opposite of their normal everyday sense - compared to a coin in aUnc condition, a coin in Good condition is actually rather awful.\nFrom now we can more clearly see a curious effect: \"grade creep\", also known as \"gradeflation\" - which is the main reason why the Americans have a much slacker grading standard than ours. The definitions of what qualifies a coin as \"Good\" or \"Very Fine\" has changed over time, and have gotten worse. As I said, originally a coin in Good was in pretty good nick.\nGradeflation happens at different rates in different places; it is faster where there has a high market demand for high-grade coins for a longer period. If the supply isn't meeting the demand, the grading standards are slackened slightly, to allow for increasing numbers of coins to reach the threshold for each grade. It's an evolutionary (or, more precisely, a devolutionary) process; tiny changes made over a long time period. Gradeflation has been more rampant in America than it has here in Australia, because of the higher collector demand generally which has meant that high-grade coins have been in demand there for longer.\nDon't think the Australian standard of grading has been immune to gradeflation, either. Today, the British grading standard is even tougher than ours; an American AU-55 ought to make an Australian EF, but would only rate a British gVF. But back when we were part of the British Empire, our grading standards were the same. And if you want to see how far our grading standards have slipped since the \"good old days\", grab a copy of an old grading guide from the 1960s. The book \"Collecting Australian Coins\" by Tom Hanley and Bill James came out in 1966, just after decimalization. Listen to these lines from the grading guide for the reverse of George V silver coinage, contained therein (emphasis mine):\nUncirculated: This would be a proof coin if the field showed more finish. The surface retains a mint-fresh lustre, detail of the heads of the kangaroo and emu are sharp, and the feathers on the emu's back show no trace of wear under the glass. Magnification discovers no blemishes such as scratches or nicks. Only the cream of coins obtained from the banks in the original mint rolls are in this condition.\nExtremely Fine: This is the condition in which all but a few of the roll coins are found. These coins show the marks, usually superficial scratches of the chutes and conveyors along which they travelled during minting processes. A good deal of their mint lustre remains and the glass should show no more than the slightest trace of wear on the uppermost feathers on the emu's back.\nOnly a tiny fraction of coins from mint rolls\nqualify as Uncirculated? Mint bag marks only visible under magnification\ncount as \"wear\"? Now that's a harsh standard, harsher even than the current British standard. Yet it's the standard many of our old-time collectors (and dealers) grew up with, and it's the standard the price guides from the 1960s would have used.\nSo, the short answer: America has had more gradeflation than us.\nDon't say \"infinitely\" when you mean \"very\"; otherwise, you'll have no word left when you want to talk about something really infinite. - C. S. Lewis","- German coin grading is no simple matter. There\nare literally thousands of different coin types\nspanning hundreds of years and a variety of\ncoining techniques. To complicate matters, there\nis the myth that European grading standards are\nstricter than U.S. standards. Then there is the\ncomplete lack of grading guides for German coins.\nWhat about descriptive grades versus numerical\ngrades? What do the Germans think of commercial,\nthird-party grading (certified, or\n- Actually, with a little experience and\nassistance, German coin grading is no more\ndifficult than grading American coins. Let's\nstart with the basics.\n- German coins may be graded using the same\nterminology as American coins, as follows:\n||Very well worn, with\nonly outlines of the major design elements\nvisible, but it should be completely identifiable\nas to date. Most or all of the legends should be\n||Well worn, but with all lettering and\ndevices outlined and with some detail beginning\n||Approximately 50% of the detail will\n||Approximately 75% of the detail will\nremain, but the coin will have no luster.\n||Approximately 80-90% of the\ndetail will remain, and the coin must have some\n||Light friction on the high\npoints, must have luster and smooth, original\n||Must have no wear, but may have\n- Uncirculated coins may be broken down further, as\n||Unworn, but lots of bagmarks,\nhairlines, and the luster or toning may be dull.\nThis is a low-end Uncirculated coin.\n||Nice luster, some bagmarks,\ngood eye appeal, may have attractive toning. This\nis a typical Uncirculated coin.\n||Excellent luster and eye\nappeal, very clean surfaces. This is a high-end\n||A gorgeous coin with great eye\nappeal and virtually perfect surfaces. These are\nthe best collectible coins.\n||The ultimate coin,\nessentially perfect in all respects. Extremely\nrare and usually worth a large premium.\n- Proof coins may be graded using the same\nstandards, but substitute \"hairlines\"\nfor \"bagmarks\", as Proof coins were\nnever meant to come into contact with each other,\nbut their surfaces are very fragile and are\neasily damaged. American coins may also be graded\non a numerical scale from 1 to 70, with 70 being\nHere's a useful conversion chart showing the\nAmerican adjectival grade followed by its\nnumerical American counterpart, then it's\nschon or \"ss\"\nstempelglanz or \"vz-st\"\nMINT STATE (MS)\nDEEP MIRROR PROOFLIKE\n(literally, \"first strike\")\n- Commercial, third-party grading has not been\naccepted by the Germans to any great degree.\nHowever, American collectors place a high degree\nof emphasis on quality, thus they actively seek\nthe highest possible grade for a given coin.\nThus, a certified coin is easier to sell to an\nAmerican collector than a \"raw\" coin;\nthe opposite is true when dealing with a German\nThe European coin grading myth: You may have\nalready heard that European grading standards are\nstricter than American grading standards. Most\nlikely, you heard it from an American coin\ndealer, and most likely, you heard it while you\nwere trying to sell your coins. This is a myth\nused by some dealers of questionable ethics to\nbuy your coins as cheaply as possible. Naturally,\nif they can knock the grade, it is to their\nadvantage. But, I promise you, some German coin\ndealers know how to overgrade their coins just as\nwell as some American coin dealers. I remember\none experience where I visited a German dealer in\nhis shop and asked to see his best quality coins.\nHe pulled out a bag of coins, all jumbled\ntogether, and proceeded to deal them out onto a\ntray like playing cards. All of the coins were\nExtremely Fine or better, but clearly were not\nUncirculated. Thinking there was a\nmisunderstanding, I repeated my request to see\nhis best coins. He responded by stating that the\ncoins on the tray were indeed\n\"stempelglanz\". Naturally, I didn't buy\nanything and my advice to you is that the next\ntime someone tells you that European grading\nstandards are stricter than American, do what I\ndid -- walk away quickly.\nThere are two areas where German coin collectors\nare picky about condition. Their Proof coins must\nbe perfect and if the coins were issued in some\nsort of packaging, the packaging had better be\nthere in perfect, original condition (this refers\nprimarily to the modern commemorative coins\nissued by East and West Germany).\nRecently, quality has become an important factor\nin the German market. Erstabschlag coins (the\nSuperb Uncirculated pieces) have begun to realize\nfantastic prices at auction, often many multiples\nof regular Uncirculated pieces. Thus, it can be\nvery advantageous for the collector to learn the\nnuances of grading Uncirculated coins.\nAdjectival grading versus net grading: I prefer\nto grade a coin based on the actual detail, then\nlist any faults. The alternative is to combine\nall the faults and derive a \"net\ngrade\". Here's an example: A coin that has\nExtremely Fine detail, but rim dings could be\ngraded either adjectivally as \"EF, rim\ndings\" or with a net grade of\n\"VF\". Obviously, the adjectival grade\ngives more information and the collector receives\nno \"surprises\" when the coin arrives in"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7b703948-6968-4668-84be-df297eb8f677>","<urn:uuid:885397be-d60e-4cd2-a35b-b7214ca9996f>"],"error":null}
{"question":"As a chess strategy enthusiast, I'm curious about the value comparison between bishops and knights - what are their relative strengths according to traditional piece valuations, and how do their movement patterns differ?","answer":"Both bishops and knights are traditionally valued at 3 points each in chess. However, they have very different movement patterns: The bishop can move any number of squares diagonally, but is restricted by other pieces in its path. In contrast, the knight has a unique 'L-shaped' movement pattern, moving two squares horizontally and one square vertically, or one square horizontally and two squares vertically. Unlike other pieces, the knight can jump over other pieces while moving to its destination square, making it the only piece with this special ability.","context":["Evaluation Functions for Cutoff Search - artificial intelligent\nEvaluation functions guess the score that may be guaranteed if a specific world state is reached. In chess, such evaluation functions have been known long before computers came along. Simply, one such function counts the number of pieces on the board for a specific player. A more complicated function scores more for the more influential pieces as queens and rooks: each pawn is worth 1, knights and bishops score 3, queen's score 9 and rooks score 5. These scores are utilized in a weighted linear function, where the number of pieces of a particular type is multiplied by a weight, and all the products are added up. For instance, if in a specific board state, player one has 1 bishop, 6 pawns ,2 rooks ,1 knight and 1 queen, then the evaluation function, for that board state f, B, would be calculated as follows:\nf(B) = 1*6 + 3*1 + 3*1 + 5*2 + 9*1 = 31\nIn bold , the numbers are the weights in this evaluation function (for example , the scores assigned to the pieces).\nPreferably, evaluation functions should be fast to calculate. If they take very much time to calculate, then less of the space will be searched in a given time restriction. Evaluation functions should, ideally also match the real score in goal states. This is, Of course not true for our weighted linear function n in chess, because goal states only score 1 for a win and 0 for a loss. Actually we do not need the match to be exact - we may use any values for an evaluation function, as long it scores more for better board states.\nA bad evaluation function may be disastrous for a game playing agent. There are 2 major problems with evaluation functions. Initially, certain evaluation functions just make sense for game states which are quiescent. A board state is quiescent for an evaluation function, f, if f's value is unlikely to exhibit wild swings in the near future. For an example, in chess, board states such as one where a queen is threatened by a pawn, where1 piece may take another without a similar valued piece being taken back are not quiescent in the next move for evaluation functions such as the weighted linear evaluation function mentioned above. To get around this problem, we might make an agent's search more sophisticated by implementing a quiescence search where given a non-quiescent state we want to evaluate the function for, we expand that game state until a quiescent state is reached, and we take the value of the function for that state. If quiescent positions are much more likely to arise than non-quiescent positions in a search, then such type of extension to the search will not slow things down very much. A search strategy may choose in chess, to delve further into the space whenever a queen is threatened to try to avoid the quiescent problem.\nIt is also bearing in mind the horizon problem, where a game-playing agent can't see far sufficient into the search space. An example of the horizon problem in Norvig and Russell is the case of promoting a pawn to a queen in chess. In the board state they present, this may be forestalled for a particular number of moves, but it is inevitable. However, with a cut off search at a sure depth, this inevitability can't be noticed until too late. It is likely that the agent trying to forestall the move would have been better to do something else with the moves it had available.\nIn the card game example above, game began are collections of cards, and a possible evaluation function would be to add up the card values and take that if it was an even number, but score 0 ,if the sum is an odd number. This evaluation function matches perfectly with the real scores in goal states, but perhaps it is not good idea. Suppose the cards dealt were: 10, 3, 7 and 9. If player one was forced to cut off the search after only the first card choice, then the cards would score: 10, 0, 0 and 0 respectively. So player 1 would select card 10, which would be terrible, as this will inevitably lead to player one losing that game by at least 12 points. If we scale the game to choosing cards from 40 rather than 4, we can see that a more sophisticated heuristic involving the cards left un selected may be a better idea.","How to Play Chess\nThe Ultimate Guide to Playing Chess for Beginners!\nChess is played on a square board that is divided into 64 squares. Regardless of the color of the squares, the lighter shade or colors are referred to as ‘white’ while the darker color or shade is referred to as ‘black’.\nThe game has evolved much through the ages, and has come a long way from what it was in the 6th century. The modern rules of the game first took form during the 13th century, which gave more freedom to the pieces in terms of mobility, such as the queen and bishop. Chess is not only a complex game that requires intelligence, cunning and wit, but it is also a highly enjoyable pastime. No wonder, it is the most famous board game in the world, with more books written and research done on it than any other board game. This is going to be a basic guide to playing chess.\nAt the start of the game the pieces are arranged in the following manner:\n- Rooks are placed at the corners of each side.\n- Knights are placed beside the rooks.\n- Bishops are placed beside the knights.\n- The queen is placed on the central square of the same colour of the player. For example, white queen will go on the white square, while the black queen will go on the black square.\n- The king takes the spot beside the queen.\n- The pawns are placed in front of the pieces.\nThere are 64 squares on a chess board that’s divided equally between black and white squares. Each square can only occupy a single piece at a time.\nChess is played between two players who move the pieces taking alternate turns. Traditionally, the player with the white pieces starts off the game. The objective of the game is to trap the opponent’s king. The attempts of either player to trap the opponent’s king is called a check, while a checkmate is when the player who has their king checked is unable to save their king. A checkmate happens when the king being checked is unable to make a move without being captured by the opponent. Both players attempt to put their opponent’s king under attack during the whole duration of the game until they have a checkmate. The player who reaches a checkmate is the winner of the game.\nNaming the Squares\nKeeping a record of the various moves is helpful in improving your standard of chess. Each square on the chessboard can be identified with a unique name that comprises of a letter and a number. The vertical files are labelled in small letters and goes from white’s left to white’s right, while the horizontal ranks are simply numbered from 1 to 8. In this way, each square on the chessboard is identified by a letter and number. The vertical rows are known as files while the horizontal rows are called ranks.\nThe Chess Pieces and their Value\nEvery piece in chess has its own unique movement. This means each piece has to follow certain rules that are specific to its type. This is the reason why it is important for players to know the value of each piece in their arsenal. While the points do not give the player an absolute evaluation, it does help when it comes to choosing when a trade is worth it and what trades to avoid. Let us take a look at the points of different pieces in chess.\n- The Pawn is worth 1 point\n- The Queen is worth 9 points\n- The Knight is worth 3 points\n- The Bishop is worth 3 points\n- The Rook is worth 5 points\nThe King has not been included in the above points chart mainly because it is of infinite value, considering whoever manages to checkmate the opponent’s king wins the game.\nMovements of Various Pieces in Chess\n- The King – The king is able to move only one square either forward, backward or diagonally.\n- The Rook – The rook moves forwards or backwards through any of the squares that are unoccupied.\n- The Bishop – A bishop can move any number of squares in a diagonal direction.\n- The Queen – The queen can move in any direction.\n- The Knight – The knight is able to move two squares horizontally and one square vertically, or it can move one square horizontally and two squares vertically in an “L” pattern. The knight is not blocked by other pieces while jumping to a new position.\n- The Pawn – A pawn can move forwards a single square if the square is vacant. If the pawn has not already made its first move, it can move two squares forward, that is, both the squares are vacant. A pawn is not allowed to move backwards or diagonally.\nA piece cannot move over any other piece in their way to get to a vacant square, unless it is a knight.\nCheck and Types of Checks\nThe word ‘check’ basically means that you’re king is in danger and must be saved.\nCheck is a condition in the game of chess where a player’s king is under threat of being captured by the opponent. In this situation, the player whose king is in check has to remove and protect their king from that square immediately.\nTypes of Checks\nThere are also various types of checks. The following are some of the ways in which players can check their opponent.\nA Discovered Check\nWhen a piece is moved out of a line of attack exposing the opponent’s king to another piece such as a rook, bishop or queen that checks the king it is called a discovered check. The piece that was moved initially can be any type of piece that belongs to the same player as the checking piece. A discovered piece is also used as a tactic to divert the attention of the player whose king is being checked, and can be moved to attack another piece of player after they have moved their king to safety. This is an effective move since the person whose king is being checked is unable to thwart the attempt of the opponent as they have no choice but to move the king to safety, thereby sacrificing another piece.\nThis move is a check from not one but two pieces of the opponent in a single move. This occurs whenever a moved piece attacks a king, resulting in a second piece checking with the discovered check. A double check is also possible when an en passant capture opens up two lines of attack at the same time. The reason why a double check is so effective is because it cannot be blocked, nor can the player capture two pieces of the opponent in one single move.\nWhenever a check has been answered by a check and especially if the second check is carried out with a piece that’s blocking the first the move is known as a cross check. While a cross check is very rare, they do happen, and are a common occurrence that’s considered as a chess problem.\nProtecting the King\nOnce a player’s king is in check they need to protect the king from that square. We shall explore the ways in which a player is able to protect their king in this position.\nThere are basically three ways in which a player can do this;\nCapture the piece that is involved in giving check\nThe first move of the player whose king has been checked by the opponent is to find a way to capture that piece. The move of capturing the piece that is involved in the check is legal if it is not protected by another piece of the opponent.\nMoving the king away from that square\nThe easiest thing to do is to simply move the king away from the square it is currently placed in, that is, if it keeps the king from getting checked. One thing to remember is that certain pieces like the bishop or knight can move multiple squares. So, it is important for the player whose king is in check to make sure that their move does not place the king in any of the squares that come in the path of the piece involved in the check. There will be times when moving the king from the square that has been checked places the king in another square which is also in check by another piece of the opponent. In this case, the player cannot move their king on that square, or any of the squares that can get the already checked king captured.\nBlocking the piece that is giving check\nAnother move is to attempt to block the piece that is giving check without having to move the king. This can be done by placing any other piece in the path of the opponent’s piece that is giving check. However, the player whose king is being checked should make sure that by moving another piece they are not exposing the king to another of the opponents pieces that could checkmate the king again.\nWhat happens when a player isn’t able to save their king from a check position?\nWell, if you do not find any legal move that can protect your king from the check position, it is considered as a ‘checkmate’ and the player whose king has been checkmated loses the game. A checkmate usually happens in the situation of a ‘double check’ where the king is being checked no matter which square it is moved to. Also, a checkmate can be delivered by any piece of the opponent except for their king. Checkmates are rare among advanced players because many players resign respectfully before forcing their opponent to a checkmate. But, with beginner and intermediate level players, a checkmate is the only way a game ends. Part of that is due to the many mistakes that are made by beginners which results in a checkmate. However, a checkmate can also take place with all the pieces still on the board, in what is called a ‘Fool’s Mate’ where the player makes a mistake of exposing their king early on during the game.\nWhile the game of chess always ends with a winner and a loser, sometimes it can also end in a draw. Also known as a ‘stalemate’ a draw can happen if;\n- Both players do not have the players to checkmate their opponent.\n- The players can declare a draw if they reach the same conclusion at least three times during a single game.\n- It is a player’s turn to move, but he does not have any legal moves left, although their king is not in check.\n- The player can also declare a draw if a total of 50 chess moves have been played by both players and not a piece has been captured.\n- Both players agree on a draw.\nPromoting a Pawn\nIt is common knowledge that a chess pawn can move only forwards and not backwards. They can also capture other pawns sideways. But, what happens when the pawn of either player reaches the last square on the opposite side of the board?\nWhenever a pawn is able to reach the last square of the opponent’s side it can be promoted to a queen, bishop, knight or rook. The promotion of the pawn is usually possible in the latter part of the game when the board is fairly empty, and can have a significant impact on the outcome.\nPlaying the game of chess also includes special moves that can either help defend the king or plan an attack on the opponent. There are two special moves that are widely used in chess.\nCastling is a special move where the player moves their king and a rook simultaneously. The move is executed by first moving the king two squares towards the rook, then moving the rook to the square on the other side of the king.\nHow many squares does the king and rook move while castling?\nThere are basically two types of castling; the short way, which is on the king’s side, and the long way which is on the queen’s side. In both these cases the king has to move two squares while the rook has to move either two or three squares. For the move to be legal, both the king and rook should not have been moved previously.\nEn Passant Capture\nAn en passant capture is the capture of the pawn which occurs immediately after the pawn moves two ranks forward from its starting position and the pawn could have been captured by the enemy if it had moved a single square forward.\nThe en passant capture move was first introduced back in 1490 when pawns were granted the ability to move two squares forward on the first move. The move was introduced to limit the freedom and power of a pawn.\nThe all important opening is where you will either make or break your game play for the entirety of the game, that is, unless your opponent does not make a stupid mistake.\n- In the Opening the player should – First things first. You have to develop your pieces.\n- Protect the King – The whole point of playing chess is to protect the king. The opening is where you will need to use a tactic known as “castling”.\n- Never focus on a single piece – Remember that this is the time where you need to develop ALL of your pieces, so don’t focus too much on a single piece.\nWhen one starts a game of chess there is no way of exactly knowing which way the game will go. One wrong move could end up in you losing a valuable piece. This is why during the middle of the game your priorities will change. This is the time to have a plan for moving forward.\n- Look for the optimal square for each piece – Are the squares your pieces are on strong or would you like to exchange them. This is the first question to ask yourself.\n- Know what you want – During this time, it is extremely important for the player to know what they want to do with their pieces.\n- Keep your King protected – No matter what point you are in the game, the player’s ultimate goal will be to protect the king, and keep it from a checkmate.\n- Try to figure out what your opponent wants – Remember that your opponent has a plan too, so try to figure out what that plan is so there are no surprises.\nSince it isn’t over yet, during the endgame game the player should:\n- Activate their king – This may seem counter intuitive, but the king is a very powerful piece in the endgame. The king is usually the last piece to be involved in a defend or attack strategy.\n- Protect and promote the pawns – In the endgame, it is easier to promote pawns because both players have less pieces. That being said, pawns are the most valuable and vulnerable during the endgame. But, if played well, the pawn can easily be converted into a queen or some other important piece that can help you turn the tables on your opponent and win the game."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1d1e0a52-00dc-4667-a157-cc07ac55f159>","<urn:uuid:de80c5ac-f577-4f05-9386-f1745d2e4b7a>"],"error":null}
{"question":"¿Cuál es la relación entre el cerebro y el sistema digestivo en el contexto del consumo de alcohol y la regulación de la glucosa?","answer":"The brain and digestive system have interconnected relationships in both alcohol consumption and glucose regulation. For alcohol, it disrupts brain communication pathways affecting mood and behavior, while simultaneously altering gut microbiota and causing intestinal permeability, which can lead to bacterial translocation and affect multiple organs. In glucose regulation, there is a specific gut-brain communication pathway where nutrients in the gut trigger signals through the vagus nerve to the brain, which then regulates glucose levels in the body. This gut-brain axis can become impaired in pathological states, leading to dysregulation of blood glucose levels. Both scenarios demonstrate the crucial bidirectional communication between the brain and digestive system in maintaining metabolic health.","context":["The National Institute on Alcohol Abuse and Alcoholism (NIAA) reports that alcohol consumption is a normal part of life for many people.Although it’s difficult to consider alcohol consumption “healthy,” it does appear that drinking responsibly and in moderation is the situation for many who indulge. “At-risk” drinking, however, can cause problems. The NIAA considers at-risk drinking to be more than four drinks per day or fourteen per week for men, and more than three drinks per day or seven per week for women.\nHow prevalent is this type of problem drinking? In 2013, 24.6% of adults reported that they engaged in binge drinking in the past month. Roughly 25% of people who exceed the NIAA limits have an Alcohol Use Disorder. In 2014, this amounted to 16.3 million adults in the U.S.\nHeavy drinking can lead to an array of serious health issues. In this article, we will focus on one of the lesser known—dysbiosis—which is an imbalance in the gut microbiota.\nThe gastrointestinal tract is home to bacteria and microbes that form an ecosystem known as the gut microbiota. This collection of bacteria is essential for digestion, the immune system, mental health, and more. The microbes contain genes, this is known as the gut microbiome.\nThe growth and maintenance of these bacteria can be influenced, positively or negatively, by several factors. A healthy diet of raw, organic, prebiotic-rich food encourages the gut microbiota to flourish. Conversely, an unhealthy diet, one that’s high in refined sugar and artificial ingredients, for example, has a detrimental effect and can cause an imbalance between good and bad bacteria. An unhealthy diet is just one possible cause of a gut imbalance. Other factors that may lead to an imbalance include:\nLet’s look at how alcohol affects the human microbiome.\nAlcohol-Induced Changes on Gut Microbiota\nSome of the bacteria that make up the gut microbiota are “good” and support good health; others are “bad.” We all have both good and potentially bad bacteria living inside of us. In fact, the digestive tract may contain up to 1000 different species of bacteria at any given time.\nMost of the time, for most people, the good bacteria keep the harmful bacteria in check.However, if that harmony and equilibrium are disrupted, bad bacteria can take hold and become a “problem.” Sometimes the problem might be inefficient digestion, sometimes it might be a more specific sickness.\nRecent research suggests alcohol can influence the gut microbiota. Alcohol abuse, especially, can alter the delicate equilibrium and disrupt the intestinal environment.\nNot only does alcohol abuse alter the gut microbiota, it can cause gut permeability, also known as leaky gut syndrome. Some researchers have hypothesized leaky gut syndrome can lead to bacterial translocation.\n“Bacterial translocation is the invasion of indigenous intestinal bacteria through the gut mucosa to normally sterile tissues and the internal organs.”  In other words, it’s when the bacteria in your gut escape to other parts of your body. This prompts a domino effect where one bad event is the catalyst for another. Alcohol consumption alters gut flora, leading to leaky gut syndrome, leading to bacterial translocation, leading to inflammatory changes in the liver and elsewhere.\nTranslocation also allows for toxins from the gut to enter the bloodstream. Among the many problems this causes, it exposes the liver to toxins and increases the chance of liver injury.\nThe Relationship Between the Liver and Intestinal Microbiota\nGut health is closely related to liver health as bacteria in the gut exert a level of control over various metabolic functions. There appears to be a symbiotic relationship between gut microbes, how they affect the microbiota, and the impact that has on overall health.\nA study published in May 2015 suggests that alcohol abuse changes the composition of gut microbiota. Several studies consider the modulation of gut microbiota as a promising strategy to reduce alcohol-induced liver injury.\nBefore looking at the best way to maintain a healthy gut microbiome, let’s briefly take a look at the other effects of alcohol on the human body.\nOther Effects of Alcohol in the Human Body\nAlcohol interferes with the brain’s communication pathways, affecting mood, behavior, and coordination, and can even disrupt the growth of brain cells. Alcohol overuse is associated with Alzheimer’s disease and other memory-related disorders.\nAlcohol takes a toll on the pancreas and the immune system and actually increases the risk of developing some types of cancer. It alters hormone balance, which causes reproductive deficits in both sexes, has a negative impact on immune system function, and affects bone health.\nThese are just some of the negative health effects associated with alcohol, but let’s get back to the main topic… how to protect the gut microbiome from damage.\nHow to Protect and Maintain the Gut Microbiome\nThe easiest way to protect the gut microbiome from alcohol is to simply abstain from consuming alcohol. Not everyone is in favor of being a teetotaller but there are more reasons to avoid alcohol than not. Not to mention that, for some, drinking can be a slippery slope that quickly becomes a bad habit and eventual addiction, complete with the accompanying health problems. Who needs it?\nIn the interest of fair reporting, there are some opinions that, because it contains polyphenols that support probiotic activity, red wine “promotes” gut health. In one study, red wine was found to increase Bifidobacterium and Prevotella. This may have beneficial effects on the gut microbiome by lowering plasma lipopolysaccharides, a type of endotoxin that triggers an immune response.\nDo follow a healthy lifestyle. Don’t consume alcohol in excess. Do eat plenty of green, leafy vegetables and whole, raw foods. If you don’t eat fermented foods, consider taking a good quality probiotic supplement to encourage a positive, balanced gut environment. The best probiotic supplements contain a blend of probiotic strains. Last year, we released Floratrex™, a premium probiotic supplement that contains 23 of the best probiotic strains available. The feedback we’ve received from customers is incredible.\nIs alcohol part of your lifestyle or do you abstain? What’s your reasons for partaking, or not? Leave a comment below and share your thoughts with us.\nby Dr. Edward Group DC, NP, DACBN, DCBCN, DABFM\n- National Institute on Alcohol Abuse and Alcoholism. Alcohol Facts and Statistics.\n- Mutlu EA1, Gillevet PM, Rangwala H, Sikaroodi M, Naqvi A, Engen PA, Kwasny M, Lau CK, Keshavarzian A. Colonic microbiome is altered in alcoholism. Am J Physiol Gastrointest Liver Physiol. 2012 May 1;302(9):G966-78. doi: 10.1152/ajpgi.00380.2011. Epub 2012 Jan 12.\n- Phillip A. Engen; Stefan J. Green, Ph.D.; Robin M. Voigt, Ph.D.; Christopher B. Forsyth, Ph.D.; and Ali Keshavarzian, M.D. The Gastrointestinal Microbiome: Alcohol Effects on the Composition of Intestinal Microbiota.\n- University of Utah Health Sciences. The Human Microbiome. Genetic Science Learning Center. 2016.\n- Hawrelak JA, Myers SP. The causes of intestinal dysbiosis: a review. Altern Med Rev. 2004 Jun;9(2):180-97.\n- Mutlu EA, Gillevet PM, Rangwala H, Sikaroodi M, Naqvi A, Engen PA, Kwasny M, Lau CK, Keshavarzian A. Colonic microbiome is altered in alcoholism. Am J Physiol Gastrointest Liver Physiol. 2012 May 1;302(9):G966-78. doi: 10.1152/ajpgi.00380.2011. Epub 2012 Jan 12.\n- National Institutes of Health. NIH Human Microbiome Project defines normal bacterial makeup of the body. NIH. June 13, 2012.\n- Medscape. Alcohol and Gut Microbiota.\n- Bode C, Bode JC. Effect of alcohol consumption on the gut. Best Pract Res Clin Gastroenterol. 2003 Aug;17(4):575-92.\n- Vaishnavi C. Translocation of gut flora and its role in sepsis. Indian J Med Microbiol. 2013 Oct-Dec;31(4):334-42. doi: 10.4103/0255-0857.118870.\n- Vishnudutt Purohit,a,* J. Christian Bode,b Christiane Bode,b David A. Brenner,c Mashkoor A. Choudhry,d Frank Hamilton,e Y. James Kang,f Ali Keshavarzian,g Radhakrishna Rao,h R. Balfour Sartor,i Christine Swanson,j and Jerrold R. Turnerk. Alcohol, Intestinal Bacterial Growth, Intestinal Permeability to Endotoxin, and Medical Consequences. Alcohol. 2008 Aug; 42(5): 349–361.\n- American Society for Microbiology. “Gut bacteria can control organ functions.” ScienceDaily. ScienceDaily, 21 March 2011.\n- Jasmohan S Bajaj MD, MS, Phillip B Hylemon PhD and Zobair Younossi MD, MPH. The Intestinal Microbiota and Liver Disease. Am J Gastroenterol Suppl (2012) 1:9–14; doi:10.1038/ajgsup.2012.3.\n- Vassallo G, Mirijello A, Ferrulli A, Antonelli M, Landolfi R, Gasbarrini A, Addolorato G. Review article: Alcohol and gut microbiota – the possible role of gut microbiota modulation in the treatment of alcoholic liver disease. Aliment Pharmacol Ther. 2015 May;41(10):917-27. doi: 10.1111/apt.13164. Epub 2015 Mar 23.\n- NIAAA. ALCOHOL’S DAMAGING EFFECTS ON THE BRAIN. Alcohol Alert. Number 63 October 2004.\n- Alzheimer’s Association. Korsakoff Syndrome. Alzheimer’s Association. 2016.\n- NIAAA. Beyond Hangovers: Understanding alcohol’s impact on your health. NIH Publication No. 13-7604. Printed September 2010.\n- National Institute on Alcohol Abuse and Alcoholism. Alcohol Alert. No. 19 PH 329 January 1993.\n- National Institute on Alcohol Abuse and Alcoholism. Alcohol’s Effects on the Body. 2016.\n- NICHOLAS EMANUELE, M.D., AND MARY ANN EMANUELE, M.D. Alcohol Alters Critical Hormonal Balance. The Endocrine System. VOL. 21, NO. 1, 1997.\n- Phillip A. Engen, Stefan J. Green, Ph.D., Robin M. Voigt, Ph.D., Christopher B. Forsyth, Ph.D., and Ali Keshavarzian, M.D. The Gastrointestinal Microbiome: Alcohol Effects on the Composition of Intestinal Microbiota. Alcohol Res. 2015; 37(2): 223–236.\n- Clemente-Postigo M1, Queipo-Ortuño MI, Boto-Ordoñez M, Coin-Aragüez L, Roca-Rodriguez MM, Delgado-Lista J, Cardona F, Andres-Lacueva C, Tinahones FJ. Effect of acute and chronic red wine consumption on lipopolysaccharide concentrations. Am J Clin Nutr. 2013 May;97(5):1053-61. doi: 10.3945/ajcn.112.051128. Epub 2013 Apr 10.","The Connection between the Gut, the Brain, and Diabetes\nBy: Krystal Jacques\nThe worldwide prevalence of diabetes has almost doubled, from 4.7% of the global adult population in 1980 to 8.5% in 2014 (1). This is thought to reflect increases in obesity, an important risk factor associated with type 2 diabetes. Now, diabetes researchers are taking a closer look at the connection between our gastrointestinal tracts and our brains to understand how gut-brain communication may impact body weight and glucose levels.\nDr. Tony Lam and his team at the Toronto General Hospital Research Institute study how the unique relationship between the gut and the brain regulates blood glucose and body weight homeostasis. After majoring in biochemistry at McMaster University, Dr. Lam completed his PhD in the Department of Physiology at University of Toronto studying liver glucose metabolism with Institute of Medical Science (IMS) faculty, Dr. Adria Giacca, whose lab studies energy excess and diabetes. “It was a challenging time-my first paper only came out towards the end of my PhD in my 5th year,” recalls Dr. Lam. However, the fulfillment of small gains each day, “performing experiments and discovering new things”, inspired him to persevere and go on to complete a postdoc (2003-2006) with Dr. Luciano Rossetti, at the Albert Einstein College of Medicine in New York.\n“Dr. Rossetti was the first to posit that the brain could sense nutrients (glucose and lipids) and hormones to subsequently regulate glucose homeostasis,” Dr. Lam explains. During his postdoc, Dr. Lam expanded on his previous PhD work by determining whether the brain plays a role in regulating changes in liver glucose metabolism in rodents with diabetes and obesity. To answer this, he performed the same experiments used during his PhD, but with the addition of blocking molecules and pathways in the brain. His work in Dr. Rossetti’s lab in a relatively new field investigating the brain’s role in diabetes and obesity caught the attention of other scientists. Dr. Lam accredits his research progress to being at the right place at the right time. “I just happened to be fortunate enough to be there to learn from him [Dr. Rossetti], which resulted in my ability to publish a few first-author papers in respectable journals.”\nDr. Lam’s early and exemplary scientific contributions on the neurophysiological profile of diabetes during his postdoc work showed that the brain-liver axis can control blood-glucose levels. In parallel, other researchers in the field had shown that nutrients in the gut activate a negative feedback pathway, via the gut-brain axis, to lower appetite. These two phenomena led Dr. Lam and his own lab in Toronto (started in year 2006) on a mission to discover whether ingested nutrients in the gut-brain communication pathway can also lower glucose levels. Specifically, the Lam lab wanted to know whether nutrients ingested after a meal could activate a negative feedback pathway that stems from the gut to the brain (via the vagus nerve afferent), and whether the brain then relays a signal to the organs of the body (e.g. liver) to regulate glucose levels.\nTo answer this question, mice and rats are implanted with a catheter tube that stretches from their upper small intestine to an opening in their upper back, via a subcutaneous route. Nutrients, such as glucose and lipids, can then be infused directly into different parts of the upper small intestine, in an attempt to mimic the natural gastric emptying flux of nutrients into the gut that is experienced after a meal. This activates the nutrient sensing pathways in this particular part of the gut.\nAfter 12 years of rigorous research, the Lam lab has shown that nutrients in the gut induce a neuronal signal to the brain, which in turn, can regulate glucose levels in healthy rodents. However, during pathological states, such as when rodents are fed a high fat diet, “a defect in these nutrient sensing mechanisms occurs, leading to a dysregulation of blood glucose levels,” explains Dr. Lam. “Interestingly, our lab has found that metformin and bariatric surgery—the two most common treatments for type 2 diabetes—can partly rescue this nutrient sensing pathway defect in the gut to restore glucose levels.” One of the Lam lab’s most recent findings was published in Cell Metabolism in January 2018 by one of his many exceptional students, Paige Bauer, who is currently pursuing a PhD (2). She performed genetic sequencing of an extract of the microbiome, the community of microbes living in the lumen of the small intestine. This revealed a correlation between the abundance of the Lactobacillaceae family of bacteria and metformin’s antidiabetic effect on the gut nutrient sensing pathway. Subsequent cause-and-effect studies were conducted and the findings indicate that metformin increases Lactobacillus and restores glucose sensing mechanisms in the upper small intestine to lower glucose levels in diabetic/obese rodents.\nThe basic scientific findings produced by the Lam lab warrants possible future clinical investigation. However, as with all meaningful research, Dr. Lam believes that “It takes time to find out whether your results in the lab can stand the test of time, and whether it’s clinically relevant. Nonetheless, we do have an idea of some molecules in this gut pathway that could work to be as effective as the existing therapies because they mimic the effect of metformin and bariatric surgery, which are both clinically effective to lower blood glucose levels.” In addition, the Lam lab’s findings on Lactobacillus are contributing to the currently exploding field of the microbiome, and to our repertoire of information on how gut bacteria has a significant role to play in several diseases.\nReflecting on his career as a researcher, Dr. Lam stresses the importance for students and scientists to be “open minded in both what you want to pursue as a career as well as in research, because nobody really knows what could happen tomorrow.” Dr. Lam continues by hinting at the philosophy he lives by, which has helped him in the rigorous research career—”It [research] is an extremely rewarding career, because it takes a lot, a lot of effort and time to validate that what you are showing is right to other people. And in exchange, to admit that you are wrong sometimes… it takes a lot of learning. And I love that about academics. It’s an art form—there’s a part of being a researcher that may not only have to do with discovering new things, but how you communicate, how you treat the field, and how you respect others.”\n- World Health Organization. Global report on diabetes. [document on the Internet]. 2016 [cited 2018 Jan 10]; Available from: http://apps.who.int/iris/bitstream/10665/204871/1/9789241565257_eng.pdf.\n- Bauer P, Duca F, Waise T, et al. Metformin alters upper small intestinal microbiota that impact a glucose-SGLT1-sensing glucoregulatory pathway. Cell Metab. 2018;27:101-117."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0a4b1d2a-8b74-407f-9c88-07c8cfde0e51>","<urn:uuid:0ba8f6fd-fc53-48b4-86f2-e56780ef0673>"],"error":null}
{"question":"What are the key principles of cricket bowling technique, and how are they being taught at Thornton Heath Cricket Club?","answer":"Cricket bowling technique is structured around four key 'tent pegs': back-foot impact where the bowler maintains a stable, upright position; front-foot impact creating an X-shape for drive and speed; ball release with the body leaning forward; and follow-through with the body driving toward the target. At Thornton Heath Cricket Club, these principles are being taught to a diverse group of 6-18 year olds under Harshil Trivedi's leadership, where they accommodate skill levels from absolute beginners to borderline county players, with over 50 students attending weekly sessions.","context":["Published: 11th December, 2019\nHarshil Trivedi of Thornton Heath Cricket Club – who won the award for Outstanding Contribution at Surrey’s OSCA’s back in August at the Kia Oval – has continued to be recognised for his efforts for coaching at his club by being shortlisted for the same award nationally at the ECB Coaching Awards at Edgbaston.\nHarshil has contributed over 10 years of experience in coaching at his club, bringing coaches together in the area to make cricket accessible for anyone who wants to take part.\nCoaching has not been the only success in recent years, he petitioned the ICC for a chance to take Afghan Refugees to the Cricket World Cup here in England and secured the tickets for free.\nWe spoke to Harshil about his journey and what the accolade meant to him.\nWhat has been your ‘cricketing journey’ in terms of your introduction to cricket?\nI first got into cricket at the age of 8 as my uncles were very avid cricket fans and with the advent of the home world cup, cricket fever was rife in the family. I haven’t looked back in the 20 years since. I started playing in school and started coaching during college as I realised that county cricket may not be in my grasp.\nWhat was your pathway into coaching and how did you get started?\nAs I was growing up, I quickly realised there was no cricketing support structure in my local area and I personally went to the Oval over winter for coaching sessions. Soon after then, I was asked to help out a holiday club with organising cricket matches at a local park (as I had knowledge and equipment) where I discovered that there was a vested interest among youth in cricket. A year later re-started what is now known as Thornton Heath Cricket Club with a handful of equipment and a bucket load of ambition. It was mentoring and slowly, through networking, I was put onto a level 1 courses and it all went upwards from then on.\nWho are you currently coaching?\nI am currently coaching boys and girls aged between 6 and 18 for a Chance to Shine project in collaboration with the Surrey Cricket Foundation in Thornton Heath. We regularly get over 50 students coming in on a weekly basis which is testament to the hard work of a great team led by Asad who has ample experience coaching at various levels which puts even the youngest kids at ease. The ability level varies between absolute beginner and borderline county player which allows any player to fit right in from day one.\nHow did you feel when you found out you had been recognised with a nomination for outstanding contribution in Surrey and to then win the award?\nI was very excited to find out that I had been nominated. There are hundreds of clubs in Surrey so it is always an honour to make the shortlist. This is the second year that I had been nominated but the amount of effort that had been placed on developing cricket over the last 12 months in the area has been unparalleled and I felt that I had a fighting chance to win.\nWhat is your ‘coaching philosophy’?\nI believe that everyone should have the access to play and learn any skill and for a coach, the facilitation of this is key, be it pro-active or re-active. Adapting to the demands of the ever-changing environment allows coaches to develop dynamically and learn from each other.\nDo you have a plan of what you intend to develop in coaching in the future?\nI would like to work on biomechanics and how that impacts on long-term bowlers careers. I have started doing that with my current group and can see benefits with minute changes to run-ups, head positioning and follow through which is very promising.\nCongratulations to Harshil, and we look forward to seeing where coaching goes in the Thornton Heath area in years to come!","The first tent peg is back-foot impact. This is the moment that a bowler’s back leg impacts in the action, as shown in figure 8.1a. The idea is for the back leg to be stable and support the whole movement, which requires no leaning back but being relaxed and upright.\nOnce the back foot impacts at this point, any other part of the body can move except the back foot, so you’ll need to look at which way this foot points. If the back foot runs parallel with the crease, this is known as having a sideways setup. If the foot points at 45 degrees (toward the square leg umpire), this is known as having a midway or semi-setup. If the back foot lands pointing straight down the pitch, this is known as having a front-on setup. This is all very important to know as a coach and as a player because the rest of the action depends on that back foot’s position. That is, the bowler’s hips and shoulders will be at the same angle and not split apart on tent peg 1 because this would create a mixed action that might lead to injuries and line-up problems.\nWhen bowlers are in the tent peg 1 position, their bowling hand is in line with their bowling shoulder, and their front knee is in line with their elbow, lifting the knee up as if attached by string. This ensures shoulders and hips line up correctly. They should be balanced and relaxed with all body weight supported on the back leg.\nThe second tent peg is front-foot impact. It’s against the front foot and leg that a bowler pulls to create drive and speed. In the tent peg 2 position, the bowler looks like a five-point star or an X shape, with hands as far apart as possible (to create a stretch) and the balance of weight equally between each foot, as shown in figure 8.1b. The feeling for a young player might be that of grabbing the batsman’s collar with the leading hand and grabbing the sightscreen from behind with the other. The position is a power position in a straight line.\nThe third tent peg is release of the cricket ball. The body leans forward, and the bowler feels as though he or she is giving a high-five with the bowling hand lined up with the bowling hip and in front of the front foot, as shown in figure 8.1c. The hips and chest face the batsman, and again the release is balanced, with everything moving toward the target.\nThe fourth and final tent peg is the follow-through. The body drives forward, and the trailing leg drives hard toward the batsman to bring the hips through, as shown in figure 8.1d. The arms rotate fully, and the top half drives forward.\nThe four tent peg approach helps young bowlers understand where they ought to be at any one time. After going over and demonstrating the four pegs, have your bowlers try to put all the positions together in one flowing movement. Once they can do it, they have achieved full bowling action. In practice, have your bowlers walk up to the crease, step into tent peg 1, and flow through their bowling action. Also have them run up to the crease, jump into tent peg 1, and bowl with a flowing motion. By using the four tent peg approach, you reinforce that bowlers are aware of these important positions."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3617554b-5bde-4559-9b22-d55fe3ae6cc1>","<urn:uuid:a76deec8-2d0c-466f-b139-c8fb7b14aebb>"],"error":null}
{"question":"¿Cuáles son las diferencias principales entre cultivar plantas en camas elevadas vs. en el suelo cuando hay riesgo de heladas? 🌱","answer":"There are several key differences in frost protection between raised beds and ground-level planting. In raised beds, the soil warms more quickly and the bed can be easily covered with hoops or tunnels to protect plants when frost threatens in early spring or late fall. However, plants in raised beds may need more protection since they drain more quickly than flat beds and don't retain heat as well. In ground-level gardens, cold air tends to sink and pool at the lowest points, making valley or slope-bottom locations more frost-prone. Ground-level gardens can be protected through methods like mulching with straw or pine needles to preserve heat and moisture, and maintaining proper moisture levels since water holds heat better than soil. Both types can benefit from covering plants with blankets or sheets when frost threatens.","context":["Frost generally is a tough factor for the permaculture backyard. For a lot of vegetation, an surprising frost may cause injury to the vegetation and even trigger a crop to fail. Different vegetation, in distinction, really profit from a frost. The flavour of broccoli, as an illustration, really improves if the plant has skilled a frost, whereas carrots get sweeter because the temperature drops.\nIt may well appear formidable to attempt to plant all of your greens and fruits at simply the correct time to allow them to be harvested earlier than, throughout or after the primary frosts of winter relying on their particular person wants, and lots of gardeners have numerous anxiousness about defending these vegetation that want it from the damaging results of a frost. There are lately, with fixed developments in know-how, far more correct methods of estimating when the primary frosts in a selected space will arrive. This attracts on data we’ve about the way in which international climate techniques transfer and work together. Nevertheless, as with many issues on the permaculture plot, such estimations can by no means be an actual science. Native elements reminiscent of side, topography, temperature and human exercise can influence upon the timing of a frost’s arrival.\nHowever there are particular strategies that the permaculture gardener can use to attempt to predict whether or not a frost is more likely to choose their plot. These are strategies that use one of many main instruments within the permaculturist’s arsenal – commentary.\nIt might sound simplistic, however among the best methods of figuring out of a frost is due in a single day is to gauge the temperature. The temperature data printed by meteorological organizations is often measured greater than a meter from the bottom (to attempt to give an correct common over a locale which has topographical variation). Lots of your vegetation might be decrease to the bottom than that so use a thermometer to examine the temperature round at-risk vegetation. That is one motive why raised backyard beds generally is a good design function if you’re in a location that’s liable to frosts. The opposite issue that may affect the temperatures throughout your website is microclimates. Permaculture gardeners make use of quite a lot of strategies to affect microclimates in several places throughout the plot, and these can affect whether or not a frost kinds or not. As an illustration, areas which are adjoining to a concrete wall that has been uncovered to the solar throughout the day are much less more likely to expertise a frost, because the wall will radiate latent warmth throughout the evening. Likewise, vegetation which are near a physique of water reminiscent of a stream or pond are much less more likely to get frost lined, because the water releases warmth absorbed throughout the day.\nObserve the sky. Whether it is cloudy, the probabilities of a frost settling are decreased. A thick masking of low clouds will cut back the chance of frost because it prevents the radiation of the warmth the Earth has absorbed throughout the day again into the ambiance. It acts quite like a blanket, preserving the warmth nearer to the bottom and so elevating temperatures.\nA windy evening can also be more likely to cut back the chance of a frost. A brisk breeze will trigger completely different elements of the air column – the colder elements decrease down and the hotter areas increased up – to combine. Nevertheless, be suggested robust wind could actually improve the probabilities of a frost forming because it sweeps heat air that’s nearer to the bottom away and replaces it with chilly.\nChilly air is denser than heat air and so it sinks (whereas heat air rises). That signifies that in case your website is on the backside of a slope or in a valley, and there’s no wind, it’s extra liable to a frost, because the chilly air will transfer down the slope and pool on the backside. This impact also can occur on a smaller scale throughout your website when you’ve got quite a lot of elevations.\nSize of Evening\nPut merely, the longer the evening, the extra seemingly a frost. Power losses from the Earth at all times exceed good points throughout the evening, and so when the nights are at their longest throughout fall and winter, there may be extra time for the vitality losses to build up, reducing the temperature and cooling the bottom.\nThe Dew Level refers back to the temperature at which the air is now not in a position to ‘maintain’ all of the moisture inside it. The moisture then condenses and a dew kinds on the backyard. When dew kinds, the warmth inside it’s launched, preserving the air temperature at or barely above the dew level, and thus stopping a frost from forming. So the extra moisture within the air, the much less seemingly a frost. This is the reason a light-weight watering of the backyard a day or two earlier than a frost is predicted may help cease it settling. It’s on this regard that timber can help in stopping frost. Bushes transpire numerous moisture via their leaves, so having numerous timber helps preserve the air temperature across the dew level.\nHaving carried out observations reminiscent of these outlined above, and decided frost is extra seemingly than to not kind in a single day in your website, one of the simplest ways to guard your vegetation is to cowl them. You need to use outdated blankets, sheets or burlap sacks. Select a cloth whose weight is suitable to the fragility of the vegetation being lined, so that you don’t injury them, and drape the fabric loosely over the foliage, securing it to the bottom by weighing it down with bricks or stones. You probably have a day or two earlier than the frost is predicted, watering the soil across the vegetation may help, as water holds warmth higher than soil. Nevertheless, keep away from soaking the bottom as this may result in the water freezing throughout the soil and damaging the roots. One widespread permaculture approach that may be a longer-term technique for stopping frost is to mulch your backyard beds. Mulching with supplies like straw, pine needles and wooden chips helps protect warmth and moisture and so prevents frosts forming.","Would you like to have more garden space with less stooping? How about spending less time doing soil preparation? Or maybe you would just like to have a little hardscaping for visual interest but you don’t want to invest a lot of time and money into construction. Raised bed gardening may be just the thing for you.\nRaised bed gardening involves building frames that can range in height from six inches to 18 inches and filling the frame with good garden soil. Frames can be any length, but most people prefer beds that are four to six feet long, or the length of a standard piece of lumber available at the hardware store.\nBenefits Of Raised Garden Beds\nRaised beds provide a number of benefits. Building a raised bed “pyramid” creates visual interest and increases the square footage available for planting. Second, raised beds can extend the growing season. Soil in raised beds warms quickly, and the bed can be covered with a hoop or tunnel to protect plants when frost threatens in early spring or late fall. Third, raising the level of the soil minimizes the need for bending or kneeling.\nThis makes planting, weeding and harvesting more convenient for individuals with mobility challenges. Fourth, watering and irrigating raised beds is easier and less wasteful than attempting to water the average garden area. Some raised bed kits come with a drip irrigation system or a sprinkler head fitted to the bed.\nFinally, the soil in raised beds can be blended to a gardener’s specifications. One raised bed can be filled with compost-rich humus for squash, melons or salad greens, while another can be filled with sandy humus that is ideal for carrots and other root vegetables. Raised beds increase possibilities for gardeners.\nBuilding Raised Garden Beds\nThe big question, of course, is how difficult is it to build one – or several – raised beds? The answer is that raised beds are very easy to construct, even for someone for whom DIY is a four-letter word.\nDie-hard do it yourselfers can build their raised beds with paver bricks, flat stones or standard 2 x 4s in whatever lengths are desired. The easiest to assemble is a basic “box” made of lumber that would be appropriate for decking; don’t use treated wood, however, because the chemicals used to treat the wood can leach into soil and contaminate edible crops.\nThe depth of the box should be a minimum of 6 inches. Lumber can be stained and sealed or left unfinished. Staining and sealing will extend the life of the fixtures and help keep them weatherproof.\nThese structures will be permanent fixtures in the landscape, so take care that the construction be aesthetically pleasing as well as functional.\nA much easier – and temporary – solution is to purchase a raised bed kit. These kits generally contain artificial lumber boards, corner brackets and anchors for the larger sizes. The brackets are on a stake with a pointed end that slides into the ground and openings that hold the lumber.\nMany of these kits are stackable, creating a significant amount of growing space but using a minimal amount of square footage. These beds assemble and disassemble quickly. Most people break these beds down at the end of every season and store the pieces in a shed or garage.\nHowever the beds are constructed, they need to be filled with soil. This is where the fun begins, because a gardener can create growing conditions in raised beds that would not normally be found in their location. For example, a bed in which herbs are to grow would require a sandy loam that drains easily.\nMidwesterners can grow hardy cacti in a soil mix that is primarily sharp sand. A standard kitchen garden or vegetable bed would need a rich loam; squash, cucumbers and pumpkins need extra manure or mushroom compost added to the soil mix for optimum fertility.\nOnce the soil has been installed, the beds can either be direct seeded or starter plants can be placed into them. Once the plants are placed or the seedlings have emerged, then it’s time to mulch. Raised beds drain more quickly than flat beds, so mulch helps to keep the soil evenly moist during the growing season and it also helps keep weeds to a minimum.\nThe kind of mulch you use depends on the plants in the bed. Tomatoes, for example, benefit from either black or red plastic mulch. Leafy greens or herbs benefit from wood chips or shredded bark, which keep soil cool during hot summers and also keep soil from splashing up onto leaves that will be going into the evening’s salad.\nBed maintenance consists of ensuring the bed is adequately watered, and applying time released fertilizer around plants at the beginning of the growing season, or applying a water-soluble fertilizer once a week. The water-soluble fertilizer can be applied with a watering can, sprayer, with a sprinkler that is focused strictly on the bed or through drip irrigation. \\\nBecause the garden area is contained, less fertilizer can be used throughout the season, as there will be less runoff and waste. Near the end of the season, the beds can be covered with clear plastic tunnels in order to stave off frost damage and extend the growing season. Gardens built in areas of extreme heat (Texas and Arizona come to mind) can be covered with shade cloth during the warmest time of the day to prevent sun scald.\nRaised bed gardening affords greater accessibility for motion-challenged individuals, creates opportunities for more plant diversity, and helps extend the growing season. Raise the standard on your garden this year and try a few raised beds."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:563564cf-2cb6-4e0a-832c-889d9967e168>","<urn:uuid:911c5ac9-d268-4fd3-a607-7edc0f5cf5c1>"],"error":null}
{"question":"How do the career prospects and earning potential compare between Economics PhD holders and Human Resources PhD graduates?","answer":"Economics PhD holders have diverse career options including academic roles, economist positions (like at the Bank of England), financial roles (such as quantitative analysts at banks and hedge funds), and management consulting. Meanwhile, Human Resources PhD holders earn an average salary of $95,000 and typically pursue careers as corporate consultants, business administration professors, or industry researchers at top universities. Both fields value research skills, with Economics PhDs being particularly sought after by management consultancies like McKinsey and Boston Consulting Group, while HR PhDs are prepared for academic positions and consulting roles focused on improving corporate social environments.","context":["Career options for Economics researchers\nFind out about the different career options available if you're undertaking research in Economics\nBroadly speaking, your options are the same as those for researchers in other disciplines; see the career options pages for more details. The aim of this page is to look at some career options that may be particularly suitable for economics researchers. One important question to ask yourself is the extent to which you want to use your subject in any future career moves. It's entirely your choice - you will have acquired and developed many skills during your PhD which are complementary to the research skills you might think define your career choice.\nIf you'd like to remain working in your discipline and would like some ideas as to how, see details of what previous Bath Economics doctoral graduates have gone on to do.\nThis covers the most obvious choices (research, academia and employment as an economist). If you want to work as an economist/economic consultant, you can apply for roles that are open to all graduates and do not specify a higher degree. Even when a higher degree is not specified, employers often value the additional knowledge, and skills gained from undertaking a postgraduate qualification. Some economist vacancies may ask for a Masters or PhD. The Bank of England has a specific entry level for PhD Economists. Work experience is highly valued for economist roles. For more information on working in economic analysis and research, see the 'Information Sources' section below.\nYou could use your economic skills and knowledge in broader contexts; options related to your subject include civil servant (e.g. with the Treasury), financial risk analyst, investment analyst, chartered accountant and statistician. The role of quantitative analyst, which usually requires a PhD in a quantitative subject, involves using research and analytical skills to design financial models used in trading and assessing financial risk. Quantitative analysts are employed in financial institutions including banks, hedge funds and investment banks. For more information on this role, view the Finance Train website and the Banking Blog on The Guardian website.\nYou might want to move out of the Economics sector, but still use your social science background. There are many occupations in which your PhD would be valuable, including management consulting, which involves helping organisations to solve problems and improve business performance. Management consultancies value the high level research and problem-solving skills of PhD students, and some companies, such as Mckinsey and The Boston Consulting group, have specific entry routes for PhD students. Some management consultancies and the larger financial services firms, including Deloitte and PwC, provide specific economic consultancy. Other areas might be human resources, which concentrates on employment policy and regulations, or market research.\nTo find out more about what these jobs involve, see the 'Information sources' section below.\nOptions outside of your subject\nIf, having got to this stage, you're not sure that you want to use your Economics any more, don't despair! You have many skills and experiences that are highly valuable to employers and you can choose from the huge range of graduate jobs on offer, from structured training with large financial services firms to management training with well-known retailers and manufacturers. The key is to think of the skills you have and enjoy using, and explore occupations that use them to suggest possible alternatives. For information on different types of jobs, explore the occupational profiles on the Prospects.ac.uk.\nSome key resources to help you research career options for economists:\n- To find out more about academic career pathways in Economics, see the career options for researchers page and talk to academics in your department.\n- Prospects.ac.uk - see especially the Options with Economics leaflet.\n- Careers for those studying economics (Careers Service helpsheet).\n- Institute of Risk Management.\n- Royal Economics Society.\n- EDIRC- list of economics departments, centres and research institutes worldwide.\n- Market Research Society - has job postings, careers advice and networking events.\n- Civil Service Fast Stream - entry route into the Government Economic Service (GES) is through the Analytical Fast Stream. If you want to work for the Government Statistical Service, entry is via the Analytical Fast Stream or through the main graduate entry programme. There is also a Graduate Scheme for HM Treasury.\n- ESRC/HEFCE guide to careers with quantitative methods.\n- Society of Business Economists.\n- University of Oxford Guide to working for think tanks.\n- University of Oxford Guide to Working in Government and Public Administration.\n- Management Consultancies Association. Organisation representing consultancy industry. Includes careers resources and vacancy listings.\nWhere to look for jobs in Economics and related fields\n- See the vacancies page for academic, research and research-related jobs.\n- For academic jobs worldwide and information about conferences in Economics, it's also worth looking at Inomics. The Royal Economic Society also has information about academic jobs, conferences, and an annual meeting event where doctoral students can meet potential academic employers.\n- Careers Service MyFuture database - look at the vacancies relevant to your subject area and also in the sections 'For PhD Students' and 'For Postdocs'. As Bath is highly targeted by the financial and consultancy sectors, this is likely to be an excellent source of opportunities. Use the 'organisations' function to identify potential employers that you may want to work for or could approach speculatively.\n- Check out the Prospects.ac.uk Employers and vacancies sections for Economists, Statisticians and Financial Risk Analysts.\n- EfinancialCareers is a good starting point for jobs in investment banking, financial risk and quantitative analysis.\n- Jobsgopublic contains a wide selection of jobs throughout the public sector, including education, health, local and central government and charities.\n- Local Government Jobs.\n- Office for National Statistics. Regularly has vacancies for both statisticians and social researchers to design and analyse the many surveys it carries out.\n- The EU Institutions recruit centrally through the European Personnel Selection Office.\n- Social Research Association - has a jobs section.\n- Policy Library - has job postings, information about policy areas and a World Directory of Think Tanks.\n- Third Sector Jobs - jobs in the charity sector.","Understanding the complex nature of business organizations takes dedicated professionals who understand leadership, teamwork, and workplace culture. A doctorate in human resources helps managers and consultants tackle these subjects to improve corporate social environments, teach business administration as professors, or conduct industry research at the nations top universities. According to PayScale, holders of a PhD in Human Resources can earn an average salary of $95,000.\nWhat Are The Admissions Requirements For A PhD In Human Resources Program?\nWhile admissions requirements vary by school, most universities expect students to have a degree in a related field such as a master’s in human resource management or business administration. Additionally, most programs require a minimum average GPA (usually around 3.0 or higher) and meet a minimum score on the Graduate Management Admission Test (GMAT). Some schools expect doctoral candidates to have hands-on work experience.\nHow Long Will It Take To Earn A Human Resources Doctorate?\nIn general, expect the program to take four to five years of full-time study. Some schools measure their degrees in credit hours; usually between 60 to 90 credits total. Most programs are not offered on a part-time basis and there is a maximum amount of time students are allowed to complete their degree.\n|1||Cornell University – PhD Human Resources – 80 Points||Ithaca, New York|\n|2||Purdue University – PhD Management: Organizational Behavior, and HRM – 72.5 Points||West Lafayette, Indiana|\n|3||University of Southern Mississippi – PhD Human Capital Development – 70 Points||Long Beach, MS|\n|4||The University of Tulsa – PhD Industrial-Organizational Psychology – 70 Points||Tulsa, Oklahoma|\n|5||University of Wisconsin-Madison – PhD Management and Human Resources – 58.75 Points||Madison, Wisconsin|\nOur Top 20 Best PhD in Human Resources Programs\nWe’ve compiled a list of 20 schools offering a PhD in human resources and related majors. Our list focuses on affordability and class size to choose the best needs-focused programs in the country. Check out our rankings below!\nCornell University – PhD Human Resources – 80 Points\nCornell’s doctoral program blends the field of human resources with related subjects like public policy, organizational psychology, and labor relations. While the school’s general graduate tuition is not the cheapest, those accepted into the program are usually “guaranteed full funding” while conducting research. Applicants are required to submit two academic letters of recommendation, a statement of purpose, and a writing sample such as a master’s thesis.\nPurdue University – PhD Management: Organizational Behavior, and HRM – 72.5 Points\nThe research-focused doctoral program at Purdue prepares students for careers at leading universities. Students can choose from a variety of research project subjects such as leadership roles, workplace equity, and employee stress factors. Interested applicants must provide a resume, statement of purpose, and at least two academic or professional letters of recommendation.\nUniversity of Southern Mississippi – PhD Human Capital Development – 70 Points\nThe University of Southern Mississippi’s human capital development program teaches professionals to assist organization leaders in developing effective workplace environments. Courses are available in an executive hybrid format that is partially on-campus and partially online. Applicants must have a 3.5 or higher GPA, current resume or CV, three letters of recommendation, and submit a Statement of Qualification.\nThe University of Tulsa – PhD Industrial-Organizational Psychology – 70 Points\nThe I-O Psychology program at the University of Tulsa focuses on the science of pairing the right people with the right jobs. Students are taught the skills needed to improve worker wellness and workplace efficiency. This degree is ideal for those looking for corporate or governmental consultation careers as well as academic positions. Applicants are required to have at least a baccalaureate degree, a background in psychology, a GPA of 3.0 or higher, and letters of recommendation.\nUniversity of Wisconsin-Madison – PhD Management and Human Resources – 58.75 Points\nThe University of Wisconsin prepares doctoral students for academic careers as assistant professors and researchers in areas such as organizational behavior, human resources, and entrepreneurship. Students take two years of coursework before focusing on their own research projects and developing a dissertation. This highly selective program requires applicants to meet the admissions requirements for both the Graduate School and School of Business.\nUniversity of Illinois-Chicago – PhD Business Administration, HRM Emphasis – 56.25 Points\nThis PhD program at the University of Illinois-Chicago teaches students the research fluency needed to become published and highly-respected educators in the field. Students can expect two years of coursework and seminars followed by dissertation work and a full year teaching one course. Applicants must have a baccalaureate with credits in math/statistics, computing/analysis, and business, as well as a 3.0 GPA, three letters of recommendation, a personal statement of interest, and a writing sample.\n*All 23 students admitted to the program over the past 10 years have received tuition waivers, a fellowship, and a graduate assistantship. In other words, if the value of the tuition and fee waiver is included, students are receiving about $50,000 per year to be a student in the program.\nTemple University – PhD Business Administration, HRM/OB – 52.5 Points\nTemple University’s Human Resource Management and Organizational Behavior PhD aims to place graduates in academic research careers. Students will work with journal-published faculty to develop their own research papers and dissertation over approximately five years of study. For this highly competitive program, applicants must have a current resume/CV, two references, and ideally a GPA of 3.7.\nUniversity of Arkansas – EdD Human Resource and Workforce Development Education – 52.5 Points\nThis education doctorate offered by the University of Arkansas is designed for those seeking careers as leaders in education, business and industry. Unlike most other doctorate programs, this degree may be earned in an entirely online setting. Individual research and a doctoral dissertation are still mandatory. Applicants must have at least a post-baccalaureate degree with a 3.0 GPA, a current resume, and submit an autobiographical sketch.\nUniversity of Miami – PhD Business Management/Organizational Behavior – 51.25 Points\nThe University of Miami’s Organizational Behavior PhD intends to place students in academic careers as university professors and researchers. Students can expect to teach an undergraduate course and work on empirical and theoretical research for publication in academic journals after completing two years of core curriculum. Interested students must have at least a baccalaureate degree, provide a current CV, and three recommendation letters.\nUniversity of Minnesota-Minneapolis – PhD Organizational Leadership, Policy, And Development – 48.75 Points\nThis PhD program from the University of Minnesota helps students research human resource development topics to provide insight into how workers interact, learn, and progress through organizations. Graduates often go into careers as executive consultants and academic faculty. Applicants must have a master’s degree or equivalent and a graduate GPA of 3.4 or higher.\nUniversity of Alabama – PhD Management, HR Study Focus – 43.75 Points\nThe PhD in Management program at the University of Alabama prepares students for career advancement in management and potential academic positions. In addition to studying human resources, students can choose to focus on other related subjects such as organizational psychology, entrepreneurship, and strategic human capital. Interested applicants must have at least an undergraduate degree with a 3.0 GPA.\nMichigan State University – PhD Human Resources and Labor Relations – 41.25 Points\nFocused on developing high-quality researchers in the field, Michigan State University’s PhD program is for scholars interested in placement at top research universities upon graduation. Applicants must hold a graduate degree and submit a CV, personal statement, and three letters of recommendation.\nLouisiana State University – PhD Leadership & Human Resource Development – 41.25 Points\nLouisiana State University’s PhD in Leadership & Human Resource Development may be tailored to suit the student’s personal research interests in the field. The goal of the degree is to place students in careers in professional consulting, organization development leadership, and academics related to their individual interests. Admission criteria includes a graduate degree with a 3.0 GPA, resume or CV, personal statement of interest, and three letters of recommendation.\nAuburn University – PhD Management, Human Resources/Organizational Behavior – 38.75 Points\nThe PhD in Business Management with a specialization in Human Resources/Organizational Behavior from Auburn University intends to turn students into well-respected faculty at leading national universities. Students in this program often have their research contributed to industry journals prior to graduation. Applicants must have a bachelor’s degree at minimum and may be required to take prerequisite courses if they do not have a graduate degree.\nUniversity of Pittsburgh – PhD Organizational Behavior and HRM – 36.25 Points\nResearchers in this PhD program at the University of Pittsburgh study how individual workers and teams drive business success, how society influences work culture, and how social responsibility influences work performance. This is an extremely competitive program where typically only two new students per year are accepted. Basic application requirements include graduate degree transcripts and three letters of recommendation.\nIndiana University – PhD Organizational Behavior/HRM – 35 Points\nIndiana University’s PhD coursework intends to place students into research positions by focusing on research methodologies and statistics. Students may choose a minor field of study interest that is approved by an advisor within the Department of Management and Entrepreneurship. Approximately three new students are admitted per year and applicants must have a graduate degree with a 3.5 GPA, three letters of recommendation, and must be deemed a good fit with faculty interests.\nUniversity of Illinois-Urbana/Champaign – PhD Business Administration, OB Emphasis – 33.75 Points\nThe program for Business Administration with a focus on Organizational Behavior at the University of Illinois Urbana-Champaign boasts flexible coursework and 1-on-1 faculty mentorship as students study how workers influence the economy and social environments within organizations. Since 2005, 98% of graduates from this program were placed in academic careers. Admission prerequisites include a master’s degree with a 3.0 GPA, proficiency in statistics, resume, and three letters of reference.\nThe Ohio State University – PhD Business Administration, HRM – 27.5 Points\nDoctoral students of the Ohio State University’s Business Administration PhD program may choose from several areas of specialization including Human Resource Management. All students admitted to this program are fully funded from four to six years at the discretion of the academic department. Acceptance into this program is very limited and applicants must provide graduate academic transcripts, three letters of recommendation, and an essay statement of intent.\nTexas A&M University – PhD Organizational Behavior/HRM – 26.25 Points\nTexas A&M’s PhD concentration on Organizational Behavior and Human Resource Management focuses research on topics such as individual worker’s influence on the workplace, effective leadership strategies, and the impact of public policy on human resource programs. Graduates typically find employment through a faculty network with other institutions or professional associations partnered with the university. Applicants must submit graduate transcripts, a personal statement essay, professional resume, and three letters of recommendation.\nUniversity of Houston – PhD Management & Leadership – 16.25 Points\nIn addition to their PhD coursework and research projects, Management & Leadership students at the University of Houston are typically hired as Research Assistants within the department where they receive financial stipends and may receive tuition waivers. Doctoral students will typically teach three or more courses and submit two research journal papers prior to graduation. Applicants will need to submit a statement of purpose, resume/CV, and three references.\nOur Ranking Methodology\nWe obtained a list of universities offering PhD programs in Human Resources or related fields and ranked them on a 100 point scale across two factors: tuition and student-faculty ratio. Tuition made up 50% of the score with 25% in-state tuition and 25% out-of-state tuition, with lower cost receiving high points. The other 50% points were awarded to school-wide student-faculty ratio where lower class sizes received more points.\nBy sorting these points and then adding them together we arrived at the total used for our ranking.\nFor more programs check out our online doctorate degrees in human resources.\nWe strive to be as accurate in our data as possible. If you work for a school on this list and see an error, please contact us so we can make a correction."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:a732ed0e-532e-4b6c-8cc7-797a3182b391>","<urn:uuid:bedd4fea-1761-4349-ac26-27241629236a>"],"error":null}
{"question":"What similarities exist between how the Maya Haab and French Revolutionary calendars divided their years into months?","answer":"Both calendar systems used unique approaches to organizing months, but with some similarities. The Maya Haab had 18 months of 20 days each, plus an additional short month called Wayeb of 5 days at the end of the year, totaling 365 days. Similarly, the French Revolutionary calendar consisted of twelve 30-day months, followed by 5 additional days called the 'sans-culottides' at the end of the year (or 6 days in leap years), also totaling 365 days. Both systems thus incorporated a similar concept of having a main sequence of regular months plus a short period of extra days to complete the solar year.","context":["The Maya calendar (and time keeping in pre-Columbian Mesoamerica) is a fascinating but poorly understood topic that has gathered much interest in recent years (particularly around 2012!). In this article, we will explain how the Maya calendar was made, how it works and how to read it.\nTime was extremely important to the Maya, they made elaborate and accurate calendars and used them in charting the movements of the sun, moon, stars and even planets.\nThese calendars served a variety of purposes both practical and sacred. They were used in astronomical calculations, recording important events, the reigns of rulers and their conquests and also in divination.\nNB: specialists say “Maya calendar” and not “Mayan calendar” (see: 10 red-flags for spotting an unreliable online resources).\n- Introduction: The Maya calendars\n- The 260-day count (Tzolk’in)\n- The 365-day count (Haab)\n- The 52-year count (Maya calendar round)\n- The Long Count\n- Maya Calendar facts\n- Further resources\nHow does the Maya calendar work?\nIn the late 19th century, Ernst Forstemann worked out how the Maya marked time. The Maya used what is now known as the calendar round which is made up of 3 interlocking cycles. A cycle of 20 names, a cycle of 13 numbers (which forms the 260-day sacred calendar) and a 365 days solar year. 52 years will pass until the three cycles line up again.\nThe following video -taken from the acclaimed documentary Breaking the Maya Code– gives a great overview:\nThe sacred calendar (Tzolk’in) 260 days.\nThe 260-day calendar, also known as the Sacred Calendar or Tzolk’in in Maya, has 20 day associated with the numbers 1-13 (20 x 13 = 260). It would run as follow: 1 Imix, 2 Ik, 3 Akbal, 4 Kan… to 13 Ben, then numeral 1 would return and get associated with the fourteenth day: 1 Ix, then 2 Men, 3 Kib and so on continuing in an endless cycle.\nEvery day was significant, similar to an astrology chart.\nNotice that the Maya did not have numbers like ours, they only had three digits; a dot standing for ‘one’, a bar standing for ‘five’ and a shell for ‘zero’ (see Maya numerical system)\nFor example: 4 would be 4 dots, 5 would be 1 bar, 10 would be 2 bars and 13 would be 2 bars and 3 dots.\nThe solar calendar (Haab) 365 days\nThis contained 19 ‘months’ – 18 months of 20 days and a closing month of 5 days (Wayeb).\nFor example: 0 Pop would be followed by 1 Pop, then 2 Pop, up to 19, then it would be 0 Wo, 1 Wo and so on.\nWhen these two calendars (Tzolkin and Haab) are working together, one day in this round such as “3 Kan 8 Pop” did not repeat until 52 years passed, which was called the Calendar Round.\nSo you basically have a number + day + number + month.\nMoving on from this was their absolute dating system, which we call the Long Count.\nLike our own calendar the Maya marked dates for more extensive time from a fixed starting point. In our calendar it is the birth date of Christ, for the Classic Maya the beginning of the present creation was 13th August, 3114 BC.\nEach great cycle lasted 5125 years and it repeated indefinitely. The first great cycle was to end on 21 December 2012. This led to the popular idea that the Maya prophesied the world was to end on that date. However, this is completely a modern invention, time was not lineal for the Maya, but cyclical and ever repeating.\nNow that you have a general idea of the Maya calendars, let’s see them in more details:\nThe Tzolk’in: the 260-day count\nThe 260-day count, which approximates the human gestation period, as well as the time it takes for a maize plant to come to fruition, is still used in some Maya communities today, mostly in the highlands of Guatemala.\nIt was, and still is, a sacred almanac that provided a chronological framework for Maya ceremonial life, and a basis for prophecies.\nBirth dates were recorded by this calendar, and the patron deity of a particular day became closely associated with the destiny of babies born on that day.\nTzolkin (Sacred Calendar)\nThe Tzolk’in is a succession of 260 days made up of the permutation of 13 numbers with 20 names (13 x 20 = 260), very much in the same ways our months consist of the association of 7 week-days (Monday to Sunday) and 28, 29, 30 or 31 numerals (for example Friday the 13th which is the 13th day of the month and the 5th day of the week).\nEach day is uniquely designated by the combination “coefficient/day-name”, and not until every single one of the numbers 1 through 13 had been attached to every one of the 20-day names was the cycle complete. It would take 260 solar days to complete.\nBelow: names and glyphs of the Tzolk’in 20 days\nThe first day of the Tzolk’in is “1 Imix”, the second is “2 Ik’”, the third is “3 Ak’bal”, the thirteenth is “13 Ben”, the fourteenth is “1 Ix”, the twenty-first is “8 Imix”, and so on.\nA particular combination will not recur until 260 days have elapsed.\nBelow: conceptual representation of the Tzolk’in cycle with numerals on the left (dot = 1, bar=5) and days glyphs and names on the right\nThis sacred calendar is still used by the highland Maya people today. Each community has their day keeper, generally a shaman, who carries out ceremonies on particular days of this calendar. People will go to the day keepers to ask questions about their future or help in curing an illness. Day keepers then ask the spirits for guidance.\nTo read an account of a Maya day ceremony – click here\nBoth the ancient and highland modern Maya believe that they have a spirit/animal companion, the ancient Maya called it Way (pronounced ‘Why’) and the Maya today call it their nahual/nawale. Your birthdate defines what animal/spirit companion you have and also tells you your character traits.\nBelow is the list of the 20 nahales and their respective calendar name, both in the Yucatecan Mayan language (in italic) and the highland Mayan language of Kaqchikel (in bold).\n- Crocodile – Imix – Imox\n- Wind – Ik – Iq\n- Night – Akbal – Aqabal\n- Sky – Kan – Kat\n- Snake – Chikchan – Kaan\n- Death – Kimi – Kame\n- Deer – Manik – Kiej\n- Venus – Lamat – Qaniel\n- Moon – Muluk – Toj\n- Dog – Ok – Tzi\n- Howler monkey, ancestor – Chuwen – Baatz\n- Tooth/Jaw – Eb – E\n- Maize – Ben – Aaj\n- Jaguar – Ix – Ix-balaam\n- Eagle – Tzikin – Men\n- Candle – Kib – Ajmaq\n- Earth – Kaban – N’oj\n- Flint, obsidian – Tijax\n- Storm – Kawak – Kawoq\n- Lord – Ajaw – Ajpu\nThe Haab: the 365-day count\nSometimes called the “Vague year” because leap year adjustment was never intercalated, the Haab is a 365-day period.\nIt is made up of 18 months of 20 days, and an extra month, called Wayeb, of only 5 days at the end of the year which gives a total of 365 days. Those 5 extra days were usually considered to be a special time.\nHaab (Sacred Calendar)\nThe months’ names are all taken from the Maya Yukatek list given to us by Bishop Diego de Landa in the 16th century: Pop, Wo, Sip, Sots’, Sek, Xul, Yexk’in, Mol, Ch’en, Yax, Sak, Keh, Mak, K’ank’in, Muwan, Pax, K’ayab, Kumk’u and Wayeb.\nThe 365-day count operates very much like our own calendar: the first month is “Pop”, and the first day of the year is “1 Pop” followed by “2 Pop”, and “3 Pop”, and so on until “19 Pop”. The following day is referred to as the “seating”, or “putting in place”, of the next month (Wo) and transcribed as “0 Wo” (yes, the Maya had figured out the concept of “zero”)\nThe unusual aspect of this calendar is that the “seating” of a given month is not considered as the first day of the month but rather as the last day of the previous one. This is akin to the tradition of calling December 31st “New Years Eve”.\nIn the Maya concept of time, the influence of a given month started on the last day of the previous one.\nBelow: names and glyphs of the Haab the nineteen months.\nThe Maya Calendar round\nIt is always written in the same order: (1) day coefficient + day name in the Tzolk’in, and (2) day number + month name in the Haab.\nFor example, January 9th 2012 was 8 Ben 1 Muwan in the Calendar Round.\nSince 260 and 365 have a least common denominator of 5, it will take 18,980 days (260×365/5), approximately 52 years, before a specific date in the Calendar Round recurs.\nThe 52-year count was in use throughout Mesoamerica.\nThe Mexica (Aztecs), for example, called these periods xiuhmolpilli, meaning “year bundle”. The start of a new xiuhmolpilli was cause for much celebration. The Maya name for these 52-year period is not known.\nThe association of the 20-day cycle in the Tzolk’in and the 20-day months in the Haab creates a noteworthy phenomenon: for any given year, the first day of all the months in the Haab will start with the same Tzolk’in day. For example, if the first day of the year (1 Pop) falls on a day Ben in the Tzolk’in, then the first day of every month in that year will be on a day Ben. These days are called “Year Bearers”.\nGiven of the short month of 5 days (Wayeb) at the end of the Haab, the first day of the following year has to be 5 days later in the Tzolk’in.\nSince there are only 20 days in the latter, there can be only 4 Year Bearers.\nDuring the Classic times, these were Ak’bal, Lamat, Ben, and Etz’nab. The same traditional Year-bearer pattern continues to hold in some traditional highland Maya communities. By the time of European contact, however, the Year-bearers used in the Yucatec calendar were K’an, Muluk, Ix, and Kawak.\nThe Maya Long Count\nMoving on from this was their absolute dating system, which we call the Long Count. Like our own calendar the Maya marked dates for more extensive time from a fixed starting point. In our calendar it is the birth date of Christ, for the Classic Maya the beginning of the present creation was 13th August, 3114 BC. Each great cycle lasted 5125 years and it repeated indefinitely.\nEvery inscription began with the date, usually the long count date and then followed by the calendar round (Tzolk’in date + Haab date).\nThe base unit in the Long Count was the day (K’in). Since the Maya used a vigesimal numeral system (i.e. based on 20, the same way our decimal system is based on 10), they then used a period of 20 days (Winal) which was in turn grouped in packs of 18 (not 20, probably to approximate the solar year) called Tun, and then the Katun which is 20 Tuns, and the Baktun which is 20 Katuns, etc\nThe Long Count then is the counting of days since the last creation and was divided into:\nK’in = 1 day\nWinal = 20 days (20 K’ins)\nTun = 360 days/K’ins (18 Winals)\nK’atun = 7,200 days (20 Tuns)\nBak’tun = 144,000 days (20 Ka’tuns)\nMaya Calendar Facts\nHow accurate is the Maya calendar?\nThe Maya did not use any system of leap days, so the Haab had an error rate of 1 day every 4 years when compared to the real solar year.\nHow many days did the Maya calendar have?\nThe Haab has 365 days (18 months of 20 days + 5). The Tzolk’in has 260 days (20 names and 13 numbers). The calendar round has 18,980 days (52 years).\nDid the ancient Maya predicted that the world would end on 21 December 2012?\nNo, they didn’t! A great cycle of the Maya Long Count was to end on 21 December 2012, but the next day the Maya believed that a new cycle would begin. There was to be no end of the world.\nIn fact, in the temple of inscriptions at Palenque, where we find the tomb of King Pakal, it was written that in AD 4772 the people would be celebrating the anniversary of the coronation of their King Pakal.\nMaya calendar activities (KS2)\nSee our post: Maya calendar activity and quizzes (KS2)\nFurther Resources on the Maya calendar\nLesson Plan (KS2) – Please note that if you are having problems downloading these files then please use your personal, rather than school’s email address. Some schools disable the ability to receive items outside of their domain.\nMichael D. Coe and Mark Van Stone (2005) Reading the Maya Glyphs. Thames and Hudson, London\nThis book includes a chapter on Time and the Calendar.\n2012 – End of The World\nAnthony Aveni (2009) The End of Time: The Maya Mystery of 2012, University Press of Colorado, Boulder\nDavid Stuart (2011) The Order of Days: the Maya World and the Truth about 2012. Harmony. New York\nFAMSI website has an excellent discussion and powerpoint of this phenomenon – http://www.famsi.org/research/vanstone/2012/index.html\nFAMSI website also has colouring books you can download for the Maya months and days – http://www.famsi.org/research/pitts/MayaMonths_English.pdf\nThere is also a selection of videos on the end of the world which can be found here","French Revolutionary Calendar\nConverting dates since 1792\n|Crane Brinton's discussion of the calendar||The names of the months|\nWhy Conversion Programs Produce Varying Results\nThere are several internet programs which convert dates between the Gregorian and the French Revolutionary (Republican) calendars. These programs usually provide for the conversion not only of dates during the period when the French calendar was in use but also of dates subsequent to Napoleon's discontinuation of the calendar on Januray 1, 1806. Conversion of dates between 1792 and the end of 1805 is of utility to historians and genealogists. Conversion of subsequent dates is mostly of interest to hobbyists and francophiles (though the French Revolutionary Calendar was briefly revived by the Paris Commune in May 1871*).\nAlthough the new calendar was not adopted until October 1793, the first day of the first year was retrospectively set for the day the Republic had been proclaimed, which happened to coincide with the autumnnal equinox in 1792: thus September 22, 1792 = 1 Vendémiaire I. A normal French calendar year of 365 days was to consist of twelve 30-day months followed by 5 \"complementary\" days, the \"sans-culottides,\" at the end of the year, i.e. in late September.\nThe original scheme was to make every year start on the day of the autumnal equinox, as observed and calculated in Paris. To accomplish this, a leapday would be inserted as a sixth \"complementary\" day if it was needed to make the following year start on the autumnal equinox. This resulted in leap years in years III, VII and XI, with leap years projected for the years XV and XX. So leap years would be determined by an astronomical observation and, unlike the Gregorian calendar, would not occur at regular four year intervals. Note the five year interval between XV and XX.\nHowever, the French declaration establishing the Revolutionary Calendar was itself somewhat ambiguous, even contradictory, in laying out a method for inserting leapdays in perpetuity, and a variety of proposed alternative methods arose. For example, the head of the commission which proposed the Calendar, Charles-Gilbert Romme**, subsequently suggested that leap year calculation be simplified by borrowing from the Gregorian calendar the 4-100-400 rule***, replacing astronomical observation with the simplicity of arithmetic and yielding a more regular cycle. Some of these alternatives are well discussed in Wikipedia.\nConversion of dates since January 1, 1806, is made problematic (and essentially speculative) by these differing methods. It is the differences among these methods which cause most of the apparently inconsistent results given by the programs on the internet.\nThere is another source of divergence, however: namely a defect in the implementation of the\ncentury years when using the Date(mm/dd/yyyy) function (although it gets 1900 and years divisible by 400 right).\nThis defect exists even in the current version (6.0.4). The converter on this page avoids the problem with a small\nworkaround, kindly provided by calendar maven Steve Morse.\n* May 6th to May 23rd: 16 Floréal to 3 Prairial LXXIX.\nLinks to conversion programs with particular features\n|Stephen Morse's on-line program\nAllows the user to select any of three defined methods for handling leap years. Allows conversions both to and from the French calendar.\n|José Luis Martin Mas's Dashboard Widget for the Mac\nConverts (only) the current date to Revolutionary Calendar format, using the 4-128 rule for leap year calculations, i.e. a leap year is a year divisible by 4, unless divisible by 128.\n|José Luis Martin Mas's iPhone/iPad app: \"Calendrier\"\nThe user can choose between the Equinoctal rule and the Romme rule for leap year handling. ($1.99)\nThe names of the months\nCrane Brinton on the French Revolutionary calendar\nThe culmination...of revolutionary propaganda [was] its new calendar. Almanacs had been from the beginning of the Revolution a favorite and successful method of spreading the word. Collot d'Herbois himself had won, with his Almanach du Père Gérard, a prize offered by the Paris Jacobins for a work to spread the new ideas in simple language.\nBut for the Jacobins of 1794 it was not enough to print good republican moral counsels, after the manner of Franklin, at the appropriate dates and seasons. The whole calendar must be made over. The existing calendar perpetuated the frauds of the Christian church (Jesus himself was probably a good sans-culotte; all the nonsense stemmed from Paul), and was highly irrational and inconvenient.\nThe new calendar, based on a report of Fabre d'Églantine, was adopted by the Convention in October, 1793. By it the year began on September 22 of the old calendar, and was divided into twelve months of thirty days each, leaving five days (six in leap years) over at the end of the last month. These five or six days were to be known as the Sans-culottides, and were to be a series of national holidays. Each month was divided into three weeks, called décades, the last day of each décade being set aside as a day of rest corresponding to the old Sunday.\nThe months were grouped into four sets of three, by seasons, and given \"natural\" names, some of which are rather attractive--vendémiaire, brumaire, frimaire (autumn); nivôse, pluviôse, ventôse (winter); germinal, floréal, prairial (spring); messidor, thermidor, fructidor (summer). The days of the décade were named arithmetically--primidi, duodi, on to décadi. In place of the old saints' days, each day was dedicated to a suitable fruit, vegetable, animal, agricultural implement.\nThe Sans-culottides were dedicated, the first to Genius, the second to Labor, the third to Noble Actions, the fourth to Awards, and the fifth to Opinion. This last was to be a sort of intellectual saturnalia, an opportunity for all citizens to say and write what they liked about any public man, without fear of the law of libel. The sixth Sans-culottide of leap years was dedicated to the Revolution, and was to be an especially solemn and grand affair. The republican era was to date from the declaration of the republic in September, 1792. When the calendar came into use, the year I had already elapsed.\nIn spite of its symmetry and its poetic months of budding and of mist, the new calendar was not a success, and Napoleon abandoned it....Workingmen preferred one day's rest in seven to one in ten; its terminology, appropriate to the climate of France, was singularly inappropriate to that of the Southern Hemisphere; it embodied a new cult, and that cult, though it profoundly influenced Christians then and since, failed completely to supplant Christian terminology. The calendar and its fate form in many ways a neat summary of Jacobin history.\n--from A Decade of Revolution, 1789-1799 (1934)\nvisits since 1998:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b159df2d-6cb6-4b1f-b879-a806556af60a>","<urn:uuid:847211f0-681c-4bae-8dee-58b62da0233a>"],"error":null}
{"question":"I'm planning to install Bermudagrass in my southern lawn. What are the key establishment methods for traditional Bermudagrass varieties, and what specific installation requirements does Tifway 419 have as a hybrid variety?","answer":"Traditional Bermudagrass varieties can be established through both seeding and sodding methods. Seeded varieties like Normal, Yuma, Sundevil, and U3 are less expensive to install and maintain. The best time to seed is from late spring when night temperatures stay above 65 degrees until late summer, with a seeding rate of 1 to 1.5 lbs per 1000 sq ft. As for Tifway 419, it's only available as sod and cannot be established from seed. While it can be laid year-round, even when dormant, it performs best when installed during the growing season. Tifway is a sterile hybrid that spreads through both above-ground stolons and below-ground rhizomes to form a dense turf, making it particularly successful in the lower transition zone and Deep South.","context":["Choosing the best turf grass for your southern lawn depends on two factors: how much sun your lawn will receive, and its geographical position within the warm seasonal grass region. For southern lawns the key grass options are: bermudagrass, St. Augustine, centipedegrass, zoysia, tall fescue, and buffalograss.\nBermudagrass Bermudagrass is a vigorous warm grass of the season that spreads through widespread stolons and rhizomes. This is a common crop in the South and Southwest. It needs full sun, and shade tolerance is very low. There are two basic classes of bermudagrasses — those which can be formed from seed, and those which can be planted only from sprigs or sod. Seeded bermudas like normal, Yuma, Sundevil, and U3 are less expensive to set up, and appear to be cheaper to maintain. The seeding rate goes from 1 to 1 1/2 lbs. By 1000 sq. M. Ft. The best time to start a bermuda lawn from seed is from late spring when temperatures in the night remain above 65 degrees until late summer.\nSodding is the most common method by which a hybrid bermudagrass lawn is formed, although sprigging is also used. Bermuda lawns can be sodded at any time of the year, but the best time is in the summer.\nFor planted bermudagrass the best mowing height is no greater than 1.5 inches. The hybrid types, often used on golf courses, have a lower height of mowing. Bermudagrasses, when properly mowed, fertilized, and fed, have no major disease or insect problems. Seeded bermudagrasses have the lowest water and fertilization requirements of all southern grasses except for buffalograss.\nSt. Augustine St. Augustine is a popular and widely used lawn grass from Florida to California in the tropical coastal areas of the south. Its popularity is primarily due to the southern grasses having the greatest shade tolerance, though it thrives even in full sun.\nSt. Augustine, with a larger blade of leaves than most other lawngrasses, spreads through the stolons. With proper fertilization and moisture, it is fairly easy to establish by sodding or plugging, and will do well in most types of soil. It is however very susceptible to freezing temperatures and killing winter; thus restricting its use in the upper south.\nThe available St. Augustine grasses vary greatly in their resistance to the cold. Before you buy, make sure the one you pick has good winter survival records in your area.\nSeveral different St. Augustine strains are available to provide a green, dense lawn for the homeowner. But only those identified as resistant to a disease called St. Augustine Decline (SAD) should be bought. By planting plugs of the latest SAD-resistant strains, older St. Augustine lawns that have SAD can be strengthened.\nDuring the summer months the best time to sod St. Augustine is when temperatures stay above 65 degrees. Of optimal growth it generally uses more water than bermuda. St. Augustine needs at least 2 inches in sun and 2.5 to 3 inches in shadow.\nZoysiagrass Zoysia isn’t as shade tolerant as St. Augustine, but is much more shade tolerant than bermudagrass. Since zoysia is the southern grass’s most winter-hardy, it does better in the upper southern regions. Zoysia isn’t resistant to drought like bermuda or buffalograss. It is planted most often as sod or as plugs. Zoysia spreads through stolons and rhizomes, and it takes a significant period of time before plugs form a complete cover.\nThere are typically two core forms of zoysia available: Emerald and Meyer. Emerald has a leaf blade much finer than Meyer, and can form a rather dense, dark green lawn. Meyer’s leaf blade is average textured like a typical bermudagrass, but can also form a thick green lawn.\nMeyer is usually more resistant to drought than Emerald although both are well suited to dry conditions. Thatch growth can be a major problem with zoysia grasses but with proper mowing, water, and fertilization, it has no serious disease or insect problems. The optimum height for mowing is about 1.5 inches.\nBuffalograss This is the only turfgrass from Texas to Canada originating in the North American great plains. This is a warm turfgrass of the season, spreading through stolons.\nBuffalograss has fine blades with a blue-green colour. It won’t form turf as thick as other grasses in the South. Buffalograss is a growing interest for low-maintenance lawn areas. It can survive extreme drought conditions, has low fertility requirements, and when left unmowed, it will usually not grow by more than 4 to 5 inches. Yet buffalograss has little resistance to the darkness.\nSome recent varieties of buffalograss, such as Prairie, Buffalawn, and 609 are only available as sod. A increasing number of varieties known as seeds are available, including Comanche, Texoka, Plains and Topgun. Buffalograss isn’t suitable for everyone, nor for many lawn areas. But its drought tolerance and low maintenance requirements in the upper south and semi-arid regions have increased interest among many homeowners. The best height to mow is 2 inches.\nTall Fescue While turf form tall fescue is generally considered a northern, cool seasonal grass, its use is increasing in the upper south, especially on well shaded lawns instead of St. Augustine’s. For the southern summers, tall fescues also have excellent heat and drought tolerance and can withstand cold winter temperatures with very few issues.\nTall fescue is a common species because it remains green, even when dormant, all winter. Using two or more enhanced tall fescues of the turf form as a blend also provides a heartier lawn than using a single older fescue variety such as K-31.\nTall fescue is a bunch of grass, and is usually formed by seeding. Fall or early spring is the best time of the year to plant tall fescue. The general rate of seeding is 8 livres. By 1000 sq. Ft., but follow the recommendations of the manufacturer set out in the box or bag. For tall fescue the optimal mowing height is 2 inches.\nCentipedegrass Centipedegrass is well suited in the south and upper south to most soils and climatic conditions, but is not so well adapted to the more and western regions of the Mississippi. The leaf blade is of a medium texture and forms a thick, fine, low-growth turf. Centipede can be formed either by seed or vegetative sod planting, and it spreads against the soil by stolons which lie flat. Sowing 1 lb at a time. By 1000 sq. Ft. It is fairly cold tolerant, and usually works well in changing areas of shade. Centipedegrass is considered a low-maintenance grass and typically produces a lawn of lesser quality than bermudagrass or St. Augustine. It does not require frequent mowing but 1.5 inches is the best mowing height.\nIn the south, many homeowners use improved perennial ryegrass for supervised bermudagrass, which keeps the lawn green throughout the winter. In the upper south, the best time to be supervised is during the first half of September. The last half of September into the first part of October is the best time in the lower South regions. Ideally, perennial ryegrass should be planted around 6 to 8 weeks before the first heavy frost’s average date, and dormant before the bermudagrass goes. The lowest seeding rate throughout the winter is 3 lbs, just to give the lawn a little green colour. By 1000 sq. 5. Ft. Nevertheless use 8 to 10 lbs. By 1000 sq. Ft. to keep the green lawn relatively dense during the winter months.\nSupervision requires no special equipment other than seed spreader or fertilizer spreader. The biggest drawback potential to supervise with perennial ryegrass is that some plants may persist as the weather turns warmer into the summer. In the Bermuda lawn the ryegrass then becomes weeds. Low mowing and less water promote bermudagrass over ryegrass, or use a post-emergent herbicide to control ryegrass.\nWhen combined, the southern grasses produce no quality lawns. Because they are distributed by stolons, mixed varieties appear to segregate each variety and form distinct patches. Choose from the southern grasses that best suit your geographic area and unique lawn conditions, i.e., sunny, cool, damp or arid. Then determine whether to use seed, sod or sprigs. Doing yourself can be a lot of work but a nice, well-kept lawn is worth it! It’s going to benefit your house, family and the world.\nMake sure you buy seed of quality!\nBe cautious about low-cost seed mixtures, as a homeowner. They also contain grasses normal, unadapted, temporary and of low quality. Better seed varieties can generally cost a little more but the potential for better lawn quality lies within that crop.\nRead the seed label and see what you get inside the box or bag.","Tifway 419 Bermudagrass\nCynodon transvaalensis Burtt Davy × Cynodon dactylon (L.) Pers.\n- Shade & Tolerance\n- 1 out of 10\n- Drought & Tolerance\n- 7 out of 10\n- Traffic Wear & Tear Tolerance\n- 7 out of 10\n- Maintenance Level\n(Time + Dollars)\n- 9 out of 10\nFor more than 50 years (until the release of new, improved hybrid Bermuda TifTuf®), Tifway provided unmatched quality and performance. Tifway is a fine textured, light green, sterile hybrid. Tifway spreads by above ground runners (stolons) and below ground rhizomes to form a dense turf.\nTifway performs well in residential and commercial landscapes, golf course fairways and tees, sports fields as well as public and private parks and recreational sites.\nTifway is most successful in the typical Bermudagrass areas of the lower transition zone and the Deep South.\nZones 7b to 11a on the 2012 USDA Plant Hardiness Zone Map or Zones 1-2 on the Turfgrass Hardiness Map.\nTifway is a warm-season grass that grows best in hot climates. In the upper south, after first frost, it will start to go dormant for the winter.\nTifway performs best in full sun.\nDrought Tolerance & Water Management\nTifway survives drought conditions by going dormant, then greens up at the first exposure to moisture. Tifway does not like poorly drained soils especially where water may stand for several days. Optimum water management, as with any grass, is dependent on soil physical characteristics and climate. In general, Tifway will require 1” of water per week.\nA well-maintained, vigorous Tifway lawn with good turf density will inhibit weed establishment, but to be sure, use of a pre-emergent herbicide (recommended for use on Bermuda lawns on the label) is recommended in the spring and fall. Tifway is tolerant to most herbicides, but carefully read and follow all herbicide product labels.\nSusceptible to the typical Bermudagrass insect pests. Recovers well with proper insect control.\nDiseases are a minor problem with Tifway when fertility is managed correctly. We recommend annual spring compost topdressing with our Soil3 organic compost to minimize disease impact (more info at soil3.com) and help control thatch.\nTraffic & Wear Tolerance\nTifway tolerates wear and traffic from activities such as football, soccer, baseball, and golf. Because of its rapid growth rate, when managed properly, recovery is normally faster than other warm season grasses with the same amount of wear.\nFor best appearance, Tifway’s fertilizer requirements are among the highest. Apply between 4 and 6 pounds of nitrogen per 1,000 square feet annually, during the growing season. Select a fertilizer that is labeled for use on Bermudagrass lawns. Do not apply fertilizer to wet turf to avoid burning. Too much fertility will result in a thatchy and unhealthy lawn. Always fertilize based on a soil test. Don’t use high nitrogen “winterizers” in the fall. Compost topdressing with Soil3 and aerating in the spring, after spring green up, can supply one of Tifway’s fertilizer applications for the year. The practice of compost topdressing with Soil3 is true organic lawn care.\nBermuda thrives on a neutral to alkaline soil with a pH range of 6 to 7. Do not apply lime to Bermuda without a soil test.\nMaintain cutting height of 0.5-2 inches. Mowing may be required on a 3 to 6 day interval. Mow no more than 1/3 of the leaf off at one mowing. Use sharp mower blades. Can be mowed with rotary mowers or robotic lawn mowers.\nTifway boasts aggressive establishment from sod during the growing season, yet Tifway sod can be laid all year long, even when dormant. Complete sod establishment instructions are on our website. Tifway is not available from seed."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ad59b294-d9a7-4477-ad45-04a2325e20d0>","<urn:uuid:37f8e203-eed7-4482-becc-dd9543f5f618>"],"error":null}
{"question":"What's better for calcium - milk or sesame seeds? 🤔","answer":"Sesame seeds provide more calcium than milk. A quarter cup of raw natural sesame seeds contains 351 mg of calcium, while one cup of non-fat milk has 316.3 mg, and one cup of whole milk contains only 291 mg of calcium. Additionally, sesame seeds are alkaline while milk is acidic.","context":["Cart (0) Close\nShowing all 6 results\nStrongly pungent and aromatic, ajwain seeds are one of the popular spices commonly feature in Indian and Middle-Eastern cuisines. Botanically, the spicy seeds belong to the family of apiaceae (umbelliferae), of the genus, Trachyspermum. Scientific name: Trachyspermum copticum.The umbellifers are the members of carrot or parsley family, which includes many herbs and spices such as dill, fennel, anise seed, and caraway. Some of common names for the seeds are ajowan seeds, carom seeds etc.\nCumin is the dried seed of the herb Cuminum cyminum, a member of the parsley family. The cumin plant grows to 30–50 cm (0.98–1.6 ft.) tall and is harvested by hand. It is an herbaceousannual plant, with a slender branched stem 20–30 cm tall. The leaves are 5–10 cm long, pinnate or bipinnate, thread-like leaflets. The flowers are small, white or pink, and borne in umbels. The fruit is a lateral fusiform or ovoid achene 4–5 mm long, containing a single seed. Cumin seeds.\nIn many parts of India and Pakistan, roasted fennel seeds are consumed as mukhwas, an after-meal digestive and breathe freshener. Fennel leaves are used as leafy green vegetables either by themselves or mixed with other vegetables, cooked to be served and consumed as part of a meal, in some parts of India. In Lebanon, it is used to make a special kind of egg omelette (along with onions, and flour) calledijjeh.\nFenugreek is a plant grown for its herbal uses as well as for the seeds, which are a spice and have medicinal properties. Fenugreek is best known for its use in curry. The fenugreek seed is yellow in color and is often made into a powder before use, but the whole seeds are also used in cooking. Common uses for whole seeds include as a pickling additive, and for flavor in some Indian food dishes. Fenugreek seed powder is often categorized as a spice. It is also often referred to as bird\\'s foot, its most common name. Fenugreek is grown worldwide but mainly found in the Mediterranean region, the Ukraine, India and China. It is very popular in India where it plays a large part in the flavor of the cuisine. It is common for the seeds to be used to make tea in Egypt. Elsewhere it is eaten as a food or used for medicinal purposes. The other parts of the plant are mostly used as an herb and not as much for medicinal purposes.\nBlack mustard seed (Brassica nigra) is native to southern Europe along the Mediterranean Sea and has been cultivated for thousands of years. Black mustard seeds are slightly stronger than brown mustard seeds. Black mustard seeds are yellow inside, usually less than .1 inch in diameter. Yellow Mustard seeds or mustard powder can be used as a condiment in a variety of dishes. Mustard seeds can be used as is or can be roasted in a skillet. While dried mustard powder does not have a very strong quality, mixing it with water initiates an enzymatic process that enhances its pungency and heat. To moderate its sharp flavor, you can either add some very hot water or an acidic substance such as vinegar, either of which will stop the enzymatic process.\nMechanically Hulled Sesame Seeds are considered as the most nutritious seeds because of its protein and valuable Minerals. The husk hull (skin) of these seeds are removed hence are called as Hulled Sesame Seeds. Prior to hulling process, these sesame seeds are thoroughly cleaned. Hulled Sesame Seeds are relatively softer and delicious as compared to Natural Sesame Seeds. Machineries used by us for various processes such as cleaning, hulling, drying, color-sorting, etc., are modern & high-tech. They are processed in absolute germ – free & hygienic conditions. Sesame seeds are full of calcium, magnesium, copper, vitamin B1, zinc and dietary fiber. They offer the most nutritional value when the entire seed is used (un-hulled).Whole sesame seeds contain about 88 mg of calcium per tablespoon of seeds. Just a quarter cup of natural sesame seeds provides more calcium than a whole cup of milk. A quarter cup of raw natural sesame seeds has 351 mg of calcium while one cup of non-fat milk has 316.3 mg, and one cup of whole milk has only 291 mg of calcium. Plus, they are alkaline whereas milk is acidic. Sesame seeds are also rich in zinc, another mineral that has a positive effect on bone mineral density. A study published in the American Journal of Clinical Nutrition found a correlation between low dietary intake of zinc and osteoporosis at the hip and spine."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ff19ee51-3502-4923-a886-afd17c6f3289>"],"error":null}
{"question":"How do modern responsive architecture installations and Arctic housing solutions differ in their approach to environmental adaptation?","answer":"Modern responsive architecture installations and Arctic housing solutions take contrasting approaches to environmental adaptation. Responsive installations like Philip Beesley's work use advanced technologies to create 'living architectures' that actively react to human presence through sensors and movement. In contrast, Arctic housing solutions like the Drift House employ passive environmental adaptation, using calibrated snow fences to harness natural snowdrift patterns for shelter construction. While responsive architecture focuses on real-time modification and interactive responses to occupants, Arctic solutions work with natural elements like snow as both material and structural components, creating thermal zones through the interaction between ETFE panels and accumulated snow.","context":["Responding to the air\nIn order to grasp this, let us take the example of a local cafe, a very quiet cafe whose atmosphere would suddenly become animated when some new arrival puts money in the jukebox. In New Babylon, each person can at any moment, in any place, alter the ambiance by adjusting the sound volume, the brightness of the light, the olfactive ambiance or the temperature. Should a small group enter a space, then the ordering of that space can become something else. Constant Nieuwenhuys (1974)\nOur personal experience is the only way we can understand acoustic space. That is why acoustic experimentation is so important. Empirical investigations will eventually make us hear forms, materials and perspectives. Bernhard Leitner (1998)\nIn recent years, a number of developments, both technological and cultural, with the potential to change the way we think about sound both on the architectural scale and on that of the city have permeated the core of advanced architectural thinking. These ideas – most notably responsive buildings and robotics, on the one hand, and an interest in the unseen, in the often invisible environmental conditions of architecture on the other – are not in principle new ideas; their roots can be easily traced back to futurist ideas of the 1950s and 1960s (for example, in the Smithson’s House of the Future, in Banham and Dallegret’s Environmental Bubble or in the early writings of Nicholas Negroponte). However, in the past decade the technical possibility of their realization has entered the domain of architecture and, indeed, of architectural practice (Smithson, Smithson, Van den Heuvel, Risselada and Colomina 2004; Banham 1965; Negroponte 1975). We have seen a large number of projects that relate to responsive or interactive architectures in recent years: buildings, components of buildings, or possible building components or environments that respond to climactic or weather conditions, user needs and desires, or other variables. Most examples of such responsive architecture, such as North House from 2009, designed and built by a consortium of universities in Canada led by Geoff Thün of the architectural design-research firm RVTR, have been pragmatic in their intent, often with the explicit goal of energy conservation; work of this sort has begun to enter mainstream culture with the arrival of consumer devices such as the Nest Learning Thermostat. Other examples have been speculative in terms of our relationship with a non-passive built environment, as in the case of Mark Goulthorpe’s Hyposurface, first exhibited at the Venice Biennale in 2000. The Hyposurface is in essence a moving wall which reacts to contact with individuals, altering, in real-time, the topology of the installation. While the Hyposurface is limited in its application, it points to the possibility of a changeable architecture capable of modification in real-time according to our whims. The work of Philip Beesley takes these ideas further, making use of new and emerging technologies to produce “living architectures” – environmental installations, often reminiscent of a fantastic forest, constructed of laser-cut plastic fronds, tubes, containers. Beesley’s many installations, such as 2010’s Hylozoic Ground, produced for the Canadian Pavilion in Venice, are not static: they recognize the presence of humans within the installation and respond as though alive. The effect is uncanny – we are not used to having our buildings recoil, as though in horror, from our presence, or to having them hiss at us. But Beesley too, like Goulthorpe, is pointing us toward a possible future architecture, one in which buildings react, seamlessly, to our presence. Indeed, Beesley’s work leads us to consider again the relationship between architecture and its occupants, to see these not so much as separate entities, but rather as symbiotic components of an environmental system, mutually affecting each other through their interactions.\nWhile the works cited above have been forerunners in the development of responsive and interactive architectures, this area of research and development has expanded rapidly within architectural circles in the past few years. A number of significant publications, such as Fox and Kemp’s Interactive Architecture, have attempted to map this work, while interactive and responsive installations using motion sensors, Arduinos and lights, sounds or movement are now commonplace in architecture schools worldwide (Fox and Kemp 2009). Over the same period we have seen a strong and rapidly growing interest in the invisible components of architecture, most notably temperature and air quality, but also – as I will discuss shortly – sound. As in the case of responsive and interactive architecture, an interest in atmospheres can be pragmatic at its roots, with the intent of revisiting modernist thinking about environmental quality (which for the most part can be reduced, for modern architects, to air quality) in order to improve human comfort; key to this work is the writing of Michelle Addington (or the built projects of leading edge engineers such as Transsolar. Other projects making use of the invisible or atmospheric in architecture take a more speculative approach, asking what the conditions of the air might mean as groundwork for architectural thinking. Transsolar’s Cloudscapes, for example, another installation for the 2010 Venice Biennale, installed, through meticulous engineering, a cloud within one of the large rooms in the Arsenale, inviting visitors to walk up a spiral ramp into the cloud, posing specific if implicit questions about the nature of our inhabitation in the air. French architect Philippe Rahm, on the other hand, has produced a number of speculative designs for houses that make use of thermal gradients as the primary organizing principle, organizing programmatic functions in relation to a pre-existing thermal field, inverting the relationships between function and environment (Rahm 2009). In short, architectural thinking is shifting away from an understanding and concern with architecture as massive, static and monumental to a consideration of the manifold and multiple systems that make up our environment in the 20th century; buildings are no longer piles of rock, but rather – as Henri Lefebvre pointed out as early as 1968 – systems of flows. As Peter Sloterdijk has framed the issue, “with the transition from the 20th century to the 21st, the subject of the cultural sciences thus becomes: making the air conditions explicit” (Sloterdijk 2009). Architecture thus moves away from the tangible and discrete, away from a singular focus on buildings, on skins and shells, and towards the consideration and design of distributed attributes. Like the work on responsive and interactive architectures, the work on atmospheric design has developed its own body of literature – most notably, perhaps, Sean Lally’s The Air from Other Planets, which refocuses architectural thinking on the multiple forms of energy that define our environments (Lally 2014).\nDuring these same years, a whole constellation of new technological developments, some now well-established and some still emerging, have aided in this cultural shift. Within the design and construction industry, these new potentials might include digital design, already a move away from the material, but more importantly, digital fabrication techniques, which start to re-forge a link between the two – digital and the material – and simulation techniques, which allow a digital evaluation of the performance (including acoustic performance) of a building, its components, or of an urban environment without needing to bother with construction. In buildings we have new regimes of sensing (including sensing of sound) and robotic-powered actuation (including electroacoustic actuation). In the broader world we now have ubiquitous computing, the internet of things, the internet of bodies. All of these technologies, and many others that are currently in development, need to be considered not as simple tools, but as powerful non-human actors within the networks that make up our contemporary environments, actors that can be made use of but not ignored.","With shelters and settlements in northern Canada currently undergoing a housing crisis due to decadent construction and energy costs, the corresponding ramifications on overcrowding and deprivation have resulted. What the Drift House aims to do is hybridize the intelligence of tradition and technology of both housing systems to offer direction on future constructions in the Arctic. Designed by Neeraj Bhatia of The Open Workshop, this housing prototype for northern climates proposes a series of snow fences that passively utilize snowdrift to form a new dwelling type by calibrating the snow fences with differential mesh openings and height. More images and architect’s description after the break.\nWith insufficient core populations to support public structures, the typology of dwelling has attained the highest level of refinement within the Arctic’s unique climate. Pre-WWII indigenous Inuit Housing Types had embedded connections to the local landscape, its orientation, materials and fabrication, while embracing the nomadic Inuit lifestyle. With zero ecological footprint, these temporal dwellings employed opportunities from the landscape and atmosphere to form a complex shelter that negotiated thermal performance, local materials, soft construction techniques, program and cultural values.\nPermanent architecture –in the form of prefabricated Government housing– was employed in an effort to assert Arctic Sovereignty during the Cold War era by the Canadian Government. Despite instilling new notions of comfort and durability, this new housing neglected the cultural and sustainable intelligence of traditional dwellings. Importing a ‘southern’ model of dwelling, these hermetic containers were highly contingent on imported materials, energy consumption, labor, and transport costs, while forcing a transformation to the Inuit lifestyle that severed a connection to the dynamic landscape. These typologies have formed the template for Arctic shelter and settlements in Canada that still persists today.\nWhile the snow fence is a static infrastructure to protect roadways, it is manipulated to produce a precise pattern of passive snow accumulation from the dynamics of snowdrift. By understanding a malleable system, such as snowdrift, architecture can be formed in response to the landscape — calibrating, mitigating and using once ‘problematic’ environments in an opportunistic manner.\nThe house is comprised of four nested surfaces formed of ETFE panels that create a series of climatic zones that are parasitic to the calibrated snowdrifts. The accumulated snow passively ‘builds’ part of the dwelling, while also serving as a counterweight to the light cantilevered structure. The nested thermal environments produce a series of thermal zones related to the interior program and traditional lifestyles.\nThis hybridized space — a series of planes positioned in relation to a dynamic system, meshes high-tech thermal materials with the landscape (snow) to create soft thermal gradients. By using snow as a material, structural counterweight, and enclosure, the Drift House is able to passively accumulate a responsive shelter through the manipulation of a generic technology (the snow fence) to form new a typology of dwelling and construction.\nArchitects: Neeraj Bhatia/The Open Workshop\nLocation: Northern Canada\nProject Research and Design Team: Neeraj Bhatia (Director), Tracy Bremer, Mary Casper, ZachariahGlennon, Alicia Hergenroeder, Brian Lee & Sonia Ramundi\nFunded by: Graham Foundation for Advanced Studies in the Fine Arts, MIT Lawrence B. Anderson Award & Rice School of Architecture Faculty Research Grant"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:7217ed9b-c9a9-4393-b9b6-e708b37dfe66>","<urn:uuid:38b0946f-05b5-4250-9fe1-99c23bb4434f>"],"error":null}
{"question":"What are the ecological impacts of zebra mussels on native species, and what prevention methods are recommended to stop their spread?","answer":"Zebra mussels cause severe ecological damage by overwhelming native species. They outcompete other mussels for food and space, and can smother native mussels by latching onto them and preventing them from moving, feeding, or reproducing properly. To prevent their spread, it's crucial to clean, drain and dry watercraft, trailers, float planes and all water-related equipment when leaving a lake or river. Standing water should be emptied away from water bodies, and unwanted bait should be disposed of in the trash. This is especially important since zebra mussels can survive out of water for 7-30 days, and their microscopic larvae can be transported in small amounts of water in boats, bait buckets, and equipment.","context":["Aquatic invasive species (AIS) are an economic and environmental danger that threaten all water bodies, and the St. Croix River is no exception.\nInvasives are defined by federal law as species that are non-native to an area that are likely to cause harm to the economy, environment, or human health. The St. Croix River already hosts populations of species such as Asian carp, banded mystery snails, and curlyleaf pondweed, each of which was likely introduced by humans.\nWhen introduced to a foreign ecosystem, invasive species tend to flourish due to a lack of natural predators or disease controls, and often crowd out native species as they take over food sources and space.\nVery often, once established, there is very little to be done to eradicate them.\n“The problem is that these invasive species are so good at adapting to these environments that…we often don’t have the people or resources to stop them,” said Trevor Cyphers, a fisheries biologist with the U.S. Army Corps of Engineers.\nEducation and prevention, therefore, are often just as important, if not more so, than restoration of already-infested ecosystems.\nOne particular invader that gets a lot of attention is the zebra mussel (Dreissena polymorpha). This small, freshwater mussel typically grows only two inches long at most, yet can become a massive problem in lakes and slow-moving rivers due to its ability to create a lot of offspring very quickly.\nByron Karns, Acting Chief of Resource Management for the St. Croix National Scenic Riverway, a unit of the National Park System, said that the ecological damage of a zebra mussel infestation fits many of the common patterns of an invasive species. Once in an area, their populations explode, and they quickly begin to overwhelm niches normally occupied by native species of mussel.\nThe zebra mussels will outcompete other mussels for food and space, and sometimes smother the natives by latching onto them and preventing them from moving, feeding or reproducing properly.\nZebra mussels can also cause economic damage.\nA good example is Bass Lake in St. Croix County, Wisconsin. Bill Holmberg is a resident living on the edge of the lake, which was diagnosed as infested with zebra mussels in 2010. Ever since the invasion, the local residents have had to deal with the economic consequences.\nBiofouling, or the accumulation of zebra mussels on surfaces put in the water, is a notable example.\n“The biggest issue for owners like myself is they cover everything you put in the water,” Holmberg said.\nBoats are less of a problem because they’re usually removed from the water before the microscopic mussel larvae, or veligers, have a chance to latch on. Docks, however, are oftentimes heavily coated since they remain in the water for the entire season, and any water that gets trapped within the motor of a boat when it’s tipped back might also contain larvae waiting to anchor themselves and clog up the system.\nOther, less obvious problems have arisen from the infestation at Bass Lake. Residents with sprinkler systems that draw from the lake have to invest in special filters to keep the mussels out. Such a filter might cost nearly $100, but to go without is to risk losing the entire sprinkler system and spending $5,000 on a new one.\nThe sharp, dead shells can also be a safety hazard to anyone walking barefoot on a beach, and the pump system normally used to keep the water level of the lake from overtaking nearby properties cannot be turned on for fear of spreading the mussels to other water systems.\nZebra mussels have already made their way into the St. Croix River, but so far they have not progressed north of Stillwater. Nor can they, without help from humans.\nKarns said that due to the method by which zebra mussels reproduce (spewing veligers into the water and allowing them to drift with the currents), they typically can’t move upstream on their own. To move up a river or from lake to lake, zebra mussels hitch rides with boaters or fishermen transporting water. Livewells, bait buckets and boats filled with water from an infested water body might contain microscopic veligers, which have the potential to become an entirely new infestation if released into an otherwise clean environment.\nPrevention, Karns said, is the most effective way to address this problem.\nA zebra mussel infestation, once it takes root, cannot be eradicated from an entire lake or river. Manual removal is impossible due to the veligers. Chemical treatments like copper sulfate and Zequanox can be effective in small, contained systems, but run into problems like cost and the risk of harming native wildlife when applied to an entire lake or section of river.\n“There’s no such thing as a whole lake treatment,” Karns said.\nMultiple natural resource organizations have mobilized efforts to prevent further spread of zebra mussels and other aquatic invasive species.\nWisconsin’s Clean Boats, Clean Waters program is one such effort operating within the St. Croix River watershed. The program organizes groups of volunteers and employees to serve as watercraft inspectors, and posts them at boat launches to remind people to inspect their equipment for plants and animals and to not transport water or live bait.\nControlling aquatic invasive species as a whole, Karns said, is going to require a combination of methods. Restoration of already-infested areas is an important aspect of the process, but oftentimes more important is education and prevention.\n“I think in some respects it’s something you have to have buy-in from the public,” he said. “Education is critical, in the beginning especially.”\nThe Bass Lake Rehabilitation District, in response to its infestation, has been implementing methods to educate people and prevent the further spread of zebra mussels. The 2016 Bass Lake Management Plan included a section on AIS, and a DNR grant has allowed them to purchase a camera for their boat landing that not only monitors for boats with AIS, but also serves as a reminder for people to inspect their equipment.\n“Be diligent in the prevention,” Holmberg said. “If there’s a public access…don’t just educate the users, try to educate the public in general.”","Zebra mussels are spread to new waters largely by contaminated boats and water-related equipment. Once zebra mussels become established they cannot be gotten rid of, so it is extremely important to stop their spread.\nHow to Stop the Spread of Zebra Mussels\nAdult zebra mussels have a hard shell and attach to anything that sits in the water. This includes boats, canoes, trailers, float planes, buoys, fishing equipment, etc. They can survive out of water for 7 to 30 days depending on temperature and humidity.\nLarval zebra mussels, called veligers, passively move downstream by water movement and are invisible to the naked eye. Veligers rely on water to survive thus they can be inadvertently carried in small amounts of water transported by un-drained watercraft, water-based aircraft, off road vehicles and water-based equipment such as bait buckets.\nAlways clean, drain and dry your watercraft, trailer, float plane and all water-related equipment when leaving a lake or river. Drain or empty any standing water away from storm sewers, lakes and rivers. Dispose of any unwanted bait in the trash, not in the water.\nFor more details on how to stop the spread of aquatic invasive species go to the Manitoba Government's website at www.gov.mb.ca/stopthespread/ais/how.html\nWhy do we need to be concerned about zebra mussels and other aquatic invasive species?\nZebra mussels are a highly adaptable, non-native species that are a significant environmental and economic concern to Manitoba.\n- They disrupt ecosystems by eating food that other species rely on and changing the quality of the water\n- They can attach to and smother other mussels\n- They attach to boats and motors reducing performance and efficiency\n- The sharp shells attach to rocks, rafts and ladders and can cut swimmer's feet\n- They can clog intake and drainage pipes\nWhat are Invasive Species?\nOrganisms (animals, plants, parasites, viruses, etc.) not native to a region that when introduced, either intentionally or accidentally, out-compete native species for available resources. Invasive species become successful in their new environments due to their high reproductive rates and absence of native predators and diseases. Invasive species can have negative economic, social, environmental and human health implications.\nWhat are Aquatic Invasive Species (AIS)\nAn aquatic invasive species (AIS) can either live in freshwater or marine environments. Most of the species of concern to Manitoba, such as Zebra and Quagga mussels, Spiny Waterflea, Rusty Crayfish and Asian Carp are freshwater species. Manitoba currently has 15 aquatic invasive species. This number is small compared to the number of aquatic invasive species found in the Great Lakes (greater than 200) and Mississippi (greater than 120) drainage basins.\nZebra Mussels in Manitoba\nZebra mussels were confirmed in Lake Winnipeg in the fall of 2013 and the Red River and Cedar Lake 2015.\nZebra mussels are small (1-3cm), clam-like freshwater aquatic animals native to Eastern Europe and Western Asia. They are highly adaptable. The female lays about one million eggs per year. Their lifespan is 2-5 years. They can adhere to hard surfaces.\nZebra mussels have caused millions of dollars in damage to the Laurentian Great Lakes area and have cost the North American economy billions of dollars to control. They were first found in North America in 1988 in Lake St. Clair which straddles the US and Canadian border in Michigan and Ontario.\nDespite the successful eradication of zebra mussels in the four treated harbours in Lake Winnipeg in 2014, Zebra Mussels were found elsewhere in the south basin and are successfully reproducing. Lake Winnipeg is in the early stages of invasion by zebra mussels - eradication is no longer an option.\nThe Riding Mountain UNESCO World Biosphere Reserve (RMBR) has developed an \"Online information kit\" that includes key link to Manitoba aquatic invasive species information sources posted by other jurisdictions, which are also striving to stop the spread at Spread the Word, Not the Mussels Information Kit"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f30eb84a-f6b9-490e-8ded-93c658a09c4a>","<urn:uuid:5aabe6d9-bc6a-4aab-87fc-52850fe4f748>"],"error":null}
{"question":"How do traditional Japanese spinning tops combine storytelling and craftsmanship, and what potential risks exist with their modern green tea extract counterparts?","answer":"Traditional Japanese spinning tops were artistic storytelling devices that combined craftsmanship with cultural commentary. They featured carved wooden figures depicting scenes from folklore and theatre, with characters that could dance, transform, or fight when spun. These tops were more than toys - they were performance pieces that could subtly mock authority figures through creative designs. In contrast, while green tea extracts containing EGCG have shown health benefits, there are potential risks with high doses. Laboratory studies have shown that concentrated green tea extracts can be toxic to rat liver cells at high concentrations, with EGCG identified as a key constituent responsible for this effect.","context":["Japanese spinning tops tell stories, aim for playfulness\nIt once was thought that the Japanese tradition of carving Edo-style spinning tops had been lost. It turned out it had just gone on the road to northern Japan – and some of it ended up in America as well.\nWhile they’re called tops, these go beyond simple spinning disks. Many are more like carved, wooden figures, and might depict scenes with characters that dance, transform or fight. On one, an ogre disguised as a priest bangs a gong when you spin his hat; on another, two discs illustrated with a dog and a robber chase each other around. Figures from folklore and theatre are represented, and whimsical scenes as well, like two frogs sumo wrestling.\nMore than just pla ythings, the tops were traditionally used for street performances, and involve original creations, not just repetitive copies.\n“They have a very varied history across different social groups and classes,” says Paula R. Curtis, a Ph.D. candidate in history at the University of Michigan. “Part of it is the artisans putting themselves into the work, interpreting their experiences and cultural background through these tops.”\nOne such artisan is Michiaki Hiroi. His was the last family making tops in Tokyo when they moved to Sendai in the north of Japan after the Second World War. His father made a living making the woodcrafts of that region, and Hiroi initially followed in his footsteps. Then one day a collector who knew about the Edo style came to the shop and discovered who he was. “This person said, oh my God, I’ve been looking for you forever,” says Curtis.\nHiroi was inspired to begin making the tops and taking on apprentices. While the stereotype of craft apprenticeships in Japan is that they’re hard to break into and not welcoming to women and foreigners, Hiroi didn’t discriminate. One of his apprentices was Janell Landis, an American who came to Japan as a missionary and taught at a university in Sendai for 30 years.\nWhen she became his apprentice in 1982, Hiroi encouraged her to bring her own experiences and culture to the craft. Where his tops might represent a figure from a Noh drama, hers included Huck Finn, Tom Sawyer and Cinderella. Instead of Japanese folklore, they reflected the stories important to her, such as a Christmas tree and a scene of the manger with baby Jesus.\nIf Christmas is fair game as a subject, then what makes a top Edo-style? Curtis, project manager of an oral history website about Landis and Hiroi, says that Hiroi’s answer focuses on the tops’ spirit: “He’s very insistent on that when he talks about them: When people look at them, it brings them a sense of joy and playfulness,”\nThat response might seem evasive or frustrating – aren’t you supposed to be able to identify a school of arts or crafts from how it looks? Not necessarily, according to Tamara Joy, curator of the Morikami Museum in Delray Beach, Florida. For Japanese crafts, a style is often distinguished by where it comes from and who is teaching it, rather than certain patterns or techniques.\nBut the term Edo also has deep cultural and historical resonance that is reflected in these tops, says Joy, whose museum owns a collection of Hiroi’s work that Landis brought back when she returned to the U.S. in 1995.\nEdo is the old name for Tokyo, but it’s also the name of the era from 1603-1868, a period that saw the development of a new middle class with more free time and spending money. This resulted in an explosion of the arts and culture. These tops are part of that bigger picture, Joy says.\nOften, they contained subversive commentary about the ruling classes. “There was a lot of sticking it to the people who were in charge,” says Joy. This had to be subtle: perhaps a play that used historical characters with parallels to the present, or a performance with elaborate tops that actually told a story.\nOne of Joy’s favourites in the collection represents a common figure of fun, the spear-bearer who led samurai processions. Paintings and illustrations mock his self-importance. “His chest is all puffed up, and he’s got a smug look on his face,” says Joy. “You couldn’t directly make fun of the samurai, but you could poke at them and their egos through this character.”\nOn Hiroi’s top, the figure’s spear moves around clumsily when his head is spun. “You could see the same character in a half-million-dollar painting,” Joy says, “and he made his way into a top.”","|| Copyright © 2008 Jing Tea Sunnyvale CA |\nEGCG Health Benefits, Detailed Studies and Discussion\nAvailability of EGCG\nYou can obtain EGCG through drinking green or wulong tea or taking a tea extract supplement which\nlists the amount of EGCG per capsule. On average, a cup of green or wulong tea will have roughly 100\nmg of EGCG per 8 ounces. The amount of EGCG present in tea varies with different brands of the\nproduct, the region the tea is grown, amount of brewing time, age of the tea leaves, etc. A cup of green\ntea or wulong tea may have as much as 20 mg of caffeine.\nBenefit of EGCG\nThere have been few human studies directly evaluating the tea extract EGCG. However, laboratory and\nanimal studies are plentiful. In lab studies, EGCG has been found to be a potent anti-antioxidant and\nto have anti-cancer abilities. In animal studies, EGCG has been found to potentially be helpful in\nblood sugar control. In human studies, EGCG has been shown to be helpful in reducing infection\nfrom the HPV virus in the cervix and the risk for prostate cancer.\nEGCG for Cancer\nEGCG induces apoptosis in human cancer cell lines. Apoptosis means the death of a cell, and in the\ncase of a cancer cell, apoptosis is a good thing. In test tubes, EGCG thwarts the growth of several\ncancer cells including breast, prostate, lungs, ovaries, and liver.\nDr. Saverio Bettuzzi from the University of Parma in Italy gave men 600 mg a day of concentrated green\ntea catechins, containing 300 mg EGCG, or a placebo for a year. All the men had premalignant\nprostate cancer, putting them at risk of progression to full cancer. After a year, only one man (3%) in\nthe group on green tea capsules developed prostate cancer compared with 9 men (28%) on placebo.\nEGCG as an Antioxidant\nEGCG's anti-oxidant action protects cells from lipid peroxidation and DNA damage induced by reactive\nEGCG and Leukemia\nA component of green or wulong tea known as epigallocatechin-3-gallate (EGCG) [epi-gallo-cat-ekin-\n3-gal-ate] helps kill leukemia cells by interrupting the communication signals they need to survive.\nEGCG and endothelial function in regards to Heart Disease\nEGCG Supplementation Reverses Endothelial Dysfunction in Patients with Coronary Artery Disease.\nJ Am Coll Nutr. 2007 Apr;26(2):95-102. Widlansky ME, Hamburg NM, Anter E, Holbrook M, Kahn DF,\nWe hypothesized that dietary supplementation with epigallocatechin gallate ( EGCG ), a major\ncatechin in tea, would improve endothelial function in humans. We examined the effects of EGCG on\nendothelial function in a double blind, placebo-controlled, crossover design study. We measured\nbrachial artery flow-mediated dilation by vascular ultrasound at six time points: prior to treatment with\nEGCG or placebo, two hours after an initial dose of EGCG (300 mg) or placebo, and after two weeks\nof treatment with EGCG (150 mg twice daily) or placebo. The order of treatments (EGCG or placebo)\nwas randomized and there was a one-week washout period between treatments. RESULTS: A total of\n42 subjects completed the study, and brachial artery flow-mediated dilation improved from 7 to 8.6\ntreatment with the final measurements made approximately 14 hours after the last dose. Placebo\ntreatment had no significant effect, and there were no changes in reactive hyperemia or the response\nto sublingual nitroglycerin. The changes in vascular function paralleled plasma EGCG concentrations,\nwhich increased from 2.6 to 92 ng/ml after acute EGCG, but were unchanged from baseline after two\nweeks of treatment (3.4). CONCLUSION: EGCG acutely improves endothelial function in humans with\ncoronary artery disease, and may account for a portion of the beneficial effects of flavonoid-rich food\non endothelial function.\nMy comments: I am not sure how to interpret this study. It appearsthat there is improvement initially\nbut then in the long run no overallmajor change was noted. Unless I am misinterpreting this study.\nAnystatisticians out there who can help? My friend Lou Mancano, M.D.,offered this opinion: \"While I\napplaud their efforts, I think oneshould be cautious before we can draw conclusions from this EGCG\nstudy.Here are some limitations: One major one is that there were no clinicaloutcomes measured;\nsecond, there was an assumption that a response in abrachial artery would be replicated in a\ncoronary artery (we know thatdifferent parts of our anatomy respond differently to supplements\nandmedications); third, we shouldn't automatically conclude that higherblood flow is necessarily\nbetter (could flow velocity and dilatationinduce plaque rupture, as we indirectly discovered with\nsublingualnifedipine 10 or 15 years ago?); fourth, we cannot make any soundrecommendations on\nthe optimal dose or frequency of EGCG use. I’drecommend waiting for more studies…especially\nsomething with measurableclinical outcomes. About the flow rate you asked about. Either thatlevel of\nchange is insignificant, or if it is, the fact that it was notsustained at 2 weeks renders it insignificant\n(possibly the body makesan adjustment - we all know our bodies (nature) is always smarter thanthe\nsubstances we place within it).\"\nEGCG and Cervical Dysplasia with the HPV Virus\nEGCG, when given to women with cervical dysplasia at a dose of 200 mg for a period of 12 weeks,\nshowed a positive response. EGCG may be a potential therapy regimen for patients with HPV infected\nProtective effects of green tea extracts (polyphenon E and EGCG) on human cervical lesions.\nEur J Cancer Prev. 2003 Oct;12(5):383-90. Department of Obstetrics and Gynaecology, College of\nMedicine, The Catholic University of Korea, Seoul, Korea.\nWe investigated clinical efficacy of green tea extracts (polyphenon E; poly E and (-)-epigallocatechin-3-\ngallate [ EGCG ]) delivered in a form of ointment or capsule in patients with human papilloma virus\n(HPV) infected cervical lesions. Fifty-one patients with cervical lesions (chronic cervicitis, mild\ndysplasia, moderate dysplasia and severe dysplasia) were divided into four groups, as compared\nwith 39 untreated patients as a control. Poly E ointment was applied locally to 27 patients twice a\nweek. For oral delivery, a 200 mg of poly E or EGCG capsule was taken orally every day for eight to 12\nweeks. In the study, 20 out of 27 patients (74%) under poly E ointment therapy showed a response.\nSix out of eight patients under poly E ointment plus poly E capsule therapy (75%) showed a response,\nand three out of six patients (50%) under poly E capsule therapy showed a response. Six out of 10\npatients (60%) under EGCG capsule therapy showed a response. Overall, a 69% response rate\n(35/51) was noted for treatment with green tea extracts, as compared with a 10% response rate (4/39)\nin untreated controls (P<0.05). Thus, the data collected here demonstrated that green tea extracts in a\nform of ointment and capsule are effective for treating cervical lesions, suggesting that green tea\nextracts can be a potential therapy regimen for patients with HPV infected cervical lesions.\nEGCG and Weight Loss\nResearch with EGCG and weigh loss in humans is still quite early, and at this time it appears that\ngreen tea and EGCG may help slightly with weight loss when used in high amounts, but long term\nbenefit versus potential risk of high EGCG consumption has yet to be determined.\nEGCG and Alzheimer's Disease\nIn rodent studies, EGCG is able to prevent the deposition of amyloid in brain tissue. Hence, it is\npossible that drinking green tea or taking an EGCG supplement could potentially reduce the risk for\nGreen tea epigallocatechin-3-gallate (EGCG) modulates amyloid precursor protein cleavage and\nreduces cerebral amyloidosis in Alzheimer transgenic mice.\nJ Neurosci. 2005 Sep 21;25(38):8807-14. Silver Child Development Center, Department of Psychiatry\nand Behavioral Medicine, University of South Florida, Tampa, Florida\nAlzheimer's disease (AD) is a progressive neurodegenerative disorder pathologically characterized by\ndeposition of beta-amyloid (Abeta) peptides as senile plaques in the brain. Recent studies suggest\nthat green tea flavonoids may be used for the prevention and treatment of a variety of\nneurodegenerative diseases. Here, we report that EGCG, the main polyphenolic constituent of green\ntea, reduces Abeta generation in both murine neuron-like cells (N2a) transfected with the human\n\"Swedish\" mutant amyloid precursor protein (APP) and in primary neurons derived from Swedish\nmutant APP-overexpressing mice (Tg APPsw line 2576). In concert with these observations, we find\nthat EGCG markedly promotes cleavage of the alpha-C-terminal fragment of APP and elevates the N-\nterminal APP cleavage product, soluble APP-alpha. These cleavage events are associated with\nelevated alpha-secretase activity and enhanced hydrolysis of tumor necrosis factor alpha-converting\nenzyme, a primary candidate alpha-secretase. As a validation of these findings in vivo, we treated Tg\nAPPsw transgenic mice overproducing Abeta with EGCG and found decreased Abeta levels and\nplaques associated with promotion of the nonamyloidogenic alpha-secretase proteolytic pathway.\nThese data raise the possibility that EGCG dietary supplementation may provide effective prophylaxis\nEGCG and Diabetes\nA rodent study shows EGCG to have promise as a remedy for diabetes.Epigallocatechin gallate\n(EGCG) supplementation alleviates diabetes in rodents.\nJ Nutr. 2006 Oct;136(10):2512-8. DSM Nutritional Products Ltd, Department of Human Nutrition and\nHealth, CH-4002 Basel, Switzerland.\nThis study investigated the antidiabetic effects of the most abundant green tea catechin,\nepigallocatechin gallate ( EGCG, TEAVIGO ), in rodent models of type 2 diabetes mellitus and H4IIE\nrat hepatoma cells. We assessed glucose and insulin tolerance in db/db mice and ZDF rats after they\ningested EGCG. Using gene microarray and real-time quantitative RT-PCR we investigated the effect\nof EGCG on gene expression in H4IIE rat hepatoma cells as well as in liver and adipose tissue of\ndb/db mice. EGCG improved oral glucose tolerance and blood glucose in food-deprived rats in a\ndose-dependent manner. Plasma concentrations of triacylglycerol were reduced and glucose-\nstimulated insulin secretion was enhanced. In H4IIE cells, EGCG downregulated genes involved in\ngluconeogenesis and the synthesis of fatty acids, triacylgycerol, and cholesterol. EGCG decreased\nthe mRNA expression of phosphoenolpyruvate carboxykinase in H4IIE cells as well as in liver and\nadipose tissue of db/db mice. Glucokinase mRNA expression was upregulated in the liver of db/db\nmice in a dose-dependent manner. This study shows that EGCG beneficially modifies glucose and\nlipid metabolism in H4IIE cells and markedly enhances glucose tolerance in diabetic rodents. Dietary\nsupplementation with EGCG could potentially contribute to nutritional strategies for the prevention and\ntreatment of type 2 diabetes mellitus.\nEGCG may have health benefit as a nutritional supplement for cancer, heart disease, certain viral\ninfections, and neurodegenerative conditions, and perhaps weight loss. However, at this time we\ndon't know the long term safety of taking a high dose EGCG supplement by itself. It is clear from\nepidemiological studies that long term drinking of green tea is safe and healthy as demonstrated in\nstudies with Japanese green tea drinkers.\nDear Dr. Sahelian,First of all, I would like to thank you for providing outstanding information about\nherbs and supplements. I rely on your research every day. I root around in Pubmed a lot, but in doing\nso I am very aware of the disadvantages of lacking a medical education. In digging up material on\ntheanine, I came across the abstract appended below. Since I take 350mg of EGCG each day, my\ninterest was aroused, to put it mildly. What is your take on this study?\nToxicity of green tea extracts and their constituents in rat hepatocytes in primary culture.\nFood Chemistry and Environmental Toxicology, University of Kaiserslautern, Erwin-Schroedinger-\nStrasse 52, D-67663 Kaiserslautern, Germany.\nRecent reports on sporadic cases of liver disorders (acute hepatitis, icterus, hepatocellular necrosis)\nafter ingestion of dietary supplements based on hydro-alcoholic extracts from green tea leaves led to\nrestrictions of the marketing of such products in certain countries of the EU. Since green tea is\nconsidered to exert a number of beneficial health effects, and, therefore, green tea products are widely\nused as dietary supplements, we were interested in the possible mechanism of hepatotoxicity of\ngreen tea extracts and in the components involved in such effects. Seven hours after seeding on\ncollagen, rat hepatocytes in primary culture were treated with various hydro-alcoholic green tea\nextracts (two different native 80% ethanolic dry extracts and an 80% ethanolic dry extract cleared from\nlipophilic compounds). Cells were washed, and reduction of resazurin, used as a viability parameter\nmonitoring intact mitochondrial function, was determined. It was found that all seven green tea\nextracts examined enhanced resazurin reduction significantly at a concentration range of 100-500\nmicrog/ml medium, while a significant decrease was observed at 1-3mg/ml medium. Decreased\nlevels were concomitant with abundant necrosis as observed by microscopic inspection of the\ncultures and with increased leakage of lactate dehydrogenase activity from the cells. In a separate\nseries of experiments, the green tea constituents (-)-epicatechin, (-)-epigallocatechin-3-gallate,\ncaffeine and theanine were tested at concentrations reflecting their levels in a typical green tea extract.\nSynthetic (+)-epigallocatechin (200 microM) was used for comparison. Cytotoxicity was found with\nEGCG only. The concomitant addition of 0.25 mM ascorbate/0.05 mM alpha-tocopherol had no\ninfluence on cytotoxicity. In conclusion, our results suggest that high concentrations of green tea\nextract can exert acute toxicity in rat liver cells. EGCG seems to be a key constituent responsible for\nthis effect. The relatively low bioavailability of catechins reported after oral exposure to green tea\nargues, however, against a causal role of these constituents in the reported liver disorders.\nA. Dr. Sahelian responds: It is very difficult to interpret lab studies done in a test tube versus what\nwould occur if the same supplement were taken orally, particularly with a meal. I am not a big fan of\ntaking large doses of isolated substances from herbs, such as a high amount of EGCG, preferring to\ntake the whole powder or a weakly concentrated extract. Many people think the higher the dose of a\nsubstance they supplement, the healthier they will be, but that may not always be the case, and often\nis not the case. So, for the time being, I don't see the need to take a high dose of EGCG unless\nperhaps under medically supervised treatment for a health condition.\n\"It is refreshing to deal with\nsomeone who is both professional"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:314dedae-7b8f-4e7a-a14c-0987fafb723a>","<urn:uuid:a9d0cbca-787e-49ff-9d29-8e6c4d0083f8>"],"error":null}
{"question":"Entre el cerebelo y la amígdala, ¿cuál afecta más directamente al aprendizaje motor y la memoria emocional? 🤔","answer":"The cerebellum and amygdala serve different learning functions: the cerebellum controls the precision of fine movements and motor skills but does not initiate movement, while the amygdala is crucial for forming and storing memories associated with emotional events. The amygdala, particularly its basolateral nuclei, mediates the effects of emotional arousal on memory strength, while the cerebellum specifically fine-tunes motor skills and coordinates movement, balance, and posture.","context":["The amygdalae (Latin, also corpus amygdaloideum, singular amygdala, from Greek αμυγδαλή, amygdalē, 'almond', 'tonsil', listed in the Gray's Anatomy as the nucleus amygdalæ) are almond-shaped groups of neurons located deep within the medial temporal lobes of the brain in complex vertebrates, including humans. Shown in research to perform a primary role in the processing and memory of emotional reactions, the amygdalae are considered part of the limbic system.\nAnatomically, the amygdala and more particularly, its centromedial nucleus, may be considered as a part of the basal ganglia.\nThe cortical nucleus is involved in the sense of smell and pheromone-processing. It receives input from the olfactory bulb and olfactory cortex. The lateral amygdalae, which send impulses to the rest of the basolateral complexes and to the centromedial nuclei, receive input from the sensory systems. The centromedial nuclei are the main outputs for the basolateral complexes, and are involved in emotional arousal in rats and cats.\nIn complex vertebrates, including humans, the amygdalae perform primary roles in the formation and storage of memories associated with emotional events. Research indicates that, during fear conditioning, sensory stimuli reach the basolateral complexes of the amygdalae, particularly the lateral nuclei, where they form associations with memories of the stimuli. The association between stimuli and the aversive events they predict may be mediated by long-term potentiation, a lingering potential for affected synapses to react more readily.\nMemories of emotional experiences imprinted in reactions of synapses in the lateral nuclei elicit fear behavior through connections with the central nucleus of the amygdalae. The central nuclei are involved in the genesis of many fear responses, including freezing (immobility), tachycardia (rapid heartbeat), increased respiration, and stress-hormone release. Damage to the amygdalae impairs both the acquisition and expression of Pavlovian fear conditioning, a form of classical conditioning of emotional responses.\nThe amygdalae are also involved in appetitive (positive) conditioning. It seems that distinct neurons respond to positive and negative stimuli, but there is no clustering of these distinct neurons into clear anatomical nuclei.\nDifferent nuclei within the amygdala have different functions in appetitive conditioning.\nDuring the consolidation period, the memory can be modulated. In particular, it appears that emotional arousal following the learning event influences the strength of the subsequent memory for that event. Greater emotional arousal following a learning event enhances a person's retention of that event. Experiments have shown that administration of stress hormones to mice immediately after they learn something enhances their retention when they are tested two days later.\nThe amygdalae, especially the basolateral nuclei, are involved in mediating the effects of emotional arousal on the strength of the memory for the event, as shown by many laboratories including that of James McGaugh. These laboratories have trained animals on a variety of learning tasks and found that drugs injected into the amygdala after training affect the animals' subsequent retention of the task. These tasks include basic classical conditioning tasks such as inhibitory avoidance, where a rat learns to associate a mild footshock with a particular compartment of an apparatus, and more complex tasks such as spatial or cued water maze, where a rat learns to swim to a platform to escape the water. If a drug that activates the amygdalae is injected into the amygdalae, the animals had better memory for the training in the task. If a drug that inactivates the amygdalae is injected, the animals had impaired memory for the task.\nDespite the importance of the amygdalae in modulating memory consolidation, however, learning can occur without it, though such learning appears to be impaired, as in fear conditioning impairments following amygdalar damage.\nEvidence from work with humans indicates that the amygdala plays a similar role. Amygdala activity at the time of encoding information correlates with retention for that information. However, this correlation depends on the relative \"emotionalness\" of the information. More emotionally-arousing information increases amygdalar activity, and that activity correlates with retention.\nWith advances in neuroimaging technology such as MRI, neuroscientists have made significant findings concerning the amygdala in the human brain. Consensus of data shows the amygdala has a substantial role in mental states, and is related to many psychological disorders. In a 2003 study, subjects with Borderline personality disorder showed significantly greater left amygdala activity than normal control subjects. Some borderline patients even had difficulties classifying neutral faces or saw them as threatening. In 2006, researchers observed hyperactivity in the amygdala when patients were shown threatening faces or confronted with frightening situations. Patients with more severe social phobia showed a correlation with increased response in the amygdala. Similarly, depressed patients showed exaggerated left amygdala activity when interpreting emotions for all faces, and especially for fearful faces. Interestingly, this hyperactivity was normalized when patients went on antidepressants. By contrast, the amygdala has been observed to relate differently in people with Bipolar Disorder. A 2003 study found that adult and adolescent bipolar patients tended to have considerably smaller amygdala volumes and somewhat smaller hippocampal volumes. Many studies have focused on the connections between the amygdala and autism.\nStudies in 2004 and 2006 showed that normal subjects exposed to images of frightened faces or faces of people from another race will show increased activity of the amygdala, even if that exposure is subliminal.\nRecent research suggests that parasites, in particular toxoplasma, form cysts in the brain, often taking up residence in the amygdala. This may provide clues as to how specific parasites manipulate behavior and may contribute to the development of disorders, including paranoia.","Our brains are arguably the single most complex system known to man. Together with the spinal cord, the brain makes up the central nervous system, which controls every aspect of our lives; from breathing to solving math problems.\n3 main sections comprise the brain: the forebrain, midbrain and the hindbrain. Each of these sections works together as well as independently to ensure we stay alive, know when we are hungry and help up stand up.\nWe will highlight these sections and explain their functions.\nThe forebrain is the largest part of the brain. It further divides into the cerebrum, thalamus and the hypothalamus. This section of our brain controls higher brain functions such as thought and action.\nAlso called the cerebral cortex, when you think about what a brain looks like, you usually picture the cerebrum. The cerebral cortex is again, further divided into 4 parts, or lobes:\n- Frontal lobe – which gives us reasoning, problem-solving, emotions, speech and problem solving/planning.\n- Occipital lobe – This gives us visual processing.\n- Parietal lobe – This lobe controls our motor skills so we can move, equilibrium and our perception of external stimuli.\n- Temporal lobe – The temporal lobe is associated with hearing, memories and speech.\nCortical folding (the wrinkles and folds in the gray matter of these lobes) allow us to have thoughts, be able to talk and understand language, use fine motor skills and use reason and logic. The more folds, the larger the available surface area to have and use these functions.\nThe cerebrum divides into 2 halves: the left and right hemispheres. The right hemisphere controls our creative and artistic abilities. Without this hemisphere, we wouldn’t have art, music, dance or poetry.\nThe left hemisphere controls reason and logic. This allows us to have reason, make decisions and solve problems. Without this hemisphere, we wouldn’t be able to recall facts and figures or understand mathematics and science.\nThe thalamus is the sensory never center of our brain. It receives sensory input from almost every part of our body. These signals travel through our nervous system to the thalamus which then directs the signal to the correct section of the cerebral cortex.\nThe thalamus allows us to know when we touch something and how it feels, pain and pleasure, as well as temperature.\nSmell is the only sensory stimulus that is not passed through the thalamus.\nWithout the thalamus, we wouldn’t know if a stove was hot or if ice was cold. We wouldn’t be able to understand what our eyes see or what our ears hear.\nThe hypothalamus is mainly involved in motivational behaviors. We understand that we are hungry, thirsty or tired because of the hypothalamus. It controls our circadian rhythm (sleep cycles) as well as emotions.\nThe hypothalamus houses the pituitary gland. With this pituitary gland and the hypothalamus, the body produces hormones. These hormones control emotions and sex drive, our mood and temperature regulation.\nThe hypothalamus is responsible for regulating our body temperature and creating homeostasis. This homeostatic pathway allows us to survive by making small changes that prevent us from freezing or starving. Regardless of changes to external stimulus such as weather, external temperature or diet.\nThe midbrain is made up of the tectum and the tegmentum. Together these work with the thalamus and hypothalamus to control alertness, hearing, vision, temperature control, emotion and sleep cycles.\nThe tectum has two primary functions: processing visual stimuli and auditory processing stimuli.\nThe tectum controls eye movement and processes the optical signals, routing them to the cerebral cortex for interpretation. It also processes the signals sent in by our ears and allows us to understand and recognize auditory signals into sounds and words.\nThe tegmentum works in conjunction with the hypothalamus to maintain homeostasis and reflexive pathways. These paths allow us to regulate our body temperature and redirect changes in our body to ensure we survive.\nAlong the reflexive pathways, the tegmentum ensures we do not have any unwanted muscle movements by sending inhibitory signals to the thalamus.\nThe hindbrain is made up of 3 main sections: cerebellum, pons and the medulla oblongata. The primary function of the hindbrain is to regulate and control involuntary actions such as breathing and heartbeat.\nThe cerebellum is responsible for the coordination of movement, balance and posture. The cerebellum takes in signals sent from the spinal cord and allows us to fine tune motor skills. We can grasp and sit and stand because of the cerebellum.\nThe cerebellum does not initiate movement but instead controls the precision of fine movements, compound movements and timing.\nThe pons is a signal super-highway. It connects the upper and lower portions of the brain and acts as a traffic cop for the incoming and outgoing signals.\nAside from its main attribute of brain communication, it also allows us to dream. The pons is responsible for our rapid eye movement (REM) sleep cycle, where we subconsciously dream.\nWithout the pons, our brains wouldn’t function as no signals would be sent or received.\nThe medulla oblongata (also called simply the medulla) is responsible for the clear majority of involuntary functions.\nThe medulla helps signals pass to and from the spinal cord and the thalamus. Moreover, it oversees our respiration, heart rhythm, blood vessel function, digestion, and even sneezing and swallowing.\nWithout the medulla oblongata, we wouldn’t live. We wouldn’t be able to breathe without thinking about it and our hearts would stop when we went to sleep.\nIt Just Works\nEvery section of the brain has a job to do. While each section is an independent work horse allowing us to do everything we need to do as well as understanding the world around us, they also work together.\nOne section couldn’t function without the other. Working in tandem as well as individually allows us to function, reason, think, learn, eat, digest and breathe. Our brain is an amazing and complex system. We couldn’t live without it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:744f6c77-532c-4ff3-91e7-7caef9705502>","<urn:uuid:f628aadf-94f8-4f25-8132-c5bbcb2c7bc4>"],"error":null}
{"question":"How do the approaches to addressing anxiety over anxiety compare with the techniques for unraveling cognitive distortions?","answer":"Anxiety over anxiety and unraveling cognitive distortions use different therapeutic approaches. Anxiety over anxiety is addressed through practical techniques like mental rehearsal, practicing with friends for feedback, and developing acceptance of human foibles and flaws. In contrast, addressing cognitive distortions involves a more analytical process of first identifying harmful automatic thoughts, understanding how these distortions took root, and then actively challenging these inaccurate beliefs. While anxiety over anxiety treatment focuses on behavioral practices and acceptance-based strategies, cognitive distortion work emphasizes the systematic examination and restructuring of thought patterns that lead to inaccurate views of reality.","context":["What can you do when your anxieties and fears greatly trouble you? You can hope for a miracle. You can hope to grow out of them. You can hope that by avoiding them they will go away. Will you pin the quality of your life on these hopes?\nWhat might the founder of rational emotive behavior therapy (REBT), Albert Ellis, say? Ellis might tell you that he once had numerous anxieties and fears, such as fearing attractive young women and speaking before groups. He overcame his anxieties and fears by working hard using cognitive techniques from philosophy and behavioral methods from psychology. You can do the same.\nLet’s explore Ellis’ three forms of anxiety: ego anxiety, discomfort anxiety, and anxiety over anxiety. The three forms normally blend and underpin specific anxieties, such as agoraphobia (fear of panicking in certain places), and social anxieties, such as public speaking anxiety. Diminishing one core anxiety normally results in lowering the intensity of the others.\nLet’s say that you are anxious about giving a speech----this can be any performance situation, such as taking a test, entering a dance contest, sketching a scene, or running a race. How might you address this anxiety when it is accompanied by the three forms of anxiety?\nEgo anxiety is mainly about you. The threat is to your image or self-worth. Knowing how you rate yourself is a key to understanding ego anxiety. For example, do you impose unrealistic requirements on yourself? If you believe that you must give a speech that rivals the best ever, and you absolutely need others to acknowledge you in positive ways, you have put yourself in emotional peril. (If you normally think using anxiety-arousing requirement terms, such as must and need, is it possible for you to cut yourself some slack in this area?) You can try another way.\n1. Consider whether you make yourself anxious about not doing well enough. By catastrophizing (evaluating a future situation by blowing possible negative consequences out of proportion) you set yourself on a path of worry and rumination about dire possibilities. You may simultaneously awfulize and amplify your tension (awfulizing = making a situation worse than it is—sometimes worse than bad—by using dramatic language, such as terrible or unbearable). Rather than ruminate about possible failures, work to do better. Can you act as if you could prepare for the talk without predicting the worse and awfulizing about consequences that have not happened? Your awareness of this alternative is a start in the direction of ridding yourself of ego anxiety.\n2. Examine how you judge your worth. Do you make your worth depend on meeting lofty standards? Do you spy on yourself and rate and judge how well you are preparing? Recognize this contingency worth issue and you are on your way to separating your global worth from your speech performances. If you are more than just an anticipated poor performance, you may feel like a more flexible you.\n3. Take an inventory of your “self”. Are you more than a public speaking performance? If you are not sure, compile a list of all your positive attributes, learnings, and experiences. You may quickly see that there is a big difference between judging your worth and judging your variable performances. You may come to see that you are neither a success nor a failure as a speaker nor a success or a failure as a person. For example, if you were to give 100 talks you’d do better with some presentations than others. You are likely to improve more by rating your performances than by rating yourself.\nDiscomfort anxiety is an exaggerated threat to your future emotional stability. If you expect to feel afraid, and believe that you can’t bear feeling tense while giving your talk, you are probably catastrophizing. You see yourself in a vulnerable, perhaps terrifying, position. You may distress yourself by describing yourself as emotionally falling to pieces (awfulizing). To avoid falling to pieces (whatever that means), you may avoid preparing for your presentation until you feel comfortable. That procrastination plan rarely turns out well. You can try another way.\n1. Explore the advantages of avoiding discomfort versus living through discomfort. The advantage of avoiding discomfort is quick relief. That’s a specious reward. Here’s the downside of a specious reward: (1) You risk reinforcing tension avoidance. (2)You set yourself up to repeat this cycle. Consider an alternative. If you allow yourself to feel discomfort, you may discover that you don’t need to fear the feeling. That’s an advantage. You’ll have one less anxiety. As a bonus you may simultaneously decrease secondary procrastination. This procrastination follows an emotional problem, such as anxiety. For example, you secondarily procrastinate when you put off dealing with your anxiety or directly facing your inhibiting fears. By productively facing your anxieties and fears you avoid this pitfall.\n2. Teach yourself to describe how you feel without dramatizing the meaning of the emotion. For example, “I don’t like feeling anxious” has a different meaning than “I can’t stand feeling anxious.” By reframing the issue with realistic toned-down language, you can avoid viewing yourself as inadequate and projecting this conclusion into the future.\n3. Put yourself into challenging situations. Make learning to tolerate tension part of engaging in useful but uncomfortable situations. By behaviorally showing yourself that you can stand tension, you are less likely to rocket normal tensions into negative emotions that you may later describe as terrible.\nAnxiety over anxiety can be a major tension amplifier—perhaps more intense than your initial anxiety. You feel anxious about giving a speech. This anxiety is distracting. You feel anxious over feeling anxious and amplify your anxiety into something worse than the anxiety that you first experienced. You can try another way.\n1. Practice mental rehearsal. Imagine yourself presenting before an audience, doing well enough, and later thinking how you can improve the next time that you give a talk.\n2. Practice your talk with a few friends. Get feedback about how you might improve. Incorporate the feedback that works best for you and repeat the performance for practice.\n3. Practice acceptance. If you were to ask Albert Ellis for one more tip, the odds are he would say to accept yourself in spite of your human foibles and flaws. Accept others and life. Acceptance is, then, taking life, as it is, including what you definitely don’t like, while at the same time refusing to upset yourself about what you can’t change. By working to develop an acceptant attitude, you are working at defusing the three core anxieties that Ellis describes.\nFor more guidance on how successfully to combat anxiety, click on: The Cognitive Behavioral Workbook for Anxiety (Second Edition)\nThis blog is part of a series to celebrate the 100th and 101st year anniversaries of Dr. Albert Ellis’ birth. Ellis is the founder of rational emotive behavioral therapy and the grandfather of cognitive-behavior therapy.\nAlbert Ellis Revisited (Carlson & Knaus 2013) is the Albert Ellis Tribute Book Series centennial book. The publisher, Routledge, offers a 20% discount on the book. Control click on this link: Albert Ellis Revisited. Type the code Ellis for the discount. The book qualifies for free shipping and handling. Bill Knaus’ royalties from this book go directly to the Denan Project charity. When you buy the book, you are helping yourself by learning ways to live life fully, and you are helping bring irrigation, crops, and health care to destitute areas of the world.\nFor other articles in this centennial (and beyond) Albert Ellis tribute blog series, cut and paste any of the below http links to your server's http request header:\nFreedom from Harmful, Negative, Thinking: http://www.psychologytoday.com/blog/science-and-sensibility/201412/freedom-harmful-negative-thinking\nSix Calming Tips for Parenting Teens: http://www.psychologytoday.com/blog/science-and-sensibility/201410/six-c...\n© Dr. Bill Knaus\nAll Rights Reserved","What is CBT?\nThis simple idea is that our unique patterns of thinking, feeling, and behaving are significant factors in our experiences, both good and bad. Since these patterns have such a significant impact on our experiences, it follows that altering these patterns can change our experiences (Martin, 2016).\nCBT aims to change our thought patterns, the beliefs we may or may not know we hold, our attitudes, and ultimately our behavior in order to help us face our difficulties and more effectively strive towards our goals.\nThe founder of CBT is a therapists named Aaron Beck, a man who practiced psychoanalysis until he noticed the prevalence of internal dialogues in his clients, and realized how strong the link between thoughts and feelings can be. He altered the therapy he practiced in order to help his clients identify, understand, and deal with the automatic, emotion-filled thoughts that arise throughout the day.\nBeck found that a combination of cognitive therapy and behavioral techniques produced the best results for his clients. In describing and honing this new therapy, Beck laid the foundations of the most popular and influential form of therapy of the last 50 years.\nThis form of therapy is not designed for lifelong participation, but focuses more on helping clients meet their goals in the near future. Most CBT treatment regimens last from five to ten months, with one 50 to 60 minute session per week.\nCBT is a hands-on approach that requires both the therapist and the client to be invested in the process and willing to actively participate. The therapist and client work together as a team to identify the problems the client is facing, come up with new strategies for addressing them, and thinking up positive solutions (Martin, 2016).\nMany of the most popular and effective CBT techniques are applied to what psychologists call “cognitive distortions” (Grohol, 2016).\nCognitive distortions: inaccurate thoughts that reinforce negative thought patterns or emotions.\nCognitive distortions are faulty ways of thinking that convince us of a reality that is simply not true.\nThere are 15 main cognitive distortions that can plague even the most balanced thinkers at times:\nFiltering refers to the way many of us can somehow ignore all of the positive and good things in our day to focus solely on the negative. It can be far too easy to dwell on a single negative aspect, even when surrounded by an abundance of good things.\nPolarized Thinking / “Black and White” Thinking\nThis cognitive distortion is all about seeing black and white only, with no shades of grey. This is all-or-nothing thinking, with no room for complexity or nuance. If you don’t perform perfectly in some area, then you may see yourself as a total failure instead of simply unskilled in one area.\nOvergeneralization is taking a single incident or point in time and using it as the sole piece of evidence for a broad general conclusion. For example, a person may be looking for a job and have a bad interview experience. Instead of brushing it off as one bad interview and trying again, they conclude that they are terrible at interviewing and will never get a job offer.\nJumping to Conclusions\nSimilar to overgeneralization, this distortion involves faulty reasoning in how we make conclusions. Instead of overgeneralizing one incident, however, jumping to conclusions refers to the tendency to be sure of something without any evidence at all. We may be convinced that someone dislikes us with only the flimsiest of proof, or we may be convinced that our fears will come true before we have a chance to find out.\nCatastrophizing / Magnifying or Minimizing\nThis distortion involves expectations that the worst will happen or has happened, based on a small incident that is nowhere near the tragedy that it is made out to be. For example, you may make a small mistake at work and be convinced that it will ruin the project you are working on, your boss will be furious, and you will lose your job. Alternatively, we may minimize the importance of positive things, such as an accomplishment at work or a desirable personal characteristic.\nThis is a distortion where an individual believes that everything they do has an impact on external events or other people, no matter how irrational the link between. The person suffering from this distortion will feel that they have an unreasonably important role in the bad things that happen around them. For instance, a person may believe that the meeting they were a few minutes late in getting to was derailed because of them, and that everything would have been fine if they were on time.\nAnother distortion involves feeling that everything that happens to you is a result of external forces or due to your own actions. Sometimes what happens to us is due to forces we can’t control, and sometimes what happens is due to our actions, but the false thinking is in assuming that it is always one or the other. We may assume that the quality of our work is due to working with difficult people, or alternatively that every mistake someone else makes is due to something we did.\nFallacy of Fairness\nWe are often concerned about fairness, but this concern can be taken to extremes. As we know, life is not always fair. The person who goes through life looking for fairness in all their experiences will end up resentful and unhappy. Sometimes things will go our way, and sometimes they will not, regardless of how fair it may seem.\nWhen things don’t go our way, there are many ways we can explain or assign responsibility for the outcome. One method of assigning responsibility is blaming others for what goes wrong. Sometimes we may blame others for making us feel or act a certain way, but this is a cognitive distortion because we are the only ones responsible for the way we feel or act.\n“Shoulds” refer to the implicit or explicit rules we have about how we and others should behave. When others break our rules, we are upset. When we break our own rules, we feel guilty. For example, we may have an unofficial rule that a customer service representatives should always be accommodating to the customer. When we interact with a customer service representative that is not immediately accommodating, we might get angry. If we have an implicit rule that we are irresponsible if we spend money on unnecessary things, we may feel exceedingly guilty when we spend even a small amount of money on something we don’t need.\nThis distortion involves thinking that if we feel a certain way, it must be true. For example, if we feel unattractive or uninteresting in the current moment, we must be unattractive or uninteresting. This cognitive distortion boils down to:\n“I feel it, therefore it must be true.”\nClearly our emotions are not always indicative of the objective truth, but it can be difficult to look past how we feel.\nFallacy of Change\nThe fallacy of change lies in expecting other people to change as it suits us. This ties into the feeling that our happiness depends on other people, and their unwillingness or inability to change, even if we push and press and demand it, keeps us from being happy. This is clearly a damaging way to think, since no one is responsible for our happiness except for us.\nGlobal Labeling / Mislabeling\nThis cognitive distortion is an extreme form of generalizing, in which we generalize one or two instances or qualities into a global judgment. For example, if we fail at a specific task, we may conclude that we are a total failure in not only this area, but all areas. Alternatively, when a stranger says something a bit rude, we may conclude that he or she is an unfriendly person in general. Mislabeling is specific to using exaggerated and emotionally loaded language, such as saying a woman has abandoned her children when she leaves her children with a babysitter to enjoy a night out.\nAlways Being Right\nWhile we all enjoy being right, this distortion makes us think we must be right, that being wrong is unacceptable. We may believe that being right is more important than the feelings of others, being able to admit when we’ve made a mistake, or being fair and objective.\nHeaven’s Reward Fallacy\nThis distortion involves expecting that any sacrifice or self-denial on our part will pay off. We may consider this karma, and expect that karma will always immediately reward us for our good deeds. Of course, this results in feelings of bitterness when we do not receive our reward (Grohol, 2016).\nMany tools and techniques found in CBT are intended to address or reverse these cognitive distortions\n9 Essential CBT Techniques and Tools\nThere are many tools and techniques used in CBT, many of which have spread from the therapy context to everyday life. The nine techniques and tools listed below are some of the most common and effective CBT practices.\nThis technique is a way of “gathering data” about our moods and our thoughts. This journal can include the time of the mood or thought, the source of it, the extent or intensity, and how we responded to it, among other factors. This technique can help us to identify our thought patterns and emotional tendencies, describe them, and find out how to change, adapt, or cope with them.\nUnraveling Cognitive Distortions\nThis is a main goal of CBT, and can be practiced with or without the help of a therapist. In order to unravel the cognitive distortions you hold, you must first become aware of which distortions you are most vulnerable to. Part of this involves identifying and challenging our harmful automatic thoughts, which frequently fall into one of the categories listed earlier.\nOnce you identify the distortions or inaccurate views on the world you hold, you can begin to learn about how this distortion took root and why you came to believe it. When you discover a belief that is destructive or harmful, you can begin to challenge it. For example, if you believe that you must have a high paying job to be a respectable person, but you lose your high paying job, you will begin to feel bad about yourself.\nInstead of accepting this faulty belief that leads you to think unreasonably negative thoughts about yourself, you could take this opportunity to think about what makes a person “respectable,” a belief you may not have explicitly considered before.\nExposure and Response Prevention\nThis technique is effective for those who suffer from obsessive compulsive disorder (OCD) and phobias. Those with OCD can practice this technique by exposing yourself to whatever it is that normally elicits a compulsive behavior, but doing your best to refrain from the behavior and writing about it. You can combine journaling with this technique, or use journaling to understand how this technique makes you feel. Those with phobias can learn to expose themselves to whatever elicits their fears, learn ways to cope with the anxiety, and not leave the feared situation until their anxiety has diminished.\nThis technique is intended to treat panic and anxiety. It involves exposure to feared bodily sensations in order to elicit the response, activate any unhelpful beliefs associated with the sensations, maintain the sensations without distraction or avoidance, and allow new learning about the sensations to take place. It is intended to help the sufferer see that symptoms of panic are not dangerous, although they may be uncomfortable.\nNightmare Exposure and Rescripting\nNightmare exposure and rescripting is intended specifically for those suffering from nightmares. This technique is similar to interoceptive exposure, in that the nightmare is elicited, which brings up the relevant emotion. Once the emotion has arisen, the client and therapist work together to identify the desired emotion and develop a new image to accompany the desired emotion.\nPlay the Script Until the End\nThis technique is especially useful for those suffering from fear and anxiety. In this technique, the individual who is vulnerable to crippling fear or anxiety conducts a sort of thought experiment, where they imagine the outcome of the worst case scenario. Letting this scenario play out can help the individual to recognize that even if everything they fear comes to pass, it will likely turn out okay.\nProgressive Muscle Relaxation (PMR)\nThis is a familiar technique to those who practice mindfulness. Similar to the body scan, this technique instructs you to relax one muscle group at a time until your whole body is in a state of relaxation. You can use audio guidance, a YouTube video, or simply your own mind to practice this technique, and it can be especially helpful for calming nerves and soothing a busy and unfocused mind.\nThis is another technique that is not specific to CBT, but will be familiar to practitioners of mindfulness. There are many ways to relax and bring regularity to your breath, including guided and unguided imagery, audio recordings, YouTube videos, and scripts. Bringing regularity and calm to your breath will allow you to approach your problems from a place of balance, facilitating more effective and rational decision making (Megan, 2016)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:d162145d-6f9f-4b1e-9c56-8c92cf361316>","<urn:uuid:aed1f9a3-bb8a-49a9-8fe9-02dc5c371219>"],"error":null}
{"question":"I'm researching desert ecosystems and their resilience. Could you explain the ecological services provided by cryptobiotic soil crusts and describe how their physical characteristics vary across different desert regions?","answer":"Cryptobiotic soil crusts provide several vital ecological services. They stabilize desert soils by binding them together with filaments, preventing wind erosion. They also enhance soil fertility by converting atmospheric nitrogen into a usable form for plants and secreting organic acids that make phosphorus more available to plant roots. Additionally, they help sequester carbon dioxide and provide safe, moist environments for seed germination. The physical characteristics of these crusts vary by region: in California's low desert, they form smooth mats less than half an inch high; in the Mojave Desert, they create bumpy crusts over an inch thick with occasional 2-inch pinnacles; and in the Great Basin Desert and Colorado Plateau, they can form impressive pinnacles up to 6 inches high. The appearance also varies in color, with crusts in the Great Basin and Colorado Plateau appearing darker than surrounding soils due to the density of organisms present.","context":["When you think of the natural side of California, you likely imagine one of California's well-known ecological communities. It might be the redwood forests of the north coast that comes to mind. Or maybe the ancient bristlecone pines in the White Mountains, the oak and grassland savannas of the central Coast Ranges, or John Muir's Great Green Wall in the Sierra Nevada conifer belt. If you've been reading the desert coverage here at KCET, perhaps the Joshua tree forests of the Mojave Desert flitter past your imagination. These natural places, along with easily a dozen more you might name, tend to define California's ecological communities in the public imagination.\nThere's one California community, though, that isn't often listed among its more scenic cousins, but which is every bit as crucial to maintaining the state's ecological health -- and every bit as fragile if mistreated. It's known as cryptobiotic soil crust, also sometimes called microbiotic soil crust, microphytic crust, cryptogamic crust, cryptogam, or just simply \"crypto.\" Regardless of the name, crypto is a slow-growing community of organisms from micro- to macro- that colonizes bare soil in arid places, and it's crucial to the health of the deserts -- and to your health as well.\nCryptobiotic soil crusts are so named because 1) they form crusts on soil and 2) they are primarily made up of cyanobacteria, green algae, fungi, lichens, and mosses that can survive long periods of drought by shutting down their metabolisms, a habit known as \"cryptobiosis.\" (\"Crypto\" comes from the Greek word for hidden, and such organisms \"hide their life\" when it gets too dry out.)\nCrypto forms when cyanobacteria start growing in bare soil, creating a web of microscopic filaments between the soil particles. Cyanobacteria, a type of bacteria once inaccurately called \"blue-green algae,\" can create their own food through photosynthesis. Feeding on the desert sunlight and whatever water is available, the cyanobacteria send tendrils through what would have been sterile soil.\nThese filaments can penetrate remarkably deeply into the earth. It's not unusual for filamentous cyanobacteria to colonize soil six inches deep, though the soil crusts they form are usually much shallower. I've broken rocks in the Mojave National Preserve and found cyanobacteria growing four inches beneath the surface of the granite, thriving quite well on just the little bit of light that penetrated the quartz.\nOther organisms find that web of cyanobacterial filaments a hospitable place to live. Other cyanobacteria that don't form filaments will colonize the web as well, though they tend to stay up top rather than delving the depths. Single-celled green algae -- true algae -- often join in as well, as do fungi that help break down waste products and dead cells of the other organisms.\nCrypto, with its web of reinforcing filaments, holds down desert soils that would otherwise be liable to blow away. Its cyanobacteria take inert atmospheric nitrogen and turn it into a chemical form that plants can use as fertilizer, and most members of the crypto community also secrete organic acids that make soil phosphorus more readily available to plant roots. The photosynthesizing organisms in crypto take carbon dioxide out of the atmosphere, potentially offering some help in the climate change arena: though each square foot of crypto may not sequester much carbon, as much as 70% of the land in some parts of the desert may be covered in crypto so that even the small contributions add up.\nCrypto grows anywhere in North America arid enough that other plant cover is sparse. Its form depends on local temperature and relative humidity. In California's low desert, crypto tends to form mat-like, smooth crusts less than half an inch high. The Mojave's cooler and slightly wetter, and there crypto forms bumpy crusts that can be a bit more than an inch thick, with occasional pinnacles up to a couple inches high. In the much cooler Great Basin desert and Colorado Plateau, cryptobiotic crusts can grow to truly prodigious heights, relatively speaking, with pinnacles rising to six inches in height.\nThe other very important thing to know about crypto is that crypto is fragile. After all, it's an entire community of organisms based on tiny bacterial filaments holding sand and gravel together. An errant hiking boot, or cow's hoof, or set of knobbly ORV tires, can undo a century of crypto growth. As the organisms that make up the crust are cryptobiotic, meaning they shut down their metabolisms when it's dry, damaged crypto can only grow back when it's wet. Though filamentous cyanobacteria can grow fairly quickly under the right conditions, the other components of crypto crusts are usually much slower growing. And if the wind kicks up, tearing away at the newly damaged crypto more quickly than it can grow back, that one bootprint can be multiplied many times.\nWhich means that a little bit of damage to a cryptobiotic soil crust on sandy soil can create new sand dunes and damaged air quality where once there was intact and healthy desert. That's not good for the desert, and it poses a distinct threat to the people who live downwind.\nBetween the ecological services crypto provide and the similar benefits offered by its non-living colleague desert pavement, we would do well to think carefully before we go out and bulldoze the desert, lest that desert up and blow away. And if you weren't planning to bring your bulldozer, you'd still be well advised to stay on the trail where there is one. You don't want the desert to take a century or two to recover from your hike.\nChris Clarke is an environmental writer of two decades standing. Director of Desert Biodiversity, he writes from Joshua Tree regularly at his acclaimed blog Coyote Crossing and comments on desert issues on KCET weekly. Read his recent posts here.","Anything but void\nVisitors to the arid regions of the world often first see an area that seems to be almost void of life. Yet here in these desert lands are commonly found a specialized community of lichen, algae, moss, fungus and cyanobacteria that are referred to as cryptobiotic soil or \"soil crust.\" These living organisms literally bind the loose desert soil together, preventing erosion and aiding in the retention of life-sustaining moisture.\nThe name, cryptobiotic soil, is created from the Greek word \"kryptos,\" which means \"hidden,\" and \"bios,\" which means \"life.\" Even though this \"hidden life\" may appear to be just clumps of dark dirt, they are alive and vital to ecosystems of the desert. Their life activities contribute to the addition of nitrogen and carbon to the desert soil, which is then used by all desert plant life.\nCryptobiotic soil contains millions of microscopic fibers that bind grains of sand together and thus hold the soil in place. It also provides a safe and moist place for seeds to grow. Over time, these soils allow for the bare sand and rock of desert regions to become colonized by the native plants communities.\nIn the United States, cryptobiotic soil crusts are commonly found in the Sonoran Desert, the Great Basin Desert, across the Colorado Plateau and within the inner Columbia Basin. Forms of soil crusts also naturally occur in Alaska and across the great prairies. Some form of biological soil crusts are found on all continents including Antarctica. What they all have in common is that they are formed by living organisms.\nCryptobiotic soil crust creates a landscape that looks like a small mountain range. Here is found a land where miniature black and white soil crust peaks, colored by species of lichen and mosses, are separated by equally miniature valleys that become a perfect location for seed germination.\nThe growth of this miniature landscape is incredibly slow. The above-ground part of the soil crust grows only about 1 millimeter per year when the area sees average rainfall. Over time and undisturbed, this surface crust can grow up to 4 inches (10 centimeters) thick.\nThe appearance of soil crust varies in topography, color and surface coverage. In the Great Basin and on the Colorado Plateau the soil crusts tend to be darker than their surrounding soils. The darkness is caused by both the density of the organisms found here and the dark nature of the cyanobacteria, mosses and lichens. In these areas, soil crust may be 70 percent or more of the living vegetation found here.\nProduct of the environment\nThe natural process that creates the uneven surface topography so common among soil crusts is the result of filamentous cyanobacteria and species of green algae. When rain does come to these desert areas, these living organisms absorb the water and swell in size. New vegetative material grows, creating a new network of filaments and sheaths. Repeated moisture results in a network of empty sheaths that remain even when the common dry conditions return and results in the nearby soil taking on the common miniature mountain appearance.\nThe fragility of life\nCryptobiotic soil crust is extremely fragile. Even though it might be 4 inches thick, it is no match for a boot or a tire. Natural areas where soil crust is common encourage visitors to the area to stay on marked trails and roads. Any off-trail activity can severely damage the living organisms that make up soil crust. Once damaged, and depending on the amount of moisture a region receives, it can take between 30 and 300 years for the soil crust to repair itself from the damage.\nOn the open range\nOn the open range of the American West, livestock is a great threat to all crytobiotic soils; so too are off-road vehicles and even military activities. For living organisms that are so vital to soil stability, atmospheric nitrogen-fixation, erosion prevention and seed germination, knowledge that crytobiotic soils even exist might be their greatest hope and protection from destruction. Shown here is the official logo of the U.S. National Park Service's \"Don’t Bust the Crust\" educational program to teach children at an early age what crytobiotic soil is, what it looks like, and its importance to the ecology of the area.\nOn the lookout\nSo next time you have a chance to visit one of the great desert areas of America or the world, keep an eye out for crytobiotic soil. When you find some, carefully kneel down and take a close look at the miniature world you will find there. You will gaze upon a key member of an arid, yet thriving, ecological system.\nLive Science newsletter\nStay up to date on the latest science news by signing up for our Essentials newsletter."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:27980479-5238-423d-9a08-fdf7c76ac1b2>","<urn:uuid:f216e3de-cede-42f5-87e0-53a5f01ff638>"],"error":null}
{"question":"What are the key differences between back-testing and agent-based modeling when it comes to predicting future market behavior?","answer":"Back-testing and agent-based modeling differ significantly in their approach to predicting market behavior. Back-testing measures a model's outcomes against real-world historical observations to evaluate accuracy, focusing on what has already happened. In contrast, agent-based modeling takes a bottom-up approach by studying interactions between large numbers of autonomous agents (like high frequency, fundamentalist, and chartist traders) to explore what might happen under various market conditions, including extreme scenarios. While back-testing relies on historical patterns, agent-based modeling can generate synthetic data and simulate market conditions beyond historical bounds, making it more suitable for exploring future possibilities and avoiding the limitations of assuming future events can be calculated with actuarial certainty from past data.","context":["Gaining insight on future results of trading strategies is not just about back-testing algorithms against historical market data. Predicting future price movements requires modeling a wide range of market conditions to explore what might happen.\n- A Refinitiv white paper examines a model for simulating financial markets through a combination of agent-based modeling, synthetic data and machine learning.\n- Agent-based modeling observes the collective behavior between large numbers of autonomous agents — high frequency, fundamentalist and chartist traders.\n- The research presents agent-based simulation models built on Simudyne software, highlighting the ability to generate price paths for various ‘what-if’ scenarios.\nFinancial professionals extensively use historical market data in order to gain insight into the effectiveness of their trading strategies.\nThe presumption is that the market comprises recurring patterns and by studying these patterns in the past, one can predict future price movements.\nThere are several limitations with this approach. Relying on the belief that future events can be calculated with actuarial certainty from past data is deeply flawed.\nIt is important to consider all possible outcomes, including those that are outside of historical bounds, for efficient modeling of uncertainty and to avoid the danger of overfitting.\nSimulating financial markets\nComputational simulations are an effective mechanism to augment historical data. They can model a wide variety of market conditions and explore what might happen under extreme situations.\nClassic simulation techniques that take a top-down modeling approach are not suitable because the dynamics of financial markets are just too complicated to be represented by structural models.\nA recently-released white paper by Refinitiv — Synthetic Reality: Synthetic market data generation at scale using agent based modeling — explores new ways of simulating financial markets by combining three technologies:\n- Agent-based modeling (ABM)\nThis has been developed as a tool of last resort, to obtain results when a phenomenon that is to be modeled is too complex for traditional approaches.\nAn ABM takes a bottom-up approach and may more realistically capture the complex dynamics of financial markets. It studies the interactions between large numbers of individuals termed agents, which possess independent decision-making capabilities.\nIt has been used in the past for simulating the interaction between military powers in the Cold War, or among societies and biological ecosystems, as well as for simulating financial markets.\nOur approach relies on the Simudyne platform, which ensures simulation can scale beyond tens of thousands of parallel agents. Our experiments combine Refinitiv’s mathematical models implemented in Python with Simudyne’s Java-based simulation software.\n- Synthetic data\nHistorical data as the sole source of modeling focuses too narrowly on what has happened, and cannot provide support for answering the question of what could happen.\nRandom data that has been generated to exhibit certain properties of financial markets may be random but it is also plausible, and can therefore serve to supplement historical data for the analysis of scenarios that go beyond the historical past.\n- Deep learning\nTraditionally each agent in ABM has a way of acting that is hardwired in the form of rules. More recently, researchers have used machine learning inside individual ABM agents to make their behavior more adjustable by the state of the simulation (i.e., the environment).\nThe approach in this white paper shows for the first time that deep learning can be used successfully as an ABM agent’s action strategy.\nAcross multiple asset classes\nThe white paper shows how thousands of traders with different strategies — high frequency, fundamentalist, chartist — interact, leading to complex overall system behavior, and how synthetic data can be used to explore bullish, bearish and flash crash scenarios.\nIt also shows how synthetic and historical data can be used together with agents equipped to adapt to their environment using deep learning.\nWe conducted evaluations on multiple asset classes across a portfolio of assets and found that the proposed agent decision mechanism outperforms other techniques. Our simulation model also successfully replicates the empirical stylized facts of financial markets.","Performance Testing: Benchmarking Vs. Back-Testing\nWhen someone asks you what a model validation is, what is the first thing you think of? If you are like most, then you would immediately think of performance metrics— those quantitative indicators that tell you not only if the model is working as intended, but also its performance and accuracy over time and compared to others. Performance testing is the core of any model validation and generally consists of the following components:\nSensitivity analysis and stress testing, while critical to any model validation’s performance testing, will be covered by a future article. This post will focus on the relative virtues of benchmarking versus back-testing—seeking to define what each is, when and how each should be used, and how to make the best use of the results of each.\nBenchmarking is when the validator is providing a comparison of the model being validated to some other model or metric. The type of benchmark utilized will vary, like all model validation performance testing does, with the nature, use, and type of model being validated. Due to the performance information it provides, benchmarking should always be utilized in some form when a suitable benchmark can be found.\nChoosing a Benchmark\nChoosing what kind of benchmark to use within a model validation can sometimes be a very daunting task. Like all testing within a model validation, the kind of benchmark to use depends on the type of model being tested. Benchmarking takes many forms and may entail comparing the model’s outputs to:\n- The model’s previous version\n- An externally produced model\n- A model built by the validator\n- Other models and methodologies considered by the model developers, but not chosen\n- Industry best practice\n- Thresholds and expectations of the model’s performance\nOne of the most used benchmarking approaches is to compare a new model’s outputs to those of the version of the model it is replacing. It remains very common throughout the industry for models to be replaced due to a deterioration of performance, change in risk appetite, new regulatory guidance, need to capture new variables, or the availability of new sets of information. In these cases, it is important to not only document but also prove that the new model performs better and does not have the same issues that triggered the old model’s replacement.\nAnother common benchmarking approach compares the model’s outputs to those of an external “challenger” model (or one built by the validator) which functions with the same objective and data. This approach is likely to return more apt output comparisons than those generated by benchmarking against older versions that are likely to be out of date since the challenger model is developed and updated with the same data as the champion model.\nAnother benchmark set which could be used for model validation includes other models or methodologies reviewed by the model developers as possibilities for the model being validated but ultimately not used. Model developers as best practice should always list any alternative methodologies, theories, or data which were omitted from the model’s final version. Additionally, model validators should always leverage their experience and understanding of the current best practices throughout the industry, along with any analysis previously completed on similar models. Model validation should then take these alternatives and use them as benchmarks to the model being validated.\nModel validators have multiple, distinct ways to incorporate benchmarking into their analysis. The use of the different types of benchmarking discussed here should be based on the type of model, its objective, and the validator’s best judgment. If a model cannot be reasonably benchmarked, then the validator should record why not and discuss the resulting limitations of the validation.\nBack-testing is used to measure model outcomes. Here, instead of measuring performance with a comparison, the validator is specifically measuring whether the model is both working as intended and is accurate. Back-testing can take many forms based on the model’s objective. As with benchmarking, back-testing should be a part of every full-scope model validation to the extent possible.\nWhat Back-Tests to Perform\nAs a form of outcomes analysis, back-testing provides quantitative metrics which measure the performance of a model’s forecast, the accuracy of its estimates, or its ability to rank-order risk. For instance, if a model produces forecasts for a given variable, back-testing would involve comparing the model’s forecast values against actual outcomes, thus indicating its accuracy.\nA related function of model back-testing evaluates the ability of a given model to adequately measure risk. This risk could take any of several forms, from the probability of a given borrower to default to the likelihood of a large loss during a given trading day. To back-test a model’s ability to capture risk exposure, it is important first to collect the right data. In order to back-test a probability of default model, for example, data would need to be collected containing cases where borrowers have actually defaulted in order to test the model’s predictions.\nBack-testing models that assign borrowers to various risk levels necessitate some special considerations. Back-testing these and other models that seek to rank-order risk involves looking at the model’s performance history and examining its accuracy through its ability to rank and order the risk. This can involve analyzing both Type 1 (false positive) and Type 2 (false negative) statistical errors against the true positive and true negative rates for a given model. Common statistical tests used for this type of back-testing analysis include, but are not limited to, a Kolmogorov-Smirnov score (KS), a Brier score, or a Receiver Operating Characteristic (ROC).\nBenchmarking vs Backtesting\nBack-testing measures a model’s outcome and accuracy against real-world observations, while benchmarking measures those outcomes against those of other models or metrics. Some overlap exists when the benchmarking includes comparing how well different models’ outputs back-test against real-world observations and the chosen benchmark. This overlap sometimes leads people to mistakenly conclude that model validations can rely on just one method. In reality, however, back-testing and benchmarking should ideally be performed together in order to bring their individual benefits to bear in evaluating the model’s overall performance. The decision, optimally, should not be whether to create a benchmark or to perform back-testing. Rather, the decision should be what form both benchmarking and back-testing should take.\nWhile benchmarking and back-testing are complementary exercises that should not be viewed as mutually exclusive, their outcomes sometimes appear to produce conflicting results. What should a model validator do, for example, if the model appears to back-test well against real-world observations but do not benchmark particularly well against similar model outputs? What about a model that returns results similar to those of other benchmark models but does not back-test well? In the first” scenario, the model owner can derive a measure of comfort from the knowledge that the model performs well in hindsight. But the owner also runs the very real risk of being “out on an island” if the model turns out to be wrong. The second scenario affords the comfort of company in the model’s projections. But what if the models are all wrong together?\nScenarios where benchmarking and back-testing do not produce complementary results are not common, but they do happen. In these situations, it becomes incumbent on model validators to determine whether back-testing results should trump benchmarking results (or vice-versa) or if they should simply temper one another. The course to take may be dictated by circumstances. For example, a model validator may conclude that macro-economic indicators are changing to the point that a model which back-tests favorably is not an advisable tool because it is not tuned to the expected forward-looking conditions. This could explain why a model that back-tests favorably remains a benchmarking outlier if the benchmark models are taking into account what the subject model is missing. On the other hand, there are scenarios where it is reasonable to conclude that back-testing results trump benchmarking results. After all, most firms would rather have an accurate model than one that lines up with all the others.\nAs seen in our discussion here, benchmarking and back-testing can sometimes produce distinct or similar metrics depending on the model being validated. While those differences or similarities can sometimes be significant, both benchmarking and back-testing provide critical complementary information about a model’s overall performance. So when approaching a model validation and determining its scope, your choice should be what form of benchmarking and back-testing needs to be done, rather than whether one needs to be performed versus the other."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7fce060a-a277-4330-b11c-ba8cd47d0ca5>","<urn:uuid:1c5146de-2dc5-4a01-b0cd-f6d4beadfc2a>"],"error":null}
{"question":"¿Qué colección especial alberga el Museo Nacional del Azulejo en Lisboa?","answer":"The National Tile Museum showcases traditional Portuguese azulejos (painted ceramic tiles) from their Moorish origins. A notable highlight is the Grand Vista de Lisboa panel, which shows the city as it appeared before the 1755 earthquake.","context":["7 Top Things to Do in Lisbonby Lisa Cheng | January 25, 2017\nThis famously hilly city, once home to the world's great explorers, is filled with cobblestone-lined alleys and charming tabernas. While an earthquake destroyed much of the city in the mid-18th century, there are still many iconic historic sights spread out across the metropolis. Our advice: be sure to bring your walking shoes and plan to refuel on pastel de nata (egg tarts) between stops.\n1. Climb high above town to St. George's Castle.\nIt’s almost impossible to be anywhere in this city without seeing the fortification looming above the skyline. And if you think the views are spectacular when you look up, you'll find they’re simply amazing when you look down from the ramparts and towers, which were constructed in the mid-11th century. From the observation platform, you can take in the entire city, from the squares and streets below to the red rooftops that slope down to the Tagus River.\n2. Be amazed by Renaissance treasures.\nBehind the stark façade of the 16th-century Jesuit Church of Saint Roch is a trove of artistic riches. It doesn’t matter which way you crane your neck: At every angle and arch, there’s enough marble and gold to inspire awe in even non-believers.\n3. Discover Lisbon's maritime history.\nBelem Tower was built as a fortification off the north bank of the Tagus River back in the 16th century. Today the squat limestone structure stands as a proud symbol of the glorious age of discovery, when intrepid mariners and navigators sailed around the globe and charted maps of new territories unknown to Europeans. As you ascend the spiral staircases to gaze out to sea, it’s easy to imagine them sailing off to lands unknown.\n4. Explore the heights of Lisbon\nThis antique wrought-iron elevator connects Lisbon’s steep elevations, specifically the Baixa district and Carmo Square, which is home to the ruins of a crumbling church. Though there are cheaper ways to get vistas of the city, there’s something romantic about getting whisked up in a polished wood carriage to a viewing platform.\n5. Appreciate the art of the ages.\nPortugal is famous for its azulejos, the painted ceramic tiles that adorn façades of churches and other buildings. The National Tile Museum, housed in a former 16th-century convent, showcases the traditional craft in all its iterations, starting from its Moorish origins. Don’t miss the large-scale panel of the Grand Vista de Lisboa, which depicts the glorious city before the devastating 1755 earthquake.\n6. Take the plunge at the Lisbon Oceanarium.\nTraverse the deep seas — without having to get wet or strap on oxygen tanks — at the largest indoor aquarium in Europe. You may see a bright butterfly fish dart through branch coral or a blacktip reef shark loom overhead. Best of all, the illusion of immersion makes these creatures seem close enough to touch.\n7. Commune with the ghosts of explorers.\nBuilt in the early 16th century, Jerónimos Monastery was commissioned by King Manuel I to honor Vasco de Gama’s 1498 discovery of a route to India. The revered navigator himself is buried near the entrance of the church, which is just as much a celebration of Portugal’s golden Age of Exploration as it is a house of worship.\nLisa Cheng is a New York-based writer for ShermansCruise who also writes for Travel + Leisure."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:af534ebd-c82c-405f-aabe-a9caf1d4edf6>"],"error":null}
{"question":"Who lived earlier: the Egyptian nobleman Khuwy or the Roman Emperor Marcus Aurelius?","answer":"The Egyptian nobleman Khuwy lived earlier, as his mummified remains date back to the Old Kingdom period, over 4,000 years ago. Marcus Aurelius lived much later, from 121 CE to 180 CE, ruling as Roman Emperor from 161 to 180 CE.","context":["The traditional Egyptians have been finishing up refined mummifications of their lifeless 1,000 years sooner than beforehand thought, in accordance with new proof which may result in a rewriting of the historical past books.\nThe preserved physique of a high-ranking nobleman known as Khuwy, found in 2019, has been discovered to be far older than assumed and is, actually, one of many oldest Egyptian mummies ever found. It has been dated to the Outdated Kingdom, proving that mummification strategies some 4,000 years in the past have been extremely superior.\nThe sophistication of the physique’s mummification course of and the supplies used – together with its exceptionally positive linen dressing and high-quality resin – was not thought to have been achieved till 1,000 years later.\nProfessor Salima Ikram, head of Egyptology on the American College in Cairo and a number one skilled on the historical past of mummification, instructed the Observer: “If that is certainly an Outdated Kingdom mummy, all books about mummification and the historical past of the Outdated Kingdom will should be revised.”\nShe added: “This might utterly flip our understanding of the evolution of mummification on its head. The supplies used, their origins, and the commerce routes related to them will dramatically influence our understanding of Outdated Kingdom Egypt.\n“Till now, we had thought that Outdated Kingdom mummification was comparatively easy, with fundamental desiccation – not at all times profitable – no elimination of the mind, and solely occasional elimination of the inner organs. Certainly, extra consideration was paid to the outside look of the deceased than the inside. Additionally, using resins is much extra restricted within the Outdated Kingdom mummies to this point recorded. This mummy is awash with resins and textiles and provides a totally totally different impression of mummification. In actual fact, it’s extra like mummies discovered 1,000 years later.”\nIt’s amongst main discoveries to be revealed in Nationwide Geographic’s documentary sequence, Misplaced Treasures of Egypt, beginning on 7 November. It’s produced by Windfall Movies, and the cameras observe worldwide archaeologists throughout the excavation season in Egypt. The mummification discovery will function in episode 4 – entitled Rise of the Mummies – on 28 November.\nIkram seems in that episode with fellow archaeologist Dr Mohamed Megahed, who says of the newest discovery: “If it’s actually Khuwy, this can be a breakthrough in Historical Egyptian historical past.”\nThe mother’s discovery in a lavish tomb within the necropolis at Saqqara was filmed in Nationwide Geographic’s earlier season. The investigation into its courting and evaluation emerges within the new sequence. Hieroglyphs revealed that it belonged to Khuwy, a relation of the royal household who lived over 4,000 years in the past.\nTom Prepare dinner, the sequence producer for Windfall Movies, stated: “They knew the pottery within the tomb was Outdated Kingdom however [Ikram] didn’t suppose that the mother was from [that period] as a result of it was preserved too properly. They didn’t suppose the mummification course of [then] was that superior. So her preliminary response was: that is positively not Outdated Kingdom. However over the course of the investigation she began to come back spherical [to the idea].”\nHistorical embalmers bathed our bodies in costly resins from tree sap, preserving the flesh earlier than they wrapped the corpse. This mummy is impregnated with high-quality resins and wrapped within the highest-grade of bandages.\nIkram says within the programme: “It’s extraordinary. The one time I’ve [seen] a lot of this type of good high quality linen has been within the twenty first dynasty.” The twenty first dynasty of Egyptian Pharaohs reigned greater than 1,000 years after Khuwy lived.\nCarolyn Payne, Nationwide Geographic’s commissioning editor, stated that what makes this sequence so uncommon is that it follows a complete group of various archaeologists throughout a season: “We did see some wonderful finds.”\nThe documentary observes: “With each new physique archaeologists unearth, the story of the mummies of Egypt turns into clearer.”","|Emperor of the Roman Empire|\nBust of Marcus Aurelius\n|Reign||8 March 161–169 |\n(with Lucius Verus);\n(19 years, 9 days)\n|Born||26 April 121|\n|Died||17 March 180 (aged 58)|\nVindobona or Sirmium\n|Spouse||Faustina the Younger|\n|Father||Marcus Annius Verus|\nMarcus Aurelius' work Meditations, written in Greek while on campaign between 170 and 180, is still revered. It serves as an example of how Aurelius approached the Platonic ideal of a philosopher–king, and how he symbolized much of what was best about Roman civilization.\nThe unforeseen consequences for the Empire were great. The returning troops brought with them a plague (the so-called Antonine Plague), which may have been smallpox. It would eventually kill about 5 million people, and severely weaken the Empire.\nAurelius fought the German tribes during a long war from 166 to 180. The pressure from the Goths moving west pushed settled Germanic tribes into invading Roman client states in Gaul, and across the Danube.\nThe plague raging in the Empire limited Rome's ability to respond. The Marcomanni tribe entered Italy, which had not been invaded for nearly 200 years, and defeated the army of the Praetorian Prefect.\nThis disaster forced Marcus to collect forces from other frontiers, and move them against the Marcomanni. The Roman army was commanded by Claudius Pompeianus, Marcus' son-in-law, with the future emperor Pertinax as one of his lieutenants. The invaders were thrown out of Italy, and Marcus began planning to cross the Danube into their territory.\nIntense diplomatic activity followed, as the Romans tried to win over various barbarian tribes in preparation for a crossing of the Danube. A peace treaty was signed with some tribes, while others became Roman allies. In 172, Marcus defeated the Marcommani, and in 173 their allies, the Quadi, were defeated. It took until 174 for the Quadi to be subdued.\nMarcus was now interrupted by a revolt in the east, led by the Roman general Avidius Cassius. He was suppressed by Marcus' army. Marcus then returned to Rome. A triumph was celebrated with his son Commodus.\nIn 177 the Quadi rebelled again, and the Marcommani joined them. Marcus arrived on the scene in August 178, and in due course the Romans won a decisive battle in what is now Slovakia. The emperor Marcus died in 180, still fighting the German tribes.\nVictories were achieved in the next couple of years, sufficient for Commodus to claim credit with a triumph. The battles had been fought by his generals.\nThe war had exposed the weakness of Rome's northern frontier. From then on, half the Roman legions (16 out of 33) would be stationed along the Danube and the Rhine. For the Germanic tribes, although for the moment checked, the Marcomannic wars were only the prelude. Later invasions in the 4th and 5th centuries would eventually end the Western Roman Empire.\nMarriage and issueEdit\nAurelius married his first cousin Faustina the Younger in 145. During their 30-year marriage Faustina bore 13 children. Only one son and four daughters outlived their father:\n- Birley, Anthony R. 1987 . Marcus Aurelius: a biography. Routledge, London. ISBN 0-415-17125-3\n- Western Civilization: sources, images and interpretations. Dennis Sherman, vol 1, 5th ed, p104\n- BBC: Past pandemics that ravaged Europe, 7 November 2005\n- Birley, Anthony R. 2000. \"Hadrian to the Antonines\", in The Cambridge Ancient History Volume XI: The High Empire, A.D. 70–192. edited by Alan Bowman, Peter Garnsey, and Dominic Rathbone, 132–94. Cambridge University Press. ISBN 978-0-521-26335-1"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b0ab1a0c-c245-4f73-a5ad-2b53c775da6e>","<urn:uuid:be591c53-919e-4336-8a9b-c672d01db878>"],"error":null}
{"question":"How does the Nyquist diagram help analyze amplifier stability and what information can be derived from it?","answer":"The Nyquist diagram is a graphical tool that shows how loop gain (Aβ) varies in phase and magnitude with frequency changes. It plots vectors from an origin O, with each vector representing both magnitude and phase angle of Aβ at different frequencies. The diagram helps analyze stability in several ways: 1) It shows the feedback factor through the distance between the Aβ locus and a point 1 unit from O, 2) It reveals the gain margin, which is the ratio between where the curve passes through the positive direction and the 1 point, expressed in dB, and 3) It shows the phase margin, which is the angle of the vector Aβ where the curve passes through a radius of 1 from point O. If the curve goes round more than 180° before turning to point O, increasing its size can cause oscillation, indicating instability.","context":["|Basic Audio is a free introductory textbook to the basics of audio physics and electronics. See the editorial for more information....|\n|Home Feedback Fundamentals The Loop Gain - Nyquist Diagram|\n|See also: Gain Control|\n|Search the VIAS Library | Index|\nThe Loop Gain - Nyquist Diagram\nAuthor: N.H. Crowhurst\nWe are fortunate that this time difference between the voltage fluctuation at different points in an amplifier at various frequencies can be designated as a phase angle. This means that we can draw a \"picture\" of the voltage with what is called a vector. It is a line drawn at an angle to correspond with the phase angle from a starting point representing zero.\nThe loop gain A(3 is the important thing to consider. We can draw a series of lines from a starting point called O (for Origin). Each line represents both the magnitude and the phase angle of Ap for a different frequency. Joining the outer ends of these lines produces a curve that represents all possible positions for the tip of the A(S vector and shows how Ap varies in phase and magnitude due to frequency changes. This curve is called a Nyquist diagram.\nIn the case of negative feedback, the feedback factor is (1 4- Ap). First we draw a line in one direction from O, representing Afi. Assuming that there is no phase shift, we can mark off 1 in the opposite direction from O and the total length of the line is then the feedback factor (1 + Ap). In the case of positive feedback, both A(3 and the distance will be measured in the same direction from O and the disance between the end of the line representing Ap and 1 will represent the feedback factor (1 - Af5).\nEither way, the feedback factor is given by the distance between the A(3 locus and the point measured off 1 unit from O. The convenience of this method is that the distance between any other point on the locus and this position, measured 1 unit from O, also represents both the magnitude and phase angle of the feedback factor.\nThus the feedback factor can be called (1 4- AP); bear in mind that Ap is not just a simple number now, but that it includes a phase angle. When this phase reaches 180°, Ap has become negative instead of positive, and represents fully positive feedback.\nA rigorous proof of this would involve mathematics beyond the scope of this book, and an exact explanation would call for a knowledge of complex numbers. Looking at it as a simple geometrical diagram, however, can give us a good picture of what happens without knowing all the mathematics.\nNow let's look a little more at the geometry relative to this diagram. The formula for the gain of an amplifier with feedback is Af = A/(l + AfJ).\nIn most amplifiers, the feedback fraction p is constant; it does not change either in magnitude or phase as we change frequency. The internal amplifier gain A is the part that changes with frequency and produces the phase shift. We could multiply the top and bottom of the fraction giving the amplification with feedback by p and still have the same results. We obtain a part of the formula for amplification with feedback that does not change with frequency 1/p, and a part that does change with frequency A|3/(l + AP).\nIn the locus diagram, we have a curve representing the locus of a point whose distance and angle from the point O represent the value of Ap in magnitude and phase. Also, the distance and angle from the point measured off 1 unit from O represents the magnitude and phase of (1 + AP). So the ratio of the distances of any point on the curve from these two points gives us the frequency-varying part of the formula for amplification with feedback.\nIt is a fact in geometry that all the points whose distances from two fixed points are in a fixed ratio form a circle. If we draw a family of circles representing different ratios of gain variation Ap/(l + AP), we have a background that will help us interpret this locus diagram curve. In the background circles shown here, the vertical straight line joins all points where Ap = 1 4- Ap. From there, curves are drawn at 0.5-db differences in ratio up to 3 db either way [Ap = 1.414 X (1 + Ap) or 1 + Ap = (1.414 X Ap)}. From there to 10 db, the circles are at 1-db intervals, and from 10 to 20 db, at 2-db.\nThe locus vector curve itself is called a Nyquist diagram. If the curve representing the locus of Ap follows one of the circles which represents a constant ratio of distances from O and 1, the gain of the amplifier with feedback would be constant, although there would be phase change in both A(5 and (14- Ap), as well as a transition from positive to negative value of Ap.\nThis particular response is impossible with any practical amplifier. At some frequency, A must fall to zero. Usually this happens at a very low frequency and at a very high frequency, due to ultimate loss in the coupling capacitors at the low-frequency end and stray capacitance between stages at the high-frequency end. Either way, when Ap falls right down toward zero, the curve must turn in and go to the point O. None of the circles representing constant ratio goes through the point O; all pass between the points O and 1 and out beyond them on opposite sides, one side or the other, except for the line that represents Ap = 1 4- Ap which is a straight line perpendicular between the two points. As a practical amplifier has frequency limitations and eventually loses gain completely at extreme frequencies, it cannot follow any of these constant-gain lines (circles) all the way.\nWe can use this diagram, however, to predict the overall response of the amplifier (with feedback) by the way the locus curve criss-crosses the circles drawn to represent different values of constant ratio. A complete Nyquist diagram starts from O and finishes at\" O, representing frequencies zero and infinity. For simplicity, we have shown half, starting from a mid-range frequency where there is no phase shift either way.\nNow we can see how this same diagram shows how much margin of safety we have between the way we are working and the beginning of oscillation. Increasing the value of Ap at all frequencies uniformly multiplies up the whole size of the curve in proportion. If the curve goes round more than 180° before it turns into the source or origin point O, increasing its size eventually causes it to go through the 1 point, which means oscillation occurs. We can see how much margin there is between the point where the curve passes through the positive direction and the 1 point. As the whole curve multiplies up in proportion as the gain or feedback of the amplifier is changed, the ratio between this distance and 1 gives the amount by which the loop gain AfJ can be increased before oscillation commences.\nThis ratio, expressed in db, is called the gain margin, because an increase by this much gain starts oscillation. This margin can actually be measured by increasing the amount of feedback until oscillation commences and then calculating the difference in loop gain between the condition at which the amplifier actually works and the amount needed to make it oscillate.\nThe other criterion of stability, as these margins are called, is the phase margin. This can be shown on the Nyquist diagram, but it is not at all easy to measure. Hence, the gain margin is probably of the most value in assessing the performance of an amplifier. On the diagram, the phase margin is the angle of the vector Ap at the point where the curve passes through a radius of 1 from point O, and point 1 from which distances (1 + Ap) are measured. It means that increasing the loop phase shift in the amplifier (or anywhere in A or P) by this angle would cause the amplifier to oscillate.\nThis statement assumes a change of the phase shift without a change of the amplification characteristic in any other way, which, in practice, is not possible, This is another reason why the phase margin is not a very practical criterion: it has no real significance, but now we have the tools.\n|Home Feedback Fundamentals The Loop Gain - Nyquist Diagram|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e5ad1b24-8b68-4ad0-abae-aaf3a9f6d9cc>"],"error":null}
{"question":"Working on a human rights project focused on discrimination tracking - what are the key dimensions being measured in South Africa's discrimination monitoring system between 2017-2019?","answer":"South Africa's discrimination monitoring system between 2017-2019 measured multiple key dimensions: perceived discrimination, experienced discrimination, and reported discrimination. The system specifically tracked intolerance and discrimination based on gender/sex, sexual orientation, political affiliation, and socio-economic status. This data was collected through various methods including attitudinal surveys, administrative data analysis, and case monitoring through institutions like the Equality Courts, the South African Human Rights Commission, and the Commission on Gender Equality. The system also included tracking responses and interventions by both governmental and civil society organizations to address these forms of discrimination.","context":["One of the key intended outputs of the National Action Plan to Combat Racism, Racial Discrimination, Xenophobia and Other Intolerances (NAP) is a regular reflection of the levels of intolerance and discrimination within South African society. This includes information on the prevalence of intolerance and discrimination, the documentation of the policies and programmes implemented by Government, Civil Society and Business to combat intolerance, an assessment of the extent to which those policies and programmes are having any effect, and the identification of areas of work that require urgent attention.\nIn its report “The State of Intolerance and Discrimination in South Africa 2017-2019”, the Foundation for Human Rights studies the levels of intolerance and discrimination in South African by relying on the existing statistical data collected between 2017-2019 from a variety of sources, and investigates the responses by government and independent bodies to address different dimensions of intolerance and discrimination in South Africa. The research was conducted in 2019 and finalized in early 2020, and hence the report and associated annexes should be read with this in mind.\nThe study has been divided into multiple components. The final report summarizes the analysis and findings from all components of the study as detailed in the respective Annexes (see below), provides a grasp on the methodologies used and includes conclusions and recommendations.\nThe study was conducted by: Katarzyna Zduńczyk; Chido Muzanenhamo; Katherine Brown; and Dr Zaid Kimmie.\nAnnex 1: The Level of Intolerance and Discrimination – Attitudinal Surveys\nThis document analyses the levels of intolerance and discrimination in South Africa in 2017-2019 using results from several attitudinal surveys. In its analysis, the report refers to the following grounds of intolerance and discrimination:\n- Gender or Sex\n- Sexual Orientation\n- Political Affiliation\n- Socio-Economic Status\nAnnex 2: The Level of Intolerance and Discrimination – Attitudinal Surveys\nThis component of the study combines a desk review of Rajen Govender’s report, The Analysis of Methods of Data Collection on Incidents of Racism, Racial Discrimination, Xenophobia and related Intolerance and quantitative analysis of administrative data on employment and education.\nAnnex 3: Justice and Human Rights System Response to Racism, Xenophobia and Other Forms of Intolerance and Discrimination\nThis component provides a snapshot of cases before the Equality Courts, the South African Human Rights Commission (SAHRC), the Commission on Gender Equality (CGE) and the cases reported through the National Task Team (hereinafter NTT) on Gender and Sexual Orientation-Based Violence Perpetrated Against LGBTI+ persons.\n4: Review and Assessment of Governmental Departments\nThis fourth component provides an in-depth analysis of government’s responses (at the national departmental level) in terms of goals, strategic objectives, programmes, projects, and interventions that align with NAP and hence, address the intolerance and discrimination. In order to identify and review government responses, an analysis was done of the Strategic Plans and Annual Reports as published by 36 government departments.\nAnnex 5: Analysis of the 25 Year Review as Published by Government\nThis fifth and last component of this study involved a desktop analysis of the ’25 Year Review’ as published by government. We have analyzed each chapter through the lenses of the NAP, assessed the government response and identified the possible shortcomings in the government’s focus.\nAnnex 6: The Levels of Perceived, Experienced and Reported Discrimination (2017 – 2018)\nThe document contains a table comparing the levels of perceived, experienced and reported discrimination between 2017-2019 based on the information from five sources, including:\n- The 2017 Foundation for Human Rights (FHR) Survey on Constitutional Awareness\n- The 2018/19 Governance, Public Safety and Justice Survey by Statistics South Africa\n- The 2017 South African Reconciliation Barometer produced by the Institute for Justice and Reconciliation\n- The 2018 Afrobarometer Survey\n- The 2017 South African Social Attitudes Survey produced by the Human Sciences Research Council\n- The South African Human Rights Commission\nAnnex 7: Baseline Checklist – Intolerance and Discrimination\nThe document contains a table that summarises the findings on the levels of intolerance and discrimination from all researched attitudinal surveys.\nAnnex 8: Alignment of Strategic Plans and Annual Reports with the NAP\nThe document contains a table that reflects the findings from the analysis of the government’s Annual Reports and Strategic Reports.\nAnnex 9: Information about the Attitudinal Surveys used in the study\nThe document contains a table that collates all most important information about each survey that we have referenced in the study.\nAnnex 10: Terminology\nThe document contains the explanation of the relevant terminology used across all research outputs."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:94a2736c-9279-466e-ae7e-46eabb38d215>"],"error":null}
{"question":"How did Aztec priests maintain control and order in their society through religious practices?","answer":"Aztec priests maintained control and order through several key roles and practices. They acted as religious leaders who told people how to behave to avoid angering the gods. Some priests were active in government and others worked as teachers. They created records in hieroglyphics and conducted sacrifices that were believed necessary to keep the gods happy. Priests used temples both large and small for worship ceremonies, personal bloodletting, and human sacrifices. The Aztecs were terrified of their gods' punishment, so they relied on priests to guide their daily worship and maintain proper religious observance.","context":["The Aztec Indians believed in many gods.\nSome of their gods controlled nature. The Aztecs believed that different gods watched over their seeds and plants and harvest. Various gods were in charge of the rain and water and wind. These gods worked together (most of the time) with the god who actually produced food, the god Chicomicoatl.\nThe Aztecs believed that gods needed rest, just like people. There were two gods of war - one for wars fought during the daytime, and one for wars fought at night. That way, one god could sleep while the while the other watched over the Aztec warriors. Aztec warriors were ready to fight at any time because they believed one or the other of their war gods would be by their side.\nThe Aztecs believed their gods had families. Some gods were related to each other by blood or through marriage. For example, Quetzalcoatl was the god of civilization. He was a very important god, but not as important as his brother, Tezcatlipoca, who was one of the two Aztec gods of war. The other war god was named Huitzilopochtli.\nThe Aztecs believed in an afterlife. After they died, they believed they would be assigned a job to do that helped their gods.\nThe Aztec people took their gods very seriously. They believed their gods would punish them if they did not worship their gods every day. In fact, they believed they would be punished if they worshipped their gods every day, but somehow, in spite in their best efforts, did not worship them enough in the eyes of their gods. They were terrified of their gods and what they might do if angered.\nThat's why worshiping their gods was a big part of Aztec daily life, and why their priests were especially important in the Aztec culture. The priests told the people how to behave so they would not anger their gods.\nAll Aztec priests were religious leaders. Some were active in government. Some acted as teachers. Some created the many records the Aztecs kept, written in hieroglyphics.\nSome priests conducted the many sacrifices needed to feed the hungry gods, to keep their gods happy. Both men and women were sacrified, but there were many more men than women. That's because a great many people sacrified were enemy warriors captured in battle. But if the Aztecs ran out of captured people to sacrifice, or in times of great despair such as famine, they sacrificed their own people.\nThe Aztecs built temples to honor their gods. Temples provided a place for the music of worship, a place for the private ceremony of personal bloodletting, and a place to conduct the many human sacrifices that the Aztecs believed were necessary to keep their gods happy. Some temples were huge structures. The Great Temple of Tenochtitlan could fit 8,000 people in its plaza at one time. Some were small shrines where priests went to talk to the gods to keep themselves on the right path.\nEverything the Aztecs did, they did to honor their gods or to make their gods happy. They truly believed if they did not keep their gods happy, the world would end."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:31e23ee0-f9ba-4d29-ad72-ad21ce4f8d39>"],"error":null}
{"question":"What role do internal audits play in organizational control, and how does IT security complement these oversight functions?","answer":"Internal audits assist management in implementing and monitoring organizational controls, serving as a subsystem within the accounting system to ensure financial integrity. IT security complements this oversight by enforcing task separation, implementing employee monitoring strategies, establishing incident response protocols, and ensuring data integrity through specific measures like software configuration control, hardware configuration management, and monitoring of system privileges to prevent unauthorized access and abuse.","context":["Research papers on internal controls\nExcerpt from research paper : implicit in the use of these internal controls is keeping strategic plans and initiatives on track this focus on continual alignment of strategies to their objectives through the use of internal controls is what separates those organizations attaining success with their supply chains or not. Internal controls can be thought of as a sub-system within the accounting system (internal controls, 2007) internal controls offer guidance, practices and procedures that the accounting system. Measuring the effectiveness of an internal control system by dr david brewer and william list, ca, hon fbcs methodology by which management can measure the effectiveness of the organisation's internal control system (ics) in addition the paper proposes a methodology for recording measuring the effectiveness of an internal control. Internal controls are methods put in place by a company to ensure the integrity of financial and accounting information, and to protect the organization's assets from misappropriation by internal sources (investopedia, 2013.\nInternal control is the responsibility of the management of organisation or corporation the internal audit division assists management in discharging its responsibilities in terms of the implementation and monitoring of. These controls come in the form of internal and external controls for the system the internal controls are the checks that are placed in the system my the company's own management and directors today more and more companies are moving from the manual accounting systems to computerized accounting information systems. Internal controls required by sarbanes-oxley act of 2002 (sox) the first research question asks whether better internal controls can enhance firm performance.\nConduct a survey on reporting on internal control by public sector entities – the results of the survey have been presented in this paper the aim of the survey is to establish the extent to which internal control reporting by public sector entities. The research paper assessed how lack of internal controls affected good corporate governance and aimed to bring out elements of good corporate the research defines internal controls and corporate governance structures and shows how the relationship can contribute towards improved operational procedures cartwright (1999) asserted that. Case studies for implementing coso’s internal control framework the institute of internal auditors research foundation (iiarf) is the global leader in providing research and knowledge resources to enhance the internal audit profession. View essay - research paper_reviseddocx from acct 531 at liberty university internal controls in accounting information systems internal controls in accounting information systems liberty. Financial management - internal controls this research paper financial management - internal controls and other 64,000+ term papers, college essay examples and free essays are available now on reviewessayscom autor: debraj chatterjee • may 21, 2018 • research paper • 1,283 words (6 pages) • 50 views.\nThis paper is from the sans institute reading room site reposting is not permitted without express written permission (poulin, 2014) target missed internal alerts and found out about the breach when they were contact ed by the department of justice (elgin, 2014) the critical controls (sans institute, 2014c) are recommended that. Factors influencing the effectiveness of internal audit in the public sector this 10 page paper considers different influences which will impact on the effectiveness of an internal audit in the public sector factors discussed include the general accounting system in the public sector, internal controls, relationship with external audit, relationship with management and management support. Control system faced by management in businesses (institute of internal auditing research foundation, 2003) today, in businesses worldwide, the internal control functioning become very important for their operation and. Full length research paper a study on components of internal control - based administrative system in secondary schools paitoon montri1, chaiyuth sirisuthi1 and preeda lammana2 1faculty of education, mahasarakham university, mahasarakham, thailand 2roi et wittayalai school, roi et, thailand. The purpose of internal controls in the accounting process - internal control is the specific procedures used within a company to safeguard its assets from employee theft, robbery, and unauthorized use and enhanc the accuracy and reliability of its accounting records by making fewer mistakes (weygandt, kimmel, and kieso, 2008.\nAccomplished, but advised that an evaluation of internal controls as both a review and test of system reliability (audit of the computer) would still need to be performed davis had a significant and positive effect on the evolution of audit theory and. Paper builds internal control framework analyzing the status of internal control and risk management of small and medium manufacturing enterprises, so as to provide a reference to small and medium manufacturing enterprises in china. Internal controls within a company fall under various categories in this paper i will discuss the establishment of responsibility, physical, mechanical, and electronic controls, segregation of duties, and independent internal verification. Accounting information systems research paper internal controls should be evaluated to determine whether they are operating effectively and to substantiate management’s assertion on the adequacy of the controls internal control testing and results should be documented, with deficiencies noted and remediation plans identified (conway.\nResearch papers on internal controls\nThe purpose of the system of internal accounting controls is to make sure a business is operating according to the policies and plans of management in regards with measures that have a direct relation to the protection of assets and to the reliability of accounting information. Journal of eastern europe research in business & economics internal control and the impact on corporate governance, in romanian listed companies internal control systems without an effective internal control system companies can confront with loses risk is. Internal controls and performance in non-governmental organizations: a case study of management sciences for health south sudan by abraham ayom ayom 002/0315/12352 a research report submitted to the faculty of business management in partial fulfillment of the. Quality assurance and quality control in research development peter bode b ensure that it has implemented appropriate internal quality control measures ensure that the data produced and reported are of known quality and uncertainty numerous papers have been.\n- New performance measurement and management control systems ted watts university of wollongong, new performance measurement and management control systems journal of applied accounting research, 13 (3), 226-241 new performance measurement and management control systems classification research paper 4 new performance measurement and.\n- International journal of scientific and research publications, volume 6, issue 6, june 2016 600 issn 2250- 3153 wwwijsrporg a study on effectiveness of the internal control system.\n- Since internal control is a very important aspect of the function of management research paper on internal auditing there is the need to have an in depth study of the subjectmany studies have already been done on the subject, therefore the researcher seek the review works already research paper on internal auditing done on the subject as this.\nAbstract we investigate whether ineffective internal control over financial reporting has implications for firm operations by examining the association between inventory-related material weaknesses in internal control over financial reporting and firms’ inventory management. The usage of accounting information systems for effective internal controls in the hotels the usage of accounting information systems for effective internal controls in the hotels nomsa mndzebele the research seeks to find out if the usage.","Where are you most at risk?\nIs your IT security as effective internally as it is externally?\nTypically, organizations understand the need for outward/public-facing IT Security. However, IT Security as a means of preventing insider attacks can often be overlooked or relegated to a lower priority\nWhat can IT Security do to prevent, detect, and mitigate against insider threats?\nImplement simple and effective prevention and monitoring measures pertaining to:\n- Account creation\n- Shared accounts\n- Account expiration\n- Workstation inactivity\n- Unauthorized use\n- Disabling accounts upon separation\n- Disabling remote access upon termination\n- Access control\n- Password management\nEnforce task separation (Who watches the watchers?)\nImpose company device/BYOD technology management:\n- Personal use policy\n- Return of company equipment upon separation\nEstablish appropriate employee collection and monitoring strategies regarding:\n- Retention policies\n- Remote access monitoring\n- User authentication\n- Abnormal processes/Activities\n- Alteration of critical data\n- Exceptions/Expedited processes\n- Suspicious downloads\n- Encrypted traffic\n- Communication applications\nEnforce additional mitigation strategies related to:\n- Data Integrity\n- Software configuration\n- Hardware configuration\n- Device modification\n- Vulnerability exploits\n- Access to backup media\n- Account compromise\n- System privilege abuse\n- Theft of IP\n- Denial of services\n- Access to sensitive information\n- Access to out of scope information\nEstablish standard incident response protocols including:\n- Response to tech complaints\n- Disaster recovery strategies\n- Notification of employee separation\n- Termination procedures for trusted business partners\nRELATED BLOG POSTS\nAn employee was a former student at a public school district. During his time as a student, he shoulder-surfed the password of a school employee who had a privileged account. A few years later, the former student was able to log into the administrative system of the school district using the stolen credentials, which had not expired.\nThe school’s information system was hosted by a third party, who hosted many of the information systems of the public school districts in that same area. The employee then used the compromised account to log into the payroll information system of another school district, at which point he stole personally identifiable information (PII) of approximately 5,000 current and former district employees.\nUsing the stolen identities, he started applying for fraudulent credit cards and made fraudulent checks. He was caught when the rightful owners of the stolen identities began noticing strange account activity occurring in their names. The employee was finally arrested at a store where a clerk had noticed that a check he had tried to cash was fake. The former student was sentenced to 10 years in prison for 1st degree computer trespassing, identity theft, forgery, and an unrelated count of drug possession.\nWhat could IT security have done to reduce the risks posed by this employee?\n- Create a password expiration date for all employees.\n- Ensure trusted business partners have equivocal security measures in place so as not to become the attack vector of least resistance.\n- Ensure the organization has a policy to safeguard personally identifiable information from inadvertent and intentional disclosure."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:bdcdd8d1-832f-4520-a380-a59e87417cc0>","<urn:uuid:dca59aef-72df-489d-b428-3c9da4a90c58>"],"error":null}
{"question":"As a tea enthusiast interested in both production methods and ethical sourcing, what are the different types of tea processing techniques, and what efforts are being made to ensure fair working conditions in the tea industry?","answer":"Tea processing techniques vary by type: Green tea requires steaming fresh leaves for a minute or roasting in a skillet, followed by drying at 250 degrees. Oolong tea involves wilting leaves in sun for 30-60 minutes, then shade-drying for up to 10 hours with regular mixing. Black tea requires rolling leaves until they darken, then drying for 2-3 days. Regarding working conditions, major improvements are being implemented through the Malawi Tea 2020 program, which aims to achieve living wages by 2020. The program has already achieved a 20% reduction in the living wage gap, established the first collective bargaining agreement in the sector, and implemented nutrition programs providing fortified meals to 18,800 workers. The initiative also supports smallholder farmers, with 65% being women, through improved farming practices and establishment of Village Savings and Loans Associations.","context":["We have been hearing about the health benefits of tea for years now. Whether, black, green, or white, tea is a powerhouse of nutrients, antioxidants, and other compounds. And it tastes delicious and provides a little boost of caffeine and the comfort that only a warm beverage can bring. Numerous medical studies have found that tea contains compounds that exhibit anti-cancer properties. This does not mean that drinking tea will prevent anyone from getting cancer, but it certainly can’t hurt. Other potential benefits include supporting and maintaining weight loss, reducing the risk of heart disease, preventing diabetes, and lowering cholesterol.\nBefore modern science could tell us how wonderful tea is, people already seemed to know. Humans have been cultivating tea plants and drinking tea for so long that no one can really say when and where it first appeared. Most likely the plant was first grown by people in the region of southwest China, northeast India, Burma, and Tibet. What we know for sure is that tea as a beverage has been around for at least 5,000 years.\nWith the popularity of tea these days, it is not difficult to find it at any grocery store, nearly anywhere in the world. You can buy inexpensive black teas or pricey, organic loose teas and everything in between. But, what about making your own? It is definitely possible to grow tea plants, harvest the leaves, and create and brew your own teas right in your own garden or backyard.\nThe Tea Plant\nAll genuine tea comes from just one species of plant: Camellia sinensis. There are four different types of tea that can be made from the leaves of this plant and those are black tea, green tea, white tea, and Oolong tea. Any other “type” of tea is not really tea and comes from a different plant. There are two subspecies of the tea tree, which are Camellia sinensis sinensis and Camellia sinensis assamica. Sinensis sinensis has smaller leaves and grows in the cool, mountainous environment of places like China and Japan. The assamica variety is a taller plant and thrives in moist, low elevation, tropical locations like India and southwest China.\nThe tea plant is a shrub that grows to about one to two meters tall. In fall, it develops small white flowers that smell wonderful. The leaves are dark green in color and have a smooth, shiny texture. The fruits that follow the flowers are hard and small and the seeds are about one quarter of an inch in diameter. The assamica tea plant can grow up to 65 feet tall and produces larger leaves.\nFor growing your own tea at home, you will probably want to avoid the assamica plant and stick with using sinensis sinensis. The assamica is very large and requires a tropical environment. Sinensis is hardy through zone eight in the U.S. However, if you live in a colder zone, you can still grow a tea plant in a greenhouse or in a container that you move indoors in the winter.\nYou can grow your tea plant from seed or from a cutting taken from an existing plant. You may also be able to find a plant at a local nursery. If you are growing from seed, germination will take about four weeks. Cover the seeds lightly with soil and keep it damp and warm. When starting from seeds, be prepared to wait a few years to make tea. It takes at least three years to get a plant that will produce enough leaves for you to make a harvest. If you are using a cutting, nurture it indoors for a year before transplanting outdoors.\nUse soil that is slightly acidic. You can buy soil that is designated for rhododendrons to keep a tea plant happy. To maintain the right acidity for your tea plant, water with soft water only. If you are growing your plant in a container, give it a shot of fertilizer a couple of times in the summer, but otherwise it will be happy. Your plant will be happy in full sun, but it also tolerates shade. It will tolerate a drought better than many of your vegetables. In other words, growing tea is not very difficult!\nIf your location is warm enough, put your plant outside in the garden in a spot where it will get sun and maybe a little bit of shade. Put it up against a wall or tree to protect it from strong winds. If you are planting more than one tea bush, put them at least three feet apart from each other. Prune them back about every four years to keep the plants productive and to keep them from getting too big and too tall. If you take care of your tea plant, you will have tea for the next 50 to 100 years.\nYour tea plant will go dormant in the winter. When new shoots appear in the spring, this is called a flush. Pick the new growth, the two smallest leaves and the bud, for your tea. The warmer your climate is, the more flushes you will get per year, which makes a greenhouse a good idea for growing tea in colder areas. Once you have plucked your new growth, you have tea. The harvest is that simple. What you do with the harvest determines whether you make black, green, white, or Oolong tea.\nThe leaves that you harvest from your tea plant are tea. To get different flavors out of those leaves, you must prepare them differently. The directions for making green and white tea are the same. For white, however, use only the buds and not the leaves.\n- Green tea. Spread out your harvested leaves and buds and leave them in a shady spot for a few hours. Steam them on the stove top for about a minute or roast them in a hot, dry skillet for a couple of minutes for a different flavor. Dry the leaves on a baking sheet in the oven at 250 degrees for about 20 minutes. You can store the dried leaves in an airtight container, or brew right away.\n- Oolong tea. To make Oolong tea, let the leaves and buds wilt in the sun for a half hour to an hour. After that, leave them in the shade or inside for up to ten hours, with regular mixing. You may dry them in the oven at 250 degrees for 20 minutes. You can also treat Oolong tea in the traditional manner, which is to skip this step. Instead, roll the leaves into thin strips or small balls. They will unfurl as you brew them, but they won’t store as long as if you dry the leaves in the oven.\n- Black tea. After you pick the leaves and buds, roll them between your fingers and hands until they turn a darker color. Spread the leaves out on a flat surface in a cool place and leave them to dry for two to three days. Dry and store the leaves as you would for green or Oolong tea.\nAn herbal tea is any beverage that is brewed from a plant other than Camellia sinensis. There is a huge amount of variety in herbal teas, and there are many, many plants that you can grow, harvest, dry, and brew to make a tasty tea. Some of the more common plants that you can use for tea are:\n- Peppermint. Peppermint tea is made from the leaves and is very refreshing. It also settles the stomach. The plant is very easy to grow, but use a container or it will take over your garden.\n- Lavender. This fragrant plant makes a tasty and soothing tea that can reduce tension and relieve headaches. Use the buds to make tea.\n- Lemon verbena. Another easy herb to grow, the leaves of lemon verbena make a zingy, lemony, and refreshing drink. Grow it in a container and bring it inside for the winter.\n- Rose hips. They are not exactly an herb, but the seed cases on your rose bushes make a tart and delicious tea. They are also very high in vitamin C.\n- Bergamot. Bergamot has a slightly citrus flavor. You can use the flowers and leaves as tea or add it to your black tea to make a homemade Earl Grey blend.\n- Chamomile. Chamomile plants are easy to grow and produce lovely, daisy-like flowers. When dried, these flowers make a tea that is calming and often used to induce sleep.\n- Jasmine. The flowers of the jasmine plant have an intoxicating smell and can add a fragrant flavor to your tea, especially green and white tea. This is a warm weather plant, so if you have cold winters, grow it in a container and bring indoors. It also requires a trellis or other type of structure on which to climb.\n©2012 Off the Grid News","Although tea sector jobs in Malawi are considered good, paying more than the Malawi national average and providing a range of benefits, living standards for tea workers are poor. Smallholder tea farmers in Malawi also struggle to make a decent living from tea – making it hard for them to provide for their families.\nHow ETP is Helping\nThe Malawi 2020 Tea Revitalisation Programme (Malawi Tea 2020) has been developed in response to concerns about low wages and poor living conditions in the Malawi tea sector.\nTogether with the Tea Association of Malawi (TAML), Oxfam, IDH (the Sustainable Trade Initiative), and GIZ (German Development Agency), we’re leading a coalition of stakeholders from across the entire tea value chain to create a competitive Malawian tea industry where workers earn a living wage and smallholders are thriving.\nDuring the build up to the TEAM UP 2015 tea sustainability conference, all key value chain stakeholders of the Malawi tea industry signed an MoU committing their support to the Malawi Tea 2020 plan, which was officially launched at the conference.\nThe programme, which is the first of its kind in the tea industry, aims to deliver six key outcomes:\n- An industry that is investing in its future and its workforce\n- Significant improvement in wages and benefits for workers – supply chain commitment to a living wage by 2020\n- Improvements in smallholder farming practices, yields, quality, income, and income diversification\n- A healthier, motivated, and productive workforce, with greater opportunities for women\n- An improved wage-setting process with greater worker representation\n- Sustainable energy use and improved environment\nThe five-year Roadmap (above) shows the programmes that together will deliver the changes required. After the first year significant progress has already been made in key areas:\n- Worker wages: Producers have raised wages several times since the programme started and an evaluation by independent wage experts showed that, despite the high inflation in Malawi, the gap between current wages and living wage has reduced by 20%.\n- Wage-setting processes: The first ever collective bargaining agreement in the tea sector was signed, between the Tea Association of Malawi and the Plantation Agricultural Workers Union.\n- Nutrition: 18,800 workers are benefitting from improvements in the lunchtime meal provided by estates, which are now fortified with essential minerals and vitamins. In addition, all tea estates are providing fresh vegetables once a week. The fresh vegetables are from the estates own gardens or local markets. Some estates are creating business opportunities for women to engage in growing vegetables on the estate.\n- Smallholder farming: Support programmes to improve quality and productivity and farmers’ incomes have been extended, this year reaching almost a fifth of Malawi’s smallholder farmers – 65% of whom are women. 13 smallholder nurseries have been established and over 150,000 new tea plants have been cultivated.\n- Community saving: 83 Village Savings and Loans Associations have been established benefitting over 2,000 farming families.\n- Competitiveness of the industry: The innovative finance fund to support the industry make investments in its long-term future now stands at €1 million and companies are accelerating replanting with higher quality clones and investment in irrigation.\nWhat the experts say\nLiving wage experts Richard and Martha Anker, who are part of the programme’s independent wages committee, and carried out the initial living wage benchmark said: “We have seen a noteworthy increase in daily wages, which will make a significant difference in the lives of the tea workers in Malawi. To afford a decent standard of living for a tea worker and family, wages will need to increase further but a significant step forward has been made.”\nRachel Wilshaw, Ethical Trade Manager at Oxfam said: “In 2013, a joint report by Oxfam, Ethical Tea Partnership, and IDH showed that the approximately 50,000 tea pluckers in Malawi were trapped in poverty, despite receiving the legally required wage and some in-kind benefits. Malawi Tea 2020 is a rare coalition of parties that have the collective power to bring about positive change in the industry. We welcome the progress made in the last year on wages, nutrition, and worker representation, particularly in the face of a devastating drought caused by the El Niño effect. Nevertheless, these vulnerable workers need all companies, retailers, buyers, and producers to step up further if a living wage is to become the norm by 2020.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b8e84ab3-e891-4576-b7cb-433bd701dd60>","<urn:uuid:e2d29f16-f3f5-45c3-b661-6719136e6ddd>"],"error":null}
{"question":"How do leadership challenges manifest in both architecture firms and job interviews?","answer":"In architecture firms, leadership challenges appear when design principals give competing or contradictory directives to project teams, causing confusion and decreased productivity. This can be resolved by establishing clear office hierarchy and designating one design executive per project. In job interviews, leadership abilities are assessed through behavioral questions like 'Tell me about a time when you influenced the outcome of a project by taking a leadership role?' Candidates must demonstrate their leadership skills by describing specific situations where they facilitated team efforts, including their contribution and the concrete results achieved.","context":["Effective Interviewing Guide\nBehavioral interviewing is based on the concept that past behavior is the best predictor of future behavior. The interviewer wants specific examples of when and how you demonstrated particular behaviors. The candidate must describe in detail an experience and how you dealt with this situation. In answering behavioral questions, you should strive to provide examples which best demonstrate that a particular positive behavior is Long-standing. Behavioral questions usually begin with:\n- “Tell me about a time when you…”\n- “Give me an example of a time when you…”\nPrior to the interview each position is assessed for the skills/competencies and characteristics that relate to job success. Then interview questions are developed to probe in to each success factor. All candidates are asked the same questions and notes are taken to use in evaluating the candidates.\n- To prepare for each interview, review each job description carefully, identify the skills and traits most likely to be assessed. Next, formulate accomplishment “stories” to illustrate the skills and traits you are likely to be asked about.\n- Make sure that your stories follow the STAR format. First, describe the Situation you faced. Then describe the Tasks that you felt needed to be accomplished. Third, describe the Actions you took. Finally, discuss the Results you obtained. Use only enough detail so that the interviewer can understand the degree to which you exercised your skills. Be prepared for questions asking about negative scenarios. When you answer these, add a fifth step, which is to state what you learned from the experience.\n- Develop two stories each for about 5 different skills so that you have enough to handle other skills that might be requested. You can always change a story slightly to highlight a different skill. Make sure your story is about a specific situation. Don’t say “Well I’ve used my leadership skills in a lot of situations.”\nSample Behavioral Interview Questions\nThe following are examples of three common MBA skills with corresponding questions and strategies for handling.\nQ: Tell me about a time when you had to present complex information to a customer or peer. How did you ensure that the other person understood?\nQ: Tell me about a time when you successfully persuaded a group to use your idea?\nA: Prepare a list of examples that stress your communication skills and develop accomplishment statements for each example. “In my position at Acme Software Co., I developed a user manual to accompany the product. I examined the best features from three top sources and adopted a very user-friendly format. After writing each chapter I conducted numerous checks and conducted a final test to ensure that the manual was highly readable and easy to update. The customer feedback has been excellent and sales are 10% above our projection.”\nAnalytical/Problem Solving Skills\nQ: How do you gather information to analyze problems? Give me an example.\nQ: What was the most challenging work or technical problem you ever encountered? What happened? What did you do?\nA: Prepare examples of problems you have solved. For each, outline the main 5-7 steps involved in solving the problem, the alternatives you considered and results you obtained. Practice until you can relate the information naturally and conversationally.\nQ: Tell me about a time when you influenced the outcome of a project by taking a leadership role?\nQ: Describe your leadership style and give me an example of a situation where you successfully lead a group.\nA: Review your past for an experience that involved working as part of a team and think about your role in facilitating, leading or contributing to the effort. Once again, use the accomplishment approach to describe the goal/situation, your contribution and the result.","3 Common Communication Problems in Architecture Firms and How to Fix Them\nCommunication in architecture is overwhelmingly associated with the visual: models, drawings, and renderings.\nWhile this is neither surprising nor unusual given the inherent physical quality of architecture, architects would also do well to recognize that perfecting non-visual communication is crucial for a productive architectural practice.\nCommunication serves to relay an idea, thought or news from one person or group to another, so when the lines of communication fail, so do the conveyance of that concept, resulting in a lack of clarity.\nIn the realm of an architecture office, communication is commonly sub-optimized within project teams, between executive and staff architects, and between the architect and client. Discussed below are three major communication problems that cover each of these relationships and suggestions for how to mitigate them:\nInternal Project Team Communication\nWhen collaborating with colleagues within an architecture practice, it is common for multiple members of the project team to be working on the same project files throughout the project duration, especially the AutoCAD files.\nWhile this may simplify file management in general when there are fewer files to maintain, file sharing can present a fair share of complications because not everybody automatically works within a file in the same way. For example, not all members of the team may assign linework to the same layers, dimension consistently or document as thoroughly as another team member.\n“You cannot put a price tag on how important internal communication is.”\nAdditionally, when architectural team members don’t communicate about their process or progress when working on the same project file, it may not be apparent what work has already been completed, and others may not understand how their colleague approached a task.\nThis makes it difficult to track production and also wastes time, as project team members then must navigate within the drawing file themselves to see what changes or updates have been made and whether they were made accurately; a team member may even end up re-doing or duplicating work that has already been done.\nPoor internal communication with regard to file usage results in inconsistency, confusion, decreased efficiency, and frustration. To prevent or resolve these issues, each architectural project team in coordination with the architectural executive should intentionally take time at the beginning of a project (and episodically throughout) to discuss and outline specific standards for project file usage. The entire project team should also describe the protocols for daily reporting among colleagues and actively commit to adhering to them.\nExecutive Management to Team Communication\nA communication issue common in boutique and larger architecture firms alike is that of competing and sometimes contradictory design ideas and directives.\nThough a goal of most architectural practices is to have a cohesive vision when it comes to design, in reality not every company’s design principals share a unified opinion.\nWhether discrepancies in design opinions occur at a small or large scale, project teams can get caught in the middle of the resulting conceptual tug-of-war. The team may have precisely created the design documents in response to a request by one design principal and a second design principal may have given a different idea because of his dissatisfaction with the resulting work – and perhaps have asked for a rework.\n“The building industry has a poor reputation for the manner in which its individuals communicate with one another.”\nAnother similar scenario is when multiple design principals provide inputs during a discussion with the project team about the design or a particular assignment without coming to a clear resolution, yet expecting that the project team understands which inputs to be implemented.\nIt can be physically and emotionally exhausting for both executives and the project teams if this dichotomy in communication perpetuates, and it also undermines the productivity of the architecture office.\nSetting a clear design direction for architectural projects and establishing an office hierarchy structure are paramount to setting up for success. Executive management can create this clarity in their practice and for their project teams by identifying one design executive per project that the team will look for direction and answers. That one design executive should be entrusted to lead the team and company either by his or her vision or by upholding the previously-agreed-upon collective vision of all of the design executives.\nArchitect to Client Communication\nClients are the essential ingredient for implementing architectural ideas, so it’s natural for architects to want to cater to their customers’ wishes in every way imaginable.\nHowever, in communicating with clients, architects do not always accurately guide their clients’ expectations, and often tend to overpromise. This could be with regard to the entirety of the architectural project or just particular aspects, most commonly schedule, budget, materials or furnishings.\nAs a result, clients can become frustrated with the progress of the project if it does not correspond with expectations, and discouraged by the performance of the company, which is detrimental to future business. Distrust can develop between the client and the architectural company, turning the relationship sour. In turn, the architectural team grows frazzled under pressure to maintain the client relationship and deliver on the commitment made.\n“A weak or eroded sense of trust can harm your reputation, cost you future business, and even drive clients toward litigation.”\nTherefore, it is essential to the success of the architect-client relationship for architects to communicate the realistic expectations based on the capability of the architectural office and what is involved in carrying out the client’s project.\nThis necessitates that architects are aware of the realities of practice to begin with, so paying attention to the performance of one’s company is critical.\nFinding a balance between the client’s perception and ideals and the actuality of their project is a delicate but necessary responsibility of the architect, and is vital to the success of the project, business, and the company-customer relationship."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:e3dc7f24-90f5-4910-9e11-dc3ee2367da4>","<urn:uuid:d2da1ed8-3e96-42bf-9fb4-63b4334c2d78>"],"error":null}
{"question":"How do urban green spaces affect allergies, and what environmental factors are intensifying their impact?","answer":"Urban green spaces can significantly contribute to allergies due to plant selection practices, particularly 'botanical sexism' where male plants are preferred due to concerns about female trees producing fruit. This bias leads to massive amounts of pollen being released in cities. The impact of these allergies is being intensified by climate change, which has increased both the intensity and duration of pollen seasons. Carbon dioxide increases have led to higher plant reproduction and pollen levels, with models predicting ragweed pollen will increase fourfold in the next 30 years. Additionally, more frequent thunderstorms due to rising sea temperatures increase ground-level pollen grain concentrations, while increased air pollutants may be partially responsible for the rise in allergic respiratory diseases in industrialized countries.","context":["Urban green spaces provide several benefits beyond aesthetics. They offer shade, help to reduce pollution, offer habitat for birds and insects, a space to meet and socialise or for kids to play. Not surprisingly the choices of plants used to populate these areas are made with these considerations in mind, unfortunately, their allergenicity is not.\nFor those without serious allergies, they may appear to be a small annoyance. In fact, allergies are a serious public health problem causing asthma, rhinitis and skin conditions and they disproportionately affect children. It also appears that this is, in part, a self-inflicted wound, spurred on by the way in which we designed our living spaces.\nThe clearest example of this is botanical sexism, a term coined by horticultural expert Thomas Orgen. It refers to the tendency for urban planners to use male plants because of concerns about the fruit produced by female trees: concerns like higher maintenance, the attraction of pests and the littering of streets with fruit.\nThis bias toward male species leads to massive amounts of pollen, the male reproductive organ of plants, being released into our cities. Orgen studied and campaigned for this issue for nearly 40 years, writing books and developing the first rating system for measuring the potential of a plant to cause allergic reactions in humans, the Orgen Plant Allergy Scale.\nThis idea of quantifying allergenic potential has now been extended to green spaces too with the Index of Urban Green Zones Allergenicity (IUGZA). “The index was created with the aim of assessing the risk of allergic symptomatology in a green area as a function of the plant species that grow in them,” explains Paloma Cariñanos, a botanist at the University of Granada who, along with colleagues, developed the index in 2014.\n“To calculate the IUGZA, an exhaustive inventory of species is carried out and each plant is assigned values for a series of biological attributes related to allergenicity.” The index parameters consider things like the pollination strategy of the plant, the duration of its flowering and the intrinsic capacity of its pollen grains to generate an allergic response. With this information each species is then assigned an Allergen Potential Value.\nRecently Cariñanos applied the index to greens spaces in 23 cities located in six Mediterranean countries and was able to show that several native ornamental species are some of the main allergy culprits. She also found that bioclimatic characteristics of the territory and design aspects, such as the density of trees and the number of species used were the variables most likely to impact the IUGZA.\nCariñanos stresses however that this work highlighting the causal and aggravating factors of allergies in the population should not be viewed in a negative light. “It is not our intention to generate public alarm or devalue the positive role that parks and gardens have in the urban ecosystem and on the welfare of the population,” she says. “Instead, it is necessary to emphasise this disservice that affects so many people and that generates social, economic and environmental costs.”\nAreas for improvement according to Cariñanos are the incorporation of exotic species whose allergenicity has not been reviewed, increasing the diversity of species used and, of course, ending the bias toward female plants.\nOrgen agrees saying, “if there isn’t a high number of female plants in a landscape, you’re going to have a problem.” However, he also points out that things like climate change are having an effect too. “The pollen season is starting earlier and lasting longer,” he explains. “I’m seeing certain species that in the past only bloomed once a year but now are blooming twice a year and this makes it more important than ever to pay attention to what we’re planting.” He also recommends specific changes in the nursery industry such as allergy tagging the plants they sell and an overall better awareness of the issue.\nIn terms of solutions, Cariñanos admits that implementing the changes highlighted by her and others work is not easy, but she sees some encouraging signs. “I know some European political parties have measures to reduce the impact of allergens on the quality of life in their electoral programs.”\nThe issue is also taken into account by the researchers of the EU project Urban GreenUP, which is studying innovative nature-based solutions aimed to renature urban landscapes worldwide. In Valladolid, Spain, one of the demonstration sites, they claim they can reduce “allergy by 50% in the long term both in intensity and duration”. More info here.\nyouris.com provides its content to all media free of charge. We would appreciate if you could acknowledge youris.com as the source of the content.","The changes in the environment and biodiversity brought on by climate change could be responsible for increases in allergies, autoimmune diseases and autism, according to a Rutgers researcher\nClimate change and disruption of the ecosystem have the potential to profoundly impact the human body. Xue Ming, professor of neurology at Rutgers New Jersey Medical School, who recently published a paper in the International Journal of Environmental Research and Public Health on the effects of climate change on allergies, autoimmunity and the microbiome — the beneficial microorganisms that live on and inside the human body — discusses how the delicate balance of the environment affects conditions such as allergies, autism and immune disorders.\nHow has climate change affected respiratory allergies?\nClimate change has worsened respiratory allergic disease and has altered the immune system’s tolerance in responding to toxins, which has led to an increase in the prevalence of immune diseases. People with chronic respiratory allergic disease that affects the nose and eyes, such as asthma and allergies, are at particular risk due to increased exposure to pollen and the increased concentration and distribution of air pollutants.\nAccording to the American Academy of Allergy Asthma & Immunology, climate change has both increased the intensity of the pollen season as well as prolonged its duration. Increases in carbon dioxide were shown to lead to an increase in plant reproduction and total pollen levels, especially those plants that thrive at high carbon dioxide concentrations. For example, ragweed pollen has been increasing in concentration, with models predicting that levels will increase by four times within the next 30 years.\nThunderstorms, which have become more frequent due to rising sea temperatures, have been found to increase concentrations of pollen grains at ground level. After absorbing water, these grains can rupture and release allergenic particles that can induce severe asthmatic symptoms in patients with asthma or hay fever.\nClimate change has also been linked to increased concentrations and distribution of air pollutants such as ozone, nitric oxide and other volatile organic chemicals. There is a growing body of evidence suggesting that these airborne environmental pollutants may be partially responsible for the substantial increase in allergic respiratory disease seen in industrialized countries over the past several decades.\nHow do changes to the ecosystem affect allergies and respiratory disorders?\nDeforestation and over-logging have led to a dramatic decrease in the diversity of plant species. As one species of plant becomes extinct, new species emerge to take their place. For example, as oak trees have been excessively harvested for architectural purposes, new species of trees have emerged. With these new trees come new forms of tree pollen, which are inhaled and ingested by humans on a daily basis.\nSimilarly, widespread pesticide use has altered the profile of insects, invertebrates and microorganisms with which we come into contact with through our soil and vegetation. As the environment is altered, our bodies are bombarded with novel organisms. The molecules which make up these organisms — known as antigens — are recognized as “foreign” by our bodies and create an inflammatory response.\nHow might a loss of biodiversity due to climate change affect non-respiratory diseases?\nAccording to the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services, biodiversity is declining faster than at any time in human history, with nearly 1 million animal and plant species are threatened with extinction due to climate change.\nThe loss of biodiversity related to climate change may affect the microbiome, potentially leading to inflammatory, autoimmune and neurologic diseases. Immunologic disorders, such as food allergies, are on the rise. For example, several studies have found that increases in carbon dioxide and temperature are correlated with changes in the composition of the peanut, making it more difficult for the body to adapt immunity.\nCould disturbances in gut bacteria affect the autism rate?\nDisruption of gut bacteria has been linked to neurologic diseases such as multiple sclerosis, autism and Parkinson’s disease. In my own research, I found abnormal amino acid metabolism, increased imbalance between free radicals and antioxidants in the body, and altered gut microbiomes among some patients with autism spectrum disorder.\nWhat steps can be taken to minimize the health risks brought on by climate change?\nWe must end the destruction of our natural environment, decrease emissions of greenhouse gases and adopt more “green” behavior. With research demonstrating links between the microbiome and autoimmune, inflammatory and neurologic diseases, it is critical that we minimize antimicrobial exposure. This may involve altering guidelines for the prescription of antibiotics by medical professionals. In addition, given that the microbiome is directly impacted by our daily environment it is important to regularly immerse ourselves in nature and familiarize ourselves with biodiverse surroundings."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:e4376d0d-0d39-4615-a567-36c598dbf515>","<urn:uuid:f5f4dab1-d6c9-4ae1-8c15-31eaefd5e769>"],"error":null}
{"question":"What types of system failures can affect web operations, and what liability risks do they create for businesses?","answer":"Web operations can be affected by various system failures, including web server crashes that make websites inaccessible or display error messages, and operating system crashes caused by hardware exceptions or internal consistency issues. These technical failures create significant liability risks for businesses, particularly those operating websites or handling sensitive customer data. Specific risks include content injury from intellectual property infringement, reputational injury from system failures, conduit injury affecting third-party systems, and impaired-access injury when customer systems become unavailable. Additionally, businesses face first-party expenses for privacy notification, forensic investigation, and crisis management if security is compromised.","context":["In computing, a crash, or system crash, occurs when a computer program such as a software application or an operating system stops functioning properly and exits. The program responsible may appear to hang until a crash reporting service reports the crash and any details relating to it. If the program is a critical part of the operating system, the entire system may crash or hang, often resulting in a kernel panic or fatal system error.\nMost crashes are the result of executing invalid machine instructions. Typical causes include incorrect address values in the program counter, buffer overflow, overwriting a portion of the affected program code due to an earlier bug, accessing invalid memory addresses, using an illegal opcode or triggering an unhandled exception. The original software bug that started this chain of events is typically considered to be the cause of the crash, which is discovered through the process of debugging. The original bug can be far removed from the code that actually crashed.\nIn earlier personal computers, attempting to write data to hardware addresses outside the system's main memory could cause hardware damage. Some crashes are exploitable and let a malicious program or hacker to execute arbitrary code allowing the replication of viruses or the acquisition of data which would normally be inaccessible.\nAn application typically crashes when it performs an operation that is not allowed by the operating system. The operating system then triggers an exception or signal in the application. Unix applications traditionally responded to the signal by dumping core. Most Windows and Unix GUI applications respond by displaying a dialogue box (such as the one shown to the right) with the option to attach a debugger if one is installed. Some applications attempt to recover from the error and continue running instead of exiting.\nTypical errors that result in application crashes include:\n- attempting to read or write memory that is not allocated for reading or writing by that application (segmentation fault) or x86 specific (general protection fault)\n- attempting to execute privileged or invalid instructions\n- attempting to perform I/O operations on hardware devices to which it does not have permission to access\n- passing invalid arguments to system calls\n- attempting to access other system resources to which the application does not have permission to access\n- attempting to execute machine instructions with bad arguments (depending on CPU architecture): divide by zero, operations on denormal number or NaN (not a number) values, memory access to unaligned addresses, etc.\nCrash to desktop\nA \"crash to desktop\" is said to occur when a program (commonly a video game) unexpectedly quits, abruptly taking the user back to the desktop. Usually, the term is applied only to crashes where no error is displayed, hence all the user sees as a result of the crash is the desktop. Many times there is no apparent action that causes a crash to desktop. During normal function, the program may freeze for a shorter period of time, and then close by itself. Also during normal function, the program may become a black screen and play the last few seconds of sound (depending on the size of the data buffer) that was being played repeatedly before it crashes to desktop. Other times it may appear to be triggered by a certain action, such as loading an area.\nCrash to desktop bugs are considered particularly problematic for users. Since they frequently display no error message, it can be very difficult to track down the source of the problem, especially if the times they occur and the actions taking place right before the crash do not appear to have any pattern or common ground. One way to track down the source of the problem for games is to run them in windowed-mode. Windows Vista has a feature that can help track down the cause of a CTD problem when it occurs on any program. Windows XP included a similar feature as well.\nWeb server crashes\nThe software running the web server behind a website may crash, rendering it inaccessible entirely or providing only an error message instead of normal content.\nOperating system crashes\nAn operating system crash commonly occurs when a hardware exception occurs that cannot be handled. Operating system crashes can also occur when internal sanity-checking logic within the operating system detects that the operating system has lost its internal self-consistency.\nSome operating systems, e.g., z/OS, have facilities for Reliability, availability and serviceability (RAS) and the OS can recover from the crash of a critical component, whether due to hardware failure, e.g., uncorrectable ECC error, or to software failure, e.g., a reference to an unassigned page.\nSecurity and privacy implications of crashes\nDepending on the application, the crash may contain the user's sensitive and private information. Moreover, many software bugs which cause crashes are also exploitable for arbitrary code execution and other types of privilege escalation. For example, a stack buffer overflow can overwrite the return address of a subroutine with an invalid value, which will cause a segmentation fault when the subroutine returns. However, if an exploit overwrites the return address with a valid value, the code in that address will be executed.\nWhen crashes are collected in the field using a crash reporter, the next step for developers is to be able to reproduce them locally. For this, several techniques exist: STAR uses symbolic execution, MuCrash mutates the test code of the application that has crashed, and EvoCrash performs evolutionary search.\n- \"Crashing Privacy: An Autopsy of a Web Browser's Leaked Crash Reports\". Retrieved 28 September 2018.\n- \"Analyze Crashes to Find Security Vulnerabilities in Your Apps\". Msdn.microsoft.com. 26 April 2007. Retrieved 26 June 2014.\n- \"Jesse Ruderman » Memory safety bugs in C++ code\". Squarefree.com. 1 November 2006. Retrieved 26 June 2014.\n- Chen, Ning; Kim, Sunghun (2015). \"STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution\". IEEE Transactions on Software Engineering. 41 (2): 198–220. doi:10.1109/TSE.2014.2363469. ISSN 0098-5589.\n- Xuan, Jifeng; Xie, Xiaoyuan; Monperrus, Martin (2015). \"Crash reproduction via test case mutation: let existing test cases help\": 910–913. doi:10.1145/2786805.2803206. Cite journal requires\n- Soltani, Mozhan; Panichella, Annibale; van Deursen, Arie (2017). \"A Guided Genetic Algorithm for Automated Crash Reproduction\": 209–220. doi:10.1109/ICSE.2017.27. Cite journal requires\n|Wikimedia Commons has media related to Computer errors.|","What is Cyber Risk?\nCyber Risk is first and third-party risk associated with e-business, the Internet, networks and informational assets.\nWho is at risk?\n- Business Owners who operate a website.\n- Business Owners who are concerned with their clients’ and employee’s information being compromised.\n- Business Owners who are concerned about copyright/trademark infringement.\n- Business Owners who are aware of the risks associated with computer hackers, viruses and other damaging computer programs.\n- Business Owners who understand the importance of upholding and preserving their professional reputation should an incident occur.\n- Business Owners who keep electronic records of clients names, addresses, phone numbers, social security numbers, credit card numbers and other sensitive information.\n- Business Owners who accept credit card payments.\n- Business Owners who may have employees that could compromise sensitive customer information or do something illegal to make some money.\n- Business Owners who use laptops, Blackberries or other portable devices that store client information.\nThird Party Liability\n- Disclosure Injury - Including lawsuits alleging unauthorized access to or dissemination of the plaintiff’s private information. (Can be extended to outsourced data processing and data storage services.)\n- Content Injury – Including suits arising from intellectual property infringement, trademark infringement, and copyright infringement.\n- Reputational Injury – Including suits alleging disparagement of products or services, libel, slander, defamation, and invasion of privacy.\n- Conduit Injury – Including suits arising from system security failures that result in harm to third-party systems.\n- Impaired-Access Injury – Including suits, civil fines and penalties arising from system security failure resulting in your customer’s systems being unavailable to its customers.\nFirst Party Cyber Crime Expenses\n- Privacy Notification Expenses – Including printing, drafting, postage, call center costs and advertisements, cost of credit-monitoring services, credit freezes and fraud alerts for affected customers (even when state law doesn’t require notification). Estimated at $30 per person.\n- Forensic Costs – Costs to determining how the breach occurred.\n- Crisis Management and Reward Expenses – Including the cost of public relations consultants to maintain the reputation of the business.\n- E-Business Interruption – Including first-dollar extra expense.\n- E-Theft and E-Communication Loss – Extended to networks outside of your company’s system.\n- E-Threat or Cyber Extortion - Including the cost of a professional negotiator and ransom payment to stop cyber attacks caused by malicious hackers.\n- E-Vandalism Expenses – Even when the vandalism is caused by an employee.\nA manufacturer hosted a site banner for a key vendor. The manufacturer was unaware that the vendor's slogan was similar to a slogan of a company based in France. The manufacturer was dragged into an international trademark infringement lawsuit. Claim Value: $700,000\nA chain of luxury hotels was expanding it's operations worldwide. They needed to upgrade their billing system to accomodate various currencies and tax rates. The chosen vendor upgrading the existing system, meeting all time requirements and milestones. However, during the final phase of installation, one of the installers accidentally erased $ 1.8 million of crucial data. As a result the customer sued the software installation company for the losses they incured. Claim Value: $1.8 Million\nDuring a national trade convention, the CFO of a prominent company read from a media kit about its products and those of competitors, including defamatory comments about the executive officers of a competitor. The competitor sued for libel and slander for $1.5 Million\nA bookseller created a Web site to promote itself. The Web site included passages from books. The publisher and author of one of the books quoted on the Web site sued the bookseller, alleging copyright infrinement and theft of intellectual property. The case settled for approximately $60,000. The bookseller incurred defense costss close to $35,000.\nA software development company was sued by one of its best customers after using the company's cost-estimating program. The custommer claimed that a defect in the software caused them to underbid several projects. After a lengthy investigation, the software was found free of any defect, and it was user error that caused them to underbid. The customer dropped the case after considerable legal expenses were incurred by the software developer."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:c0370aec-0f34-4e2f-8e97-1d3b0e65bf4e>","<urn:uuid:9240e847-a688-419f-945f-e0ba9a3a6134>"],"error":null}
{"question":"What roles does the secretory component play in IgA function, and how does IgA deficiency affect blood transfusions?","answer":"The secretory component (SC) serves two main functions in IgA: first, it's responsible for IgA secretion across mucosal membranes through its involvement in the uptake and transport process. Second, SC acts as a stabilizer of the IgA molecule, making secretory IgA more resistant to proteolytic enzymes compared to IgA without SC. Regarding blood transfusions, IgA deficiency poses a significant risk because 40-50% of IgA-deficient individuals develop antibodies against IgA itself, which can lead to serious reactions when receiving blood transfusions or blood products containing even small amounts of IgA.","context":["Immunoglobulin synthesis is the defining property of B lymphocytes and plasma cells. Rest-ing B lymphocytes synthesize only small amounts of immunoglobulins, which are mainly inserted into the cell membrane. Plasma cells, considered as end-stage cells arrested at the late G1 phase with very limited mitotic activity, are specialized to produce and secrete large amounts of immunoglobulins. The synthetic capacity of the plasma cell is reflected by its abundant cytoplasm, which is extremely rich in endoplasmic reticulum (Fig. 6.1).\nNormally, heavy (H) and light (L) chains are synthesized in separate polyribosomes of the plasma cell. The amounts of H and L chains synthesized on the polyribosomes are usually balanced so that both types of chain will be combined into complete IgG molecules, without surpluses of any given chain. The assembly of a complete IgG molecule can be achieved by 1) associating one H and one L chain to form an HL hemi-molecule, joining in the next step two HL hemi-molecules to form the complete molecule (H2L2), or 2) form-ing H2 and L2 dimers that later associate to form the complete molecule.\nThe synthesis of heavy and light chains is slightly unbalanced in favor of light chains. Free light chains can be effectively secreted from plasma cells and are eliminated in the urine in very small concentrations. When plasma cells undergo malignant transformation, this unbalanced synthesis of light chains may be grossly aberrant, which is reflected by the elimination of the excessively produced light chains of a single isotype in the urine (Bence Jones proteinuria). In contrast, free heavy chains are generally not secreted.\nThe heavy chains are synthesized, transported to the endoplasmic reticulum, where they are glycosy-lated, but secretion requires association to light chains to form a complete immunoglobu-lin molecule. If light chains are not synthesized or heavy chains are synthesized in excess, the free heavy chains associate via their CH1 domain with a heavy-chain binding protein, which is believed to be responsible for their intracytoplasmic retention. In rare cases, the free heavy chains are structurally abnormal and are secreted. Free heavy chains are usually retained in circulation because of their molecular weight—about twice that of light chains.\nPolymeric immunoglobulins (IgM, IgA) have one additional polypeptide chain, the J chain. This chain is synthesized by all plasma cells, including those that produce IgG. How-ever, it is only incorporated to polymeric forms of IgM and IgA. It is thought that the J chain has some role in initiating polymerization, as shown in Figure 6.2. IgM proteins are as-sembled in two steps. First, the monomeric units are assembled. Then, five monomers and one J chain will be combined via covalent bonds to result in the final pentameric molecule. This assembly seems to coincide with secretion in some cells in which only monomeric subunits are found intracellularly. However, in other cells the pentameric forms can be found intracellularly and secretion seems linked to glycosylation.\nSecretory IgA is also assembled in two stages, but each one takes place in a different cell. Dimeric IgA, containing two monomeric subunits and a J chain joined together by disul-fide bridges, is predominantly synthesized by submucosal plasma cells, although a minor portion may also be synthesized in the bone marrow. Secretory component (SC), on the other hand, is synthesized in the epithelial cells, where the final assembly of secretory IgA takes place. Two different biological functions have been postulated for the secretory component.\nFirst, SC is responsible for secretion of IgA by mucosal membranes. The process in-volves uptake of dimeric IgA, assembly of IgA-SC complexes, and secretion by the mu-cosal cells.\nThe uptake of dimeric IgA by mucosal cells is mediated by a glycoprotein related to SC, the polyimmunoglobulin receptor (Poly-IgR). Poly-IgR is constituted by a single polypeptide chain of approximately 95,000 daltons, composed of an extracellular portion with five immunoglobulin-like domains, a transmembrane domain, and an intracytoplas-mic domain. It is expressed on the internal surface of mucosal cells and binds J-chain–con-taining polymeric immunoglobulins.\nThe binding of dimeric IgA to poly-IgR seems to be the first step in the final assem-bly and transport process of secretory IgA. Surface-bound IgA is internalized and poly-IgR is covalently bound to the molecule, probably by means of a disulfide-interchanging en-zyme that will break intrachain disulfide bonds in both IgA and poly-IgR and promote their rearrangement to form interchain disulfide bonds joining poly-IgR to an α chain.\nAfter this takes place, the transmembrane and intracytoplasmic domains of the re-ceptor are removed by proteolytic cleavage, and the remaining five domains remain bound to IgA, as SC, and the complete secretory IgA molecule is secreted (Fig. 6.3).\nBasically the same transport mechanisms are believed to operate at the hepatocyte level. The hepatocytes produce poly-IgR, bind and internalize dimeric IgA reaching the liver through the portal circulation, assemble complete secretory IgA, and secrete it to the bile. Secretory IgA must also flow back to the bloodstream, because small amounts are found in the blood of normal individuals. Higher levels of secretory IgA in blood are found in some forms of liver disease, when the uptake of dimeric IgA backflowing from the gut through the mesenteric lymph vessels takes place, but its secretion into the biliary system is compromised. Under those circumstances, secretory IgA assembled in the hepatocyte backflows into the systemic circulation.\nAmong all J-chain–containing immunoglobulins, the poly-IgR has higher binding affinity for dimeric IgA. In IgA-deficient individuals IgM coupled with SC can be present in external secretions. It is believed that the same basic transport mechanisms are involved, starting by the binding of pentameric IgM to the poly-IgR on a mucosal cell and proceed-ing along the same lines outlined for the assembly and secretion of dimeric IgA. The fact that secretory IgM, with covalently bound SC, is detected exclusively in secretions of IgA-deficient individuals is believed to reflect the lower affinity of the interaction between poly-IgR and IgM-associated J chains (perhaps this as a consequence of steric hindrance of the binding sites of the J chain). Therefore, the interaction between IgM and poly-IgR would only take place in the absence of competition from dimeric IgA molecules.\nThe second function proposed for SC is as a stabilizer of the IgA molecule. This con-cept is based on experimental observations showing that secretory IgA or dimeric IgA to which SC has been noncovalently associated in vitro are more resistant to the effects of pro-teolytic enzymes than monomeric or dimeric IgA molecules devoid of SC. One way to ex-plain these observations would be to suggest that the association of SC with dimeric IgA molecules renders the hinge region of the IgA monomeric subunits less accessible to pro-teolytic enzymes. From a biological point of view, it would be advantageous for antibod-ies secreted into fluids rich in proteolytic enzymes (both of bacterial and host origin) to be resistant to proteolysis.\nCopyright © 2018-2020 BrainKart.com; All Rights Reserved. Developed by Therithal info, Chennai.","What Is IgA Deficiency?\nIgA deficiency is a defect in the immune system characterized by an inadequate amount of the antibody class known as immunoglobulin A (IgA). Antibodies are large proteins produced by the body to fight infections caused by invaders, such as bacteria, viruses, fungi, parasites, and other foreign agents. The immune system responds to the protein component (antigens) of these invaders by producing large quantities of specific antibodies, including IgA. These antibodies bind to the antigens on the surface of an invader (bacteria, for example) and prevent them from multiplying, thus halting the spread of disease in the body.\nIgA deficiency is the most common immunodeficiency disorder among whites, affecting roughly one in every 600 individuals of Caucasian descent. IgA is found in the saliva, mucous membranes, and intestinal secretions, where it may serve as the first line of defense against various sinus, respiratory, and gastrointestinal infections. Consequently, a deficiency of IgA heightens one’s vulnerability to such infections.\nPeople with IgA deficiency are also more susceptible to certain other disorders, including autoimmune diseases such as rheumatoid arthritis, systemic lupus erythematosus, and chronic hepatitis; respiratory allergies and asthma; chronic diarrheal diseases; and some types of cancer, such as squamous cell carcinoma of the lungs and thymoma (cancer of the thymus gland). However, many people with IgA deficiency are relatively healthy and do not suffer from these infections and disorders.\nIt is important to note that 40 to 50 percent of people with IgA deficiency develop antibodies to IgA itself. This can lead to serious reactions to blood transfusions or other blood products that contain small amounts of IgA. There is currently no cure for IgA deficiency, so treatment is aimed at easing symptoms and controlling associated diseases.\nWhat Causes IgA Deficiency?\n- IgA deficiency is most often a hereditary disorder.\nSymptoms of IgA Deficiency\n- Frequent or chronic infections of the sinuses, lungs, ears, upper respiratory tract, throat, eyes, or gastrointestinal tract\n- Mild to severe allergies (e.g., asthma and food allergies)\n- Diarrhea or abdominal cramps\nIgA Deficiency Prevention\n- There is no known way to prevent IgA deficiency.\nIgA Deficiency Diagnosis\n- The diagnosis is made by detecting low levels of IgA in the blood and the saliva.\n- Radioimmunoassay, ELISA, or automated laser nephelometry are performed to measure serum immunoglobulin concentrations.\nHow to Treat IgA Deficiency\n- Antibiotics are prescribed to treat associated bacterial infections. These should be taken for the full term prescribed.\n- Nonprescription pain relievers may be taken to treat minor pain and fever. Acetaminophen (not aspirin) should be given to children.\n- Patients with IgA deficiency should receive vaccines to protect them against flu viruses and pneumococcal pneumonia.\n- Measures must be taken to control underlying disorders that may be associated with IgA deficiency. These include malignant tumors, asthma, and autoimmune disorders (such as rheumatoid arthritis and systemic lupus erythematosus).\nWhen to Call a Doctor\n- Make an appointment with a doctor if you or your child develop frequent or persistent infections.\nJohns Hopkins Symptoms and Remedies: The Complete Home Medical Reference\nSimeon Margolis, M.D., Ph.D., Medical Editor\nPrepared by the Editors of The Johns Hopkins Medical Letter: Health After 50\nUpdated by Remedy Health Media"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:243df7dd-5156-4681-ac6d-aebb2301b250>","<urn:uuid:a20815a7-32b9-4ca8-9569-4b369b264e0a>"],"error":null}
{"question":"請問奇亞籽和魚油補充劑在omega-3來源上有什麼主要區別？","answer":"The key difference is in their forms and sources. Fish oil supplements derive omega-3s mainly from oily fish bodies, specifically containing EPA (Eicosapentaenoic acid) and DHA (Docosahexaenoic acid). Chia seeds, on the other hand, are plant-based and provide omega-3s in their natural seed form. While fish oil supplements can have side effects like burps, belching, stomach upsets, and bloating, there have been no serious negative consequences recorded from chia seed consumption.","context":["Let’s define what dietary fatty acids are. All fats are made of chains ofdouble or single bonded carbon atoms of varying lengthslinked with hydrogen atoms- to form fatty acid chains with a carboxyl group (C=O), thathas a hydroxyl group (O-H) attached at the end.\nThis discussion is on the most important dietary fatty acids required for healthy living. They are Omega-3, Omega-6 and Omega 9, fatty acids among others. In saturated fatty chains there are no vacant or unsaturated linkages and cannot be further hydrogenated. Double bonds as mentioned earlier are only in unsaturated fats. In monounsaturated fats –(MUFA)like olive oil has only one pair of double bonds are empty of hydrogen atoms and hence referred to as mono-unsaturated fatty acids, whilst in polyunsaturated fatty acids (PUFA) there are many more vacancies. To simplify further, let’s say you are getting into a compartment of the train. If there is one vacant seat in the compartment –then it is in a state of mono-unsaturation: if there is more than one vacant seat scattered all over then it would be apoly-unsaturated situation. If all seats are filled, then it would be in a saturated position like the saturated oils like coconut, dairy or meat fat.\nWe cannot use these dietary fatty acids in a haphazard way, they should be used in the right balance and proportion, as any imbalance may contribute to a number of chronic diseases.\nOmega-3 fatty acids are anti-inflammatory fatty acids whilst Omega-6 fatty acids are inflammatory fatty acids. One should consume more omega-3 type of fatty acids in the ratio of 3:1 with omega-6 types. Omega-9 fatty acids like olive oil have no limits- more the merrier. Mediterranean’s just pour ad lib on salads\nIt is observed that Americans eat more inflammatory omega 6 fats, while just 20 percent come from the anti-inflammatory omega 3. In Japan, that number is reversed 80 percent of their fats come from oily fish while only 20 percent are inflammatory omega 6 fats.\nOmega -3 fatty acids are polyunsaturated fatty acids like the Omega -6 type of polyunsaturated fats, but it’s called an omega-3 type because there is a vacant double bond in the chemical structure, three carbon atoms from the “omega “or tail end of the molecular chain.\nThese Omega-3 polyunsaturated fats are called “essential” fats meaning that your body cannot manufacture them and you need to get from your diet. Omega 9 is produced by the body but limited amounts.\nThe essential chemicals in omega-3 are Eicosapentaenoic acid (EPA) and Docosahexaenoic acid (DHA)\nEPA has 20- carbon atoms in the chemical chain, produces a chemical called Eicosapentaenoic acid.\nDocosahexaenoic acid is found inside meat of cold water fish such as Mackerel, tuna, halibut, salmon and herring Meat and eggs have low levels. Mother’s milk provides DHA in the breast milk.\nDHA is included in some infant formula to promote proper infant brain, eye and nerve developments. It is also given for migraine headaches, attention deficit hyperactivity (ADHD) and other nervous system dysfunctions.\nOmega 3 and 6 increase blood circulation throughout the body and prevents heart disease, lower triglycerides and blood pressure and can help with depression, mental health and arthritis. These fatty acids build healthy body cells, increases brain function, takes a critical role in cognitive development and learning protects against diabetes, cancer and lower blood cholesterol.\nIn fact, a recent and the most comprehensive review on omega 3 fats looked at 19 studies from 16 countries (including 45,637 participants) and found that those with the highest levels of omega 3 fats in their blood had lower risks of heart attacks.\nMakes sense how not getting sufficient amounts of these crucial fatty acids can profoundly affect your health.(EcoWatch)\nOmega-3 fatty acids are found mainly in the body of oily fish.It also comes from some green vegetables, and vegetable oils, walnuts. The benefits of fish oil supplements include overall healthier cardiovascular system and in the elderly lowers the risk of developing age-related macular degeneration which leads to loss of vision.\nSide effects of fish oil are –burps and belching, stomach upsets, bloating, indigestion and unpleasant taste in the mouth.\nUncommon side effects are-skin rashes, back pain, flu like symptoms and minor infections.\nIf you find that you are allergic to fish oil do not take supplements\nOmega-6 helps hair growth, skin health and the reproductive systems.\nOmega 9 fatty acids like Olive oil contain oleic acid, which is considered for heart health and preventing stroke. It also helps improve the immune system and lowers blood cholesterol.Health benefits are similar to omega-3 and omega – fatty acids.\nIn addition to finding in olive oil, omega-9 fatty acids are found in sesame oil, chia seeds, pecans, macadamia nuts and avocados.\nI am certain that most of you are on daily fish oil capsules.","Chia seeds are traditional herbs of Mexico and the US. For the Native Americans, legendary myths have reiterated the usage of this herb in the age of the Aztecs, Mayans and Incans. A member of the sage family Salvia Hispanica, the seeds of this plant was used as a staple food crop and was also used to prepare nutritious juices. The word ‘Chia’ in the Mayan language stands for strength and this food source was used mainly by the messengers who travelled large distances carrying a small packet of this energy source stored during the journey. It is also known as the ‘Indian running Food’ by many.\nThe tiny black and white seeds of Chia are rich in omega-3 fatty acids. They are the potent source of carbohydrates, fibre, protein, numerous vitamins like A, B, E, and D, and several minerals such as calcium, magnesium, phosphorus, potassium, iron, sulphur, iodine, zinc, copper, sodium, manganese, niacin, silicon, thiamine and antioxidants. One ounce of Chia seeds is proven to have 137 calories, 11 grams of fibre, 4 grams of protein, 12 grams of carbohydrate, 9 grams of fat and multiple other vitamins and minerals. Moreover, it is free of sugar which makes it even more delectable for fitness freaks. The advantageous part of consuming Chia seeds is that, it needs no processing unlike many and has a tasty flavour like nuts. The easiest and most popular way of consuming it is by mixing with water and gulping it down. It can help one to avoid food cravings for a long time and sustain the body energy at the same time.\nChia seeds have become popular gradually although, it is not yet much visible in the European consumerist market. Several recipes have also come up to use it in different dishes. It is even recommended to have at least a teaspoon of Chia seeds right before starting with the day. To start a good, zestful, energetic morning a spoonful of Chia with enough water to drink can boost your spirit up and help you retain energy through the odds of the day for a longer stretch.\nChia seeds are most commonly added to beverages and also used as garnishing ingredient over cooked vegetables, salads, cereals and other baked products. As a good substitute to food, chia seeds can work as the powerhouse of energy and prevent one from excess calorie intake. An ounce of Chia seeds in the whole day meal chart can work wondrous and cause weight loss successfully. It becomes even better a weight loss food due to its rich fibre content as well. Almost half the fibre value required from food intake in a single serving is provided by Chia seeds.\nThe essential fatty acids in these seeds are extremely necessary for our bodies to build up the multiple cells that are in constant constructive mode throughout the body . It regenerates good hair and skin and protects the heart.\nCalcium an essential nutrient for human beings is an unavoidable source for our survival. Right from birth our mother’s milk becomes the sole source of the calcium we need. Without calcium our entire skeletal structure would become fragile and porous. Malnutrition, a serious issue across the world has attacked millions of individuals who are deprived of the basic nutrition and women are the prime victims of this attack. Osteoporosis, a major issue with bones is more common for those without proper calcium nutrition. Chia seeds have more than 3 times the source of calcium than found in skimmed milk.\nThe reason Chia seeds makes one feel heavier with every time of consumption is because the seeds have the unique capability to trap moisture every time they are exposed to any kind of liquid. As a result of this, they can trap the water content inside their shell which makes them slippery from outside. The hydrophilic property of the seeds helps to absorb more than 12 times of water than its actual weight. When we consume it, the water inside each seed makes the actual quantity of the seeds more than what it is and we feel fuller. Moreover, a food that can help in good digestion is always pro weight loss program. The gel like effect of these seeds helps to segregate the carbohydrate from the digestive enzymes which in turn prevents the creation of sugar from the carbohydrate. Thus, the fat storage in the wrong body sections is prevented. The rich fibre content in the seeds helps in gradual digestion which keeps us away from untimely food intakes.\nJust like Garcinia Cambogia, it balances the blood sugar level in the body. While the seeds of chia substitute the food intake, Garcninia Cambogia does not work as a meal substitute. It rather stirs the secretion of serotonin level in our body that regulates our food cravings and moods from time to time.\nChia seeds also restore the omega- 3 fatty acids that yet again balance energy without much food consumption in real. Omega-3 is also good for improving digestion, and relieving few kinds of arthritis pain. Metabolism is the key to weight loss. Any food source that stimulates good metabolism should be encouraged, although few have their own lists of side effects as well.\nThere has been no serious negative consequence recorded from the use of Chia. Yet, in case of any doubt it is preferable to get a doctor’s advice before using it. In the recent time, it is used mainly by athletes to keep them strong and sturdy and fat free. Several pharmaceutical companies have gradually popularised the use of this product but it needs to be promoted more across the world. This natural multi nutrient food can save many malnourished sections of the society. It needs to be cultivated on larger scaled with mass production.\nNot only is it a good egg substitute or milk substitute, it is used in mainly preparing puddings, soups, salads, yoghurts and even making meatballs with bread crumbs. Its thickening agent is good for preparing many dishes. A little smash of lemon juice with chia seeds is a wonderful drink to gulp down during the strenuous hours of the day. Many find it better than flax seeds that need more care before consuming. It needs processing and then powdering which altogether is a time consuming process.\nChia seeds are a great way to start a day but it should not be forgotten that such supplements work better when we control our mind to eat more nutritiously along with these products and maintain a hygienic and healthy living."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:215dce9f-9a69-4e9e-a6c9-ad22be9a70ca>","<urn:uuid:20af008f-733a-487c-bbf4-b5dc10ff29a9>"],"error":null}
{"question":"Como dev de sistemas embebidos, necesito saber: ¿Cuáles son los beneficios de implementar guías de modelado y qué desafíos presentan para la ejecución en tiempo real?","answer":"The benefits of implementing modeling guidelines include ensuring modeling consistency, enabling model reuse, facilitating team integration, and meeting industry standards like ISO 26262 and DO-178C. However, when implementing these models in real-time execution, challenges arise because real-time programs must execute synchronously with actual time, causing computational delays that don't exist in simulation. This creates processing inefficiencies since sample intervals cannot be shortened or lengthened as they can during simulation, particularly affecting multirate systems that require careful handling of rate transitions.","context":["Embedded systems continue to become larger and more complex. At the same time, design teams are becoming more dispersed, both geographically and in terms of team members’ skills and experience. In this challenging development environment, implementing modeling guidelines to ensure modeling consistency is vital. Modeling guidelines establish a homogenous approach within the design team, making it easier to reuse the models for new projects. Guidelines also help new members of the team become familiar with your development process.\nFor some projects, implementing modeling guidelines is not simply a best practice; it is a requirement. Software systems deployed in high-integrity applications in aerospace and other industries must satisfy rigorous development and verification standards. Industry standards such as ISO 26262, EN 50128, IEC 61508, and DO-178C make modeling guidelines a prerequisite (Figure 1).\nThis article describes best practices for creating, implementing, validating, and promoting new modeling guidelines. The approach recommended will enable you to efficiently apply the standard within the team and ease the qualification of your modeling guidelines.\nDefining Rules: Considerations and Best Practices\nModeling guidelines can and should meet several requirements, including readability, design reuse, and problem-free exchange of Simulink® models. The modeling guidelines enable the verification not only of models and model objects but also other artifacts, including object properties and workspace variables.\nFor this reason a modeling guideline consists of a set of rules. Each guideline lists the checks applicable to that guideline and provides detailed recommendations for resolving issues.\nIt is important to define rules that can be clearly understood and automatically verified. If a rule cannot be verified automatically, you will need to weigh the importance of having the rule against the time required for its manual verification.\nTable 1 provides examples of how ambiguously worded or subjectively defined rules can be improved.\n|The model shall be readable||Subjective||Replace with “Each sheet of the model shall contain less than 50 blocks. Maximum depth of subsystems is 10.”|\n|The model shall generate code||Incomplete||Complete with “The model shall be compliant with Embedded Coder target ert.tlc.”|\n|The model should be optimized for efficiency.||Vague||Specify the model parameters that should be optimized.|\nTable 1. Sample rules with recommended improvements.\nBefore creating a new rule, consider using the rules available in Simulink, Simulink Check™, Embedded Coder®, IEC Certification Kit, and DO Qualification Kit. Most of the rules in these products are qualifiable and can be used in certification processes. By reusing these existing rules, you reduce the effort of creating your own modeling guidelines not only in the short term but also in the long term, as the rules are maintained for each new release of MATLAB.\nEach rule in your modeling guidelines should use a template that includes the following elements:\n- Level (mandatory or optional)\n- Application examples (at least one example that complies with the rule and at least one that violates the rule, preferably with a workaround to enable the design team to deal with it)\n- A reference to the individual responsible for implementing the check (for an existing rule)\nIn addition, the modeling guidelines should specify the version(s) of MATLAB and Simulink to which the rules apply. Verification options change from release to release. For example, in R2012b some verification tools support bus of arrays but not array of buses. By associating the rationale with the product version, you avoid including obsolete rules in your guidelines. The modeling guideline should also include references to all external standards documents.\nFigure 2 shows a sample guideline.\nImplementing the Modeling Guidelines\nThe Model Advisor in Simulink Check provides a framework for enforcing modeling guidelines. The Model Advisor checks a Simulink model for inconsistencies and for model objects that do not comply with the guidelines. Each guideline lists the checks applicable to that guideline and provides detailed recommendations for resolving issues\nThe tool proposes many checks that can be reused. Alternatively, you can create custom checks using an API available through Simulink Check. A Fix action can be associated to a check to correct any violations found.\nAll rules implemented in the Model Advisor should use the same template. For each failed check, include its location, the problem, and a suggested fix. It is a good idea to include a “See also” paragraph that links directly to the modeling guidelines documentation.\nGenerally, one rule defined in the modeling guidelines matches one check in Model Advisor. Each rule will be implemented in a separate MATLAB file. The Model Advisor API eases the formatting of the check by proposing which MATLAB functions to include, as well as where to include images, tables, and formatting. Figure 3 shows an example of a formatting check.\nThe Simulink or Stateflow APIs will be used mainly during the authoring of the check to verify the Simulink model. For example:\n% Look for all 'Inport' blocks using find_system function inportBlockList = find_system(bdroot,'BlockType', 'Inport'); % Get name of all Inport Blocks using get_param function inportBlockName = get_param(inportBlockList,'Name');\nTable 2 shows examples of functions from the Simulink API that you can use to create your own checks and verify some properties of the Simulink model.\n|find_system||Finds systems, blocks, lines, ports, and annotations|\n|get_param||Gets system and block parameter values|\n|bdroot||Returns name of top-level Simulink system (useful during the writing of the check)|\n|gcb||Gets pathname of current block (useful during the writing of the check)|\nTable 2. Sample API functions.\nNotice that some checks require an update of the Simulink model, as they need to manage data types or dimensions of signals or ports. A compiled model is very useful because it identifies all port data types and port dimensions for each block.\nThe check definition must use the following format to make it post-compile:\nrec.Title = 'Check if root inport name begins with \"in_\" prefix'; setCallbackFcn(rec, @checkRootInportName,'None','StyleOne'); rec.CallbackContext = 'PostCompile';\nA check that requires an update to the model starts with the character “^”. For ease of identification it is best to separate these checks from others (for example, by placing them at the end of the list of checks). Note that checks can share data for efficient computation using callbacks.\nValidating the Modeling Guidelines\nProvide failed and passed model examples for each rule. Keep these models as simple as possible to ensure that the check catches all violations.\nYou can use these models for the validation or certification of the modeling guidelines checks and for performing non-regression tests.\nFigure 4 shows a sample architecture for test models.\nOrganizing and Promoting the Modeling Guidelines\nThe Configuration Editor in Model Advisor enables you to organize and keep only the rules you want to share. The custom configuration of Model Advisor is contained in a MAT-file that can be shared with others.\nFigure 5 shows an example of modeling guidelines customized with the Configuration Editor. Notice that all unused checks have been removed from the Model Advisor interface.\nNow you are ready to deploy the modeling guidelines to your design teams. The first step is to create a package that includes the following:\n- Current version of your modeling guidelines\n- Implementation of the checks\n- Simulink models used to perform validation or non-regression tests\n- Custom configuration or MAT-file created by the Model Advisor Configuration Editor\n- MATLAB file\nsl_customization.mfor deploying the custom configuration\nIt is a good idea to apply a subset of the modeling guidelines at the beginning of the project. Phased implementation will make it easier for the design team to adopt the new guidelines. It will also enable you to set checks relevant to a specific stage in your development process.","Simulink® models run at one or more sample times. The Simulink product provides considerable flexibility in building multirate systems, that is, systems with more than one sample time. However, this same flexibility also allows you to construct models for which the code generator cannot generate real-time code for execution in a multitasking environment. To make multirate models operate as expected in real time (that is, to give the right answers), you sometimes must modify your model or instruct the Simulink engine to modify the model for you. In general, the modifications involve placing Rate Transition blocks between blocks that have unequal sample times. The following sections discuss issues you must address to use a multirate model in a multitasking environment. For a comprehensive discussion of sample times, including rate transitions, see What Is Sample Time?, Sample Times in Subsystems, Sample Times in Systems, Resolve Rate Transitions, and associated topics.\nThere are two execution modes for a fixed-step model: single-tasking and multitasking. These modes are available only for fixed-step solvers. To select an execution mode, select model configuration parameter Treat each discrete rate as a separate task. When you select this parameter, multitasking execution is applied for a multirate model. When you clear this parameter, single-tasking execution is applied.\nA model that is multirate and uses multitasking cannot reference a multirate model that uses single-tasking.\nExecution of models in a real-time system can be done with the aid of a real-time operating system, or it can be done on bare-metal target hardware, where the model runs in the context of an interrupt service routine (ISR).\nThe fact that a system (such as The Open Group UNIX® or Microsoft® Windows® systems) is multitasking does not imply that your program can execute in real time. This is because the program might not preempt other processes when required.\nIn operating systems (such as PC-DOS) where only one process can exist at a given time, an interrupt service routine (ISR) must perform the steps of saving the processor context, executing the model code, collecting data, and restoring the processor context.\nOther operating systems, such as POSIX-compliant ones, provide automatic context switching and task scheduling. This simplifies the operations performed by the ISR. In this case, the ISR simply enables the model execution task, which is normally blocked. The next figure illustrates this difference.\nTo generate code that executes as expected in real time, you (or the Simulink engine) might need to identify and handle sample rate transitions within the model. In multitasking mode, by default the Simulink engine flags errors during simulation if the model contains invalid rate transitions, although you can use the model configuration parameter Multitask rate transition to alter this behavior. Parameter Single task rate transition is available for the same purpose for single-tasking mode.\nTo avoid raising rate transition errors, insert Rate Transition blocks between tasks. You can request that the Simulink engine handle rate transitions automatically by inserting hidden Rate Transition blocks. See Automatic Rate Transition for an explanation of this option.\nTo understand such problems, first consider how Simulink simulations differ from real-time programs.\nBefore the Simulink engine simulates a model, it orders the blocks based upon their topological dependencies. This includes expanding virtual subsystems into the individual blocks they contain and flattening the entire model into a single list. Once this step is complete, each block is executed in order.\nThe key to this process is the ordering of blocks. A block whose output is directly dependent on its input (that is, a block with direct feedthrough) cannot execute until the block driving its input executes.\nSome blocks set their outputs based on values acquired in a previous time step or from initial conditions specified as a block parameter. The output of such a block is determined by a value stored in memory, which can be updated independently of its input. During simulation, computations are performed prior to advancing the variable corresponding to time. This results in computations occurring instantaneously (that is, no computational delay).\nA real-time program differs from a Simulink simulation in that the program must execute the model code synchronously with real time. Every calculation results in some computational delay. This means the sample intervals cannot be shortened or lengthened (as they can be in a Simulink simulation), which leads to less efficient execution.\nConsider the following timing figure.\nNote the processing inefficiency in the sample interval\nThat interval cannot be compressed to increase execution speed because,\nby definition, sample times are clocked in real time.\nYou can circumvent this potential inefficiency by using the multitasking mode. The multitasking mode defines tasks with different priorities to execute parts of the model code that have different sample rates.\nSee Multitasking and Pseudomultitasking Modes for a description of how this works. It is important to understand that section before proceeding here.\nSingle-tasking programs require longer sample intervals, because all computations must be executed within each clock period. This can result in inefficient use of available CPU time, as shown in the previous figure.\nMultitasking mode can improve the efficiency of your program if the model is large and has many blocks executing at each rate.\nHowever, if your model is dominated by a single rate, and only a few blocks execute at a slower rate, multitasking can actually degrade performance. In such a model, the overhead incurred in task switching can be greater than the time required to execute the slower blocks. In this case, it is more efficient to execute all blocks at the dominant rate.\nIf you have a model that can benefit from multitasking execution, you might need to modify your model by adding Rate Transition blocks (or instruct the Simulink engine to do so) to generate expected results."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:6c266f0f-fada-48b0-80a4-b3b2e1551798>","<urn:uuid:3c2d361e-850b-41b0-8dc0-d28aeb230931>"],"error":null}
{"question":"When did Henry II become Roman Emperor?","answer":"Henry II received the imperial coronation in Rome from Pope Benedict VIII on February 14, 1014.","context":["Henry II, Roman Emperor, St.\nHENRY II, ROMAN EMPEROR, ST.\nReigned from June 7, 1002 until July 13, 1024; b. Bavaria or Hildesheim (?), May 6, 973; d. Grona by Göttingen, Germany. The son of Duke Henry II, the Quarrelsome, of Bavaria, and Gisela, daughter of King Conrad of Burgundy, and the great grandson of King Henry I of Germany, Henry received his earliest education from Bishop Abraham of Freising. Later, he began a clerical education at the cathedral school of Hildesheim, and he finished his education in Bavaria with Bishop Wolfgang of Regensburg, by whom he was introduced to the monastic reform emanating from Lotharingia. In 995, upon his father's death, he became Duke Henry IV of Bavaria and a loyal supporter of his second cousin Emperor Otto III. He married (995–97/1000) Kunigunde, countess of Luxembourg. After Otto III's death, Henry contended for the throne and had himself elected and anointed king in June at Mainz by Archbishop Willigis. Henry II's significance as king lay in his attempts to establish undiminished royal power over secular princes, his complex and manifold relations with the church, his efforts to integrate the realm, and his elevation of a sacral notion of rulership to a new height, which one finds expressed verbally, visually, and liturgically in the sources. He received the imperial coronation in Rome from Benedict VIII on Feb. 14, 1014.\nHenry II's elevation to kingship did not come easily. In January 1002, when Otto III died in Italy without an heir, three main candidates emerged for the German throne. Of these, Henry had the strongest hereditary claim to succession, yet a large part of the nobility opposed Henry or did not initially support his candidacy. To fortify his position, Henry seized the regalia, including the Holy Lance, from the entourage bearing Otto III's body from Italy to Aachen through Bavaria, and in a series of ritual acts he played the role of next of kin and presumptive successor to the throne. Despite the opposition, Henry managed to have himself elected, anointed, and crowned king at Mainz in June by a small but influential group of nobles and churchmen. Thereafter, he achieved final recognition on the battlefield and made his first royal progress (Umritt ) through the realm. On this progress, Henry had his election and kingship acclaimed and formally recognized by the peoples of the several duchies through a ritual repetition of ceremonial and constitutive acts.\nIn Henry II's foreign policy, three areas stand out: his long protracted feud or war with Boleslav Chrobry of Poland, his alterations of Otto III's policies regarding Italy and imperial ambitions, and his efforts to stabilize the West and establish the hereditary claim of the German king to the kingdom of Burgundy. One can argue that under Henry II the eastern borders of the German kingdom began to stabilize, the missionary expansion of the tenth century slowed, and Henry pursued an imperial policy within achievable limits. Henry's hostilities with Boleslav lasted most of his reign and drove him to ally with the heathen Liutizi against the Christian Boleslav. Henry's contemporaries criticized him harshly, and this criticism, coupled with the half-hearted support given Henry by the Saxon nobility, muted his effectiveness. He had to settle for compromises in 1005, 1013, and 1018, which granted Boleslav lands in the East as fiefs, yet curtailed Boleslav's takeover of Bohemia.\nHenry's policies in Italy mark a pronounced shift of emphasis, of his rulership in comparison to his Ottonian predecessors. Whereas Otto III spent over fifty percent of his reign in Italy, Henry spent only seven percent of his reign there. He made only three trips to Italy, in 1004 to foil Arduin's usurpation of the Italian kingship, in 1013–14 to support Pope Benedict VIII by reissuing the Privilegium Ottonianum and to acquire the imperial coronation, and in 1022 to reassert imperial dominion over Capua and Salerno in the face of Byzantine advances in southern Italy. Despite his infrequent visits to Italy, Henry's charters, regardless of where issued, document intensive rulership activity there. Finally, Henry's initiatives in Burgundy set the stage for the acquisition of that kingdom by his successor, Conrad II.\nHenry II's internal policies developed from his concept of a divinely ordained kingship with undiminished royal power in both the secular and the ecclesiastical spheres. Henry strengthened the authority of the king over German dukes, princes, and prelates, systematically augmented the wealth and the political and economic servitium regis of bishoprics and royal monasteries, and supported a general reform movement in the Church. Attempting to break up concentrations of princely power, Henry moved with varying success against dukes and princes alike. He managed to integrate the southern duchies more fully into the realm, to diminish the power of numerous magnates, and often to empower churchmen in their place. Henry's enrichment and empowerment of royal churches, both episcopal and monastic foundations, came with significant increases in royal dominion over these institutions, especially the king's right to invest, sometimes even appoint, bishops, abbots, and abbesses, and to employ candidates from the royal chapel in ecclesiastical positions throughout the realm. Thus, he used the royal church as well as itinerant kingship to bolster his power and to integrate the realm.\nNevertheless, Henry took his divinely conceded obligations seriously. He participated in monastic and episcopal prayer fraternities and became a canon in several cathedral chapters. The reinstatement of the bishopric of Merseburg (1004) and the foundation of the bishopric Bamberg (1007) count as Henry's greatest ecclesiastical achievements. He also founded numerous monasteries and imposed a Lotharingian-based monastic reform on many powerful royal monasteries. Finally, he took active part in numerous German synods and with the pope in general reforming synods of 1014 and 1022. He died in 1024 at the royal residence of Grone and received burial in the cathedral at Bamberg. Soon after his death legends began to circulate about the chastity of his marriage and his religious character. When Pope Eugene III canonized him in 1146, Henry became the sole medieval German king to be so honored.\nFeast: July 15; in the diocese of Bamberg, July 13.\nBibliography: adalbert, Die Vita sancti Heinrici regis et confessoris und ihre Bearbeitung durch den Bamberger Diakon Adelbert, ed. m. stumpf, Monumenta Germaniae Historica, Scriptores rerum Germanicarum 69 (Hanover 1999). adalbold, Vita Heinrici, ed. h. van rij, Nederlandse Historische Bronnen 3 (Amsterdam 1983). Monumenta Germaniae Historica, Diplomata regum et imperatorum Germaniae 3: Heinrici II. et Arduini Diplomata, ed. h. bresslau (Hanover 1900–03). j. f. bÖhmer, Regesta Imperii 2 (Sächsisches Haus 919–1024)/4: Die Regesten des Kaiserreiches unter Heinrich II. 1002–1024, new ed. by t. graff (Vienna 1971). s. hirsch, Jahrbücher des Deutschen Reiches unter Heinrich II., 3 v. (Berlin 1862–75 repr. 1975). h. hoffmann, Mönchskönig und rex idiota: Studien zur Kirchenpolitik Heinrichs II. und Konrads II (Monumenta Germaniae Historica Studien und Texte 8; Hanover 1993). r. klauser, Der Heinrichsund Kunigundenkult im mittelalterlichen Bamberg. Historischer Verein Bamberg, Berichte 95 (Bamberg 1957). b. schneidmÜller and s. weinfurter, eds. Otto III. und Heinrich II.: Eine Wende?, Mittelalter-Forschungen 1. (Sigmaringen 1997). r. folz, \"Le légende liturgique de saint Henri II, empereur et confesseur.\" Clio et son regard. Mélanges d'histoire, d'histoire de l'art et d'archéologie offerts à Jacques Stiennon, ed. r. lejeune and j. deckers (Liège 1982), 245–58. a. wolf, \"Quasi hereditatem inter filios : Zur Kontroverse über das Königswahlrecht im Jahre 1002 und die Genealogie der Conradiner.\" Zeitschrift der Savigny-Stiftung, Germanistische Abteilung 112 (1995): 64–157. Kaiser Heinrich II. und seine Zeit in Historischer Verein Bamberg, Bericht 133 (1997). s. weinfurter, Heinrich II. Herrscher am Ende der Zeiten (Regensburg 1999)."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:75bb8c23-f8e9-4352-b981-461b43f1fcc0>"],"error":null}
{"question":"How do rising interest rates affect both corporate bonds and REIT investments?","answer":"Rising interest rates negatively impact both corporate bonds and REIT investments. For corporate bonds, increasing borrowing costs make older securities with lower coupons less valuable. For REITs, as demonstrated by funds like VNQ, rising interest rates can cause sharp price declines because the cost of financing properties, such as mortgages, increases with rising rates.","context":["The US Treasury market, valued at approximately $25 trillion, has witnessed a substantial increase in investments from US households since the Federal Reserve initiated its rate hikes last year. This surge in household holdings, from less than $1 trillion to around $2.5 trillion, represents the highest level in 25 years, according to Torsten Slok, chief economist at Apollo Global Management (NYSE: APO ).\nThe 10-year Treasury yield , a key influencer of the US economy, recently peaked at nearly 4.5%, the highest since late 2007. This increase significantly affected Wall Street, triggering a sell-off in major technology stocks and other rate-sensitive sectors following hints from the Federal Reserve of potentially maintaining higher policy rates for an extended period.\nOn Friday, however, the 10-year Treasury yield slightly retreated to 4.4%, offering some relief to the stock market. Despite this minor respite, equities faced substantial weekly losses. The consumer discretionary segment of the S&P 500 index declined by 5% over the week, indicating investor apprehension that companies offering nonessential items such as luxury goods, vehicles, and vacations could be impacted by an economic downturn.\nShares of Tesla Inc. (NASDAQ:NASDAQ: TSLA ) and Amazon.com Inc. (NASDAQ:NASDAQ: AMZN ), both part of this year's \"Magnificent Seven\" group of high-performing stocks, also suffered losses this week. Tesla's shares fell over 7%, while Amazon's shares decreased by approximately 6.7%.\nIncreasing borrowing costs pose a risk not only to consumers but also to large corporations needing to repay significant debts in the upcoming years. Consequently, older securities with lower coupons are now seen as less valuable.\nRising yields have erased annual gains across a wide section of the bond market. The benchmark Bloomberg U.S. Aggregate index is forecasted to yield a -0.6% return this year and -14.4% over a three-year period, according to FactSet data. The iShares Core U.S. Aggregate Bond ETF, which tracks the Bloomberg index, was down 2.1% for the year through Friday.\nThis article was generated with the support of AI and reviewed by an editor. For more information see our T&C.\nAdd Chart to Comment\nWe encourage you to use comments to engage with users, share your perspective and ask questions of authors and each other. However, in order to maintain the high level of discourse we’ve all come to value and expect, please keep the following criteria in mind:\n- Enrich the conversation\n- Stay focused and on track. Only post material that’s relevant to the topic being discussed.\n- Be respectful. Even negative opinions can be framed positively and diplomatically.\n- Use standard writing style. Include punctuation and upper and lower cases.\n- NOTE: Spam and/or promotional messages and links within a comment will be removed\n- Avoid profanity, slander or personal attacks directed at an author or another user.\n- Don’t Monopolize the Conversation. We appreciate passion and conviction, but we also believe strongly in giving everyone a chance to air their thoughts. Therefore, in addition to civil interaction, we expect commenters to offer their opinions succinctly and thoughtfully, but not so repeatedly that others are annoyed or offended. If we receive complaints about individuals who take over a thread or forum, we reserve the right to ban them from the site, without recourse.\n- Only English comments will be allowed.\nPerpetrators of spam or abuse will be deleted from the site and prohibited from future registration at Investing.com’s discretion.","The Vanguard Group is one of the world's largest investment management and advisory services companies. It primarily manages mutual funds and exchange-traded funds (ETFs) with about $8.1 trillion in assets under management (AUM) as of March 31, 2022. Founded in 1975, the company offers a total of 431 funds, 204 of which are available in the U.S. Vanguard is a full-service broker, offering low-cost investing for long-term buy-and-hold investors and retirement savers.\nAmerican investors have access to several tax-advantaged saving plans, including 401(k)s, individual retirement accounts (IRAs), and Roth IRAs. The main difference between a Roth IRA and a traditional IRA is that the former is funded with after-tax dollars. That means that contributions to Roth IRAs are not tax-deductible, as they are with traditional IRAs.\nUnlike a traditional IRA where withdrawn funds are taxed, a Roth IRA allows investors to withdraw funds tax-free. The figures for each fund are current as of March 8, 2023.\n- Vanguard was founded in 1975 and has more than 200 funds available in the U.S. and more than $8 trillion in assets under management.\n- When making a retirement account, a broad stock fund, and broad bond fund provide a good foundation.\n- Roth IRAs allow you to avoid paying taxes on investment returns by investing after-tax income.\n- These include bond funds and REIT funds.\n- VTI and BND can serve as good starting points when looking for Roth IRA investments from Vanguard.\nBest Types of Funds for a Roth IRA\nA Roth IRA is a qualified retirement account funded with after-tax dollars that grows on a tax-exempt basis. A traditional IRA, on the other hand, uses pre-tax dollars, giving the account holder a tax deduction for the contribution year. Traditional IRAs grow tax-deferred, where income tax must be paid on contributions and gains at the time of withdrawal in the future.\nSince the Roth IRA comes with no deferred tax liability, it is best to use it to benefit from the tax exemption. This means identifying high-yield investments that pay a high and steady stream of income in the form of interest, dividends, or other distributions. Some options to include in a Roth that can act as tax shelters are:\n- Bonds because they pay regular interest\n- Real estate investment funds (REITs) because they pay at least 90% of their operating income as dividends\n- Preferred shares because they pay relatively high and regular dividends\nRoth IRAs come with annual contribution limits. The annual limit for 2023 is $6,500 in 2023. Individuals 50 and older can contribute an additional $1,000 for a total of $7,500. Keep in mind that your income may limit whether you're able to contribute to a Roth IRA. In 2023, the phaseout range for individuals who contribute to Roth IRAs is:\n- $138,000 and $153,000 for singles and heads of household\n- $218,000 and $228,000 for married couples filing jointly\n- $0 and $10,000 for married couples filing separately\nStrategically placing high-yielding retirement assets in a Roth account and other assets that may pay low or no income in a traditional account is known as asset location, and it's a key way to minimize your tax exposure come retirement.\nVanguard Total Stock Market ETF (VTI)\n- Expense Ratio: 0.03%\n- AUM: $1.21 trillion\n- 12-Month Trailing (TTM) Yield: 4.40%\n- 1-Year Trailing Total Return: -8.23%\n- Inception Date: May 24, 2001\nThe Vanguard Total Stock Market ETF (VTI) aims to track the performance of the CRSP US Total Market Index. As an ETF, VTI offers investors access to a broad portfolio of equities, and shares of VTI can be traded throughout the day like a stock. VTI is passively managed, utilizing an index-sampling strategy that keeps investor expenses low.\nVTI's portfolio consists of 3,969 stocks, with the top 10 accounting for just under a quarter of total invested assets. The fund's holdings include large-cap, mid-cap, and small-cap equities across growth and value styles. The median market cap of VTI's holdings is $117.7 billion. Virtually all of the holdings are U.S.-based companies; only 0.16% are foreign.\nInvestors who want to create wealth through price appreciation should consider a broad-based equities fund such as VTI. Although equities have higher volatility than bonds, they have a track record of returning more over a long period of time. If investors have a low-risk tolerance or are nearing retirement, they might consider a more income-oriented portfolio.\nVTI is the ETF equivalent of The Vanguard Total Stock Market Index Fund (VTSAX). This is based on the composition of both funds.\nVanguard Total Bond Market ETF (BND)\n- Expense Ratio: 0.03%\n- AUM: $289.3 billion\n- 12-Month Trailing (TTM) Yield: 0.56%\n- 1-Year trailing Total Return: -9.76%\n- Inception Date: April 3, 2007\nThe Vanguard Total Bond Market ETF (BND) tracks the Bloomberg U.S. Aggregate Float Adjusted Index, a broad, market-weighted index. This ETF gives investors broad exposure to the taxable investment-grade U.S. dollar-denominated bond market. The fund excludes inflation-protected and tax-exempt bonds.\nBND holds 10,164 bonds. The average duration of a BND bond is 6.6 years, and the average effective maturity was 8.9 years. Just over two-thirds of bonds are U.S. government bonds. While the remaining third is investment-grade debt, 14.10% of the total is BBB bonds, which are among the lowest-rated on the investment-grade ladder.\nBND is intended to serve as a core bond holding for investors. Bonds generally have low volatility and provide a steady supply of interest payments to the investor. Taxes on interest payments are deferred in a Roth IRA account, making it an ideal place to hold these kinds of investments. A broad-based bond fund is typically a lower-return, lower-risk investment vehicle than a stock fund. As a result, bonds can be useful both for risk-averse investors and for portfolio diversification.\nBND is the ETF equivalent of The Vanguard Total Bond Market Index Fund (VBTLX). This is based on the composition of both funds.\nThe Vanguard REIT Index ETF (VNQ)\n- Expense Ratio: 0.12%\n- AUM: $69.7 billion\n- 12-Month Trailing (TTM) Yield: 3.95%\n- 1-Year Trailing Total Return: -13.25%\n- Inception Date: Sep. 23, 2004\nThe Vanguard REIT Index fund ETF (VNQ) invests in stocks issued by REITs. These are companies that purchase and manage office buildings, hotels, and other real property. This REIT tracks the MSCI US Investable Market Real Estate 25/50 Index. This index is primarily an equity REIT index, and the VNQ holds REIT shares in 167 different companies.\nBecause REITs must pay out 90% or more of their operating income as dividends to shareholders, REIT funds tend to have relatively high yields. These can cause a series of taxable events that are deferred in a Traditional IRA, but which are exempt in a Roth. As a result, a Roth IRA is a good place to hold REITs.\nTake note that REIT funds are quite sensitive to interest rates, and share prices can fall sharply as rates increase. This is because the cost of financing properties, such as mortgages, also increases as interest rates rise.\nVNQ is the ETF equivalent of The Vanguard REIT Index Fund (VGSLX). This is based on the composition of both funds.\nWhat Is the Cost to Open a Vanguard Roth IRA?\nIn a few cases, the minimum investment needed to open a Vanguard Roth IRA may be as low as $1,000. For most Vanguard funds, the minimum investment necessary is $3,000, and that minimum is higher at some funds. There is no fee to actually open the account.\nDoes Vanguard Offer Roth and Traditional IRAs?\nYes, Vanguard offers both Roth and traditional IRAs. As an investor, this means that you can open either account (or both) with Vanguard as long as you meet the minimum investment requirements (if any).\nHow Much Does It Cost to Open a Vanguard Roth IRA?\nVanguard charges annual account service fees of $20 per Roth IRA account if the total Vanguard mutual funds and ETFs in the account are at least $1 million. However, this fee can be waived by signing up for the company's e-delivery service or by maintaining at least $1 million in qualifying assets. Investors should not overlook that they also pay through expense ratios at individual funds.\nCan You Day Trade in a Roth IRA?\nLike other retirement accounts, a Roth IRA does not allow for margin, which can limit day trading activities including selling short. You can, however, still actively trade in your Roth IRA. Because a Roth is tax-exempt, any short-term capital gains earned would also be exempt, which is an advantage. However, you may rack up transaction fees.\nWhat Index Fund Should I Invest in my Roth IRA?\nMany experts advise using buy-and-hold index funds for retirement account investing. Since a Roth IRA is tax-exempt, the best type of index fund would be one that pays out a lot of dividends or other distributions. Index funds to look for, therefore, would be focused on those that pay a high yield, such as bond funds (especially high-yield bonds), real estate funds (especially REITs), and preferred stock indexes. You should put low-yielding index funds in a tax-deferred account such as Traditional IRA or 401(k).\nThe Bottom Line\nRoth IRAS can be an effective tool for retirement saving, and broad bond and stock funds provide diverse, inexpensive foundations. Investors may wish to go further depending upon how much time, energy, and risk tolerance they have for investing. For many investors who are short on time, it's a practical decision to pick a few large, diverse bond and stock funds and continue to invest in those. Some investors will choose another course, by opting to add investments in individual companies, including riskier small-cap stocks, and through targeted sector investing and stock analysis."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:bd3a4b1b-bc26-4cef-b424-9de33b1f2204>","<urn:uuid:83a13c39-abdd-4dbd-ab64-d92a7759365b>"],"error":null}
{"question":"What key similarities exist between Brahms' Piano Quartet No. 1 in G minor and Piano Quartet No. 3 in C minor regarding their romantic influences?","answer":"Both piano quartets demonstrate strong romantic influences but in different ways. The G minor quartet (No. 1), written around 1857, features rich inventiveness, expressive warmth, and dramatic elements, culminating in a bawdy Rondo alla Zingarese finale. The C minor quartet (No. 3), though started around the same time, was reworked in 1873 and carries deeper romantic significance tied to unrequited love - specifically referencing Goethe's Werther who commits suicide over love for his friend's wife. It features a yearning, tragic first movement characterized by descending seconds, and a deeply moving love song in the third movement, possibly expressing Brahms' feelings for Clara Schumann.","context":["Our next Workshop\nACMP's Self-Rating Guide\nGems from CMNC's Newsletter\nTake a look at CMNC's Library!\nFrequently asked questions about Chamber Music and Workshops\nLinks to Other Music Pages\nRecommended Piano Quartets\nPianists interested in playing chamber music should buy the music to all of these works and LEARN the piano part.\nThen, get together with your string player friends and have a great time!\nMozart Beethoven Mendelssohn Schumann Brahms Dvorak Faure Turina\nSTANDARD PIANO QUARTETS\nby William T. Horne\nThere are probably many different criteria for listing standard chamber music works, so it may be difficult to agree on what works should be included in a standard list. However, a chamber musician, particularly a pianist, who would like to start acquainting himself or herself with the literature, might appreciate any help in the daunting task. And so I shall essay to attempt such a list.\nMozart. Without a doubt the first works recommended would be the two piano quartets of Mozart. He contracted to write three quartets in 1785 for Hoffmeister when the combination of piano, violin, viola, and cello was still rather unusual. Mozart treated all four instruments as equals, a novel development, but the publisher complained they were too hard and refused to pay for them, so Mozart quit after writing two. The more familiar is the G minor, K. 478, with three movements, all gems. It is perhaps more balanced for the strings than the second Quartet in Eb major, K. 493. The second movement, Larghetto, is particularly delicate and lovely. The finale is again an inventive Rondo.\nBeethoven. There is a piano quartet version of what was originally a wind and piano quintet by Beethoven, Piano Quartet, Op. 16 in Eb major. Published in 1801, this is a delightfully melodic work, not quite as hard as many quartets. The opening Grave, moves into an Allegro ma non troppo with three main themes. The Andante cantabile is delightfully ornamented. The final movement is a Rondo, complete with a jaunty tune, and high spirits.\nMendelssohn. I will mention in passing the three piano quartets by Felix Mendelssohn, opus numbers 1,2, & 3, written when he was only thirteen! They are somewhat simplistic, with florid, difficult piano parts, and simple string writing. I would not recommend them as standard works, but they are fun to read through.\nSchumann. The next major work I would select is the Piano Quartet in Eb major, Op. 47 by Robert Schumann. It is the first masterful quartet of the Romantic era, written in 1842 immediately after his three string quartets, op. 41. Of four movements, it begins with a hymn like introduction and then a vigorous, motoric first movement. The second movement is like a Mendelssohnian scherzo, quick and magical. The Andante is highly romantic, a song of love, allegedly to his wife, Clara. The final movement is almost manic, with exuberant themes, one after another, pouring forth.\nBrahms. Perhaps the highest achievements of romantic piano quartet writing are the three quartets of Johannes Brahms. Each is a masterpiece in itself. The first two were written in tandem around 1857, and served to introduce Brahms when he relocated in Vienna, the music capital of Europe. Possibly the first, Piano Quartet in G minor, Op. 25 is the more popular. The first movement, based on a four note figure, is rich with inventiveness and vigor. The second movement, entitled Intermezzo, has muted strings and a wistful melody played over a driving, insistent rhythm. The third movement, Andante, manages to be both stately, warm and expressive, all simultaneously. The finale, a bawdy Rondo alla Zingarese, pleases both audiences and players with its drama and fun. It has four themes, pressing onto a molto presto ending. The second Quartet, in A major, Op. 26, demonstrates Brahms's habit of composing matching works at the same time, with a variant style of working out his material. The A major Quartet is more gentle and reflective in the first two movements. The Scherzo gallops along, alternating between a plaintive melody and a turbulent trio. The Finale, Allegro, is another folk tune in Hungarian style, but much more processed and cerebral that the first's Zingarese movement - a variant of the style. The third Quartet, Op. 60 in C minor, was written about the same time as the other two, in the agitated period around the death of Robert Schumann in 1856, but set aside for 17 years and then reworked by Brahms in 1873. It is the only work that he ever made a programmatic illusion to- referring to the younger Werther of Goethe's novel, who kills himself for unrequited love of his friend's wife. He transposed the work to C minor, and its dramatic first movement is colored by the descending second - a recurrent, yearning, tragic gesture. The second movement, a Scherzo, is in F minor, with a disturbingly misplaced accent. In the third movement, in E Major, one finds a deeply moving love song, possibly a restatement of his affection for Clara Schumann. [This movement is the favorite of many musicians.] The finale, Allegro comodo, was composed anew, and reflects an autumnal, more wistful, I think, point of view. It often reminds me of a chilly wind fluttering leaves. All of these magnificent works are very difficult for piano and strings, but possibly the most rewarding of piano quartets.\nDvorak. There are two wonderful piano quartets by Antonin Dvorak. The first, in D major, op. 23, is optimistic and charming. It was written shortly after his marriage and his first successful works had been published. The first movement is somewhat Schumannesque with a theme with varied rhythms. The second is the best movement with a series of variations on a folk melody, very Slavic. The last movement is perhaps weaker; it combines a scherzo and finale, but even this movement is fresh and enjoyable. Dvorak's second Quartet, op. 87, in Eb major, is quite different- a big, muscular, fully developed work of an assured composer. The first movement is powerful, Allegro. The second movement is an Adagio in six flats, with exquisite melodies and fluctuating moods. The third movement, Scherzo, I call the \"zither movement\", because the piano part mimics café music in the back streets of Eastern Europe. The finale, again in six flats, is a rollicking folk tune, bounding along to a grand finale.\nFaure. In 1877 Gabriel Faure, at age 27, suffered a romantic rejection and dissolution of his betrothal. He turned to chamber music and produced his greatest chamber work, the Piano Quartet, op. 15 in C minor. The first movement manages to be both lively and dignified, with florid piano writing. The second movement, a Scherzo, has strings playing pizzicato over a carefree melody; later the strings are muted. The whole movement is airy, sparkling. In the Adagio, a superb movement, he is working through his grief; it starts most tragically, later warms to a serenade, but dies away in an echo of the beginning. The finale, has a driving rhythm, galloping in the piano, with soaring melodies in the French manner. It leads to a joyous coda. The whole work is suffused with Faure's interesting harmonies, not quite yet Impressionistic.\nTurina. Unfortunately the twentieth century has not furnished a great supply of piano quartet works, perhaps none to include in a list of standard works, but I would like to mention the Piano Quartet, op. 67, of Joaquin Turina, a Spanish composer. This is a melodic, colorful piece of four movements, all with a Spanish flavor, more modern chords. It may be easier for piano than many of the other works listed, but is very enjoyable. Try it out. There are a myriad of other works, so that one never knows where to quit, but, I think, this is at least a beginning. Perhaps other fine works, if not so well known, can be an addendum later. August, 2000\nThat's it, folks. Happy playing!\nIf you have comments or suggestions, email us at firstname.lastname@example.org\nCopyright 2000 W.T.Horne\nHome | Site Map | Contact CMNC","Notes and Editorial Reviews\nReviews of the original recordings that make up this set:\nPiano Quartet No 2, Clarinet Trio\nThis is an impressive addition to the Nash Ensemble’s growing catalogue of Brahms recordings. And as with their recording of the String Sextets, and of the First and Third Piano Quartets, a real sense of collective endeavour permeates these performances. You really get the impression that the works are being played as chamber music: passionate without being histrionic, precisely coordinated but with freely expressive solo lines, and balanced to give each player equal prominence.\nThe scoring of the Clarinet Trio – clarinet, cello, piano – helps to delineate each of the voices, and in this work it\nis to the credit of the players and the sound engineers alike that so much coherency is achieved in the ensemble. All three players come across with a warm yet focused tone. The democracy of the Nash Ensemble’s approach is demonstrated by the fact that the clarinet never seems to dominate as a solo instrument. Again, this may in part be due to the sound engineering and the way that the upper register of the piano has a roundness of tone that perfectly complements the clarinet’s sound. Dynamic and tempo markings are observed but never exaggerated, the\npoco F at the opening for example, is interpreted as an indication of clarity of tone and phrasing rather than an actual loud dynamic, thereby retaining a sense of mystery for this slow introduction.\nThe louder passages in the opening movements of both works demonstrate the extraordinary facility the Nash Ensemble has for presenting chamber music as chamber music. Brahms cranks up the tension, and the volume, but the players never let the music’s intimacy suffer. All the passion is there, but there is never any danger excess. Surprise dynamic jumps in the finale of the Clarinet Trio are another case in point; each\nsF jumps out of the texture, but never to the extent of disrupting the music’s lyrical continuity.\nFine balance and close communication between the players also characterise the Nash Ensemble’s reading of the Second Piano Quartet. So there is never any danger of the piano competing with the strings. As in the Clarinet Trio, the roundness of the piano tone really helps it to integrate into the texture of the other instruments. And yet despite that integration, the sound of each of the instruments is always clearly audible. I’m particularly impressed by the sound of the cello in the mix. It’s not a particularly bottom-heavy balance, but the cello really sings.\nPerhaps these performances are a little too sophisticated? Is there enough rustic charm in the scherzos? Enough drama to engage in the Allegros? Well, from where I’m sitting they gauge it just right. True enough, I would probably be just as content with a reading that was a little more boisterous, provided it retained the same balance and ensemble. But, as I say, this is chamber music played as chamber music. Intimacy and immediacy are the guiding principles here, from the communication between the players to the clarity and warmth of the sound engineering.\n-- Gavin Dixon, MusicWeb International\nThis is the third release in the Nash Ensemble’s series of Brahms chamber music recordings for Onyx; already issued are the sextets (reviewed by Jerry Dubins in\n31:4) and the First and Third Piano Quartets (reviewed by the undersigned in 32:6). At least two other groups have recently recorded Brahms’s complete string chamber music: the Verdi Quartet on Hänssler, and the Leipzig Quartet on MDG; while we will have to see whether the Nash survey will be as comprehensive, it is already more broad, including two works with piano that neither of the two standing string quartets has recorded. The Leipzig series does include the Piano Quintet, with pianist Andreas Staier; I have not yet heard that recording.\nAs with the Nash’s two earlier Brahms discs, the playing here is polished, intelligent, and expressive. One of the most difficult problems of the F-Major Quintet is the tempo relationship between the first movement’s two themes; many groups, most jarringly the Amadeus Quartet in its 1960s version, speed up abruptly at the second theme, as the overall rhythmic activity changes from eighth notes to quarter-note triplets. In this recording the tempo is a bit faster, but not so much as to disrupt the musical flow. There are many felicitous touches as well, for example, the\nplaying that underscores the echo effect at m. 84 of the second movement. The fugal finale is taken at a rapid clip, but the Nash’s musicians—violinists Marianne Thorsen and Malin Broman, violists Lawrence Power and Philip Dukes, and cellist Paul Watkins—maintain complete rhythmic and technical control.\nThe first movement of the great G-Major Quintet (\nAllegro non troppo, ma con brio\n) is taken at a near-ideal tempo, fast enough to be exciting but not so fast as to blur the music’s magnificently complex polyphony. Again, as in the F-Major, small details are telling: for once the viola “cadenza” at m. 66 of the second movement is perfectly in tune! And, again, the\ncoda of the finale is played both fearlessly and brilliantly.\nAs has become customary, all repeats are taken. The recording is warm and natural, if perhaps a bit bright; the crucial cello part is adequately audible in the dense texture of the G-Major, but the overall blend is still a bit bass-shy. Of competing versions, that of the Verdi Quartet with violist Hermann Voss has a fuller low end, but suffers from a weak-sounding first violin; in addition, each of the quintets is coupled with one of the sextets, which some collectors may find inconvenient. The Leipzig Quartet with Hartmut Rohde plays the first movement of the G-Major with somewhat less abandon, but benefits from warmer sonics, making its version marginally preferable. Of stand-alone versions of the quintets, my gold standard remains the 1982 Nonesuch recording by the Boston Symphony Chamber Players, led by the superb Joseph Silverstein and anchored by cellist Jules Eskin. The music-making is exemplary, and the early digital sound is simultaneously more transparent than that of this new version and richer as well; for once, perhaps aided by the engineers, the cello has sufficient weight, and there is none of the “digital glare” for which recordings of the period are often criticized. Still, it would be hard to go wrong with this new CD, particularly for collectors who have enjoyed the Nash Ensemble’s previous two Brahms discs for Onyx. This recording ranks near the top of a very high-class field.\nFANFARE: Richard A. Kaplan\nPiano Quartets No 1 & 3\nThis is the second release in what reportedly will be an “ongoing series” of Brahms chamber-music recordings by the Nash Ensemble; their recent recording of the two string sextets received high praise from Jerry Dubins in\n31:4. The pairing of the first and last of Brahms’s piano quartets—the C-Minor receives relatively few stand-alone recordings—is usually a sign that a complete set is in the works; the length of the A-Major Quartet makes the present CD coupling the only practical one.\nThere is much to like in the present two performances; I would be inclined to echo Dubins’s rave were it not for two things. First, I listened to this disc the day after auditioning a truly extraordinary recording of two other Brahms works by the Arcanto Quartet (reviewed elsewhere). Where the Arcanto musicians find new insights everywhere in the C-Minor String Quartet, here the Nash players (Ian Brown, piano; Marianne Thorsen, violin; Lawrence Power, viola; and Paul Watkins, cello) are content to let Brahms’s music speak for itself. This works fine in the flamboyant G-Minor Quartet; the playing is first-rate top-to-bottom, and the recording is very natural sounding, so that ensemble is emphasized over individual lines. This is a polished, no-gimmicks performance that one can live with.\nSecond, though, the C-Minor Quartet, op. 60, while not as difficult technically, presents much greater\nchallenges. I have discussed this at some length already in these pages, e.g., in my review in the May/June 2009 issue of the recording by Nicholas Angelich and colleagues, and in 32:2 (November/December 2008) by Xiayin Wang and the Amity Players in the identical program. Again I find the Nash Ensemble version well played and mostly effective, but it falls just short of conveying fully the powerful contrasts inherent in the score. More than once, Brahms alluded to Goethe’s\nin writing about this work; given that its complex compositional history intertwines considerably with Brahms’s equally complex relationship with Clara Schumann—the piece had its genesis at the time of Robert Schumann’s final illness and death—one can understand the sort of emotional extremes the music suggests. Joanna Wyld’s fine program notes document this “programmatic” element in detail.\nThe performance has its strong points; the first movement is particularly effective, with the exception of an uncalled-for\nat the big B-Major arrival in the development. But the Scherzo, placed second, lacks vehemence—if a “scherzo” is a joke, then this is a very nasty joke indeed—and the beautiful E-Major Andante, according to some a portrait of Clara herself, is attractive, but too fast to be truly exquisite. The Finale, with exposition repeat taken, is likewise well played, but simply too comfortable; given the implicit program, the C-Major ending may be one of the darkest passages ever written in that key; one further misses the hold-your-breath tension that the chromatic scales leading to that ending can embody. I have long been recommending the recording—shamefully out of print—by Tamás Vásáry and a crack team of Berlin Philharmonic string soloists for DG; after hearing the Nash performance, I listened again to that older recording for the first time in quite a while, and yes, the vehement music is more intense, the beautiful more moving.\nThe bottom line, therefore, is that this G-Minor Quartet ranks with the best, and that only the fact that others have probed the contrasting moods of the C-Minor more fully prevents it from receiving a top ranking as well. Recommended for all but the most demanding collector.\nFANFARE: Richard A. Kaplan\nThe subtlety, the sheer dexterity of the interplay between the members of the Nash Ensemble in this wonderful disc can be dizzying to hear. It’s not simply a question of how fast they can play the notes; it has more to do with the speed, nimbleness and yet profundity with which they can switch emotions, dipping in and out of moods seemingly at will. This is very classy playing indeed.\n-- Gramophone [9/2007]\nWorks on This Recording\nSextet for Strings no 2 in G major, Op. 36 by Johannes Brahms\nWritten: 1864-1865; Austria\nSextet for Strings no 1 in B flat major, Op. 18 by Johannes Brahms\nWritten: 1859-1860; Germany\nQuintet for Strings no 2 in G major, Op. 111 by Johannes Brahms\nWritten: 1890; Austria\nQuintet for Strings no 1 in F major, Op. 88 by Johannes Brahms\nWritten: 1882; Austria\nQuartet for Piano and Strings no 3 in C minor, Op. 60 by Johannes Brahms\nIona Brown (Piano)\nWritten: 1855-1875; Austria\nQuartet for Piano and Strings no 2 in A major, Op. 26 by Johannes Brahms\nIona Brown (Piano)\nWritten: 1855-1861; Germany\nQuartet for Piano and Strings no 1 in G minor, Op. 25 by Johannes Brahms\nIona Brown (Piano)\nWritten: 1855-1861; Germany\nTrio for Clarinet, Cello and Piano in A minor, Op. 114 by Johannes Brahms\nRichard Hosford (Clarinet)\nWritten: 1891; Austria\nBe the first to review this title"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:246d3b1e-5769-4971-8a68-f8008034967d>","<urn:uuid:05bf3065-a325-45f0-881e-ac65cdfcd07a>"],"error":null}
{"question":"¿Qué métodos se utilizan para transportar materiales en proyectos de techos altos y cuáles son los riesgos de seguridad asociados? 🏗️","answer":"For transporting materials in high-rise roofing projects, several methods are used: freight elevators through interior spaces, helicopters or sky cranes, mobile or tower cranes (most typical), tear-off tarps with corner straps, skip boxes, construction elevators on building exteriors, and debris chutes. Regarding safety risks, these operations face multiple hazards: falling or airborne debris is especially dangerous, high winds can be 10-50 mph stronger on the roof than ground level, materials can become loose and airborne, and workers must use proper fall protection systems. Personal Fall Arrest Systems (PFAS) must limit arrest force to 1,800 pounds, prevent falls over 6 feet, and be inspected before each use. Additionally, anchor points must meet 5,000-pound strength requirements or maintain a safety factor of at least two under qualified supervision.","context":["The complexities of re-roofing a high-rise facility are numerous. It’s not the actual roofing trade work that’s challenging – it’s project coordination.\nThe quality and thoroughness of project planning often determines the success of the re-roof project. A lot of information must be collected upfront. Facilities professionals should be ready to discuss building access, security, work restrictions, and parking with the selected roofing contractor before the project scope is even defined.\nHere are some factors you should take into consideration if you’re thinking about a re-roof for your high-rise building…\nStart with Communication\nConsulting with the city on details such noise ordinance is a necessity. Requirements change from city to city, but permits are usually required for parking, blocking a sidewalk, use of a crane, or street shutdown. If traffic is going to be obstructed, a permit is also needed to reroute vehicles.\nIt’s also a good idea to alert the local fire chief in advance of the project in case any emergencies or injuries were to occur laborers on the roof or to people walking along the building perimeter.\nAdditionally, if you’ve leased space to cellular companies, be ready to involve them very early on in the project – they will typically be the ones responsible for lifting and moving cell towers.\nCrew Size, Hours, and Speed\nIn hotel or multifamily towers, roofing crews will most likely need to start working later in the day to avoid disturbances. In an office building, a re-roof project might require nighttime work in order to be less disruptive. Beyond being less of a distraction for daytime tenants, traffic is also less, which makes it easier to block roadways for loading or unloading. The availability of parking spaces for laborers is less of an issue, as is the risk of heat-related illness for laborers.\nEvening work, however, requires extensive pre-planning meetings, along with development of a detailed project and communication plan between the facilities manager and roofing contractor. It also requires bright lights that, along with noise, could be offensive to occupants in neighboring buildings. The lighting also results in shadows that can compromise work quality and create a safety hazard for laborers. Additionally, project coordination is critical because workers typically can’t purchase additional supplies or equipment during evening hours.\nThe quicker your roofing contractor can complete the work, the better (for all involved parties). It may be possible to increase crew size if:\n- Crews can work in shifts. One crew can complete noisy tasks at night, and another can do quieter detail work during the day.\n- You have more than one roof area. Multiple crews can work simultaneously.\n- Access is challenging. A roofing contractor may use an additional crew on the job just for support.\nTear-Off and Loading\nDisposing of tear-off and staging and loading materials pose a greater logistical challenge in a metropolitan environment. If street closure is necessary to accomplish this, your contractor will usually subcontract out traffic control to set up necessary detour signs for vehicle rerouting. Job-site postings must be of specific reflective materials and dimensions, so typical postings won’t meet code requirements. How and where they are displayed must be in compliance to minimize exposure to property loss and personal injury claims.\nCrews may want to use space within a parking garage or designated loading zone, or along the building perimeter, for staging materials. Which space they use will depend on access and, in some situations, what neighboring property owners will allow. If using a parking garage, smaller trucks must be utilized to clear the eight-foot headway limit.\nIf space is limited, phased material deliveries will cut down on the amount of ground staging area square footage required. Overloading the roof with materials is usually not feasible because of roof congestion and the higher risk of materials becoming loose and airborne.\nPhased deliveries will require extra crane mobilizations unless the crane is brought in and left erected, and then the operator and rigger are scheduled as needed.\nBecause high-rises don’t usually have a large roof area and loading materials is a logistic challenge, careful planning is necessary to order the right amount of materials and use them entirely so crews don’t have to transport them up and down.\nMoving materials either from the roof (i.e. tear-off of an existing roof system) or to the roof (i.e. the new roof system and equipment) can be done any number of ways.\nRoofing crews can take debris down in a freight elevator and go through a lower-level interior space to dump it. While this may seem like the most economical option, man hours really add up because of how labor-intensive the process can be. When bringing new materials up to the roof via a freight elevator, crews may have to access interior space again if the elevator stops a floor below and doesn’t go all the way to the roof.\nAnother option is a helicopter or sky crane. Your roofing contractor will subcontract helicopter lifting with a company experienced in the process. Careful planning is required so that a helicopter with proper lifting capacity is used for the load weight and size of the roofing materials. Weather limitations will require flexible scheduling.\nHelicopter lifting presents many safety hazards (high winds from aircraft rotors can turn dust and debris into airborne projectiles). Street shutdown may also be necessary to protect the people and property below.\nCranes (either mobile or tower) are the most typical way to transport materials to and from the roof. Set up time will be lengthy, though.\nWhen disposing of the existing roof system, a tear-off tarp can be used. These have corner straps that are sewn in and hook to the headache ball. They encapsulate debris and are lightweight; when not in use, they can be folded up for storage. Skip boxes can also transport debris from the roof to the ground using a crane, but they require more space for storage.\nUnless your facility is a historic building, the best option to move materials may be via a construction elevator, which is set up on the outside of the building. It too has a slow set-up, but is the least disruptive option for building occupants.\nWhen it comes to disposal of roof tear-off, a chute is the least expensive option – but it requires quite a bit of labor for installation, maintenance, and tear-down. For a multistory chute, debris is brought down a freight elevator, and then disposed of through a lower-story window. A crane is necessary only during chute set-up and tear-down.\nUnless the roofing contractor subcontracts the construction of a stair tower platform or construction elevator, their crews will be using the building interior for roof-to-ground egress. Finishes should be protected from potential damage, with plastic covering cover carpet, elevators, etc.\nSecurity is another consideration. Additional guards may be required if roofing crews need entry through multiple locked doors, stairwells, and elevators, or if exterior doors need to be kept open as roofing crews load and unload materials. Work with your roofing contractor to decide if they should hire security personnel to accompany crew members, or if you should alert your own security team that additional resources will be necessary.\nProviding advance notice to building security is also required, especially if the roofing crew will be working evenings and weekends, and require access when security staffing is normally light. Special security badges, keys, codes, etc. may also be necessary.\nHigh-rise roofing projects pose unique safety risks that must be addressed. Falling or airborne debris is especially hazardous. The roofing contractor’s project manager should have designated laborers whose sole responsibility encompasses waste management. An intense housekeeping plan must be developed and executed with vigilance. Make sure the roofing contractor you hire has stringent safety policies and enforcement procedures; you can review their safety record for validation.\nCatch-fence or windscreen erected around the roof’s perimeter will assist in keeping lightweight material and debris from being blown over the edge. A scaffold tunnel in front of the building can cover the sidewalk to protect pedestrians from debris and/or tools and equipment being knocked over the edge.\nHigh winds are one element of weather that make it unsafe for the roofing crew (and anyone below or in neighboring buildings). The wind speed may be 10 to 50 mph higher on the roof than the ground. Material piles placed on the roof have to be maintained constantly to ensure that the tarps are secure. Hardhats may also be blown over the edge.\nCranes are equipped with wind meters; whenever necessary, the project will be halted to protect workers. Crews can also carry hand-held wind meters or use weather apps on their smart-phones to assist in wind-speed monitoring. High winds can result in unplanned shutdowns; in these cases, precautions need to be taken so the roof is not left exposed to the elements.\nFresh air intakes should be shut off to avoid fumes from roofing adhesives being transferred inside the building.\nAs with all roofing projects, the contractor’s crew should know which fire exits are available for their use. They should also be familiar with the building’s evacuation plan in case of emergency. Because it may take longer for emergency personnel to respond to a worker who is severely injured on the roof, or falls and is restrained by a fall protection harness, workers should also be trained to administer first aid and CPR, and rescue fellow crew members.\nA roofing contractor with experience working on tall, metropolitan buildings is already aware of these challenges; they can be accounted for in advance with proper planning and execution. Safety and crew capacity are equally important criteria on re-roof projects. The right roofing contractor will help you avoid the pitfalls of poor project planning.","Roof repair projects require workers to complete jobs on existing buildings. These roofs are rarely designed with pre-existing fall protection safety so workers making the repairs must plan ahead and implement protection that will reduce their risk of dangerous falls.\nWorking at heights is a hazard for any worker, but workers completing roof repairs may face additional risk if the roof is unstable, deteriorating, or if workers are uncertain how to use fall protection on a roof that is already weatherproofed.\nThere are several fall protection methods that workers can use during roof repair jobs to ensure the work proceeds safely. For repair jobs, roofers have several options including scaffolding, aerial lifts, and various types of conventional fall protection.\nBefore beginning the job, focus on identifying fall protection needs including precautions to prevent slip, trip, and falling object hazards. The best fall protection choice depends on the type of building and what area of the roof the worker needs to access to make the repair.\nGuard against falls through skylights or other roof openings. Use a guardrail system, PFAS, or protective cover that will support two times the weight of a worker.\nIf necessary to protect workers below from falling debris, set up a work zone while roofers remove old roofing materials from the repair area.\nWorkers should be careful of air hoses and power cords for nail guns and other electrical equipment. If a worker steps on one, hoses and cords can slip underfoot and lead to falls.\nRemember to place any removed shingles or replacement tiles in a safe location. If unsecured, these materials can visually blend in against the roof and create a dangerous trip hazard.\nNew materials staged on the roof should be placed so that they are safe and secure.\nThe structural integrity of the roof must be assessed before or during the roof repair process. If workers notice signs of deterioration on the roof, like dry rot, as old weatherproofing is removed, a competent person should evaluate the area and all necessary precautions must be taken to protect the roofers.\nWhen repairs are to be completed along the edge of a roof, workers can use a scaffold or man lift. Regardless of the condition of the roof, scaffolds and lifts provide a safe, stable work platform.\nFor smaller tasks or shorter roof repair projects, scissor lifts or aerial lifts may be more efficient than installing scaffolds. Aerial lifts are a practical way to get to a customized height above or below the roof level. Do not overload the lift when loading material.\nOSHA Standard 1926.453(b)(2)(iv) Employees shall always stand firmly on the floor of the basket and shall not sit or climb on the edge of the basket or use planks, ladders, or other devices for a work position.\nWhen properly constructed and used, scaffolds can provide suitable fall protection for roof repairs. Guardrails installed along the open side of the scaffold provide fall protection.\nSafe access must be provided because roofers also risk falling when climbing on or off a scaffold. Cross braces must not be used as a means of access for scaffolds.\nOSHA Standard 1926.451(g)(1) Each employee on a scaffold more than 10 feet above a lower level shall be protected from falling to that lower level.\nWhen scaffold platforms are 2 feet above or below a point of access, workers must use portable ladders, hook-on ladders, stair towers, ramps, or other safe means of access.\nA Personal Fall Arrest System (PFAS) is usually the system of choice for most roofers. A PFAS will safely stop (arrest) a worker who is falling from a working level. It consists of an anchor point, a connector, and a body harness.\nOSHA Standard 1926.502(d)(21) Personal fall arrest systems shall be inspected prior to each use for wear, damage and other deterioration, and defective components shall be removed from service.\nWhen a personal fall arrest system is chosen as the means of fall protection on a job site, the PFAS must:\n- Limit the maximum arresting force on a worker to 1,800 pounds when used with a body harness.\n- Be rigged so that a worker can neither free fall more than 6 feet nor contact any lower level.\n- Bring a worker to a complete stop and limit the maximum deceleration distance a worker travels to 3.5 feet.\n- Have sufficient strength to withstand twice the potential impact energy of a worker free falling a distance of 6 feet or the free fall distance permitted by the system, whichever is less.\n- Be inspected prior to each use for wear, damage, and other deterioration. Defective components must be removed from service.\n- Workers must be trained in the safe use of the system.\nSelecting a location to install an anchor is a critical step in avoiding a fatal fall. An anchor gives the worker a secure point to tie off the lifeline for a fall arrest system. An anchor for a fall arrest system must meet the 5,000-pound strength requirement or maintain a safety factor of at least two (twice the impact load) under supervision of a qualified person.\nWhen available, existing anchors might be effective points for a worker to tie off. From ground level inspect the ridge cap and last rows of shingles for permanently installed anchors. If present, these may be fastened to the top chord or other frame part during construction. Anchors could also have been installed with the original roof, using a low-profile style sometimes painted to match the roof color (making it less obvious from the ground).\nIf the roof doesn’t have permanent anchors, new anchors can be installed before repair work begins. If attaching a new anchor, roofers must fix it to the truss or rafter structure underneath. Roof sheathing does not provide enough support by itself. Always follow the anchor manufacturer’s installation instructions.\nDepending on the size of the repair job and the number of workers who need to be on the roof, it might be necessary to install more than one anchor. Where practical, consider leaving roof anchors in place. It will make the current job simpler and reduce the burden for roofers in the future.\nIf the employer does not use ladders, scaffolds, or aerial lifts, and can demonstrate that it is not feasible or would create a greater hazard to use conventional fall protection equipment (guardrails, safety nets, or PFAS) when working at heights of 6 feet or greater, the employer must develop a written site-specific fall protection plan prepared by a qualified person. This person could be the owner, the supervisor, or a worker who has extensive knowledge, training and experience with fall protection and is able to solve problems relating to fall protection. States with OSHA-approved State Plans may have additional requirements for written fall protection plans.\nOSHA Standard 1926.502(k)(1) The fall protection plan shall be prepared by a qualified person and developed specifically for the site where the leading edge work, precast concrete work, or residential construction work is being performed and the plan must be maintained up to date.\nThe site-specific fall protection plan must document at each location why the use of conventional fall protection equipment is not feasible or will create a greater hazard. The plan must also describe the alternative methods that the employer will use so that workers are protected from falls. Workers and their supervisors must be trained on the proper use of those other fall protection methods.\nConventional fall protection equipment can reduce or eliminate the chances of a fatal fall. Written site-specific fall protection plans ensure that protection continues, even when conventional fall protection methods are determined to not be feasible."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:e93a80d1-d174-4f0d-abe4-ff2d0efdc7f3>","<urn:uuid:8569ef61-117f-4a4b-8ab6-a8a673a411ae>"],"error":null}
{"question":"Can both mixed media tattoos and traditional Chinese symbol tattoos incorporate text elements effectively?","answer":"Yes, both styles can incorporate text elements but in different ways. Mixed media tattoos often combine lettering with photorealistic images and abstract patterns, frequently using keywords from poems, songs or literary works as part of the overall artistic composition. Chinese symbol tattoos, on the other hand, use traditional Hanzi characters which themselves are the central artistic elements, with each character having specific meanings (like love, strength, family) drawn from a system of over 40,000 characters. However, while mixed media tattoos intentionally juxtapose different styles including text, Chinese symbol tattoos focus on the aesthetic and semantic value of the individual characters themselves.","context":["Like everything, tattoos evolve. Gone are the muddy green tattoos of yesteryear: images of anchors and eagles, or a broken heart with an arrow through it.\nAs technology has improved, tattoos have morphed into multicolor images in an amazing range of designs, often displaying extraordinary detail and created with great technical skill. And it hasn’t stopped there.\nWith mixed media tattoos that ingeniously combine different visual styles it’s all been taken to a new level. It’s not over the top to say that mixed media tattoos elevate ink from simple craft into the domain of contemporary fine art.\nWhat are Mixed Media Tattoos?\nWhat is this mad and exciting mixing of visual styles all about? Traditionally, as in most art, all the elements of a tattoo are styled to work with each other to form a coherent whole. In art, every style has its own particular characteristics.\nYou don’t get art nouveau swirls in an angular art deco piece. One is built around flowing lines and the other depends fundamentally on geometric forms and straight lines.\nMixed media tattoos break all those rules about unified wholes and styles that are composed of matching and coordinated parts. Instead, they put together elements you’d never normally expect to see alongside each other.\nUsually, tattoos are made in the style of a particular artistic medium and are mainly, or entirely, of one visual type.\nThat might be imitation brush strokes, echoing the medium of watercolor painting; or typographical letters and symbols, as in word tattoos; or photorealistic images that (as the name says) look just like a photograph.\nOther elements that are used in mixed media tattoos include abstract patterns and texture effects. These are combined and juxtaposed in innovative and striking ways.\nA typical mixed media tattoo features a photorealistic image at its center, with surrounding details in completely different styles. Often that’s a portrait of a person, but not necessarily. Almost anything goes.\nThink of a nineteenth century scientific illustration married with patches of angular geometric motifs, or lettering, or other modern graphic elements. Or a photo of a contemporary cultural icon combined with cartoon-style characters against a textured background.\nThe exciting thing is that there are so many possibilities for new and unusual ideas. These tattoos usually make rich use of color, though elements such as text or photograph-like images may be included in a black (or black and gray) palette.\nA Contemporary Tattoo Style\nIn some ways, mixed media tattoos follow on from new school tattoos, which emerged around the 1970s and 1980s. That tattoo trend introduced bright colors, bold lines and new subject matter, with cultural references ranging from famous actors to Disney characters.\nLogic says that the different styles used in mixed media tattoos are just incompatible, but in the hands of a skilled artist, the result is like something you’ve never seen before. It shouts ‘twenty first century’ in every way.\nMany of the leading mixed media tattoo creators working today have a background in art and graphic design.\nDave Paulo, a prominent tattoo artist based in Portugal, started out as an architect. Volker Merschky and Simone Pfaff began their careers in graphic design and interior design, but now carry out their tattoo work in tandem with painting, photography and music.\nThe work of these tattoo artists has been described as experimental. It could also be described as cutting edge, because it really is at the forefront of contemporary tattoo art.\nGerman artists Merschky and Pfaff’s own style of mixed media tattooing has even been trademarked under the name ‘Trash Polka’ – ‘trash’ because it violates conventional standards of beauty and ‘polka’ because the artists compare their work to a musical composition.\nTheir designs synthesize the trio of components that are typical of mixed media tattoos – photorealistic motifs, abstract and surreal images and lettering, typically using a mainly red and black palette.\nWhat do these Tattoos Mean?\nSo much for the style – but what about the images used? What do these tattoos mean and what message do they send? Merschky and Pfaff have described how they work.\nClients provide a rough idea of the themes they like, and some significant keywords from a poem or song or literary work. The tattoo artists then create a design on a computer. It’s a lot different from going to a tattoo parlor with an image of what you want and just getting it replicated.\nBut the key thing about mixed media tattoos is that the meanings of the motifs aren’t necessarily the most important thing.\nThe meaning lies just as much in the design (especially in the relationships between the different components of the tattoo) and the creative skill of the artist/designer. It’s the artist’s ability to generate dynamic designs with the wow factor that really makes the magic happen.\nSo the point of these tattoos is that the visual effect is the thing, rather than the specific motifs featured. It all refers to some sophisticated ideas in art theory and art history. Practicing artists know that art isn’t just about ‘representing’ something that exists in the world.\nArt is all about how that thing (or concept) is made into something you can see. Think of the human figure. In the hands of a non-artist it could be a stick figure. An artist makes it into something so much more – and that’s where much of the meaning is.\nArt doesn’t have to have any coded meaning at all. Its primary meaning can be its visual impact. That’s exactly what mixed media tattoos aim to do – look amazing, without necessarily having any profound significance.\nAre Themes and Motifs Irrelevant?\nThemes and motifs doesn’t have to be irrelevant. If you want a mixed media tattoo you’ll still want the images and styles used to be something you like.\nPhotorealistic images of skulls regularly appear as the central motif in mixed media tattoos but, let’s face it, not everyone likes skulls. The trick lies in the treatment of the motif, whether it’s your dog, Mickey Mouse, Marilyn Monroe or King Kong.\nOne technique is to mix the styles that are recognizably from different eras. The central image could be a piece of classical sculpture, against a backdrop of lettering in a funky, very modern font.\nAnother example of contrasting the old and the new can be seen in the work of the American tattoo artist John Yogi Barrett. His hallmark style is called ‘neotraditional’.\nHe experiments with combining realistic portraits, which are a recent innovation in tattooing, with the old-school style of tattoos that dates back to the nineteenth century. The only limit is the artistic imagination.\nRelated Post: Portrait Tattoos and Their Meanings\nIf you like the idea of ink that really puts the art into body art, mixed media tattoo might be just the thing for you. Note that they’re usually quite large and they’re definitely not understated. And be sure to find a tattoo artist with a track record, who can be sure to achieve the right result.\nIt’s not just a matter of throwing together a bunch of mismatched images, patterns and styles. It still takes a skilled eye to make sure they work together to achieve the effect. It may be a clash of styles but you need a true artist to do it properly.","Tattooing in one form or another has been practiced for centuries. Originating in Asia, the art of tattooing for religious meaning, cultural tradition or just for appearances has spread worldwide. While some tattoos are chosen because the wearer has an affinity for the artwork, many tattoos have a universal meaning that goes beyond the basic graphic.\nThe following are a few examples of the meanings of tattoos that have a universal symbolism for wearers, artists and admirers.\nThe Maori, the native tribe of New Zealand, are known for their distinctive facial tattoos. The tattoos are not restricted to the face, but usually take up a part or even all of the face. The deep grooves and patterns of Ta Moko, the name the Maori give the process of making the tattoos, are a storytelling device. People from the tribe who wear these tattoos are retelling the history of their family. In fact, the tribe considers people that are not Maori, but copy and wear the tattoos, phonies who are performing a form identity theft. If a person wishes to wear tattoos in the style of the Maori, they should seek out an artist who is familiar with the original patterns. An artist experienced with this style of tattoo can create one in the style of the originals without copying exactly.\nChinese Symbols Tattoo\nWearing Chinese symbols has become extremely popular in the Western world, however they are not popular in China. When a Chinese character is used for a tattoo, more often than not it is in the form of traditional Hanzi. The written language of Hong Kong, Taiwan and Macau, Hanzi consists of over 40,000 characters. There is also the option of Simplified Chinese, created to further literacy in mainland China, Singapore and Malaysia. Although they look like delicate little works of art, each symbol has its own meaning. The most popular Chinese symbols used for tattoos are love, strength and family. People who wish to wear these beautiful tattoos must do their research and make sure they fully understand which character they choose. Although all the characters are beautiful to look at, sometimes a person requests a tattoo with a certain meaning, but for whatever reason, ends up with a tattoo that has a decidedly different meaning.\nThe teardrop tattoo first appeared in the 1960’s. The meaning of this tattoo can vary depending on how it is drawn. An empty teardrop can symbolize a friend or a loved one that was killed, and now the wearer of the teardrop is looking for revenge. The teardrop is filled in with ink when revenge has been achieved. In prison, the teardrop can also be a warning to other prisoners that the wearer has committed a murder.\nThe dolphin has been considered a highly social and intelligent animal with a special connection to the human race. Dolphin tattoos can have several meanings. Since dolphins have a playful nature, many times people with the same attitude and personality will get the tattoo to represent themselves. The dolphin can also represent being free or achieving a dream. Another common meaning of this tattoo relates to the mammal having a dual nature. It is from the sea but it breathes air and a dolphin tattoo wearer may be representing their own dual nature with their tattoo.\nThe butterfly is an image that often represents change or a metamorphosis. A caterpillar changes into a butterfly and very likely the wearer of a butterfly tattoo has gone through a transition or a transformation. The butterfly can demonstrate a literal change, such as growing from a child to an adult, or a symbolic one, such as persevering through a difficult time.\nThe rose tattoo is a complex and has many different meanings depending on the color and style of the artwork. Worn by both men and women, the rose can mean love or inner and outer beauty. Thorns added to the rose may serve as a reminder that beauty is only skin deep. A single rose with thorns can mean the pain a person had to go through to achieve true love. A rose that is black will mean love lost or even a memorial. White roses represent innocence and yellow can mean faith and hope.\nLike the rose, the meaning of the clown tattoo changes depending on how it is drawn. Clowns can be happy, sad, evil and jesters. While the clown tattoos can be straightforward, they can also represent humor, sadness and uncertainty. These tattoos are sometimes worn by gang members with a laugh now and cry later or a play now and pay later meaning behind the clown image.\nRichly symbolic in literature and mythology, the phoenix rising out of the ashes is a straightforward image in tattoo art. Resurrection, rebirth and reaching a new height is what the phoenix means in any context. When choosing the image of the phoenix, people tend to choose the phoenix in the process of rising up from the ashes.\nThe star tattoo is one of the oldest, and it has behind it a rich history of meaning. To the sailors who sported the tattoo in the past, nautical stars represented safely navigating home. It has similar meanings now. The tattoo is popular with people in the armed forces and represents returning home safely. Another very different meaning for this tattoo belongs to prison culture. Each point on the star represents a year that a person has spent behind bars.\nWhile there are commonly found meanings in tattoos, it is also necessary to note that sometimes the meanings of tattoos are nothing more than someone liked the way it looked in the book. As any person who sports a tattoo will agree, sometimes a rose is just a rose.\nWolves are seen as symbols of courage, loyalty and strength in Native American culture. The Romans and the Greeks considered the wolf a sacred animal and a sign of power. It is essential to note that the wolf is viewed as a bad omen in the Celtic and Chinese cultures.\nThe dove is one of those images that has a historic meaning that goes back for centuries. On its own, the dove represents peace. When two doves are pictured together, they can symbolize two people in a loving relationship. To demonstrate a fresh start or a rebirth, often a single dove with a palm branch will be used for the tattoo.\nThe signs of the zodiac are used in tattoos when people identify with the attributes associated with their sign. There are positive and negative qualities, images, and colors associated with each of the 12 signs that can be incorporated into an individual’s zodiac tattoo.\nImages of the sun are found throughout many different cultures going back to ancient times. Seen universally as a giver of life, a few of the many meanings behind the sun tattoo are strength, endurance, fertility, and rebirth. The sun can be drawn in several different styles depending on if it is representing a specific tribe or not. When the sun and the moon are drawn together, the meaning of the tattoo changes and it becomes feminine. Together, the sun and moon represent a man and a woman who are joined to each other spiritually.\nWhile the peace sign is a highly recognizable symbol, there are many ways that people who wear it can add variety to it and make it a more personal image. People with friends or family in the military will add a yellow ribbon or dog tags to signify their military connection. A peace sign can also represent that a person who is going through a rocky transformation or a tumultuous time is keeping their mind focused on the goal. Drawing around the peace sign or within the peace sign also turns the symbol into something more meaningful. Often people will request the flag of a country or a dove to make the picture more meaningful.\nThe tiger has often been considered the leader, or the king, of the animal kingdom. When a person chooses a tiger for their tattoo, they are using the image to convey strength and power. Tigers can also mean pride, beauty, passion, violence, and in some cultures, punishment for those who have done something wrong. The Japanese samurai use the tiger as their primary emblem for all of the same reasons that people choose to have the tiger tattoo. People who are interested in the tiger tattoo should bear in mind that white tigers are only the result of inbreeding, and thus all white tigers are born with handicaps of some sort. This information should be considered by anyone who wishes to have a white tiger tattoo instead of a standard tiger. Siberian tigers can reach a length of over ten feet and represent the greatest power among all the images of the tiger.\nThe octopus tattoo has a significant meaning to those who choose to wear one. Famous for being able to regenerate a lost limb, the octopus can represent a trauma that a person has successfully triumphed over in their past. Many cultures consider the number eight a mystical number, because it is the symbol for infinity. The octopus has eight legs and becomes sacred when it represents the nature of the sacred number.\nThe heart tattoo is one of the more famous images, and it is a longtime favorite for tattoo enthusiasts. At the turn of the century sailors often sported the tattoo with the name of a girlfriend, wife, or their mother as a lucky charm. The most obvious current meaning of this tattoo is love. Wisely or unwisely, it is often used to represent a romantic relationship when names are written on or around it. More obscure meanings include representation of the soul or the spiritual center.\nKokopelli was worshiped by Native Americans and can have a variety of meanings depending on the tribe. Commonly, the god is considered a representative of fertility and agriculture, and the overall theme is renewal and growth. He is also associated with tricks and has a fun loving persona. When Kokopelli is drawn, he is often holding what some believe is a flute and others say is a blowgun.\nThe rose may be in a category by itself, but many flowers have their own meanings when they are represented in a tattoo.\n- Pansy – Seen as the flower of remembrance and togetherness, this flower can be used when a tattoo is meant to be a memorial of a friend or loved one.\n- Cherry Blossom – Known mostly as a feminine tattoo, the cherry blossom represents beauty and love in China, and how brief life is in Japan.\n- Lily – The lily has many different meanings, and depending on the origins, it can actually have opposite definitions. In ancient Egypt, the lily symbolized fertility. In medieval times it was a sign of purity. The tiger lily often represents pride or wealth, and the calla lily can be an image that represents physical beauty. The orange lily can mean hate or anger.\n- Daisy – This is another flower that is closely associated with women and is more common for a woman to wear. They are usually drawn delicately and are small. The daisy represents purity, innocence in and loyalty in a romantic relationship.\n- Sunflower – A large and bright flower, the sunflower is usually drawn as it is in real life. It can often represent loyalty and strong admiration."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:cb6d4bd6-2cdd-40c8-aac2-d9ecfcce641d>","<urn:uuid:18f4f9c4-72e4-4920-9c0f-52edc844ceb7>"],"error":null}
{"question":"How has the business model of journalism evolved, and what are the modern challenges in maintaining journalistic standards?","answer":"The business model of journalism has shifted from traditional family-owned media outlets to new ownership patterns, with digital millionaires buying or starting news media outlets - similar to how successful businesspeople founded newspapers during previous industrial revolutions. Meanwhile, maintaining journalistic standards faces significant challenges: objectivity has been replaced with bias, integrity has been compromised by advertising dollars and sponsorships, and fact-checking skills are declining. Many new media initiatives, while journalistically relevant, struggle with funding and haven't yet found sustainable business models, particularly those supported by international foundations.","context":["Gumersindo Lafuente has been innovating in digital media for more than a decade. Most recently, he launched Por Causa, a nonprofit foundation that uses data journalism to investigate the causes of poverty and inequality.\nHe talked about his new initiative in Part I of his interview with IJNet, here.\nIn Part II, below, Lafuente discusses lessons learned from a previous startup, which projects inspire him and why journalists must embrace tech.\nGumersindo Lafuente: Soitu has nothing to do with Por Causa; the time when it was launched was different, and so was the model. Soitu was born in a time that is very different from the one in which we live today. Back then we were, in a sense, at the highest point of the financial bubble. It was a very ambitious project that had a purely journalism side to it and also a side related to technology for journalism.\nAt that time, considering the economy and the type of funding we got, we decided to make it a very ambitious project.\nOur project was certainly so innovative that it would have been a much better fit in today's social media world than when it was born. Soitu was a project almost tailored to the world of Twitter and Facebook. When Soitu was launched, Facebook did exist, but it didn’t have the relevance it has today, and Twitter didn’t exist. Soitu and Twitter were born almost simultaneously.\nIt is easy to find lessons today when so many years have passed and [the economic crisis in Spain is so deep] but at the time it wasn’t so simple. If it were simple, we wouldn’t have made certain mistakes. I think we did make some mistakes.\nOur project was much more interesting and more powerful than we thought, especially the technological aspect of it. And perhaps we failed to realize the full potential of the technology our company had, so that the journalism aspect of it could survive. We didn’t have it easy either. Our shareholder, who was extremely generous at first, didn’t make it easy for us at the end. The violent collapse of the world economy, and of the Spanish economy, didn’t make it simple for us either.\nIJNet: Are there any new media startups that have caught your eye?\nSome of them are niche sites, others focus on conflict, others on politics…. They are very relevant from the journalism standpoint. They have a problem almost everyone has: funding. Many of them are funded by international foundations, some of them are from the U.S., and they haven’t found a sustainable business model yet.\nI think that will come about. There's one in Argentina that I find very interesting: Chequeado.org. Another interesting initiative is Poderopedia in Chile. Some of them are news outlets, others are sites that are dedicated to creating tools to [keep tabs on] the powerful.\nHere in Spain, there are new media, like El Diario.es, which has been around for more than a year, and InfoLibre. But although they are very interesting projects, they are still extremely traditional. Because of the economic situation in Spain, it is difficult to find funding for certain types of innovation.\nWhat we see is that traditional media publishers don’t seem to find the way, and they are leaving that field open for innovative people. Things like the purchase of the Washington Post by [Jeff] Bezos will keep happening, and we'll see in the coming months or years how digital millionaires will buy or start news media outlets.\nThis isn’t really new. In every industrial revolution, the people who made money founded media outlets. That happened in Spain in the late 19th century and happened again in the early 20th century. Those who were successful in business either bought or founded newspapers. Something similar is going to happen now, after years of media outlets being in the hands of families that were passing the baton from generation to generation.\nIJNet: What skills do journalists need today? What advice would you give to journalism students?\nGL: There are general tips that have to do with being aware that journalism is an extremely delicate job, which you have to [handle carefully] at all times. Also, take into account that our world is now experiencing an absolutely revolutionary change, and therefore there are many issues on which there are no absolute truths.\nThird, do not be afraid of innovation, and do not be afraid of technology. A few years ago you would advise a journalist to travel a lot and learn many languages. Today, although it's still important to have curiosity and perseverance, it is essential to understand technology. It's as important today as learning to speak a new language. And understanding that technology is what allows journalism to be competitive and innovative.\nFor students, besides being passionate about journalism, if they know their way around technology, they will have a much brighter future in journalism.\nMaite Fernández is IJNet’s managing editor. She is bilingual in English and Spanish and has an M.J. in multimedia journalism from the University of Maryland.\nThis interview was conducted in Spanish and translated into English by Maite Fernández.","There’s a lot wrong with journalism these days. Objectivity has been replaced with blatant bias. Integrity has been lost to advertising dollars and sponsorships. Fact-checking and research are dead skills. True journalism is a dying art.\nIf you want to be a journalist — whether for news, sports, food, or whatever other topic — then there are some important things you need to know before diving in. And while you’d benefit greatly by attending journalism school, maybe you can’t afford to right now.\nFortunately, the Internet is chock full of free resources for learning journalism. Here are some of the must-read (or must-watch) materials that you shouldn’t overlook or miss.\nTexts and Books\nEthics is arguably the most important aspect of journalism — more so than writing, editing, interviewing, etc. After all, what is journalism? According to the Society of Professional Journalists (SPJ), journalism is:\nThe free exchange of information that is accurate, fair and thorough.\nAnd that simply isn’t possible unless you, the journalist, approach each piece with an ethical mindset that promotes integrity above all else. That’s why the SPJ has a free Code of Ethics available online that you can reference at any time from anywhere.\nAccording to the SPJ, ethical journalism adheres to four principles:\n- Seek truth and report it.\n- Minimize harm.\n- Act independently.\n- Be accountable and transparent.\nCheck out the full Code of Ethics to learn more. You can even download it as a handy PDF for print-outs.\nHere is a free PDF handbook by Deborah Potter, who founded NewsLab (an online resource center for journalists) and taught journalism at The Poynter Institute. She’s even been a correspondent for CBS News and CNN.\nIn short, she knows her stuff — and as you read this handbook, you’ll realize that she really knows her stuff. This 68-page PDF document covers seven critical aspects of modern journalism:\n- What Is News?\n- Getting the Story\n- Telling the Story\n- Editing the Story\n- Broadcast and Online\n- Specialized Journalism\n- Ethics and Law\nOverall, it’s easy to read but densely packed with useful tips, concepts, and guidelines for being a great independent journalist in this era of fast information. Download the free PDF while you can!\nWriting for Success is a textbook that’s 100% free to download and read. It starts with the basics and takes you through the most important bits of the English language, eventually leading you towards critical thought and careful selection of which words to use when writing.\nThis book isn’t just about mastery of the language itself, but the ability to use language to convey what you really mean. Words are the only tools available to journalists, so it makes sense to sharpen those skills as much as possible, right?\nWriting for Success is available in five formats: PDF, EPUB, MOBI, WXR, or web online.\nCommunication Theory is one of the featured books on Wikibooks, which is an online repository for open-content textbooks across all kinds of topics. Some wikibooks are incomplete or lacking in good content, but this one is absolutely spectacular.\nIn short, Communication Theory explores various theories on how humans share, encode, and decode what they know, what they need, and what they expect from each other. It’s not a book specifically about journalism, but the concepts within will prove useful to any good journalist.\nHowever, be warned that this book is dry, dense, and tough to get through. You may want to put it towards the back of your reading list. Communication Theory is also available in a printable version and a PDF version.\nDid you know that the BBC has a free online library of resources for people interested in learning about journalism, broadcast, and production technology? Despite how you might feel about the BBC itself, these resources are incredibly useful for any budding journalist.\nThe Journalism section is broken into seven main sections:\n- Skills (e.g. interviewing, reporting, writing)\n- Subject Guides (e.g. religion, military, science, sport)\n- Law (e.g. courts, copyright, privacy)\n- Values (e.g. impartiality, truth, accuracy)\n- Safety (e.g. hostile environments)\n- Getting a Job\n- BBC’s Style Guide\nEvery entry is a short and concise article with an accompanying video. Is it a legit replacement for journalism? Not even close, but it will teach you a lot of good things and give you a smart starting place for further research.\nNews University is a free online resource run by The Poynter Institute, which is a non-profit school specifically geared towards journalism. The Poynter Institute is actually the owner of the Tampa Bay Times, so everything you see here is based on real expertise and experience.\nThere are two ways to take advantage of News University. First, check out the available Journalism and Training Resources that cover tips and articles related to reporting, writing, editing, etc. Second, enroll in one or more of the University’s free webinar courses.\nThe webinar schedule is constantly updating with new courses, but some are available year-round. For example, the Cleaning Your Copy and Freedom of Information courses are self-directed and very useful for new journalists.\nNews College is a simple website with a lot of brief but useful tips ranging across multiple areas, including reporting, writing, editing, story hunting, and more. It isn’t being actively updated at this time, but you can still use it as a kind of archive for journalism tips.\nAgain, a lot of the tips and articles are short and succinct, so I wouldn’t use this website as my only source of knowledge and training. However, it makes for an awesome long-term reference of tips. Just bookmark it and come back to it from time to time to refresh yourself on the basics.\nThe Knight Center for Journalism is a special program at the University of Texas at Austin that aims to train and empower journalists in Latin America and the Caribbean. In short, it wants to help raise the ethical and professional standards of journalism.\nWhile the program has offered distance learning opportunities in the past, there hasn’t been one since 2014. Instead, the best way to take advantage of this site is to visit the digital library of free books where you’ll find a handful of books that can be downloaded free of charge.\nLEARNO is a new online learning platform that launched in late 2015. It’s still new so don’t expect much from it in terms of selection, but it’s noteworthy because it currently has four courses — all related to journalism — that are all free.\nThe four courses are:\n- Verification: The Basics\n- Google Search for Journalists\n- Managing Data Journalism Projects\n- Doing Journalism with Data\nAll four courses together run close to 500 minutes in length, which is a hefty amount of content to get for free.\nWhere Else to Learn Journalism Online?\nThe Internet is a deep and vast resource and we’re aware that there are probably dozens — maybe even hundreds — of other free resources and courses for learning journalism online. You may even want to check out the courses on Udemy or these other online course sites.\nAnd don’t forget that journalism is more than just CNN and BBC these days. Citizen journalism on social media is becoming more and more prominent, and these skills can help you improve in that way as well.\nDid you find these online resources useful? What other sites are good for learning journalism online? Is journalism school necessary? We’d love to hear your thoughts in the comments below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:84d7c67e-0bda-41ad-a7ff-8b2cad383f13>","<urn:uuid:cda41a36-c280-4d44-8bf7-626c0c64758a>"],"error":null}
{"question":"What are the main structural defects that can cause concrete deterioration?","answer":"The main structural defects that can cause concrete deterioration include: design or construction errors that result in stress cracking or foundation movement, corrosion of steel reinforcement due to inadequate concrete cover or compaction, chemical attack from acids or sulphates, fire damage leading to spalling and cracking, substance reactions in concrete aggregates, thermal contraction or expansion issues, shrinkage during curing, structural creep from inadequate design, plastic shrinkage during water evaporation, plastic settlement in new concrete, and construction faults such as lack of steel reinforcement cover or honeycombing in concrete.","context":["Prior to commencing repair of concrete structures, it is always advisable to investigate the possible causes of this degradation. Nothing will be gained by carrying out a repair in haste.\nCauses of deterioration may be divided into recurring and non-recurring:\n- If the recurrence of deterioration is acceptably low, then it is normally acceptable to restore the structure as nearly as possible to its original state.\n- If there is an unacceptable risk of recurrence, the structure should be repaired and the fundamental cause of deterioration should be eliminated or controlled to acceptable limits.\nOnce the cause of deterioration is known, a decision of the extent of repair is required. This includes the parameters of durability, strength, function, and appearance of the structure after the repair process is completed.\nAfter the above decision is made, the choice of repair material and repair technique could be investigated.\nCauses of the concrete deterioration of a concrete structure:\n- Structural Defects: Design or construction errors resulting in stress cracking or foundation movement.\n- Corrosion of Steel Reinforcement: Concrete deficiencies in cover, compaction, presence of chloride, rusting resulting in spalling (i.e. concrete cancer) or cracking of overlying concrete.\n- Chemical attack: Etching or softening of the surface may be caused by acid attack or concrete may crack or spall due to sulphate attack.\n- Fire damage: A fire can cause spalling and cracking.\n- Substances in Concrete Aggregates: Alkali aggregate or sulphate reaction causing concrete expansion with cracking and spalling. This occurs slowly over many years.\n- Thermal Contraction or Expansion: Concrete contracts and expands with variations of temperature and when the contraction or expansion is restrained cracking or spalling may occur.\n- Shrinkage During Curing: As concrete sets shrinkage occurs and if this is restrained cracking may occur soon after construction.\n- Structural Creep: Inadequate design may fail to allow for long term creep due to the shortening of columns or deflection of suspended floor slabs and beams causing cracking or damage.\n- Plastic Shrinkage: Evaporation of the water content of concrete during the curing process may cause fine plastic cracks if not fully protected by curing compounds.\n- Plastic Settlement: New concrete can hang on formwork or reinforcement causing cracks in columns, beams or walls.\n- Construction Faults: Lack of cover over steel reinforcement, honeycombing in concrete, voids in concrete, blow-holes in concrete surfaces.\nIdentification of the causes of the deterioration of the concrete structure, extent of damage and recommended remediation methods:\n- Visual inspection\n- Review of construction documentation such as specifications, drawings, geotechnical reports, quality control on concrete batching and concrete mix designs.\nDepending on the extent of deterioration testing may include any combination of the following:\n- Visual examination\n- Laboratory testing of samples\n- Non-destructive testing\n- Chloride content\n- Sulphate content\n- Concrete strength\n- Moisture content\n- Compression tests\n- Cover over steel\n- Reinforcement deterioration\n- Nature of cracking\n- Aggregate quality\n- Concrete mix design\n- Rate of deterioration\n- Rebound Schmidt hammer tests\n- Ultrasonic measurements\n- Pull-off tests\n- Break-off tests\n- Core tests\nGenerally experienced concrete repair technicians will identify the possible causes and the recommended solutions after a visual examination of the degraded areas.\nThis depends on the extent of damage and the feasibility of repair.\n- Extent of corrosion of reinforcement\n- Extent of loss of strength of reinforcement\n- Extend of loss of the bond between the reinforcement and the concrete\n- Extent of deficiency in concrete cover over reinforcement\n- Extent of deflection due to cracking in the tensioned areas.\n- Extent of honeycombing in concrete\n- Extent of porosity of concrete\n- Extent of damage and loss of strength due to sulphate attack.\n- Extent and width of cracking\nThis will determine to nature and type of repairs required or whether repairs are feasible.\nIf repairs are not feasible then complete replacement of the damaged structural element, such as, beams, columns, slabs, stairs etc may be unavoidable.\nTHE MORAL TO THE STORY IS THAT IMMEDIATE ACTION MUST BE TAKEN WHEN THE PROPERTY OWNER BECOMES AWARE OF THE EARLY SYMPTOMS OF CONCRETE DEGREDATION OR DETERIORATION IN ORDER TO AVOID A MASSIVE ESCALLATION IN THE COST OF REMEDIATION OR , IN THE EXTREME, THE NEED FOR COMPLETE REINSTATEMENT OF THE STRUCTURAL ELEMENTS."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:29fb4f3a-ad13-420f-9028-dd4c352377aa>"],"error":null}
{"question":"What are the fundamental uses of histograms for exposure control in digital photography, and how do RGB color histograms provide additional advantages for image analysis?","answer":"Histograms are essential tools for achieving perfect exposure in digital photography by providing a graphical interpretation of brightness values, with 256 discrete levels from 0 (total black) to 255 (total white). For proper exposure, it's recommended to keep important details between values 10-245. The histogram helps verify if exposure matches scene values - dark scenes should weight left, bright scenes right. RGB color histograms offer additional advantages by displaying individual red, green and blue channels separately. This enables detection of color-specific clipping that might be missed in a luminance-only histogram. For instance, the red channel could show clipping while the overall luminance histogram appears normal. Color histograms can also help evaluate white balance - when spikes for all three colors align at the same position, it indicates neutral/balanced color (though this may not apply to naturally color-dominant scenes like blue skies).","context":["The Histogram Is Your Best Friend\nby Steve Bedell\nThere are three things that constitute a “perfect” digital file. Perfect exposure, accurate white balance, and a controlled brightness range. In this article, we’re going to focus on the first—perfect exposure. And in the process, we’re going to spend a lot of time giving you a much better understanding of just what those spiky graphs on the back of your camera mean.\nFirst, let’s get a definition of what a histogram is. I’ve seen it described in many ways but I think the best way to describe it is this: A histogram is a graphical interpretation of the brightness values in the photo. If you think about that for a minute, unless you spend your days taking photos of Kodak Gray Cards, each histogram will look different. There is no such thing as a “‘perfect” histogram. There seems to be some misconception out there that a good histogram will look something like a hill, with little information on each end and plenty of information in the middle. Nonsense. If you photograph a black cat on a dark background, the only information will be on the left (darker values) side of the histogram. Take a photo of snow, and the opposite will be true.\nLet’s look at those values and what they stand for. The accepted wisdom is that current digital cameras can capture about a five stop dynamic range. We’ll have to touch on brightness range here briefly for a full understanding of the histogram. That means that if the difference between the darkest and brightest value in the scene is five stops or less, you should be able to have detail from the darkest shadow to the brightest highlight—if you’ve exposed it properly. If the scene that presents itself is greater than five stops, such as a strongly backlit situation, then some areas of the scene will not have detail and it will show on the histogram as a line shoved up against the left or right side. How much and what sections of the photo are without detail will be determined by your exposure. If it’s a static scene, multiple exposures can be taken at different values and then merged together, but that is beyond the scope or intent of this article.\nWhat about the philosophy that as long as it’s close, we can fix it later in Photoshop? That’s not a very good idea. Whether you shoot raw or JPEG, proper exposure is the foundation of an excellent file. Even though we have many tools at our disposal to help “correct” our mistakes, it is much easier and less time consuming to just get it right in the first place. A thorough understanding of exposure and the role the histogram plays in supplying us with valuable information is the first step in creating high quality files.\nLet’s take a look at the values a histogram will show us. There are 256 discrete levels of brightness. The left edge starts at zero and represents total black. The far right ends at 255 and is total white. Anything left of zero will be black devoid of detail. Anything right of 255 will be burned out highlights with no detail. In practice, it’s a good idea to give yourself a little breathing room and not allow any values that you want to have detail to be between 10 and 245—about 10 points on each side.\nI made a little “L” graph on the left side of the histogram. The horizontal axis shows you the values as described above. The vertical axis is about volume. The more pixels in a given value, the higher the spike.\nThe photo of model Heather Fournier shows lots of information in the center of the histogram and less towards the edges. You can see by looking at the photo that that’s just how it should be. I marked in red the very small area you can see on the right side of the histogram that represents the overexposed highlights, also marked in red on the image. You would not adjust your exposure because of these. If you found them objectionable, you could change camera position, move the model, or clone them out.\nUsing the histogram in conjunction with your camera meter should allow you to get perfectly exposed images very easily once you understand just what’s happening. Use your preferred in-camera metering method—spot, center-weighted, matrix, or even a hand held meter. Take a look at the image and the histogram together. Do they match? By that I mean do the values in the scene coincide with the values in the histogram? Lots of dark areas will give you a histogram weighted to the left and the opposite is true. A scene with a wide variety of values should give you values throughout the histogram. If you see room for improvement, just use you cameras plus or minus exposure compensation controls until the values fall where they should be.\nA Tale Of Three Histograms\nAll this talk about values and where they go may make a lot more sense to you once you see the histogram and the photo together. So let’s look at three different photos and their histograms so you can really visualize what we’ve been talking about.\n1. White Snow\nI’ve included the horse and building on the side as a reference but you can see when you have a scene where most of the values are white, then a proper exposure will show a histogram with most of the information on the right side. The highest value I can find in the image using the info tool in Photoshop is 245, which will allow me to have detail even in the brightest highlights.\n2. Studio Portrait\nLook at the studio portrait of Dan Kleinmann. He’s got dark clothing against a dark background. The only light areas are his face and hair. So most of the values are very low, as represented by the histogram. Note that even though the values are crowded up against the left side, the lowest number I could find using the info tool in Photoshop is about 15. That is why the values are close to, but do not touch the left side. There is still some detail in the darkest values.\n3. White Tiger\nThis photo was taken using aperture priority and matrix metering with no compensation. Since there are dark, light, and middle tones in the image, we have a histogram that shows a wide range of values throughout the image. I marked in red the area that the info tool is showing.","Color Histograms are included in many digital cameras today and offer some distinct advatanges over the standard luminance histogram.\nIn order to understand why color hisotgrams are an important but often overlooked tool for photographers, we will first discuss what color histograms are. Then we will look at some of the advantages of a color histogram. And finally we will discuss what can be learned from a color histogram and how to use that information to improve our photos.\nUnderstanding color histograpms starts with understanding histograms in general. In photography a histogram is simply a graphical representation of the number of pixels in the image that fall within a certian range, either luminance or color. For example for a normal luminance histogram the graph shows the number of pixels for each luminance or brightness level from black to white. The higher the peak on the graph the more pixels are at that luminance level. With a color histogram the principle is the same but instead of seeing the levels of black graphed you will now see the number of pixels for each of the three main colors.\nLearning to read and understand a histogram can be a great way to double check that our photo is properly exposed and to make quick exposure adjustments so we end up with a better exposed photo.\nWhat are color histograms?\nA color histogram is a simply a histogram that shows the color level for each individual RGB color channel. Also known as “Three Color Histograms” these displays are found on some DSLR and high end cameras.\nA three color histogram is read similar to a normal luminance histogram. However instead of showing the distribution of pixels from black to white as with the luminance histogram, a three color histogram shows the brightness distribution for each color individually. This is important because you can easily see if one color is overexposed and clipped which means that you might be loosing important detail for that color range.\nThere are two basic types of three color histograms. The first is the RGB histogram that shows a combination of all three colors and possibly even the luminance histogram all together. The second are individual histograms for each separate color.\nWhat is the advantage of a three color histogram?\nThe advantage of a color histogram is that you can see if individual colors have been clipped or are over or under exposed.\nThis is important because it is possible for a luminance histogram to show little if any clipping while a three color histogram would show clipping on one color channel. This would result in a loss of texture or detail for that color similar to what you get with an over-exposed image.\nIn the photo above you see that th luminance histogram shows no clipping (spikes on either the black or white side of the scale). Yet when you look at a color histogram for the same image you can easily see that the red channel is slightly clipped which means you could be losing some detail in the highlight areas of that color range. There is also a spike on the left hand side of the blue histogram which could mean that the blue channel is slightly clipped in the shadow areas.\nGenerally having some clipping on just one color channel is not as much of a problem as when all three colors are clipped but it really depends on your image and the look you are trying to capture. That is the case for the flower image above. While the red channel is clipped the image still retains enough luminance detail in the other colors that the clipping is not really an issue in this photo.\nWhat can I learn from a color histogram that is different than a normal luminance histogram?\nColor histograms can help you determine if the white balance of your image is correct. If the spike for each color is located at the same place on the histogram for each color channel then the photo is balanced or neutral. This can be a good way of doing a quick check of your white balance but it is important to realize that it does not work on all images. For example an image that contains a lot of one color such as a blue sky will usually have the spike for that color offset even if the white balance is correct.\nColor histograms and luminance histograms are important tools for evaluating the exposure of a photo. If we see spiking or clipping on the right hand side of the image we know that the image is likely over-exposed a problem that can quickly be remedied by a quick adjustment to the exposure compensation. On the other hand if the spike is on the left hand side of the image then the shadow areas might be under exposed necessitating an increase in brightness.\nBoth over exposed and under exposed images mean we are gernally losing valualbe detail and a slight adjustment to our exposure can make for a much better image. Learning to read and use a histogram will help you become a better photographer."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f025bddf-f84b-42d9-9912-eac90171bed5>","<urn:uuid:fa83549a-f760-4628-8b4f-8f4fee90e8a3>"],"error":null}
{"question":"Could you compare the financial communication tools used in Raleigh's watershed protection efforts with Oregon's public water system oversight? What frameworks do they use to engage stakeholders?","answer":"In Raleigh, the Environmental Finance Center developed an interactive dashboard tool that visually demonstrates how small fee increases impact water bills and generate conservation revenue. This tool helps utility managers communicate rate changes transparently, showing how nominal fee increases (like the $0.08 per 1,000 gallons) affect both residential and non-residential customers. In Oregon, communication is structured through multiple state agencies, with the DEQ providing fact sheets to guide the public on water quality, drinking water protection, and stormwater permitting. The Oregon Department of Human Services focuses on increasing public knowledge and participation through their Drinking Water Program, while maintaining databases and maps to share information about water sources and contaminants.","context":["The challenges associated with designing and implementing water conservation finance strategies are multifaceted. First, stakeholders must agree on the initial environmental problem and forge an agreement of action. Second, participants must identify who should pay for protection efforts and where the money will come from. Third, local governments and utility mangers must be able to anticipate how different funding strategies will impact residents and non-residents. And fourth, officials must be able to communicate all of this information to stakeholders clearly, effectively, and transparently.\nNowhere are these challenges more at play than in efforts to clean up the Upper Neuse River Basin (UNRB) located in North Carolina’s piedmont region.\nThe environmental problem\nThe UNRB is comprised of three tributaries that flow from upstream sources, including the City of Durham, through the Falls Lake Reservoir, which is the primary water source for the City of Raleigh. Due to heavy urbanization, polluted runoff, and erosion, Falls Lake and many of its tributaries remain in a perpetual state of impairment.\nPaying for clean-up\nLocal governments and land protection organizations have joined forces to help protect land in the Upper Neuse. The partnerships led to the creation of the Upper Neuse Clean Water Initiative (UNCWI) with the goal of protecting surface water quality in the UNRB through land conservation. The City of Raleigh created a watershed protection fee to fund conservation projects which has created a stable recurring source of revenue that can be leveraged with funds from other organizations. Prior to the creation of the fee, Raleigh had provided a majority of local government funding through fixed annual appropriations. These appropriations were generally financed by a nutrient impact fee levied on new development. In total, Raleigh’s contributions channeled approximately $6 million between 2005 and 2009, serving as a reliable source of direct revenue from the city.\nModeling different financing strategies and understanding the impact on customers\nIn order to address these policy considerations, the EFC developed an interactive “dashboard,” a visual system that helps users model different funding scenarios and visually communicate performance metrics. The Capacity for Watershed Protection Investment Dashboard is a user-friendly and transparent tool that can help utility and city managers quickly showcase different revenue options for conservation goals and communicate the financial impacts on residential and non-residential billing. Users can select the target amount needed to fund capital or conservation costs, enter the loan term and interest rate, and quickly understand total annual contributions from residents and non-residents as well as long-term watershed revenue projections.\nCommunicating financial strategies effectively\nIncreasing customers’ water rates can be controversial and political. Given the contested nature of fees, it is hugely important for utility managers and local officials to communicate the impacts on customers in a clear and open manner. The graphic below, adapted from the Capacity for Watershed Protection Investment Tool, displays a $0.08 per 1,000 gallons sold fee increase in monthly water bills and total generated revenue in Raleigh. The bar graph on the left displays the percent increase in monthly bills, ranging from a 2.2 percent increase ($0.47) for residents to a 2.6 percent increase ($2.52) for non-residents. The table on the right features total revenue generated based on the $0.08 fee increase as well as annual funds for watershed protection over a ten-year period.\nOne of the most useful aspects of the dashboard is its ability to showcase how a small increase in base charges can generate significant revenue for watershed conservation. The ability to communicate this information to customers can lead to considerable advances in watershed conservation efforts. For example, in 2011 Raleigh and Durham’s utility staffs used the dashboard to demonstrate the revenue-generating capacity from a small increase in monthly water rates. That year, Raleigh passed a 1-cent per 100 gallon fee on all municipal water users, which costs the average household about 40-cents per month and generates around $1.5 million per year for land acquisition and other water quality protection projects\nOvercoming the communication challenge\nThe clean-up goals put forth under UNCWI highlight the need for effective, long-term financial planning. Although Raleigh, Durham, and other municipalities in the UNRB watershed are working to achieve pollution reduction goals, there is a constant push for additional watershed protection funds. Identifying effective financial strategies can be both technically and politically challenging, making clear and effective communication ever more important. The EFC’s dashboard is an innovative tool that can help managers communicate revenue-generating strategies in a way that is both visual and easy to understand.\nThe tool displays graphically what is hard to capture in words: nominal changes to water fees can mean significant opportunity for water conservation. Conveying that message is half the battle.\nReady to try it yourself?\nThe EFC created a generic Capacity For Watershed Protection Investment Dashboard in which users can input information about a specific utility’s current water, wastewater, and stormwater rates (check here for rates data for NC). The tool generates models showcasing various funding capacities to generate revenues for watershed protection. It also includes other options such as creating a “watershed fee” through property tax bills instead of the utility bill. Funds generated by these options can be used as a match for grants that require a cost-share. Alternatively, the funds can be used to amortize a loan, since the tool demonstrates to lenders how the funds will be generated for loan repayment.","States have the primary responsibility for ensuring the safety of public water supplies within their borders. Each state develops a system to administer programs that have been enacted by Federal legislation. In Oregon, water quality issues are addressed by three state agencies; the Departments of Agriculture, Environmental Quality, and Human Services.\nAmong the many goals of the Oregon Department of Agriculture is protection of the state’s natural resource base. Water is a crucial resource and is carefully protected through several programs administered by the Natural Resources Division.\nWith a mission to conserve, protect, and develop natural resources on public and private lands, this Division promotes the viability of agriculture as a source of revenue within the state. Within its primary program areas, water quality, confined animal feeding operations (CAFO), and soil & water conservation districts (SWCD) relate directly to water protection issues.\nConfined Animal Feeding Operations (CAFO).\nSoil and Water Conservation Districts (SWCD).Water Quality Division.Oregon Association of Conservation Districts (OACD) www.oacd.org\nOregon Natural Resources Conservation Service (NRCS): www.or.nrcs.usda.gov\nOregon Department of Fish and Wildlife (ODFW): www.dfw.state.or.us\nOregon Watershed Enhancement Board (OWEB): www.oweb.state.or.us\nOregon State University (OSU) Extension Service: www.extension.oregonstate.edu/index.php\nNational Association of Conservation Districts (NACS: www.nacdnet.org\nFarm Service Agency (USDA-FSA): www.fsa.usda.gov\nUnited States Department of Agriculture (USDA): www.usda.gov/wps/portal/usdahome\nOregon Department of Forestry (ODF): www.oregon.gov/ODF/Pages/index.aspx\nOregon Department of Environmental Quality (DEQ): http://www.oregon.gov/DEQ/Pages/index.aspx\nEach state has its own agency to address environmental concerns. In Oregon, the DEQ is responsible for protecting and enhancing Oregon’s water and air quality, for cleaning up spills and releases of hazardous materials, and for managing disposal of wastes. The DEQ also has authority to operate federal environmental programs within the state, such as the Clean Water and Resource Conservation and Recovery Acts (CWA and RCRA).\nWater Quality Division\nThe DEQ’s Water Quality Division administers programs in the areas of surface water protection, watershed management and also sets water quality standards. Regional divisions also maintain individual websites with local information. Water Quality Division 811 SW 6th, Portland Oregon 97204 800-452-4011.Surface Water Protection.\nRunoff from impervious areas such as paved streets, parking lots, and building rooftops often contain pollutants that can affect water quality. This stormwater management is achieved through the issuance of permits.\nNational Pollutant Discharge Elimination System (NPDES)Water Pollution Control Facilities (WPCF) is achieved with the DEQ Groundwater Protection Program. This program was established to provide technical assistance for communities to develop groundwater protection programs, which are generally more cost effective than implementing a contamination response. Read more at www.oregon.gov/deq/wq/wqpermits/Pages/default.aspx permits. The primary purpose of a WPCF permit is to prevent discharges to surface waters and to protect groundwater from contamination. Permits are issued for land irrigation with wastewater, wastewater lagoons, and onsite sewage disposal systems or for any discharge of wastewater to the ground; discharge to surface water is not allowed. This permit is also used to prevent nuisance conditions such as odors and mosquitoes. permits are required for stormwater discharges that leave a site through a “point source” and reach surface waters either directly or through storm drainage. A point source is a natural or human-made conveyance of water through pipes, culverts, ditches, or any other type of channel.\nWatershed Management Water Quality Standards.\nAdditional water-quality projects and programs of the Oregon DEQ include\n- The Clean Water State Revolving Loan Fund finances water quality improvements.\n- Fact Sheets provide the public with guides for water quality, drinking water protection, groundwater, wastewater management, and stormwater permitting.\n- Nonpoint Source Pollution information and grants are administered for sources that pollute Oregon rivers, lakes and streams through runoff.\n- Underground Injection Control (UIC) program regulates any man-made structure that releases water below the ground, such as storm drains and sewers.\n- Wastewater System Operator Certification trains domestic wastewater system operators.\n- Water Quality Limited Stream list for plans to attain and maintain water quality standards including Total Maximum Daily Loads, (TMDLs).\n- Water Quality Standards are benchmarks used to determine whether Oregon rivers and lakes are adequate to support aquatic life, recreation, drinking, agricultural uses.\nTo contact Oregon DEQAccording to DEQ website data, approximately 75% of Oregon’s citizens get their drinking water from public water systems. While these systems are regulated by the Oregon Department of Human Services, (see below) the DEQ works to identify and protect watershed and groundwater recharge areas to reduce the risk of contamination, cut the cost of treatment, and minimize the risk of local water-borne illness. Water quality standards are set by the Oregon Drinking Water Protection Program. Through this program, the DEQ works with DHS to study water sources, maintain databases, maps, and inventory contaminants. You may contact the DEQ Drinking Water Protection Program Coordinator at (503) 229-5413, or visit the website at http://public.health.oregon.gov/healthyenvironments/drinkingwater/sourcewater/pages/whppsum.aspx\nTelephone: (503) 229-5696\nToll Free in Oregon: (800) 452-4011\nTTY: (503) 229-6993\nFax: (503) 229-6124\nE-mail: [email protected] Include your county when e-mailing DEQ.\nAddress: 811 SW Sixth Avenue, Portland, OR 97204-1390\nDepartment of Human Services (DHS)\nAs the largest department in state government, the Department of Human Services has a mission of “helping people to become independent, healthy and safe.” Five strategies aimed at accomplishing these goals include helping low-income families achieve self-sufficiency, protecting vulnerable Oregonians from abuse and neglect, helping low-income families obtain needed health care, assisting seniors and people with disabilities live as independently as possible, and protecting public health.\nWater Resources Department\nThe vision of the Water Resources Department is to assure sufficient and sustainable water supplies are available to meet current and future needs. The Department’s mission is to serve the public by practicing and promoting responsible water management through two key goals:\n- To directly address Oregon’s water supply needs, and\n- To restore and protect streamflows and watersheds in order to ensure the long-term sustainability of Oregon’s ecosystems, economy, and quality of life.\nBy law, all surface and ground water in Oregon belongs to the public. The Water Resources Department is the state agency charged with administration of the laws governing surface and ground water resources. The Department’s core functions are to protect existing water rights, facilitate voluntary streamflow restoration, increase the understanding of the demands on the state’s water resources, provide accurate and accessible water resource data, and facilitate water supply solutions.\nThe Oregon Watershed Enhancement Board (OWEB) is a state agency led by a policy oversight board tasked to maintain healthy watersheds and natural habitats that support thriving communities and strong economies. OWEB’s programs support Oregon’s efforts to restore salmon runs, improve water quality, and strengthen ecosystems that are critical to healthy watersheds and sustainable communities. OWEB administers a grant program funded by the Oregon Lottery to support efforts by Oregonians seeking to create and maintain healthy watersheds.Resources\nSpecial Districts Association of Oregon (SDAO) www.sdao.com\nState of Oregon: Bureau of Labor and Industries (BOLI): www.boli.state.or.us\nOregon State Legislatur: www.oregonlegislature.gov/\nWater Resources Department: www.wrd.state.or.us/\nOregon Department of Environmental Quality (DEQ): www.deq.state.or.us/\nOregon Department of Justice: www.doj.state.or.us/\nOccupational Safety and Health Administration (OSHA) www.osha.gov/\nOregon Employment Department www.employment.oregon.gov\nOregon Department of State Lands (DSL) www.oregon.gov/DSL/Pages/index.aspx\nOregon State Police (OSP) www.oregon.gov/OSP/\nOregon Department of Revenue (DOR) www.oregon.gov/DOR/\nAs part of its public health protection goal, DHS administers the Drinking Water Program to assure safe drinking water for Oregon residents. The program focuses resources on the areas of highest public health benefit and promotes voluntary compliance with drinking water standards.\nThe Drinking Water Program\n- Reducing and preventing contamination of public drinking water supplies\n- Improving water system operation and management through training and technical assistance programs for water system operators, managers, engineers, and lab staff\n- Improving adequacy, reliability, and viability of public water systems\n- Increasing public knowledge, participation, and support for safe drinking water\n- Implementing Environmental Protection Agency drinking water standards and state drinking water regulations.\nThe Drinking Water Advisory Committee (DWAC)Groundwater Protection Program\n- Groundwater Training Manual\n- Aquifer Sensitivity (Susceptibility)\n- Groundwater Under the Direct Influence of Surface Water (GWUDI)\n- Natural Factors Influencing the Composition of Groundwater\n- GWUDI Flow Chart and Explanation\n- Protocol for the Evaluation of Hydraulic Connection\n- Water Quality Assessment Data Sheet\n- Using Source Water Assessments in Land Use Planning\nContact the Oregon State DHS Drinking Water Program at:\n800 NE Oregon St. Suite 611, Portland OR 97232-2162.\nPhone: 971-673-0405; Fax: 971-673-0457\nAdmin. Hours 8am-5pm M-F"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:bc7776e3-47f2-4b1a-9401-3e93c30550ed>","<urn:uuid:185ac1ae-866d-40c9-9836-198807f5cfd4>"],"error":null}
{"question":"Which scientific breakthrough has had a more immediate impact on consumer applications: conductive plastics discovered in 2000 or CRISPR developed in 2012?","answer":"Based on the information provided, CRISPR has had a more immediate and widespread impact since its 2012 development compared to conductive plastics from 2000. While conductive plastics showed potential for applications in organic solar cells and biosensors, the documents indicate scaling up to industrial levels remained challenging. In contrast, CRISPR has seen rapid adoption and widespread applications - from do-it-yourself kits available for $150 to agricultural uses that are already unregulated by the USDA. CRISPR is being used for everything from disease treatment to improving beer taste, and its impact has been so significant that it was called 'The Next Step in Human Evolution' by Forbes magazine.","context":["For the 12 Days of “The STEM Chicksmas” we’re highlighting 12 scientists who have contributed something innovative and exciting to their field. It is the season of giving, and these brilliant minds have given incredible gifts to the scientific community! This year we’re looking at 12 Nobel Prize winners from the past 15 years in the fields of Physics, Chemistry, and Physiology or Medicine.\nDay Nine: The 2000 Nobel Prize in Chemistry.\nWhen we think of plastics, we generally think of them as insulators, materials that inhibit the flow of electrical current. A common application of this insulating ability is the plastic that coats wires to protect us from receiving a shock if we touch them. However, what if plastics could conduct electrical charge the way metals can? It turns out that certain plastics can conduct—but only if you make them correctly. This discovery by Alan J. Heeger, Alan G. MacDiarmid and Hideki Shirakawa earned the Nobel Prize in Chemistry in year 2000.\nThe conducting ability of a material is determined by its electronic structure. The energy of electrons is quantized, which means electrons can only be at certain energy levels. However, this also means that electrons cannot exist within a certain energy range. This forbidden energy range is called the band gap, and in solid materials it is the gap between the conduction and the valence band. The valence band is the band where electrons exist in a non-excited material. If a voltage is applied to the material and the band gap is small enough, electrons can jump to the conduction band. However, this is only possible if the valence band is not fully occupied. This movement of electrons is an electrical current! Using band gaps we can define three types of materials: conductors (no band gap, like metals), semi-conductors (a small band gap), and insulators (a very large band gap). In general, the longer a material is, the smaller the band gap gets. This is relevant when talking about plastics, because chain length is a very important factor.\nWhen we talk about plastics we’re actually talking about polymers, very large molecules made up of a chain of repeating units. In organic polymers, this is a carbon chain. The unit can include double bonds, atoms such as oxygen or chloride, or functional groups like alcohols. The groups that are on the carbon chain determine its properties. The type of polymer the Nobel Prize winners looked at were organic polymers with alternating double bonds, specifically polyacetylene. They found that when they “doped” these polymers, they became conductive. When you dope a substance, you introduce a hole (“p-doping) or an electron (“n-doping”) into the chain via a chemical reaction, either reduction or oxidation. This is important because generally, carbon chains have fully occupied valence bands. If you p-dope it, you turn one of the double-bonds into a single electron. Since an electron is missing, it appears that you have a positive charge which we call the hole. The single electron and hole can move along the chain fairly easily. If you n-dope it, a more complicated mechanism causes the transport of charges across the molecule.\nBy discovering conductive plastics, Shirakawa et al revolutionized the future of electronics. Plastics are cheap and easy to produce, and making them conductive also is a fairly simple chemical reaction. They can potentially be used in organic solar cells and biosensors. It is difficult to scale up their processing to industrial scales, but if this can be achieved conductive polymers could be the future of our devices.\nTo read more, check out the Nobel Prize website here.","October 8, 2020 — Yesterday the Royal Swedish Academy of Sciences announced the 2020 Nobel Prize in Chemistry was awarded jointly to Dr. Emmanuelle Charpentier and Dr. Jennifer A. Doudna “for the development of a method for genome editing” through CRISPR/Cas9 genetic scissors.\n“There is enormous power in this genetic tool, which affects us all,” said the chair of the Nobel Committee for Chemistry. “It has not only revolutionised basic science, but also resulted in innovative crops and will lead to ground-breaking new medical treatments.”\nThis marks the first time the Nobel Prize for Chemistry has been awarded jointly to two women, and Drs. Charpentier and Doudna are only the sixth and seventh women in history to win the chemistry prize.\nTo mark this historic occasion, we revisit Luis Granados’ 2018 analysis of CRISPR exploring the profound implications for the future of gene-editing technology.\nThere are some things that are interesting and/or useful to learn about. The causes of the Korean War, Post-Impressionism, how to make a soufflé. There are other things that you really must learn about if you are to be a responsible humanist. Things that fundamentally affect our future as a species fall into this category. Things like CRISPR.\nWhat is CRISPR?\nCRISPR is an acronym for “Clustered Regularly Interspaced Palindromic Repeats.” It’s a technique for steering an enzyme, called Cas-9, to a targeted location in the immensely long double helix of DNA. Once there Cas-9 does its thing, which is to snip out a segment of that DNA. The two separated strands then join back together. Or, in a more advanced form, the steering piece (a slice of RNA) can carry with it a replacement segment, which is carefully installed in the proper spot. Some recent research suggests it may be possible to avoid the “snipping” part, and simply use the RNA steering mechanism (aka “biology’s search engine”) to turn particular genes on or off.\nWhat makes CRISPR so special?\nIt’s easy and it’s CHEAP! And it works. DNA, the master code for life, can now be edited almost as easily as this article can be edited. It may not be exaggeration to call CRISPR, as Forbes magazine did last December, “The Next Step in Human Evolution.”\nWhat’s the upside?\nThe days of any number of diseases may be numbered: sickle-cell anemia, muscular dystrophy, cystic fibrosis, diabetes… In agriculture, CRISPR is a vastly more precise (and cheaper) technique for tailoring better (or at least different) animals and plants than “traditional” GMO. (It’s already being used to make a better-tasting beer.) A short step beyond that, CRISPR may also be used to enhance human capabilities, exterminate disease-bearing species like mosquitos, or bring extinct species like the woolly mammoth back to life.\nWhat’s the downside?\nSome people, like me, are a little nervous about the “enhance human capabilities” part. And the unknowable side effects on finely tuned ecological systems. So is Jennifer Doudna, the person most responsible for developing CRISPR in the first place. Like Robert Oppenheimer, the “Father of the Atomic Bomb” who strove to restrict its use, Doudna is raising big red warning flags about going too far, too fast with CRISPR. Example: removing a gene called PSK9 can significantly reduce cardiovascular disease—but it’s also been shown to increase the risk for diabetes.\nCan CRISPR make plants and animals that glow in the dark?\nDoes CRISPR really work as well as advertised?\nIt seems to. There was a kerfuffle last year when researchers reported a suspicion that the Cas-9 was making some random cuts along with the intended ones, but the scientific method has worked its magic and those researchers now admit they were wrong. Moreover, even though the technique has only been around since 2012, the pace at which it’s taking hold is breathtaking. A recent article suggests the CRISPR technique could be used to edit thousands of genes at once, and another CRISPR breakthrough has allowed researchers to turn skin cells into stem cells.\nHow permanent are the genetic changes?\nThat’s a critical question. Some types of changes affect only the organism treated. Others affect the “germline,” so that offspring have at least a 50-50 chance of inheriting the change. This makes a huge difference. If a mutation with unforeseen side effects (or for that matter, intentional harm created by a terrorist) gets loose in the ecosystem, it may be extremely difficult to contain.\nSomething this powerful and dangerous is kept carefully under wraps, right?\nWrong. Just the opposite. You can buy a do-it-yourself CRISPR kit for about $150 and start fooling around with genes in all kinds of organisms to your heart’s content. Last year an foolish exhibitionist injected himself with a CRISPR dose designed to make him more muscular and streamed it live on Facebook. There isn’t much reason yet to believe his effort was successful, but of course he’s spawning copycats. In the agricultural sector, the US Department of Agriculture just announced it’s not going to regulate CRISPR-modified food at all.\nWill CRISPR only be available to the rich?\nKits for experimenting with CRISPR are cheap, but a full-blown program to accomplish a desired goal without causing negative side effects is likely to be quite expensive. We have a terribly hard time paying for medical care as it is—a rush of expensive new enhancement options will ratchet up pressure on a system that’s already broken. My guess is that, yes, many CRISPR options will only be available to the rich. Some people say that’s a problem.\nShould government do more to crack down on this?\nIn Canada, editing genes that can be passed down to future generations is a criminal offense with a maximum penalty of ten years in jail. But that won’t solve the problem by any means. China isn’t cracking down on anything, and is racing ahead with experiments on humans. And no one has any idea what folks are creating with the do-it-yourself CRISPR kits. The regulation of CRISPR would seem to be an urgent topic for an international treaty, but I’m not aware of any movement toward that end.\nWhat can you do?\nLearn as much as you can. The links in this article are a good start. Then, start bugging politicians about what they’re doing about it. This is too big just to keep our fingers crossed and hope everything turns out OK."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5dfa3b98-d5c3-4a28-98c9-301decd2be72>","<urn:uuid:05c9fa0a-5cd2-4941-8ee5-996924c91ed5>"],"error":null}
{"question":"I've been reading about ADAS sensors - how are they driving market growth, and what security measures are needed to protect these systems?","answer":"ADAS sensors are driving significant market growth due to increasing adoption of autonomous vehicles, supporting functions like speech recognition, eye tracking, driver monitoring, and gesture recognition. The market includes camera-based machine vision systems, driver condition evaluation systems, sensor fusion ECUs, and radar-based detection units. On the security front, ADAS systems require both software and hardware security measures, as software security alone is inadequate. Key security measures include IP protection, encrypted configuration bit streams, protection against physical attacks like differential power analysis (DPA), and ensuring secure authentic communications into and out of devices.","context":["The global automotive sensors market generated the revenue of $25,723.8 million in 2019. Furthermore, the market is projected to advance with a CAGR of 7.4%, during the forecast period (2020–2030), and generate $58,215.3 million revenue in 2030. The powertrain sensors application category in terms of volume dominated the market in 2019.\nHowever, the ADAS application category is expected to generate the fastest growth in the automotive sensors market during the forecast period. At present, these sensors find their major application in level 1, 2, and 3 autonomous vehicles, and with the introduction of level 4 and 5 autonomous vehicles in 2023 and 2025, respectively, the demand for these sensors is expected to spur up significantly.\nTechnological advancement Is a Major Trend in the Market\nOne of the key trends witnessed in the automotive sensors market is the ongoing technological advancements. For instance, the concept of sensor fusion has gained much popularity in the automobiles. Sensor fusions are intended to combine the best information available from each of their systems installed in the vehicles. Moreover, the increasing installation of electronic devices in the vehicles has fueled the need for various automobile sensors for engine management systems; heating, ventilation, and air conditioning (HVAC) systems; and other application areas.\nIncreasing Adoption of ADAS Sensors Is Driving the Growth of the Market\nThe increasing demand for advanced driver-assistance system (ADAS) sensors is a major driver for the growth of the automotive sensors market. With the growing adoption of autonomous vehicles of different levels, these sensors are becoming more and more crucial for the vehicles. Autonomous vehicles have several functions, such as speech recognition, eye tracking, driver monitoring, virtual assistance, gesture recognition, and natural language interface, which are highly dependent on advanced automotive sensors. Further, the development of the ADAS, which includes camera-based machine vision systems, driver condition evaluation systems, sensor fusion engine control units (ECUs), and radar-based detection units, creates higher demand for ADAS sensors in the market.\nSegmentation Analysis of Automotive Sensors Market\nGeographical Analysis of Automotive Sensors Market\nAsia-Pacific (APAC), led by China, was the most dominant region in terms of volume in the automotive sensors market. Moreover, the region is expected to witness the fastest growth as well during the forecast period. The dominance of China is buoyed by its high production capacity as well as its continuous implementation of automotive innovations in terms of electric and autonomous vehicles. These vehicles require advanced and greater number of automotive sensors in the vehicles, which is a major driver for the market.\nCompetitive Landscape of Automotive Sensors Market\nThe global automotive sensors market is moderately fragmented in nature, with the top six players accounting around 60.0% of the total market share in 2019. Among the major players, Robert Bosch GmbH led the market in 2019. Moreover, the historical trend exhibits that the company consistently remained the market leader between 2015 and 2019, with significant fluctuations in share over the mentioned period.\nBrowse report overview with detailed TOC on \"Automotive Sensors Market Research Report: By Type (Pressure Sensor, Temperature Sensor, Position Sensor, Oxygen Sensor, Motion Sensor, Optical Sensor, Torque Sensor, Gas Sensor, Level Sensor), Application (ADAS, Powertrain, Chassis, Body), Technology (MEMS, Non-MEMS), Vehicle Type (Passenger Car, Commercial Vehicle), End Use (OEM, Aftermarket) – Global Industry Share, Size, Growth and Demand Forecast to 2030\" at:https://www.psmarketresearch.com/market-analysis/automotive-sensors-market\nIn December 2019, Continental AG introduced a contact sensor system that is able to pick up the sound signal of low-speed impacts and detect scratches and damages on the vehicle. The new sensor would improve the safety in automated parking, thereby assisting in the development in self-driving vehicles. Furthermore, in April 2019, NXP Semiconductors N.V. entered into a strategic collaboration with HawkEye Technology Co. Ltd. to develop autonomous vehicle in the Chinese market. Under the partnership, HawkEye Technology Co., Ltd. would offer its deep expertise in 77 GHz radar sensor to NXP Semiconductors N.V., in order to develop 77 GHz radar solutions for the Chinese automotive market.\nSome other players operating in the market are DENSO CORP., Valeo SA, Panasonic Corp., OmniVision Technologies Inc., ON Semiconductor Corp., Sensata Technologies Holding plc, TE Connectivity Ltd., NXP Semiconductors N.V., Analog Devices Inc., Continental AG, Delphi Technologies PLC, Infineon Technologies AG, Melexis NV, and Allegro MicroSystems LLC.","Biggest security threats for embedded designers\nEmbedded system designers face a number of threats to the applications that they develop for the Internet of Things (IoT). One of the biggest threats comes from IoT devices that end-users can access, such as commercial networked HVAC systems, wireless base stations, power stations, network gateway systems, and avionics networking.\nAnother example is the connected car, including the advanced driver assistance system (ADAS) that encompasses intelligent, interconnected vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Since vehicles are fielded systems, they are accessible by people with malicious intentions. There can be serious consequences – up to and including loss of life – if, for instance, ADAS systems cannot ensure that V2V and V2I messages originate from a trustworthy source and are not modified between sender and receiver.\nWith these and other systems, software security, alone, has proven inadequate to protect user-accessible devices against known threats. What is needed is a combination of software and hardware security. For example, today’s FPGA SoCs can be used to implement a hardware security scheme that compliments the software and strengthens the system. Ideally, the hardware and software solution should combat three types of security breaches:\n1. Design security: This includes IP protection and ensuring that configuration bit streams are encrypted and protected. In addition, designs need to incorporate a method to ensure there is no overbuilding or cloning of the design possible.\n2. Hardware security: Designers also need to certify that user-accessible devices are resistant to physical attacks. For example, differential power analysis (DPA) attacks can extract keys and other vital device information.\n3. Data security: This element ensures that communications into and out of the device are authentic and secure.\nEmbedded system program managers and development teams must design these types of protections into their products while best leveraging the characteristics of the underlying platform. The result should be a robust protection network with no single point of failure. Some key methods for achieving this goal include:\n∑ Risk assessment: System penetration testing should be used for a detailed system evaluation, to assess critical system data/functions, discover vulnerabilities, enumerate threats, and outline the likelihood and consequence of system compromise.\n∑ Protection planning: Using risk assessments and any other compiled data, developers should seek to understand protection implementation costs and design options for mitigating identified system vulnerabilities and ensuring successful system verification and validation.\n∑ Attack scenario testing: This can include a black box approach, pitting experienced reverse engineers with state-of-the-art attack tools against a system in a deployed setting to reveal vulnerabilities that cannot otherwise be found during most other evaluation exercises.\n∑ DPA side-channel analysis and mitigation: Side-channel attacks are currently the most practical method for compromising cryptography implementations. It is important to regularly perform measurable, objective, and repeatable testing for resistance to side-channel attacks for applications where adversaries have the ability to observe side channels (i.e., power draw, timing, EM emanations) during on-device cryptographic operations.\nIn today’s cyber hacking world, it is essential for every public and private organization to proactively address security issues. Embedded system designers can help their customers in this area by creating secure designs that are protected from today’s rapidly evolving threats, including those posed by a rapidly growing ecosystem of interconnected, user-accessible hardware."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:a250b0fe-98f4-4cfa-ba75-f96c3ee8901a>","<urn:uuid:6fd90e2b-d1fc-4ceb-b428-5c07dc11374d>"],"error":null}
{"question":"What are the differences in trainability between German Shepherds and Australian Cattle Dogs, and how do their exercise needs compare?","answer":"German Shepherds are extremely trainable, able to learn tricks with 95% accuracy after just one teaching session and master them in 3-5 sessions. They rank third among the world's most intelligent dogs. For Australian Cattle Dogs, while they are also smart and quick learners, they can have an independent streak and sometimes try to be the boss, requiring firm establishment of authority during training. Regarding exercise, both breeds have high exercise needs - German Shepherds require at least 30 minutes of daily exercise plus mental stimulation, while Australian Cattle Dogs need at least one hour of exercise per day and plenty of space to run and play.","context":["The ever-obedient, confident, and very loyal German Shepherd is one of the planet’s most popular breeds of dogs. Aside from being the dependable members of law enforcement, search and rescue, and military organizations around the globe, they make very faithful companions. No one can ever question the loyalty of a German Shepherd. And despite its imposing size and commanding presence, these dogs are very friendly and gentle on kids, making them one of the best dogs to have in a growing family.\nSmart, confident, and courageous, German Shepherd Dogs (GSDs) are currently classified by the American Kennel Club in the Herding Group of dogs and ranks second in the organization’s most popular breeds. German Shepherds have a life expectancy of anywhere between 9 and 13 years, although it is not unusual to see some dogs outliving their breed standards.\nInstantly recognizable with its black saddler and tan- or red-colored body and limbs, the GSD has a domed forehead, black nose, strong jaws, and a long, albeit square-cut muzzle. Perhaps one of the most characteristic features of a GSD is its large ears standing erect, although these are often pulled back with movement. It has a self-assured and intelligent look in its brown, medium-sized eyes.\nThe stance of a German Shepherd dog (GSD) is one of grace and refinement. Instead of the clearly sharp angles in other dog breeds, GSDs come with easy, gentle curves when viewed from the side. It is this naturally smooth flowing of their body contours that give the GSD amazing speed and agility. Their natural gait is one of an easy-and-free trot, although they can easily flip a switch and you’d see them dashing like a greyhound.\n- Rin Tin Tin, the movie world’s first canine superstar, cemented the position of the German Shepherd as one of the world’s most easily recognizable dog breeds.\n- The average male German Shepherd can grow up to 26 inches and weigh between 65 and 90 pounds. Females can grow up to 24 inches and weigh between 50 and 70 pounds.\n- During the First World War GSDs were renamed by the Americans as Shepherd Dogs because they didn’t want the dog to be associated with Germany. At the same time, the English also renamed the GSD as Alsatian Wolf Dogs. The name Alsatian is still used by many Europeans to this day.\n- German Shepherds are also called ‘German Shedders’ because of their natural tendency to shed a lot. They have medium-length fur in a double-coat setup. It is the undercoat that sheds heavily especially during spring and autumn. They are also known to ‘blow’ – shedding a lot at once that people describe it as a snowstorm.\nConfident, fearless, and direct; these are ideal qualities of a German Shepherd. It is intelligent, devoted, fun-loving, easy to train, and a very devoted dog especially if its parents have good temperaments, too. You can ensure the best in your German Shepherd if it is trained and socialized early on to become familiar with different people, sounds, and sights.\nGSDs are working dogs and as such require physical exercise and a good dose of quality mental stimulation. While they can be successfully raised in kennel situations, they will still require plenty of interaction from their human masters. They want to work. There is nothing more fulfilling for a German Shepherd than be able to perform even the most mundane of tasks. It enjoys plenty of walks or even going on a hike. Play ball with your GSD and you’ll find out its stamina is a lot greater than yours. Involve it in dog sports and it will easily bring home the bacon every time.\nExercise and mental stimulation is a must for GSDs: lest they grow bored, lonely, and destructive. As such, they are best reserved for pet parents who are themselves smart and active and who can provide these dogs with focused attention, training, and exercise. Above all, German Shepherds love lots of one-on-one time with their human masters.\nRelated Post: Best Dog Food for German Shepherd\nGerman Shepherds are very intelligent. Studies show that they can perform a trick 95% of the time just by teaching them the trick only once. Mastery of a certain trick can be achieved in as little as 3 to 5 sessions whereas other dog breeds will typically require a few weeks to several weeks just to master a single trick alone. GSDs rank third among the world’s most intelligent dogs following Border Collies and Poodles. However, if you think that German Shepherds will perform the trick on their own because they’re intelligent, think again. They are intelligent only when they are trained early on. This makes puppy training an absolute must. Remember, being gifted with intelligence is nothing if they cannot put this intellect to work.\nOne of the most interesting aspects about German Shepherds is that they have this reputation for being aggressive. Their stance, size, and body language coupled with their instinctive protectiveness make them excellent deterrents and watchdogs. While this reputation is quite accurate, there is actually another side to it that makes the GSD such a revered member of the family. GSDs are known to be very friendly with kids as well as other pets especially when they grew up together. They’re very affectionate with their families, too.\nThings You Should Know\nGetting a German Shepherd requires a lot of careful consideration. These are very active, smart, intelligent dogs that require the same qualities from their human masters or at least have a fundamental understanding of their needs. To help you decide whether the GSD is for you or not here are a few things you need to know about this dog breed:\nGerman shepherds are highly intelligent dogs. They’re smart and can easily be trained using the correct techniques and positive reinforcement. Animal behavior experts say that GSDs can easily learn a trick in as few as 3 repetitions, with many GSDs showing 95% accuracy of the behavior with just one repetition. That being said, training a GSD should start right at puppyhood. Learning the different steps and techniques of puppy training is crucial to harnessing the full trainability of GSDs. Equally important are the use of treats and other positive reinforcers.\nGerman shepherds are primarily motivated by the ability to work. They are originally bred as sheepdogs so one can only expect that they live to work. This is the very essence of their intelligence. They need something to work on so that they can use their cognitive capabilities. As such, their training is usually designed in such a way that it looks and feels like work for GSDs. They love scent detection, herding, and other physically-demanding activities that are best reserved for dogs that have exceptional stamina.\nGerman shepherds are aptly called “German shedders” because of their predisposition to shed heavily, especially in spring and fall. During these times of the year, GSDs require frequent bathing and brushing of their coat to help remove loose hair. A dog shedding brush may also be used to facilitate the removal of the excess hair. Whether or not it is spring or fall, German shepherds still require weekly brushing to help promote healthier and cleaner coats. You may also want to get a heavy-duty pet hair vacuum cleaner to help get rid of loose hair in your home.\nWhile German shepherds are known to be very sturdy, they do present with health problems that are very common in large dog breeds. For instance, GSDs are prone to the development of hip dysplasia, a condition whereby the hip joints failed to develop in a more normal manner leading to the thigh bone not fitting properly into the joint capsule of the hips. When the condition is not managed this can lead to arthritis and can severely impact the quality of life of your German shepherd. Remember that GSDs are highly active work dogs. Problems in their joints will impair their mobility; hence, their activities will also be impaired.\nAnother major cause of concern among GSD owners is degenerative myelopathy which is a lot similar to multiple sclerosis. What happens is that the GSD develops a progressively slow, creeping paralysis of its hindquarters. Other health conditions that GSDs are known to have include heart murmurs, cardiomegaly, heart valve diseases, visual problems, epilepsy, hemangiosarcoma, bleeding disorders, exocrine pancreatic insufficiency, and immune-mediated diseases.\nA German shepherd is generally a good choice for a dog if you’re looking for a hound that is:\n- Affectionate with its family\n- Incredibly kid-friendly\n- Very easy to train\n- Smart and intelligent\n- Handsome, regal, and well-built\n- Very eager\n- Loyal and faithful\n- Obedient and protective\n- Fearless, hardworking, and strong-willed\nWhile these characteristics can easily make a German shepherd the perfect choice for many dog-lovers, they are not ideal for someone who:\n- Doesn’t know how to be the leader of the pack\n- Doesn’t have the time to train the dog\n- Cannot spend at least 30 minutes of exercise every day with their dog\n- Cannot devote at least 15 minutes of quality one-on-one bonding time with their dog\n- Doesn’t like to brush and bathe their dog regularly and frequently especially during spring and fall\n- Doesn’t like the idea of having plenty of loose hair on the carpet, furniture, and other items in the house\n- Lives in very hot temperatures since the double-coat nature of GSDs can lead to overheating\nThe modern German Shepherd has its humble beginnings from 6 different breeds of European sheep herding dogs. These included the Italian Bergamasco Shepherd, the Lupino del Gigante, the Cane de Pastore della Lessinia e del Lagorai, the Cane Paratore, the Pastore d’Oropa, and the French Berger Picard. By the middle of the 19th century, attempts were already made to standardize the different breeds. From these efforts came the characteristics that are desired from a sheep herding dog: intelligence, keen sense of smell, strength, and speed.\nBy 1891, the Phylax Society was formed to institutionalize the standards of sheep herding dogs in Germany. While the group did not last, some of its former members persevered in their quest for standardizing the breed that will become the quintessential sheep herder in Germany. One of these ex-members is Max von Stephanitz who firmly believed in the idea that dogs should be specifically bred for working. He admired Germany’s native sheepdogs for their ability, strength, and intelligence, but this did not satisfy him to consider the breed as the ideal working dog.\nIn 1899, all these changed when von Stephanitz was introduced to Hektor Linksrhein, a sheepdog that is the result of selective breeding that lasted several generations. Hektor’s beauty, intelligence, loyalty, and strength were all that von Stephanitz wanted in the perfect working dog. He bought Hektor from its owner and renamed it Horand von Grafrath. Soon after, von Stephanitz established the Verein fur Deutsche Schaferhunde or the Society for the German Shepherd Dog. This formally introduced the German Shepherd Dog to the world with Horand officially being recognized as the very first German Shepherd.\nHorand became the progenitor of all modern German Shepherds. The dog was introduced in the US towards the early parts of the 1900s. They were instantly loved by dog-lovers because of their highly-adaptable, intelligent, attractive, and hardworking nature. The very first GSD registered with the AKC was Queen of Switzerland in 1912. It was the following year that the German Shepherd Dog Club of America was formally established.\nUnfortunately, the outbreak of the First World War saw the decline in the popularity of the breed as they were inadvertently correlated with the enemy. But the war only cemented the GSD’s unique characteristics as a working dog. It braved artillery fire, tanks, and landmines just so it can bring much-needed supplies to the German frontlines.\nIt was after the war that Rin Tin Tin and Strongheart revived the popularity and favorability of the breed to the American dog-loving society. The breed’s popularity peaked with the crowing of Sieger Pfeffer von Bern as the AKC’s Grand Victor for 1937 and 1938. Unfortunately, with World War II and the growing anti-German sentiment, the breed’s popularity again took a hit.\nAfter the Second World War, their popularity steadily increased, culminating in the number 2 spot of the AKC’s most popular breeds, second only to the Labrador, in 2016.\nThe German shepherd continues to wow dog-lovers everywhere. Their proud stance, characteristic coat, and their irrefutable reputation for intelligence, speed, strength, loyalty, dedication, protectiveness, and faithfulness all make the GSD a great dog to have for pet parents with the same characteristics.\nDog Food Mat\n- German Shepherd Dog – The American Kennel Club","Australian Cattle Dog\nHigh energy levels and active minds!\nAustralian Cattle Dog Scientific Classification\n- Scientific Name\n- Canis lupus\nRead our Complete Guide to Classification of Animals.\nAustralian Cattle Dog Conservation Status\nAustralian Cattle Dog Locations\nAustralian Cattle Dog Facts\n- Strong minded yet loyal and loving\n- Should be trained from an early age and respond best to firm and fair training\n- Average Litter Size\n- Common Name\n- Australian Cattle Dog\n- High energy levels and active minds!\n- Herd Dog\nAustralian Cattle Dog as a Pet:\n- General Health\n- Energy Level\n- Tendency to Chew\n- Family and kid friendliness\n- Yappiness / Barking\n- Separation Anxiety\n- Preferred Temperature\n- Average climate\n- Exercise Needs\n- Friendly With Other Dogs\n- Pure bred cost to own\n- Dog group\n- Male weight\n- -45 lbs\n- Female weight\n- -45 lbs\nThis post may contain affiliate links to our partners like Chewy, Amazon, and others. Purchasing through these helps us further the A-Z Animals mission to educate about the world's species.\nView all of the Australian Cattle Dog images!\nThese herding dogs were first bred in Australia in the 1800s. An Australian cattle dog is the result of breeding a blue merle collie with a dingo. They were bred to herd livestock.\nSee all of our expert product reviews.\nThe large, pointed ears of this dog as well as its dark eyes make it an endearing addition to any household. Keep in mind these are social dogs that like to be with family members or other pets at all times.\n3 Pros and Cons of Owning an Australian Cattle Dog\n|A very loyal dog|\nAustralian cattle dogs become very attached to their families. This makes them welcome companions.\n|Unhappy when left alone|\nThese dogs are social and don’t like to be left alone for long periods of time. In fact, they can become destructive if left by themselves.\n|Very little barking|\nThese dogs don’t bark a lot. So, a family that wants a dog that is quiet (but not still) most of the time is going to appreciate a red or blue heeler.\n|Needs daily exercise|\nWhether it’s a red or blue heeler, these dogs need exercise every day.\nThey are known for getting along with other dogs in a household.\n|Not an apartment dog|\nThe high energy level of these dogs means they need a lot of space to move around in. They aren’t a good choice for apartment life.\nHistory and Origins\nThe Australian Cattle Dog played an important role in the development of the beef industry in Australia, which was a significant part of the economy. In the early 1800s, people from Britain began to move away from the original settlements along the coast and into the grassy plains of the western inland. This area was ideal for raising cattle, so herding dogs were needed to help make it possible.\nHealth and Entertainment for your Australian Cattle Dog\nSee all of our expert product reviews.\nIn the beginning, Australian livestock owners imported British Smithfield dogs, but these breeds were not suitable for the hot weather, rough terrain, and long travel times to market in their new homes. Therefore, ranchers began a lengthy effort to create a herding dog that could handle the conditions of the Australian outback. To do this, Smithfields were bred with Dingoes and other breeds like Scottish Highland Collies. As a result, farmers developed the strong and reliable herding dogs they desired.\nGeorge Elliot from Queensland was a major contributor to the development of the Australian Cattle Dog, as he bred Dingoes with Collies and sold the puppies to farmers. This created a dog that was close to being the ideal herding dog for Australia. Later, Jack and Harry Bagust refined the breed even more by mixing in Dalmatians, and a sheepdog called the Black and Tan Kelpie. This combination of breeds produced the ACD that we know today.\nThe Australian Cattle Dog was accepted into the American Kennel Club in 1980 and became a founding member of the Herding Group when it was established in 1983.\nSize and Weight\nThis is a medium-sized breed with short hair. The average height of a male is 19 inches, while a female is 18 inches tall at the withers. Both males and females weigh 45 pounds when fully grown at 16 months. An 8-week-old puppy should weigh 10.5 pounds.\nThe record for the oldest Australian cattle dog is held by Bluey, who lived to be 29 years old.\n|Male||19 inches||18 inches|\n|Female||45 lbs (fully grown)||45 lbs (fully grown)|\nCommon Health Issues\nProgressive retinal atrophy is a common health issue of this dog. This is a degenerative disease that affects the photoreceptor cells in the eye. It is a genetic condition and can lead to blindness. Hip dysplasia is another common health issue. The hip joint consists of a ball and socket. When a dog develops hip dysplasia, the ball and socket don’t fit together in the right way. Over time, this inherited condition causes deterioration of the hip joint. A third common health issue is elbow dysplasia. This genetic condition involves abnormal cell growth. It causes malformation of the elbow joint. Elbow dysplasia may not appear in a dog until it’s at least 4 months old.\nThe most common health issues for an Australian cattle dog are:\n- Progressive retinal atrophy\n- Hip dysplasia\n- Elbow dysplasia\nTemperament and Behavior\nEnergetic is the best word to describe the behavior of this breed. This is a working dog bred to herd cattle, sheep, or any other livestock. A family with children is going to have a loyal, entertaining pet.\nThis dog has a friendly personality. They love to be around humans and other pets.\nOne of its most notable traits is its intelligence. They were bred to handle herding responsibilities. Their intelligence makes obedience training relatively simple.\nHow to Take Care of an Australian Cattle Dog\nWhen caring for this breed, there are certain things to keep in mind. Giving a red or blue heeler the proper diet, grooming, exercise, etc., can help to keep it in good health. Factoring common health issues into its care can also contribute to the well-being of a puppy or adult dog.\nThe Best Dog Food\nNot surprisingly, puppies need a different type of care than adult dogs.\nPuppy food: This breed of puppy needs food high in protein. This ingredient supports the activity of this highly energetic dog. Fiber contributes to a puppy’s digestive health, and fatty acids support skin and coat health. Vitamin A supports healthy retinal function, which can protect against progressive retinal atrophy. Calcium and vitamin D are nutrients that help a puppy guard against hip dysplasia.\nDog food: Just as puppies need protein, so do adults. This nutrient contributes to a dog’s muscle health. Food with a small percentage of carbohydrates gives an adult dog energy while not causing excess weight gain. Vitamins A and C support eye health and provide antioxidants to guard against disease. Calcium keeps bones and joints strong, which can help to prevent hip and elbow dysplasia.\nYour Australian Cattle Dog needs well-balanced food to fuel all that energy and joyful activity. That’s why A-Z Animals thinks the best food for Australian Cattle Dogs is Blue Buffalo Wilderness High Protein, Natural Adult Dry Dog Food. If you want to read more, have a look at the complete list of food for you Blue Heeler that we compiled.\nIn addition to providing a healthy balance of protein and carbohydrates to satisfy non-stop Australian Cattle Dogs’ appetite and build lean muscle, this food also has just enough calcium and phosphorus for strong bones, teeth, and cartilage. There’s glucosamine and vitamin A, too, so elbow/hip dysplasia and eye problems will be less likely to slow your Australian Cattle Dog down.\nCheck Chewy or Amazon for this product.\n- Contains real chicken, sweet potatoes, and peas\n- Added omega-3 and omega-6 fatty acids, and antioxidants\n- High protein option, best for active dogs\n- Made from the finest natural ingredients\nMaintenance and Grooming\nHow much does a red or blue heeler shed? These dogs have short-haired, double coats, so they’re moderate to heavy shedders. A proper grooming routine can reduce the amount of loose dog hair around a home.\nThis dog requires weekly brushing. A soft brush with boar hair bristles is a great tool to use. Start at the dog’s head and brush toward its tail, following the natural direction of its coat. A grooming glove is an effective tool for the dog’s legs and tail.\nIt’s a good idea to bathe this dog once per month to remove odors and dirt from its coat. The hair of an Australian cattle dog’s coat resists moisture, so it will dry quickly.\nFur loss or irritated skin due to the presence of fleas or mites are also common afflictions.\nThis herding dog is smart and a quick learner. So, training is a relatively easy process. One thing to keep in mind is these dogs can have an independent streak. They share this trait with border collies. Australian cattle dogs are the boss of the herds they are moving along. So, sometimes, they try to be the boss of a household. This makes it all the more essential that an owner establishes who is boss during training. As with any other lesson, these dogs are quick to catch on! Using treats during training is helpful in making sure lessons are effective.\nThese dogs have a lot of energy. So, they need exercise at least one hour per day. It’s best to give them a lot of space to run and play. Just think, this working dog was bred to run through acres and acres of fields following herds of sheep or cattle. They need space to stretch their legs. This is part of the reason why they are not appropriate pets for people who live in apartments. A large yard, a big, enclosed field, or a park are all good places to take this dog. Exercise isn’t a luxury. It contributes to their overall health.\nFetch, pulling a rope, frisbee, rolling a ball, or playing chase are just a few of the games these dogs enjoy. A large dog park is another fun place to take an Australian cattle dog.\nPuppies of this breed grow to be fairly large in a short amount of time. They weigh 10.5 pounds at eight weeks. So, these puppies need a lot of space for exercising and growing right from the start. Plus, they are the most ideal choice for a family. These are social puppies/dogs that like to be with family members.\nThough this dog is a good choice for families with children, there are some things to bear in mind. This is a working dog at heart. They were bred for herding. So, it’s in their DNA to herd things even if there are no cattle or sheep around. Part of their herding behavior is to nip at the heels of the livestock. Some Australian cattle dogs may try this with children as they run through the yard. Training can help with this behavior. Also, if a family is able to get an Australian cattle puppy, it’ll grow up knowing how to behave around young children.\nDogs Similar to the Australian Cattle Dog\nSome dogs that are similar to Australian cattle dogs include Border collies, Welsh Corgis, and German Shepherds.\n- Border collies: This is another herding dog with the same intelligence and alertness as an Australian cattle dog. It’s also easy to train.\n- Welsh Corgis: The body of a Welsh Corgi is muscular and compact like the Australian cattle dog. It’s also known for its intelligence and focus.\n- German Shepherd: Another energetic dog with intelligence and loyalty. Both this dog and the Australian cattle dog have pointed ears that are always listening.\nAustralian Cattle Dogs vs the Blue Heeler\nThe red and blue heeler are the two types of Australian cattle dog. The blue heeler got its name from the blue tone of its coat, while a red heeler has a reddish tint to its fur. Both dogs are the result of breeding a blue merle collie and a dingo in Australia. There’s no difference between the red and blue heelers besides the coat color.\nFamous Australian Cattle Dogs\nAustralian cattle dogs are a favorite pet of some famous individuals, whether they’re native to Australia or not.\n- Actor Mel Gibson owns an Australian cattle dog\n- Actor Matthew McConaughey owns one\n- Actor Owen Wilson owns one named Marley\nPopular names for Australian cattle dogs include:\nAustralian Cattle Dog FAQs (Frequently Asked Questions)\nHow much does an Australian cattle dog cost to own?\nThe average cost of a purebred Australian cattle dog is $600. Vet costs range from $200 to $600 annually depending on its overall health. The food bill for this dog ranges around $100 per month depending on the brand of dog food.\nIs an Australian cattle dog good with kids?\nYes, if it grows up alongside young kids and has training to teach it how to interact with younger family members.\nHow long does an Australian cattle dog live?\nNormally, the lifespan of this dog is 13 to 15 years.\nIs an Australian cattle dog a good family dog?\nYes. It’s best to bring a puppy into a family with young children so the dog learns early on how to interact with young ones.\nHow big do Australian cattle dogs get?\nMale Australian cattle dogs grow to 19 inches tall at the withers while females grow to be 18 inches tall. Both male and females weigh around 45 pounds.\nCan Australian cattle dogs be left alone?\nThey can be left alone. But, these dogs are social and would rather be with people.\nWhat does an Australian cattle dog look like?\nBoth the red and blue heelers have a muscular, compact body. Red heelers have a tan coat with a reddish tint while a blue heeler has a blue mottled coat with a tan belly. Some blue heelers have faces that are half blue speckled, half white. Both types of Australian cattle dogs have pointed ears, a long fluffy tail and searching dark eyes.\nIs the Australian cattle dog the same as a blue heeler?\nYes. A blue heeler is just one of the two types of Australian cattle dog.\nAre Australian Cattle Dogs herbivores, carnivores, or omnivores?\nAustralian Cattle Dogs are Omnivores, meaning they eat both plants and other animals.\nWhat Kingdom do Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the Kingdom Animalia.\nWhat class do Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the class Mammalia.\nWhat phylum to Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the phylum Chordata.\nWhat family do Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the family Canidae.\nWhat order do Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the order Carnivora.\nWhat genus do Australian Cattle Dogs belong to?\nAustralian Cattle Dogs belong to the genus Canis.\nWhat type of covering do Australian Cattle Dogs have?\nAustralian Cattle Dogs are covered in Hair.\nHow many babies do Australian Cattle Dogs have?\nThe average number of babies an Australian Cattle Dog has is 5.\nWhat is an interesting fact about Australian Cattle Dogs?\nAustralian Cattle Dogs have high energy levels and active minds!\nWhat is the scientific name for the Australian Cattle Dog?\nThe scientific name for the Australian Cattle Dog is Canis lupus.\nWhat's the difference between a Blue Heeler and an Australian Shepherd?\nThere are many differences between a Blue Heeler and an Australian Shepherd. Blue Heelers have dense and thick fur, while Australian Shepherds have fluffy coats. The Australian Shepherd also lives longer than Blue Heelers on average.\nWhat are the key differences between Border Collies and Australian Cattle Dogs?\nThe key differences between Border Collies and Australian Cattle Dogs are appearance, temperament, trainability, and health.\nWhat are the key differences between a Catahoula Leopard Dog and an Australian Cattle Dog?\nThe key differences between a Catahoula Leopard Dog and an Australian Cattle Dog are size, coat, temperament, health issues, and training.\nWhat are the key differences between Catahoula Leopard Dogs and Blue Heelers?\nThe key differences between Catahoula Leopard Dogs and Blue Heelers are appearance, coat, temperament, health issues, exercise, and training.\nWhat are the key differences between the Texas Heeler and Blue Heeler?\nThe largest difference between the Texas Heeler and Blue Heeler is that the Blue Heeler is purebred, while the Texas Heeler is a cross between the Australian Cattle Dog and Australian Shepherd. Other distinguishing traits include their size, appearance, temperament, trainability, barking level, and grooming frequency.\nThank you for reading! Have some feedback for us? Contact the AZ Animals editorial team.\n- Vet Street, Available here: http://www.vetstreet.com/dogs/australian-cattle-dog\n- American Kennel Club, Available here: https://www.akc.org/dog-breeds/australian-cattle-dog/\n- Cattle Dog World, Available here: https://cattledogworld.com/australian-cattle-dog-history/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:caca882a-0890-4978-bc90-cee1a83e3fe6>","<urn:uuid:ba756a89-7867-4313-a713-86c5788f408c>"],"error":null}
{"question":"What are the main types of allergic reactions and what role does hydroxyzine pamoate play in treating them?","answer":"There are four main types of allergic reactions: Type 1 (immediate IgE-mediated response involving histamine release), Type 2 (cytotoxic reaction causing cell death), Type 3 (immune complex formation), and Type 4 (cell-mediated involving T cells). Hydroxyzine pamoate specifically targets Type 1 reactions by functioning as an antihistamine. It is particularly effective in treating allergic symptoms like chronic hives (urticaria), severe itching (pruritus), and contact dermatitis. The medication works by blocking histamine receptors, preventing the release of chemical messages that cause allergic symptoms, and typically reaches maximum effectiveness within 30-60 minutes of administration, with effects lasting 4-6 hours.","context":["Allergy: What is it?\nAllergy is a hypersensitivity (hyperreactivity) of the body’s immune system against a xenobiotic substance that can act as an allergen or antigen. This type of reaction by our immune system is undesirable since it predisposes the affected tissue to damage.\nThe allergic response is mediated by different mechanisms that may be due to an antigen-antibody reaction, or cell-mediated. The predisposition of an individual to different allergens is not predictable; hence, antiallergic drugs become essential. The drugs are used to decrease the reactivity of the cells to allergens.\nIf not taken proper care, the allergic response may become fatal.\n- How do they work?\nThe reaction of cells to an allergen can be classified into four broad categories –\n- Type 1 reaction – this allergic reaction is the IgE antibody-mediated. The response is immediate (within seconds – minutes). IgE sensitizes basophils in the peripheral blood or mast cells and causes them to produce anaphylactic mediators like histamine, serotonin, intestinal peptides, prostaglandins, leukotrienes upon degranulation. These mediators get released in high amounts that show the pharmacological effect.\n- Type 2 reaction – this reaction is cytotoxic, i.e., causes cell death. The result of such a response is tissue damage due to the binding of antibody (IgM, IgG) to the antigen. The damage is caused by cell membrane lysis, phagocytic action of macrophages on the target cell, and direct damage to the affected cell.\n- Type 3 reaction – this reaction results from the antigen-antibody complex, causing an immune complex formation. The complement system gets activated, and complex-mediated tissue injury by exogenous antigens (microbes), or endogenous antigens (blood components).\n- Type 4 reaction – it is a cell-mediated reaction, involving T cells, antigens, and macrophages. The macrophages present an antigen to T cells, by which cell-mediated immune memory develops. Upon subsequent exposure, T cells cause direct cytotoxicity, lymphokine, and inflammatory mediator recruitment.\nThe drugs, therefore, are used to inhibit these types of responses. The treatment initially begins with allergen elimination or avoidance, followed by drug interventions to prevent mediators’ exaggerated release.\n- Different classes of these drugs\nThe major types of antiallergic drugs are –\n- Antihistaminic – dimedrol, diprazine, loratadine, cetirizine, levocetirizine, fexofenadine\n- Mast cell stabilizers – cromolyn, nedocromil, lodoxamide, pemirolast\n- NSAIDs – ketorolac, flurbiprofen, tromethamine\n- Corticosteroids – fluoromethanol, betamethasone, hydrocortisone, prednisolone\nDrugs used to relieve generalized symptoms of immediate allergic response –\n- Adrenomimetics – adrenaline\n- Methylxanthines – euphylline, theophylline\n- Ca+2 preparations – calcium chloride, calcium gluconate\n- Indications in which they are preferred\n- Antihistaminic used in histamine-mediated allergic reactions – allergic rhinitis, allergic conjunctivitis\n- Mast cell stabilizers used in vernal keratoconjunctivitis, keratitis, allergic keratoconjunctivitis, and giant papillary conjunctivitis, itching\n- NSAIDs are used to inhibit inflammatory response mediated by prostaglandins and in seasonal allergic reactions\n- Corticosteroids are used in extreme allergic reactions, to suppress the overall immune response and inflammatory response throughout the body.\n- Combinations of drugs used to treat drug-mediated allergic response (e.g., penicillin)\n- Side effects\n- Antihistaminic – sedation, peripheral anti-secretory effects, CNS dysregulation, palpitations\n- Mast cell stabilizers – burning and stinging pain at the site of administration, watery, puffy eyes (ophthalmic preparations)\n- NSAID – transient stinging and burning sensation, allergic reaction & keratitis in rare conditions\n- Corticosteroids – overall decrease in immune response, secondary infection, anti-prostaglandin effects\n- Adrenomimetics – increased sympathetic effects\n- Methylxanthines – hypotension, arrhythmias, tremors, insomnia, GI disturbances\n- Ca+2 preparations – bradycardia, vasodilation, GI disturbance, hypercalcemia, arrhythmias\nv Antihistaminic – Sensitivity to specific antiallergic drugs, pregnancy, and lactation\nv Mast cell stabilizers – sensitivity of the patient to drug, and formulation\nv NSAIDs – sensitivity to NSAID, corneal epithelial defect (ophthalmic preparation), pregnancy\nv Corticosteroids – pregnancy, immunodeficient patients\nv Adrenomimetics – pheochromocytoma, patients on sympathomimetic therapy\nv Methylxanthines – sensitivity to specific drugs\nv Ca+2 preparations – patients with the risk of cardiovascular events\nDrugs used in anaphylaxis\n- What are they\nAnaphylaxis can be explained as a severe allergic reaction that has a rapid onset and may be fatal. The hyperactivation of mast cells and basophils in the blood involved crosslinking IgE and aggregation of other mediators. Activation of mast cells or basophils rapidly respond and release preformed mediators like histamine, peptides, and proteoglycans. This is further accompanied by phospholipase A2 activation, followed by prostaglandins, leukotrienes, and PAF (platelet-activating factor).\nA release of inflammatory cytokine and chemokines is also mediated that include tumor necrosis factor (TNF), interleukins. Many of these mediators are thus believed to be responsible for the pathophysiology of anaphylaxis. Anaphylaxis can thus be a response to multiple mechanisms of allergic responses. The drugs used to suppress such an exaggerated body response are known as anaphylactic drugs.\n- How do they work\nSeveral mediators have variable effects on different organs that precipitate serious implications. The table below comprises the key mediators and their multiple effects on organs.\nTable 1 – Mediators and their physiological effect\n|Chemical mediator||Physiological effect|\n|Histamine||Vasodilation, increase in vascular permeability, tachycardia, increased cardiac contractility, hypersecretion|\n|Prostaglandin D2||Bronchial, pulmonary, and coronary vasoconstriction, peripheral vasodilator|\n|Leukotrienes||Bronchoconstriction, increased vascular permeability, airway remodeling|\n|Platelet-activating factors||Potent bronchoconstriction and increased vascular permeability|\n|Tumor necrosis factor (TNF)||Neutrophils activation, chemokine synthesis, recruitment of other immune cells|\nThe drugs that are used for management, either act on the chemical mediator release, or they are used to relieve the physiological effects elicited by these mediators.\n- Different classes of these drugs\nSince anaphylaxis is a life-threatening condition, the management is focused on emergency stabilization, followed by a combination of drug therapy.\nv Emergency management – Airway, breathing, and circulation resuscitation to ensure the survival\nv Vital stats monitoring – oxygen administration with cardiac monitoring and oxygen saturation.\nv Airway management: in case of severe respiratory distress, tracheal intubation should be immediately performed. Investigation for angioedema and following fluid removal should be done immediately to reduce the chance of respiratory failure.\nv Decontamination: the patient should be physically examined for any causative agent, and removed if found, followed by removal of the endogenous agents by chelation or mechanical removal.\nDrug of choice – Epinephrine – 0.3 to 0.5 mg (0.3 to 0.5 mL of the 1:1000 dilutions) I.M. repeated every 5 to 10 minutes as per response/relapse.\nAfter controlling the emergency crisis successfully, a drug regimen specific for anti anaphylaxis action is used.\nSecond-line therapy –\nv Corticosteroids – methylprednisolone, hydrocortisone, prednisolone\nv Antihistaminic – cimetidine, ranitidine, diphenhydramine\nv Anti-asthmatic medications –albuterol, ipratropium bromide, magnesium sulfate\nv Glucagon – for patients on β blockers, to manage refractory hypotension\n- Indications in which they are preferred?\nBefore the management therapy is initiated, a patient needs to be examined for proper diagnostic criteria. The investigation for clinical criteria is essential since the treatment protocol includes heavy doses of drugs, which may precipitate further complications.\nClinical criteria for anaphylaxis include –\n- Acute onset of illness lasting minutes to several hours, affecting the skin and/or mucosal tissue (pruritus hives/urticaria, flushing, edema). The onset shows associations with one or more than one of the conditions – i) Respiratory insufficiency – breathlessness, difficulty breathing, wheezing ii) Hypotension\n- Symptoms of organ dysfunction – loss of body control, pain, hypotonia\n- If a patient shows signs after exposure to an antigen (minutes-hours) as – skin, mucosal tissue reaction, respiratory compromise, fall in blood pressure with associated symptoms and GI symptoms of abdominal cramps, and vomiting.\n- Patients get exposed to a known allergen and develop hypotension rapidly.\nThe use of treatment is indicated as soon as these symptoms develop. The early symptoms for anaphylaxis include –\n- cutaneous flushing\n- chest tightness\n- life-threatening laryngeal edema (symptom – hoarseness in voice)\n- Side Effects\nThe side effects of these medications are as same as when used as an antiallergic drug. The side effect of epinephrine includes tachycardia, arrhythmias, and tremors.\nSince anaphylaxis is a life-threatening condition, the treatment protocol cannot be contraindicated as it may result in morbidity.\nHowever, caution should be taken with the patients using adrenergic blockers since epinephrine administration may precipitate a severe hypertensive crisis, upon high adrenergic stimulation.","If you’re one of millions of Americans suffering from allergies, including seasonal allergies, each year, you may never have heard of hydroxyzine pamoate, but you’ve likely heard of the brand name version of the medication, Vistaril.\nWhile hydroxyzine pamoate isn’t as well known as some other antihistamine medications, it has a ton of useful applications.\nHydroxyzine pamoate is used to treat a wide variety of conditions ranging from anxiety disorders to allergic reactions, hives, contact dermatitis, and more.\nSome people find that they have multiple medical conditions that can be effectively treated by hydroxyzine pamoate, since the medication treats common conditions.\nHydroxyzine pamoate has many practical applications and is an affordable, accessible drug for the majority of Americans.\nWhat is Hydroxyzine pamoate?\nHydroxyzine pamoate is an antihistamine with sedative, anxiolytic, and antiemetic effects.\nThe medication works by reducing the activity of the histamines, a naturally occurring substance in the body.\nHydroxyzine pamoate is available under the brand name Vistaril, which is a prescription medication used to treat anxiety and tension, nausea and vomiting, and severe itching (pruritus) from allergic conditions, including chronic hives (urticaria) and contact dermatitis.\nConditions Treated with Hydroxyzine pamoate\nHydroxyzine pamoate has a wide variety of clinical applications. Hydroxyzine pamoate is commonly used to treat the conditions listed below, but less common applications, including renal impairment, vomiting, and alcohol withdrawal have also been treated.\nHydroxyzine pamoate is a highly useful medication due to its expansive list of treated conditions.\nAllergies and Skin Conditions\nHydroxyzine pamoate is most frequently used to alleviate allergies and allergic reactions, as the medication is an antihistamine that blocks the body’s natural production of the substances that cause allergic reactions (histamines).\nHydroxyzine pamoate is most frequently used to treat skin reactions caused by allergens, including chronic hives (urticaria), severe itching (pruritus), and contact dermatitis.\nHydroxyzine pamoate can also be used to treat general itching or inflammation of the skin due to an allergic reaction.\nThe other major application of hydroxyzine pamoate is the treatment of anxiety. While researchers don’t know exactly how hydroxyzine pamoate works to treat anxiety, they do understand that the medication changes the effects of two different chemicals in our bodies: histamine and serotonin.\nHydroxyzine pamoate produces a calming effect as it blocks histamines, giving you the feeling of drowsiness or sleepiness that sometimes accompanies other antihistamine medications, like benadryl.\nWhat sets hydroxyzine pamoate apart from other antihistamines is that the medication also has an effect on serotonin, a chemical which affects mood.\nThis unique effect of hydroxyzine pamoate on serotonin is the reason why the medication is the only antihistamine used to treat anxiety.\nAlthough hydroxyzine pamoate is capable of treating many different anxiety disorders, it is most commonly used to treat generalized anxiety disorder (GAD).\nWhen used for the treatment of anxiety, hydroxyzine pamoate is taken on a daily basis at room temperature and is often combined with another medication, such as Celexa.\nUnlike other medications used to treat anxiety and panic disorder, such as Ativan or Klonopin, hydroxyzine pamoate is not a controlled substance and it carries a lower risk of addiction and dependence than some other medications.\nBecause hydroxyzine pamoate has a calming and sedative effect, the medication is sometimes used to treat insomnia in people who have difficulty sleeping.\nUnlike Ambien, Lunesta, and other prescription sleeping aids, hydroxyzine pamoate is not a controlled substance and does not have the same risk of addiction as some other medications used to treat occasional insomnia.\nHydroxyzine pamoate is used on an as-needed basis when used to treat sleeping issues.\nHydroxyzine pamoate is sometimes used In surgical settings to treat anxiety and nausea that can occur prior to anesthesia.\nAs an antihistamine, hydroxyzine pamoate has a calming effect on the central nervous system, or CNS.\nOne of the lesser known applications of hydroxyzine pamoate is the treatment of a painful bladder condition known as interstitial cystitis, which hydroxyzine pamoate is FDA-approved to treat.\nInterstitial cystitis is characterized by a number of bladder symptoms, including urinary frequency, urinary urgency, recurrent pelvic pain, pelvic pressure, and discomfort in the bladder and pelvic region.\nPatients who suffer from interstitial cystitis will often use hydroxyzine pamoate in tandem with other medications, such as Urogesic Blue, to treat interstitial cystitis symptoms.\nThe medication affects mast cell degranulation, which is thought to play a role in interstitial cystitis symptoms among patients who also have a history of allergies, migraines, and irritable bowel syndrome.\nWhen used to treat interstitial cystitis, hydroxyzine can help reduce nocturia (night time urination), daytime frequency, and painful urination and sexual intercourse.\nHow it Works\nWhen it comes to treating allergic reactions, hydroxyzine pamoate works to ease the symptoms of allergic reactions by blocking the natural substances, called histamines, that your body produces during an allergic reaction.\nHistamines can cause itching, hives, sneezing, and just about any other miserable allergy symptom you can think of which would otherwise need to be treated with cetirizine or levocetirizine.\nHydroxyzine pamoate binds to histamine receptors, just like anticholinergic agents, in the body, preventing the cells from releasing the chemical messages that cause sneezing, itching, and increased mucus production.\nWhen used to treat anxiety, hydroxyzine pamoate works by altering the effects of histamine and serotonin, two naturally occurring chemicals in the body.\nDoctors and researchers do not understand the exact mechanism that makes hydroxyzine pamoate effective for treating anxiety, but it is generally agreed that the medication helps to balance out the levels of serotonin in the brain.\nThe calming effect produced by the antihistamine is enhanced by hydroxyzine pamoate’s effect on serotonin, helping to reduce anxiety.\nThanks to a large number of manufacturers of the generic name of the medication, hydroxyzine pamoate is generally covered by Medicare and most insurance plans.\nCost depends on the form and the dosage, but the medication can generally be acquired for less than one dollar per pill (and sometimes significantly less). Use of a pharmacy discount card or paying the cash price can also lower the cost for uninsured individuals or those whose insurance does not cover hydroxyzine pamoate.\nThe table below compares the costs associated with a 30-day supply of both generic hydroxyzine pamoate and the brand name medication, Vistaril.\nCost of a 30-Day Supply of Vistaril Compared to Hydroxyzine Pamoate\n25 mg capsules\n50 mg capsules\nHydroxyzine pamoate is a multi-purpose drug with many benefits. Because hydroxyzine pamoate comes in many forms and strengths and is produced by a large number of manufacturers, it is generally very affordable, even for uninsured and underinsured patients.\nBecause hydroxyzine pamoate can be used to treat a variety of conditions, patients who take the medication to help manage their anxiety may also find that the drug has the added benefit of reducing the symptoms of your seasonal allergies as well.\nHydroxyzine pamoate is considered safe and effective and is generally well-tolerated and can be used on patients of all ages under appropriate supervision.\nIt can be taken with or without food, at regular times, or as needed, depending on the condition being treated.\nHydroxyzine pamoate comes in a wide variety of forms and strengths, each of which is well-suited to treatment of a particular condition.\nHydroxyzine pamoate tablets come in 5 mg, 25 mg, and 50 mg doses, while capsules are manufactured in 25 mg, 50 mg, and 100 mg strengths.\nHydroxyzine pamoate also comes in the form of a syrup/oral suspension at a strength of 10 mg/5 mL, and in an injectable solution at strengths of 25 mg/5 mL and 50 mg/5 ml.\nHydroxyzine pamoate reaches its maximum effect approximately 30 to 60 minutes after it is taken, and adverse effects last for approximately 4 to 6 hours.\nYour health care professional will choose the appropriate dose of hydroxyzine pamoate for your depending on a number of factors, including the condition being treated, the severity of the condition, and your age, weight, and gender.\nIn some cases, your doctor may prescribe a dosage “range” depending on the severity of your symptoms on a given day.\nFor example, you may be instructed to take 25 mg – 50 mg (1 to 2 capsules) per day depending on the severity of chronic hives (urticaria) managed by hydroxyzine pamoate. Hydroxyzine pamoate is considered safe for long term use, but most patients take it for four months or less, depending on the condition being treated.\nSide Effects of Hydroxyzine pamoate\nHydroxyzine pamoate has side effects similar to those of other antihistamines, such as over the counter allergy medications like Benadryl or Zyrtec. If you have taken allergy medication before, you’re likely already familiar with some of the common side effects of hydroxyzine pamoate, which include:\n- Dry mouth\n- Skin rash\nInvoluntary motor activity (tremor or convulsions) – generally observed when doses are taken that are higher than recommended\n- Urinary retention\n- Stomach distress\nOther serious side effects associated with hydroxyzine pamoate can include:\n- Allergic reactions\n- Increased heart rate\n- Unsafe heart beat\n- Double vision\n- Blurred vision\n- Loss of appetite\nIronically enough, it is possible to have an allergic reaction to a medication designed to treat allergic reactions.\nAlthough allergic reactions to hydroxyzine pamoate are uncommon, they are possible. If you experience signs of an allergic reaction to hydroxyzine pamoate, such as hives, difficulty breathing, or swelling of your face, lips, tongue, or throat, you should seek medical attention immediately.\nReferences, Studies and Sources:\nOwner, entrepreneur, and health enthusiast.\nChris is one of the Co-Founders of Pharmacists.org. An entrepreneur at heart, Chris has been building and writing in consumer health for over 10 years. In addition to Pharmacists.org, Chris and his Acme Health LLC Brand Team own and operate Diabetic.org and the USA Rx Pharmacy Discount Card powered by Pharmacists.org.\nChris has a CFA (Chartered Financial Analyst) designation and is a proud member of the American Medical Writer’s Association (AMWA), the International Society for Medical Publication Professionals (ISMPP), the National Association of Science Writers (NASW), the Council of Science Editors, the Author’s Guild, and the Editorial Freelance Association (EFA).\nOur growing team of healthcare experts work everyday to create accurate and informative health content in addition to the keeping you up to date on the latest news and research."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0cbe35c6-0112-4f63-99f2-fa872191987b>","<urn:uuid:efe1d592-228f-4259-b120-88cbf856e645>"],"error":null}
{"question":"What are the key components of the forest ecosystem decision support tool for Belgium?","answer":"The decision support tool consists of three main components: 1) The RMI atmospheric model that down-scales IPCC scenarios for detailed climate projections over Belgium, 2) The UA ANAFORE forest management model that simulates water, carbon and nitrogen exchanges for Belgian forest types, and 3) Two supplementary modules dealing with nutrients and pollutants that incorporate element mobility in relation to water and nitrogen within the system.","context":["Research project SD/RI/06A (Research action SD)\nIn the global change context, forest ecosystems are key components for successful mitigating strategies, as they are supposed to act through two main avenues: (i) increased carbon storage in biomass following increased productivity and (ii) decreased fossil fuels emissions due to increased wood utilization either as biomass energy or as substitution low energy-inputs materials.\nThe extent to which forest ecosystems will be able to effectively ensure these roles in the future remains however poorly documented, due to an intricate web of direct and indirect effects of global change.\nDirect effects are due to a combination of stress events (e.g. extreme droughts) and of diffuse pollution (contamination, nutritional unbalances). Indirect effects are those generated by alternative energy sources used to mitigate climate change impacts. The impacts of radionuclides on forests are also to be considered as a possible side effect of mitigation strategies.\nThe objective of this project is to generate a decision support tool (DSS) to analyse risks to forest ecosystems and forest responses as a result of extreme climate events, with a focus on addressing long-term effects on water, carbon and nutrient cycling in Belgian forest ecosystems. By extension, the tool will be able to simulate the enhanced dispersion of certain elements (radionuclides and trace metals) at the biosphere/geosphere interface, through the coupling of element fluxes to the above-mentioned water, carbon and nutrient fluxes. In all, this type of study has an obvious potential for linking climate (extreme droughts) with forest growth (biomass) and element cycling (heavy metal contaminated ground water and sites, nuclear waste disposal) for the better evaluation of environmental solutions.\nThe DSS will be based on a combination of existing models and approaches designed to predict direct ecosystem responses to extreme climate events (i.e. extreme droughts) – including a quantification of the associated risks, as well as associated effects such as the redistribution of pollutants at the soil / vegetation / atmosphere level as they follow the biogeochemical cycles. The following components are at the centre of the DSS system:\n• The RMI atmospheric model, which will be used to down-scale Intergovernmental Panel for Climate Change (IPCC) scenarios at higher resolution over Belgium, such as to produce detailed projections of precipitation, temperature, heat waves, wind phenomena and cloud formation affecting incoming radiation as a consequence of extreme events.\n• The above climate model will be used as input for the UA ANAFORE forest management model, which is designed to simulate water, carbon and nitrogen exchanges at the ecosystem level for the major forest types of Belgium. The ANAFORE model will also be used to simulate forest yield quantity and quality in function of forest management for the different scenarios.\n• The ANAFORE model would be supplemented by the addition of two modules, dealing with nutrients and pollutants, respectively, such that the relative mobility of elements in respect to water and nitrogen within the system is incorporated, by including such factors as retardation and bioaccumulation in the module. In effect, this will imply the mathematical coupling of pollutant and nutrient fluxes to water, carbon and nitrogen fluxes within the ANAFORE model.\nIntegration of the system will be tested by selection of specific climate extreme scenarios and validation through existing datasets of water, carbon and other substances in Belgian forests. The work on nutrients will focus on P, Ca, Mg and K. The pollutants that will be considered are heavy metals / micronutrients (Cl, Mn, Cu), as well as radioactive elements (36Cl, 41Ca, 94Nb and 126Sn). Including elements relevant to long-term ground radioactive contamination provides added value to the tool for studying enhanced long-term dispersion from underground disposal nuclear sites into the biosphere as a result of severe water table fluctuations (specific risk assessment).\nInteraction between the different partners\nThe partners of the project have different modelling expertise (see above). Their models/model results will be integrated into the final DSS as described above.\nExpected results and/or Products\nThe final goal is a decision support system (DSS) based on a user-friendly interface to query a database containing geospecific data on belgian forest growth, health, soil contamination and soil nutrient status as influenced by climate and forest management. Special emphasis will be on the production of risk maps over Belgium.\nIntermediate deliverables will be:\n1. Downscaled climate scenario’s\n2. Input database geospecific forests/soil/pollutants/nutrients over Belgium\n3. Nutrient module\n4. Pollutant module\n5. Integrated forest model\nPartner 1, UA\nThe research team of Plant and Vegetation Ecology of the University of Antwerpen (UA) has a long-term experience with the ecophysiology and functioning of plants, and of trees in particular. In most of the current projects at least part of the role of the research group deals with the integration of experimental results in functional models.\nPartner 2, RMI\nThe Royal Meteorological Institute (RMI) is the national meteorological service of Belgium, providing an extensive range of weather forecasts, climatological, environmental and other services. The meteorological and hydrological modeling unit is part of the international consortiium ALADIN. The unit has solid experience in numerical model development and regional downscaling of the past climate.\nPartner 3 SCK•CEN\nThe group Biosphere Impact Studies of the SCK•CEN is involved in terrestrial and aquatic radioecology and impact studies, focusing mainly on the mechanistics of radionuclide availability, transfer and biological uptake in the soil-plant environment and biogeochemical cycling of radionuclides in ecosystems.\nPartner 4, UCL\nThe general interest of the Earth and Life Institute, UCL University, is to understand the reactivity of forest ecosystems, stands and trees to resources (e.g. carbon, nutrients, water) and/or controllers (e.g. light), using an ecosystem approach. The main disciplines are related to ecophysiology and ecology.\nProf. Dr. R. Ceulemans, Dr. Gaby Deckmyn\nPlant and Vegetation Ecology Research Group (PLECO),\nDepartment of Biology,\nUniversity of Antwerpen\nTel 32 3 265 2256\nFax 32 3 265 2271\nThe present composition of the follow-up committee is given (name of each member and their institution). This list will, if necessary, be updated on the FEDRA website. A bit of extra information can be noted (target group, etc.).\nProf. Dr. Nicole van Lipzig, Department of Earth- and Environmental Sciences, Katholieke Universiteit Leuven\nDr.ir. Vincent Kint, Agentschap voor Natuur en Bos, Brussel\nDr. Mark Smits, Universiteit Hasselt, Centrum voor Milieustudies / Center for Environmental Studies, Universitaire Campus\nDr. Ir. Vincent Brahy, Département de l’Etude du Milieu Naturel et Agricole (DEMNA), Cellule Etat de l’Environnement Wallon (CEEW)\nDr. Ir. Marc Herman, Département de l’Etude du Milieu Naturel et Agricole (DEMNA), Direction du Milieu Forestier, Observatoire Wallon de la Santé des Forêts (OWSF).\nIr. Stéphane Vanwijnsberghe, Bruxelles Environnement (IBGE), Direction ‘Qualité de l’environnement et gestion de la nature’, sous-direction ‘Forêt et Nature’.\nDr.ir. Hans Verbeeck, Labo voor Plant Ecologie, Faculteit Landbouw- en Toegepaste Biologische Wetenschappe, Universiteit Gent.\nRik De Vreese, Vereniging voor Bos in Vlaanderen (VBV)\nProfessor George Shaw, University of Nottingham, UK, Division of Agricultural & Environmental Sciences\nDr. Tom Anthonis, INBIO, Centre of Expertise, Industrial Biotechnology and Biocatalysis, Faculty of Bioscience\nDr. Bea Kayaerts, Strategische Adviesraad Ruimtelijke Ordening (SARO, RWO)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:836f87e0-596c-49f3-99f0-8d0b45ab6d62>"],"error":null}
{"question":"What determines the maximum travel capabilities of compression vs torsional springs?","answer":"Compression springs' maximum travel is determined by two factors: the solid height and the free length, with potential for increased travel by extending the free length. In contrast, torsional springs' travel capability is fixed and can only be changed by adjusting the spring dimensions to reduce stress, such as modifying the spring index or adding coils - though these changes would also weaken the spring. Torsional springs measure travel in degrees of rotation rather than linear distance.","context":["Spring Travel Distance Calculator\nSpring Calculator program used to calculate the amount of travel, deflection or distance a spring is able to achieve under an applied load.\nSpring design can be a little bit overwhelming at times. Your spring’s force, maximum safe travel, and maximum safe load are the factors that will tell you whether your spring will work correctly in your device or not. Spring Creator includes three spring travel calculators; compression spring travel calculator, extension spring travel calculator, and torsional spring travel calculator.\nYou must know all of the limitations involved inside of your device to set the physical dimensions as well as the tolerances for these dimensions. That is the basic part of designing your spring. When you get into spring rate, load, and travel; things get a little more complicated. When designing your spring the dimensions you started with may be too strong or too weak. You define your spring’s strength using spring rate. If it is too strong, your spring might not be able to travel enough due to it being highly stressed. On the other hand, your spring might be too weak and the loaded height it reaches is lower than the required loaded height. This means that you will have to make adjustments to the spring dimensions. The force chart provided tells you what modifications you must make in order to adjust the spring rate.\nThe Travel Chart\nCompression Spring Travel Calculator\nCompression spring travel comes up in two categories when using our compression spring travel calculator. One of the sections is “Maximum Travel Considering Solid Height”. This section shows the maximum safe travel you are able to reach with your design considering its solid height. The other one of the sections is “True Maximum Travel”. This section shows you the potential travel your spring might be able to achieve if you make the free length longer. To utilize this full potential travel you must subtract the Maximum Travel Considering Solid Height from the True Maximum Travel and add the product to the free length. The True Maximum Travel will sometimes be higher than the Maximum Travel Considering Solid Height unless you’ve already topped out the potential travel. If you have maxed out all of the travel you are able to achieve from your design, these values will be the same.\nCompression Spring Deflection\nExtension Spring Travel Calculator\nExtension spring travel also shows two categories when using our extension spring travel calculator. In one of the sections it is shown as “Maximum Safe Travel”. This is also a potential amount of travel you would be able to extend your tension spring to if you were to remove the hooks. This value is shown because the strength of your hooks also depends on the type of wire you’re using. The other value shown is “Maximum Safe Travel Considering Hook Stress”. Most extension springs will have hooks on them so this value shows you exactly how much your spring will be able to travel with the dimensions, material type, and type of hooks you put on your spring. The Maximum Safe Travel will also be greater than the Maximum Safe Travel Considering Hooks Stress if your spring has hooks. If it doesn’t have hooks, the values will also be the same.\nExtension Spring Deflection\nTorsional Spring Travel Calculator\nTorsion spring travel is only shown in one section within our torsional spring travel calculator. This is because that amount of travel will not change unless your spring dimensions are adjusted so that it is under less stress. This would mean adjusting the spring index or adding a few coils but these adjustments would make your spring weaker as well. Torsional springs deflect in degrees of rotation. This is why the torsion spring rate per degree is measured in inch-pounds per degree (in-lb/deg). The torsion spring’s legs are the ones that make the torsional spring work. In most cases you will have one stationary spring and the other one will be what we call the “moment arm”. The moment arm is the spring leg that is to be in motion. In some cases, both spring legs will torque.\nTorsional Spring Deflection","Mechanical Springs Constant\nDefinition: The constant rate of force per a defined distance traveled of a mechanical spring made of helically wound round wire.\nMechanical springs consist of coils. The constant force these mechanical springs are able to exert through their coils is based upon the many physical dimensions and sometimes custom modifications the spring might have. There are different types of mechanical springs; mechanical compression springs,mechanical extension springs, mechanical torsional springs, and mechanical conical tapered springs. Mechanical cone springs do not exert a constant amount of force per distance traveled due to the variation in the outer diameters.\nMechanical Compression Springs:\nMechanical compression springs have pitch in between the coils which make it possible for these to be compressed. Their force depends on the tightness of the coils' diameters and the amount of pitch in between these coils. If your mechanical compression spring has many coils and little pitch in between them, there is a chance that it will compress down to its solid height depending on the spring index (tightness of the coils). If it has very little coils with a lot of pitch in between them, the mechanical spring will be very strong but, due to the stress in between its coils and the tightness of the diameter, it might not be able to deflect to its solid height without taking a permanent set. The spring constant of mechanical compression springs is defined as pounds of force per inch (newtons per millimeter). It is calculated by dividing the required load you will be applying on the spring by the amount of travel it should deflect to reach your desired loaded height as shown in the formula below.Rate (k) = Load (F) ÷ Travel (x) k = F ÷ x\nIf your spring will travel 2\" (inches) under a load of 10lbf (pounds of force), the spring rate will be of 5lbf/in (pounds of force per inch).k = F ÷ x k = 10 ÷ 2 k = 5\nMechanical Extension Springs:\nMechanical extension springs don't have any pitch in between its coils. They are made by tightly winding the coils one on top of the other to be able to exert an amount of force when it is being extended. They are manufactured with hooks on the ends which latch on to the pieces in the mechanism that will be pulling on the spring. The force of your spring also depends on the spring index and the amount of coils. In this case, the amount of coils is based on the spring's body length (length of spring without the hooks) and the wire diameter. The less coils your spring has and the tighter the spring index is, the more force and stress it will have. Meaning that it will be a strong spring but it will not be able to deflect much. In between the coils there is already a stored energy which is released once the extension spring is extended to a point where light shows in between its coils. This is the tension that allows for your spring's coils to be fully compressed. The initial tension is an additional force besides the one based on your spring rate. This means that to calculate a mechanical extension spring's rate, you will need to subtract the initial tension from the load and then divide the result by the amount of traveled distance required to meet an extended length as shown in the following formula.Rate [k] = (Load [F] – Initial Tension [IF]) ÷ Travel [x] k = (F – IF) ÷ x\nIf your spring is required to travel 0.5\" (inches) under a load of 2lbf (pounds of force) and has an initial tension of 0.5lbf (pounds of force), the spring rate will be of 0.875lbf/in (pounds of force per inch).k = (F – IF) ÷ x k = (2 – 0.5) ÷ 0.5 k = 1.5 ÷ 0.5 k = 3.5\nMechanical Torsion Springs:\nMechanical torsion springs also don't have any pitch in between the coils like mechanical extension springs do but these execute a radial force instead of a linear one like both extension and compression springs do. The way to know a mechanical torsion spring from a mechanical extension spring is the difference in the ends. Mechanical torsion springs have legs which help it torque inside a mechanism while mechanical extension springs have hooks to be pulled or extended. For torsional springs, the same rule applies for more and less force as it does to compression and extension springs. More coils generate less force while tight spring indexes generate more force and vice versa. To calculate the constant force of a mechanical torsion spring the units of measurement change from pounds of force to inch-pounds of force; and the units for travel change from inches to degrees of deflection due to this being a rotational force. We calculate the rate per degree by dividing the amount of force by the required degrees of travel as shown in the formula below.Rate per Degree (k) = Torque (T) ÷ Deflection (x) k = T ÷ x If your spring will be traveling 90º (degrees) of deflection under a load of 9 inch-pounds, the rate per degree is of 0.1 in-lbs/deg. (inch-pounds per degree). k = T ÷ x k = 9 ÷ 90 k = 0.1"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:33bef673-b47d-4510-b439-86804ea9da65>","<urn:uuid:8ed6e04c-d0b8-40a1-b241-aed5fcf3d5c5>"],"error":null}