{"question":"For a comparative study on plant stress factors, how do the mechanisms for managing chloride toxicity differ from those for managing boron toxicity in irrigation water?","answer":"The mechanisms for managing chloride and boron toxicity are distinct but both require careful attention. Chloride toxicity is managed through a sophisticated biological mechanism where plants use the SLAH1 and SLAH3 channel complex to regulate chloride intake, especially under saline conditions. This allows plants to reduce chloride uptake while maintaining essential nitrate absorption. In contrast, boron toxicity occurs at very low concentrations despite being an essential nutrient. Management practices for both toxicities include proper leaching and increased irrigation frequency, but boron specifically requires avoiding fertilizers containing boron and careful crop selection, as different crops have varying sensitivities to these ions.","context":["It is common knowledge that salt consists of the cation sodium and the anion chloride. However, the substance used to season food has been a cause of great concern to farmers for some time now: In times of climate change, more and more agricultural areas have to be irrigated. This inevitably leads to the increasing salinisation of soils, that is the accumulation of sodium and chloride ions.\nPlants that grow on such soils usually have a hard time. And that is for a reason: Higher doses of chloride have a toxic effect on plant development. In contrast, they need the anion nitrate as an essential source of nitrogen to build proteins and multiply their DNA. The Würzburg plant scientists Dietmar Geiger and Rainer Hedrich have recently studied whether and how plants are capable of distinguishing between the nutrient nitrate and the harmful chloride. They present the results of their research in the current issue of the renowned journal Current Biology.\nTwo channels filter nitrate and chloride\nElongated cells that pervade the plant body like a system of tubes conduct water and nutrients from the roots into the shoot. Specialised cells inside the roots load nutrients absorbed from the soil onto this conducting system. In these loading stations, the Würzburg researchers detected the two anion channels SLAH1 and SLAH3 which are responsible for regulating the passage of nitrate and chloride.\nIn cooperation with the Spanish working group of Dr Colmenero-Flores, the scientists studied genetically modified plants in which SLAH1 or SLAH3 is missing. The sap of these mutants ascending to the shoot through the plant's vascular system only contained half the amount of chloride ions. The nitrate content, however, remained unchanged. Hence, the researchers concluded that both anion channels regulate the entry of chloride into the shoot.\nBiophysical studies uncover chloride switch\nTo pinpoint the anion filter in charge of nitrate in the channels, the researchers next took a closer look at the channel molecules. For this purpose, they measured the anion current through SLAH1 and SLAH3 directly using biophysical methods. \"We found SLAH1 to be incapable of conducting anions in the first place and SLAH3 to mainly conduct nitrate,\" Professor Rainer Hedrich describes the unexpected result.\nIn the course of further studies, the scientists found the explanation for their strange finding: \"The alleged contradiction between the nitrate and chloride contents in the sample plants and in the mutants was resolved when we brought the two anion channels together,\" Professor Dietmar Geiger explains. It turned out that the two channels form a functional complex. \"Each time SLAH1 enters into the complex, the anion filter in SLAH3 will switch from nitrate to chloride and vice versa,\" Geiger further.\nWhere does this switch play a role? The Spanish colleagues delivered the answer to this question. In order to determine the identity of the chloride-nitrate switch in the plant, they simulated salt-affected soils to the plants. The higher the salt load the roots of the sample plants were exposed to, the more SLAH1 was withdrawn from the anion channel complex. Hedrich: \"In this process, the chloride-conducting complex gradually evolves into a nitrate-conducting status.\" This allows the plant to maintain its intake of nitrate as a vital source of nitrogen without taking damage by the salinisation-related increase in chloride concentration.\nResults published in \"Current Biology\"\nWith their studies on the salt tolerance of plants, the Würzburg plant scientists Dietmar Geiger and Rainer Hedrich together with their colleagues from Seville and Riyadh have demonstrated a wholly new concept of anion intake regulation in the vascular tissue of the roots. According to the researchers, the discovery of the regulatory anion channel SLAH1 will not least have an impact on optimising the salt tolerance of crops in the future.","Irrigation Water Quality\nAll the care you take to ensure the health of your plants will have gone to waste if you give them poor-quality water to drink—Guy Sela takes us through the science behind irrigation water quality…\nBoth irrigation water quality and proper irrigation management are critical to successful crop production. The quality of irrigation water might affect both crop yields and the physical condition of the soil, even if all other conditions and cultural practices are favorable. In addition, different crops require different irrigation water qualities.\nThis means that testing irrigation water prior to selecting a site and the crops to be grown is critical. The quality of some water sources might change significantly with time or during certain periods—like dry or rainy seasons—so it is recommended to have more than one sample taken, at different time periods.\nThe parameters that determine irrigation water quality are divided into three categories: chemical, physical and biological. In this article the chemical properties of irrigation water will be discussed.\nThe chemical characteristics of irrigation water refer to the content of salts in the water as well as to parameters derived from the composition of salts in the water—parameters such as EC/TDS (electrical conductivity/ total dissolved solids), SAR (sodium adsorption ratio), alkalinity and hardness.\nThe primary natural source of salts in irrigation water is weathering of rocks and minerals. Other secondary sources include atmospheric deposition of oceanic salts (salts in rainwater), saline water from rising groundwater and the intrusion of sea water into groundwater aquifers. Fertilizer chemicals that leach to water sources might also affect irrigation water quality.\nProblems Related to Irrigation Water Quality\nIrrigation Water Salinity\nThe main problem related to irrigation water quality is water salinity—which refers to the total amount of salts dissolved in the water, but it does not indicate which salts are present.\nHigh levels of salts in irrigation water reduce water availability to the crop (because of osmotic pressure) and cause yield reduction. Above a certain threshold, reduction in crop yield is proportional to the increase in salinity level. Different crops vary in their tolerance to salinity and, therefore, have different thresholds and yield reduction rates.\nThe most common parameters used for determining irrigation water quality in relation to its salinity are EC and TDS. If irrigation water salinity exceeds the threshold for the crop, yield reduction will occur.\n|TDS ppm or mg/L||EC dS/m||Salinity hazard|\n|500 – 1,000||0.8 - 1.6||Medium|\n|1,000 – 2,000||1.6 - 3||High|\n|> 2,000||> 3||Very high|\nSodium Hazard and Irrigation Water Infiltration\nThe parameter used to determine the sodium hazard is SAR, or sodium adsorption ratio. This factor indicates the amount of sodium in the irrigation water in relation to calcium and magnesium. Calcium and magnesium tend to counter the negative effect of sodium.\nHigh SAR levels might result in a breakdown of soil structure and water infiltration problems. Soil tends to seal and become hard and compact when it’s too dry.\nIronically, higher salinity reduces the negative effect of sodium on soil structure. When sodium levels in the soil are high in relation to calcium and magnesium—in other words, when the SAR is high—flushing the soil with good-quality irrigation water will only exacerbate the problem.\nToxicity of Specific Ions\nIrrigation water quality can be also determined by the toxicity of specific ions. The difference between a salinity problem and a toxicity problem is that toxicity occurs within the plant itself, as a result of the accumulation of a specific ion in the leaves. The most common ions that cause a toxicity problem are chloride, sodium and boron. As they do with salinity levels, crops differ in their sensitivity to these ions.\nSpecial attention should be given to boron levels because toxicity occurs at very low concentrations, even though boron is an essential plant nutrient—toxic levels of even a single ion in irrigation water might make the water unsuitable.\nThere are some management practices that can help reduce the damage, however. These practices include proper leaching, increasing the frequency of irrigations, avoiding overhead irrigation, avoiding the use of fertilizers containing chloride or boron, and selecting the right crops.\nAlkalinity and pH\nAlkalinity is the sum of the amounts of bicarbonates (HCO3-), carbonates (CO32-) and hydroxide (OH-) in water. It is expressed as mg/l or meq/l CaCO3. Alkalinity buffers the water against sudden changes in pH. If the alkalinity is too low, any addition of acidic fertilizers will immediately lower the pH. In container plants and hydroponics, ions released by plant roots may also rapidly change the pH if alkalinity is low.\nManaging Irrigation Water Quality Problems\nInfiltration Problems Resulting From Low Irrigation Water Quality\nAs mentioned earlier, SAR (sodium adsorption ratio) is an irrigation water parameter used to predict problems of water infiltration into soil. SAR is determined as:\nApart from water shortage—which is a result of water infiltration problems—some other related problems might occur as well; for example, weed growth, diseases, poor aeration, poor germination of seeds or root rot.\nVarious measures can be taken to overcome water infiltration problems that are related to water quality, including reducing the SAR of the water supply, cultivation and tillage, the addition of organic residues, irrigation management, and water or soil amendments.\nSoil Amendments and Irrigation Water Quality\nThe purpose of soil amendments is to counter the effect of sodium by increasing the soluble calcium content or by increasing the salinity of the irrigation water.\nGypsum and other calcium-supplying materials\nGypsum is the most commonly used soil amendment. Since water infiltration problems caused by sodium affect mainly the upper few centimeters of soil, repeated small applications of gypsum—incorporated at lower rates into a shallow depth—are preferred over a single large application.\nIf the salinity of the irrigation water is low (EC<0.5 ds/m), gypsum can be added to the irrigation water at rates of one to four meq/l of dissolved calcium.\nWhen lime (CaCO3) is present in soil, some acids or acid-forming amendments can be used—these will cause calcium to be released into the soil solution. Some of these amendments are elemental sulfur, sulfuric acid and ferric sulfate.\nThese amendments improve soil structure and water infiltration by keeping the soil porous.\nBlending Irrigation Water Sources\nWater infiltration can be improved either by increasing irrigation water salinity or by reducing the SAR. By diluting the irrigation water source with water having a lower sodium concentration, the SAR of the irrigation water is reduced, even if calcium and magnesium concentrations are higher.\nManagement of soil and irrigation water salinity\nWhen salts build up in soil or in the growing medium, their concentration might become excessive. Salts are added to soil via irrigation water and with applied fertilizers. Applying more water than is needed by the crop leaches the salt below the root zone, deeper into the soil or out of the growing medium (when growing container plants). It is important to know how much to leach and when. The leaching requirement can be estimated from the following equation:\nLR = ECw/ [5*ECe - ECw)]\nWhere LR is the minimum leaching requirement for the crop, ECw is the electrical conductivity of the irrigation water in ds/m (irrigation water salinity) and ECe is the soil EC tolerated by the crop, measured in a saturated soil extract.\nThe total irrigation water amount that has to be applied to meet both crop demand and your leaching requirement can be estimated from the equation:\nAW = ET/ (1-LR)\nWhere AW is the amount of irrigation water that has to be applied, ET is the crop water demand and LR is the calculated leaching requirement. For example:\n- Crop water demand - 30m3/ha/day\n- ECe = 2.5 ds/m, ECw = 1.2 ds/m\n- LR = 1.2/(5*2.5-1.2) = 0.1\n- AW = 30/(1-0.1) = 33.33 m3/ha/day\nKnowing the total amount of irrigation water to apply is not enough for managing salinity—irrigation intervals must also be considered. The appropriate irrigation intervals will depend not only on crop water demand, but also on factors such as the salinity threshold of the crop and the soil’s capacity to hold water.\nApplying the same amount of water to two soils with different characteristics will result in different wetting patterns and depths. Irrigation depth in a heavy soil is lower than in a sandy soil, since heavy soils hold more water than sandy soils—this means that heavy soils require higher application amounts of irrigation water at larger intervals in order to prevent an accumulation of salts exceeding the salinity threshold of the crop.\nIt must be noted, though, that salinity in the root zone increases between irrigations as a result of crop water uptake and water evaporation from the soil. The timing of leaching is not critical provided the salinity threshold of the crop is not exceeded."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:3fb35ca8-d97e-436a-b393-eb2a21fa0813>","<urn:uuid:bf8a29d9-886a-411c-b842-3e60634851f8>"],"error":null}
{"question":"As someone interested in salmon farming, what are the benefits of farmed salmon for consumers and what environmental risks does it pose?","answer":"While farmed salmon is widely available in supermarkets, there are significant concerns to consider. Farmed salmon is marketed as a healthy choice and source of omega-3, though testing has shown some products overstate their omega-3 content, with some claiming up to three times the actual amount. On the environmental side, salmon farms pose serious risks to wild salmon populations. The farms create concentrated areas of adult salmon that become breeding grounds for sea lice parasites. These parasites can be fatal to juvenile wild salmon, with research showing wild salmon near fish farms are 73% more likely to be infested than those away from farms. Additionally, the 'natural' claims about farmed salmon are questionable, as their diet consists largely of abattoir by-products and requires artificial supplements to achieve their pink color, unlike wild salmon that get it naturally from eating krill.","context":["A Consumer New Zealand investigation shows farmed salmon, which is sold as fresh and smoked products in supermarkets, bears little resemblance to some of the “natural” claims made about its production.\nKing Salmon, which produces more than half of all farmed salmon from its operations in the Marlborough Sounds, claims “the feed replicates the natural diet of wild salmon”. However, Consumer NZ says that a major part of the fish’s diet is abattoir by-products – offcuts from poultry processing including feathermeal, as well as bloodmeal from cattle, pigs and sheep. Only a small proportion of farmed salmon’s diet comes from marine sources.\nSkretting Australia produces the pellets fed at most salmon farms here. It says just under 10 percent of the feed is fishmeal. Fish oil comprises seven percent – it has to be added to the pellets because it’s the main source of omega-3 for farmed salmon. Wild fish derive it from the algae and other marine plants found in the fish they eat.\nConsumer NZ CEO Sue Chetwin said farmed salmon’s artificial habitat also meant its feed had to be supplemented with astaxanthin, a carotenoid pigment, to give the fish their distinctive pink flesh. In the wild, salmon get their pink glow from eating krill and other crustaceans.\nKing Salmon has recently been certified by the Global Aquaculture Alliance, a trade-based group, which certifies sustainable choices.\nChetwin said the industry was keen to promote itself as a healthy, sustainable choice, but despite the rosy public relations, this was intensive farming which sat at odds with the “natural” claims made for products in store.\nConsumer also tested the omega-3 levels in five smoked salmon products. Salmon is regarded as a good source of omega-3. However, testing found some products that overstated their omega 3 content – one by three times the amount.\nFood manufacturers are allowed to show average values so small variations are expected. However, one product claimed more than triple the omega-3 indicated by the lab test.\nAoraki Smokehouse Cold Smoked Salmon (50g) claimed it contained 8.07g of omega-3 per 100g. Testing of a sample of this product indicated 2.2g per 100g. The company told Consumer it was aware the claim was incorrect and had changed its packaging.\nCountdown’s Signature Range Smoked Salmon Slices (50g) stated an omega-3 level of 3.2g per 100g. Testing for Consumer indicated 1.4g of omega-3 per 100g. Countdown withdrew its Signature Range smoked salmon after being notified of the results, stating this level of variation wasn’t acceptable. It expects to have correctly labelled product back in store in November.\nOther products tested included Regal Smoked Salmon Cold Smoked Slices (100g), Southern Ocean Smoked Pieces (100g) and Primesmoke Smoked Salmon Slices (50g).\nTesting of a sample of Regal salmon indicated 1.8g of omega-3 per 100g; testing of the Southern Ocean salmon indicated 2.4g per 100g. Both products, which are produced by King Salmon, stated an omega-3 content of 3g. The company said the levels stated on its packaging were based on historic test data compiled from four independent lab tests. It is retesting and will amend packaging if warranted by the results.\nTesting of the Primesmoke salmon found 1.8g of omega-3 per 100g, compared with a label claim of 1.4g.","For anglers who like chasing salmon and steelhead, sea lice are a topic of great interest for a couple of reasons. Today Mike Sanders, our General Manager at Deneki Outdoors, gives us the rundown!\nWhat Are Sea Lice?\nSea Lice are planktonic (adj. – of or relating to plankton) fish parasites that live in sea water. They thrive in tidal or protected areas. They attach to the outer body and feed on mucous, skin and blood of salmon and steelhead they like most anadromous fish actually. We have really good reason to hate them but there is a pretty cool aspect to them as well – such is life eh?\nWhy We Love ‘Em\nSea lice attach themselves to most adult salmon as they swim through fertile lice grounds on their return to rivers to spawn. Sea lice can not survive for long in fresh water. For that reason – when we catch a fish at BC West or Alaska West that is covered with sea lice and we do that a lot – we know that fish has just entered the Dean or the Kanektok.\nSea lice carry their eggs in two long eggs sacks that look like tails. Whether it’s true or not – legend has it that the tails fall off the lice long before the lice dies in fresh water. A fish with lice is fresh but if the lice have tails (the longer the better for some freaky reason) we think that is a super ocean fresh fish. Chromer toads are a turn-on for sure but if they are covered in long tailed lice – it just doesn’t get any better.\nWhy We Hate ‘Em\nIt’s really common for most all adult salmon to have sea lice attached to them. But its not so common for fry that are just starting their ocean life phase to be infested. Juvenile salmon or steelhead have thin skin and even one louse can inflict a fatal wound to an immature fish.\nExperts have noticed a spike in juvenile fish infestations in recent years. Why now? What is different? Adult salmon and juvenile salmon do not commonly exist in close proximity in the wild. The advent of farmed salmon has changed that. Farmed salmon share the same water as the migratory juvenile salmon and high concentrations of adult salmon are perfect breeding grounds for nasty parasite. This new habitat change has led to the spike in juvenile infestations, and some experts quote that wild salmon near fish farms are 73% more likely to be infested than salmon not near fish farms.\nUser groups and experts are very concerned about this data, and this is an especially hot issue for BC. Most all of the fish farmed in BC are Atlantics and if introducing a foreign salmon species to some of the world’s premier wild salmon and steelhead waters is not enough of a problem – Atlantics seem to be fond prey to the little planktonic parasites.\nSalmon farming is illegal in the Exclusive Economic Zone in Alaska, but farming salmon is legal in British Columbia. When farming first started in BC, Alaska fishery scientists initially feared farmed Atlantic salmon would escape and compete for food with native Alaska populations of wild salmon. Now the new sea lice connection to fish farms research shows there is a lot more to worry about than just escapement.\nIt does not take a marine biologist to deduce that farming fish near populations of wild salmon is a bad idea if you value wild salmon. Many conservation groups are trying to educate people to this issue – the Native Fish Society to name only one. We think you should know what kind of salmon you are eating and think twice about eating farmed salmon.\nAnd that’s that. Sea lice may be tiny little critters but they’re big friends and big enemies, both at the same time."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:c229be07-a712-4552-961c-04dabb11ab86>","<urn:uuid:d940723b-b84a-4e78-9964-a3c197fe0554>"],"error":null}
{"question":"How do consequences differ between ignoring a parking infraction notice versus a court order to pay money?","answer":"Ignoring a parking infraction notice generally has less severe consequences - you can simply pay the ticket to acknowledge guilt or request to contest it in writing. However, ignoring a court order to pay money can result in serious consequences including property seizure, asset freezing, bankruptcy proceedings, or even police action leading to prosecution, fines, or imprisonment. Additionally, authorities may trace hidden assets, enforce against associated companies, shareholders, directors, or insurers of the party concerned.","context":["How can I enforce a court order?\nA court order is an official proclamation by a judge (or panel of judges) that defines the legal relationships between the parties to a hearing, a trial, an appeal or other court proceedings. Such ruling requires or authorizes the carrying out of certain steps by one or more parties to a case. A court order must be signed by a judge.\nOnce you have obtained an order from a court, sometimes called an injunction, you may need to take steps to ensure it is obeyed or enforced.\nEnforcement in general\nIf you feel a court order is not being complied with, the first question to ask is whether or not the order is enforceable. Sometimes an order needs further action or may even require another court order to be enforceable.\nFor example, if a court has decided that you are entitled to compensation, this by itself is unenforceable and of no practical use. You will need to get the court, or another responsible body, to assess exactly how much compensation is due before you can demand enforcement.\nSome orders are fairly general, others quite precise. The more precise and detailed the order, the easier it is generally for the person against whom it is made to know what they must or must not do. It is also easier to determine whether the order is being complied with or if it has been breached. Compare\n- a) “Members of the X tribe should be permitted reasonable access to ancestral lands for religious purposes”\n- b) “The members of the X tribe listed in Schedule A, shall be permitted to access any part of the land shown on the plan at Schedule B, for a period not exceeding 8 hours in any one visit, at the times and on the dates listed in Schedule C”\nIf you have obtained a court order which is enforceable and if there is a clear breach of the court order, the breaching party may be committing a criminal offence or may be in “contempt of court”. This may lead to police action, prosecution, fines, or imprisonment. However, you will need to find out what the correct procedure is to get the responsible authority to act.\nEnforcing a court order depends on the type of order in question. The three main types of court order are:\n- An order to do or not to do something\n- An order to pay money. Somebody must pay money to someone else\n- A declaration or clarification of the legal rights or obligations of someone. For example, the court may declare a contract void.\nWe will look at enforcement procedures for each of these types of order in turn.\nEnforcing a court order to do or not to do something\nIf you have obtained an order of this kind, the first step is to make sure that the order is notified to everyone who may need to take action. Sometimes, enforcement requires administrative action by someone other than your opponent in court. To enforce an order against one government office or ministry, you may need to contact another ministry or office. Similarly, to enforce an order against the “Head Office” of a company, you may have to contact the company’s operations department or their contractors or security personnel.\nYou should communicate the terms of the order to:\n- the most relevant people within the company/ministry concerned\n- other persons or organisations involved in the activity in question\n- Anyone affected by the order who is or might be involved in enforcing it.\nIn some legal systems there are bailiffs whose job it is to officially notify decisions of a court. If you can have the court order delivered by a bailiff, it becomes impossible for the recipient to deny knowledge of the order.\nSome people against whom orders are made deliberately create confusion by referring complainants to the wrong department within the company or ministry or by claiming ignorance of an order.\nMake sure that the senior management or top personnel are contacted in writing (by letter or email or fax). You may need to research the names of senior management as people often conceal or withhold the identities or contact details of key personnel. If in doubt, address the letter generically to “The Minister/Ministry” or “The Company Secretary/Managing Director”. This minimises the risk of an organisation claiming ignorance of an order.\nA court order to pay money\nThe court may order that somebody pays money to an injured party. Your task of enforcing it will be easier if the order is clear as to how much is to be paid, by when, and whether interest may also be payable. It should also state how much the party concerned must pay at a particular time. If this is not the case, you may need further clarification from the court.\nMechanisms for getting money\nAn important question is whether the party who has to pay actually has the money required or can raise it. If not, you will not be able to enforce the order unless they borrow it from elsewhere or you can persuade or lawfully compel someone else to pay on their behalf. For example, that “someone else” could be an associated company of the company concerned, or a relative of the person concerned, or someone who owes them money.\nSome legal systems have specific mechanisms to enforce payment. For example, the claimant or a public authority may be able to\n- Attach or freeze money or property belonging to the paying party or money owed to that party by someone else\n- Seize property or money of the paying party\n- Start proceedings to declare the paying party bankrupt, or put a company in liquidation or receivership so that its assets will be managed by someone else.\nEven if the party concerned has the money, they may not pay or may try to avoid paying.\nSome parties take steps to hide assets. Tracing assets abroad, or in hidden accounts, or in accounts in other people’s names, may be possible but is highly technical, often expensive and time consuming, and will require professional assistance.\nSimilarly, it may be possible in some cases to enforce an order against associated companies, shareholders, directors, or insurers of the party concerned. Again this is a complex process.\nIt may be possible to involve the police or the authorities to take action against a party who fails to obey a court order to pay money. They might be prosecuted or fined or imprisoned.\nA court order declaring or clarifying legal rights or establishing legal facts\nSometimes an order may declare that certain conduct is lawful or unlawful, or that certain people have specific rights or obligations. For example: “it is lawful for the residents of X community to protest peacefully on the public highway in relation to the proposed mining project at Y” or “it is unlawful for the Ministry of Z to grant licences to log the forest at K without consideration of an Environmental Impact Assessment complying with Schedule C of the Logging Act 2008”.\nTurning such a court order into something of practical use may require further action or pressure at a political or administrative level to ensure that the necessary procedures or framework are in place.\nSo, for example, a declaration that a party has a right to free schooling or equal pay or clean water will not of itself provide the school or pay for water. A declaration that conduct is unlawful or discriminatory will not in itself make it stop or provide compensation for any past unlawful conduct. Sometimes an order will require further laws to be made, for example, to make a state compliant with its obligations.\nThe decision may require or authorise further action by the claimant or by others.\nThe order may in practice prohibit certain actions even if it contains no specific prohibition, for example, a declaration that X company is not entitled to fell trees over a certain size and age without a permit.\nGeneral tips on enforcement\nHere are some general tips on how to enforce a court order.\nPublicise your victory\nPublicity given to a court order may be important in at least two ways\n- It may provide pressure on a person to comply or “shame” them into complying\n- It may deter people from breaching the order even if legal enforcement is problematic\nYou may be able to persuade other individuals or organisations to help ensure compliance, either by formal process or by informal pressure. Such allies might include\n- The police or authorities, to prosecute or investigate\n- Regulatory bodies or government offices and departments\n- Civil society organisations who may provide wider publicity or help a campaign\n- The media\n- Other people who will benefit from compliance with the order – groups or communities affected by the same issue, even if not party to the court action","Wondering what is a traffic summons?\nYou received a court summons for speeding or traffic violation laws and are looking to find out what it entails.\nWhat’s a summons from a police and what’s the consequence?\nIn this article, we go over the traffic summons in detail so you know all there is to know about it.\nLet’s dive right in.\nTable of Contents\nWhat is a traffic summons\nA traffic summons is a summons issued by the police or a peace officer compelling someone to appear in court so his guilt or innocence can be determined on a charge.\nIf you received a traffic summons, your court presence is mandatory.\nIt also means that the traffic offense for which you are summoned to court is of relative importance requiring you to appear in court and defend your case in front of a judge.\nYou cannot just ignore the traffic summons, you need to make sure you appear in court.\nWhat’s a summons from a police\nSummons or citations are issued by the police when there is a traffic violation or behaviour that can constitute a crime.\nWhen someone is caught in violation of the traffic laws or criminal laws while driving, the police will write a citation against the person to appear in court.\nOften, the citation is issued directly on the scene at the moment the police noticed and apprehended you for traffic violations.\nWhat is a court summons for speeding\nA speeding summons is a traffic summons issued as a result of a speeding offense.\nIf a speeding summons has been issued to someone, they will need to appear in court and answer to the speeding charges outlined in the speeding ticket summons.\nIf the court finds the person innocent, the speeding summons will be dismissed.\nIf the court finds the person guilty of the speeding offense, then the court will define the appropriate penalty and sanctions to issue to the person charged with speeding.\nTherefore, a court summons for speeding ticket is issued only when speeding was at issue and required that you appear in court.\nTypes of traffic tickets\nDepending on your jurisdiction, you may have different types of summons for traffic violations.\nYou can get any of the following:\n- Parking infraction notice\n- Offense notice\nParking infraction notice\nA parking infraction notice is generally not that serious and you’ve probably got many of them.\nWith the parking infraction notice, the owner of a car is informed that they parked in a prohibited place.\nBy paying the parking ticket, you are essentially pleading guilty and resolving the ticket.\nYou can also contest the parking ticket and ask for a trial date to dispute the ticket.\nWhen you get an offense notice, this means that you have been in violation of traffic laws going from minor severity to major one.\nYou can receive an offense notice for speeding, missing a stop sign or driving through a red light.\nA summons is issued for serious offenses with criminal consequences.\nWhen you are issued a traffic summons, you are asked to appear in court to answer to the accusations made against you.\nYou can face sanctions such as the suspension of your driver’s license, fine and even jail.\nYou must make sure you give the summons issued to you all the importance it deserves as you can end up with a criminal record.\nWhat is contained in the traffic summons\nIf you have received a citation to appear from a police officer, it must contain some important information.\nYou should have the name of the court where you are asked to appear along with the court’s address.\nYou should also find an indication as to when is your initial court appearance date and time.\nGenerally, you should also find some information about the offense you’ve been charged with as well.\nAre you guilty when the traffic summons is issued\nIt’s important to note that you are not necessarily guilty because of the fact that a traffic summons was issued to you.\nA police officer who issued the summons to you had reasons to believe that you committed a violation of the traffic laws or criminal laws.\nThe police provided you with the summons so you have the opportunity to answer the charges laid against you before a judge.\nWhen the court will ultimately hear your case, the judge will make the final decision on your level of guilt.\nThe judge can either dismiss the traffic charges or find you guilty.\nConsequences of ignoring a court summons for speeding\nA speeding summons, just like any other type of summons, is an important legal document.\nYou are asked to present yourself to court so it can be determined if you should be held responsible for the traffic violation or not.\nRemember, some traffic violations can result in criminal penalties such as:\n- Careless driving\n- Reckless driving\n- Driving under the influence of alcohol\n- Driving under the influence of drugs\n- Crash causing material or bodily harm\n- Street racing\nIf you do not show up to court on the date required, the court may issue an arrest warrant against you.\nThe warrant for your arrest is a mandate given to the police to find you and detain you until you appear in front of a judge to answer the charges laid against you.\nThis also called a “bench summons” as the judge sitting on the bench issues the summons commanding the police to find and arrest the offender.\nNot respecting a traffic summons can be significant.\nA parking summons, or a non-moving violation, does not force you to appear in court unless you elect to contest the traffic ticket.\nIf your parking ticket has a parking summons date, you can fully resolve it by paying the ticket before the court appearance date.\nIn some jurisdictions, the parking ticket will not have a traffic fine summons and will rather ask you to either pay the ticket, and acknowledge you were guilty, or send a written request to contest it.\nIf you send a written request, then you’ll receive a summons to appear in court and contest the traffic ticket.\nYou received a summons for a traffic ticket and now you must appear in court.\nHow serious is this?\nThe summons to court for speeding, traffic law violation or even the violation of the criminal laws can be serious.\nYou should read the traffic summons carefully to understand when you are asked to appear in court and what are the charges laid against you.\nThe mere issuance of a court summons does not mean that you are guilty of the charges outlined in it.\nYou have the opportunity to go to court and contest the charges.\nIf you are not sure how deal with the summons, you should contact a lawyer experienced in the field to give you guidance.\nWe hope this article helped you better understand what is a traffic summons.\nWe would love to hear your feedback on this article.\nDrop us a comment!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:0fe198a1-8b1d-4c38-a1da-8e2d833d27b3>","<urn:uuid:b03a6401-f5c6-4796-bd79-ce9b9f07dbd8>"],"error":null}
{"question":"How does the speed of sound waves compare when traveling through air versus when measured using multiple radar frequencies for cloud ice particles?","answer":"Sound waves and radar measurements of ice particles operate at very different speeds and frequencies. Sound waves travel at 340 m/s in air at room temperature, while radar measurements of cloud ice use multiple frequencies (3 GHz, 35 GHz, and 94 GHz) that operate at much higher speeds. For ice particles, at 3 GHz (10 cm wavelength), particles scatter in the Rayleigh regime as their size is always less than the wavelength. At higher frequencies like 94 GHz, small particles scatter in the Rayleigh regime while larger particles scatter in the Mie regime. This multi-frequency approach allows better estimation of particle size and ice water content, unlike sound waves which simply propagate through their medium at a consistent speed.","context":["By: Karina McCusker\nHow do we measure cloud ice, and why do we need to?\nIce particles in clouds have complex geometries, making them more difficult to understand than droplets. As a result, ice clouds are a source of uncertainty in weather and climate simulations. To improve this, high-quality global observations are required. Microwave remote sensing instruments, such as radars and radiometers, allow observations of cloud over large areas on a continuous basis. This data is useful for improving microphysical schemes and evaluating numerical weather prediction and climate models.\nMeasurements of atmospheric cloud ice may also be assimilated into forecasts. For many years a major limitation to forecasts was that only clear-sky radiances were assimilated into numerical weather prediction models, and cloudy cells were discarded. This means a great deal of useful information was lost, thus in recent years a lot of focus was put into enabling all-sky assimilation of satellite radiances. At the ECMWF, all-sky assimilation of microwave data has been shown to have the largest relative impact on the quality of the 24-hour operational weather forecasts of all observations. Currently microwave data is assimilated using a combination of clear-sky and all-sky techniques, but by October an exclusively all-sky assimilation framework will be used for microwave observations.\nTo address the above points it is necessary to understand the relationship between the size and shape of an ice particle and its microwave scattering properties. To obtain information on particle properties, such as size, shape, and mass, direct measurements are also required. Thus, a novel dataset has been obtained as part of the PICASSO campaign, involving co-located in-situ aircraft observations with remote sensing measurements from ground-based radar. Unique tracking was used to ensure the same cloud was sampled by the aircraft instruments and the radars. Data from synchronized 3, 35, and 94 GHz radars was collected, allowing studies of how scattering by snowflakes changes with wavelength. Further details can be found in this blog post. Here we use the aircraft probes and multi-frequency radar information from the PICASSO dataset to begin to evaluate different ice particle shape models.\nWhat are the benefits of using multiple radar frequencies?\nIce particles in clouds have a wide range of sizes, from frozen cloud droplets of about 10 μm to large aggregates of crystals that can reach 4-5 cm. At 3 GHz (i.e. 10 cm wavelength), the particle size is always less than the wavelength. This means the particles scatter in the Rayleigh regime. If we were to consider the case of homogeneous spheres in the Rayleigh regime, the amount of scattering would increase as the sixth power of particle size (D6). This is a bit more subtle for ice, where generally scattering is proportional to mass2. Either way, larger particles tend to scatter much more than smaller particles. For higher frequencies such as 94 GHz (shorter wavelengths), small particles scatter in the Rayleigh regime, but larger particles (which are comparable in size to the wavelength) will scatter in the Mie regime. This means that by using a combination of measurements at two different frequencies (i.e. the dual-frequency ratio; DFR), we can get a better indication of the size of the particles, consequently improving estimations of ice water content (IWC). Triple-frequency measurements have shown potential for providing information on particle shape/structure and density (e.g Kneifel et al. (2011; 2015)), along with potential to identify regions of aggregation, melting, and riming (Dias Neto et al., 2019). Stein et al. (2015) used triple-frequency observations to evaluate particle models, and we can perform similar experiments using the PICASSO dataset.\nWhy do we need to evaluate particle models?\nRTTOV-SCATT is a fast multiple-scattering radiative transfer model designed to assimilate all-sky MW radiances in numerical weather prediction (Bauer et al., 2006; Saunders et al., 2020; Geer et al., 2021). Assimilation of observations requires accurate hydrometeor scattering models, and optimisation of particle representation is necessary in order to extend all-sky capabilities to include higher frequencies and observations from new sensors, e.g. the Ice Cloud Imager. The default in version 13 of RTTOV-SCATT is to use a range of realistic, non-spherical particles to represent frozen hydrometeors (i.e. snow, graupel, and cloud ice). These are obtained from the ARTS scattering database (Eriksson et al., 2018), as outlined in table 1 of Geer et al. (2021). Here we examine 4 particle mixtures from the ARTS scattering database – plates, columns, block columns, and ICON snow.\nExamples of experiments performed\nResults of the simulated IWC and radar reflectivity (Z) are shown in Fig. 1 for one of the aircraft runs on 13th February 2018. The in-situ measured particle-size distributions (PSDs) were used to perform the simulations. The red lines show the measurements obtained from the Nevzorov probe and the CAMRa 3 GHz radar, respectively. We find that none of the 4 particle mixtures simultaneously provide a good fit to IWC and Z. However, the block mixture tends to overestimate measurements of both quantities, and the column mixture underestimates measurements.\nFigure 1: – (a) Simulated and measured IWC. The IWC measured using the Nevzorov probe is shown in red, with the IWC simulated using the in-situ measured PSDs and the 4 particle mixtures shown by the other colours, as outlined in the figure legend. (b) Same as panel (a) but for the 3 GHz radar reflectivity.\nWe also looked at the dual-frequency ratio, and found that columns predict a larger value of DFR(3,35) than the other mixtures (Fig. 2a), while ICON snow predicts a lower value of DFR(35,94) than the other shapes (Fig. 2b). Fig. 2c shows the DFRs calculated for all the runs during this case study, plotted in triple-frequency space with DFR(35,94) on the x-axis and DFR(3,35) on the y-axis. The dots are the values simulated using the in-situ measured PSDs, and the lines are simulated using exponential PSDs. The large variation of the dots from the lines shows that even if the chosen shape model is realistic, commonly-used parameterisations of the PSD (such as assumptions of exponential and gamma distributions) may still introduce a large error to the calculations.\nWe are currently in the process of comparing the DFR simulations to measurements in order to evaluate the particle models and determine whether any of them are realistic. This work will be useful to guide microphysical schemes and assumptions that are used in weather and climate models, and in data assimilation.\nFigure 2: (a) Simulated DFR calculated at 3 and 35 GHz for one of the aircraft runs. (b) Same as panel (a) but for 35 and 94 GHz. (c) The two DFRs calculated for all the runs during this case study, plotted in triple-frequency space with DFR(35,94) on the x-axis and DFR(3,35) on the y-axis. The dots are the values simulated using the in-situ measured PSDs, and the lines show the results calculated using exponential PSDs.\nKneifel, S., M. S. Kulie, and R. Bennartz, 2011: A triple‐frequency approach to retrieve microphysical snowfall parameters, J. Geophys. Res., 116, D11203, https://doi.org/10.1029/2010JD015430.\nKneifel, S., A. von Lerber, J. Tiira, D. Moisseev, P. Kollias, and J. Leinonen, 2015: Observed relations between snowfall microphysics and triple-frequency radar measurements. J. Geophys. Res. Atmos., 120, 6034– 6055, https://doi.org/10.1002/2015JD023156.\nDias Neto, J., and Coauthors, 2019: The TRIple-frequency and Polarimetric radar Experiment for improving process observations of winter precipitation, Earth Syst. Sci. Data, 11, 845–863, https://doi.org/10.5194/essd-11-845-2019.\nStein, T. H. M., C. D. Westbrook, and J. C. Nicol, 2015: Fractal geometry of aggregate snowflakes revealed by triple-wavelength radar measurements, Geophys. Res. Lett., 42, 176–183, https://doi.org/10.1002/2014GL062170.\nBauer, P., E. Moreau, F. Chevallier, and U. O’Keeffe, 2006: Multiple-scattering microwave radiative transfer for data assimilation applications, Quarterly Journal of the Royal Meteorological Society, Wiley, 132 (617), pp.1259-1281, https://doi.org/10.1256/QJ.05.153.\nSaunders, R., J., and Coauthors, 2020: RTTOV-13 science and validation report, EUMETSAT NWP-SAF.\nGeer, A. J., and Coauthors, 2021: Bulk hydrometeor optical properties for microwave and sub-mm radiative transfer in RTTOV-SCATT v13.0, Geosci. Model Dev. Discuss. [preprint], https://doi.org/10.5194/gmd-2021-73, in review.\nEriksson, P., R. Ekelund, J. Mendrok, M. Brath, O. Lemke, and S. A. Buehler, 2018: A general database of hydrometeor single scattering properties at microwave and sub-millimetre wavelengths, Earth Sys. Sci. Data, 10, 1301–1326, https://doi.org/10.5194/essd-10-1301-2018.","Presentation on theme: \"Waves A disturbance in a medium that transfers energy and momentum.\"— Presentation transcript:\nWaves A disturbance in a medium that transfers energy and momentum\nTo produce a Wave: A vibration (disturbance) A medium – a substance to travel through.\nExamples of Waves Sound Light Water\nThere are two types of waves\nTransverse Transverse – the individual wave particles move perpendicular to the velocity of the wave. Examples: Electromagnetic waves (light waves, radio waves, microwaves, x-rays) Wave on a string\nLongitudinalLongitudinal – the individual wave particles move parallel to the velocity of the wave. Examples: Sound Waves\nParts of a wave: Amplitude Wavelength Frequency – The number of wave cycles in 1 second. Units 1/s = Hertz (Hz)\nWave Interference – The combination of two or more waves.Interference Constructive interference – Two waves combine to make a bigger wave. Destructive interference – Two waves combine to make a smaller wave.\nwavelength (m) Period (s) velocity wavelength = (velocity)(Period) wavelength (m) frequency (hz)\nThe Wave Equation v = velocity of the wave (m/s) λ = wavelength (m) f = frequency (1/s = Hz)\nExample1: A sound wave has a frequency of 256 Hz. What is the wavelength? The speed of sound is 340m/s.\nExample 2: A radio wave has a frequency of 96.9MHz. What is the wavelength? The speed of light is 3.0 x 10 8 m/s.\nStanding Waves on a String The velocity of a wave on the string depends on the mass per length of the string and the tension in the string. v = velocity of the wave (m/s) F T = Tension in the string (N) m = mass of the string (kg) L = length of the string (m)\nThe fundamental frequency 1 st Harmonic. L\nThe fundamental frequency 2 nd Harmonic. L\nThe fundamental frequency 3rd Harmonic. L\nThe fundamental frequency 4th Harmonic. L\nThe fundamental frequency 5th Harmonic. L\nSummary f n =nf 1 f n = nth harmonic n = 1, 2,3, ….. f 1 =1 st harmonic (fundamental frequency)\nConditions for interference L2L2 L1L1 P δ = path difference = L 2 – L 1 Constructive Interference δ = 0, λ, 2λ, 3λ ……. δ = nλ n = 0, 1, 2, 3, … Destructive Interference δ = λ/2, 3λ/2, 5λ/2 ……. δ = (n+ ½)λ n = 0, 1, 2, 3, ….…\nSound Waves The speed of sound in air at room temperature is 340m/s. The speed of sound increases with increasing temperature. The speed of sound in water is 1500m/s. The speed of sound in aluminum is 5100m/s.\nPhysics Human Perception frequency Intensity/Amplitude loudness Pitch\nPressure fluctuations in air due to a vibrating tuning fork. Applet\nFrequency range of the human ear.\nSound intensity and the decibel scale\nBeats Beats occur when two sound waves have slightly different frequencies interfere with one another. The number of beats per second is called the beat frequency. The beat frequency is determined by subtracting the two frequencies.\nStanding Sound Waves in a Tube The wave travels at the speed of sound (340m/s) Open ends must have an antinode Closed ends must have a node. A pressure wave is set up in the tube. A tube open at both ends acts just like the string. A tube closed at one end only has odd harmonics.\nOpen Tube Just like the string\nClosed Tube Odd Harmonics\nResonance occurs when the driving frequency matches the natural frequency, resulting in large amplitude vibrations. Here are some examples of resonance Pushing someone on a swing. The Tacoma Narrows bridge. Breaking a wine glass with a sound wave Earthquakes totally destroying some buildings and not damaging others.\nThe Doppler Effect is a change in frequency (pitch) due to the relative motion of the sound source and observer. As the sound and listener approach each other the frequency is higher. As the sound and listener move away from each other the frequency is lower. The Doppler effect also occurs with light producing the red and green shift of distant stars. Doppler radar is used to track weather systems"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:0acef1ba-fdd0-4392-b566-a261442cb4dc>","<urn:uuid:c4288816-73f9-4723-aa3c-68e395bd0d3a>"],"error":null}
{"question":"在进行长时间运动时，如何预防low blood sugar和hyponatremia these two dangerous conditions?","answer":"To prevent low blood sugar: check blood glucose frequently, carry quick-absorbing glucose sources like tablets or sports drinks, and eat a carbohydrate snack if blood sugar is 70-100 mg/dl before exercise. For exercise over 60 minutes, additional carbohydrates may be needed. To prevent hyponatremia: have a hydration plan, supplement water with electrolyte beverages for exercise lasting over 1 hour, monitor body weight before and after exercise, know individual sweat rate, and avoid excessive drinking (>1.5 L/hour). Both conditions require proper nutrition timing and individualized protocols.","context":["Regular, moderate exercise can help to prevent Type 2 diabetes and reduce or slow complications from Type 1 or Type 2 diabetes. But what about more intense physical activity with endurance sports such as marathons or triathlons? With healthy training and nutrition management to meet your personal diabetes goals, you can achieve improved body mass index and blood glucose control, and fewer hypoglycemic (low blood sugar) episodes while participating in endurance sports.\nBefore You Begin\nWhen it comes to sports, diabetes management is always the first priority. First, talk to your diabetes doctor about an insulin pump, continuous glucose monitor (CGM) and pre-training medical testing. Determine your safe blood glucose range for training and competing. Once you've gotten the go-ahead from your doctor, consult a registered dietitian nutritionist specializing in sports and diabetes care.\nWhen You Start\nAvoiding hypoglycemia (low blood sugar) is important before, during and after endurance training. If blood sugar is 70 to 100 mg/dl before exercise, then a snack that includes 15 grams of carbohydrate is recommended. For exercise that lasts longer than 60 minutes, additional carbohydrate may be needed to keep blood sugar within a safe range. When starting an endurance sport, follow these five tips:\n- Check your blood sugar frequently, and stay in the blood glucose range that you and your physician decide upon.\n- Always carry a quickly absorbable form of glucose — glucose tablets, sports drinks, gels or energy bars — when training.\n- Train with a partner until you are skilled at avoiding hypoglycemia.\n- Wear a medical alert ID bracelet, or any medical tag that helps to alert paramedics or emergency responders of your diabetes and any additional important medical condition that may require immediate or special attention.\n- Eat and drink before, during and after exercising. Hyperglycemia is worse with dehydration, and high blood sugar levels can cause the body to lose additional water. Urine color should be pale yellow throughout the day.\nEat Right for Optimal Blood Glucose Control When Training\nPlan meals, snacks and beverages to meet your blood glucose targets, and make adjustments depending on how your blood sugar is responding to your training. Consistent carbohydrate intake throughout the day is recommended and as well as including well-balanced meals that incorporate quality sources of carbohydrate, lean protein and healthy fat. Adjustments to medications, including doses of insulin, may also be required, but this should be done under the supervision of your physician.\nWithin 30 minutes after exercising, a carbohydrate snack can help prevent hypoglycemia, although low blood sugar can occur even hours after exercising. Eat a regular meal within two hours of exercising and continue to check your blood sugar regularly.\nFueling strategies for diabetes and endurance sports are highly individualized, so an RDN that specializes in sports nutrition can help with the development of an individualized meal plan.\nReviewed August 2016 Christine Gerbstadt, MD, RD, is a former spokesperson for the Academy of Nutrition and Dietetics, author and board certified physician practicing at Walter Reed National Military Medical Center in Bethesda, MD.","Hyponatremia is a medical condition termed for a low concentration of sodium in the blood (serum). By definition, hyponatremia occurs when serum sodium levels in the plasma fall below <135mEq/L. This has been shown to occur in up to 30% of ultra-endurance participants. Hyponatremia is mainly caused by overhydration, but can also be caused by intake of hypotonic fluid in excess of sweat and urine output, excessive sodium losses, or other hormonal dysfunctions that affect the maintenance of sodium stores in the body. The table below shows the risk factors associated with hyponatremia.\nHow do you prevent hyponatremia?\n- Have a hydration plan in place\n- Supplement water with electrolyte beverages, especially if exercise is lasting longer than 1 hour\n- Universal guidelines are not realistic due to the following factors\n- Variation in individual sweat rate\n- Variation in individual sweat sodium concentration\n- Environmental conditions\n- Record body weight before and after exercise to monitor fluid consumption\n- Know the sweat rate to determine fluid consumption during exercise\n- This also helps establish individual hydration plans\n- Know the signs and symptoms of hyponatremia\n- Have an emergency plan in place for dealing with hyponatremia\n- Monitor the duration and intensity of exercise for determining risk of hyponatremia\n- Educate athletes of risks from fluid overload and encourage moderate hydration.\n- Establish individualized hydration protocol based on personal sweat rate and sports dynamic.\n- Consume adequate dietary sodium.\n- Allow 8-14 days of training in the heat for acclimatization.\n- Identify pre-exercise hyponatremia by recording body weight each day\nWhat puts an individual at risk for hyponatremia?\n|Exercise duration greater than 4 hours or slow pace|\n|Low body weight|\n|Excessive drinking (<1.5 L/hour) during the event|\n|Abundant availability of drinking fluids at event|\n|Nonsteroidal anti-inflammatory drugs|\n|Other drugs associated with SIADH (SSRI’s)|\n|Extreme hot or cold environment|\nLook for these symptoms in athletes when hyponatremia is suspected:\n- Signs and Symptoms vary depending on severity and are related to cerebral edema caused by the osmotic flow of fluid into the brain cells\n- Patients that are asymptomatic or mildly symptomatic can present with any of the following:\n- Weakness, dizziness, headache, nausea, and/or vomiting and the resulting serum sodium levels range from 129-134mEq/L\n- Patients with more severe hyponatremia can present with any of the following:\n- Serum sodium levels less than 129mEq/L, presents with signs and symptoms of seizures, coma and death\nHow do you know if this is hyponatremia?\n- Indication of hyponatremia based on onset of symptoms\n- Type, duration, and intensity of exercise\n- Amount of fluid consumed\n- Post exercise body weight is greater than pre exercise body weight\n- Measurement of blood sodium levels\n- A measure <130mEq/L would indicate moderate-severe hyponatremia and coincide with observation of symptoms\n- Measurement of vitals\nWhat else could this be?\n- Exertional Heat Stroke\n- Heat Exhaustion\n- Heat Cramps\n- Cardiac Condition\n- Exertional Sickling\n- Respiratory Condition\nHow do you treat an individual with hyponatremia?\n- Treatment varies depending on severity of hyponatremia\n- DO NOT provide normal saline solution or fluids\n- Asymptomatic or mildly symptomatic\n- Treated with fluid restriction and observed until either serum sodium levels return to within normal limits or there is a resolution of symptoms and spontaneous diuresis\n- Consume oral hypertonic saline (e.g. bouillon) or salty foods such as potato chips, pickles, jerky\n- Hypertonic saline IV should be considered if a blood sodium level can be measured\n- Severe Hyponatremia\n- 3% hypertonic saline should be administered immediately due to the risk of cerebral edema that can ensue if treatment is delayed\n- It is also recommended that patients presenting with hyponatremia receive supplemental oxygen in case cerebral edema leads to hypoxia· The following flow chart represents when an athlete should be transported to the nearest hospital\nWhen can the individual return to activity?\n- Athlete will need to follow up with his/her primary care physician\n- Blood sodium levels will need to measure within normal limits (>135mEq/L)\n- Return to full activity should follow a graded exercise protocol similar what would be done during a period of exercise/heat acclimatization\n- Athlete will need to be educated on proper hydration before, during and post exercise to avoid the risk of suffering from hyponatremia again.\nRecommended Equipment List\n- Hypertonic saline\n- IV equipment\n- Portable blood Na+ analyzer kit (e.g. i-stat)\n- Salty foods (e.g. bouillon cubes, pretzels, canned soup, and potato chips, pickles)\n- Rectal thermometer (used to rule out exertional heat stroke)\n- Blood pressure cuff\n- Almond CS, Shin AY, Fortescue EB, et al. Hyponatremia among runners in the boston marathon. N Engl J Med. 2005;352(15):1550-1556.\n- Armstrong LE, McDermott BP. Exertional hyponatremia. In: Casa DJ, eds. Preventing Sudden Death in Sport and Physical Activity. Sudbury, MA: Jones & Bartlett Learning. 2012:185-199.\n- Binkley HM, Beckett J, Casa DJ, Kleiner DM, Plummer PE. National Athletic Trainers’ Association position statement: exertional heat illnesses. J Athl Train. 2002;37(3):329-343.\n- Casa DJ, Clarkson PM, Roberts WO. American College of Sports Medicine roundtable on hydration and physical activity: consensus statements. Curr Sports Med Rep. 2005;4:115-127.\n- Casa DJ, Armstrong LE, Hillman SK, Montain SJ, Reiff RV, Rich B, Roberts WO, Stone JA. National Athletic Trainers’ Association position statement: fluid replacement for athletes. J Athl Train. 2000;35(2):212-224.\n- Convertino VA, Armstrong LE, Coyle EF, et al. American college of sports medicine position stand. exercise and fluid replacement. Med Sci Sports Exerc. 1996;28(1):i-vii.\n- Hew-Butler T, Ayus JC, Kipps C, Maughan RJ, Mettler S, Meeuwisse WH, Page AJ, Peid SA, Rehrer NJ, Roberts WO, Rogers IR, Rosner MH, Siegel AJ, Speedy DB, Stuempfle KJ, Verbalis JG, Weschler LB, Wharam PM. Statement of the second international exercise-associated hyponatremia consensus development conference, New Zealand. Clin J Sport Med. 2008;18(2):111-121.\n- Montain SJ, Sawka MN, Wenger CB. Hyponatremia associated with exercise: risk factors and pathogenesis. Exer Sport Sci Rev. 2001;29(3):113-117.\n- Noakes TD, Sharwood K, Speedy D, et al. Three independent biological mechanisms cause exercise-associated hyponatremia: Evidence from 2,135 weighed competitive athletic performances. Proc Natl Acad Sci U S A. 2005;102(51):18550-18555.\n- Rosner MH. Exercise-associated hyponatremia. Semin Nephrol. 2009;29(3):271-281.\n- Rosner MH, Kirven J. Exercise-associated hyponatremia. Clin J Am Soc Nephrol. 2007;2(1):151-161.\n- Siegel AJ, Verbalis JG, Clement S, et al. Hyponatremia in marathon runners due to inappropriate arginine vasopressin secretion. Am J Med. 2007;120(5):461.e11-461.e17.\n- Speedy DB, Noakes TD, Rogers IR, et al. Hyponatremia in ultradistance triathletes. Med Sci Sports Exerc. 1999;31(6):809-815.\n- Toy BJ. The incidence of hyponatremia in prolonged exercise activity. J Athl Train. 1992;27(2):116-118."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:e9b74b5d-03fd-47e2-bef9-446675e2894e>","<urn:uuid:e490651c-5647-48eb-833f-7930c913fdbd>"],"error":null}
{"question":"As someone planning major dental work: What's the difference in typical recovery time between a bone grafting procedure and wisdom teeth removal?","answer":"Bone grafting procedures typically require 6-9 months to heal before dental implants can be placed, while wisdom teeth removal has a much shorter recovery period of approximately three days for initial recovery, though full healing may take several weeks or months. After bone grafting, bed rest is recommended for one day with limited physical activity for one week, while wisdom teeth removal patients can typically resume normal activities sooner but need to follow specific care instructions like using ice packs and avoiding straws or smoking.","context":["Over a period of time, the jawbone associated with missing teeth atrophies or is reabsorbed. This often leaves a condition in which there is poor quality and quantity of bone suitable for placement of dental implants. In these situations, most patients are not candidates for placement of dental implants.\nWhat is Bone Grafting?\nWe now have the ability to grow bone where it is needed. If the bone under your gum is not tall enough, not wide enough or both, you will need a procedure to add bone to your jaw before implants can be placed. This not only gives us the opportunity to place implants of proper length and width but, it also gives us a chance to restore functionality and esthetic appearance. This procedure is known as bone grafting and there are different types.\nWhat are the different types of bone grafting treatment?\nBone augmentation is a term that describes a variety of procedures used to “build” bone so that dental implants can be placed. These procedures typically involve grafting (adding) bone or bone-like materials to the jaw. The graft can be your own bone or be processed bone (off the shelf) obtained from a cadaver. After grafting, you have to wait several months for the grafted material to fuse with the existing bone.\nSinus lift procedure: This procedure involves elevating the sinus membrane and placing the bone graft onto the sinus floor, allowing implants to be placed in the back part of the upper jaw.\nRidge-augmentation: In severe cases, the ridge has been reabsorbed and a bone graft is placed to increase the ridge height and/or width.\nNerve- repositioning: The inferior alveolar nerve, which gives feeling to the lower lip and chin, may need to be moved in order to make room for placement of dental implants to the lower jaw.\nThese procedures may be performed separately or together, depending upon the individual’s condition. There are several areas of the body which are suitable for attaining bone grafts. In the maxillofacial region, bone grafts can be taken from inside the mouth, in the area of the chin or third molar region or in the upper jaw behind the last tooth. In more extensive situations, a greater quantity of bone can be attained from the hip or the outer aspect of the tibia at the knee.\nThese surgeries are performed in as in-office procedure under local anesthesia or in a surgical suite under IV sedation or general anesthesia. After discharge, bed rest is recommended for one day and limited physical activity for one week.\nWhat is the healing process?\nEveryone heals differently after a bone augmentation procedure, you will be given antibiotics, pain medication and an antibacterial mouthwash. You will be asked to avoid certain foods and will be told how to avoid putting pressure on the area while it heals. If you wear a denture, you may not be able to wear it for a month or longer while the area heals. If you have natural teeth near the bone graft, your dentist may make a temporary removable bridge.\nThe bone graft will take about six to nine months to heal before dental implants can be placed. At that time, the titanium screws used to anchor the bone block in place will be removed before the implant is placed.","Wisdom Teeth Extraction\nWisdom Teeth Extraction Basics\nA Full Bony Wisdom Tooth Extraction\nImpacted Wisdom Tooth & Cyst Formation\nThe majority of people have four third molars—or wisdom teeth — which usually appear between the ages of 17 and 24. Extractions of wisdom teeth are recommended when the jaw becomes crowded and other teeth are affected, the wisdom tooth becomes impacted or misaligned, or other serious issues develop such as infection or a cyst.\nChildren 18 years and younger are eligible for a screening Panorex (x-ray) and wisdom teeth that should be removed can be with or without IV sedation as a covered expense under Medicaid.\nWisdom Teeth Removal Procedure\nWisdom teeth are easier to remove when the patient is younger since their roots are not completely formed, the surrounding bone is softer, and there is less chance of damaging nearby nerves or other structures. Removal of wisdom teeth at a later age becomes more complicated as the roots have fully developed (may involve the nerve), and the jawbone is denser.\nA local anesthetic is used to numb the area surrounding the wisdom tooth. In addition, intravenous sedation may be used. It is recommended that patients do not eat or drink six hours before surgery.\nAfter surgery, cotton gauze is placed over the wound to stop bleeding. Dissolvable stitches may be used if necessary.\nWisdom Teeth Removal Recovery\nRecovery from wisdom teeth removal lasts approximately three days, although full healing may take several weeks or months. Acetaminophen or ibuprofen is recommended for pain. In some circumstances, more potent painkillers may be prescribed by your oral surgeon.\nTo speed up the healing process from wisdom teeth removal, bite gently on the cotton gauze and change the pads as necessary. If you still have bleeding after 24 hours, contact your oral surgeon. Keep your head propped on a pillow to decrease bleeding. Your mouth will be numb from the anesthesia, so be careful not to bite your lip, tongue, or cheek. Use an ice pack on the side of your face for periods of 15 minutes on and 15 minutes off for the first 24 hours after extraction. During the next two days use a warm washcloth on the side of your face in case of swelling. Rinse your mouth with warm saltwater. Do not use mouthwash.\nDuring recovery your diet should consist of liquids and soft foods such as pudding or gelatin. Gradually add solid foods as healing progresses. Avoid hot liquids and alcohol for the first 24 hours after wisdom teeth removal surgery.\nDo not use a straw or smoke. Sucking can loosen clots and increase bleeding. Smoking increases the risk of infection by germs and other cigarette contaminants. Avoid touching the area with your tongue or fingers, and be gentle when brushing your teeth and tongue. If necessary, stitches will be removed within a week.\nContact your oral surgeon if you experience:\n• Prolonged bleeding (more than 24 hours)\n• Dry socket (throbbing pain 3-5 days after surgery)\n• Difficulty opening your jaw (trismus)\n• Numbness (paresthesia) in the mouth or face\nPeople with the following health issues may need to take antibiotics before and after surgery:\n• Heart defects\n• Liver disease\n• Weakened immune system\n• Artificial joints\n• History of endocarditis"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:7ef41482-eb65-4cd6-86eb-d6c26f860b45>","<urn:uuid:a2a1d9ee-198a-4c44-b0da-f8ca11b9922f>"],"error":null}
{"question":"I'm interested in text preservation - what's the main contrast between how medieval scribes handled Paulus's epitome of Festus versus how they typically created palimpsests?","answer":"The handling methods were notably different. With Paulus's epitome of Festus, medieval scribes actively chose to copy and preserve it, to the point where it became so popular that it replaced Festus's original work in every library. In contrast, when creating palimpsests, medieval scribes deliberately erased existing texts by scraping or washing the parchment to reuse it for new content. This erasure was often done for economic reasons due to the high cost of parchment, though sometimes also for cultural reasons such as overlaying pagan texts with religious content. The process of creating palimpsests often resulted in the complete loss of the original text, though in some cases the underlying writing remained faintly visible and could later be recovered.","context":["An early editor, Antonio Agustin, in his preface to his edition of 1559, describes the transmission as follows:\nIn these twenty books, which he entitled de verborum significatione, or priscorum verborum cum exemplis, Sextus Pompeius Festus abridged the books of Verrius Flaccus on the same subject. For he omitted the words which were, in Verrius’ own words, ‘too old, and dead and buried and were of no use and authority’. He dealt with the same words [that Verrius had discussed] more clearly and more briefly, setting out the original words in a smaller space. He also provided a critical treatment of examples found in other sources. He often corrected Verrius’ errors, and he always explained most learnedly why he did so.\nNow this book had the misfortune to suffer harm of several kinds very long ago. For we could not find out either who this Festus was, or when he wrote this work. Only one or two references to it are to be found here and there in Charisius and Macrobius.\nWhile the whole book was still extant in the time of Charlemagne, one Paulus thought it would be useful if he made a sort of epitome of the parts he liked best. Ignorant men liked his book so much that it took Festus’ place in every library.\nOne codex survived the slaughter. But that was like a soldier whose comrades have been defeated and massacred, and who creeps along at random with his legs broken, his nose mutilated, one eye gouged out, and one arm broken. This book supposedly came from Illyria. According to Pio and Poliziano, Pomponio Leto had some pages of it; Manilius Rallus had the greater part. Angelo Poliziano received the book from them, went over it, and copied it, and he tried to use it in his Miscellanea to emend a verse of Catullus. Using this same copy by Poliziano, Pier Vettori has begun, with his customary learning, to emend the vulgate text of Festus at various points in his Variae lectiones.\nThe remains of the codex passed to Aldo Manuzio, who tried to combine them with the epitome of Paulus, thus making one body from two sets of parts. But so much was omitted [or] changed in publication that it was still necessary for other critics to intervene. Achille Maffei, the brother of Cardinal Bernardino, has another copy, similarly confIated from both texts; it is fuller than the Aldine. Thus there have been three recensions of the same text, all imperfect. There is the old MS of half of Festus; of this, nothing remains before the letter M, and from that letter to the end barely half of what there used to be. The second text is Paulus’s epitome. As we show in this edition, even the most ignorant can see from a comparison of the texts how carelessly that was put together. The third text is that conflated from the other two, like those of Aldo and Maffei, and our own.\nStirring stuff! Anthony Grafton, who translated the Latin  rightly remarks, “by no one has [the story] ever been told in livelier terms”.\nGrafton corrects the picture slightly. Various editions of the epitome by Paul the Deacon started to appear in print from 1471 onwards. The solitary codex to survive the Middle Ages is Naples, Bibliotheca Nazionale IV.A.3, written in the second half of the eleventh century, probably at Rome. It originally contained sixteen gatherings, the first seven of which had already been lost by the time that it reappeared in the fifteenth century. He continues:\nThe nine that remained had also been damaged by fire, so that some leaves were missing, and on many leaves most or all of the outer column of the text was also lost. Manilius Rallus, a Greek from Sparta who became a successful Roman Catholic churchman and Neo-Latin poet, brought it to Italy at some time before 1477. He is said to have found it in Dalmatia.\nRallus lent this codex to Pomponio Leto, who found it most helpful for his pioneering research into Roman antiquities. He drew on the new codex for his university lectures on Varro and other authors. Unfortunately, he treated the codex with his usual lack of scruple – he kept the eighth, tenth, and sixteenth gatherings, which have subsequently disappeared, and must be reconstructed from a number of surviving transcripts.\nThese statements about the ms. Grafton references to the edition of W. M. Lindsay (1913), p.iii-xi (the statements about Leto are from elsewhere).\nHowever Fay Glinister disagrees on one important point:\nWhen the manuscript surfaced, some time before the death of the humanist and philosopher Lorenzo Valla (1406-1457), it was already incomplete.\n For the date, see Lorenzo Valla, Le postille al”Institutio oratoria’ di Quintiliano, eds. L. Cesarini Martinelli and A. Perosa (Padua 1996). There had previously been a claim that the MS was found in Dalmatia in the 1470s, by the Greek Manilius Rhallus; it is now evident that this was a mistake.\nI presume from this hasty reference that there is evidence that Valla referred to Festus (and not to the epitome of Paul the Deacon), but without access to the Valla text, it is not clear what the argument is.\nLindsay on the other hand tells us:\nIn Illyrico codicem repertum fama erat, sed non satis certa.\nIt is supposed that the codex was found in Illyria, but this is not quite certain.\nNo reference is given for this statement. Rhallus’ claim to discovery is based on his edition of the epitome by Paul the Deacon in 1471, in which he refers in the preface:\nNuper cum legissem Pompei Festi mutilatos libros qui priscorum verborum inscribuntur, vehementer dolui quod tantum opus integrum non remansit.\nRecently when I read the mutilated books of Pompeius Festus which are inscribed priscorum verborum, I greatly regretted that such a work should not be preserved complete.\nBut whether this refers to the manuscript, or to the epitome is not clear.\nThe Illyria story seems to derive from the preface of the editio princeps, 1500, at Milan, from Io. Angelus Seinzenzeler, which contained Nonius, Festus with Paul the Deacon, and Varro. The editor was Io. Baptista Pius. In his preface he writes:\nHis quae nobis venerunt ex codice pervetusto et ob hoc fidelissimo, qui ex Illyrico Pomponio Laeto fuerat oblatus, …\nThese things, which came to us from a very old and therefore very reliable codex, which was brought from Illyria by Pomponio Leto, …\nThere are no other references to a find in Illyria in Lindsay. It would be good to clarify precisely what is, and is not, known about the circumstances of the rediscovery.\n-  Anthony Grafton, Joseph Scaliger: A study in the history of classical scholarship, Clarendon, 1983, p.134. ↩","|This article needs additional citations for verification. (August 2014) (Learn how and when to remove this template message)|\nA palimpsest (//) is a manuscript page, either from a scroll or a book, from which the text has been scraped or washed off so that the page can be reused for another document. Pergamene (now known as Parchment) was made of baby lamb or kid skin (best made in ancient Pergamos) and was expensive and not readily available, so, in the interest of economy a pergamene often was re-used by scraping the previous writing. In colloquial usage, the term palimpsest is also used in architecture, archaeology, and geomorphology, to denote an object made or worked upon for one purpose and later reused for another, for example a monumental brass the reverse blank side of which has been re-engraved.\nThe word \"palimpsest\" derives from the Latin palimpsestus, which derives from the Ancient Greek παλίμψηστος (palímpsēstos, \"again scraped\"),a compound word that literary means \"scraped clean and ready to be used again\". The Ancient Greeks used wax-coated tablets, like scratch-pads, to write on with a stylus, and to erase the writing by smoothing the wax surface and write again; this practice was adopted by Ancient Romans, who wrote (literally scratched on letters) on wax-coated tablets, which were reuseable; Cicero's use of the term \"palimpsest\" confirms such a practice.\nBecause parchment prepared from animal hides is far more durable than paper or papyrus, most palimpsests known to modern scholars are parchment, which rose in popularity in Western Europe after the 6th century. Where papyrus was in common use, reuse of writing media was less common because papyrus was cheaper and more expendable than costly parchment. Some papyrus palimpsests do survive, and Romans referred to this custom of washing papyrus.\nThe writing was washed from parchment or vellum using milk and oat bran. With the passing of time, the faint remains of the former writing would reappear enough so that scholars can discern the text (called the scriptio inferior, the \"underwriting\") and decipher it. In the later Middle Ages the surface of the vellum was usually scraped away with powdered pumice, irretrievably losing the writing, hence the most valuable palimpsests are those that were overwritten in the early Middle Ages.\nMedieval codices are constructed in \"gathers\" which are folded (compare \"folio\", \"leaf, page\" ablative case of Latin folium), then stacked together like a newspaper and sewn together at the fold. Prepared parchment sheets retained their original central fold, so each was ordinarily cut in half, making a quarto volume of the original folio, with the overwritten text running perpendicular to the effaced text.\nFaint legible remains were read by eye before 20th-century techniques helped make lost texts readable. To read palimpsests, scholars of the 19th century used chemical means that were sometimes very destructive, using tincture of gall or, later, ammonium bisulfate. Modern methods of reading palimpsests using ultraviolet light and photography are less damaging.\nInnovative digitized images aid scholars in deciphering unreadable palimpsests. Superexposed photographs exposed in various light spectra, a technique called \"multispectral filming\", can increase the contrast of faded ink on parchment that is too indistinct to be read by eye in normal light. For example, multispectral imaging undertaken by researchers at the Rochester Institute of Technology and Johns Hopkins University recovered much of the undertext (estimated to be more than 80%) from the Archimedes Palimpsest. At the Walters Art Museum where the palimpsest is now conserved, the project has focused on experimental techniques to retrieve the remaining text, some of which was obscured by overpainted icons. One of the most successful techniques for reading through the paint proved to be X-ray fluorescence imaging, through which the iron in the ink is revealed. A team of imaging scientists and scholars from the USA and Europe is currently using spectral imaging techniques developed for imaging the Archimedes Palimpsest to study more than one hundred palimpsests in the library of Saint Catherine's Monastery in the Sinai Peninsula in Egypt.\nA number of ancient works have survived only as palimpsests. Vellum manuscripts were over-written on purpose mostly due to the dearth or cost of the material. In the case of Greek manuscripts, the consumption of old codices for the sake of the material was so great that a synodal decree of the year 691 forbade the destruction of manuscripts of the Scriptures or the church fathers, except for imperfect or injured volumes. Such a decree put added pressure on retrieving the vellum on which secular manuscripts were written. The decline of the vellum trade with the introduction of paper exacerbated the scarcity, increasing pressure to reuse material.\nCultural considerations also motivated the creation of palimpsests. The demand for new texts might outstrip the availability of parchment in some centers, yet the existence of cleaned parchment that was never overwritten suggests that there was also a spiritual motivation, to sanctify pagan text by overlaying it with the word of God, somewhat as pagan sites were overlaid with Christian churches to hallow pagan ground. Or the pagan texts may have merely appeared irrelevant.\nTexts most susceptible to being overwritten included obsolete legal and liturgical ones, sometimes of intense interest to the historian. Early Latin translations of Scripture were rendered obsolete by Jerome's Vulgate. Texts might be in foreign languages or written in unfamiliar scripts that had become illegible over time. The codices themselves might be already damaged or incomplete. Heretical texts were dangerous to harbor – there were compelling political and religious reasons to destroy texts viewed as heresy, and to reuse the media was less wasteful than simply to burn the books.\nVast destruction of the broad quartos of the early centuries took place in the period which followed the fall of the Western Roman Empire, but palimpsests were also created as new texts were required during the Carolingian Renaissance. The most valuable Latin palimpsests are found in the codices which were remade from the early large folios in the 7th to the 9th centuries. It has been noticed that no entire work is generally found in any instance in the original text of a palimpsest, but that portions of many works have been taken to make up a single volume. An exception is the Archimedes palimpsest (see below). On the whole, Early Medieval scribes were thus not indiscriminate in supplying themselves with material from any old volumes that happened to be at hand.\n- The best-known palimpsest in the legal world was discovered in 1816 by Niebuhr and Savigny in the library of Verona cathedral. Underneath letters by St. Jerome and Gennadius was the almost complete text of the Institutes of Gaius, probably the first student's textbook on Roman law.\n- The Codex Ephraemi Rescriptus, Bibliothèque Nationale de France, Paris: portions of the Old and New Testaments in Greek, attributed to the 5th century, are covered with works of Ephraem the Syrian in a hand of the 12th century.\n- The Sana'a palimpsest is one of the oldest Qur'anic manuscripts in existence. Carbon dating indicates that the undertext (the scriptio inferior) was written probably within 15 years before the death of the Islamic prophet Muhammad. The undertext differs from the standard Qur'anic text and is therefore the most important documentary evidence for the existence of variant Qur'anic readings.\n- Among the Syriac manuscripts obtained from the Nitrian desert in Egypt, British Museum, London: important Greek texts, Add. Ms. 17212 with Syriac translation of St. Chrysostom's Homilies, of the 9th/10th century, covers a Latin grammatical treatise from the 6th century.\n- Codex Nitriensis, a volume containing a work of Severus of Antioch of the beginning of the 9th century, is written on palimpsest leaves taken from 6th-century manuscripts of the Iliad and the Gospel of Luke, both of the 6th century, and the Euclid's Elements of the 7th or 8th century, British Museum.\n- A double palimpsest, in which a text of St. John Chrysostom, in Syriac, of the 9th or 10th century, covers a Latin grammatical treatise in a cursive hand of the 6th century, which in its turn covers the Latin annals of the historian Granius Licinianus, of the 5th century, British Museum.\n- The only known hyper-palimpsest: the Novgorod Codex, where potentially hundreds of texts have left their traces on the wooden back wall of a wax tablet.\n- The Ambrosian Plautus, in rustic capitals, of the 4th or 5th century, re-written with portions of the Bible in the 9th century, Ambrosian Library.\n- Cicero, De republica in uncials, of the 4th century, the sole surviving copy, covered by St. Augustine on the Psalms, of the 7th century, Vatican Library.\n- Seneca, On the Maintenance of Friendship, the sole surviving fragment, overwritten by a late-6th century Old Testament.\n- The Codex Theodosianus of Turin, of the 5th or 6th century.\n- The Fasti Consulares of Verona, of 486.\n- The Arian fragment of the Vatican, of the 5th century.\n- The letters of Cornelius Fronto, overwritten by the Acts of the Council of Chalcedon.\n- The Archimedes Palimpsest, a work of the great Syracusan mathematician copied onto parchment in the 10th century and overwritten by a liturgical text in the 12th century.\n- The Sinaitic Palimpsest, the oldest Syriac copy of the gospels, from the 4th century.\n- The unique copy of a Greek grammatical text composed by Herodian for the emperor Marcus Aurelius in the 2nd century, preserved in the Österreichische Nationalbibliothek, Vienna.\n- Codex Zacynthius – Greek palimpsest fragments of the gospel of Saint Luke, obtained in the island of Zante, by General Colin Macaulay, deciphered, transcribed and edited by Tregelles.\n- The Codex Dublinensis (Codex Z) of St. Matthew's Gospel, at Trinity College, Dublin, also deciphered by Tregelles.\n- The Codex Guelferbytanus 64 Weissenburgensis, with text of Origins of Isidore, partly palimpsest, with texts of earlier codices Guelferbytanus A, Guelferbytanus B, Codex Carolinus, and several other texts Greek and Latin.\nAbout sixty palimpsest manuscripts of the Greek New Testament have survived to the present day. Uncial codices include:\nPorphyrianus, Vaticanus 2061 (double palimpsest), Uncial 064, 065, 066, 067, 068 (double palimpsest), 072, 078, 079, 086, 088, 093, 094, 096, 097, 098, 0103, 0104, 0116, 0120, 0130, 0132, 0133, 0135, 0208, 0209.\n- Petroglyphs of Arpa-Uzen – rock art from the Bronze and Iron Ages later covered by Saka pictorials\n- Lyons, Martyn (2011). Books: A Living History. California: J. Paul Getty Museum. p. 215. ISBN 978-1-60606-083-4.\n- According to Suetonius, Augustus, \"though he began a tragedy with great zest, becoming dissatisfied with the style, he obliterated the whole; and his friends saying to him, What is your Ajax doing? He answered, My Ajax met with a sponge.\" (Augustus, 85). Cf. a letter of the future emperor Marcus Aurelius to his friend and teacher Fronto (ad M. Caesarem, 4.5), in which the former, dissatisfied with a piece of his own writing, facetiously exclaims that he will \"consecrate it to water (lymphis) or fire (Volcano),\" i.e. that he will rub out or burn what he has written.\n- \"In the Sinai, a global team is revolutionizing the preservation of ancient manuscripts\". Washington POST Magazine. September 8, 2012. Retrieved 2012-09-07.\n- The most accessible overviews of the transmission of texts through the cultural bottleneck are Leighton D. Reynolds (editor), in Texts and Transmission: A Survey of the Latin Classics, where the texts that survived, fortuitously, only in palimpsest may be enumerated, and in his general introduction to textual transmission, Scribes and Scholars: A Guide to the Transmission of Greek and Latin Literature (with N.G. Wilson).\n- The Institutes of Gaius, ed W.M. Gordon and O.F. Robinson, 1988\n- Sadeghi, Behnam; Goudarzi, Mohsen (March 2012). \"Ṣan'ā' 1 and the Origins of the Qur'ān\". Der Islam. Retrieved 2012-03-26.\n|Wikisource has the text of the 1911 Encyclopædia Britannica article Palimpsest.|\n- OPIB Virtual Renaissance Network activities in digitizing European palimpsests\n- Brief note on economic and cultural considerations in production of palimpsests\n- PBS NOVA: \"The Archimedes Palimpsest\" Click on \"What is a Palimpsest?\"\n- Rinascimento virtuale a project for the census, description, study and digital reproduction of Greek palimpsests\n- Ángel Escobar, El palimpsesto grecolatino como fenómeno librario y textual, Zaragoza 2006"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:30236630-a2a2-42b2-ab87-068d70efdf05>","<urn:uuid:9ce8a3f8-409a-4196-ab49-cd6f73315114>"],"error":null}
{"question":"What action did UEFA take regarding Crimean football clubs in 2014?","answer":"UEFA rejected the move of Crimean clubs from Ukrainian to Russian leagues following Russia's occupation of the territory. UEFA declared Crimea would be considered a 'special zone for football purposes' until the conflict has been resolved.","context":["By James M. Dorsey\nHuman Rights Watch (HRW) has accused world soccer body FIFA of allowing FIFA-sanctioned matches to be played on occupied land in the West Bank in violation of FIFA rules and has demanded that the group ensure that future games be staged within the borders of Israel prior to the 1967 Middle East war.\nThe HRW allegations bring to the forefront longstanding similar assertions by the Palestine Football Association (PFA) that Israel is illegally allowing teams from Israeli settlements on occupied West Bank land to play in Israeli leagues. Palestinian efforts to get Israel sanctioned faded into the background after the Palestine Football Association (PFA) last year failed to muster sufficient votes to suspend the Israel Football Association’s (IFA) FIFA membership.\nHRW released its report in advance of a FIFA meeting scheduled for October in which the group is expected to discuss barring Israeli soccer clubs from playing in the West Bank. The Israel Football Association has complained that Tokyo Sexwale, the head of a FIFA committee established to deal with Israeli-Palestinian soccer issues, would be presenting his report without giving the IFA an opportunity to review it.\nHRW’s demand that Israeli West Bank teams play in Israel proper potentially muddles issues involving the legitimacy of the settlements and the occupation. By demanding that West Bank settlement teams play on pitches in pre-1967 Israeli territory, HRW effectively accepts Israeli settlement policy.\nThe demand further leaves Israeli military policy that restricts Palestinian access to Israeli settlements unchallenged. HRW may have been better served by demanding that Israeli settlement teams be barred from competition in Israeli leagues and be included in Palestinian ones. Such a demand would have clearly differentiated between Israel proper and the West Bank, put pressure on Israel’s military to reverse discriminatory policies, and put the PFA on the spot in terms of including settlement teams.\nPFA President Jibril Rajoub unsuccessfully tried to persuade FIFA at its congress in Mexico in May to ban Israel from allowing teams from Israeli settlements to play in Israeli leagues. Mr. Jibril identified five settlement teams competing in Israel: Beitar Givat Ze’ev, Beitar Ironi Ariel, Ironi Yehuda, Beitar Ironi Ma’aleh Adumim and Hapoel Bik’at Hayarden. Sixty-six members of the European parliament this month backed the PFA demand in an open letter to FIFA.\nThe PFA and IFA’s position reflect the views of their respective governments. Palestine, supported by a majority in the international community views the West Bank as territory occupied by Israel for the past 49 years since it was conquered during the 1967 war. The IFA justifies participation of settlement teams in its leagues on the ground that the West Bank is disputed territory whose future has yet to be determined.\nThe HRW campaign against the Israeli settlement teams came as Palestine Authority President Mahmoud Abbas told the United Nations General Assembly earlier this month that he would put forward a Security Council resolution that would condemn the Israeli outposts. Without mentioning the United States by name, Mr. Abbas called on Washington not to veto the resolution.\nUS President Barak Obama reportedly raised with Israeli Prime Minister Benyamin Netanyahu on the side lines of the General Assembly “profound US concerns about the corrosive effect that that (settlements) is having on the prospects of two states.” Settlements are expected to feature prominently in a framework for Israeli-Palestinian peace talks Mr. Obama may put forward before leaving office in January. Israel has increased the construction of settlements by 40 percent this year compared to last year.\nThe battle between Israel and Palestine in FIFA is a forerunner of likely similar confrontations in multiple international organizations as Palestine seeks to force Israel to halt its settlement activity before engaging in any new negotiations to end the Israeli-Palestinian conflict.\nFIFA was the first international organization to accept Palestine as a member without it being an internationally recognized state. Growing international unease, including in the United States, Israel’s foremost ally, has however paved the way for Palestine to build on the FIFA example and apply to a host of UN organizations, including the International Criminal Court, as a member state.\nHRW Israel and Palestine Authority director Sari Bashi argued that FIFA in the wake of adopting a human rights policy earlier this year, was not applying to Israel its rules and past practices in similar situations such as Crimea, Nagorno Karabakh and the self-declared northern Cypriot state.\nEuropean soccer body UEFA in 2014 rejected the move of Crimean clubs from Ukrainian to Russian leagues following Russia’s occupation of the territory. UEFA said Crimea would be considered a \"special zone for football purposes\" until the conflict has been resolved.\nSimilarly, FIFA has refused to recognize Northern Cyprus which unilaterally declared itself independent following a 1974 Turkish invasion or the predominantly Armenian enclave of Nagorno Karabakh that is part of Azerbaijan but occupied by Armenia. The denial of recognition meant that teams from the two territories are barred from FIFA competitions and not allowed to participate in leagues of the occupying nation.\nA report commissioned by FIFA and written by Harvard professor John Ruggie, the author of the United Nations Guiding Principles on Business and Human Rights (UNGP), which outline the human rights responsibilities of businesses, advised the soccer body to adhere to the principles.\nThe HRW report asserts on the basis of the fact that both Israel and Palestine are members of FIFA that “by allowing the IFA to hold matches inside settlements, FIFA is engaging in business activity that supports Israeli settlements, contrary to the human rights commitments it recently affirmed.”\nHRW said that “doing business in the settlements is inconsistent with these commitments.” It said that “settlement football clubs provide part-time employment and recreational services to settlers, making the settlements more sustainable, thus propping up a system that exists through serious human rights violations… The clubs provide services to Israelis but do not and cannot provide them to Palestinians, who are not allowed to enter settlements except as labourers bearing special permits. Because of this, football teams, for example, operating in the settlements, are available to Israelis only, and West Bank Palestinians may not participate, play on the teams or even attend games as spectators.”\nThe report noted that in the case of sports club Givat Ze’ev, “the IFA, and therefore FIFA as well, are holding matches on a playing field that was rendered off-limits to its Palestinian owners, two families from neighbouring Beitunia who were unable to access their land after Israel built the settlement in 1977 and prevented Palestinians from entering it. The Palestinian town of Beitunia has lost most of its agricultural land because of Israeli military orders barring access and physical barriers.”\nThe issue of soccer teams from Israeli settlements on the West Bank has been gaining traction in recent months. A petition organized by advocacy group Avaaz and signed by 150,000 people demanded that Mr. Sexwale “uphold FIFA’s own rules and provide fair recommendations to evict Israeli settlement teams from FIFA. There should be zero tolerance for the six teams that flagrantly ignore international law and operate in occupied territory. Settlement football teams legitimise the illegal occupation and condones the suffering the Palestinians face as a result,” the petition said.\nIn comments to HRW on the report, Shay Bernthal, chairman of the Ariel Football Club, a West Bank settlement team, insisted that the clubs were not discriminatory or racist. While HRW was referring to West Bank Palestinians in its assertions of discrimination, Mr. Bernthal noted that Palestinians with Israeli citizenship played for settlement teams much like they play for squads in Israel proper.\n“You did not mention that the collaboration between me and clubs from the sector [Arab citizens of Israel] is excellent. You did not mention the club’s activities against racism and violence, and you did not mention what concrete action I took to try and promote peace: a game against a Palestinian club, having two Muslim players on my adult team and more,” Mr. Bernthal said.\nIFA legal advisor Efraim Barak, responding to the report and contacts between the IFA and Ms. Bashi, employed the fiction upheld by all international and national sports associations that sports and politics are separate.\n“We make no distinction between any of the Israeli football teams that are active in the IFA and have players from different nationalities and backgrounds playing together in comradery and full cooperation, regardless of where the clubs are located. The same holds true for clubs located in places whose final status is to be determined,” Mr. Barak wrote in what is an inherently political statement that aligns the IFA with Israeli government policy.\nDr. James M. Dorsey is a senior fellow at the S. Rajaratnam School of International Studies, co-director of the University of Würzburg’s Institute for Fan Culture, and the author of The Turbulent World of Middle East Soccer blog, a recently published book with the same title, and also just published Comparative Political Transitions between Southeast Asia and the Middle East and North Africa, co-authored with Dr. Teresita Cruz-Del Rosario."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:873272e8-38b1-48dc-a555-6db1989014e0>"],"error":null}
{"question":"I'm a UK investor dealing with North American shares - where can I obtain a medallion guarantee without traveling to the US?","answer":"Title Research can now issue medallion guarantees in the UK, after getting approval to join the US Securities Transfer Agents Medallion Programme (STAMP). This allows personal representatives to obtain the medallion guarantee in the UK within days rather than weeks, eliminating the need to travel to the US or Canada.","context":["Protecting shareholders or their estate representatives from liability & fraud\nStamp of Assurance: The Story of Medallion Signature Guarantees\nSignature guarantees date back to the 19th century and have evolved over the decades to the system now used today. To mark our milestone on this journey, we take a look at the history of medallion guarantees and how they protect shareholders or their estate representatives from liability and fraud.\nMedallion signature guarantees are unique to the transfer of securities in the US and Canada. If you are administering an estate with shares listed in North America, the chances are you will need a medallion guarantee before the shares can be transferred to the beneficiaries.\nTens of thousands of UK investors bought shares in high profile companies such as Cadbury Schweppes, Tyco and IBM, all of which are now listed in the US.\nThe LawSkills Monthly Digest\nSubscribe to our comprehensive Monthly Digest for insightful feedback on Wills, Probate, Trusts, Tax and Elderly & Vulnerable client matters\nNot complicated to read | Requires no internet searching | Simply an informative pdf emailed to your inbox including practice points & tips\nSubscribe now for monthly insightful feedback on key issues.\nAll for only £98 + VAT per year.\nIt used to be the case that share transfer documents had to be stamped with a medallion guarantee in the US or Canada. However, Title Research can now issue the medallion guarantee quickly and easily in the UK, after we lobbied for and secured approval to join the US Securities Transfer Agents Medallion Programme (STAMP). So for the first time, personal representatives can take advantage of a UK-based service, allowing them to obtain the medallion guarantee in a matter of days, rather than weeks.\nThe first signature guarantees date back to the 19th century, and the current system has evolved over the last 25 years into a highly effective way of protecting against fraud. Each medallion guarantee stamp has a unique barcode, if a stamp goes missing; the missing code is registered to ensure it cannot be used fraudulently.\nStamps combine overt and covert security features to combat counterfeiting, such as invisible ink obtained by us from the STAMP programme administrator in the US. Only special machine readers held by share registrars can detect this ink. This may all sound a bit “007” and if your clients are wondering why on earth they need to have their documents stamped with invisible ink from America, the answer lies in an Act of Congress signed into law by President Bush in 1990, to deal with the growing incidents of stock fraud in the 1980s. The Penny Stock Reform Act 1990 amended the Securities and Exchange Act 1934, and granted the Securities & Exchange Commission (SEC) the authority to establish more robust standards for share transfers.\nActing on this authority, the SEC enacted Rule 17AD-15 in 1992, which enables share registrars to reject a request for transfer if the medallion guarantor is not a member of an approved medallion signature guarantee programme. One such programme is STAMP run by the Securities Transfer Association. The programme now has over 7000 financial institutions in the US and Canada, mostly banks.\nFor decades prior to the SEC rule, it was the issuer of the shares and the share registrar who were liable for improper share transfers. The SEC rule effectively shifted liability to the guarantor issuing the medallion signature guarantee, usually the banks.\nIn a milestone for the development of medallion guarantees, Title Research is pleased to join over 7000 financial institutions in the US and Canada as members of STAMP. Like all members, we’ve been vetted to ensure good financial standing and undertaken the required training on how to securely issue and safeguard medallion signature guarantees. So now there’s no need to go Stateside to get your medallion guarantee – we’ve got you covered right here.\nTo obtain a medallion signature guarantee or to enquire about our range of share transfer and asset repatriation services, call us on 0345 87 27 600\nFREE monthly newsletter\nWills | Probate | Trusts | Tax | Elderly & Vulnerable Client\n- Relevant learning and development opportunities\n- News, articles and LawSkills’ services\n- Communications which help you find appropriate training in your area"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:a9f9b4b3-22b6-4346-b79f-716993fc5453>"],"error":null}
{"question":"How are scientists studying both the age and potential extinction of ancient trees? What methods do they use to date them and what threats are they facing?","answer":"Scientists use multiple methods to study ancient trees, including radiocarbon dating and tree-ring analysis to determine their age. For very old or hollow trees, they've developed methods that combine tree-ring cross dating with radiocarbon wiggle matching techniques. These ancient trees, particularly bristlecone pines that have survived for nearly 5,000 years, are now facing extinction due to climate change. The warming temperatures are forcing them to compete with faster-growing species like limber pines in higher elevations, while those in lower elevations may succumb to less favorable conditions. Researchers have noted an alarming increase in the death rate of trees between 100 and 300 years old in various global environments.","context":["Charlotte pearson organizes samples for radiocarbon dating she says tree rings provide a window into the past credit: mari cleven for decades, radiocarbon dating has been a way for scientists to get a rough picture of when once-living stuff lived the method has been revolutionary and remains . The field of radiocarbon dating has become a it is not clear to what extent this circular process has influenced the final tree-ring calibrations of radiocarbon. Radiocarbon dating, the timber could be radiocarbon dated through mass spectrometry, and the accuracy could be assessed against the tree rings. Are sub-fossil forests in auburn and fife, washington victims of the same explosive eruption at mount rainier—testing the hypothesis with radiocarbon dating and tree-ring analysis. Radiocarbon dating of a very mass spectrometry radiocarbon dating the radiocarbon dates of three angiosperm tree with reliable dating .\nCarbon dating gets a reset the problem, says bronk ramsey, is that tree rings provide a direct record that only goes as far back as about 14,000 years. But new research shows that commonly accepted radiocarbon dating standards can the authors measured a series of carbon-14 ages in southern jordan tree . By measuring the amount of carbon-14 in the annual growth rings of trees grown in southern jordan, radiocarbon dating could be misrepresenting important details. In the process of dating the oldest trees, which are often hollow, we developed a new method that combines tree-ring cross dating and wiggle matching radiocarbon techniques on wood samples extracted from the stem and from exposed roots.\nAbstract in late 2004, grootboom, probably the largest known african baobab (adansonia digitata l), collapsed unexpectedly in northeastern namibia ten wood. Discovery of radiocarbon dating the university of chicago american chemical society “seldom has a single discovery redwood and fir trees, the age of which. Radiocarbon dating are please note radiocarbon is a journal, not a dating lab tree-ring sequences, age-depth models, etc.\n“archaeology has the ability to open unimaginable vistas of thousands, even millions, of years of past human experience” – colin renfrew when it comes to dating archaeological samples, several timescale problems arise for example, christian time counts the birth of christ as the beginning . Are the long tree-ring chronologies used to calibrate radiocarbon dates reliable an objective analysis says yes. What is radiocarbon dating very old trees such as north american bristlecone pine are ideal for constructing long and accurate radiocarbon-14 dating in action. Archaeologists are rewriting history with the help of tree rings from 900-year-old trees, wood found on ancient buildings and through analysis of the isotopes (especially radiocarbon dating) and chemistry they can find in that wood. We observe a substantive and fluctuating offset in measured radiocarbon ages between plant material growing in the southern levant versus the standard northern hemisphere radiocarbon calibration dataset derived from trees growing in central and northern europe and north america.\nRadiocarbon dating uses the naturally occurring isotope this is because radiocarbon dating gives the date when the tree ceased its intake of carbon-14—not when . Normally these trees can live for hundreds, this means the ages of the trees that patrut extrapolated from his radiocarbon dating results could be very . Radiocarbon dating gets a postmodern makeover radiocarbon dating tree rings today today, dendrochronologists all over the world follow in douglass’ footsteps, .\nNine of africa's 13 largest and oldest baobabs have suddenly collapsed and died, likely due to climate change, say scientists who study these ancient and iconic trees. Radiocarbon dating measurements produce ages in radiocarbon years, which must be converted to calendar ages by a process called calibration calibration is needed because the atmospheric 14 c / 12 c ratio, which is a key element in calculating radiocarbon ages, has not been constant historically. “combining the effects of these two trees, we see a site that was actually occupied for 245 years (from 2095 to 1850 bce) appearing - using conventional radiocarbon dating - to have been occupied for 30,600 years (from 40,000 to 9,400 bce)”.\nScientists describe an 'event of an unprecedented magnitude' among africa's oldest and largest baobab trees of the trees using radiocarbon-dating . 22 hours ago new university of arizona-led research uses tree rings to shed light on discrepancies between archeological and radiocarbon evidence in dating the ancient volcanic eruption of thera. In the same article, the authors go on to note that radiocarbon dating of these same trees show a statistical accuracy of agreement of only 267%. Radiocarbon dating and the bible is carbon-14 dating (or radiocarbon dating) always reliable and beyond question are all radioactive dating methods unreliable.","- Bristlecone pine, the world's oldest species of tree, may go extinct due to changing temperatures caused by climate change in the frigid mountains where they used to thrive.\n- Some of these trees are almost 5,000 years old, so their loss would be another devastating blow to the environment brought on by manmade climate change.\nAt 4,845 years old, Methuselah, a Great Basin bristlecone pine (Pinus longaeva) in the White Mountains of California, is one of the oldest trees on Earth. It’s so old, in fact, that it was only a seedling when the Egyptian pyramids were being constructed. Since it first broke ground, the pine tree has survived wars, cultural upheavals, extreme weather conditions, and whatever else this world threw at it. Now, after more than 48 centuries of life and history, this tree is facing its toughest challenge yet: climate change.\nAround the world, giant, old trees like Methuselah stand as a living history of this planet, but a new study suggests that climate change could lead to their extinction. “I think what’s going to happen — at least in some areas — is that we’re going to lose bristlecone,” said Brian Smithers, an ecologist at the University of California and co-author of a recent study of the iconic forest giants.\nBristlecone pine have historically thrived in the upper elevations of California’s White Mountains because they have adapted to survive the temperatures found just below the tree line. As temperatures warm and the tree line moves higher up the mountains, it’s expected that this particular species would simply begin to seek out higher elevations as well. However, those higher areas are being populated more heavily by a different species, limber pines, thanks to the help of a bird species that spreads its seeds. “Whoever can get there first wins,” Smithers said. “And it looks like limber pine is just better able to get there quicker.”\nAs these limber pine trees grow, they could use up the light and water supplies that the slower-growing bristlecone pines that do sprout in the higher elevations need to survive, eventually taking over those higher elevations completely. The bristlecone pines left in the lower elevations could then succumb to the warmer, less desirable conditions brought on by climate change.\nThe loss of our planet’s oldest tree species isn’t exclusive to the bristlecone pine. According to an article published by ThinkProgress back in 2012, leading ecologists have already “documented an alarming increase in the death rate of trees between 100 and 300 years old in many of the world’s forests, woodlands, savannahs, farming areas, and even in cities.” As a response, researchers are urging an increased focus on efforts that can help us identify the cause of this rapid loss of historic trees so that strategies can be immediately implemented to manage its effects.\nA more recent study published in Nature World News looked into how the growth rate of trees changed historically under previous climate conditions. Those researchers projected that the United States will see a 75 percent decrease in its growth rate of trees in the southwest, and as the planet gets warmer, this will push numerous forests into critical levels as early as 2050. When that happens, our forests will be unable to help protect us from the effects of climate change by offsetting carbon emissions.\nCombine this negative impact on the planet’s tree species with the ever-increasing number of other plants and animals facing extinction, and you can see that humans are far from the only species battling the devastating consequences of manmade climate change."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:590aa897-1bb4-45a0-b281-9e88ec3d2ccd>","<urn:uuid:e23a195f-eefe-404d-8f39-4c8456d45b67>"],"error":null}
{"question":"In what ways do non-compete agreements protect business interests, and what are the limitations on their enforcement for different employee categories?","answer":"Non-compete agreements protect business interests by safeguarding trade secrets, confidential information, and customer relationships. In New Hampshire, they can specifically protect goodwill, contacts developed during employment, and employee influence over customers. In Minnesota, they can protect sensitive information, trade secrets, specific investments, strategies, and workplace training tools. However, there are significant limitations. New Hampshire generally prohibits non-compete agreements for low-wage workers and physicians. Additionally, the agreements must not be injurious to public interest. In Minnesota, employers must prove they provided sufficient tools and training to employees, and must demonstrate potential irreparable damage to business interests for the agreement to be upheld.","context":["It is not unusual for a company to use a non-compete agreement to prevent their employees from taking a job with a competitor or to prevent them from sharing confidential information related to the company. There are several important points you need to know about a New Hampshire non-compete agreement, including what can be protected, what cannot be enforced, and how you can draw up a non-compete agreement that you can defend.\nIs a Non-Compete Legally Enforceable in New Hampshire?\nYes, a non-compete agreement can be legally enforced in New Hampshire. It is not unusual for companies to ask their employees to sign a non-compete agreement before they are hired. If an employee violates any of the stipulations in the agreement, then the company could pursue damages against the employee in court.\nIt is important for employees to read and understand the non-compete agreement before they sign it, and it is also critical for employers to be aware of the information they included in the non-compete agreement because it has to be legally defensible in court.\nThere are several examples of information that can be protected by a New Hampshire non-compete agreement. For example, trade secrets and confidential information relevant to the employer’s business interests can be protected by a non-compete agreement. In addition, goodwill, contacts developed during employment, and the employee’s influence over the employer’s customers can also be protected by the scope of the non-compete agreement.\nReasonable Use and Exemptions\nThere are some limitations that can be placed on a non-compete agreement in New Hampshire. For example, low-wage employees  and physicians are generally not allowed to be subjected to a non-compete agreement.\nIn addition, the non-compete agreement should not be greater than necessary to protect the legitimate business interests of the employer. Finally, the non-compete agreement cannot be injurious to the public interest or disproportionately harsh to the employee.\nA few common questions that people ask about non-compete agreements in New Hampshire include:\n- Enforceable when terminated without cause? Not decided\n- Employee non-solicitation agreement permitted? Yes\n- Customer non-solicitation agreements permitted? Yes\n- Does continuing employment equal sufficient consideration? Yes\nNon-Compete Agreement New Hampshire Limitations\nIn general, employers need to make sure they are not placing an undue burden on their employees by asking them to sign a non-compete agreement.\nLimitations on time\nIn the event of a breach of a non-compete agreement signed in New Hampshire, you have 3 years to file a lawsuit.\nNon-compete agreement New Hampshire geographical limitations\nNew Hampshire law does not specify a specific geographic limitation regarding non-compete agreements; however, reasonable restrictions must be applied to the non-compete agreement for it to be enforceable. If the geographic limitation does not make sense given the nature of the employer’s business interests, then it might be difficult to defend in court.\nNew Hampshire Non-Compete Agreement Sample\nIf you want to draw up a New Hampshire non-compete agreement, we have a New Hampshire non-compete agreement template below, which is available in PDF and Word format:","Agreements that restrict employees from participating in competitive activities and adjuration of company business during one’s time of employment and after employment have been commonplace in Minnesota. These agreements can be universal in many industries.\nWhile many Minnesota employees may view non-compete agreements as troublesome, employers view it is a practical way to protect the business they have established. Non-compete agreements can also be used to protect the remaining employees and confidential workplace information.\nIt is not a fact that a non-compete agreement cannot be enforced against an employee and block the employee from finding a new job with a company that is a competitor. If an employee agrees to sign a non-compete agreement that has been deemed valid, it could very well be enforced against the employee after the employee has agreed to accept a position with a competing business.\nOn the other hand, if the employee does not accept a position with a competitor of a former employer and the actions taken do not result in any damages to the former employer, it is not likely that the signed non-compete agreement would stand. Not every non-compete agreement will be flawless and valid; there are several factors that can result in the non-compete agreement being flawed.\nIt is important to discuss a non-compete agreement with a Minnesota employment lawyer before agreeing to sign the agreement. It will also be beneficial to an employee to discuss the signed non-compete agreement with an employment lawyer before accepting a position with a new employer if there are concerns about the new position.\nWhile non-compete agreements may not be the most favorable documents that are signed during employment, they can certainly be viewed as enforceable. How is it determined if the non-compete agreement is enforceable?\nGenerally, the court will perform a balancing act against the interest of the employer and the employee’s right to earn an income. The non-compete agreement will be viewed as valid the interest of the employer weighs more than the employee’s right to be employed an earn an income.\nHow Is An Enforceable Non-Compete Agreement Drafted?\nAcross the United States, the laws involving drafting a non-compete agreement will not be the same. A non-compete agreement that is viewed as enforceable in one state may not be enforceable in the next state over. This is why it is so important to understand the laws in Minnesota. Non-compete agreement contract forms there are found online and printed may not be acceptable for employers and employees in Minnesota.\nWhen a non-compete agreement is drafted, it involves a process of including various Minnesota laws that will ensure the contract can be viewed as enforceable. Non-compete agreements in Minnesota that are written properly will generally be enforced. However, the non-compete agreement can be voided if there are errors in the non-compete agreement.\nA Valid Non-Compete Agreement\nIn order to determine if a non-compete agreement is valid, there will be a review of the facts on a case-by-case basis. In Minnesota, the courts will determine if the employee was given consideration for the agreement. What does this mean? Consideration, specifically adequate consideration, refers to the value bargained by the two parties.\nIf a non-compete agreement is signed at the start of one’s employment journey, the promise of a position with the company could be enough to create a valid non-compete agreement. What if the non-compete agreement is signed after the start of one’s employment journey? In this case, the non-compete agreement will only be valid if the non-compete agreement included supplementary consideration, such as money.\nIs The Agreement Protecting The Interest Of The Business?\nAfter determining if the non-compete agreement is valid based on adequate consideration, the courts in Minnesota will evaluate the non-compete agreement to determine if the agreement is used to protect the interest of the business. If the non-compete agreement has been put in place to protect the business and/or the employer, the non-compete agreement can be viewed as valid.\nSome business interests that can be protected by a non-compete agreement include the prevention of discussing sensitive information or disclosing trade secrets. Non-compete agreements may also be used to protect particular investments, strategies, or training tools that are used in the workplace.\nIn order for the non-compete agreement to be valid, the employer will need to prove that the employee was equipped with enough tools and training. In every case, the court will need to determine that the employer’s business could be damaged beyond repair if the interest of the business is not protected before the non-compete agreement can be upheld.\nWhat If The Non-Compete Agreement Is Too Broad?\nIf a court determines that the non-compete agreement is too broad or more extensive than necessary to protect the interest of the business, the agreement can be voided. However, under the blue pencil doctrine, a court can take the broad restrictions and enforce the non-compete agreement to the extent that the agreement is reasonable. If a non-compete agreement states that an employee cannot work across the globe, the court may make a change to the contract and limit it to only the state of Minnesota.\nWhen discussing non-compete agreements and cases, the courts will decide will either motion in favor of an injunction or a restraining order. The courts will determine if the employee can be stopped from being involved in activities that may be viewed as competitive. In some cases, the employee will be awarded damages.\nWhat will happen if the employer makes the decision to file a lawsuit against the employee in an attempt to have the non-compete agreement enforced? If this is the case, the case could eventually go to trial. However, the majority of lawsuits will not have to go to trial because they can be settled out of court between both sides.\nIf you have a non-compete agreement that needs to be reviewed, contact Villaume & Schiek and allow us to review the agreement. We will determine if the non-compete agreement you have signed is valid or if you can use the agreement to request a waiver. If you have been told that you have violated the agreement and there is a possibility a lawsuit will be filed against you, please contact us today so we can help you understand your legal rights and your options.\nDisclaimer: The information you obtain at this site is not, nor is it intended to be, legal advice. You should consult an attorney for advice regarding your individualsituation. We invite you to contact us and welcome your calls, letters and electronic mail. Contacting us does not create an attorney-client relationship. Please do not send any confidential information to us until such time as an attorney-client relationship has been established."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:f9bb3da0-d1e8-4301-b0f1-1ca757dd7bd4>","<urn:uuid:9c6a368c-098b-4da5-a5d8-1ff84476f55c>"],"error":null}
{"question":"During World War II, what role did Juan Pujol play as a spy to help the D-Day invasion, and what modern security concerns exist regarding personal data in fitness tracking devices?","answer":"Juan Pujol was a self-made master spy who played a crucial role in the D-Day invasion by deceiving Hitler. Working with MI5, he created an elaborate network of 27 fake sub-agents and convinced the Nazis that the Allied invasion would occur at Calais rather than Normandy. His deception was so successful that Hitler held back some of his best panzer divisions even months after D-Day, believing Pujol's reports about a million-man army that would attack Calais. As for modern fitness tracking devices, they present significant security risks as their collected data isn't protected under HIPAA standards and can be vulnerable to hacking through Bluetooth connections. This data, including location information and biological metrics, can be intercepted by third parties or sold to data brokers, potentially leading to serious privacy breaches and exploitation by bad actors.","context":["07.11.12 10:00 AM ET\nThe Spy Who Tricked Hitler: The Story of Double Agent Juan Pujol and D-Day\nThe Nazis rated him their best spy in England; the Brits said he was the greatest secret agent of the war. Meet Juan Pujol, the spy who helped save the D-Day invasion—and who was entirely self-made. Stephan Talty tells his remarkable story in his new book, Agent Garbo.\nMany people dream of becoming spies. Life as James Bond or a character from an Alan Furst novel seems racy and well-paid. But when faced with actual espionage—the training, the danger, the use of other human beings for motives that can feel questionable or even sordid—they forget about it. They move on.\nJuan Pujol didn’t move on. At the beginning of World War II, this brilliant young Spaniard wanted to “start a personal war with Hitler,” and espionage was his chosen method of doing it. He was willing to risk his life—and that of his gorgeous wife, Araceli—to have a great adventure and perhaps save the world in the process.\nThere were a few problems with this plan, though. Pujol wasn’t a spy. He was an ex-chicken farmer and hotel manager who was, in the spring of 1941, stuck in a one-star dump in Madrid. He had no training in espionage. He had no contacts in the espionage world. He was living in a Fascist-controlled country that was infiltrated by thousands of Nazi agents and informers. He had about as much chance of becoming a world-class intelligence operative as you or I have of winning the gold in the Olympic steeplechase.\nOnly one year later, Pujol had transformed himself into something almost unprecedented in the long history of intelligence. He was on his way to becoming a completely self-made master spy. By that time, Pujol was a rising star in MI5’s stable of double agents. The Germans trusted him implicitly. He conducted missions that involved global assets and caused the Nazis to send fighter planes and destroyers to attack convoys that didn’t even exist. The Allies were so in awe of his powers of confabulation that they’d given him the code name “Garbo,” because he was the greatest actor they’d ever seen.\nAnd when the Allies began planning D-Day, it was Pujol who was chosen to lead the deception effort. He would be the point of the spear in convincing Hitler that the Normandy landings, the greatest invasion in human history, was in fact a fake, and that a million-man army was about to attack him along another length of French coast. Pujol was going to be lead actor in the most complex wartime deception ever conceived.\nThis simply doesn’t happen. Nobodies don’t will themselves into the game in the course of one short year. But somehow this mischievous and enigmatic man had done just that. At a price, however.\nAt the beginning of his career, Pujol had only one thing to recommend himself as a spy: raw imagination. He’d been a dreamer since childhood, which he spent “covered in bandages” after acting out wild adventures as an explorer or the Hollywood cowboy Tom Mix. His parents didn’t understand him, didn’t understand why he crashed his tricycle through plate-glass windows and nearly decapitated himself while acting out some daydream. Pujol tried to explain that his imagination “controlled” his brain, and that he was powerless to stop it.\nThat imagination cursed him throughout his early life. Pujol was so convinced that he had a great part to play in world affairs that he was a disaster at everyday life. He was a bad student and an even worse soldier who spent all of the Spanish Civil War trying not to kill anyone. His beloved father died thinking his son was a failure. His Catholic mother was in despair.\nWhen the German divisions began rolling through Poland, however, something changed in Pujol. His father had taught Pujol to fight for freedom and individual dignity, and the young man saw all that going up in smoke along the Western Front. He decided to take up arms in his own unorthodox way. And he had only one weapon to offer the Allies: the brain that had almost ruined his life.\nPujol and his new wife, Araceli, went to work. First he went to the British embassy in Madrid and offered his services. “Your services of what?” was the reply. He would try on four separate occasions to volunteer for the Allied cause and be shot down every time.\nThere was only one alternative. He would have to pretend to be a Nazi and volunteer for the German side, then turn double agent. Madrid at the time was practically controlled by the Third Reich, so this was a seriously dangerous option, but he went ahead anyway.\nPujol marched into the German embassy and soon hornswaggled an experienced Nazi spy-runner named Federico into believing his wild stories about his friends and contacts in key wartime positions. (None of these friends or contacts actually existed). To prove his bona fides to the Nazi agent, Pujol traveled to Lisbon, the WWII capitol of intrigue. In Lisbon, even the hotel chambermaids worked for one spy service or another and people sold their heirlooms and their relatives to get to the West. There were thousands of desperate men and women trying to escape Portugal. Pujol arrived and joined the sorry-looking crowds looking for a break, a way into the game.\nAfter drifting around for days, Pujol found a fellow Spaniard who was carrying a special diplomatic visa, which would allow the owner to leave on the seaplane that flew out of Lisbon for points west every day. Pujol instantly made friends with the man (Pujol was very good at that), accompanied him to the cafés and restaurants for weeks, then set his plan in motion. He set him the Spaniard with a supply of drinks and chips at one of the local casinos, then snuck into the man’s hotel room, found the visa and photographed it. He sent the Spaniard on his way then visited a series of shops in Lisbon and had the visa reproduced down to the special stamps. The forged document was something that men in Lisbon would have killed for, and it convinced Federico that Pujol was the real thing. Soon he was snapped up by the Abwehr, the German intelligence service, and sent into action.\nPretending to be heading to London, Pujol settled in Lisbon and sent a stream of intricately detailed reports on British armaments, Allied airfields, massive troop movements, and convoys headed toward the besieged island of Malta. They were utterly convincing and 100 percent fake, cribbed from propaganda films, flyers, and phone books. When British analysts later studied the messages, they refused to believe that Pujol had never set foot in England.\nThroughout his early career as a spy, Pujol was one phone call or one background check away from being executed. He lived by the slimmest of margins. “It seemed a miracle that he’d survived so long,” said his MI5 handler later on. Pujol agreed. “It was crazy. I had no idea what I was doing.”\nThe self-made spy finally convinced the Allies that he wanted to work for them and was smuggled into England. At his debriefing in London, he told his British handlers why he’d volunteered to fight the Nazis. His brother, Joaquin, had been vacationing in France when he came across a horrific scene: the Gestapo pulling out refugees from their hiding places in a French home and executing them in cold blood. The MI5 officer, a half-Jewish artist named Thomas Harris, listened to the gruesome tale and afterward declared that Pujol would make a “marvelous agent.”\nIn doing the research for Agent Garbo, my book on Pujol, I discovered what became one of the more fascinating details of his story: at the time Pujol was being debriefed, his brother Joaquin had never been to France. He’d never been out of Spain. The story was a complete fantasy, created by Pujol to make sure the Allies believed him and that he would be allowed to live out his dream.\nSoon afterward, Pujol and Harris began one of the great partnerships in espionage history: they sent the Abwehr airplane manuals baked into cakes, created an army of 27 fake sub-agents to feed the Nazis fake narratives, made battleships vanish from the Indian Ocean and pop up thousands of miles away. An MI5 advance man toured the English countryside for hotels the imaginary informants could “stay” at and pubs they could describe in their bulletins. The Germans rated Garbo their best spy in England; he was even awarded the Iron Cross, something that amused Pujol to no end.\nThe Allies were just as impressed. Churchill read his dispatches at night. J. Edgar Hoover wanted to meet him.\nWhen it came time to concoct a scheme to “cover” D-Day, Pujol was given the job of convincing Hitler that the Allies would attack Calais and not Normandy. Few believed he would succeed, but Garbo and a handful of other double agents began by creating an imaginary million-man-strong army. George S. Patton was assigned as commander. Made-up Morse code and stacks of Garbo’s eyewitness reports was bolstered by physical trickery: gargantuan fake oil depots, sham tanks, and airfields. One British soldier even imitated the British General Montgomery, to trick the Germans into believing that the sham invasion was real.\nPujol was nervous throughout the run-up to D-Day. He would walk the parks of central London, passing by the American G.I.’s and their British girlfriends, knowing that many of them would live or die by the lights of his acting skills. (He should have been nervous: a 1943 dress rehearsal for the landings had resulted in a total failure of the deception plan and the deaths of hundreds of French civilians). Araceli, his partner who’d helped him create Garbo, was lonely and jealous; he spent too much time with Harris, not enough with her. His marriage was breaking up.\nBut when the invasion came, the plot to deceive Hitler was a tremendous success. Months later, the Führer was still holding back some of his best panzer divisions, still believing that Garbo’s million-man army would appear at Calais. One message that Garbo sent on June 9th was seen by the Führer and became the key factor in keeping some of the best German divisions away from Normandy. When he met Thomas Harris, General Eisenhower told him: “Your work with Mr. Pujol most probably amounts to the equivalent of a whole army division. You have saved a lot of lives.”\nThe other reviews were equally strong. The British spy Anthony Blount called Pujol’s work “the greatest double cross operation of the war.” But the British historian Roger Fleetwood-Hesketh put it best: “It couldn’t have been done without him… It was Garbo’s message…which changed the course of the battle in Normandy.” Garbo emerged from D-Day as the greatest double-agent of the war, perhaps of all time.\nAfter his services were no longer needed—MI5 tried to “sell” him to the Soviets but the traitor Kim Philby nixed the deal—Pujol and Araceli escaped to Venezuela with their small children. His attempts to start a new life ended in failure and misery. His business plans came to nothing, and Araceli left him and took the kids back to Madrid, where she married an American entrepreneur, a former body double for Rudolf Valentino. Pujol had to start from scratch. He eventually lost contact with his children by Araceli. They wouldn’t be reunited until nearly four decades later, when Pujol reemerged for the 40th anniversary of D-Day, much to the astonishment of his sons and daughter, who thought he’d died in Angola of malaria.\nIf Pujol had followed his talents to their natural ends, he would have become one of the great scam artists. A Bernie Madoff type. But in reading his private letters in Madrid, I was able to see why this never happened. Pujol’s operatic gift for the flimflam was paired with a set of ideals that he described in his private letters (always capitalizing the first letter) as “Humanist.” It’s an old-fashioned term, not used much anymore, but Pujol believed in it single-mindedly.\nIn other words, he enjoyed the game for its own sake, but he limited his victims to the Nazis. We’re lucky, in more ways that one, that this was the case.","- Fitness trackers and apps from companies including Google, Apple, Garmin and Strava offer a convenient way to stay on top of health and wellness, monitoring body metrics like sleep quality and heart rate.\n- But even the biggest brands focused on security and reputation can be hacked or share personal data in other unintended ways with serious, sometimes devastating, consequences.\n- Data collected by a fitness app is not protected like health information under the law, making social and location settings, and login credentials, critical for a user to set properly before making these devices part of their daily life.\nFitness trackers, which help keep tabs on sleep quality, heart rate and other biological metrics, are a popular way to help Americans improve their health and well-being.\nThere are many types of trackers on the market, including those from well-known brands such as Apple, Fitbit, Garmin and Oura. While these devices are growing in popularity — and have legitimate uses — consumers don't always understand the extent to which their information could be available to or intercepted by third parties. This is especially important because people can't simply change their DNA sequencing or heart rhythms as they could a credit card or bank account number.\n\"Once the toothpaste is out of the tube, you can't get it back,\" said Steve Grobman, senior vice president and chief technology officer of computer security company McAfee.\nThe holiday season is a popular time to purchase consumer health devices. Here's what you should know about the security risks tied to fitness trackers and personal health data.\nStick to a name brand, even though they are hacked\nFitness devices can be expensive, even without taking inflation into account, but don't be tempted to skimp on security to save a few dollars. While a less-known company may offer more bells and whistles at a better price, a well-established provider that is breached is more likely to care about its reputation and do things to help consumers, said Kevin Roundy, senior technical director at cybersecurity company Gen Digital.\nTo be sure, data compromise issues, from criminal hacks to unintended sharing of sensitive user information, can — and have — hit well-known players, including Fitbit, which Google bought in 2021, and Strava. But even so, security professionals say it's better to buy from a reputable manufacturer that knows how to design secure devices and has a reputation to upkeep.\n\"A smaller company might just go bankrupt,\" Roundy said.\nFitness app data is not protected like health information\nThere can be other concerns beyond having a person's sensitive information exposed in a data breach. For example, fitness trackers generally connect to a user's phone via Bluetooth, leaving personal data susceptible to hacking.\nWhat's more, the information that fitness trackers collect isn't considered \"health information\" under the federal HIPAA standard or state laws like California's Confidentiality of Medical Information Act. This means that personally revealing data can potentially be used in ways a consumer might never expect. For instance, the personal information could be shared with or sold to third parties such as data brokers or law enforcement, said Emory Roane, policy counsel at Privacy Rights Clearinghouse, a consumer privacy, advocacy and education organization.\nSome fitness trackers may use consumers' health and wellness data to derive revenue from ads, so if that's a concern, you'll want to make sure there's a way to opt out. Review the provider's terms of service to understand the its policies before you buy the fitness tracker, Roundy said.\nDefault social, location settings may need to be changed\nA fitness tracker's default settings may not offer the most stringent security controls. To boost protection, look at what settings can be adjusted, such as those related to social networking, location and other sharable information, said Dan Demeter, security researcher at cybersecurity provider Kaspersky Lab.\nDepending on the state, consumers can also opt out of the sale or sharing of their personal information to third parties, and in some cases, these rights are being expanded, according to Roane.\nCertainly, device users should be careful about what they post publicly about their location and activities, or what they allow to become public by default. This data could be searchable online and used by bad actors. Even if they aren't acting maliciously, third parties such as insurers and employers could get access to this type of public information.\n\"Users expect their data to be their data and use it how they want it to be used,\" Roane said, but that's not necessarily the case.\n\"It's not only about present data, but also about past data,\" Demeter said. For instance, a bad actor could see all the times the person goes running — what days and hours — and where, and use it to their advantage.\nThere are also a number of digital scams where criminals can use information about your location to make an opportunity seem more plausible. They can claim things like, \"I know you lost your wallet at so and so place, which lends credibility to the scammer's story,\" Grobman said.\nLocation data can prove problematic in other ways as well. Roane offers the example of a women seeking reproductive health care in a state where abortion is illegal. A fitness tracker with geolocation services enabled could collect information that could be subpoenaed by law enforcement or be purchased by data brokers and sold to law enforcement, he said.\nUse strong password, two-factor authentication, and never share credentials\nBe sure to secure your account by using a strong password that you don't use with another account and enabling two-factor authentication for the associated app. And don't share credentials. That's never a good idea, but it can have especially devastating consequences in certain circumstances. For example, a domestic violence victim could be tracked by her abuser, assuming he had access to her account credentials, Roane said.\nAlso be sure to keep the device and the app up-to-date with security fixes.\nWhile nothing is foolproof, the goal is to be as secure as possible. \"If somebody tries to profit from our personal information, we just make their lives harder so it's not that easy to hack us,\" Demeter said."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:1233ed03-e76b-42de-996a-58a14f044a5b>","<urn:uuid:760aa365-b7f3-4b99-94d6-4a27433898e1>"],"error":null}
{"question":"What are examples of how pharmaceutical companies and design education programs approach problem-solving and innovation differently?","answer":"Pharmaceutical companies primarily use Big Data and focus groups to drive innovation, with research involving 26 pharmaceutical companies showing they identify specific processes where Big Data supports innovation in commercial environments. In contrast, design education programs take a more hands-on approach through frameworks like the Community Engaged Learning Lab at Queensland University of Technology, where students and academics work directly with community organizations on interdisciplinary action research projects. While pharmaceutical companies rely on data analysis and market research, design education emphasizes service-learning experiences and direct community engagement to explore complex problems and develop innovative solutions.","context":["Bavarian Nordic A/S, Zug, Switzerland\nCite: Pesqueira A. The impact of big data on innovation and value generation in pharmaceutical sales and marketing. J. Digit. Sci. 3(2), 19 – 36 (2021). https://doi.org/10.33847/2686-8296.3.2_3\nAbstract. Using Big Data in the pharmaceutical industry is a relatively new technology, and the benefits and applications are yet to be understood. There are some cases currently being piloted, but others have already been adopted by some pharmaceutical organizations, proving the unmet need in a field that is still in its infancy. This paper aims to understand how and if Big Data can contribute to commercial innovation, as well as future trends, investment opportunities.\nParticipants from 26 pharmaceutical companies participated in different focus groups where topics were grouped by individuals and evaluation areas were discussed to discover any potential connections between Big Data and Innovation in commercial pharmaceutical environments. This study used the collected data to analyze and draw conclusions about how many life sciences leaders and professionals already know about Big Data and are identifying examples and processes where Big data is supporting and generating innovation.\nIn addition, we were able to understand that the industry is already comfortable with Big Data, and there were some very accurate research results regarding the most pertinent application fields and key considerations moving forward. Using the network analysis findings and the relationships and connections explained by respondents, we can reveal how Big Data and innovation are interconnected.\nKeywords: Big Data, Digital, Pharmaceutical Industry, Focus Group, Commercial.\n- Pesqueira A, Sousa MJ. (2020) Pharmaceuticals and Life Sciences: Role of Competitive Intelligence in Innovation. InHandbook of Research on Emerging Technologies for Effective Project Management, pp. 237-254. IGI Global, https://www.igi-global.com/chapter/pharmaceuticals-and-life-sciences/239222, last accessed 2021/06/26.\n- Ringberg T, Reihlen M, Rydén P. (2019) The technology-mindset interactions: Leading to incremental, radical or revolutionary innovations. 2019 May 1;79: pp.102-13. Industrial Marketing Management, https://www.sciencedirect.com/science/article/pii/S0019850118304218, last accessed 2021/05/24.\n- Laouisset DE. (2021) Algeria import substitution policy: the case of the pharmaceutical industry. les cahiers du cread. Apr 20;37(1):5-39, https://www.ajol.info/index.php/cread/article/view/206142/194380, last accessed 2021/07/14.\n- Lahane S, Kant R, Shankar R. (2020) Circular supply chain management: A state-of-art review and future opportunities. Journal of Cleaner Production.\n- Bibri SE. (2019) On the sustainability of smart and smarter cities in the era of big data: an interdisciplinary and transdisciplinary literature review. Dec;6(1): pp.1-64. Journal of Big Data, https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0182-7, last accessed 2021/07/16.\n- Zhang D, Pee LG, Cui L. (2021) Artificial intelligence in E-commerce fulfillment: A case study of resource orchestration at Alibaba’s Smart Warehouse. 2021 Apr 1; pp. 57:102304. International Journal of Information Management.\n- Wiener M, Saunders C, Marabelli M. (2020) Big-data business models: A critical literature review and multiperspective research framework. Mar;35(1): pp.66-91. Journal of Information Technology, https://scholar.google.com/scholar?output=instlink&q=info:zxQHdZOuuKkJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=11377637586559752519&oi=lle, last accessed 2021/06/12.\n- Verma S. (2017) Big Data and advanced analytics: Architecture, techniques, applications, and challenges. 2017 Oct 1;4(4): pp.21-47. International Journal of Business Analytics (IJBAN).\n- Mariani MM, Nambisan S. (2021) Innovation analytics and digital innovation experimentation: the rise of research-driven online review platforms. Nov 1; pp. 172:121009. Technological Forecasting and Social Change.\n- Preim B, Lawonn K. (2020) A survey of visual analytics for public health. Feb Vol. 39, No. 1, pp. 543-580. InComputer Graphics Forum, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cgf.13891, last accessed 2021/06/13.\n- Büyüközkan G, Göçer F. (2018) Digital Supply Chain: Literature review and a proposed framework for future research. Computers in Industry. May 1;97: pp. 157-77.\n- Zhao J, Xue F, Khan S, Khatib SF. (2021) Consumer behavior analysis for business development. Aggression and Violent Behavior. Vol. 1 pp.101591.\n- Ciruela-Lorenzo AM, Del-Aguila-Obra AR, Padilla-Meléndez A, Plaza-Angulo JJ. (2020) Digitalization of agri-cooperatives in the smart agriculture context. proposal of a digital diagnosis tool. Sustainability. 12(4): pp.1325, https://www.mdpi.com/2071-1050/12/4/1325/pdf, last accessed 2021/08/17.\n- Saqib N. (2020) Positioning–a literature review. Jul 11. PSU Research Review. https://www.emerald.com/insight/content/doi/10.1108/PRR-06-2019-0016/full/html\n- Vickers I, Lyon F, Sepulveda L, McMullin C. (2017) Public service innovation and multiple institutional logics: The case of hybrid social enterprise providers of health and wellbeing. Dec 1;46(10): pp.1755-68. Research Policy, https://www.sciencedirect.com/science/article/pii/S0048733317301373, last accessed 2021/07/10.\n- Aghmiuni SK, Siyal S, Wang Q, Duan Y. (2020) Assessment of factors affecting innovation policy in biotechnology. Jul 1;5(3): pp. 180-90. Journal of Innovation & Knowledge. https://www.sciencedirect.com/science/article/pii/S2444569X1930054X, last accessed 2021/08/26.\n- Teece DJ. (2018) Dynamic capabilities as (workable) management systems theory. 2018 May;24(3): pp.359-68. Journal of Management & Organization.\n- Orlandi LB, Zardini A, Rossignoli C. (2020) Organizational technological opportunism and social media: The deployment of social media analytics to sense and respond to technological discontinuities. May 1;112:385-95. Journal of Business Research.\n- Lüdeke‐Freund F. (2020) Sustainable entrepreneurship, innovation, and business models: Integrative framework and propositions for future research. Feb;29(2): pp.665-81. Business Strategy and the Environment, https://onlinelibrary.wiley.com/doi/pdf/10.1002/bse.2396, last accessed 2021/07/26.\n- Pratono AH, Darmasetiawan NK, Yudiarso A, Jeong BG. (2019) Achieving sustainable competitive advantage through green entrepreneurial orientation and market orientation: The role of inter-organizational learning. Mar 11. The Bottom Line, http://repository.ubaya.ac.id/34237/1/AS7169462365265921547944711931_content_1.pdf, last accessed 2021/07/16.\n- Farzaneh M, Ghasemzadeh P, Nazari JA, Mehralian G. (2020) Contributory role of dynamic capabilities in the relationship between organizational learning and innovation performance. 2020 Jun 3. European Journal of Innovation Management.\n- Chege SM, Wang D. (2020) The impact of entrepreneurs’ environmental analysis strategy on organizational performance. Jul 1;77: pp.113-25. Journal of Rural Studies.\n- Lee C, Hallak R, Sardeshmukh SR. (2019) Creativity and innovation in the restaurant sector: Supply-side processes and barriers to implementation. Jul 1;31:pp. 54-62. Tourism Management Perspectives.\n- Panda DK. (2017) Coevolution and coexistence of cooperation and competition in inter-organizational collaboration: Evidence from Indian management consulting industry. Feb 20. Journal of Global Operations and Strategic Sourcing.\n- Ross JW, Beath CM, Mocker M. (2019) Designed for digital: How to architect your business for sustained success.2019 Sep 24. MIT Press.\n- Dubey R, Gunasekaran A, Childe SJ, Blome C, Papadopoulos T. (2019) Big data and predictive analytics and manufacturing performance: integrating institutional theory, resource‐based view, and big data culture. Apr;30(2): pp. 341-61. British Journal of Management, https://pearl.plymouth.ac.uk/bitstream/handle/10026.1/13321/Final_BJM_Revised%20Version_Authors%20Affiliations.pdf?sequence=1, last accessed 2021/07/26.\n- Mikalef P, Boura M, Lekakos G, Krogstie J. (2019) Big data analytics capabilities and innovation: the mediating role of dynamic capabilities and moderating effect of the environment. 2019 Apr;30(2): pp. 272-98. British Journal of Management, https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2631308/Big%2BData%2BAnalytics%2BCapabilities%2BR2.pdf?sequence=1, last accessed 2021/08/12.\n- Faisal-E-Alam M. (2020) The Influence of Quality on Consumers’ Purchase Intention between Local and Multinational Cosmetic Firm. 3(1): pp.01-11. Journal of International Business and Management. https://www.researchgate.net/profile/Md-Faisal-E-Alam/publication/338498835_The_Influence_of_Quality_on_Consumers’_Purchase_Intention_between_Local_and_Multinational_Cosmetic_Firm/links/5e26f95992851c89c9b5c289/The-Influence-of-Quality-on-Consumers-Purchase-Intention-between-Local-and-Multinational-Cosmetic-Firm.pdf, last accessed 2021/07/16.\n- Rey JE.(2020) Career-Advising from the Primary Role Academic Advisor’s Viewpoint: A Qualitative Case Study. Rowan University. https://rdw.rowan.edu/cgi/viewcontent.cgi?article=3840&context=etd, last accessed 2021/07/15.\nPublished online 28.12.2021","Design thinking frameworks as transformative cross-disciplinary pedagogy\nAnderson, Neil, Adam, Raoul, Taylor, Pauline, Madden, Dianna, Melles, Gavin, Kuek, Christopher, Wright, Natalie, & Ewens, Bev (2014) Design thinking frameworks as transformative cross-disciplinary pedagogy. Australian Government Office for Learning and Teaching, Department of Education, Sydney, N.S.W.\nFinal report for the Australian Government Office for Learning and Teaching.\n\"This seed project ‘Design thinking frameworks as transformative cross-disciplinary pedagogy’ aimed to examine the way design thinking strategies are used across disciplines to scaffold the development of student attributes in the domain of problem solving and creativity in order to enhance the nation’s capacity for innovation. Generic graduate attributes associated with innovation, creativity and problem solving are considered to be amongst the most important of all targeted attributes (Bradley Review of Higher Education, 2009).\nThe project also aimed to gather data on how academics across disciplines conceptualised design thinking methodologies and strategies. Insights into how design thinking strategies could be embedded at the subject level to improve student outcomes were of particular interest in this regard. A related aim was the investigation of how design thinking strategies could be used by academics when designing new and innovative subjects and courses.\"\nCase Study 3: QUT Community Engaged Learning Lab Design Thinking/Design Led Innovation Workshop by Natalie Wright Context\n\"The author, from the discipline area of Interior Design in the QUT School of Design, Faculty of Creative Industries, is a contributing academic and tutor for The Community Engaged Learning Lab, which was initiated at Queensland University of Technology in 2012. The Lab facilitates university-wide service-learning experiences and engages students, academics, and key community organisations in interdisciplinary action research projects to support student learning and to explore complex and ongoing problems nominated by the community partners. In Week 3, Semester One 2013, with the assistance of co-lead Dr Cara Wrigley, Senior Lecturer in Design led Innovation, a Masters of Architecture research student and nine participating industry-embedded Masters of Research (Design led Innovation) facilitators, a Design Thinking/Design led Innovation workshop was conducted for the Community Engaged Learning Lab students, and action research outcomes published at 2013 Tsinghua International Design Management Symposium, December 2013 in Shenzhen, China (Morehen, Wright, & Wrigley, 2013).\"\nImpact and interest:\nCitation counts are sourced monthly from and citation databases.\nThese databases contain citations from different subsets of available publications and different time periods and thus the citation count from each is usually different. Some works are not in either database and no count is displayed. Scopus includes citations from articles published in 1996 onwards, and Web of Science® generally from 1980 onwards.\nCitations counts from theindexing service can be viewed at the linked Google Scholar™ search.\nFull-text downloads displays the total number of times this work’s files (e.g., a PDF) have been downloaded from QUT ePrints as well as the number of downloads in the previous 365 days. The count includes downloads for all files if a work has more than one.\n|Additional Information:||\"Acknowledgements Professor Anderson and the team at James Cook University (JCU) would like to acknowledge the academics at Queensland University of Technology, Swinburne University, Edith Cowan University and Charles Darwin University for their contribution to successfully organising design thinking lectures and workshops in Darwin, Brisbane, Melbourne and Perth. Key academics included Dr Oksana Zelenko and Dr Jenny Lane. We would also like to thank Dr Gavin Melles, Dr Bev Ewens, Dr Christopher Kuek and Natalie Wright for their case study contributions to the report. We would also like to thank the Office for Learning and Teaching for funding the project.\"|\n|Divisions:||Current > Schools > School of Design\nCurrent > QUT Faculties and Divisions > Creative Industries Faculty\n|Copyright Owner:||Copyright 2014 Australian Government Office for Learning and Teaching, Department of Education|\n|Deposited On:||20 May 2015 22:01|\n|Last Modified:||22 May 2015 00:19|\nRepository Staff Only: item control page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:dad63e25-007a-42ab-a150-9453b0a0f452>","<urn:uuid:0d967731-e82c-40aa-9b25-df1d37d6535b>"],"error":null}
{"question":"As a construction worker who regularly switches between indoor and outdoor tasks, what should I wear for comfort and what safety equipment is mandatory?","answer":"For comfort, you should wear loose-fitting clothing with plenty of pockets like cargo pants, and dress in layers that can be removed to prevent overheating. For outdoor work, lightweight, light-colored clothing is recommended, along with sunscreen. As for mandatory safety equipment, Personal Protective Equipment (PPE) includes safety glasses or face shields for eye protection, work boots with slip-resistant and puncture-resistant soles, properly fitting gloves appropriate for the task, hard hats if there's risk of falling objects, hearing protection for high-noise tasks, and joint support like knee pads or back braces. Remember that some PPE may be mandatory depending on the job, such as hard hats on construction sites.","context":["It is a common misconception that tradespeople need to wear construction gear or coveralls for their jobs. While there are some instances where this might be the case, it is not always necessary. In fact, many tradespeople wear regular clothes and make a point of looking as professional as possible on the job site.\nThis trend has been growing in recent years with more people seeking out careers in trades such as plumbing, carpentry, electrician work and HVAC installations among others. However, because these jobs can often involve messy conditions like grease splatters from working on cars or sawdust from woodworking projects, what you wear really depends on the type of work being done at any given time so it’s important to have a wide range of clothing.\nWhat to wear on the job site\nTradespeople have a lot of different options when it comes to what they wear on the job site. In general, it’s important to dress for the type of work you’re doing and the weather conditions.\nIf you’re working in a construction setting, it’s important to wear clothing that is tough and can withstand tough conditions like dirt, dust, and paint.\nCoveralls are a good option for this type of environment, but many tradespeople also wear jeans and a t-shirt or a construction hat to keep the sun out of their eyes. Some businesses provide shirts with a company logo and use heat press machines for uniform branding. It’s also important to wear sturdy shoes or boots that can protect your feet from injuries.\nHow to dress according to the weather\nWhen you’re working in the trades, it’s important to be prepared for all types of weather. In hot weather, it’s important to wear light clothing that will keep you cool and comfortable. A hat or sunglasses can also help protect you from the sun.\nIn cold weather, it’s important to dress in layers so you can stay warm. You should also make sure to wear boots or other shoes that will keep your feet warm and dry. Remember that you’ll most likely be sweaty when you’re working, so being able to remove layers of clothing is important to prevent overheating.\nTips for dressing when you’re in the field all day long\nIf you’re going to be working in the field all day long, it’s important to dress comfortably and in a way that won’t distract you from your work. Loose fitting clothing with plenty of pockets, like cargo pants or shorts will allow you to stay cool and move around.\nA t-shirt with a collar can also look professional while keeping you comfortable in the field. It’s important to wear clothes that are durable but comfortable since there may be sharp objects or rough surfaces on site that could tear clothes.\nThe importance of personal protective equipment\nThe importance of personal protective equipment (PPE) can’t be overemphasized, especially when it comes to hazardous materials on the job site. Gloves, masks, boots and other PPE can protect you from harmful substances and debris, which is why it’s important to always use them when necessary.\nIn some cases, using PPE may be mandatory. For example, many construction jobs require workers to always wear hard hats. In other cases, PPE is optional but highly recommended. If you’re working with hazardous materials, it’s always a good idea to take precautions and protect yourself as much as possible.\nIn this article, we’ve covered some of the basics that any tradesperson should know about choosing the right workwear for a job. Now go out there and do an amazing job with whatever it is you’re working on today. Happy building or fixing or mowing or painting!","The construction phase of a rehab is exciting, but it’s also one of the riskiest times of your project. Minimizing the risk of a severe injury during this phase is key to retaining your profit and protecting your business. Whether you are a do-it-yourself-er or hire out the rehab process to an experienced general contractor, you should do a cursory walk around the premises and be able to tell if you have a safe operation. To do that, you will need some basic knowledge about what injuries are common in construction and the markers of a safe jobsite. What are they?\nCommon Types of Construction Injuries\nIt may not surprise you that the U.S. Board of Labor Statistics reported construction as the #2 industry for fatal work injuries in their most recent report (2017). That year there were 971 fatalities. Hundreds of thousands of non-fatal injuries also occur in the construction industry each year. So, what are the most common types of injuries?\nOSHA’s “Fatal Four”\nMany of you may be familiar with, or have at least heard of OSHA, the Occupational Safety and Health Administration. Congress established the organization in 1970 to “assure safe and healthful working conditions for working men and women by setting and enforcing standards and by providing training, outreach, education and assistance.” Their so-named “Fatal Four” are the most common types of fatal construction injuries:\n- Falls — about one third of construction fatalities\n- Struck by Object or Vehicle – one quarter of struck-by vehicle fatalities involve construction workers\n- Electrocutions – about 250 electrical-related fatalities occur each year\n- Trenching and Excavation – the fatality rate for excavation is 112% higher than for general construction\n(Statistics courtesy of https://www.osha.gov)\nCommon Non-fatal Injuries on the Jobsite\nBecause construction is such physical work and involves many types of tools and equipment, the potential for non-fatal injuries is high. There seem to be an infinite number of ways one could be bruised, pinched, cut and more. To name just a few, non-fatal injuries on the jobsite are often caused by:\n- Hand & Power Tools (Hammers, screwdrivers, nail guns, saws, sanders, blow torches, utility knives)\n- Heavy Equipment (Dozers, forklifts, boom lifts)\n- Pollution (Asbestos, lead, drywall dust, sawdust, fumes, latex paint)\n- Noise (Loud saws, sanders, drills and other power tools)\n- Flammable Liquids/Materials (Stain, paint, paint thinner)\n- Slips/Trips/Falls (Slick or uneven surfaces, stairs, ladders, cluttered jobsites)\n- Lifting heavy objects (Boxes of tile, bathtubs, cabinetry)\n- Repetitive motion injuries (Being on your knees while installing flooring, sanding, loading and unloading materials)\n- Cuts and puncture wounds from materials laying around the jobsite (Sheet metal, nails)\n- Concussions from falling objects\nTake a look around your jobsite and put on your injury-prevention goggles. What hazards do you see? How will you remedy them?\nJobsite Safety Basics\nKeeping a busy jobsite tidy can be a challenge. Tight deadlines can also tempt workers to forego safety measures to stay on schedule. Getting too comfortable with heights or power tools can have severe, sometimes fatal, consequences. On the flip side, a clean, safe jobsite can increase efficiency, maximizing your potential profit. What are some of the key components of a safe and efficient work environment?\nPPE – Personal Protective Equipment\nDressing appropriately for the type of work you’ll be doing is important for any type of job. Personal Protective Equipment is an integral part of any rehabber’s uniform for keeping him/herself safe. PPE includes:\n- Eye & Face Protection: Safety glasses, face shields.\n- Foot Protection: Work shoes/boots with slip-resistant and puncture-resistant soles. Safety toes made from composite material (metal toes can still be crushed and shear off toes).\n- Hand Protection: Gloves with the correct fit. Correct gloves for the type of job (paint/stain, chemicals, rough surfaces, handling boards with nails).\n- Head Protection: Hard hats if in danger of falling objects from above. Hard hats should have no dents, cracks or deterioration and should be replaced after a heavy blow or electrical shock.\n- Hearing Protection: Earplugs or earmuffs in high-noise applications.\n- Joint Support: Knee pads, back or knee braces, tool belts with suspenders to take weight off the back and hips.\n- Headlamp: Adequate lighting for your workspace and to help avoid slip and falls.\nHousekeeping on the Jobsite\nProbably the simplest and most crucial task to injury prevention on the jobsite is keeping the worksite clean and free of hazards. Not keeping a clean jobsite can lead to:\n- Damage to tools and equipment, materials and the structure of the house.\n- Loss of production – cleanliness and organization help efficiency.\n- Fire hazards.\n- Physical injuries such as: slips/trips/falls, cuts, bruises, sprains, breaks, eye injuries, and more, including death.\nYou will need a plan for:\n- How trash and construction debris are removed.\n- How materials will be stored.\n- How tools will be organized and stored when not in use.\nJobsite Cleanliness Tips:\n- Store all tools and materials not in use in their proper place.\n- Clean up messes in a timely fashion – clean as you go.\n- Keep walkways and driveways clear.\n- Be aware of common trip hazards: electrical cords, air hoses from compressors, unfinished transitions between floor surfaces, uninstalled materials, tools that are in-use and the like.\n- Work as a team to keep the jobsite clean and safe.\nWeather Considerations – Working in the Heat\nAs the weather begins to warm up, heat-related illness can sneak up on workers if they are not mindful of the weather conditions they will be working in throughout the day. Heat exhaustion is not usually life-threatening, but it can lead to dizziness, headaches and fatigue that may make a worker more susceptible to other injuries. Heat stroke can make you lose consciousness, and puts strain on your heart and blood vessels, increasing the risk for heart failure or stroke. To “beat the heat”:\n- Dress for success with the 3 L’s: wear Lightweight, Light-colored, Loose clothing. (Still be mindful of anything that could get caught in machinery.)\n- Use sunscreen.\n- Drink fluids continuously throughout the day – don’t wait to get thirsty. Water is the best and other drinks that support electrolyte balance are good too. Stay away from anything with caffeine or alcohol.\n- If possible, build up to longer periods of sun exposure gradually. Try to stay in the shade during 10am-3pm when heat is the most intense and choose a place in the shade for any outdoor work stations.\n- Be in the know – water, concrete and sand reflect the sun and can increase its intensity.\n- Eat a well-balanced diet with fresh fruits and veggies and avoid hot, heavy or greasy foods.\n8 Basic Jobsite Safety Tips to Share with Your Crew\n- Be aware of your surroundings.\n- Keep a clean workspace.\n- Be intentional about taking breaks to prevent fatigue and quit when you’re tired.\n- Ask for help in carrying heavy objects or use tools that will help you carry awkward objects safely, such as furniture dollies.\n- Properly dispose of hazardous materials (i.e. rags with paint thinner, etc.).\n- Ventilate areas properly when working with flammable materials.\n- Don’t remove safety features on power tools (i.e. trigger guards on nail guns, blade guards on saws).\n- Dress properly for safety (i.e. heat, cold, nothing that will get caught in equipment such as jewelry).\n“Nearly 6.5 million people work at approximately 252,000 construction sites across the nation on any given day.” – OSHA\nOSHA – who specifically needs to comply with their regulations?\nThe mission of OSHA is pretty simple. Under the OSH Act, “employers are responsible for providing a safe and healthful workplace.” It can be a little confusing as to who needs to comply with OSHA regulations, so if you are unsure about your status under the OSH Act, it is best to consult an attorney. Regardless of whether or not you are required to report any injuries that occur at your rehab, their advice is wise to follow, and their resources are helpful. What follows are a sampling of OSHA tools and resources that can help you achieve a safe jobsite.\nOSHA Tools & Resources:\nPrevention Videos (v-Tools)\n- Cover topics like falls, sprains and strains, carbon monoxide, electrocutions and more.\n- 2-4 minutes long, sample incidents based on true stories that resulted in worker injuries.\n- Link: https://www.osha.gov/dts/vtools/construction.html\n“Recommended Practices for Safety & Health Programs in Construction”\n- Booklet that helps employers, workers and worker representatives (unions, etc.), with a proactive framework for addressing safety and health issues on jobsites.\n- Includes a place to do a general self-evaluation and track progress in implementing the safety practices.\n- Link: https://www.osha.gov/shpguidelines/docs/8524_OSHA_Construction_Guidelines_R4.pdf\nOn-site Consultation Program for Small Business Employers\n- Free and confidential.\n- Helps employers assess whether there are hazards at their worksites.\n- Consultants provide advice on how to comply with OSHA standards and help establish injury and illness prevention programs.\n- Separate from enforcement activities and don’t result in penalties or citations.\n- Link: https://www.osha.gov/dcsp/smallbusiness/consult.html\n- Or call: 800-321-OSHA (6742)\nOSHA Help for New Businesses: https://www.osha.gov/OshDoc/data_General_Facts/newbusinesses-factsheet.html\nWant the whole “kit and caboodle”? You can find OSHA’s Construction Regulations and Standards HERE.\nLicensing & Insurance\nMany investors worry that hiring someone who is licensed and insured will drive up their rehab cost and make it impossible for them to turn a profit on a project. While larger construction companies may cost more because they have more overhead, hiring someone who is inexperienced or isn’t adequately insured could end up hurting your livelihood far worse than if you had simply allowed more room in the budget for construction costs. Simply put, if you can’t afford to do a project safely, it is probably not the right deal.\nFinal Clean-Up (The Golden Nugget)\nWhether you’re the one swinging the hammer or the one calling up your GC for progress reports on your rehab, educating yourself about the entire construction process can help you reduce unnecessary and costly injuries. This knowledge may impact the type of deals you buy too; carefully consider how much work will need to be done and if you can afford to do it safely. Rushing the rehab process at the cost of safety or selecting a contractor without the proper experience and insurance protection can rack up a list of liabilities you don’t want to pay for. We hope these tips will help you rehab safely!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:e8b995ca-a6a1-4fb3-aa7d-62e69d92b5ef>","<urn:uuid:3715d1f9-05c6-4135-a7eb-47143a70a5ce>"],"error":null}
{"question":"As someone interested in historical commemorations, how do the memorial efforts for the 1956 Hungarian Revolution compare to those for the 1944 Warsaw Uprising in terms of museum presence and public monuments?","answer":"The Warsaw Uprising has more extensive memorial efforts, featuring a dedicated Warsaw Rising Museum opened in 2004 with over 800 items and 1500 photographs across 3000 square metres, plus multiple monuments including the grand Warsaw Uprising Monument and the Little Insurgent memorial. The Hungarian Revolution's commemoration appears more political in nature, with the government spending millions on billboards showing young 1956 street fighters and specially commissioned songs, while physical reminders are limited to bullet holes still visible in Budapest buildings.","context":["The day Soviet tanks crushed democracy, and the bitter battle over its legacy\nBy Stefan J. Bos, Chief International Correspondent BosNewsLife in Budapest. A co-production with Australian Broadcasting Corporation (ABC).\nBUDAPEST, HUNGARY (BosNewsLife/ABC)– Millions in Europe have been remembering Soviet forces storming the streets of Budapest, killing thousands and crushing Hungary’s fight for freedom in 1956. But as the country lurches to the political right under Prime Minister Viktor Orbán, the revolution is being recast as a patriotic fight against foreign domination … with the EU villainised as the new oppressor.\nCommemorating the Freedom Fight, Orbán said that, “Freedom-loving peoples of Europe must save Brussels from Sovietisation”.\nSpeaking to a crowd of thousands on the National Day of Remembrance, that marks the beginning of the uprising, he condemned the European Union’s humanitarian migration policies, portraying them as a kind of tyranny.\nBullet holes of Hungary’s 1956 battles for freedom seen in Budapest buildings.\n“We, Hungarians, want to remain a European nation, rather than become an ethnic minority in Europe,” he said.\n“It is only our national independence that can save us from being devoured by an empire.\nThe 1956 revolution has become Orbán ‘s touchstone for Hungary’s anxiety about sovereignty.\nNow a leading figure of that uprising, Imre Mécs , has called Prime Minister Orbán‘s leadership “a tragedy”. “He is a tragedy, not only for Hungary, but for all of Europe,” Mécs, 83, told the ABC and BosNewsLife. “A normal person would defend European values.”\nThe 1956 revolution began on 23 October with a few hundred university students protesting against Soviet repression, and snowballed to 200,000 protesters on the streets, with people rising up around the country.\nThey demanded an end to one-party rule, and appealed for press freedom and democracy.\nBut 19 days later — 60 years ago today — 100,000 Russian soldiers and thousands of tanks rolled into town and in a bloody fusillade devastated democratic hopes for another 35 years.\nRemembering the tragedy Hungarian Prime Minister Viktor Orbán appears on stage speaking during the remembrance ceremony of the hungarian revolution\nSurvivors still recall the many victims in their nightmares.\nBye, bye comrades an iconic poster reads referring to the end of occupying Soviet forces in Hungary in 1990\n“I witnessed how people toppled the statue of Soviet leader Stalin in Budapest,” says Imre Czigány, 70, remembering the crowd chanting: “Russians out, Russians go home.”\n“I was only 10, but I remember everything. The shooting and the fighting went on where I was living. I saw the tanks rolling, I saw the fights,” he remembers, close to tears.\n“I saw the dead bodies lying on the ground. I will never forget that. Never, never. And I don’t want to.”\nIt was, he says, “where Hungarians really showed where they are: on the side of freedom”.\nHis voice trembles as he admits, “I am still emotional about it. This was a turning point in my life”.\nCzigány was among the tens of thousands who rallied to hear Prime Minister Orbán in front of Budapest’s Neo-Gothic-style parliament building on Remembrance Day.\nDemonstrators blocked by police in hungary\nThe Government spent millions of dollars on the event, with billboards lining the streets showing young 1956 street fighters carrying arms, and specially commissioned songs.\nBut rather than celebrating hard-won democratic freedoms, the Prime Minister, who has clamped down on the free press and become increasingly authoritarian, has built a narrative around the rebel fighters, striking a patriotic chord.\nÁgnes Szent-Ivány, 63, was there supporting Orbán, even though she and her family fought for freedom during the Soviet regime.\n“Hungarians voted three times to have Mr. Orban as prime minister, the last two times with a two-thirds majority,” she says, explaining he stands for Hungarian independence.\nHer family was persecuted by the Soviets for being Catholic intellectuals. She says the regime divided the country.\n“There were those who were against the Soviet system and those who collaborated with the Soviets to gain many benefits,” she explains.\nRousing the crowd, Orbán was drowned out by thousands of protesters, blowing whistles and shouting, “dictator!”\nClearly irritated and shouting, Orbán accused the protesters of being Soviet-style communists. Without irony he aligned himself with the cause of freedom.\n“There was a revolution against the communist world,” he yelled above the din.\nSzent-Ivány agreed, calling the protesters “Godless atheists”.\nThe protesters hold different views.\n“Viktor Orbán’s policies are exactly the kind Hungarians rebelled against in 1956,” says Opposition Leader Péter Juhász, Vice President of the Together party which organised the protests.\nHe questions Orbán’s democratic credentials.\n“Hungarians stood up to Soviet domination, while Orban has committed Hungary to Russia for decades,” says Juhasz referring to a recent nuclear power deal.\nFormer freedom fighter, Mécs, rejects Orbán’s rewriting of history.\n“Communists died for freedom too,” he says.\nHe should know. The famed Liberal elder statesman and author spent six years on death row for his role in the 1956 Revolution, before being released in 1963.\nWith teary eyes he holds a picture of a young fallen comrade.\n“He was a communist but fought for freedom. I said farewell to him just before his execution,” he says.\nMécs stayed in Hungary, becoming a liberal politician when democracy finally arrived.\nBut in the purge of dissidents that followed the 1956 crackdown, 200,000 Hungarians fled the country. The young Imre Czigány was one.\nHis father had been facing the death penalty for subversion when freedom fighters stormed the prison in 1956 and freed him.\nIn the vicious aftermath of the uprising, the family fled to Germany.\n“We had to leave at the last second, otherwise they would have taken my father back to prison,” Czigány says.\nThe flood of refugees were embraced in Western nations.\nIt’s drawn comparisons with the hostile razor-wire fence that greets the current influx of refugees streaming out of the Middle East, and rejection of EU refugee quotas by the staunchly anti-migrant Orbán.\nHe’s portrayed them as yet more invaders threatening Hungary, invoking the shadow of a centuries old battle, when Hungary was overrun by Muslim invaders from the Ottoman Empire.\nIt’s etched deeply into Hungarian consciousness, much as Gallipoli is in Australia.\nFriends and foes agree that he skilfully uses fear of foreign invaders to increase his popularity, and distract attention from economic difficulties and other social ills in this former Soviet state.\nIt leaves his people sharply divided over liberal European values.\nMr Czigány, who lived in Germany and Belgium before he eventually returned to Hungary, is, like his country, “wandering between two worlds”.\nHe and others regret that after finally being freed from Soviet domination in 1990, Hungary’s society is more divided than ever, posing a threat to the already shaky European Union.\nUNITED IN SUFFERING\nYet divided as they are over politics, Hungarians of all ages appear united in suffering. The 1956 Revolution also impacted younger generations. The 49-year-old journalist Tamás S. Kiss pauses in front of the building in Budapest where his father was interrogated by Hungarian police.\n“My father was a freedom fighter. When he was just 19 years old he and his student friends raided a police station to gather weapons to fight the Soviets,” he told BosNewsLife.\n“After the Revolution failed he had to flee because he would have been executed. He lived in South Africa where I was born. Years later, my mom got homesick and he agreed to return to Hungary with his family.”\nIstván Kiss wasn’t warmly welcomed. “My father was interrogated in Budapest soon after he arrived in 1983 because of his past. Every second month he had to go to report, despite having received an “Amnesty”.”\nTamás S. Kiss suffered too. “I wasn’t allowed to go to the vet university because of my father’s role in the 1956 Revolution and despite securing a promising job at the Central Veterinarian Institute.”\nA few years ago, his father passed away. “Sometimes I wonder what he would have thought about Hungary today. He always warned us: ‘the old habits never die’.”\nFormer freedom fighter Mécs is calling for change. “Just as Moses in the Bible had to lead the people of Israel from Egypt to the Promised Land, so Hungary has to deal with its past. I think we need a real new generation of leadership to bring Hungary true freedom,” he says.\n(BosNewsLife’s NEWS WATCH is a regular feature following general news developments especially in but not limited to (former) Communist countries and other (former) autocratic states impacting the Church and compassionate professionals worldwide. BosNewsLife is based in Budapest, Hungary. )","A few months ago, I visited Warsaw for the International Conference of Historical Geographers. Whenever I visit a new place I try to find out as much as I can about its history of radicalism and dissent, and there’s no doubt that Warsaw has plenty of that. In Part 1 of this post, I wrote about the Warsaw Ghetto Uprising in April and May 1943, and the ways that it is remembered in Warsaw’s streets and museums. Part 2 is about the Warsaw Uprising, which lasted for 63 days in 1944. The Uprising has an entire museum dedicated to it, as well as an impressive monument.\nDuring the summer of 1944, the German Army was retreating across Poland, pursued by the Soviet Army. The Polish Home Army undertook uprisings in several cities in order to help the Soviet Army, and to assert Polish sovereignty–there were fears that the German occupying force would just be replaced with a Russian one. As the Soviet Army advanced towards the Vistula river, the Home Army in Warsaw decided to begin their own uprising on 1st August. It became the largest military effort of any resistance movement during the Second World War.\nThe uprising was only ever supposed to last a few days, until the Soviet Army reached Warsaw. However, the Soviets halted their advance on the eastern bank of the Vistula, and the resistance forces ended up fighting, almost entirely unsupported, for 63 days. The Home Army, aided by other groups including the National Armed Forces and the communist People’s Army, quickly took control of large sections of Warsaw. These areas were separated from each other however, and communication was difficult. The resistance fighters had received training in guerrilla warfare, but they were inexperienced at prolonged fighting in daylight and severely under equipped.\nOn the 4th of August, the Germans started to receive reinforcements, and began to counterattack. The following day, they began a systematic massacre of civilians in order to crush the resistance’s resolve. The strategy backfired however, only making the people of Warsaw more determined. Resistance fighters captured the ruins of the Warsaw ghetto (see Part 1), and liberated the Gesiowka concentration camp. At the end of August, the resistance decided to abandon the Old Town; the area was evacuated through the city’s sewers, which also served as a major means of communication for the resistance. The resistance eventually surrendered to the Germans on 2nd October; the expected help from the Soviets never came. The city wasn’t captured until 17th January 1945, giving the Germans plenty of time to systematically destroy the city and transport many of its residents to work and concentration camps.\nLife in Warsaw was very hard during the uprising, for civilians as well as resistance fighters. There were severe shortages of food; people largely survived on ‘spit soup,’ made from barley captured from the Haberbusch i Schiele brewery. The media flourished in the city however, multiple newspapers were published frequently, and 30,000 metres of film documenting the uprising was produced.\nWarsaw Rising Museum\nThe Warsaw Rising Museum was opened in 2004, to mark the 60th anniversary of the uprising. The Museum contains more than 800 items and 1500 photographs and videos spread over 3000 square metres. It covers all aspects of the uprising, and provides visitors with a huge amount of information. It is arranged chronologically, and I would recommend following the order of the galleries carefully (you go from the ground floor to the top, then work your way back down, which could be more clearly sign posted). I think you need at least 3 hours to see everything, and I would recommend stopping halfway through for a drink and a slice of cake in the cafe, otherwise you will get too tired to take it all in properly. A highlight for me was the Kino Palladium, a small cinema that shows footage of the uprising that was used to make newsreels. I was also particularly moved by the collection of armbands. Soldiers in the uprising didn’t have uniforms, so used red and white armbands to identify themselves. Some people personalised theirs, and it really brought the human element of the uprising home to me.\nMonuments and Memorials\nThe Uprising Museum is located in Freedom Park, where you can also find several memorials connected to the uprising. The memorial wall documents the name of more than 10,000 resistance fighters who died during the fighting. Set within the wall is a bell dedicated to General Antoni Chrusciel, one of the uprising’s leaders. There is also a memorial to the estimated 150,000 civilians who lost their lives during the uprising, as well as the 550,000 who were deported from the city after the uprising failed.\nSet into the city walls surrounding Old Town is the Little Insurgent, a memorial to the children and young people who served as orderlies and runners during the uprising. The statue is based on a small plaster statuette created after the war by sculptor Jerzy Jarnuszjiewicz. It was paid for by former scouts, and unveiled in 1983 by Jerzy Swiderski, a cardiologist who had served as a scout during the uprising. It is a moving reminder of how the uprising consumed every aspect of Warsaw; even children could not escape the brutality.\nThe best-known memorial to the uprising, the Warsaw Uprising Monument, is on a much grander scale. Located on the southern side of Krasinki Square, the momument was unveiled in 1989, and is up to 10 metres tall. The monument has two sections: the larger represents a group of insurgents in combat, running from a collapsing building; the smaller section, in the foreground of the above photo, shows fighters and civilian woman climbing into a manhole. This is an acknowledgment of the significance of the city’s sewer system to the uprising. The monument is impressive, and you’d be hard pushed to walk past without stopping for a closer look. Monuments and statues can often blend into the street around them, which I think defeats one of the key objectives of memorials; drawing attention to the event, person or people it is meant to commemorate. There is no danger of the Warsaw Uprising Monument failing to attract attention.\nLike all cities, Warsaw’s past is inscribed into its streets, buildings and public spaces. Warsaw’s history is more violent than many cities–it has faced more than it’s share of death, destruction, and upheaval, and not just during the Second World War. There a number of different approaches to dealing with such a traumatic history in Warsaw: the city’s museums use different balances of objects and multimedia; and the monuments work on different scales, from the small and personal to the grand and official. Which approaches work best probably depends on personal taste, but the fact that so much effort and thought has gone into all of these commemorative practices demonstrates an admirable relationship with the past.\nSources and Further Reading\nFrederico. “The Warsaw Uprising Museum.” Odd Urban Things. Last modified 13th March 2017, accessed 25th August 2018. Available at https://www.oddurbanthings.com/warsaw-uprising-museum/\nPolish Tourism Organisation. “Monument of the Little Insurgent in Warsaw.” no date, accessed 25th August 2018. Available at https://poland.travel/en/museum/monument-of-the-little-insurgent-in-warsaw\nSimkin, John. “Warsaw Uprising.” Spartacus Educational. Last modified August 2014, accessed 25th August 2018. Available at http://spartacus-educational.com/2WWwarsawU.htm\nThe Warsaw Rising Museum. “The Warsaw Rising Museum.” No date, accessed 25th August 2018. Available at https://www.1944.pl/en/article/the-warsaw-rising-museum,4516.html\nTrueman, C N. “The Warsaw Uprising of 1944.” The History Learning Site. Last modified 18th May 2015, accessed 25th August 2018. Available at https://www.historylearningsite.co.uk/world-war-two/world-war-two-and-eastern-europe/the-warsaw-uprising-of-1944/\nWikipedia. “Warsaw Uprising.” Last modified 21st August 2018, accessed 25th August 2018. Available at https://en.wikipedia.org/wiki/Warsaw_Uprising\nWikipedia. “Warsaw Uprising Monument.” Last modified 28th March 2018, accessed 25th August 2018. Available at https://en.wikipedia.org/wiki/Warsaw_Uprising_Monument"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:8bcdf70c-bed6-4ce1-b8d7-244f20be4606>","<urn:uuid:fdaa33ad-de86-4fe5-9886-11f14fa16f01>"],"error":null}
{"question":"What role did June Tyson play in Sun Ra's Arkestra performances, and how much energy did U.S. data centers consume in 2013?","answer":"June Tyson was a permanent member of Sun Ra's Arkestra from 1968 until her death in 1992, serving as vocalist, dancer, choreographer, costume director and violinist. She had a unique vocal timbre described as haunting and ethereal, often providing a grounding element for listeners in Ra's complex cosmic performances. As for U.S. data centers, they consumed an estimated 91 billion kilowatt-hours of electricity in 2013, equivalent to the annual output of 34 large 500-megawatt coal-fired power plants.","context":["Illuminating the Eye of the Cosmic Needle\nBy Cat Celebrezze\nSun Ra, readers. Say it with me. SUNNNNNNRAAAAAA. As monikers go, this is the mother lode. Mysterious and mystic, evoking ancient worlds founded by futuristic aliens. \"It's my vibrational name,\" Ra commented once 1. As artists go, ‘enigmatic' and ‘vibrational' nail it, since the musician Sun Ra was, far, far, ahead of his time 2. So far, in fact, he was from the future, a cosmic entity that \"only returned to say he left,\" according to Amiri Baraka 3.\nSonically, there are a thousand Sun Ra's to experience: the high aptitude jazzman of the 1950's; the 1960's and 1970's leader of the electronic maelstrom orchestra bent on hurdling you through the cosmos via epic, improvisational cacophony; the moogified outer space alien seen in the 1974 Afrocrypticfuturist film, Space Is the Place; the surprisingly funky and hypnotic fusionist found on his 1978 release, Lanquidity. Like Ravi Shankar and Grateful Dead, Sun Ra's prodigious discographic output is dominated by a plethora of live recordings, punctuated by ground-breaking studio releases, and diluted by repackaged and recombined re-releases. Add to that with the cosmologic mythos and persona he created (\"a disguised twin of tomorrow\" 4) to bring forth his message of the interstellar beyond to come, Sun Ra is a sonic and existential universe unto himself. No matter how good your listening skills, sooner or later you'll find yourself overwhelmed, awed, and in a state of dread and wonder by the sheer magnitude and density of his catalog. To our current, dilapidated cosmic situation, Sun Ra offers no yantra, or mantra, or mandala to help map the universe he says is to come, or, in fact, already is and was. He just throws you into it.\nSo thank Ra for June Tyson. But who is June Tyson?\nBorn in North Carolina in 1936, Tyson met Ra in 1968 and became a permanent member of his Arkestra as vocalist, dancer, choreographer, costume director and violinist, until her death in 1992 (notably just six months before Ra himself). But unlike others in Ra's orbit, not much is written specifically about her. Often she is mentioned quickly and in passing as cutting an odd figure - both \"untrained\" and female - amidst the cohort of male and technically endowed musicians Ra cultivated, like John Gilmore or Marshall Allen. It's no secret that Ra preferred male relationships to female relationships; his view of women (and drink and drugs) as distractions is well documented 5.\nTyson, it seems, was the exception that proved the rule. As such, her role in that creative universe has been given short shrift, often seen in the \"reductive, masculinist aesthetic register of muse, siren, handmaiden, and devotee\" 6. At the same time, Tyson herself in the very few places she is quoted, comes across with a strong conviction in her acolyte status to Ra as artist and teacher 7. So it's complicated.\nThere is a danger in attempting to correct her alleged relegation to a \"static figure of sexual difference\" and to rehabilitate her legacy in (laborious) terms of \"the voice in the minoritarian operation of the becoming- woman of music\" 8 - it runs over Tyson's agency with a philosophical Mack truck 9. And yet it's not enough to see her solely as the mother-sister remembered by people who knew her and worked with her 10. That leaves her significance on the level of personal relationships and ignores her very real sonic participation in the Arkestra. So let's all consider ourselves neophytes for a moment and hear what June Tyson's voice attunes us to within the cosmos of the Arkestra, the philosophy and musicology of Sun Ra, and the complicated jazz terrain known as Experimental Afrofuturism. Think of her as a kind of Beatrice Virgil, a reoccurring presence in the music for us listeners as we travel through Sun Ra's realms.\nTyson had a voice of unique timbre, somewhere between haunting and ethereal, rooted in both a genteel North Carolinian accent and the strong, fierce, emotive vocalizations found in the blues shouter tradition. And as with that tradition, Tyson had a fearlessness with regard to imperfection, using such asymmetries instead to cast her voice with an expressiveness - something key to listeners treading, uninitiated, into Sun Ra's territory. It is a quality that gives her voice a texture far and away and deep, both existentially and physically, like the sounds Voyager transmitted back to earth as it has traversed out of the heliosphere. Called \"sempiternal\" 11 - a word with roots in Latin that means \"everlasting and eternal\" - Tyson's voice is a quixotic yet friendly aural peculiarity.\nTwo early examples of her specific vocal temperament are found in surprisingly straightforward fashion on Ra's 1970 release, My Brother The Wind Vol II on the tracks \"Somebody Else's World\" and \"Walking on the Moon.\" Here, her performances are layered and rhythmic, totally at home in the context of Ra's bluesy, vamping approach that nonetheless includes early instances of hyper speed fugues by both Ra himself and John Gilmore, on tenor saxophone, and Marshal Allen, on alto. Listening to Tyson on these two tracks lays the groundwork for her as a ‘familiar' within Sun Ra's universe, a focal point that keeps center stage only momentarily and which gives the listener a grounding in the traditions Ra is working from and transforming (in literal fashion with the track, \"Contrast\"). This release established Tyson as an elemental part of the Arkestra cosmos.\nTyson is heard again on \"The Satellites Are Spinning\" from Ra's The Solar Myth Approach Vol. I (1971) in another role she would frequently take as part of cosmic chorus or what Ra referred to as his \"Space Ethnic Voices\". Ra often had musicians recede into a chanting and recitative mode that could be either mysterious (as it is on the Solar Myth rendition of \"Satellites\"), or a call and response, as found on \"Nuclear War\" from A Fireside Chat with Lucifer (1981), or an outright boisterous party as it is on the 1972 version of \"Rocket Number Nine\" on Space is the Place. In all these examples, despite being part of a larger group, Tyson's voice is very discernible, there in the midst of others but so, so uniquely identifiable.\nFrom here, things go off on the planetary tangents of Ra's more famously fugue-centered compositions. Tyson is again heard at the very end of \"Strange Worlds\" from The Solar Myth Approach Vol. II (1971) after an intense set of extra-chromatic sonic explorations. Hearing her emerge from the fantastic cacophony has all the power of creation myth and frankly, is a relief, even if it arrives just before the tape gets cut off. You can go only so long in Dante's Divine Comedy without Beatrice or Virgil to hold your hand. Ra must have had a sense of the respite and opening that Tyson's vocals provided, and appreciated their beauty, especially considering The Paris Tapes (1971). Lose your mind and listen to \"Love in Outer Space Part 1\" and \"Part 2\" and then find it again when Tyson comes in with the calm rejoindered vocals on \"Somebody Else's Idea / World\" and \"Space is the Place.\"\nSame thing is true of the 1971 live recording called Black Myth / Out In Space that opens and proceeds with intense Ra calisthenics but then at around the fifty-one minute mark Tyson bobs up and allows you to regain your center of (non)gravity before the improvisational madness and maelstrom begins again 12. Again, it happens at the one hour and twenty-nine minutes mark when Tyson comes forth with a poignant version of \"Walking on the Moon\" - different from the one heard on My Brother the Wind in that we have not bluesy vamp that serves as context; only the feeling of jittery pause before the giant of improvisation returns to stomp all over our bearings (for our own good, mind you!). This recording doesn't let the humor remain absent for too long though: Tyson returns in the playful \"Space Chants Medley,\" leading Ra and the Arkestra in a rhetorical romp that tells us why the Moon should not be the ultimate destination on our collective space travel adventure.\nProbably the most popular toe-dip into the Sun Ra ouevre is the posthumous album Soundtrack to the Film Space is the Place 13, released in 1993 but recorded in Oakland in 1972 (not to be confused with studio release called Space is the Place). Tyson's performance on these tracks is outstanding and illustrates how she is acting as a transposer for Ra's vision for us, as non extraterrestrials. Compare her performance of \"The Satellites are Spinning\" on Solar Myth and The Complete Nothing with the performance here. Her voice is out front and pitched to emphasize the hymnal qualities of the composition. Or compare the version of \"We Travel the Spaceways\" here with Tyson in front with the one the appears on Ra's 1978 Disco 3000 release, which does not include Tyson. Same composition but the emotional presentation of the song is altered with her vocalizations. A stark example of how Tyson acted as a expressive translator of sort is in this call-and-response excerpt from the film, Space is the Place:\nIt is this expressiveness that allows the listener a foothold into the compositions, one from which to experience the more disorienting aspects of Ra's improvisational force. Listening to the Space is the Place soundtrack is a see-sawing experience between states of calm and states of chaos. And almost always Tyson is what ushers in the calm, though she is certainly there in the chaos (listen to the flow between \"Cosmic Forces\" and \"Outer Spaceways Incorporated\") 14. So much of what Sun Ra's music is about is the presentation of a churning unpredictable universe. Whether you consider it chaos or highly-skilled improvisation probably has something to do with your genre proclivities. But even if your ear finds these sounds convivial, the break that Tyson's performance provides over and over again allows listeners a moment to feel they are not so lost and buffeted by the intensity of Ra's talent and cosmic message he conveys through the Arkestra.\nTyson's voice cannot be heard on all Sun Ra releases. She is much more frequently listed as personnel on live recordings than on studio releases. Probably more often than not, she was a visual presence that interpreted the Ra and the Arkestra as dancer and choreographer and of course, costume designer. But when she did take to the mic, she was an aural anchor for audiences, an \"unknown-knowing voice\" 15 assuring us that we are not alone in the chaos. She was the familiar of Ra and often I imagine their relationship similar to how they look together in the footage of the Arkestra in Egypt around the 4 minutes mark 16: stepping forward and stepping backward in sync, animated by sonic, cosmic friendship with each other and all the musicians and dancers that surround them.\nFor those of us that either flail embarrassingly in the unmoored freedom found in the outer void of Sun Ra's space-music or freaked out completely at his notion of an unfixed identity, buffeted and shaped and alive in a cosmic drama (listen to the synthesizer evolutions in the trilogeic \"The Wind Speaks,\" \"Sun Thoughts,\" and \"Journey to the Stars\" on My Brother the Wind, 1970);\nFor those of us that feel like an awkward party guest while witnessing the conversation on \"Enlightenment\" between Sun Ra and John Cage at Coney Island in 1986;\nFor those of us who don't have the arkestral chops to decipher how John Gilmore goes from rock-solid hard bop to avant garde altissimo planes, to (literally) blow apart the post-modal jazz universe;\nFor those of us who know too well the 'Truth' is cobbled together by the hammers of language and power and, as John Szwed so aptly summed up Ra's twist on it, that both those forces are in \"a state of babble\" 17 - but still crave an interlocutor to guide us;\nThere is June Tyson.\nSo thank Ra for her - and thank Her for Ra - for the uninitiated, she illuminates the eye of the needle that you have to pass through to get to the cosmos of Sun Ra and his Arkestra. And once on the other side, she makes sure we don't get lost. Give an ear for listening to this cosmic balladeer and translator. If not for her, the Sun Ra enigma might remain an unknowable unknowable.\n1982's \"Sometimes I'm Happy\" via Spotify\nAll dates are recording dates (rather than release dates) taken from the immensely well-research discography available on the Sun Ra Arkestra website now under the tutelage of Marshal Allen: http://www.sunraarkestra.com/sunradisco/list.php\nJune Tyson- Spotify Playlist\n Szwed, John. Space is the Place: The Lives and Times of Sun Ra. Da Capo Press, 1988. p. 86.\n \"Sun Ra came from the galaxies decades before Isaac Hayes whipped off his multicolored robe and became Black Moses, shackled in gold chains; before Parliament arrived on the Mothership, or Hawkwind took their first ride on the Silver Machine; before Ziggy Stardust fell to Earth from Mars; before Dr. Octagon left his native Jupiter; before Kanye West donned a Margiela mask and longed for his own spaceship to fly past the sky. In his spangled capes and violet cloaks, his painted third eye, his mesh caps and pyramid hats and pharaoh's headdresses and solar antennae, Sun Ra ushered in an utter sense of liberation, mystery, and free expression.\" \"The Interstellar Style of Sun Ra\" by Rebecca Bangel, The Pitchfork Review, https://pitchfork.com/features/from-the-pitchfork-review/9866-the-interstellar-style-of-sun-ra/\n Amiri Baraka, Eulogies, 1996, p. 171, quoted in Space is the Place (ibid) p. 388.\n \"The Cosmic Age\" poem by Sun Ra. The Immeasurable Equation. WAITAWHILE, 2005, pg. 107.\n Szwed, p. 250.\n \"The June Tyson Sessions: Remixperiments with Vocal Materiality and the Becoming-Woman of Cosmic Music\" on Women and Performance, 5/22/2014. https://www.womenandperformance.org/ampersand/ampersand-articles/the-june-tyson-sessions-remixperiments-with-vocal-materiality-and-the-becoming-woman-of-cosmic-music.html\n \"June Tyson: Sometimes I'm Happy\" by Adam Lore (which reprints parts of the interview Phil Shaap of WKCR did with Tyson in 1987). http://www.50milesofelbowroom.com/articles/353-june-tyson.html\n \"The June Tyson Sessions: Remixperiments with Vocal Materiality and the Becoming-Woman of Cosmic Music\" on Women and Performance, 5/22/2014.\n I'm being a little rough on Nick Bazzano here. He has written a very substantial, if academic, paper on June Tyson. Using a combination of gender performativity theory and French heterodox philosophy, he makes a solid case for Tyson being a true citizen in Ra's \"sonic affective impossible.\" The paper is accompanied by his remix of recordings of Tyson (in collaboration with Alex Silva and Willie Avendano) that reasserts, in Bazzano's words, \"the performativity of the materiality of Tyson's voice, that key which Sun Ra pressed, but failed to let resound to its fullest cosmic potential.\" It's hard, however, not to ask if the act of remixing replicates the problem of the conductor playing the musicians as objects, a criticism which Bazzano levels against Ra: \"In the words of Ra's figuration of his musicians as \"keys,\" Ra pressed June, and June sounded, sounding not improvisationally or immanently, but as the exact image of thought (or echo of thought?) that Ra planned.\" Isn't the electronic splicing and remixing of Tyson's vocals very much a \"pressing of keys\" that produces a planned \"image of thought (or echo of thought?)\" of Bazzano's own theoretical stance? Regardless of this quibble, Bazzano's paper is worth the read and the remix (on Soundcloud) worth the listen. https://soundcloud.com/nmb2115/the-june-tyson-sessions.\n \"Remembering June Tyson.\" Art Yard Records. http://artyardrecords.co.uk/remembering-june-tyson-by-knoel-scott/\n \"Satellites Are Spinning\" by Paul Younquist. Paris Review, 8/13/2017. https://www.theparisreview.org/blog/2017/08/03/satellites-are-spinning-notes-on-a-sun-ra-poem/#more-113398\n Black Myth / Out in Space by Sun Ra available here: https://www.youtube.com/watch?v=tvoEQoVDtFk\n Soundtrack to the Film Space is the Place available here: https://www.youtube.com/watch?v=AZZx0oEdFpc\n Ibid, starting at around the 24:50 minute mark above until 30:55.\n \"The Universe Sent Me\" poem by Sun Ra. The Immeasurable Equation. WAITAWHILE, 2005, pg. 404.\n Sun Ra and the Arkestra in Egypt available here: https://www.youtube.com/watch?v=5azChH6Z7QA\n Szwed, p. 384.\n|MAIN PAGE||ARTICLES||STAFF/FAVORITE MUSIC||LINKS|","Data center energy efficiency is at the forefront of hot topics for data centers. And, for good reason. Data centers use a truly astonishing amount of energy each year. The Natural Resources Defense Council (NRDC) noted just how much energy data centers use, “In 2013, U.S. data centers consumed an estimated 91 billion kilowatt-hours of electricity, equivalent to the annual output of 34 large (500-megawatt) coal-fired power plants (and, the NRDC notes, the equivalent of enough electricity to power all the households in New York City twice over). Data center electricity consumption is projected to increase to roughly 140 billion kilowatt-hours annually by 2020, the equivalent annual output of 50 power plants, costing American businesses $13 billion annually in electricity bills and emitting nearly 100 million metric tons of carbon pollution per year.” Improving energy efficiency in a data center is incredibly important but can seem like a daunting task for many data center managers who often are not even certain where to begin, if they have the budget to make changes, or if the changes they make will truly make an impact.\nOne of the first, and most practical ways to begin a shift towards improving energy efficiency is to take a real look at energy usage. What is using the most energy, should it be using that much energy and can anything be eliminated? Often, energy is being wasted on ghost infrastructure or outdated energy-draining equipment. But, as many data center managers know, it can be difficult to keep track of all of the infrastructure in a data center or truly know what is using the most energy of inefficiently using energy. That is why a good DCIM plan is important so that data center managers can work with the most current information rather than outdated information and make well-informed decisions going forward. Once you have sufficiently audited data center energy usage and are able to make well-informed decisions for improvement you can move onto the next step. Next, make immediate changes to improve energy efficiency while also looking at long term improvements. Long term improvements are incredibly important and it is wise to look at how to remain sustainable in the future but while making decisions for the future you can make some immediate changes such as hot aisle/cold aisle arrangements or other containment options that may help improve energy efficiency. While other improvements are more difficult or costly to implement, containment arrangements can be made and executed relatively quickly and will make a big impact. While making immediate changes it is important to get budgetary approval for bigger changes and, once approved, begin moving forward with changes that will help your data center remain efficient in the future. This most likely means upgrading equipment to the most current, energy efficient options. All equipment has a lifespan and once they are getting a big old they will likely become energy inefficient. If you have budgetary approval to make improvements with heating and cooling options as well it is a great choice to improve your cooling ability within a data center because cooling is typically one of the biggest expenses in a data center. Lastly, explore green options like making a switch to more fully utilizing cloud storage or implementing cooling with outside air sources so that you can be as energy efficient as possible now and in the future."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:c128255d-0de6-421a-903a-55a36254fd2a>","<urn:uuid:b4b75be2-0c67-4150-9fbe-83b6f17b1cfb>"],"error":null}
{"question":"What special measures are taken to ensure homogeneity in gold ore CRMs?","answer":"For gold ore CRMs, stringent homogeneity testing is performed using neutron activation analysis at a reduced analytical subsample size of 0.5 grams. The company uses sophisticated proprietary technology to ensure high levels of homogeneity, with industry-leading repeatability demonstrated for all gold ores down to a 0.5 gram analytical subsample.","context":["Certified Reference Materials (CRM)\nMining companies have a responsibility to investors, financiers and the board of directors, to ensure that appropriate samples have been collected and that assays are compliant with NI 43-101 regulations. Monitoring laboratory data improves confidence in ore reserves, drill target selection and company management.\nAs the North American representative for Ore Research & Exploration Pty Ltd (ORE) Analytical Solutions Ltd. provides Certified Reference Materials for Mining & Exploration. ORE is a leading producer of certified reference materials (CRMs) for the mining, exploration and analytical industries. Led by respected expert, Dr Paul Hamlyn, ORE’s core competency is the preparation of the highly regarded OREAS range of ready-to-use CRMs client-supplied materials sourced from advanced projects and operations throughout the world.\nOREAS certified reference materials (CRM) are intended to provide a low cost method of evaluating and improving the quality of precious and base metal analysis of geological samples.\n- To the explorationist, they provide an important control in analytical data sets related to exploration from the grass roots level through to resource definition.\n- To the mine geologist, they provide a tool for grade control in routine mining operations.\n- To the analyst, they provide an effective means of calibrating analytical equipment, assessing new techniques and routinely monitoring in-house procedures.\nWhy Choose CRM's from Analytical Solutions & ORE?\nFull Service Facility\n- ORE offers a complete CRM service, including preparation, packaging and are certified to ISO 9001 standards. ORE’s extensive crushing, milling, blending and packaging facilities are housed in separate buildings dedicated to the production of quality assured CRMs.\nDedicated to preparation of certified reference materials since 1988\n- ORE has been producing certified reference materials (CRMs) for over 20 years. The production of CRMs, both the OREAS range and site specific (custom), is ORE’s core business. In over 20 years ORE has NOT produced a Custom or OREAS CRM that was uncertifiable due to poor repeatability.\nSpecifically tailored for exploration and mining projects\n- The OREAS range of CRMs are prepared from natural ore and rock samples sourced from deposits throughout the world and cover a spectrum of concentrations and mineralisation styles. Developed exclusively for the mining industry, they are distributed worldwide in over 60 countries. Packaging is predominantly in 10 and 60g single-use units using robust laminated foil pouches with erasable labels or in bulk form in lots of 500g to 25kg. Sulphide-bearing materials are packaged under inert gas or vacuum packed to ensure long term stability. Custom packaging is available. ORE CRMs can be used to directly and quantitatively evaluate laboratory analytical precision and accuracy, without the ambiguity associated with products of lesser quality.\nHundreds of satisfied customers worldwide\n- ORE is highly respected internationally, with over 400 clients worldwide. The ORE Assay Standards (OREAS) range of CRMs have been developed specifically for the mining industry. ORE is proud to boast a high level of repeat business from many satisfied clients, ranging from major mining corporations to mid-tier and junior miners along with numerous commercial laboratories.\nExperience with all ore types, tailings and metallurgical products\n- CRMs service benefit from our extensive experience in the production of precious, base metal, REE and lithogeochemical materials. We have produced CRMs for numerous ore types including: gold, gold-silver, gold-copper-(molybdenum), copper, copper-gold-uranium, copper-molybdenum, iron, nickel laterite, nickel sulphide, zinc oxide, Zn-Pb-Ag sulphide and oxide, polymetallic (Au-Cu-Zn-Pb-Ag), platinum group elements, tungsten, uranium, rare earth elements (REE), manganese, phosphate, concentrates, tails and related metallurgical materials, tin, molybdenum, chromium, phosphorous, niobium-tantalum and multi-element lithogeochemistry.ORE is able to offer quality assurance on the homogeneity of all CRMs produced, regardless of the nature of source materials.\nGuaranteed homogeneity of even the most difficult ores\n- Even for nuggety gold source materials the superior level of homogeneity exhibited by ORE’s CRMs means that observed variance can be attributed almost exclusively to laboratory measurement error. Homogeneity test results are included in all Certificates of Analysis with all ore grade gold CRMs undergoing stringent homogeneity testing by neutron activation analysis at a reduced analytical subsample size of 0.5 grams.\nPreparation and certification to ISO standards\n- Sophisticated proprietary technology ensures high levels of homogeneity, with industry-leading repeatability demonstrated for all gold ores down to a 0.5 gram analytical subsample. All CRMs are prepared and certified to ISO standards and are supplied with comprehensive documentation. ORE uses an established network of world class analytical laboratories in round robin certification programs generally by evaluation at a minimum of 15 recognized mineral testing laboratories.\n- Joint ventures to develop CRMs available on request"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d455fa09-75c2-4e4e-b31b-f05255aede11>"],"error":null}
{"question":"How do moisture-related preparation requirements compare between sealing stamped concrete surfaces and sealing brick paver patios?","answer":"Both surfaces require careful attention to moisture management but have different specific requirements. For stamped concrete, the surface must be completely clean and dry before sealing, with special attention paid to low spots in the stamp pattern where water may accumulate. The concrete needs several weeks of curing time depending on weather conditions. For brick pavers, after power washing, the patio requires multiple hot sunny days to thoroughly dry out, as the ground beneath the pavers becomes soaked. This drying period is crucial because sealing over trapped moisture can cause the entire project to turn milky-white. Additionally, with pavers, the joint sand must be completely dry before applying sealer to ensure proper stabilization.","context":["- Concrete Stamps\n- Get the Look - Stamping Pictures\n- Buying Tips for Concrete Stamping Tools: Advice that will help you make smart choices\n- Concrete Stamp Cost\n- Types of Concrete Stamps: A comparison chart of popular stamps & patterns\n- Stamping & Texturing Tools\n- Concrete Stamp Mats\n- Concrete Texturing Skins\n- Medallion Stamps\n- Texture Rollers\n- Step Forms & Liners\n- Installing Stamped Concrete\n- How to Stamp Concrete\n- Coloring Stamped Concrete\n- Sealers for Stamped Concrete\nSealers for Stamped Concrete\nThe final step in the installation of stamped concrete is to apply a sealer suitable for exterior exposure conditions. Sealers provide several important benefits:\n- They block the penetration of stains, oil, deicing chemicals and other contaminants.\n- They protect stamped concrete surfaces from abrasion and wear.\n- The make stamped concrete easier to clean and maintain.\n- They enrich the color intensity of stamped concrete, whether the color is integral or provided by a dry-shake hardener and antiquing release.\n- They add sheen to the surface, ranging from satin to high gloss.\nHow to choose the right sealerThe primary type of sealer used for exterior stamped concrete flatwork is a solvent- or water-based acrylic. Acrylic sealers are easy to apply, economical, and most important, breathable, allowing moisture in the slab to escape. They also are UV resistant and non-yellowing. Solvent-based acrylics generally perform better than water-based products for outdoor use because they penetrate well and are less likely to turn white, or milky. (Learn more about acrylic sealers.)\nBefore buying a sealer, check with the manufacturer to make sure the product is appropriate for your particular project. Here are some important questions to ask:\n- What conditions will the sealer be exposed to?\n- Is the sealer breathable?\n- What is the drying time?\n- What is the coverage rate?\n- What type of finish and surface appearance do you want to achieve?\n- What is the life expectancy of the sealer, and does the manufacturer provide a warranty?\n- How easy is the sealer to apply?\n- See more frequently asked questions here.\nHow to apply sealer to stampedBefore applying sealer to stamped concrete, the surface must be clean and dry to ensure good adhesion. Also, you should allow the concrete to cure sufficiently -- generally several weeks after placement, depending on weather conditions. With stamped concrete, it’s especially important to make sure the surface is free of moisture in low spots of the stamp pattern, where water may accumulate.\nThere are several tools you can use to apply sealer to stamped concrete, depending on the product you’re using. Pump-up or low-pressure sprayers are best for applying one-part, solvent-based sealers with a solids content below 35%. Airless, or low-pressure high-volume (LPHV) sprayers, can handle both water- and solvent-based sealers and permit very controlled application rates, allowing large areas to be sealed in the shortest time. You can also apply water- or solvent-based sealers with a paint-type roller. To work the sealer into the textured surface and stamp pattern depressions, be sure to use a roller with a fairly thick nap (about 3/8 inch). One of the best application techniques when sealing stamped or textured surfaces is to combine spraying followed back rolling to keep the sealer from settling in low spots and to help distribute the sealer uniformly.\nLearn more about choosing the best applicator for concrete sealer.\nStamped concrete sealer dos and don’ts\n- DON’T apply the sealer too thickly, or it will just lie or puddle on the surface rather than penetrate. Acrylic sealers are designed to go down very thinly, at thicknesses of only 1 to 2 mils.\n- DO apply sealer in two thin coats rather than one thick heavy coat to achieve more uniform coverage.\n- DON’T apply a solvent-based sealer over a water-based product because the solvent can eat or soften the existing water-based sealer.\n- DO refer to the sealer manufacturer's application guidelines for specifics on surface preparation, recommended application tools, and dry times between coats.\nFor more information on concrete sealer application and troubleshooting, see Fixing Common Sealer Problems.","When my neighbor first decided to install a paver-block patio in the back of his home, he chose paver stones for several good reasons: he thought they would last longer than other materials like wood or poured concrete, they have a really “rich” elegant look to them, and most of all, he thought that they would be maintenance free. Well, he almost had it right. They do look great and they’re incredibly durable, but it wasn’t long before he learned that they aren’t completely maintenance free.\nAfter several seasons of rain, snow and sun, the patio just didn’t look as good as it first did when he had it installed. The color in the pavers was beginning to fade from the sun in some areas and the sand between the stones had been washed away by rainwater. Weeds had started growing in the gaps between the pavers and he even began to see ant colonies starting up, using his precious paver sand as their new home.\nThe solution to his problem was fairly simple. It was time to clean the brick paver patio up and seal it for protection, and though he was somewhat reluctant at first to attempt this project on his own, as I began walking him through the various steps to sealing pavers, he decided it didn’t sound that tough and he ought to be able to seal his patio himself.\nI concurred. “Just follow these simple brick paver sealing steps, and when you’re done your patio will look like it was done by the pros!”\n1). First, take your time\nI can’t stress this point enough. From the beginning to the end, with every step in between, take your time. When you rush a project you make mistakes, and mistakes are what the pros are able to avoid. Cleaning the surface a little better or rinsing a little more could mean the difference between a successful project and a failed one. Maybe it’s allowing just a little more time between coats for your sealer to dry or waiting a few more hours before re-installing your furniture. Take your time and the chances are much better that you’ll be satisfied with the finished result.\n2). Acids can be great cleaners\nOften times when you inspect a paver patio prior to washing, you’ll notice stains. Leaves that have been left to lie on the patio can leave tannin stains. Iron furniture that sits on the patio will most likely leave rust stains. Any stains you find need to be removed prior to sealing pavers or they will be sealed in and become permanent, so now is the time to work on them. The simplest way to get the stains out is by using an acid cleaner. Conventional muriatic acid works well and it’s cheap, but it burns your skin and the vapors from it are terrible. Instead, I use a “safer” cleaner, such as MasonrySaver Safer Brick & Masonry Cleaner.\nFor severe stains, I simply pour a little cleaner at full strength on the stain and give it a few minutes to work before rinsing. If the whole patio is fairly dirty, dilute the cleaner 4:1 (four parts water to one part cleaner) and, using a garden-type pump up sprayer, spray the entire surface with cleaner. Allow the cleaner to remain wet on the surface for 5 minutes or so, and then rinse using a power washer.\n3). Clean, clean, and clean some more\nIn regards to the prep work, sealing pavers isn’t much different than staining your deck or painting your house. Much of the quality of the job comes down to how well you prepare the surface.\nPatios that have weathered will often have mildew, moss, and algae growing on the stones and in the sand between the joints. Weeds and any other foreign debris need to be removed; and for a patio, that means a good cleaning. The easiest way to clean your brick patio is with a power washer.\nMake certain that you select a washer with enough power to do the job. I try to never use anything rated less than 2400 p.s.i. Select a tip that will fan the water out slightly, 15°- 25° or so, and always work in sections.\nPatios should be constructed so that water runs away from the home. So begin working near the house or the high side of the slope of the patio, rinsing debris down the patio’s natural flow, away from the house. You’ll loose some of the sand between the joints during the cleaning process and that’s okay. We’ll replace the sand once the patio is cleaned up. Just be careful not to blow out more sand than is necessary while you’re doing your cleaning.\n4). Level uneven pavers\nWith the patio now cleaned, look around and inspect the condition of the pavers. Over time, paver stones can settle or shift, making them unlevel. If you have some bricks that are out of place or unlevel, use a screwdriver and a stiff putty knife to pry them out. Once removed, you can add a little sand, smooth it out with the blade of your putty knife, and re-install the bricks. A couple of taps with a rubber mallet should make the bricks level once again.\n5). Break time\nThis is the part you’re going to love. Go take a break! Remember when I said to take your time? Well, that advice is very applicable now. After all of the cleaning that you did, the ground under those pavers is soaking wet and needs to dry out before proceeding any further. Give your brick paver patio a couple of good hot sunny days to dry out before applying any sealer. If you don’t, you could end trapping moisture under the sealer and the whole project will turn a milky-white color!\n6). Re-grout the joints\nThe last remaining step in getting the patio ready to seal is to re-grout the joints between the stones with fresh sand. This step isn’t hard, but it does take a little time. To do this, I use a fine grade of run-of-the-mill play box sand. Pour a small amount on the patio and begin sweeping it into the joints using a push broom. Sweep in both directions…back-to-front and left-to-right to ensure that the joints get completely filled. As the sand gets used up by filling the joints, pour a little more onto the patio and keep sweeping. You’ll most likely need to brush over the entire patio several times to get the joints all evenly filled.\n7). Take a look at waterborne sealers\nWhen you choose your sealer, you’ll find that the two choices are solvent-based and water-based. I hate solvent-based sealers for the following reasons:\n- They’re bad for our environment\n- If they get on my grass or flowers they kill them\n- They smell terrible\n- Everything has to be cleaned up with messy paint thinner\nWater-based sealers are a much better choice. Besides being both easy to use and easy on the environment, they offer another amazing advantage – they “stabilize” the sand. When water-based sealers cure, they will harden the joint sand and lock it into place. This helps to keep the joint sand from washing out during future rains. An example of this type of product is MasonrySaver Paver Sealer. It’s a great brick paver sealing product that will stabilize the sand as well as give a rich, paver sealer wet look. Some of the older water-based sealers had a tendency to “blush”, or cloud up if they encountered any moisture or rain, but the MasonrySaver product is much more forgiving and virtually blush-resistant.\n8). Read the can and follow the directions\nEvery manufacturer’s product can be little different so always read the directions. It only takes a few minutes but it will ensure that you have all the right information before you get started. It will give you valuable information such as how many coats of sealer to apply, how long to wait between coats, how to properly apply the product, and how to clean it up. There are a number of variations to these questions so read the label first and you’ll be likely to get it right the first time.\n9). Brick paver sealing\nNow the fun stuff begins. This section will walk you through how to seal pavers on your brick patio.\nTo do this, I love to use a garden-type pump up sprayer, but I’ll warn you in advance, don’t buy a cheap one! The better sprayers (they usually cost over $50 bucks) have a metal fan-style spray tip and do a much better job than the inexpensive weed sprayers. When you’re looking for a sprayer, the key here is to look for a “fan-style tip”.\nAny of the patio edges that need to be cut in can be done so using a 4” nylon brush. After cutting in the edges you’re ready to start spraying. Water-based sealers normally require that two coats be applied. So coat the entire patio, then wait for the sealer to dry enough that it can be walked on (normally an hour or two). After an hour or two has passed, do the same thing all over again, applying two coats in total. If you put the paver sealer on too heavy and the sealer starts to puddle, use a brush or dry roller to get rid of the puddling by simply brushing or rolling it out.\n10). More is definitely not better\nSo if a little is good then a lot must be better, right? No. Remember, we’re sealing concrete or brick paver stones and these stones are sitting on a bed of sand. When the sand gets wet and the sun comes out, it’s going to draw the moisture out of the wet sand and paver stones. These pavers need to be able to “breathe”, or allow the water vapor pass through them. When too much sealer is applied, the water vapor can have trouble passing through, resulting in a white “blushed” appearance, or even potentially peeling.\nDon’t be fooled. I know how great this stuff can look if you really lay it on heavy and how cool that high gloss finish is when sealing pavers, but blushing and peeling stinks! Limit your application to two coats and you won’t have a problem.\n11). Maintaining your paver patio sealer\nOkay, great job. Now there’s just one more thing – paver sealer maintenance. Blow the leaves off when they fall to keep them from staining, rinse the dirt and debris that seems to just show up over time, and, as the weather begins to dull down the finish and the patio loses its luster, simply rinse it off with a garden hose and apply a light maintenance coat of paver sealer. Seal the paver stones before the patio gets too weathered. It’s really that simple.\nSo that’s it – “How to seal pavers on your brick patio”. Go ahead and try sealing pavers yourself and see what the results are when you follow these tips. You might just surprise yourself!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7bb3b437-f650-46ec-9716-3fbba1d06c7d>","<urn:uuid:d7c61976-872f-4a37-b1bd-f16d92407ff1>"],"error":null}
{"question":"How do scientists' detection methods differ when searching for Kordylewski clouds versus Earth Trojan asteroids, and what makes these observations challenging?","answer":"The detection methods and challenges differ significantly. The Kordylewski clouds were confirmed using linearly polarizing filter systems fitted to camera lenses and CCD detectors, which can detect polarized light reflected from dust. These clouds are considered among the toughest objects to find despite being as close as the Moon. For Earth Trojans, detection is challenging because they appear close to the Sun in the sky during most of their orbits. They have been spotted using various methods, including NASA's Wide-field Infrared Survey Explorer spacecraft for infrared scanning, and dedicated searches by programs like the Catalina Sky Survey. Both space missions (Osiris-Rex and Hayabusa 2) have also conducted searches at the L4 and L5 points for Trojans.","context":["Astronomers confirm existence of two faint dust clouds orbiting Earth alongside the moon\n- Pseudo-satellites were reported in 1961 by astronomer Kazimierz Kordylewski\n- But, scientists have been unable to find them since, casting doubt on existence\n- A Hungarian team has now found them in a point known as L5, not far from Earth\nResearchers in Hungary have detected what could be a pair of elusive dust clouds orbiting just 250,000 miles from Earth, putting to rest a long-running debate on their existence.\nThe two pseudo-satellites were first reported in 1961 by Polish astronomer Kazimierz Kordylewski, but remained controversial because they are so difficult to spot.\nNow, scientists at a private observatory in Hungary say they’ve finally found the faint objects, revealing they sit in a semi-stable points that forms a triangle with the Earth and Moon.\nResearchers in Hungary have detected what could be a pair of elusive dust clouds (illustrated in the graphic above) orbiting just 250,000 miles from Earth, putting to rest a long-running debate on their existence\nThe Kordylewski clouds, named after the researcher that first reported their existence, were finally confirmed with the use of linearly polarizing filter system, fitting on a camera lens and CCD detector at the observatory.\nPolarizing filters transmit light with a particular direction.\nThe clouds were said to sit in a region known as a Lagrange point, specifically in one dubbed L5.\nThis is one of five points of stability in the Earth-Moon system, where gravitational forces lock objects in the vicinity in their relative position.\nBack in 1961, Kordylewski observed two faint clusters at L5 – but, they haven’t been seen since. That is, until now.\n‘The Kordylewski clouds are two of the toughest objects to find, and though they are as close to Earth as the moon are largely overlooked by researchers in astronomy,’ says Judit Slíz-Balogh.\n‘It is intriguing to confirm that our planet has dusty pseudo-satellites in orbit alongside our lunar neighbour.’\nThe clouds were said to sit in a region known as a Lagrange point, specifically in one dubbed L5 (shown above). This is one of five points of stability in the Earth-Moon system, where gravitational forces lock objects in the vicinity in their relative position\nIn the new study, researchers led by Gábor Horváth of Eötvös Loránd University modeled the clouds to hone in on the best way to find them.\nThe team captured exposures of their purported location, revealing polarized light reflected from dust.\nAccording to the team, the patterns line up with their earlier predictions, and Kordylewski’s initial observations.\nWHAT IS INTERSTELLAR DUST?\nWe often think of the vast areas of space between the stars as being completely empty. But this is not the case.\nMuch of the space between the stars is filled with atomic and molecular gas, primarily hydrogen and helium, and tiny pieces of solid particles or dust.\nThe dust is composed mainly of carbon, silicon and oxygen. In some places this interstellar material is very dense, forming nebulas.\nThis dust is made up of small solid particles, like fine sand, and has a major effect on the formation and evolution of galaxies.\nInterstellar dust is an important constituent of the galaxy.\nIt obscures all but the relatively nearby regions in visual and ultraviolet wavelengths.\nIt absorbs the photons in these wavelengths and reradiates the absorbed energy in the far-infrared part of the spectrum.\nThis means it provides around 30 per cent of the total luminosity of the galaxy.\nDoes interstellar dust exist from the start of the universe?\nThe initial solids created at the start of the solar system consisted entirely of shapeless silicate, carbon and ices.\nThis interstellar dust was mostly destroyed and then reworked by processes that led to the formation of planets.\nSurviving samples of this pre-solar dust are most likely to be preserved in comets that formed in the outer solar nebula, meaning they stayed clear of the destructive forces that created planets.\nThese are tiny glassy grains called GEMS (glass embedded with metal and sulfides) that are typically only tens to hundreds of nanometers in diameter, or less than a hundredth of the thickness of a human hair.\nResearchers led by the University of Hawai'i at Manoa made maps of the element distributions using electron microscopes and found that the carbon in them decomposes when exposed to gentle heating.\nThis suggests that these grains could not have formed in the hot inner solar nebula.\nInstead they are much more likely to have formed in a cold radiation-rich environment, such as the outer solar nebula or pre-solar molecular cloud.\nAfter ruling out optical phenomena and other objects that could be mistaken for the clouds, the researchers say they’ve finally confirmed their existence.\nThe exact source of the dust clouds remains uncertain, though the experts suspect they are regions where interplanetary dust collects.\nL5 and its counterpart L4 are being eyed as possibly locations for orbiting space probes, given their stability.\nThe new findings mean the dust will also have to be taken into consideration as these plans move forward, to determine whether the material poses a threat to equipment or astronauts.","A recently discovered asteroid appears to be an Earth Trojan, orbiting a gravitationally stable area with only one other known occupant.\nEarth has a second Trojan asteroid sharing its orbit, reports amateur Tony Dunn on the Minor Planet Mailing List. The asteroid, dubbed 2020 XL5, is a few hundred meters across and its orbit is tied to a gravitationally stable region ahead of Earth in its orbit.\nTrojans are asteroids gravitationally locked to stable Lagrange points either 60° ahead (L4) or behind (L5) the planets in their orbits around the Sun. 2020 XL5 was found around the L4 point. Massive Jupiter has more than 9,000 Trojans. In theory, Trojan orbits would be stable around every planet except Saturn, where Jupiter’s gravity pulls them away. So far, Trojans have been found sharing orbits — at least temporarily — with Neptune, Uranus, Mars, Venus, and Earth.\nEarth Trojans are hard to find because during most of their orbits, they appear close to the Sun in the sky. Not only that, but the gravitational resonance does not hold them in lockstep at 60° ahead and behind of the Earth, explains Dunn. Instead, the objects trace paths around the L4 and L5 points, which are themselves moving as Earth orbits the Sun.\nNASA's Wide-field Infrared Survey Explorer spacecraft spotted the first Earth Trojan, 2010 TK7, also locked to the L4 point, in October 2010 when it scanned the infrared sky 90° from the Sun. Two other observers recovered it a few months later with the Canada-France-Hawaii Telescope. It's slightly smaller than 2020 XL5.\nThe orbits of our two Trojans are best visualized along with that of Earth’s and, in the case of 2020 XL5, the orbits of all the inner planets. When viewed relative to Earth, 2010 TK7 drifts between a spot close to Earth to the L3 point on the other side of the Sun from Earth, but it doesn’t pass through the L4 point. The orbit of 2020 XL5 ranges more widely, drifting inward to inside Venus’s orbit and outward almost to Mars.\nThe wide-ranging orbit shows “[2020 XL5] is almost certainly a garden-variety bit of rock that went [close to] Venus and got perturbed into an orbit with a period very close to one year,” says Bill Gray of Project Pluto.\nAldo Vitagliano, a retired Italian chemist and author of the Solex orbital software, said on the MPML that the orbit should remain stable for 2,000 to 4,000 years, but gravitational tugs would eventually move it to another orbit. So far 2020 XL5 has only been observed for only a few weeks, and amateur astronomer Sam Deen says we may have to wait until November or December until more observations can be made to pin down its orbit.\nThe first Earth trojan, 2010 TK7, comes within 20 million kilometers (12 million miles) to Earth every few hundred years; it is currently drifting away. Models show its orbit is stable enough to stay in a one-to-one resonance with Earth for about a quarter million years. While there are Earth Trojan orbits that are stable for the life of the solar system, no objects have been found occupying them.\nTwo spacecraft on their way to visit near-Earth objects searched Trojan regions in 2017, but NASA's Osiris-Rex found nothing at L4 and the Japanese Hayabusa 2 found nothing at L5. However, the observations were not definitive, and in 2019 Renu Malhotra (University of Arizona) wrote that the Earth could still have up to several hundred Trojans at least a few hundred meters in diameter, amounting to several percent of the some 10,000 near-Earth objects of that size.\nA population of Earth Trojans should have survived since the planet formed if its orbit hasn’t changed since then, she says. Their existence — or lack thereof — has other implications, too. Searching for ancient Trojans could help explain why the leading hemisphere of the Moon has about 70% more young craters than the trailing side, a difference current models can’t explain. Earth Trojans slowly escaping from their orbits might account for the extra young craters.\nNow, Malhotra says, astronomers are stepping up their search for Earth Trojans. The Catalina Sky Survey has expanded the area it covers, and a group at the Vera C. Rubin Observatory is also planning observations once that observatory comes online in a year or two."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:01bb55b3-a054-4bc4-97b2-80f37c8c3988>","<urn:uuid:5a77b3c0-d3ef-4007-b7ba-c1ea26621926>"],"error":null}
{"question":"How does the DDA protect against discrimination in premises access, and what specific design elements are recommended for vision-impaired accessibility?","answer":"Section 23 of the DDA makes it unlawful to discriminate on disability grounds in providing access to premises that the public can enter or use. For vision-impaired accessibility specifically, recommended design elements include: continuous accessible paths of travel free from obstacles, proper contrast in different forms (color, texture, and luminance), appropriate lighting considering different sensitivity levels, safe stair design to prevent unexpected drop-offs, clear signage with tactile alternatives, and tactile ground surface indicators (TGSI's) for hazard and directional information.","context":["The Disability Discrimination Act 1992 (DDA) was an act passed by the Parliament of Australia in 1992 to promote the rights of people with disabilities in certain areas such as housing, education and provision of goods and services. It shares a common philosophy with other disability discrimination acts around the world that have emerged in the late 20th and early 21st century, as well as earlier civil rights legislation designed to prevent racial discrimination and sex discrimination.\nInformation on The Australian Disability Discrimination Act 1992 which aims to promote equal opportunity and access for people with disabilities in Australia.\nIn the late 20th and early 21st centuries, a number of countries have passed laws aimed at reducing discrimination against people with disabilities.\nThese laws have begun to appear as the notion of civil rights has become more influential globally, and follow other forms of anti-discrimination and equal opportunity legislation aimed at preventing racial discrimination and sexism which began to emerge in the second half of the 20th century.\nThe Australian Disability Discrimination Act 1992 makes disability discrimination unlawful and aims to promote equal opportunity and access for people with disabilities in Australia.\nBasically the Act provides for:\nA stronger whole-of-government, whole-of-community response to the rights and needs of people with a disability.\nA framework for the provision of high quality services and supports for people with a disability.\nThe Australian Disability Discrimination Act protects individuals across Australia from direct and indirect discrimination in many parts of public life, such as employment, education and access to premises.\nThe Act makes harassment on the basis of disability against the law. It also protects friends, relatives and others from discrimination because of their connection to someone with a disability.\nFor instance Section 23 of the Australian Disability Discrimination Act makes it unlawful to discriminate on the grounds of disability in providing access to or use of premises that the public can enter or use.\nBuilding access issues also arise under other DDA provisions including in relation to employment, access to services, and accommodation.\nSection 25 of the DDA makes it unlawful to discriminate in provision of accommodation on the grounds of disability of a person or his or her associates.\nObjectives of the DDA Australia\n\"To eliminate, as far as possible, discrimination against persons on the ground of disability in the areas of: work, accommodation, education, access to premises, clubs and sport; and the provision of goods, facilities, services and land; and existing laws; and the administration of Commonwealth laws and programs; and to ensure, as far as practicable, that persons with disabilities have the same rights to equality before the law as the rest of the community; and to promote recognition and acceptance within the community of the principle that persons with disabilities have the same fundamental rights as the rest of the community.\"\nUnder the Act, individuals can lodge complaints of discrimination and harassment with the Australian Human Rights Commission.\nComplaints made under the DDA are made to the Australian Human Rights Commission (previously known as the Human Rights and Equal Opportunity Commission, HREOC), which also handles complaints relating to the Racial Discrimination Act 1975, Sex Discrimination Act 1984, Age Discrimination Act 2004 and the Human Rights and Equal Opportunity Commission Act 1986.\nIn Australia, national and state laws cover equal employment opportunity and anti-discrimination in the workplace. You're required by these laws to create a workplace free from discrimination and harassment. It's important that as an employer, you understand your rights and responsibilities under human rights and anti-discrimination law. By putting effective anti-discrimination and anti-harassment procedures in place in your business you can improve productivity and increase efficiency.\nOver the past 30 years the Commonwealth Government and the state and territory governments have introduced anti-discrimination law to help protect people from discrimination and harassment.\nThe following laws operate at a federal level and the Australian Human Rights Commission has statutory responsibilities under them:\nCommonwealth laws and the state/territory laws generally cover the same grounds and areas of discrimination. However, there are some 'gaps' in the protection that is offered between different states and territories and at a Commonwealth level.\nIn addition, there are circumstances where only the Commonwealth law would apply or where only the state law would apply.","When vision is impaired it affects people's mobility in different ways. This has important implications for design in general and for accessibility in particular. A person with impaired vision may not be able to see the ground at his or her feet, detect hazards on the footpath, look ahead in his or her direction of travel, or recognise steps and changes in the ground level.\nAccessibility and Universal Design\nAccessibility often refers to the design of products, devices, services, or environments for people with a disability. The concept of accessible design incorporates both “direct access” (i.e. unassisted) and \"indirect access\" where a person can use assistive technology (e.g. screen readers) to access mainstream information.\nAccessibility also has a strong relationship with the concept of ‘universal design’ - the process of creating products and environments that are usable by people with the widest possible range of abilities, operating within the widest possible range of situations. This concept promotes that all things and places should be accessible to all people - whether they have a disability or not.\nGuide Dogs NSW/ACT and Good Design\nGuide Dogs NSW/ACT believes that good environmental design benefits the community as a whole, not just those with vision impairment. Good design uses natural features and materials without modifying the environment unnecessarily. This consistency of design assists people with vision impairment to be safe and independent when negotiating their way around the community.\nEffective design and construction can significantly improve access, so that all pedestrians may travel more safely and more independently. Considering design features at the outset is important as it can eliminate the need for expensive modifications at a later date.\nAccess Advice Service\nGuide Dogs NSW/ACT regularly deals with individual requests that come from people with vision impairment and assists them in addressing their access issues or problems in their community. In situations where the person has not been successful in resolving the issue alone, Guide Dogs NSW/ACT will help advocate on their behalf.\nGuide Dogs NSW/ACT also deals with access requests from Councils, Architects, Access Consultants and Transport providers. We are approached for help with interpreting the functional implications of the requirements – or for advice on ‘best practice’ where no Codes or Standards exist (e.g. in the outdoor domain).\nGuide Dogs NSW/ACT regularly provides advice on good design features that benefit people with impaired vision. Aspects to be addressed include:\nRoad crossings –Poorly designed or constructed road crossings are the single most hazardous aspect in the built environment for people who have impaired vision.\nA continuous, accessible path of travel – Obstacles placed on the footpath can pose a serious threat to people who are blind or have impaired vision. An accessible path of travel makes moving around much easier and safer.\nContrast – For people who have impaired vision, contrast clues are useful for independent travel and for detecting potential hazards. Contrast exists in different forms with the main types relating to colour, texture and luminance.\nLighting – People who have impaired vision often have very different functional needs for lighting. While some people need lots of light to see clearly, others require lower light levels because they are particularly sensitive to glare.\nStairs – Unexpected \"drop-offs\" are among the biggest fears of people who have impaired vision. The drop can be a step, stair or platform edge at a railway station. Effective design and construction will assist people to negotiate stairs and other drop-offs safely and independently.\nSignage – Many people who have impaired vision have some residual vision – some are able to read print signage, however many are not. It is therefore necessary, to provide alternatives to signage to ensure effective communication. These may include tactile symbols, verbal announcements or one-on-one assistance to identify or get to a specific location.\nHazard minimisation – People with vision impairment often encounter hazards and obstacles as they travel through the environment. Simple design solutions that minimise hazards will benefit the entire community.\nTactile indicators or tactile ground surface indicators (TGSI’s) – Tactile ground surface indicators (TGSI’s) are raised domes and stripes placed in patterns on the ground to provide tactile information. Their colour and luminance contrast provides information to people with vision impairment about hazards and directions. Guide Dog can provide advice on the use and placement of TGSI’s.\nTo find out more about access issues for people with vision impairment and how you can improve environmental design, contact our Community Education Department on (02) 9412 9300"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:89b8f453-45df-4b68-a1dd-55ceabce7375>","<urn:uuid:41ff914f-8d34-4113-86c4-a96f65545b3c>"],"error":null}
{"question":"I'm studying Islamic history and psychology: what are the key objectives of studying Prophet Muhammad's life history, and how does this relate to modern social media's impact on self-reflection?","answer":"There are five key objectives in studying Prophet Muhammad's life history: understanding his personality, finding a noble exemplary figure for all aspects of life, easier understanding of the Quran, seeing a complete portrait of Islamic teachings, and learning from his success as a preacher. Interestingly, while studying religious figures promotes self-reflection, modern social media platforms can inhibit it through dissociation - a state where users lose track of time and become disconnected from their actions, with 42% of users reporting feeling 'all-consumed' during social media use.","context":["Reading the life history of the Prophet Muhammad is not limited to spelling out the names of events, places, dates and other formalist-textual things. But, there is a more substantive purpose and value than any past event, in which the Prophet (s) became his main figure.\nThe life history of the Prophet Muhammad or commonly termed as Sirah Nabawiyah, is a series of life stories of the Prophet Muhammad from the moment he was born until the end of age. In fact, some historians write long before the prophet’s birth.\nwhat is the purpose of reading the life history of the Prophet? To know the full event name of the date and place? Or what?\nreading history including the life history of the Prophet Muhammad, not limited to knowing the textual things above. Reading history can not be separated from the name of events, dates, places and other textual issues. But, behind the data, there are substantive things that we can quote.\nIn fiqh al-Sirah (pp. 21-22), Shaykh Sa’id Ramadhan al-Buthi (d. 2013 AD) explains the more basic objectives in studying the life history of the Prophet (s). He said, there are at least five purposes that we need to apply in reading Sirah Nabawiyah.\n- 1 First, understanding the personality of Prophet Muhammad through the course of his life.\n- 2 Second, by knowing the life story of the Muhammad, one will find a noble exemplary figure in all aspects of life.\n- 3 Third, make it easier to understand the Qur’an,\n- 4 Fourth, by studying the life story of the Prophet Muhammad, one will be able to see a complete exemplary portrait in Islamic teachings.\n- 5 Fifth, an example for the da’i (preachers) in the life of the Messenger of Allah.\nFirst, understanding the personality of Prophet Muhammad through the course of his life.\nThis is important because not only introduces the Messenger of Allah as a genius and respected figure among his people, but also knows the Prophet Muhammad as a messenger of God who is legitimized by revelations that come down directly from His Presence.\nSecond, by knowing the life story of the Muhammad, one will find a noble exemplary figure in all aspects of life.\nUndoubtedly, all the deeds of life have been done by the Messenger of Allah. That is what Allah has asserted in the verses of the Qur’an which reads,\nلَقَدْ كَانَ لَكُمْ فِي رَسُولِ اللَّهِ أُسْوَةٌ حَسَنَةٌ\nIt means, “Surely in the Messenger of Allah there is a good example for you.” Qs. Al-Ahzab : 21)\nInterpreting the verse above, Ibn Katheer explained that this verse is the most important basis in making the Messenger of Allah as a role model. Both in speech, deeds and all the twists and turns of life.\nTherefore, Allah commands people to follow the Prophet (s) during the battle of Ahzab in terms of patience, firmness, alertness, and struggle, and still waiting for a way out of Allah swt. See Tafseer Ibn Katheer, juz’ 3, p. 483.\nThird, make it easier to understand the Qur’an,\nbecause most of the verses of the Qur’an have been explained by the hadiths of the Prophet. The story of Sa’ad ibn Hisham ibn Amir confirms this.\nSa’ad once asked Siti ‘Aisha about the prophet’s morals. Then Siti ‘Aisha replied,\nكَانَ خُلُقُهُ الْقُرْآنَ أَمَا تَقْرَأُ الْقُرْآنَ قَوْلَ اللَّهِ عَزَّ وَجَلَّ (وَإِنَّكَ لَعَلَى خُلُقٍ عَظِيمٍ)\nIt means, “His morality is the Qur’an, do you not read the Qur’an, the Word of Allah Azza wa Jalla: and indeed you are above the great ethics.” (HR. Ahmad)\nFourth, by studying the life story of the Prophet Muhammad, one will be able to see a complete exemplary portrait in Islamic teachings.\nWhether it is a matter of religion, law, or morals. These three components are certainly in the Prophet (s).\nFifth, an example for the da’i (preachers) in the life of the Messenger of Allah.\nWe know that the Messenger of Allaah (peace and blessings of Allaah be upon him) was sent as a preacher among his people. The Prophet (pbuh) is the most successful da’i in history.\nIn fact, Shaykh Musthafa as-Shiba’i compiled a book of the history of the Prophet (s) that outlines the points of study that can be used as examples for preachers in the book entitled Sirah al-Nabawiyah Durus wa ‘Ibar.\nSuch are some important objectives that become a reference in studying the life story of the Prophet Muhammad saw. Reviewing Sirah Nabawiyah, means reviewing all the details of the life of the greatest figure with a complete example of life for each breath of life.","Disrupted sleep, decrease life satisfaction and poor vanity are just some of the adverse psychological well being penalties that researchers have linked to social media. In some way the identical platforms that may assist folks really feel extra linked and educated additionally contribute to loneliness and disinformation. What succeeds and fails, scientists say, is a perform of how these platforms are designed. Amanda Baughan, a graduate pupil specializing in human-computer interplay on the College of Washington, research how social media triggers what psychologists name dissociation, or a state of diminished self-reflection and narrowed consideration. She offered outcomes on the 2022 Affiliation for Computing Equipment Laptop-Human Interplay Convention on Human Elements in Computing Techniques. Baughan spoke with Thoughts Issues editor Daisy Yuhas to elucidate how and why apps want to alter to present the individuals who use them better energy.\n[An edited transcript of the interview follows.]\nYou’ve proven how altering social media cues and shows may enhance well-being, even when folks strongly disagree on points. Are you able to give an instance?\nThe design of social media can have quite a lot of energy in how folks work together with each other and the way they really feel about their on-line experiences. For instance, we’ve discovered that social media design can truly assist folks really feel extra supportive and type in moments of on-line battle, offered there’s a bit little bit of a nudge to behave that method. In a single examine, we designed an intervention that inspired individuals who begin speaking about one thing contentious in a remark thread to change to direct messaging. Folks actually preferred it. It helped to resolve their battle and replicated an answer we use in-person: folks having a public argument transfer to a personal area to work issues out.\nYou’ve additionally tackled a unique downside popping out of social media utilization known as the 30-Minute Ick Issue. What’s that?\nWe in a short time lose ourselves on social media. When folks encounter a platform the place they will infinitely scroll for extra data, it may set off an identical neurocognitive reward system as in anticipating a profitable lottery ticket or getting meals. It’s a robust method that these apps are designed to maintain us checking and scrolling.\nThe 30-Minute Ick Issue is when folks imply to examine their social media briefly however then discover that half-hour have handed, and once they notice how a lot time they’ve spent, they’ve this sense of disgust and disappointment in themselves. Analysis has proven that individuals are dissatisfied with this ordinary social media use. Lots of people body it as meaningless, unproductive or addictive.\nYou’ve argued this expertise is much less a matter of dependancy and extra a problem of dissociation. Why?\nDissociation is a psychological course of that is available in many varieties. In the most typical, on a regular basis dissociation, your thoughts is so absorbed that you’re disconnected out of your actions. You could possibly be doing the dishes, begin daydreaming and never take note of how you might be doing the dishes. Otherwise you would possibly search immersive experiences—watching a film, studying a e-book or taking part in a recreation—that move the time and trigger you to overlook the place you might be.\nThroughout these actions, your sense of reflective self-consciousness and the passage of time is diminished. Folks solely notice that they dissociated in hindsight. Consideration is restored with the sense of “What simply occurred?” or “My leg fell asleep whereas we had been watching that film!”\nDissociation generally is a constructive factor, particularly if it’s an absorbing expertise, significant exercise or a wanted break. But it surely can be dangerous in sure instances, as in playing, or are available in battle with folks’s time-management targets, as with social media scrolling.\nHow do you measure folks’s dissociation on social media?\nWe labored with 43 contributors who used a customized cell app that we created known as Chirp to entry their Twitter accounts. The app let folks work together with Twitter content material whereas permitting us to ask them questions and take a look at interventions. So when folks had been utilizing Chirp, after a given variety of minutes, we’d ship them a questionnaire primarily based on a psychological scale for measuring dissociation. We requested how a lot they agreed with the assertion “I’m presently utilizing Chirp with out actually listening to what I’m doing” on a scale of 1 to five. We additionally did interviews with 11 folks to be taught extra. The outcomes confirmed dissociation occurred in 42 p.c of our contributors, they usually usually reported shedding observe of time or feeling “all-consumed.”\nYou designed 4 interventions that changed folks’s Twitter expertise on Chirp to scale back dissociation. What labored?\nProbably the most profitable had been customized lists and studying historical past labels. In customized lists, we pressured customers to categorize the content material they adopted, comparable to “sports activities” or “information” or “buddies.” Then, as an alternative of interacting with Twitter’s major feed, they engaged solely with content material on these lists. This method was coupled with a studying historical past intervention through which folks acquired a message once they had been caught up on the most recent tweets. Moderately than persevering with to scroll, they had been alerted to what that they had already seen, and they also targeted on simply the most recent content material. These interventions diminished dissociation, and once we did interviews, folks stated they felt safer checking their social media accounts when these modifications had been current.\nIn one other design, folks acquired timed messages letting them know the way lengthy that they had been on Chirp and suggesting they go away. In addition they had the choice of viewing a utilization web page that confirmed them statistics comparable to how a lot time they’d spent on Chirp prior to now seven days. These two options had been efficient if folks opted to make use of them. Many individuals ignored them, nevertheless. Additionally, they thought the timed messages had been annoying. These findings are fascinating as a result of quite a lot of the favored time-management instruments obtainable to folks appear like these time-out and utilization notifications.\nSo what may social media firms be doing in a different way? And is there any incentive for them to alter?\nProper now there’s a lot working towards individuals who use social media. It’s unimaginable to ever totally compensate for a social media feed, particularly when you think about the algorithmically inserted content material comparable to Twitter’s trending tweets or TikTok’s “For You” web page. However I feel that there’s hope that comparatively easy tweaks to social media design, comparable to customized lists, could make a distinction. It’s essential to notice that the customized lists considerably diminished dissociation for folks—however they did not considerably have an effect on time spent utilizing the app. To me, that factors out that lowering folks’s dissociation will not be as antithetical to social media firms’ income targets as we’d intuitively suppose.\nWhat’s most essential for folks utilizing social media now to know?\nFirst, don’t pile a bunch of disgrace onto your social media habits. Hundreds of individuals are employed to make you swipe your thumb up on that display screen and maintain you doing what you’re doing. Let’s shift the duty of designing protected and fulfilling experiences from customers to the businesses.\nSecond, get conversant in the well-being instruments which are already supplied. TikTok has a function that, each hour, will inform you that you simply’ve been scrolling for some time and may contemplate a break. On Twitter, customized lists are a function that already exists; it’s simply not the default choice. If extra folks begin utilizing these instruments, it may persuade these firms to refine them.\nMost essential, vote for people who find themselves occupied with regulating expertise as a result of I feel that’s the place we’re going to see the largest adjustments made."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:f76e327a-289d-48ce-83c6-e3005ae6583c>","<urn:uuid:28ea44cd-4054-4290-8fcb-a13be7579900>"],"error":null}
{"question":"What types of personal artifacts were commonly stored in 19th century books, and what are the recommended preservation methods for these delicate items?","answer":"In 19th century books, people commonly stored various personal items including locks of hair (often as mementos of children or distant friends), sewing needles with thread, photographs, paper doll clothes, and pressed flowers or leaves (particularly in poetry books). For preserving such delicate items, preservation experts recommend using archival boxes, implementing proper filing systems, and following specific storage guidelines. Items should be protected from light damage, humidity, and rodents. Different materials require different preservation approaches - for instance, photographs need special technical consideration for both black-and-white and color preservation, while paper-based materials require specific environmental conditions and monitoring equipment.","context":["The presentation both in class and in the library by Dr. Stauffer was pretty incredible. The amount of history that can be extracted from notes within the marginalia of Victorian-era books is very deep and thanks to modern technologies like google, able to be explored. He estimated that 12.5% of Victorian-era books contain significant historical marginalia. Dr. Stauffer’s process of looking up who people were and their relationships based on their marginalia so many years later almost seems like science-fiction.\nThe frequency of physical artifacts found within the books was also something really unusual that caught my attention. Sewing needles, locks of hair, and dolls as well as botanical insertions are all common within these older books. The botanical insertion portion was especially interesting as we were able to follow how a habit or action by people (inserting a flower into a book) became practice by publisher’s incorporating printed flowers and other botanical designs into the margins and illustrations where people normally would press one in. This practice evolved into layers of images being incorporated into the works of writer’s like Wordsworth. Images in works like that were connected to other images that sprawled over pages in a psychedelic style. I guess they could be considered meta-illustrations.\nThe social-historical aspect of the event was immense as well. The social function and form of annotation as a type of communication similar to a modern day email was very cool. Notes to lovers and friends were scrawled on pages or passages that reminded people of each other. Following the Victorian-era books also came pre-annotated in a sense as a result of the practice. Life stages (specifically those of women) were included in books with parts left blank for a mother to fill out for a daughter as she grew up.\nThe major role of books within peoples lives during the Victorian-era was put into perspective as a result of Stauffer’s work. Books were not just educational, religious or recreational, they were major social tools as well. Book traces open-source accessibility is a wonderful layout for a really intriguing project.\nDr. Stauffer visited Stevenson, with books in hand, ready to discuss not only the 19th century texts, but also the various former owners as well. He explained how many books during that time period weren’t just used for reading alone. According to Dr. Stauffer, while books were considered valuable, they weren’t seen as precious heirlooms or artifacts yet. So, many people would annotate within the margins, doodle on the pages, etc. There were even books that would be passed back and forth between multiple people and they would write out conversations on the pages like a primitive form of text messaging. But, people didn’t stop there when it came to utilizing a book.\nMany owners would leave personal objects in their books such as flowers, photographs, paper doll clothes, and locks of hair. There was even a book that Dr. Stauffer brought in that contained a sewing needle with a bit of thread that had been “stuck” in one of the pages (I’d love to know how they were able to do that without ripping the paper). During his presentation, Dr. Stauffer went into greater detail about the botanicals that were discovered within those books. Apparently, it was a very common practice within the 19th century to press flowers and leaves between the pages of a book. It was usually for sentimental reasons or for a decorative purpose, particularly if the book contained poetry or some flowery type of language. The poets of the day soon caught on to the idea and would write with the flower-pressers in mind. Publishers and editors reflected the practice by having flowers already printed on the page. But, I wonder if people appreciated that initiative?\nThe main idea behind Book Traces is to not only manage books from the 19th century, but to recognize that these physical copies have a historical purpose. They are artifacts from that time period. They don’t just reveal what was popular to read at the time, but what the personal histories were of the people who held those books. Dr. Stauffer’s project holds great evidence of how people would communicate not only with the text they were reading, but also with the other people in their daily lives.\nThe book traces activity and event was a very good experience, as it not only gave us information and an idea on past cultures, it also gave us insight on individuals. Books are very powerful as they have the ability to help us learn, find new perspective, and relate to the reader. Many, if all the writing we came across in the old books were things the writer was going through at the time; and the story or poem spoke exactly to their situation.\nDuring the introduction of last class, one of the writings was in a poetry book. Throughout the book, we could see texting before technology as a woman and a man were having a conversation with one another and taking pieces of poems to tell how they felt about each other. They had a system of passing the book back and forth. unfortunately, it didn’t end happily ever after as we were able to see the relationship crumble and not in a way the public needed to see. That however is was makes it fascinating is that many of the books had sentimental value to the owner. Whether it was something passed down in the family, a gift from a friend or loved one, or just a personal scrape book. At the time of writing whatever came to mind at that moment, there was no thought of having random eyes on it. These books ended up having strangers read them because the owner has passed and now there belongings are sold and given away.\nSome other material we came across during the hands on activity was a book called “Reveries of a Bachelor.” The notes we found were from an older gentleman wishing for the old days of being a bachelor. The end of the second Reverey was a poem he wrote about being a sad man with no one to care for him. The end of the book had notes of the man talking to other men who had the same feelings. Another book had writing on every page as they changed the story into a satire.\nDuring the event, a lot of what Andrew Stauffer presented was share in class like the amount of hair and flowers were left in books. Placing locks of hair in books was a common practice in the 19th century. It could have been a bookmark, or just a place to store a piece of life. Flowers were also assumed to be forgotten bookmarks. Others were botany samples as the 19th century was very floral; used on covers of most anthologies. Some new information was they found was a memorial for Annie Dearing of the John Dear family where they botanical insertion and notes from a religious text. Another book discussed was called “Songs of Seven” which is about a woman’s life every seven years until age 49. The book was given to a child at the age of seven and was marked up by the person that gave it to her to indicate the next stage of life. What was also cool to find out was it gave a voice to women who weren’t able to publish during that period.\n“Books have an afterlife and changes the future culture. Material books will always be relevant.” Andrew Stauffer\nThe book traces event held by Dr. Stauffer was enjoyable and piped my interests as an archivist. The event was well described by Dr. Stauffer as he outlined his goals to us in the class. The goals were to use texts dating from the 1800’s up to 1929, the age of copyright, to find annotations written by their original owners and to interpret the meaning of these annotations and what the book represented in the 19th century. He explained how books back then were shared possessions and that they often switched hands. People used to communicate using books and was a common form of messaging, such as how we have electronic texting and emails today. His books were host to a variety of materials, love letters, lecture notes, analytical satire, physical we can better understand 19th century culture and values. The class event went really well, with each of us receiving a copy of one of his historical books from his library in Virginia. He was incredibly excited to share his findings and for us to interpret them.\nHe gave me a book that was heavily transcribed by the original owner. It seems as if he was making fun of the book by adding lines of his own and adding his own form of humor between the pages. It seemed like this copy was meant to be passed to a friend because of the way it is written. Its a lot of language devices to make the text humorous and more enjoyable for the next reader. The book is a book of poems and are very sappy and romantic in nature. Its possible to believe that this person hated that form of writing and wanted to make light of it and belittle the lesser known author.\nThe lecture after class was a little less comprehensive due to time constraints and topics he was focusing on. the presentation regarding flowers in books was somewhat shaky towards the end and I felt that he got really nervous presenting to the large group that appeared. Every teacher from the history department was there and there felt like a pressure from the back of the room. He wasn’t a 100% confident on the flora in the books which also lead to a rough patch in his lecture since it was focusing on that exact flora he couldn’t name. The flora is often symbolic as he describes in his speech and is used to mark a passage often in memory to the life of a loved one or to add depth to a poem or piece of scripture. It seems like an easy fix with some identification help and I could be misinterpreting the situation at the end of the lecture. In comparison to his class lecture, I felt that he was much more enthusiastic and dealt with the smaller group on a personal level and a fluidity that you couldn’t find in his group lecture. I left the class feeling inspirited to help with archival work and better prepared to keep an eye out for annotations in books. They provide insight to that era’s thoughts, feelings, and culture. To me these things are irreplaceable and once we lose them, they’re gone forever to history. understanding the feelings of an era allows us to better understand our future. Another reason history and culture need to be respected and observed.\n-Review by Cory Price\nLast class, we had the honor of having Andrew Stauffer conduct a lecture about his project on book tracing. We were also able to attend his talk in the library. Both talks were extremely eye opening and informational. Throughout the paper I will discuss types of materials that were often stored in books, the content of marginalia, the purpose of further research and what I found most interesting from the talk.\nFirst and foremost I appreciated Stauffer’s passion for his project and movement because it made me more inclined to learn about it and ways in which I could be involved. Stauffer began his presentation by showing the audience pictures of artifacts that he and his students found in books. A common practice in the 1800s was storing locks of hair on the inside of books. Some stored locks of hair to remember their child’s first haircut, or as a memento from one distant friend to another. There was a needle and thread found in books that once belonged to a seamstress. Flowers and other botanicals were mostly found in books of poems. It dawned on me that, although it seemed strange to me, people stored personal items in books because it was always with them. Sort of how our phones are always with us- it is like an extension of ourselves. It was common for people to adventure on their day-to-day lives with a book in hand. It is not as common today, as social media has become our source of entertainment, and the access of books is now at our finger tips.\nBefore books were simplified to an app on our cell phones, physical books were a convenient way for people to interact and communicate with one another. Books served many purposes for soldiers in war. For instance, Stauffer showed pictures of written memories between two veterans, a soldier’s geometrical calculations of what angle he should point his missile to successfully shoot a target, and even one troop’s location on an unfinished sketch of a map. It was interesting to see the differences in content and use of marginalia in specific books. Like the love notes written back and forth between two lovers in a book of poems, or the thoughts of a grieving mother who just lost a child, or son to war. Often found in personal books were meanings of what lines in the poem meant to the owner, along with how he/ she identified with the content. Some owners even crossed out parts of the text and wrote it in their own words, which I found to be hilarious. It displayed humor, self-pity, and sense of self in a society. Shauffer showed us one example of a young woman who questioned her faith in God, herself and “mankind” who did everything to stifle and suppress her. I found this very interesting because despite my passion for empowering women, I always had this misconception of what women were like centuries and decades ago. Many assume that because women were suppressed and kept from gaining higher education that we would not find many traces of them in books, but we did- and they had a lot to say. Many of the traces found today were written by women. This information was the biggest take away piece for me. I would love to find out if there are book traces of famous women leaders and if they have personal, case by case information leading up to historic events such as women gaining the right to vote and furthering their education. For this very purpose, book traces is a movement built upon more than sentimental value.\nThere is significance in knowing to whom books once belonged to, and the content of their annotations because it gives us an understanding of what people were like and how they interacted with each other. We also see how books have evolved from physical to a digital form with textual-like characteristics to simulate the real thing. For instance, instead of finding an imprint of flowers in today’s books, we would find digital illustrations of a flower pinned on a page, or leaf designs around the trim of a picture. Techniques of book markers or imprints of flowers are still used today, in this way. These little details are still important to us because we still hold on to them even as we evolve into a more digital way. There is much meaning to be found in book traces that we will never discover until we look beyond the pages.\nThe Book Traces event was enlightening. Andrew Stauffer brought a handful of books to our class from the time period of 1800-1923. These books, which he gave to each of us, were special because they contained authentic marginalia from that time period. In my book–a book of poetry from the 1800s–someone had underlined various lines of sad content. There was only one written marginalia in the book–a date, which Professor Stauffer explained was most likely the date things went wrong with the writer’s relationship.\nProfessor Stauffer explained the marginalia in the other books as well. For example, Ryan’s book had marginalia which effectively tried to alter the story printed in the book. So funny and extensive were the alterations that the class joked the writer must’ve been an SNL writer before SNL existed. Another book, called Reveries of a Bachelor, had written in it notes from the reader who clearly found the book relatable.\nNext, Professor Stauffer pulled up a slideshow of images of books with marginalia. He read the marginalia to us and subsequently told us the stories of the writers based on his and his students’ research. We learned that a lot of the books that had been donated to UVA were sent from homes of Confederate families. The most touching marginalia recounted how the writer, drunk and feeling sentimental, purchased a book which reminded him of the past with his friend. This marginalia was in letter form and was gifted to the recipient with the book.\nAt the actual Book Traces speech, Professor Stauffer covered much of the same content, since more students and staff were present. However he also included new information. For example, he talked about why books older than 1800 weren’t included in his research (because they were stored away in vaults due to their fragility and rarity). He also talked about flowers as marginalia–often left in books to commemorate loved ones or to illustrate excerpts dealing with flora.\nThis all deals a lot with the content of our course because we have been learning about how ancient, or simply old, practices are carried over in today’s publishing/technology. Professor Stauffer made a profound statement about how the marginalia between two lovers in the same book was much like how we may text each other today.","Preservation and Conservation for Libraries and Archives\n\"When materials aren't available due\nto deterioration, missing pages, disconnected covers, or other problems, it can\nbe frustrating for users and librarians alike. The answer is to provide\nappropriate care for the collection from the outset, while also guiding staff on\nmaking needed repairs. In Preservation and Conservation, two experts show\nlibrary administrators and decision makers optimal collection preservation\ntechniques, what it takes to set up a conservation work area, and safe ways to\nmount a small exhibit. In between those responsible for repairs will find easily\nlearned, step-by-step instructions to repair and conserve books and documents.\nAppendixes include care of photographs as well as suppliers lists and additional\nresources. For any library, archive, or historical society committed to getting\nmaterials back into circulation as quickly as possible, this reference offers a\none-stop solution. From the issues relevant to directors to hands-on\ninstructions for technicians, it's an excellent reference for the entire\nPreservation of Library and Archival Materials: A Manual\nmuseums hold over 4 million cubic feet of archival materials and 7 million\nvolumes of bound materials. This comprehensive, user friendly guide to\npreservation of paper-based library and archival materials focuses on preventing\nor slowing down their deterioration. The manual includes information on\npreservation planning, storage, emergency procedures, environmental standards\nand monitoring equipment, digital imaging concerns, and other preservation\nissues. Also included are lists of conservation service and supply companies,\nand illustrated procedures, checklists, and reading lists, making this a\ncomplete resource for any museum or archive.\"\nConservation of Photographs\n\"Technical and professional information on the stability of the photographic image, both black-and-white and color. Recommended for anyone interested in photo preservation and fine-art photography. Covers restoration of deteriorated images, preservation through reproduction, storage, display, and more.\"\nA Preservation Guide: Saving the Past & Present for the Future\nThe Permanence and Care of Color Photographs: Traditional and Digital Color\nPrints, Negatives, Slides, and Motion Pictures\n\"Don't let accumulated certificates, photographs, documents, and other family heirlooms deteriorate in files or shoe boxes. This book provides simple guidelines to ensure that your fragile treasures will survive for future generations. The author offers specific instructions for maintaining and storing everything from paper and photographs to motion picture film, sound recordings, and textiles. She also provides tips on recovering from disasters such as floods and fires. \"\n\"An important source of information for any institution charged with the preservation of color prints, negatives, color slides, motion pictures, and black and white prints. Twenty illustrated chapters cover all aspects of color preservation, plus comprehensive recommendations on the storage and display of black and white photographs.\"\nOrganizing and Preserving Your Heirloom Documents\n\"Genealogists and non-genealogists alike inherit diaries, memoirs, letters, papers, or memorabilia from their relatives and ancestors. This book shows readers how to safely collect, preserve, and even publish some of these treasured heirlooms.\n\"Organizing and Preserving Your Heirloom Documents is filled with practical, readable, guidelines, useful tips, and ideas on how to: locate, organize, and transcribe family documents; care for fragile, older papers; annotate and illustrate documents; conduct historical research; construct a documentary volume; publish heirloom documents.\"\nDouble Fold: Libraries and the Assault on Paper\nThis one kept me awake at night for a long, long time! It is a must-read for anyone who loves old newspapers and books!\nNicholson Baker attacks the archiving practices of libraries. His witty,\npassionate, persuasive, and highly informed polemic argues that the urgency with\nwhich librarians convert newspapers and brittle books into microfilm, and\ndispose of original textual works, stinks of specious cost-cutting agendas--and\nresults in the loss of valuable archival information. (And who knew there was a\nmicrofilm lobby in Washington?) A New York Times Notable Book for 2001.\"\nThe Organized Family Historian: How to File, Manage, and Protect Your Genealogical Research and Heirlooms\n\"It can take hours to research family history and it is easy to become inundated with stuff - paper records, recordings, photographs, notes, artifacts, and more information than one would imagine could ever exist. The usefulness of the collection is in the organization - using computers, archival boxes, files, and forms to help you put your hands on what you need when you need it. Also included, in this book, are instructions on the best ways to store and preserve one-of-a-kind family relics. Fifth in the National Genealogical Society's Guide series, The Organized Family Historian will follow the same user-friendly format that makes the other books helpful at any level of genealogical experience. The NGS offers readers 100 years of research and experience.\"\nSaving Stuff: How to Care for and Preserve Your Collectibles, Heirlooms, and Other Prized Possessions\nThis is the definitive book on\npreserving by the man who has overseen the Smithsonian's preservation of\neverything from the papers of our founding fathers to the Fonz's leather jacket.\n\"From a fragile antique quilt to a child's macaroni artwork, this book offers expert advice on saving those priceless objects from entropy for the \"museum of you.\" Williams, senior conservator at the Smithsonian Institute, shares his extensive knowledge on the art of preservation, offering at-home techniques for battling damage from light, humidity, rodents and other pests, like careless friends and family members. Divided into easily navigable chapters, the book offers step-by-step guidelines, lists of supplies needed and numerous rules for preserving everything from \"family treasures\" to \"really valuable stuff,\" with specifics on caring for objects including record players, political memorabilia, fine art, vintage clothing and more. Sidebars detail tips (e.g. how to turn the pages of a vintage book), bust myths (don't store silver in plastic wrap-it'll cause tarnish) and offer \"Smithsonian Stories,\" quirky anecdotes about the Institution's collection. Peppered with personal stories by Jaggar, an amateur collector, about her past maintenance mess-ups, the book is written in clear, concise language that explains these professional techniques to any reader looking to safeguard his loot.\"\nYour Family Photographs\nOunce of Prevention: A Guide to the Care of Papers & Photographs"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b370cfc6-64b7-4a2d-9a59-e5addbabe75b>","<urn:uuid:696e0bc4-6992-44e1-b3e6-4dd1eaaec018>"],"error":null}
{"question":"How do insomnia and PTSD nightmares affect workplace performance and which has more severe economic impact?","answer":"Both conditions significantly impact workplace performance, but insomnia has a larger economic impact. PTSD nightmares cause decreased total sleep time and poor sleep quality, leading to job losses, irritability with coworkers, fatigue, and lack of energy. Insomnia, however, costs employers over $90 billion annually through reduced productivity and workplace accidents. Specifically, insomnia accounts for 7% of costly workplace accidents and errors, costing $10,148 more per incident than other medical conditions, with annual costs of $32 billion in accidents/errors and $59.8 billion in lost productivity through 11.3 days of lost work performance per affected individual.","context":["A standard part of any psychiatric evaluation involves inquiring about a patient’s sleep. Hidden in the answers that follow the basic question of, “How are you sleeping?” are the clues that are needed to diagnose what is ailing the patient seeking help from me.\nFor those with depression, they typically report early morning awakening (i.e. they wake 3-4 hours earlier than needed) and are not able to return to sleep. Those with anxiety disorders often complain of not being able to fall asleep (initial insomnia), they toss and turn for hours, their minds “racing” with anxious thoughts and worries. For those with mania they report that they can’t sleep at all for their energized and overcharged bodies simply have no need for sleep.\nMy patients with PTSD often report an amalgamation of all of the above in addition to a specific complaint–nightmares.\nNightmares–those threatening or scary dreams that leave you crying out in your sleep, thrashing around in your bed or waking up in a blind panic, soaked in sweat and with your heart pounding in your throat.\nNightmares–a very common complaint for those living with PTSD. Some studies reporting up to 80%, of those with PTSD, experience nightmares that have them reliving or re-experiencing the traumatic event for months or years after the actual event took place.\nNightmares are not only commonly experienced by those living with PTSD but they occur frequently too, sometimes several times a week so their impact on the lives of those living with PTSD can be profound.\nThe differences in sleep amongst those with PTSD related nightmares (compared with those who do not have PTSD) are tangible, they have:\n-increased phasic R (REM) sleep activity\n-decreased total sleep time\n-increased number and duration of nocturnal awakenings\n-decreased slow wave sleep (or deep sleep)\n-increased periodic leg movements during both REM and NREM sleep\nIn short, their sleep is less efficient and associated with a higher incidence of other sleep related breathing disorders\nClinically, this translates to the sad stories I hear all too often: People turn to alcohol or illicit drugs to “escape” the nightmares or their chronically poor sleep quality leads to other problems such as depression and anxiety. Others start to fear sleep or simply don’t function that well—they lose jobs, are irritable and short tempered with their loved ones, feel tired and lack energy. The nightmares and poor quality sleep chips away at their lives over weeks, months and years.\nAs a psychiatrist, there is a certain amount of dread associated with learning that your patient is experiencing nightmares for the very simple fact that nightmares related to PTSD can be very hard to treat.\nThe first approach is to treat the underlying condition i.e. the PTSD. I offer the patient evidence based psychotherapies and, if necessary, medications that I know work for PTSD and hope that, with time, the frequency and intensity of the nightmares will start to decrease as the underlying PTSD is treated.\nBut often times, despite PTSD treatment, patients still complain of nightmares. What can I offer them then?\nA Psychotherapeutic Option\nImage Rehearsal Therapy (IRT) is one option:\n– IRT is a modified CBT technique that utilizes recalling the nightmare, writing it down and changing the theme. i.e. change the storyline to a more positive one\n– The patient rehearses the rewritten dream scenario so that they can displace the unwanted content when the dream recurs (they do this by practicing 10-20mins per day)\n– In controlled studies, IRT has been shown to inhibit the original nightmare by providing a cognitive shift that refutes the original premise of the nightmare\nThough it is a well tolerated treatment, the issue remains that a patient has to be willing and able to commit to IRT for it to work.\nThis leaves a need for alternative options for patients who are unable to commit to this type of treatment.\nA Medication for Nightmares\nRecently, hope has been offered in new research published about the medication- Prazosin\nPrazosin is an alpha adrenergic receptor antagonist (traditionally used as an antihypertensive agent). It acts to reduce the level of activating neurochemicals in the brain and, via this action, is thought to damp down neurological pathways which are overstimulated in people with PTSD.\nWhilst clinically psychiatrists have been using prazosin for the treatment of PTSD related nightmares for years, the fact remains we still need more evidence, from controlled trials, to support its efficacy. A small randomized controlled trial of prazosin for sleep and PTSD has, recently, made a much needed contribution to that evidence base.\nIn a 15 week trial involving 67 active duty soldiers with PTSD, the drug was titrated up based on the participant nightmare response over 6 weeks. Prazosin was found to be effective in improving trauma related nightmares and sleep quality and, in turn, associated with reduced PTSD symptoms and an improvement in global functioning.\nThis is encouraging, and increases the enthusiasm with which I will recommend this treatment to my patients with PTSD.\nStill, the profound effect nightmares have on the quality of life of those living with PTSD highlights that more needs to be done to expand the array of options available to clinicians, like me, to help these patients.","Mental Health Topics\nOn this page:\nInsomnia is Costly to the Workplace\nResearch suggests that insomnia costs employers more than $90 billion annually in reduced productivity and workplace accidents and errors. With as many as one-fifth of working Americans reportedly experiencing insomnia over the past year, sleeplessness is clearly a major public health issue with implications that extend beyond the bedroom.\nInsufficient sleep is considered a major public health concern and one that affects as many as 50 to 70 million Americans (Colton et al., 2006). The term “sleep disorder” is wide ranging and can describe numerous types of sleep difficulties, including short sleep duration, unsatisfactory sleep quality despite having adequate duration (often termed non-restorative sleep), breathing-related sleep disruptions (e.g., snoring, sleep apnea), nightmares or night terrors, and problematic sleep behaviors (e.g., restless legs syndrome, sleepwalking).\nDespite differences in their symptoms and causes, sleep disorders as a whole are associated with a host of negative outcomes, such as an increased risk of certain medical diseases (e.g., cancer, hypertension, obesity) and mental disorders (particularly depression); higher mortality; increased suicidal thoughts and behaviors; and poorer quality of life. Sleep disturbances also contribute significantly to motor vehicle accidents, workplace errors and accidents, and reduced productivity due to absenteeism and work impairment (Swanson et al., 2011).\nWhat is Insomnia Disorder?\nInsomnia disorder is one of the most common sleep disturbances, occurring in approximately one in three working U.S. adults (Centers for Disease Control and Prevention, 2012). While many individuals may use the term insomnia to describe the experience of insufficient sleep, insomnia disorder is a mental disorder that can only be diagnosed when specific criteria are met. The criteria are listed in the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5), the handbook that psychiatrists, other physicians, and mental health professionals use to guide diagnosis of a mental disorder (American Psychiatric Association, 2013).\nInsomnia disorder is defined by the presence of poor sleep characterized by:\n- difficulty falling asleep (known as delayed sleep onset or initial insomnia);\n- difficulty staying asleep (known as sleep maintenance insomnia or middle insomnia), and/or wakening early and being unable to return to sleep (termed late insomnia);\n- In order to be diagnosed, the sleep disturbance must negatively impact a person’s functioning in important areas, such as work or school, completing daily responsibilities, or maintaining interpersonal relationships. Symptoms must be experienced for at least three nights per week for at least three months, despite having adequate opportunities for sleep.\nCharles F. Reynolds III, MD, University of Pittsburgh Medical Center endowed professor in geriatric psychiatry and chair of the DSM-5 work group that helped developed the latest insomnia disorder criteria, notes that the effects of the disorder are far-reaching in terms of its symptoms and impact throughout the body.\n“Insomnia disorder results in unmet sleep need, causing daytime problems, such as needing more time to process and react to information and having difficulty with complex thinking tasks, like problem solving. Further, irritability and other issues of mood regulation; difficulties interacting and maintaining relationships with others; and carbohydrate craving with weight gain can occur,” says Dr. Reynolds. “It is also a risk factor for the onset of common mental disorders, such as depression and substance use disorders.”\nIn fact, a recently published study in the Journal of the American Medical Association (Bernert, Turvey, Conwell, & Joiner, 2014) found that older adults (age 66 to 90 years) who reported poor sleep, including difficulty falling asleep and experiencing non-restorative sleep, had a 1.4-times greater risk of dying by suicide over the 10-year period in which the study was conducted.\nInsomnia in the Workplace\nGiven the scope of the impact of insomnia, it is not surprising that the disorder has been linked to problems in the workplace. The importance of adequate sleep and time away from work has increasingly become the subject of public discussion in light of revisions to work policies affecting positions of high risk to public safety, such as physician trainees, commercial airline pilots, and commercial vehicle drivers.\nThe most recent results from the America Insomnia Survey (AIS) support the idea that sleeplessness is negatively impacting the U.S. work environment (Shahly et al., 2012). Data were collected from 4,991 working Americans interviewed by telephone about insomnia and 18 other chronic medical conditions (e.g., cardiovascular, respiratory, and neurological disorders). An estimated 20% of those surveyed reported experiencing insomnia for at least 12 months, with even higher rates reported among women and workers age 45 to 64 years.\nResearchers found that insomnia was associated with approximately 7% of all costly workplace accidents and errors and almost 24% of the overall costs of all accidents and errors — higher than any of the other medical conditions examined. The total costs of accidents and errors attributed to insomnia were higher than costs due to other conditions by an average of $10,148 per incident. And the annual cost of insomnia-related workplace accidents and errors was estimated to be more than $32 billion.\nInsomnia also appears to compromise productivity, leading to missed days at work and low performance while at work. Recent data, also from the AIS, suggest the disorder results in significant reductions in work performance, yielding an annual rate of 11.3 days of lost work performance per individual with insomnia (Kessler et al., 2011). The study authors estimated that the annual cost of lost productivity due to insomnia is $59.8 billion.\nAlso, insomnia can be linked to certain other mental and medical disorders that themselves may impair work performance or attendance. Insomnia is considered a risk factor for anxiety and depression, both of which have a negative impact on participation in the labor market, likelihood of employment, and years of education (which in turn can affect employment status and earning potential). Cardiovascular and metabolic conditions that co-occur with insomnia disorder — such as diabetes, obesity, hypertension, and hypercholesterolemia — are costly to treat and pose a substantial burden to workplace costs and productivity.\nTips for Employers\nOne of the more challenging aspects of treating insomnia stems from the fact that only a fraction of individuals with the disorder seek medical treatment (Morin, 2006). Poor sleep is so widespread that it is practically seen as normal in American society; consequently, many people often are not aware that a disorder is present and that treatment can help. Integration of insomnia management into employee wellness programs can help provide basic education to raise awareness about the seriousness of symptoms and the usefulness of formal medical treatment. Workplace wellness programs can educate employees about the variety of treatment options available while leveraging employee assistance programs to offer a variety of such interventions.\n- Sleep Hygiene Education: Sleep is a behavior and, like many other behaviors, can be altered by adopting new habits. Employee wellness programs should always include basic education on sleep hygiene to help workers shape healthier sleep routines on their own. This includes developing a regular schedule in which one goes to bed at the same time each night and wakens at the same time each morning; reducing environmental distractions, such as cell phones, televisions, and other electronics; and ensuring bedrooms are dark, quiet, and cool in temperature. Large meals, caffeine, and alcohol should be avoided close to bedtime.\n- Access to Professional Treatment: While improving sleep hygiene is often all that is needed to relieve insomnia, some individuals will need more formal treatment by a professional. Many of these treatments do not involve medication and can be extremely effective in restoring healthy sleep. Some individuals, in consultation with their physicians, may decide that short-term treatment with medication is the best course of action.\n- Stress Management Programs: Insomnia commonly emerges when people feel stressed—and vice versa: getting inadequate sleep itself can be incredibly stressful. Employee wellness programs that include approaches to stress management (such as mind-body exercises, engaging in relaxing activities, and maintaining a healthy lifestyle) can potentially help alleviate sleep disturbances as well.\n- Flexible Schedules: Employers also can contribute by allowing for flexible work schedules and reducing the need for late work days. If shift work is required, employers should be lenient in offering adjustable shift rotations to the extent possible so that workers stay well-rested.\n- Work Expectations: Finally, businesses should be vigilant about their internal policies regarding work expectations and hours. The drive to succeed that can result in pushing personnel to increase workloads can actually backfire and undermine productivity and results.\nThe health of a company starts with the health of its workers. Investing greater efforts into ensuring employees are well rested is likely to pay off many times over.\nA guide on Insomnia from one of the leading resources for sleep disorder information, the National Sleep Foundation, an organization dedicated to sleep health education and advocacy.\nEmily A. Kuhl, Ph.D., owner and operator of Right Brain/Left Brain, LLC, is a consultant to the Partnership for Workplace Mental Health and a medical writer and editor in the Washington, D.C., area.\n- American Psychiatric Association. (2013). Diagnostic and Statistical Manual of Mental Disorders (5th ed.). Arlington, VA: American Psychiatric Publishing.\n- Bernert, R. A., Turvey, C. L., Conwell, Y., & Joiner, T. E., Jr. (2014). Association of poor subjective sleep quality with risk for death by suicide during a 10-year period: A longitudinal, populationbased study of late life. JAMA Psychiatry. Advance online publication. doi: 10.1001/jamapsychiatry.2014.1126\n- Centers for Disease Control and Prevention. (2012). Short sleep duration among workers — United States, 2010. Morbidity and Mortality Weekly Report, 61(16), 281–285.\n- Colten, H. R., & Altevogt, M. B. (Eds.). (2006). Sleep disorders and sleep deprivation: An unmet public health problem. Washington, DC: National Academies of Science.\n- Kessler, R. C., Berglund, P. A., Coulouvrat, C., Hajak, G., Roth, T., Shahly, V.,...Walsh, J. K. (2011). Insomnia and the performance of U.S. workers: Results from the America Insomnia Survey. Sleep, 34(9), 1161–1171.\n- Morin, A. K. (2006). Strategies for treating chronic insomnia. American Journal of Managed Care, 12(8 Suppl), S230–S245.\n- Shahly, V., Berglund, P. A., Coulouvrat, C., Fitzgerald, T., Hajak, G., Roth, T., . . . Kessler, R. C. (2012). The associations of insomnia with costly workplace accidents and errors: Results from the America Insomnia Survey. Archives of General Psychiatry, 69(10), 1054–1063. doi: 10.1001/archgenpsychiatry.2011.2188\n- Swanson, L. M., Arnedt, J. T., Rosekind, M. R., Belenky, G., Balkin, T. J., & Drake, C. (2011). Sleep disorders and work performance: Findings from the 2008 National Sleep Foundation Sleep in America poll. Journal of Sleep Research, 20(3), 487–494."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:79290779-61be-435d-a60c-5b4537dbb7e0>","<urn:uuid:8ffeab9e-2784-4158-8691-8a371159b758>"],"error":null}
{"question":"What are the optimal monitoring solutions when recording? Looking at closed-back headphones for vocal recording in Ableton versus using small tube amps for guitar monitoring - which is better for preventing audio bleed?","answer":"Closed-back headphones are specifically recommended for vocal recording as they prevent audio bleed that the microphone could pick up. When recording vocals in Ableton Live, this is particularly important to maintain clean recordings. For guitar recording through small tube amps, while monitoring through the amp itself is common practice, you need to be careful about potential noise issues - especially when using condenser microphones which will pick up more environmental sound. This is why many recording setups use both - closed-back headphones for vocal monitoring to prevent bleed, while carefully positioning and isolating small tube amps when recording guitar parts to minimize unwanted noise capture.","context":["Learn how to record vocals in Ableton Live. This guide takes you through the steps for recording vocals and offers tips for capturing perfect vocal takes.\nHow to Record Vocals in Ableton Live\nIn today’s music world, recording vocals from a home studio is the default setup for many artists. With so much music technology at our disposal, we can all record and produce commercial ready songs right from our bedroom. This guide explains the fundamental strategies used to record vocals using Ableton Live.\nBefore you get into the process of recording vocals, ensure your equipment is ready to go. Here is the basic equipment needed to record vocals:\n1. Computer: Laptop or desktop computer with high specs to avoid crashes.\n2. Digital Audio Workstation (DAW): This guide uses Ableton Live.\n3. Microphone: Condenser microphones work great for recording vocals. There are lots of other options for different budgets as well, so do some research.\n4. Audio Interface: Used to connect studio monitors, microphones, instruments, headphones, and other gear to a computer.\n5. Studio Headphones: Closed-back headphones are better for recording. They prevent audio bleed that the mic could pick up.\nPreparing Ableton Live for Recording Vocals\nAbleton Live provides two separate views to work with audio: Arrangement View and Session View. For this guide, we’ll work in Arrangement View. Here are the steps to prepare Ableton Live for recording vocals:\nStep 1 | Connect Your Microphone to Ableton Live\nFor those using a condenser mic: Connect your microphones XLR cable to an audio interface input. Then turn on the ‘Phantom Power’ (48V) on your audio interface to power your microphone. There is also USB microphones that can connect directly into your computer.\nStep 2 | Setup Live’s Preferences\nSetup your microphone input in Ableton Live. Go to File > Preferences. With Preferences open, go to the Audio tab and select the ‘Audio Input Device’ chooser. Next, select your audio interface. Also, make sure to select the ‘Audio Output Device’ where your headphones are connected. If you connected them directly to your computer, select ‘Built-in Output.’\nStep 3 | Set Your Songs Tempo\nSet the tempo of your song. Live’s default tempo is 120 BPM. You can change the BPM from the Tempo control located on the Control Bar. Also, use Live’s Tap Tempo button if you have a specific beat in mind but don’t know the BPM. Click the ‘Tap’ button once every beat and Live will determine the tempo for you.\nStep 4 | Enable the Metronome\nThe easiest way to record in sync is to use the built-in metronome. Enable the Metronome button from the Control Bar. It will begin ticking after hitting the Record or Play buttons.\nStep 5 | Create an Instrument Track\nIt’s helpful to have a beat or an instrument track playing when recording vocals. I love starting with a guitar track. Piano tracks are also great. Either record an instrument, import a sample loop or use an existing arrangement.\nAlso, set a loop around the instrumentation or beat if you don’t have a full song or if the instrument track is short. Select a section of the Arrangement and click the Control Bar’s ‘Loop’ switch.\nStep 6 | Create an Audio Track for Recording Vocals\nCreate a new Audio track for recording your vocals. Click CMD+T [Mac] or CTRL+T [PC].\nOpen the mixer’s In/Out Section. Select the input for your microphone from the ‘Input Channel’ chooser.\nStep 7 | Enable Recording\nRecord-enable the track by clicking the ‘Arm Arrangement Recording’ button located in the I/O Mixer Section.\nStart recording vocals by enabling the ‘Arrangement Record Button’ located on the Control Bar.\nRecording Vocals with Audio Effects\nApply audio effects using Sends and Return tracks. This method allows you to treat your tracks with parallel processing. Parallel processing is more effective than inserting effects on the vocal track. Why? It enables you to apply effects while also preserving the original audio signal. For example, Return tracks host the effects devices. Then each track has corresponding Send controls that feed the audio into those effects.\nTips for Recording Vocals\nMake the recording process more efficient and effective with these tracking tips. There are different strategies for recording vocals. You can record a full song in one take or record smaller parts. Mixing these strategies also tends to be a very efficient technique. Here is how it works:\nRecording a Full Take\nI like to start by recording the whole song for at least two takes. Then, I’ll focus on recording separate sections. Recording the song as a whole allows the vocalist to warm up. It also helps them get comfortable with the direction and dynamics of the song. Moreover, recording one take sounds more natural compared to recording separate sections and comping them together.\nRecording Short Phrases\nA flawless, one-take vocal performance is rare. Many choose to record vocals section by section. This process is known as ‘comping’ which is short for compilation. Have the vocalist run through their parts several times, even if they make mistakes. Keep the recording going. They’ll eventually nail that perfect take. It’s also better to have too many takes than not enough when it comes to editing.\nWorking with Another Vocalist\nWhen working with another vocalist, ensure you give them feedback after every take. Singing is a challenging task, and recording psychology makes everything a little sensitive. Giving your vocalist constructive feedback can also make them feel more comfortable, and helps them improve their performance.\nManaging Your Vocal Recordings\nBy the end of your recording session, you will have many different takes for editing. Keeping them in large chunks may make your job harder. One way to manage them easier is to chop them into different parts. Split audio clips in Ableton Live with the shortcut CMD+E [Mac] or CTRL+E [PC]. You can also consolidate parts with CMD+J [Mac] or CTRL+J [PC].\nThe trick is to pick the best takes after recording and combine them to make it sound like one take. Again, this procedure of combining different takes is called ‘comping.’ So, my suggestion before starting to comp a track is to work with a clear and rested mind. Listening to dozens of vocal takes and trying to pick the best one can be challenging. It’s better to have a separate editing session with fresh ears. For instance, I start editing vocal tracks a while after recording them, especially if it was a long recording session.\nThese vocal recording strategies provide a foundation for more advanced techniques. As you improve your music production skills, you’ll discover many different methods to get to the same results. Keep in mind that it takes time and effort to make great songs. The more songs you produce, the better you’ll get. So get started with recording some vocals, and most importantly, have fun making your music!","So, why would I want to use a Boost Pedal between my guitar and my amp? First off, I have a nice guitar I’m super happy with and I’ve used this same guitar for gigging, teaching and recording most of my career. It’s an old 68 Gibson. I’ve always found that although I like the sound, the output has always been low and a little thin.\nThe other thing is that like many jazz guitarists, I like to use a small amp. I especially like all the small tube amps with EL84 tubes. This of course is my own personal taste. Even though, for a long time I opted for a nice loud, lightweight “Acoustic Image.” It wasn’t my ultimate jazz guitar sound but volume was never an issue and it sounded pretty good.\nCurrently, I’m back to using a small tube amp and love the sound. It’s great for recording but a little shy for volume on some gigs and when I have my regular Monday night rehearsals with the big band, I’m really pushing it.\nI was curious about the pickups in my guitar and if their were any known issues with some of the older Gibson Pickups. Like most Gibson owners, we hope we have the most famous of pickups, the PAF’s. Those things are worth a fortune so we all hope we have a pair just for the sake of bragging rights if nothing else. A quick search on Google and a visit to a Gibson forum, Wikipedia, and a few other sites I soon discovered I have T-Tops. I also discovered that Gibsons from 68 with T-Tops can have low output and a thin sound.\nWhy do they sound this way? From what others have suggested, the pickups are just old and lose their magnetic charge. Apparently you can have a guitar tech take apart your pickups and re-magnetize them or you can replace the magnets with fully charged magnets of your choice. I don’t really think I want to do that. If they end up sounding vastly different, I’ll be a very unhappy jazzer.\nSo back to the pedal. I had tried something years ago that was a little homemade pedal with in/out jacks and a single knob. It was described as giving you more volume but it wasn’t like a distortion pedal. I remember that it really made my guitar speak and it also gave a nice little boost in volume.\nWell here I am again in that same place; love my tone but It’s not loud enough. Do I buy a bigger, louder, heavier amp? I remembered that pedal I had tried and started doing some research. Low and behold there are dozens of companies making Boost Pedals. There are all kinds of prices, brands, features and so on.\nOne thing I will add is that when I plug in my Fender Strat with Lace pickups, I have tons more volume than I get from my Gibson. Interesting! So I’m thinking the Boost Pedal should give me a little more oomph going into the amp. At this point I’m sold. Time to spend a few bucks and order one of these little boxes.\nI went on Amazon and found a Donner “Boost Killer.” No, not the best name for a “Boost” pedal, I know. The name kind of suggests it’s going to take something away. Why would anyone want to kill their boost? It wasn’t the cheapest and certainly not the most expensive. It seemed to get very good reviews and was described as being a steal for the price. Perfect!\nIt’s very small (good thing) but along with it’s small footprint is the obvious problem: no room inside for a battery. Be forewarned, you will also need a 9V power supply which you will also have to carry around.\nSo how does it sound? So far I’m pleased. I’m getting a hotter signal into the amp with no added distortion at all. This pedal will distort though so play around with the volume and gain controls until you find something you like. Is my amp way louder now? No, of course not. I do have a little more volume for sure and the fact that there is a hotter signal going into the amp has added a little more tone and sparkle to my sound.\nSo there you have it. I’ve joined the guitar pedal craze! I’m a jazz guitarist and I own a guitar pedal!\nApart from all the time we spend practicing and doing gigs, it’s becoming more and more common for us as jazz guitarists to be able to record ourselves. Today, more than ever, musicians from all musical styles own and use a certain amount of home recording gear. It has become an essential part of our careers for us to be able to do everything from making our own demos, composing and recording music as well as providing guitar tracks for a project that we can upload to another artist or producer who lives halfway around the world.\nSo much gear, so much to know and so many opinions about what is best. In this post, I’d like to look at some of the different microphone choices to record a jazz guitar amplifier. What are some of the better microphones we can use to capture a good representation of what a straight up, clean jazz guitar should sound like?\nOne of the things you will encounter in your quest to find which microphone to buy is that the majority of the reviews and recommendations come from musicians and recording engineers who work predominantly in the pop and rock genres. All those great microphone reviews are focussed on capturing a guitar player shredding through a Marshall stack. Although some of what they find may be true and will still translate to the kinds of sounds we play, for the most part, there is much more for us to know.\nOk, so recording jazz guitar, what do we need to know? Let’s start with microphone types. There are 3 types of microphones you can use to record jazz guitar: Dynamic, Condenser and Ribbon. If you want to know more about each type, click here.All are great microphones that can be placed in front of your guitar amp. Which is best? Which type should you buy? The good news, all 3 types of microphones can produce great results. In the end, it all comes down to your own tastes, goals and what you are willing to spend.\nWe’ll listen to 2 dynamic microphones: Shure SM57 and Beyerdynamic M201 N (C). Dynamic microphones are generally on the less expensive side. Dynamic microphones will most often have a narrower frequency response and don’t require phantom power. Now it’s important to note that a narrower frequency response is not necessarily a bad thing. What that means is it’s not going to pick up the really low or the really high frequencies. We don’t really want the boomy low notes or the super high overtones anyway. So, in essence, dynamic mics are sort of providing a nice EQ for us. Dynamics will help take out some of the stuff we don’t really want anyway.\nFor condensers, we’ll listen to a Shure Beta 181 with supercardiod capsule and a Neumann TLM 102. Condenser microphones have a much wider frequency response, usually from 20-20,000 Hz and do require phantom power. Although most audio interfaces today do provide phantom power, there are still some that don’t so it’s always good to check. Condenser microphones are much more acurate and will pick up much more detail, both in a good way and in a bad way. This of course means that if you are in a noisy environment, the microphone will capture all of that noise as well. Any amp hum, ringing tubes, finger noise, street noise, all of this will be more evident when using a condenser mic. At the same time, more of the colour, nuance and the dynamics will also come through giving you a much richer sound.\nRibbon mics are a favorite for guitar players. I would describe them as being both accurate and flattering. The sound of a ribbon microphone is probably somewhere between that of a dynamic and a condenser. They seem to capture a lot of detail but always in a good way. Ribbon microphones do not require phantom power and in fact phantom power can severely damage a ribbon microphone. Ribbon microphones also need to be matched with a very good preamp that can provide 70 db or more of good, clean gain. If you are planning on using a ribbon microphone with a soundcard which does not have enough gain, there are solutions like the “Cloudlifter” or “Fethead” which offer an additional 20 db or so of ultra clean gain. The Ribbon microphone we will listen to is the AEA R84. It is a beautiful modern day recreation of (or perhaps a microphone inspired by) the vintage RCA 77. The AEA R84 is an expensive microphone but there are many excellent low cost Ribbons for as low as $99 which provide amazing results on guitar amps. (Apex, Cascade Microphones, MXL…..)\nHere’s the what and how used for all of the recorded examples. Microphones, placed around an inch and a half from the speaker grill, went through a BAE 1073MP preamp into a Universal Audio Apollo Quad into Pro Tools 2018. For the AEA R84 Ribbon Microphone, I used a Grace Design M101 preamp. The Grace has a Ribbon mode which works very well with the R84 and provides very clean, high gain levels.\nI played my 1968 Gibson ES-175 using the neck pickup through a Traynor YCV20 with JJ tubes and a 12″ bass speaker. Why a bass speaker? I’ve always preferred the sound of my guitar through a bass amp. Using a bass speaker helps to cut a lot of the unwanted highs you get from most amps. At this point, I like tube amps. I keep going from jazz amp (Polytone) to tube amp, (Mesa Boogie) back to jazz amp, (Acoustic Image) and now once again a tube amp. (Traynor) It’s funny because I’m sure the listener never actually notices the difference anyway. In the end, we sound like we sound.\nBelow are the recorded examples for each microphone. Since some of the microphones have higher signals than others, I’ve matched volumes to help make comparisons easier. Also, there are 2 examples for each microphone: one with just guitar and one with piano bass and drums. In most cases the sound of the guitar alone doesn’t tell us enough. How it’s going to sound in the context of an entire mix is much more important.\nTape emulation, parallel compression and reverb have been added to all examples with piano, bass and drums. The solo guitar examples have no processing. No EQ has been added to any of the examples.\nShure SM57 Solo Guitar\nShure SM57 Guitar with Trio\nBeyerdynamic M201 N (C)\nBeyerdynamic M201 Solo Guitar\nBeyerdynamic M201 Guitar with Trio\nShure Beta 181 (Supercardiod Capsule)\nShure Beta 181 (Supercardiod Capsule) Solo Guitar\nShure Beta 181 (Supercardiod Capsule) Guitar with Trio\nNeumann TLM 102\nNeumann TLM 102 Solo Guitar\nNeumann TLM 102 Guitar with Trio\nAEA R84 Solo Guitar\nAEA R84 Guitar with Trio\nWhich sounds best? To me, they all sound like they get the job done very well. To be fair, these are all microphones I like to use on guitar amps and especially when recording jazz guitar. They do all sound a little different, and bring out different aspects of the amp sound. My intentions are not to say one is better than the other or give you a list of “the best” microphones to record jazz guitar. It’s more of a chance to have one more listen from a clean jazz guitar perspective.\nThis comparison has actually been pretty revealing to me. Hearing all these microphones side by side has made me reconsider some of my own choices.\nIt’s also important to note that I did play the example 5 times in a row. Because I wanted to place the microphone in the exact same spot each time I could really only use one microphone at a time. I know some people like to record the guitar once direct and then do the re-amp thing. I don’t have confidence in that approach. So even though I tried to play exactly the same each time, I’m sure I was influenced slightly by the sound of each microphone and may have reacted to each to a certain degree.\nAs always, I hope my Blog Posts are helpful and that this one in particular in some small way will help you to find the right microphone that works for your own style of playing and sound."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7ab3aef2-bcce-4b3d-ad12-419967d97f1e>","<urn:uuid:f9096be8-f04a-4e57-9db4-882bcdd04bc7>"],"error":null}
{"question":"How do the problem-solving approaches of PDCA and DMAIC compare in terms of their key phases and objectives?","answer":"PDCA and DMAIC are both structured problem-solving approaches but have different phases. PDCA consists of Plan-Do-Check-Act phases, focusing on testing solutions through pilots before full implementation. DMAIC consists of Define-Measure-Analyze-Improve-Control phases, emphasizing statistical analysis and process control. While PDCA is more focused on iterative testing and refinement of solutions, DMAIC is more focused on measuring and analyzing process variations to achieve specific quality targets like Six Sigma level performance (3.4 defects per million opportunities).","context":["Create a model before you build the final solution.\nSomething needs to change: Something's wrong, and needs to be fixed, and you've worked hard to create a credible vision of where you want it to be in future. But are you 100% sure that you're right? And are you absolutely certain that your solution will work perfectly, in every way?\nWhere the consequences of getting things wrong are significant, it often makes sense to run a well-crafted pilot project. That way if the pilot doesn't deliver the results you expected, you get the chance to fix and improve things before you fully commit your reputation and resources.\nSo how do you make sure that you get this right, not just this time but every time? The solution is to have a process that you follow when you need to make a change or solve a problem; A process that will ensure you plan, test and incorporate feedback before you commit to implementation.\nA popular tool for doing just this is the Plan-Do-Check-Act Cycle. This is often referred to as the Deming Cycle or the Deming Wheel after its proponent, W Edwards Deming. It is also sometimes called the Shewhart Cycle.\nDeming is best known as a pioneer of the quality management approach and for introducing statistical process control techniques for manufacturing to the Japanese, who used them with great success. He believed that a key source of production quality lay in having clearly defined, repeatable processes. And so the PDCA Cycle as an approach to change and problem solving is very much at the heart of Deming's quality-driven philosophy.\nThe four phases in the Plan-Do-Check-Act Cycle involve:\nThese are shown in Figure 1 below.\nThere can be any number of iterations of the \"Do\" and \"Check\" phases, as the solution is refined, retested, re-refined and retested again.\nThe PDCA Cycle encourages you to be methodical in your approach to problem solving and implementing solutions. Follow the steps below every time to ensure you get the highest quality solution possible.\nFirst, identify exactly what your problem is. You may find it useful to use tools like Drill Down , Cause and Effect Diagrams , and the 5 Whys to help you really get to the root of it. Once you've done this, it may be appropriate for you to\nmap the process that is at the root of the problem.\nNext, draw together any other information you need that will help you start sketching out solutions.\nThis phase involves several activities:\nOur section on Practical Creativity includes several tools that can help you generate ideas and solutions. Our section on Decision Making includes a number of tools that will help you to choose in a scientific and dispassionate way between the various potential solutions you generate.\nThe phrase \"Plan Do Check Act\" or PDCA is easy to remember, but it's important you are quite clear exactly what \"Do\" means. \"\"Do\" means \"Try\" or \"Test\". It does not mean \"Implement fully.\" Full implementation happens in the \"Act\" phase.\nIn this phase, you measure how effective the pilot solution has been, and gather together any learnings from it that could make it even better.\nDepending on the success of the pilot, the number of areas for improvement you have identified, and the scope of the whole initiative, you may decide to repeat the \"Do\" and \"Check\" phases, incorporating your additional improvements.\nOnce you are finally satisfied that the costs would outweigh the benefits of repeating the Do-Check sub-cycle any more, you can move on to the final phase.\nNow you implement your solution fully. However, your use of the PDCA Cycle doesn't necessarily stop there. If you are using the PDCA or Deming Wheel as part of a continuous improvement initiative, you need to loop back to the Plan Phase (Step 1), and seek out further areas for improvement.\nThe Deming Cycle provides a useful, controlled problem solving process. It is particularly effective for:\nClearly, use of a Deming Cycle approach is slower and more measured than a straightforward \"gung ho\" implementation. In true emergency situations, this means that it may not be appropriate (however, it's easy for people to think that situations are more of an emergency than, in reality, they really are...)\nPDCA is closely related to the Spiral Development Approach which is popular in certain areas of software development, especially where the overall system develops incrementally. Spiral Development repeats loops of the PDCA cycle, as developers identify functionality needed, develop it, test it, implement it, and then go back to identify another sub-system of functionality.\nThe Plan-Do-Check-Act (PDCA) Cycle provides a simple but effective approach for problem solving and managing change, ensuring that ideas are appropriately tested before committing to full implementation. It can be used in all sorts of environments from new product development through to marketing, or even politics.\nIt begins with a Planning phase in which the problem is clearly identified and understood. Potential solutions are then generated and tested on a small scale in the \"Do\" phase, and the outcome of this testing is evaluated during the Check phase. \"Do\" and \"Check\" phases can be iterated as many times as is necessary before the full, polished solution is implemented in the \"Act\" phase.\nThis site teaches you the skills you need for a happy and successful career; and this is just one of many tools and resources that you'll find here at Mind Tools. Subscribe to our free newsletter, or join the Mind Tools Club and really supercharge your career!\nThis ensures that you don’t lose your plan.\nPlease enter your username or email address and we'll send you a reminder.\nYour log in details have been sent to the email account you registered with. Please check your email to reset your login details.\nPlease check your Inbox, and click on the link in the email from us. We can then send you the newsletter.","Lean Six Sigma and a sample application\nLean Six Sigma is a concept that aims to improve process performance by minimizing waste and reducing variations. It is a method that combines Lean Manufacturing, Lean Enterprise and Six Sigma principles to eliminate waste and improve quality.\nThe origins of Lean Six Sigma can be traced back to 1986 when Motorola came up with strategies to compete with higher quality Japanese products. Japan used the Kaizen approach (continuous improvement) in product development to produce world-class products of high quality.\nIn the 1990’s, an American businessman called Larry Bossidy introduced Six Sigma in Manufacturing and soon after he was engaged to introduce the concept in GE.\nIn early 2000’s the two concepts of Lean Manufacturing (Reduction of waste) and Six Sigma (higher process quality leading to reduced variability) came together as a single concept called Lean Six Sigma. The concept then found acceptance in other industries such as Healthcare, Finance, Retail and Supply chain etc.\nLean focusses on eight kinds of waste (Muda is Japanese word for waste) inherent in processes;\nSix Sigma focuses on improving the quality of process outputs by identifying and removing the causes of defects and minimising variability in processes.\nLean Six Sigma aims to achieve continuous flow of quality outcomes, by exposing constraints between process steps and reducing variability between and within the process steps through a cycle of iterative improvements. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases similar to Six Sigma.\nBasic Concepts of Six Sigma\nSix Sigma quality is a statistical term used to indicate how well a process is controlled in terms of its variability from the mean. It is a fundamental nature of any process that over time and scale, variations will creep in due to a variety of reasons or factors. The aim of Six Sigma is to keep the process running within acceptable limits from a mean (or arithmetic average of a process data set).\nThe word Sigma ( σ ) is the standard deviation or the spread around the mean or central tendency. In simple terms, Six Sigma quality performance means 3.4 defects per million opportunities. It is important to note that not all processes, products or systems need to function at Six Sigma quality level. Other than for critical processes involving high safety requirements, such as healthcare, pharmaceuticals, airplanes, manufacturing, etc. it is enough for most processes to function at 3 Sigma or 4 Sigma. The trade-off between achieving Six Sigma or lower levels of Sigma is simply cost and often it is not practical or cost-effective to aim for a high level of Sigma. The table below illustrates the number of defects per million opportunities (DMPO) at various levels of Sigma. It is easily evident as to how efficient processes need to be at Six Sigma level.\nSigma Level DMPO\n2 σ 308,537\n3 σ 66,807\n4 σ 6,210\n5 σ 233\n6 σ 3.4\nLean Six Sigma Case Study\nThe objective of this case study is to illustrate how to apply Six Sigma thinking and concepts to organizational problems and processes.\nImagine a retail organization that uses disparate core systems such as CRM (Customer relationship management), ERP (Enterprise Resource Planning), Analytics and Financial Accounting to run its business. This organization has 100,000 unique Customer master records that are regularly referenced in sales, order management, delivery, invoicing and accounts receivable transactions. Each Customer master record has 10 attributes associated with it as shown below;\nCustomer Master Records (100,000 unique records)\nCustomer Pricing Code\nThis structure implies that there are 1 million elements associated with Customer master data.\nThese Customer master records are created, referenced and updated separately by different individuals, in different departments and business units, depending on their role and function. For example, the Finance Department may manage elements of Customer master data relating to invoicing and accounts receivable. The Sales team may manage elements relating to Customer orders. As is typical in many organizations and situations, disparate systems and independent work functions cause the following issues with Master data;\nDuplicated master data across systems that are out of sync\nWasted effort in data maintenance\nErrors that get accumulated over time because of data changes made in multiple systems\nBusiness risk arising from poor governance, etc.\nDMAIC Approach to improve the process of data management\nThe DMAIC (Define, Measure, Analyse, Improve and Control) concept can be applied to the above case problem, as follows;\nDefine (The Problem)\nMaster Data, maintained separately in multiple business systems, has been observed to contain an unacceptable level of errors (defects) causing unnecessary manual intervention (extra processing) that is costing the organization money, business responsiveness and customer satisfaction.\nMeasure (The Process parameters and Sigma)\nAn important step in Six Sigma Analysis is to measure the key operating parameters of the process in consideration to understand current levels of Sigma (Standard Deviation from mean). The table below shows the impact of errors (Defects per million elements) in terms of cost and time. An error can be broadly defined as any situation relating to any Customer master data element that requires changes to data arising from any non-business driven reason. Sigma (standard deviation) can be determined by sampling in a specific section or department or the entire organization at data element levels or entire Customer Master record levels.\nIt has been assumed that it takes an average of 10 minutes to fix an error at a cost of $50/hour. These assumptions can be validated in the Analysis stage by end users or by Six Sigma experts.\nLet us assume that sampling shows there are likely to be 200,000 errors (20% of the total volume of Customer Master data elements). From the table, we can see that the current process of managing customer master data reveals a Sigma between 2 and 3, implying it is costing the organization at least $556,725 in remediating these process errors. It is useful to note that data volume may also grow at 20% year on year and with growth in master data volume there is also an increase in DMPO.\nAnalysis (Why are there errors in the process?)\nThe errors in the process may be occurring due to one or a combination of the following reasons;\nAsynchronous editing of Customer master data elements in different systems\nLack of a concept of Master data and downstream systems for data leading to uncontrolled changes\nLack of formal data governance policies and procedures defining how Master data is created and edited\nImprove (How can the process be improved?)\nThe process for managing Customer master data and its elements can be improved through several options;\nNominating one of the systems (CRM or ERP or Financial or Analytics systems) as the Master system for Customer data and setting up integrations between that designated Master system to propagate changes\nImplementing a centralized, organization-wide Master Data Management (MDM) system to manage and control all Customer master data. This MDM system will then propagate all changes to master data to all other systems referencing that data through integrations (This may be a costlier option but improve process accuracy the most and help achieve 4 σ or 5 σ efficiencies)\nSetting up a manual governance process to manage changes to Master data in all systems in a coordinated way (This may be the cheapest option but may not improve process accuracy significantly and in a sustained manner)\nControl (How can the process be controlled?)\nRegardless of which option is chosen to improve the process efficiencies, it is important to ensure that process parameters are regularly measured and action taken to remediate. This is done by implementing good data governance initiatives.\nLean Six Sigma is a concept that aims to improve process performance and efficiencies by reducing waste and eliminating errors. It relies on collaboration amongst team members to achieve that. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases to reduce errors. As such it is a evidence-based, data driven approach to improvement that focuses on defect prevention. Lean Six Sigma initiatives improve customer satisfaction and profitability by reducing variation, waste and cycle time while promoting work standardisation and flow.\nIt is recommended that organizations new to the Lean Six Sigma begin by implementing the Lean approach to make the workplace as effective and efficient as possible, reducing waste and using value stream mapping to improve throughput. When process problems persist, the more technical Six Sigma statistical tools can be applied by a team of specialists in various areas of the overall process."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:782277cf-2e99-4b82-b94d-09146cf767a6>","<urn:uuid:02bb5903-0b04-4b5d-bf7f-7f6c52131372>"],"error":null}
{"question":"What role did scientific experiments and research play in addressing public health concerns in both food safety reform and opiate addiction control?","answer":"Scientific experiments were crucial in both cases but used differently. For food safety, as chronicled in The Poison Squad, Dr. Harvey Washington Wiley conducted dramatic experiments to demonstrate the dangers of food additives and preservatives, using scientific evidence to drive regulatory change. In contrast, with the opiate epidemic of the 1890s, scientific advances helped address the crisis indirectly - through the development of alternatives like aspirin, acceptance of germ theory, vaccines, and x-rays, which reduced the medical need for opiates. The scientific approach in food safety was more direct and intentional in driving policy change, while in opiate control it was part of broader medical progress.","context":["Each year Madison Public Library partners with UW-Madison to extend the perennially popular Go Big Read program into the broader community with book discussion groups and programs at our libraries.\nThe 2018-2019 Go Big Read selection is The Poison Squad: One Chemist’s Single-Minded Quest for Food Safety at the Turn of the Twentieth Century by Deborah Blum, the dramatic true story of the fight for food safety in the United States at the turn of the twentieth century, led by the inimitable Dr. Harvey Washington Wiley. Detailing the complex interchanges of industry, media, and government regulation with a bracing clarity, The Poison Squad offers a prescient perspective on the enormous social and political challenges we face today. Read more about the selection on the UW-Madison website.\nBook Club Kits\nMadison Public Library is pleased to announce that it will again have book club kits for the UW-Madison Go Big Read title available to the public. Beginning Wednesday, August 28, 2019, over 100 copies of The Poison Squad by Deborah Blum will be available for checkout for individuals interested in hosting discussions. Kits will also include suggested discussion questions, interviews with Blum, and extensive additional material to facilitate conversation. Kits can be ordered by calling 266-6300 beginning August 28 and will be distributed on a first call, first served basis. See the Book Club Kits page for more information about other kits.\nGo Big Read events will be hosted by the UW and Madison Public Library locations through November, so if copies are not readily available in September, readers can typically find more copies available later in the fall. The book is also available for checkout as a downloadable eBook and audiobook via Overdrive.\nAbout the Author\nDeborah Blum is director of the Knight Science Journalism Program at MIT, and editor of Undarkmagazine, (undark.org). In 1992, she won the Pulitzer Prize for a series on primate research, which she turned into a book, The Monkey Wars. Her other books include The Poisoner's Handbook, Ghost Hunters, Love at Goon Park, and Sex on the Brain. She has written for publications including The New York Times, Wired, Time, Discover, Mother Jones, The Guardian and The Boston Globe. Blum is a past president of the National Association of Science Writers, a fellow of the American Association for the Advancement of Science, and a lifetime associate of the National Academy of Sciences.\nWhat is Food and and Who Decides: Insects as Food Panel Discussion\nAbout the Book\n- Article: “The 'Poison Squad' That Shook America's Faith in Preservatives.” The Atlantic, August 28, 2018.\n- Article: “We Have Food Safety Laws Thanks to 19th Century ‘Poison Squad.’” Ars Technica, November 26, 2018.\n- Book Review: “The Man Who Pioneered Food Safety.” The New York Times, October 16, 2018.\n- Book Review: “Safety Last.” Bookforum, Sept/Oct/Nov 2018 Issue.\n- Interview with the Author: “How A 19th Century Chemist Took On The Food Industry With A Grisly Experiment.” NPR, October 8, 2018.\n- Podcast: “Keeping it Fresh: Preservatives and the Poison Squad.” Gastropod, August 27, 2018\n- Radio Show: “Borax: It’s What’s for Dinner.” Science Friday, October 5, 2018.\n- Radio Show: “Food Regulation: ‘Only The Brave Dare Eat The Fare.” 1A, September 26, 2018.\n- Article: “Harvey W. Wiley.” FDA Consumer magazine, The Centennial Edition, January-February 2006.\n- Website: U.S. Food and Drug Administration\n- Website: Foodsafety.gov.\n- Bittman, Mark. A Bone to Pick: The Good and Bad News about Food, with Wisdom, Insights, and Advice on Diets, Food Safety, GMOs, Farming, and More. 2015.\n- Dawson, Paul L. and Sheldon, Brian W. Did You Just Eat That?: Two Scientists Explore Double-dipping, the Five-second Rule, and Other Food Myths in the Lab. 2018.\n- Di Justo, Patrick. This is What You Just Put in Your Mouth?: From Eggnog to Beef Jerky, the Surprising Secrets of What's Inside Everyday Products. 2015.\n- Evershed, Richard and Nicola Temple. Sorting the Beef From the Bull: The Science of Food Fraud Forensics. 2016.\n- Farrimond, Stuart. The Science of Cooking. 2017.\n- Kurlansky, Mark. Milk!: A 10,000-year Food Fracas. 2018.\n- Leonard, Christopher. The Meat Racket: The Secret Takeover of America's Food Business. 2014.\n- Pollan, Michael. In Defense of Food: An Eater's Manifesto. 2008.\n- Simon, Bryant. The Hamlet Fire: A Tragic Story of Cheap Food, Cheap Government, and Cheap Lives. 2017.\n- Sinclair, Upton. The Jungle. 1906.\n- Warner, Melanie. Pandora's Lunchbox: How Processed Food Took Over the American Meal. 2013.\n- Food, Inc. (DVD) 2009.","Information Project: Intoxication across ages- Historical Perspectives\nInside the Story of America’s 19th-Century Opiate Addiction\nDoctors then, as now, overprescribed the painkiller to patients in need, and then, as now, government policy had a distinct bias\nThis cartoon from Harper’s Weekly depicts how opiates were used in the 19th century to help babies cope with teething. (Harpers Weekly)\nhe man was bleeding, wounded in a bar fight, half-conscious. Charles Schuppert, a New Orleans surgeon, was summoned to help. It was the late 1870s, and Schuppert, like thousands of American doctors of his era, turned to the most effective drug in his kit. “I gave him an injection of morphine subcutaneously of ½ grain,” Schuppert wrote in his casebook. “This acted like a charm, as he came to in a minute from the stupor he was in and rested very easily.”\nPhysicians like Schuppert used morphine as a new-fangled wonder drug. Injected with a hypodermic syringe, the medication relieved pain, asthma, headaches, alcoholics’ delirium tremens, gastrointestinal diseases and menstrual cramps. “Doctors were really impressed by the speedy results they got,” says David T. Courtwright, author of Dark Paradise: A History of Opiate Addiction in America. “It’s almost as if someone had handed them a magic wand.”\nBy 1895, morphine and opium powders, like OxyContin and other prescription opioids today, had led to an addiction epidemic that affected roughly 1 in 200 Americans. Before 1900, the typical opiate addict in America was an upper-class or middle-class white woman. Today, doctors are re-learning lessons their predecessors learned more than a lifetime ago.\nOpium’s history in the United States is as old as the nation itself. During the American Revolution, the Continental and British armies used opium to treat sick and wounded soldiers. Benjamin Franklin took opium late in life to cope with severe pain from a bladder stone. A doctor gave laudanum, a tincture of opium mixed with alcohol, to Alexander Hamilton after his fatal duel with Aaron Burr.\nThe Civil War helped set off America’s opiate epidemic. The Union Army alone issued nearly 10 million opium pills to its soldiers, plus 2.8 million ounces of opium powders and tinctures. An unknown number of soldiers returned home addicted, or with war wounds that opium relieved. “Even if a disabled soldier survived the war without becoming addicted, there was a good chance he would later meet up with a hypodermic-wielding physician,” Courtright wrote. The hypodermic syringe, introduced to the United States in 1856 and widely used to deliver morphine by the 1870s, played an even greater role, argued Courtwright in Dark Paradise. “Though it could cure little, it could relieve anything,” he wrote. “Doctors and patients alike were tempted to overuse.”\nOpiates made up 15 percent of all prescriptions dispensed in Boston in 1888, according to a survey of the city’s drug stores. “In 1890, opiates were sold in an unregulated medical marketplace,” wrote Caroline Jean Acker in her 2002 book, Creating the American Junkie: Addiction Research in the Classic Era of Narcotic Control. “Physicians prescribed them for a wide range of indications, and pharmacists sold them to individuals medicating themselves for physical and mental discomforts.”\nMale doctors turned to morphine to relieve many female patients’ menstrual cramps, “diseases of a nervous character,” and even morning sickness. Overuse led to addiction. By the late 1800s, women made up more than 60 percent of opium addicts. “Uterine and ovarian complications cause more ladies to fall into the [opium] habit, than all other diseases combined,” wrote Dr. Frederick Heman Hubbard in his 1881 book, The Opium Habit and Alcoholism.\nThroughout the 1870s and 1880s, medical journals filled with warnings about the danger of morphine addiction. But many doctors were slow to heed them, because of inadequate medical education and a shortage of other treatments. “In the 19th century, when a physician decided to recommend or prescribe an opiate for a patient, the physician did not have a lot of alternatives,” said Courtwright in a recent interview. Financial pressures mattered too: demand for morphine from well-off patients, competition from other doctors and pharmacies willing to supply narcotics.\nOnly around 1895, at the peak of the epidemic, did doctors begin to slow and reverse the overuse of opiates. Advances in medicine and public health played a role: acceptance of the germ theory of disease, vaccines, x-rays, and the debut of new pain relievers, such as aspirin in 1899. Better sanitation meant fewer patients contracting dysentery or other gastrointestinal diseases, then turning to opiates for their constipating and pain-relieving effects.\nEducating doctors was key to fighting the epidemic. Medical instructors and textbooks from the 1890s regularly delivered strong warnings against overusing opium. “By the late 19th century, [if] you pick up a medical journal about morphine addiction,” says Courtwright, “you’ll very commonly encounter a sentence like this: ‘Doctors who resort too quickly to the needle are lazy, they’re incompetent, they’re poorly trained, they’re behind the times.’” New regulations also helped: state laws passed between 1895 and 1915 restricted the sale of opiates to patients with a valid prescription, ending their availability as over-the-counter drugs.\nAs doctors led fewer patients to addiction, another kind of user emerged as the new face of the addict. Opium smoking spread across the United States from the 1870s into the 1910s, with Chinese immigrants operating opium dens in most major cities and Western towns. They attracted both indentured Chinese immigrant workers and white Americans, especially “lower-class urban males, often neophyte members of the underworld,” according to Dark Paradise. “It’s a poor town now-a-days that has not a Chinese laundry,” a white opium-smoker said in 1883, “and nearly every one of these has its layout” – an opium pipe and accessories.\nThat shift created a political opening for prohibition. “In the late 19th century, as long as the most common kind of narcotic addict was a sick old lady, a morphine or opium user, people weren’t really interested in throwing them in jail,” Courtwright says. “That was a bad problem, that was a scandal, but it wasn’t a crime.”\nThat changed in the 1910s and 1920s, he says. “When the typical drug user was a young tough on a street corner, hanging out with his friends and snorting heroin, that’s a very different and less sympathetic picture of narcotic addiction.”\nThe federal government’s efforts to ban opium grew out of its new colonialist ambitions in the Pacific. The Philippines were then a territory under American control, and the opium trade there raised significant concerns. President Theodore Roosevelt called for an international opium commission to meet in Shanghai at the urging of alarmed American missionaries stationed in the region. “U.S. delegates,” wrote Acker in Creating the American Junkie, “were in a poor position to advocate reform elsewhere when their own country lack national legislation regulating the opium trade.” Secretary of State Elihu Root submitted a draft bill to Congress that would ban the import of opium prepared for smoking and punish possession of it with up to two years in prison. “Since smoking opium was identified with Chinese, gamblers, and prostitutes,” Courtwright wrote, “little opposition was anticipated.”\nThe law, passed in February 1909, limited supply and drove prices up. One New York City addict interviewed for a study quoted in Acker’s book said the price of “a can of hop” jumped from $4 to $50. That pushed addicts toward more potent opiates, especially morphine and heroin.\nThe subsequent Harrison Narcotic Act of 1914, originally intended as a regulation of medical opium, became a near-prohibition. President Woodrow Wilson’s Treasury Department used the act to stamp out many doctors’ practice of prescribing opiates to “maintain” an addict’s habit. After the U.S. Supreme Court endorsed this interpretation of the law in 1919, cities across the nation opened narcotic clinics for the addicted – a precursor to modern methadone treatment. The clinics were short-lived; the Treasury Department’s Narcotic Division succeeded in closing nearly all of them by 1921. But those that focused on long-term maintenance and older, sicker addicts – such as Dr. Willis Butler’s clinic in Shreveport, Louisiana – showed good results, says Courtwright. “One of the lessons of the 20th-century treatment saga,” he says, “is that long term maintenance can work, and work very well, for some patients.”\nCourtwright, a history professor at the University of North Florida, wrote Dark Paradise in 1982, then updated it in 2001 to include post-World War II heroin addiction and the Reagan-era war on drugs. Since then, he’s been thinking a lot about the similarities and differences between America’s two major opiate epidemics, 120 years apart. Modern doctors have a lot more treatment options than their 19th-century counterparts, he says, but they experienced a much more organized commercial campaign that pressed them to prescribe new opioids such as OxyContin. “The wave of medical opiate addiction in the 19th century was more accidental,” says Courtwright. “In the late 20th and early 21st centuries, there’s more of a sinister commercial element to it.”\nIn 1982, Courtwright wrote, “What we think about addiction very much depends on who is addicted.” That holds true today, he says. “You don’t see a lot of people advocating a 1980s-style draconian drug policy with mandatory minimum sentences in response to this epidemic,” he says.\nClass and race play a role in that, he acknowledges. “A lot of new addicts are small-town white Americans: football players who get their knees messed up in high school or college, older people who have a variety of chronic degenerative diseases.” Reversing the trend of 100 years ago, drug policy is turning less punitive as addiction spreads among middle-class, white Americans.\nNow, Courtwright says, the country may be heading toward a wiser policy that blends drug interdiction with treatment and preventive education. “An effective drug policy is concerned with both supply reduction and demand reduction,” he says. “If you can make it more difficult and expensive to get supply, at the same time that you make treatment on demand available to people, then that’s a good strategy.” Read More"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:54f849d5-0c43-4338-8761-472899d563e6>","<urn:uuid:179a836c-149f-4fcf-9c67-d8bc4d01c8a1>"],"error":null}
{"question":"Did Leo Fender and Eric Clapton both play the Stratocaster guitar?","answer":"No - Leo Fender never learned to play the guitar despite founding one of the most popular guitar brands. Legend has it he couldn't even tune one, as he was too busy tinkering with them. In contrast, Eric Clapton not only played the Stratocaster but fully embraced it - after initially playing Gibson guitars, he switched to the Stratocaster and has continued playing it for over three decades.","context":["I love music. I feel its beats in my bones. Even human voices are acapella treats – except those whose mission is spite and hate. Their sour voices sow discord.\nHere’s a discussion of some of my favorite musical instruments: Piano, tuba/sousaphone, flute, and guitar.\nWhen a piano is played, each key controls a hammer that strikes a taut-and-tuned string inside, under its elongated top. The sound resonates from strings, making it a stringed instrument like a harp or a guitar — but the action that produces the sound is a strike, putting it in the percussion category, along with other melodic instruments like the xylophone or steel drum. Thus, pianos are considered to be both a percussion and stringed instrument. Both describe how the piano works. Some consider the piano to be a form of hammered dulcimer, another hammered-string instrument that’s hard to pin down. It’s also a keyboard instrument, which are never just keyboard instruments alone — that would also include the pipe organ (wind), harpsichord (string), glockenspiel (percussion), and synthesizer (electronic).\nThe latter are fundamental to rock ‘n roll bands.\nPianos are large instruments, so they stand like a table on the floor. Another large instrument is a tuba, held, hug-like, in a seated player’s lap.\nIn a Harvard University basement, the King Tuba, possibly commissioned by John Philip Sousa, is about 7 feet tall and 100 pounds. It was restored in 2019 and occasionally makes appearances at performances. John Philip Sousa created the Sousaphone (duh) because he wrote march music and needed the bass instrument to be carried by a member of a marching band. The instrument has been popular for a century, featured high school and college football half-time entertainment. My grandpa played one, while my dad played trombone.\nAt the opposite end of the tonal spectrum of a band is the flute.\nFlutes were among the earliest musical instruments, present in ancient cultures throughout the world. The oldest one that modern archaeologists have discovered so far is 35,000 years old, uncovered in the Hohle Fels Cave near Ulm, Germany. It was created in the middle of the last Ice Age.\nFlutes have also been components of modern rock ‘n roll bands: Jethro Tull, the Moody Blue, and certain incarnations of Jefferson Starship.\nLeo Fender Didn’t Play Guitar\nFender is one of the most popular guitar and amplifier brands, perhaps best known for its enduring classic the Stratocaster electric guitar. The founder’s interest and expertise, however, was solidly in creation, not performance. Before inventing the first mass-produced solid-body electric guitar, Leo Fender was a radio repairman who sometimes tinkered with his musician friends’ instruments. He left the music to Les Paul.\nWhile he briefly played piano and saxophone as a youth, even after decades in the guitar business, he never actually learned how to play the instrument. Legend has it that he couldn’t even tune one. He was too busy tinkering with them: Country music guitarist Bill Carson (who has been dubbed the “test pilot of the Stratocaster”) told Reverb that Fender would show up to his gigs to swap out equipment.\nHere’s one of the most famous Stratocaster creations: “Layla” by Derek and the Dominos, aka Eric Clapton and misc. crew:","GUITAR TECHNIQUE SESSIONS .... EXERCISE OF THE WEEK ... KNOWLEDGE DATABASE\nEach week a new exercise designed to improve the guitarist's left hand dexterity and stamina OR a technique session will be added to this site. Follow this link!\nThe year was 1950 and Leo Fender had just revolutionized the music world. The Telecaster, then known as the Broadcaster, the first solid bodied guitar to be mass produced, had been released. Country and Western musicians were the intended market, Leo had no way of foreseeing the impact his invention would have. By 1953, Leo Fender was feeling the need to upgrade and change his Telecaster. He'd been selling them for four years and artist opinions and suggestions led him to believe that the instrument could be improved upon. For one thing, the slab-bodied Telecaster felt and looked chunky, the hard edges dug into the player's ribs. The headstock had been described as having the aesthetic of a pretty girl in a bad dress. The Telecaster bridge, while\noffering more range of adjustment than just about anything else on the market, was still only able to adjust intonation on string pairs, causing some upper-register tuning problems. Simple by today's standards, the Telecaster had been both ground-breaking and a solid workingman's tool. But, it was felt that things could be improved upon. Leo scratched his head and sat down at his drawing table.\nHow does one improve on a really excellent design? I believe Leo looked at his design goals and decided to start with a blank sheet of paper. He was looking for sonic flexibility, so three pickups would be incorporated into the design. The bass/treble slant pickup placement of the Telecaster was a winning idea, so it was retained. Two additional exposed pole pickups were placed in the neck\nposition and at the midpoint between neck and bridge pickups. The body--now there was a sticky issue. Players had suggested contouring the wood to afford more comfort for the player's ribcage. Leo removed wood from both the face of the guitar where the player's right forearm would rest and then scooped out the back, reducing body thickness in the waist to almost a knife edge. I believe this loss of mass forced Leo to add the now famous upper horn to compensate for possible neck-droop.\nThe bridge, now there was a stroke of pure genius. No production electric had a single piece vibrato unit that would return the instrument to anything like an in-tuned condition. Leo streamlined the bridge/vibrato into one sleek unit that utilized five powerful springs to balance string pull and return th e guitar to pitch after use. The bridge incorporated six totally\nindependent saddle pieces so that both intonation as well as string height could be set for each string. No other production\ninstrument could boast of this adjustability. The string arc could be adjusted to the camber of the fingerboard. No other guitar was so comfortable to hold and play. Here was a player's instrument!\nThankfully, Leo decided that the upgraded instrument would not replace the Telecaster, but augment the Fender product line. The Stratocaster was born. This has been the case in Fender history since. The Strat was meant to replace the Tele but didn't. Then, in the early sixties, the Jaguar and Jazzmaster were to replace the Strat and Tele, but didn't. Thankfully, we've kept the Strat and Tele, as Jags and Jazzmasters come in and out of style, the two workhorses don't!\nThis is strange in and of itself, as the Jaguars and Jazzmasters are\nexceptionally fine instrument with character and appearance rivaling or\nsurpassing the strat and tele.\nAlthough Buddy Holly, Dick Dale, Hank Marvin and scores of other professional musicians embraced the futuristic Stratocaster, by the mid sixties, the design was falling from popularity with players. Oddly, it was the design's huge success that almost was its downfall. Fender's only real competition in the solid bodied guitar market, Gibson, had produced the Les Paul model and, later, the SG series which did nothing to further guitar technology, but were merely overly complex 'me-toos' in most ways, good guitars in their own right, but hardly ground-breaking. (The ES series of semi acoustic arch top electric guitars were innovative designs.) In fact, because of their complex construction, they were much more fragile than a Fender.\nIn the early sixties, Fenders were more sought-after by guitarists, so when young aspiring guys in England like Eric Clapton and others of that generation couldn't get a Fender, they picked up and settled for the more frumpy and less desirable Gibsons. Of course, when these same guys hit it big in the States, American kids didn't want Fenders any longer! Their heroes played Gibsons. By the end of the sixties, Fenders were out of favor and basically to be found in the cut-out bins! I'm sure you've all heard the story of Eric Clapton buying a dozen Fenders in a music store in Nashville for a hundred bucks a pop! Then came Hendrix.\nJimi Hendrix saw the potential of the Stratocaster in a way Leo Fender would have imagined impossible in 1952. He single-handedly put Fender back on the map. Soon Eric Clapton chucked his fat little 'Paul and went to a Strat--and in the thirty three years (as of this writing) since, he's never looked back! It is a legend that Jimmy Page recorded mostly with his Yardbirds-era Tele. I think it is a safe assumption he either retired the Tele from the road when it became too beat up, or that he went to the Paul to eliminate feedback inherent with the single coil pickups. Since the Woodstock Era, taste in popular music has changed, some styles come and go, some styles just disappear, but the Stratocaster has been there for all of them.\nEven though the Stratocaster has represented guitar-perfection from day one, and the BASIC design has gone unchanged for almost fifty years, there have been design changes over the years. These changes have often stemmed from a desire to improve the instrument, others, unfortunately, were schemes hatched purely to cut production costs. Some model years are considered inferior. While you can find some genuinely horrible Strats out there, these are truly few and far between. A decent Strat from any year is a good guitar. Walter Trout, famed blues guitarist, loves his early Seventies Strat. Yngwie Malmsteen plays Strats mostly from the late sixties early seventies period, both these men play guitars that some Strat players wouldn't touch. Check out the vintage Fender ad above. I've only seen this ad once, in a scholastic rag I received in 1973! There is a school of player for whom the late fifties, early sixties--up until the CBS take-over in 1964--are the only guitars to play. Indeed, Rory Gallagher would only play one Strat, his Strat, a 1962 model he purchased at the age of fifteen in 1963!\nI own or have access to six Strats. Why? Each one is representative of a different style of construction and era of Fender 'style'. I find the early sixties reissue to be a great guitar for R&B, blues and contemporary Christian. My '94 and '95 Strats are ideal for most styles of music and are the only ones, out of the box, that were truly comfortable for metal and contemporary 'heavy' music. My '72 (see photo at head of page, that's me in 1974 with a '73 Strat) was just great for funk, but with the addition of Texas Special pickups, is now better suited to blues, traditional 70's and 80's rock, and the adult contemporary sound. Maybe it's me, but the 1968 Strat has a great vibe for Stones, Beatles, and sixties-era hippie stuff. My favorite Strat (at the moment) is my Rory Gallagher Tribute model, which is actually not really a Fender at all, but a home-brew of genuine Fender parts as well as aftermarket replacement parts and hardware from my sock drawer! PLEASE CLICK HERE to read about various Fender construction techniques\nStampede - Bronx Garage Band\nOf course, I didn't begin with a Strat, In the beginning, I was a Telecaster man. In fact, I still am -- I've still got my original 1971 Tele purchased new from Manny's Music on 48th Street in Manhattan, August 1972. This guitar, still in regular service, is a year older than the lead vocalist in my current ensemble. Some mornings I feel so old... Here's a photo of me and my Tele at a Valentine's Day dance in 1973. I was sweet sixteen and never kissed. (Ahem, well hardly!) Man, I sure wasn't a snappy dresser! At least my trousers matched the drums! The band was called Stampede, and existed from April of 1972 until June of 1973. The original lineup was Anthony Pernice on guitar, Mark Berlingeri on guitar, and Peter DiRoma, drums. I came along in May of '72 as the bass player, but replaced Pernice on guitar when his family moved out of state. Bert Muriello replaced Pete DiRoma soon after. Victor Minei was added on Saxophone as well as Joey Pane on keyboard (a Hammond Porta-B) and Ray Davis vocals and cowbell. [And again, the internet reunites old friends. Anthony Pernice came to a book signing at the Nyack library--the first time Harry and Anthony had seen each other in thirty one years!]\nMark Berlingeri and Victor Minei during our rendition of Brown Sugar . Vic was probably the most musically trained of the lot of us. He was excellent at transposing whatever the two lame guitar players were yelling at him by way of chords and structure. Vic played in the Mount St. Michael marching band as well. The Telecaster you see Mark playing here is a 1966 model that he bought used for $125.00 in 1971. It had the coolest alligator skin case. He played the Tele through a SUNN Solo II combo amp --a very high-powered transistor unit with not too much tone. He managed though.\nMark and his Tele. That's the one that got me hooked on the Fender sound and feel. Here's a shot of Mark playing up a storm while Ray rings his bell. All these shots of Stampede were taken at the same gig (like you couldn't have figured that one!) at Redeemer Lutheran Church on Barnes Avenue in the Bronx -- a Valentine's Day dance for junior high kids in February of 1973.\nLeft to right, back row: Ray Davis, Mark Berlingeri, Joey Pane. Left to right front row: Bert Muriello, Harry Pellegrin, Victor Minei.\nJim Abbott was our roadie -- and he did an amazing job considering that he was the only guy doing it! (Photo courtesy of Michael Giga, who I never could convince to become our bass player.)\nIn 1975, I returned to Mount St. Michael with another Class of '74 grad, Jay Monaco and played a couple of tunes with him and Tommy Manna, class of '75 and a bass player named Kevin (out of frame.) I was using an SG in my own band, but found that my Tele contrasted and complimented Tommy's Les Paul much better . Here we are in a dress rehearsal early in the evening. Well, I was dressed... I wish I had worn a pair of jeans and a tee shirt!\nSame night, same rehearsal. Had to get our bass player into the picture. Photos courtesy of Mike Giga, who was kind enough to document much of my early years on film. This guy should have become a famous rock or combat photographer. He was always excellent at capturing an image despite any and all obstacles--weather, security, inadequate light, cheap beer...\nHere I am with the 1971 Telecaster in April of 2004. I wish the old man looked as good as the old Tele !\nOf course, the old man has his 1972 Strat in Olympic White to relieve the need for a seventies issue Strat. Having grown up with seventies era Fender's, that particular (and unpopular) vintage seems the most natural and comfortable. So what can I tell you?\nBy Harry George Pellegrin. The\nfirst in the Gary Morrissey series of mysteries. Dealing\nwith modern subject matter in the classic style of the 1940's\nMystery Noire masters--think Raymond Chandler in New York in the\n1980's... LOW END is the story of a drug addict who is\nmurdered after he believes he has found evidence of a major\ngovernment conspiracy. Is it only drug-induced paranoia?\nMight be, except his paranoia could be considered justified: he\nwas murdered, after all. Friend Gary Morrissey takes it\nupon himself to find out just what happened and lands himself in\nSee more info...\nClassic Guitar Method\nwritten, transcribed, edited and arranged by Harry G. Pellegrin: Now in one volume, much of what the novice classical guitarist will need to know to lead him or her to the recital stage. From proper instrument care and maintenance to the necessary technical skills, musical mind-set, and the standard repertoire—all exposed and explored with enough detail and insight that the student will wish to keep this book handy years to come as a ready reference source.\nSee more info...\nDEEP END: The Wreck\nof the Eddie Fitz\nHarry George Pellegrin. A mystery novel. Involving a\nsemi-professional musician and a Kreyol death cult, DEEP END\ntakes the reader from the bottom of Long Island Sound to the\nsteamy streets and Blues clubs of New Orleans. Alternative\nspirituality does battle with the common working man. Published by\nPAB Entertainment Group in association with LULU.com.\nSee more info...\nOriginal Music by Harry G. Pellegrin:\nis a departure for me\nas it is totally keyboard. Well, the guitar did show up on one\n\"...Reflecting Pools is a notable first album [for\nMr. Pellegrin]. A dramatic sense of tonality and mood are\npropelled by exemplary musicianship and exciting compositional\n...And containing nine tracks that are relaxing, inspirational\n-- sounds like a snooze. Not really, this is great stuff to\nlisten to on a rainy afternoon, while with your significant\nother (nudge, nudge, know what I mean?)\nPlease visit the Reflecting Pools\npage on this site or\nAVAILABLE NOW !\nThe Classic Guitar Method: Now in one volume, much\nof what the novice classical guitarist will need to\nknow to lead him or her to the recital stage. From\nproper instrument care and maintenance to the\nnecessary technical skills, musical mind-set, and\nthe standard repertoire—all exposed and explored\nwith enough detail and insight that the student will\nwish to keep this book handy years to come as a\nready reference source.\nWith the aid of a good teacher, the student will\nrapidly progress through The Classic Guitar Method\nattaining technical proficiency and musical\nThis method stems from the need to incorporate a\nnumber of schools into a single cohesive curriculum.\nYears of honing a logical approach to the guitar and\nthe creation of music culminate in this volume. As a\nself-proclaimed Disciple of Valdés-Blain , much of\nthat famed teacher's focus can be found in Mr.\nPublished by PAB Entertainment Group, P.O. Box 2369\nScotia, New York 12302\nPlease go to www.lulu.com to order.\nSOME GREAT FENDER-RELATED SITES\nWelcome to Fender .com!\n|Guitars , basses, parts, and accessories. A dealer locator and internal search are also provided.\nwww. fender .com/ - 3k - Cached - Similar pages\nSquier® Guitars by Fender ®: The Official Website\n|Official Website of Squier Guitars and Basses by Fender .\nwww.squier guitars .com/ - 15k - Jul 12, 2005 - Cached - Similar pages\nVintage Guitars Info - Fender , collecting vintage guitars fender ...\n|Overview of collecting vintage Fender guitars , basses and amps. ... The Fender Stratocaster (and Telecaster) from the 1950's put the solidbody electric ...\nwww.provide.net/~cfh/ fender .html - 79k - Jul 12, 2005 - Cached - Similar pages\nVintage Guitars Info - Fender custom color finishes on vintage ...\n|But all the importers seemed to have brought Fender guitars into the UK *without* ... The 1956 Fender catalog states, \" Stratocaster guitars are available in ...\nwww.provide.net/~cfh/ fender c.html - 101k - Cached - Similar pages\n[ More results from www.provide.net ]\nWhat's New? The The Two New Albums!\nHey, the new albums are out! That's right, finally a follow-up to the reissue of my old album from the late 1980's and its sequel as well.\nReflecting Pools is a departure for me as it is totally keyboard. Well, the guitar did show up on one track... Reflecting Pools is an ethereal journey into the realm of relaxation. In That Zone is a more classically structured exploration of mood and personality."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:8cd76394-7a13-4418-a584-83fbfe11d824>","<urn:uuid:e44dc71c-e5c5-449f-8776-f10e8c541851>"],"error":null}
{"question":"What is the correct way to fix the redundancy in 'mutual respect for each other'?","answer":"To fix the redundancy in 'mutual respect for each other', simply remove 'for each other' since 'mutual' already means directed and received by each toward the other.","context":["As noted in the article 200 Common Redundancies, one way to streamline your writing is to eliminate repetitious expressions. Redundancies (also called pleonasms and deadwood) can bore your readers and distract from the words that matter.\nNOTE: To view this tip sheet without ads, click on the printer icon near the top of the page.\n- The problem was defused before it became an acute crisis.\nCrisis means \"a crucial or decisive point or situation\" or \"a traumatic change.\"\nTip: Don't be cute: cut acute.\n- The Card Act requires advance warning of changes in interest rates and card fees.\nIf a warning isn't given in advance of an event, by definition it's not a warning.\nTip: Back off from advance.\n- All throughout the holiday weekend, the women talked about the election and the issues it raised.\nThroughout means \"all through\" or \"during every part of.\"\nTip: You don't need all at all.\n- When adults die unexpectedly, their deaths are usually followed by an autopsy to determine the cause of death.\nAutopsy means \"examination of a cadaver to determine the cause of death.\"\nTip: Let the cause of death rest in peace.\n- Laser light focused on tiny plastic spheres caused them to move toward each other and bond together.\nThe verb bond means \"to join securely.\" Things can't bond apart.\nTip: Sever together.\n- Why do brides traditionally carry a bouquet of flowers when they are getting married?\nBouquet means \"a small cluster or arrangement of flowers.\"\nTip: Toss the flowers.\n- I used to commute back and forth between Vienna, where I lived, and Munich, where my husband lived.\nBack and forth means \"moving from one place to another and back again.\"\nTip: Enough with the back and forth.\n- Investment in arts and culture produces other desirable benefits.\nBy definition, you can't have an undesirable benefit (\"something that promotes or enhances well-being\").\nTip: Dump desirable.\n- BP contracted for $50 million worth of TV advertising to manage their image during the course of the disaster.\nBecause during means \"throughout the course or duration of,\" this phrase is verbose.\nTip: Strike the course of--of course.\n- Bond got closer to finding the man responsible for the betrayal of Vesper--the woman he loved and lost by a fatal murder.\nFatal means \"causing death,\" and murder means \"the unlawful killing of one human by another.\"\nTip: Kill fatal.\n- As a former veteran of the armed forces, I have high regard for those who served honorably in the military.\nBy definition, a veteran is a person who has served in the armed forces.\nTip: Discharge former and armed forces.\n- How do you clean the front headlight lens on a 2002 Honda Accord?\nIf a car's headlight (a light mounted on the front of a vehicle) isn't up front, it's probably not worth cleaning.\nTip: Remove front.\n- Following his defeat in the leadership race, the shadow foreign secretary is keeping the party guessing about his future plans.\nOnly a time traveler would bother making plans for the past.\nTip: Disregard the future.\n- My grateful thanks to all who have worked so hard to bring this part of our undertaking to a satisfactory conclusion.\nThanks, but no thanks: grateful means thankful.\nTip: Please get rid of grateful.\n- The resident has no legal right to occupy his or her assigned room or any other residence hall room during the interim period of time between academic terms.\nYou have to go to college to come up with a double redundancy like this one.\nTip: Expel interim and of time.\n- Our paper on private sector funding is evidence of joint collaboration between two departments that in the past haven't always got on.\nAs used here, collaboration means a joint effort.\nTip: Snuff out the joint.\n- Yes, you can be a part of a live studio audience on a real Hollywood TV set at one of the major TV studios!\nUnless, of course, you'd prefer to sit over there with the dead people.\nTip: Kill live.\n- We bring people together from all backgrounds to learn and develop mutual respect for each other.\nMutual means directed and received by each toward the other.\nTip: Forgo for each other.\n- One of our goals with Google TV is to finally open up the living room and enable new innovation from content creators.\nAn innovation is something newly introduced.\nTip: Out with the new.\n- This government is fighting a party of \"no\" that is only interested in nostalgia for the past.\nIn contrast to \"future plans,\" nostalgia means a longing for something past.\nTip: Forget of the past.\n- Some of the potential benefits of the information revolution may have been over exaggerated.\nTo exaggerate means to overstate, so over exaggerate is the equivalent of over overstate.\nTip: Omit over.\n- To borrow an overused cliché, I am fast approaching the belief that what is not broken should not be fixed.\nQuestion for philosophy majors: if a cliché is underused, is it still a cliché?\nTip: Don't use overused.\n- Two new books argue that the hottest idea in physics is just a passing fad.\nBy definition, a fad is passing: a fashion taken up with enthusiasm for a brief period of time.\nTip: Take a pass on passing.\n- The woman lived on past memories, robbing herself of any chance of happiness.\nAll memories depend on the past.\nTip: Leave the past behind.\n- It's my personal opinion that public relations is not the best place to spend scarce marketing dollars.\nThe possessive determiner my is personal enough for any opinion.\nTip: Don't get personal.\n- She wasn't prepared to spend precious political capital on battles that could be postponed until later.\nTo postpone means to delay until a future time--that is, until later.\nTip: If you can't identify a more specific time (say, \"later in the year\" or \"later in the session\"), cut until later.\n- The reason I write is because I find comfort in words.\nThe word reason implies cause; because implies reason.\nTip: Replace because with that, or drop \"The reason . . . is.\"\n- Whenever the three of us got together, we reverted back to our old habits.\nTo revert means to go back to a former state, practice, or condition.\nTip: Bounce back.\n- If you've never stayed at a Manor Inn before, you're in for an unexpected surprise.\nA surprise may be pleasant or unpleasant, but it's always unexpected.\nTip: Toss out unexpected.\n- The viper's poisonous venom may destroy muscle and red blood cells.\nVenom means poison or a poisonous secretion.\nTip: Extract poisonous.\nFor practice in applying these tips, see our Exercise in Eliminating Deadwood."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:a0849d7d-e042-4ed1-9a14-51890d0f4664>"],"error":null}
{"question":"What are the similarities and differences between the protein aggregates found in ALS caused by SOD1 mutations versus those involving TDP-43? First, what are the key features of each type? Después, ¿cómo se comparan en términos de su composición y comportamiento?","answer":"The protein aggregates in SOD1-linked ALS and TDP-43-linked ALS show distinct characteristics. SOD1 aggregates are composed almost entirely of intact SOD1 protein, with only trace amounts of other nerve proteins present by chance. These aggregates don't contain significant amounts of protein fragments or extensive chemical modifications. In contrast, TDP-43 aggregates form through its C-terminal region which is highly prone to aggregation. TDP-43 contains prion-like glutamine/asparagine (Q/N) rich domains that make it aggregation-prone. While both types of aggregates contribute to neurodegeneration, TDP-43 aggregates are found not only in ALS but also in other conditions like FTLD-U, Alzheimer's disease, Parkinson's disease and chronic traumatic encephalopathy. Additionally, TDP-43's prion-like domains suggest potential cell-to-cell spread, while such properties haven't been demonstrated for SOD1 aggregates.","context":["A Clearer Picture of Cloudy Eyes\nA new study provides more insight into cataracts, the leading cause of vision loss and blindness in the elderly, finding that small pieces of a perfectly normal protein become toxic during the aging process.\nA cataract results from deterioration in the highly ordered assembly of crystallin proteins in the eye lens. Normally, the ordered structure keeps lenses clear and able to efficiently transmit light. However, crystallins gradually break down during aging, causing the lens to become opaque and scatter light instead. Besides age, other risk factors such as diabetes, ultraviolet radiation, or drugs like corticosteroids can also contribute to cataracts.\nLike cataracts themselves, the exact mechanisms governing their formation are cloudy, but Krishna Sharma and colleagues found that tiny bits of crystallin greatly contribute to this process.\nThey compared a range of human donor lenses and found that aged and cataract lenses accumulated about four times as many short (~10-20 amino acids) crystallin fragments compared to young lenses. These fragments could readily bind full-length crystallins, which disrupted their natural shape and organization and caused them to become insoluble.\nIronically, these tiny fragments are a by-product of the eye’s efforts to stay healthy; when a crystallin becomes damaged, other proteins chew it up to remove it; but occasionally the process is incomplete, leaving tiny pieces that can cause greater damage.\nCorresponding Author: K. Krishna Sharma, Department of Ophthalmology, University of Missouri School of Medicine, Columbia, MO; Phone: 573-882-8478, email: Sharmak@health.missouri.edu\nALS Aggregates are composed of only one protein\nResearchers have provided a big new clue to help combat amyotrophic lateral sclerosis (ALS), deciphering that the dense protein aggregates that contribute to the nerve decay of ALS are composed of just one protein: superoxide dismutase (SOD1).\nWhile the aggregation of mutated SOD1, a protein that normally protects cells from free radical damage, is a tell-tale sign of familial ALS, the exact composition of these aggregates has been unclear. Identifying the other proteins present and if they are modified in some way could help answer how they form and why they are so toxic.\nJulian Whitelegge and colleagues Joan S. Valentine and David Borchelt used mass spectrometry to uncover the components of these aggregates and discovered, somewhat surprisingly, that they were composed almost entirely of SOD1 (some samples contained trace amounts of random abundant nerve proteins that likely got there by happenstance).\nIn addition, their analysis of ALS mouse spinal cords showed almost all the SOD1 was fully intact protein and not partial or damaged fragments. Likewise, the researchers did not find evidence for extensive chemical modifications (that were not readily removed by DTT treatment).\nWhile many questions about these aggregates still remain, this study has given scientists a starting point, suggesting that aggregation is an intrinsic property of mutant SOD1, very much like the amyloid plaques associated with Alzheimer’s.\nCorresponding Author: Julian Whitelegge, The NPI-Semel Institute, David Geffen School of Medicine, UCLA; Phone: 310-794-5156; email: email@example.com\nThe American Society for Biochemistry and Molecular Biology is a nonprofit scientific and educational organization with over 11,900 members in the United States and internationally. Most members teach and conduct research at colleges and universities. Others conduct research in various government laboratories, nonprofit research institutions and industry. The Society’s student members attend undergraduate or graduate institutions.\nFounded in 1906, the Society is based in Bethesda, Maryland, on the campus of the Federation of American Societies for Experimental Biology. The Society’s purpose is to advance the science of biochemistry and molecular biology through publication of the Journal of Biological Chemistry, the Journal of Lipid Research, and Molecular and Cellular Proteomics, organization of scientific meetings, advocacy for funding of basic research and education, support of science education at all levels, and promoting the diversity of individuals entering the scientific work force.\nFor more information about ASBMB, see the Society’s Web site at www.asbmb.org.\nAAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert! system.","| Article Access Statistics|\n| Viewed||2961 |\n| Printed||83 |\n| Emailed||2 |\n| PDF Downloaded||62 |\n| Comments ||[Add] |\n| Cited by others ||1 |\nClick on image for details.\n|Year : 2013 | Volume\n| Issue : 2 | Page : 107-110\nProtein aggregates and regional disease spread in ALS is reminiscent of prion-like pathogenesis\nDepartment of Neurology, University of Miami Miller School of Medicine, Clinical Research Building, 1120 NW 14 Street, Suite 1317, Miami, FL 33136, USA\n|Date of Submission||27-Feb-2013|\n|Date of Decision||08-Mar-2013|\n|Date of Acceptance||05-Mar-2013|\n|Date of Web Publication||29-Apr-2013|\nDepartment of Neurology, Clinical Research Building, 1120 NW 14 Street, Suite 1317, Miami, FL 33136\nSource of Support: None, Conflict of Interest: None\nAmyotrophic lateral sclerosis (ALS) typically commences in a discrete location in a limb or bulbar territory muscles and then spreads to the adjacent anatomical regions. This pattern is consistent with a contiguous spread of the disease process in motor neuron network resulting in progressive motor weakness. The etiology of ALS onset and the mechanism of the regional ALS spread remain elusive. Over the past 5 years, identification of mutations in two RNA binding proteins, trans active response (TAR) DNA-binding protein (TDP-43) and fused in sarcoma (FUS), in patients with familial ALS has led to a major shift in our understanding of the ALS disease mechanism. In addition to their role in RNA metabolism, TDP-43 and FUS form protein aggregates in the affected neurons. More recent findings demonstrating that both TDP-43 and FUS contain glutamine/asparagine (Q/N) residue-rich prion-like domains have spurred intense research interest. This brief review discusses the prion-related domains in TDP-43 and FUS and their implication in protein aggregate formation and disease spread in ALS.\nKeywords: ALS, aggregates, FUS, prion, Q/N domain, TDP-43\n|How to cite this article:|\nVerma A. Protein aggregates and regional disease spread in ALS is reminiscent of prion-like pathogenesis. Neurol India 2013;61:107-10\n| » Introduction|| |\nFirst described in clinical detail by Charcot  in 1869, amyotrophic lateral sclerosis (ALS or motor neuron disease, MND) is one of the earliest known neurodegenerative diseases. Following Charcot's initial description, Gowers  in 1886 commented on variable anatomic sites of disease-onset and independent and variable abnormalities of the upper motor neuron and lower motor neuron in ALS, thus broadening the clinical spectrum of the disease. Gowers particularly emphasized focal onset and contiguous spread of the disease process in his description, which is still a classic: …from the part first affected the disease spreads to other parts of the (same) limb. Before it has attained a considerable degree in one limb, it usually shows itself in the corresponding limb on the other side (homologous part)…\nThis original and astute clinical observation by Gowers has been repeatedly witnessed by practicing neurologists well over a century. The classical focal and asymmetric onset of ALS in spinal or bulbar region and then its spread to contiguous group of motor neurons is now so well known that an experienced physician can often predict the 'logical' spread in a particular case.  Recent detailed autopsy studies of ALS patients have confirmed that loss of motor neurons is most pronounced at the site of onset and diminishes in a gradient fashion with further distance from that site.  The pathophysiologic mechanism underlying the focal onset and regional spread of ALS is unclear. Over the past two decades, several aberrant phenomena including excessive oxidative stress, excitotoxicity, mitochondrial dysfunction, inflammation, and altered axonal transport have been implicated in ALS pathogenesis. However, it is not easily discernable how any of these generalized processes could by itself explain the focal initiation or the progressive spread of the disease through the motor neuronal pool.\nApproximately 10% of ALS cases are familial (fALS), mostly autosomal dominant, and the rest are sporadic (sALS). For almost 15 years, the only gene clearly associated with fALS was the Cu-Zn superoxide dismutase 1 (SOD1) gene, which accounts for 20% of fALS cases. The identification of SOD1 mutations  in 1993 ushered in the molecular era of ALS research, and significant insight into ALS pathogenesis has been gained through identification of pathways directly affected by the toxicity of mutant SOD1. Two main discoveries in mutant SOD1-mediated ALS in rodent models are the demonstration that the SOD1 protein aggregates produce a toxic gain of function that causes neuronal loss and the disease can spread in a noncell autonomous fashion in the nervous system. ,\nA major shift in our understanding of ALS pathogenesis occurred in 2006 with the identification of a 43-kDa transactive response (TAR) DNA-binding protein (TDP-43) as a key pathological substrate of cellular inclusions in sALS and non-SOD1 fALS, and frontotemporal lobar degeneration with ubiquitinated inclusions (FTLD-U).  It was soon followed in 2008 by the successful discovery of dominant mutations in the TDP-43 gene as a primary cause of ALS,  thus providing the proof of principle that aberrant TDP-43 can trigger neuronal degeneration and cause ALS. The identification of TDP-43 mutations was soon followed by the discovery of mutations in another RNA/DNA-binding protein, fused in sarcoma (FUS) in 2009, as a primary cause of fALS.  Together, TDP-43 and FUS gene mutations account for approximately 10% of the fALS cases. Both in TDP-43- and FUS-linked ALS, the age and site of disease-onset and clinical progression are variable, as in sporadic ALS, and both show characteristic pathological features of the sALS. Incomplete penetrance has been documented for several of TDP-43 and FUS mutations, accounting for some sALS cases.\nBoth TDP-43 and FUS are predominantly nuclear proteins involved in diverse aspects of RNA metabolism;  however, in ALS disease tissue both are observed as aggregates in the cytosol of affected neurons. ,,, This finding suggests that aberrant protein aggregation and its cytosolic accumulation may play a key role in ALS pathogenesis, akin to the central role of protein misfolding and aggregations observed in other neurodegenerative diseases.  Interestingly, both TDP-43 and FUS contain 'prion-related' glutamine/asparagine (Q/N) rich domains and in the case of TDP-43, almost all the ALS-associated mutations occur in this region. \nIn living organisms, proteins are regularly synthesized and degraded each day. During protein synthesis, nascent polypeptide chain exits from ribosomal tunnel and passes through a highly organized and precisely monitored protein quality control systems (see review , ). The newly formed protein is folded in its three dimensional structure, steered by molecular chaperons, and transported to appropriate subcellular site(s) where it executes its physiological functions. In prokaryote and eukaryote cells, an equally efficient and cooperative protein clearance system known as ubiquitin-proteasome system (UPS) and lysosomal autophagy also exists to clear the abnormally misfolded proteins and protein aggregates in order to maintain cellular proteostasis and normal health (reviewed elsewhere  ). For clearance through UPS, misfolded peptide is first targeted and tagged to ubiquitin, a process known as ubiquitination. Ubiquitination requires an isopeptide bond between the targeted protein and ubiquitin; the covalent isopeptide bond is generated between the glycine and lysine aminoacids of the protein moieties. In this context, it is interesting that most pathogenic ALS-associated TDP-43 and FUS mutations occur in the glycine-rich regions of these proteins. Also interestingly, rare mutations in ubiquilin2  and SQSTM1 (sequestone1/p62),  which are involved in the clearance of misfolded and aggregated proteins, are shown to cause TDP-43 positive cellular aggregates and ALS in rare fALS cases. , Excess of misfolded proteins and uncleared aggregates are inherently toxic to cell, a phenomenon well demonstrated in in-vitro cell models.  Why do certain proteins, such as TDP-43, FUS, amyloid-β, tau, etc., aggregate, how these aggregates cause cell toxicity and whether such aggregate-related toxicity is transferrable from cell to cell is an area of intense current research in neurodegenerative diseases.\nCurrently, prion protein is the only known example of a protein capable of propagating a self-replicating conformation that can spread like infectious particles (transmissible spongiform encephalopathies) across cells and individuals. However, additional proteins are now recognized that can exhibit 'prion-like' behavior under certain circumstances. Such proteins with more than one conformation are known to exist in yeast, invertebrate and mammalian cells.  In these cases, unlike in human prion diseases, the adoption of an alternate protein conformation and template-based spreading of this altered conformation by additional conversion of normal forms, appear not to be deleterious and cause disease, but instead regulates the physiological function of the aggregated protein.\n'Prion-like' behavior of proteins is best characterized in yeast Sup35, Ure2, and Rnq1 proteins.  For example, the Sup35 protein is normally required for stop-codon recognition and translational termination; however, under certain stressful conditions it can form a self-propagating fibrillar-b-sheet conformation transmissible to offsprings. Such self-propagating fibrillar-b-sheet conformation seems to be dependent on the N-terminal region, which is characteristically rich in Q/N residues. Because this Q/N rich region is required for prion-like propagation, it is referred to as the 'prion domain'. Induction of the yeast Sup35 prion state leads to loss of Sup35 function and thereby widespread read through of stop codons, allowing the rapid emergence of novel phenotypes, a molecular adaptive strategy for yeast survival under stressful conditions. \nOther examples of prion-like behavior and Q/N rich proteins include CREB (an RNA binding protein) protein in Aplysia  and Pumilio protein in Drosophila.  Q/N rich CREB and Pumilio proteins regulate synaptic activity  and postsynaptic translational suppression,  respectively. Finally, the mammalian proteome contains several Q/N rich prion domains that may similarly use self-aggregation to modulate their activity. A well-studied example is the RNA binding protein TIA1, which is a key component of stress granules, cytoplasmic RNA-protein complexes formed under conditions of cellular stress, which mediates mRNA translational suppression.  The prion-related Q/N domain of TIA1 is essential for self-aggregation and stress granule formation. Thus, one consistent theme for proteins containing prion-related Q/N rich domains from yeast to mammals is stimulus-induced (environmental stress-induced) conformational change leading to self-aggregation, which then alters protein function to orchestrate an adaptive response. Several algorithms based on the amino acid sequence have been used to predict proteins that bind to DNA/RNA and also contain prion-related domains in both yeast and human proteomes. Using one recent algorithm, Markov Model algorithm, trained on known yeast prion-domain containing proteins, FUS and TDP-43, were predicted as the 15 th and 69 th most likely to contain prion-related domains out of nearly 30,000 proteins in the human proteome.  Although the existence of the prion-related domains in TDP-43 and FUS appears convincing, investigations into their role in the normal and pathological functions of these proteins warrants further research.\nNeuronal cytoplasmic inclusions of TDP-43 and FUS are observed in cases of sporadic and familial ALS. For TDP-43, the C-terminal region is highly prone to aggregation, both as purified protein in vitro or when expressed as a fragment in yeast or cultured mammalian cells. ,, This strong tendency of the Q/N rich C-terminus of TDP-43 to self-associate and form aggregate is likewise consistent with the behavior of other prion-related Q/N domain containing proteins. Curiously, TDP-43 pathology (nuclear clearing and cytoplasmic aggregates) is observed not only in ALS, but also in affected brain regions in FTLD-U, Alzheimer disease, Parkinson disease and chronic traumatic encephalopathy, and in inclusion body myopathies.  This is quite consistent with the possibility that TDP-43 aggregation is part of a general response to cellular stress, and is likely mediated by the prion-related domain.\nAlthough wild-type TDP-43 and FUS readily aggregate in vitro, this generally does not occur in normal cells (without pathogenic mutation); the intracellular array of protein folding chaperones probably inhibits this phenomenon. In this context, it is known that the \"yeast prion\" domains can switch their conformation between two states: an intrinsic unfolded state and an aggregated state that can impose its conformation on its unfolded counterpart.  If such a phenomenon can occur with wild-type TDP-43 and FUS, then whether it indeed occurs in sALS and what tips the initiation and accumulation of aggregates remain important unresolved questions.\nRecently, attention has been focused on the concept that misfolded proteins involved in neurodegenerations (such as amyloid-β, tau, and synuclein) could be propagated from cell to cell in a prion-like fashion. , Although experimental evidence for this hypothesis is not robust yet, it is attractive as a potential explanation for the clinically observed spread of neurodegenerative disease through particular neuronal network. Given that prion-related Q/N domains are capable of developing altered conformers, which then can recruit aggregation of the native proteins, the TDP-43 and FUS provide a potential molecular substrate to study the spread of de novo and seeded aggregates in recently created TDP-43 and FUS animal models.\n| » Conclusion|| |\nThe identification of mutations in DNA/RNA binding proteins TDP-43 and FUS as causative of ALS, and the demonstration of TDP-43 as a major constituent of cytoplasmic inclusions in patients with sALS and non-SOD1 fALS, has been a major step toward understanding the pathobiology of the disease. Both these proteins appear to be aggregation-prone, and their ALS-linked mutants appear to exhibit greater degree of aggregation-propensity. TDP-43 and FUS also harbor prion-like domains, raising a tantalizing possibility that focal accumulation of cellular aggregates, and their progressive spread into the motor neuron pool, could be the underlying mechanism of the temporo-spatial spread of the disease. It will be interesting to see if seeded aggregates of these proteins are found to spread into the neuronal network of the nervous system. The emergence of protein aggregates as pathognomonic feature in ALS opens unparalleled opportunities toward therapeutic targets and drug discovery strategies based on the pathways of cellular protein aggregate formation and disease spread in ALS.\n| » References|| |\n|1.||Charcot JM. Sclerose laterale amyotrophique. Oeuvres Completes. Vol 2. Paris: Bureux du Progres Medical; 1897. p. 249-66. |\n|2.||Gowers WR. Manual of diseases of the nervous system. London: Churchill; 1886-8. |\n|3.||Verma A, Tandan R. RNA quality control and protein aggregates in amyotrophic lateral sclerosis: A review. Muscle Nerve 2013;47:330-8. |\n|4.||Ravits J, Laurie P, Fan Y, Moore DH. Implications of ALS focality: Rostral-caudal distribution of lower motor neuron loss postmortem. Neurology 2007;68:1576-82. |\n|5.||Rosen DR, Siddique T, Patterson D, Figlewicz DA, Sapp P, Hentati A, et al. Mutations in Cu/Zn superoxide dismutase gene are associated with familial amyotrophic lateral sclerosis. Nature 1993;362:59-62. |\n|6.||Boillée S, Yamanaka K, Lobsiger CS, Copeland NG, Jenkins NA, Kassiotis G, et al. Onset and progression in inherited ALS determined by motor neurons and microglia. Science 2006;312:1389-92. |\n|7.||Yamanaka K, Chun SJ, Boillee S, Fujimori-Tonou N, Yamashita H, Gutmann DH, et al. Astrocytes as determinants of disease progression in inherited amyotrophic lateral sclerosis. Nat Neurosci 2008;11:251-3. |\n|8.||Neumann M, Sampathu DM, Kwong LK, Truax AC, Micsenyi MC, Chou TT, et al. Ubiquitinated TDP-43 in frontotemporal lobar degeneration and amyotrophic lateral sclerosis. Science 2006;314:130-3. |\n|9.||Sreedharan J, Blair IP, Tripathi VB, Hu X, Vance C, Rogelj B, et al. TDP-43 mutations in familial and sporadic amyotrophic lateral sclerosis. Science 2008;319:1668-72. |\n|10.||Kwiatkowski TJ Jr, Bosco DA, Leclerc AL, Tamrazian E, Vanderburg CR, Russ C, et al. Mutations in the FUS/TLS gene on chromosome 16 cause familial amyotrophic lateral sclerosis. Science 2009;323:1205-8. |\n|11.||Verma A. RNA processing errors in amyotrophic lateral sclerosis. Ann Indian Acad Neurol 2011;14:231-6. |\n|12.||Walker LC, Diamond MI, Duff KE, Hyman BT. Mechanisms of Protein Seeding in Neurodegenerative Diseases. Arch Neurol 2012:70:304-10. |\n|13.||Lagier-Tourenne C, Polymenidou M, Cleveland DW. TDP-43 and FUS/TLS: Emerging roles in RNA processing and neurodegeneration. Hum Mol Genet 2010;19:R46-64. |\n|14.||Chhangani D, Mishra A. Protein Quality Control System in Neurodegeneration: A Healing Company Hard to Beat but Failure is Fatal. Mol Neurobiol 2013 [Epub ahead of print]. |\n|15.||Chen B, Retzlaff M, Roos T, Frydman J. Cellular strategies of protein quality control. Cold Spring Harb Perspect Biol 2011;3:a004374. |\n|16.||Sherman MY, Goldberg AL. Cellular defenses against unfolded proteins: A cell biologist thinks about neurodegenerative diseases. Neuron 2001;29:15-32. |\n|17.||Deng HX, Chen W, Hong ST, Boycott KM, Gorrie GH, Siddique N, et al. Mutations in UBQLN2 cause dominant X-linked juvenile and adult-onset ALS and ALS/dementia. Nature 2011;477:211-5. |\n|18.||Fecto F, Yan J, Vemula SP, Liu E, Yang Y, Chen W, et al. SQSTM1 mutations in familial and sporadic amyotrophic lateral sclerosis. Arch Neurol 2011;68:1440-6. |\n|19.||Bucciantini M, Giannoni E, Chiti F, Baroni F, Formigli L, Zurdo J, et al. Inherent toxicity of aggregates implies a common mechanism for protein misfolding diseases. Nature 2002;416:507-11. |\n|20.||Udan M, Baloh RH. Implications of the prion-related Q/N domains in TDP-43 and FUS. Prion 2011;5:1-5. |\n|21.||Halfmann R, Alberti S, Lindquist S. Prions, protein homeostasis, and phenotypic diversity. Trends Cell Biol 2010;20:125-33. |\n|22.||Si K, Lindquist S, Kandel ER. A neuronal isoform of the aplysia CPEB has prion-like properties. Cell 2003;115:879-91. |\n|23.||Salazar AM, Silverman EJ, Menon KP, Zinn K. Regulation of synaptic Pumilio function by an aggregation-prone domain. J Neurosci 2010;30:515-22. |\n|24.||Harrison PM, Gerstein M. A method to assess compositional bias in biological sequences and its application to prion-like glutamine/asparagine-rich domains in eukaryotic proteomes. Genome Biol 2003;4:R40. |\n|25.||Cushman M, Johnson BS, King OD, Gitler AD, Shorter J. Prion-like disorders: Blurring the divide between transmissibility and infectivity. J Cell Sci 2010;123:1191-201. |\n|26.||Furukawa Y, Kaneko K, Watanabe S, Yamanaka K, Nukina N. A seeding reaction recapitulates intracellular formation of Sarkosyl-insoluble transactivation response element (TAR) DNA-binding protein-43 inclusions. J Biol Chem 2011;286:18664-72. |\n|27.||Lee SJ, Desplats P, Sigurdson C, Tsigelny I, Masliah E. Cell-to-cell transmission of non-prion protein aggregates. Nat Rev Neurol 2010;6:702-6. |\n|This article has been cited by|\n||Prion-like Mechanism in Amyotrophic Lateral Sclerosis: are Protein Aggregates the Key?\n| ||Shynrye Lee,Hyung-Jun Kim |\n| ||Experimental Neurobiology. 2015; 24(1): 1 |\n|[Pubmed] | [DOI]|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:8bd2ca45-11a1-4000-8e6f-1e127379c42e>","<urn:uuid:29c876d3-a025-4c61-bf30-f025525334c8>"],"error":null}
{"question":"I'm researching evolutionary adaptations and sustainable agriculture. What are the specific genetic changes observed in PCB-resistant killifish, and what are the main techniques used in vertical farming systems?","answer":"In PCB-resistant killifish, the AHR2 protein consists of 951 amino acids, with nine varying among individuals, resulting in 26 different protein forms. The New Bedford Harbor killifish show distinct variant patterns compared to nearby populations. For vertical farming, there are three main techniques: hydroponics, which grows plants in water using mineral fertilizer solutions without soil; aeroponics, which uses mist or nutrient solutions without any growing medium; and aquaponics, which combines fish farming with hydroponics to create a symbiotic system where fish waste fertilizes plants and plants filter the water for fish.","context":["Solving an evolutionary puzzle\nNew Bedford Harbor pollution prompts PCB-resistance in Atlantic killifish\nFor four decades, waste from nearby manufacturing plants flowed into the waters of New Bedford Harbor—an 18,000-acre estuary and busy seaport. The harbor, which is contaminated with polychlorinated biphenyls (PCBs) and heavy metals, is one of the EPA’s largest Superfund cleanup sites.\nIt’s also the site of an evolutionary puzzle that researchers at Woods Hole Oceanographic Institution (WHOI) and their colleagues have been working to solve.\nAtlantic killifish—common estuarine fishes about three inches long—are not only tolerating the toxic conditions in the harbor, they seem to be thriving there. How have they been able to adapt and live in such a highly contaminated environment?\nIn a new paper published in BMC Evolutionary Biology, researchers found that changes in a receptor protein, called the aryl hydrocarbon receptor 2 (AHR2), may explain how killifish in New Bedford Harbor evolved genetic resistance to PCBs.\nKillifish are prey fish that do not migrate. They live their whole lives in the same area, generally within a few hundred yards of the spot where they were hatched. Unlike fish that may come in and out of the harbor sporadically during the summer months to feed, the killifish are there year round and spend winters burrowing into the contaminated sediment.\nNormally when fish are exposed to harmful chemicals, the body steps up production of enzymes that break down the pollutants, a process controlled by the AHR2 protein. Some of the PCBs are not broken down in this way, and their continued stimulation of AHR2 disrupts cellular functions, leading to toxicity. In the New Bedford Harbor killifish, the AHR2 system has become resistant to this effect.\n“The killifish have managed to shut down the pathway,” said Mark Hahn, a biologist at WHOI and coauthor of the paper. “It’s an example of how some populations are able to adapt to changes in their environment—a snapshot of evolution at work.”\nThe research team, which includes colleagues from the Atlantic Ecology Division of the U.S. Environmental Protection Agency, the Boston University School of Public Health, and the University of North Carolina at Charlotte, used a “candidate gene” approach, sequencing the protein-coding portion of three candidate resistance genes (AHR1, AHR2, AHRR) in fish from the New Bedford site and six other locations, both clean and polluted, along the northeast coast.\nLooking for single nucleotide polymorphisms (SNPs) or subtle variations in the DNA sequence, they found differences in AHR2, which plays an important role in mediating toxicity in early life stages.\n“The function of this receptor is what mediates the toxic effects,” said Sibel Karchner, a coauthor and biologist in Hahn’s lab. “If you don’t have a functional receptor, then you’re not going to get the toxic effects as much as a fish that does.”\nAHR2 in killifish has 951 amino acids and nine of those vary among individuals. The different combinations of amino acid variants lead to 26 different forms of the protein.\n“We see that the pattern of variants present in the New Bedford Harbor killifish is much different from the patterns at nearby sites, which is unexpected under normal circumstances,” Hahn said. “There are a few protein variants that are common in New Bedford Harbor killifish, but uncommon elsewhere. Similarly, the protein variants that are most common at the nearby reference sites are much less common in the New Bedford Harbor killifish.”\nA companion paper published in BMC Evolutionary Biology by colleagues at the EPA lab in Narragansett, RI, that used a “candidate gene scan” approach—examining SNPs from 42 genes associated with the AHR pathway—also identified AHR2 as a gene that appears to be under selection and is likely to be involved in the resistance. The results suggest that evolution of resistance in independent populations of killifish converges on the same target gene.\n“The results of these studies and the genetic tools developed in the course of these studies are helping to dissect how evolution occurs on a contemporary (rather than geological) scale and why some species are more likely to adapt to a rapidly changing world,” said Diane Nacci, a research biologist at EPA and coauthor on both papers.\nAHR2 is also the same gene identified in a 2011 Science paper by WHOI biologists and colleagues from New York University and NOAA on PCB-resistant tomcod from the Hudson River. AHR2 proteins in the Hudson River tomcod are missing two of the 1,104 amino acids normally found in this protein.\n“Even though the specific molecular changes that are found in PCB-resistant tomcod and killifish are different, in both species AHR2 seems to be one of the genes—possibly the major gene—that is responsible for the resistance,” Hahn said.\nWhile the killifish themselves seem to be immune to the toxic effects of the PCBs, they can still transfer contaminants up the food chain. They’re a major source of food for bluefish, striped bass and other fish eaten by humans.\nDespite their healthy appearance, there could be unknown negative costs for the New Bedford Harbor killifish associated with the resistance to PCBs. Researchers will look next at whether the adaptation affects how the killifish are able to respond to other kinds of stressors in their environment, such as low oxygen levels.\n“Obviously, the fact that they are resistant to PCBs allows them to survive in this really polluted environment, but what will happen once the harbor gets cleaned up? There could be costs that make it no longer adaptive for these fish to live there,” Hahn said.\n“It’s a fascinating example of how human activities can drive evolution,” he added. “The ability to adapt to changing conditions is going to become even more important as humans impact the environment, whether it’s from ocean acidification or increasing temperatures or other types of global changes that are occurring.”","As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a4ea4980-efed-4bb6-be84-626b2e187e01>","<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>"],"error":null}
{"question":"What are the molecular mechanisms of renal glucose handling, and what clinical manifestations arise when these processes are disrupted in Type 1 diabetes?","answer":"In the kidney, glucose reabsorption occurs through sodium-coupled glucose cotransporters (SGLT) in the proximal convoluted tubule. SGLT2 provides high-capacity transport in early segments, while SGLT1 offers high-affinity transport in later segments. Glucose exits cells through GLUT2 transporters. When blood glucose exceeds the transport maximum of about 200mg/dL, glycosuria occurs. In Type 1 diabetes, this disruption manifests as excessive urination, extreme thirst, hunger, and fatigue. Without treatment, cells cannot utilize glucose, leading to fat metabolism and ketone production, potentially causing diabetic ketoacidosis and long-term complications including kidney failure and neuropathy.","context":["Table of Contents\nGlucose in the Urine\nRecall that kidney function can be broken down into four steps:\nAbout 180 liters of plasma is filtered through the kidney per day. This equates to the glomerular filtration rate. During the filtration process large quantities of water, electrolytes, and other small molecules pass from the circulatory vasculature (renal arterioles) and into the lumen of the glomerulus (Bowman’s capsule). As this filtrate passes through the nephron a great deal of water and other material is reabsorbed. This functions to concentrate the nitrogenous waste and maintain homeostasis.\nMaterial is also actively secreted depending on conditions in the body such as pH and potassium concentrations. The final concentrated urine passes through the ureters, into the bladder, and is excreted. D-glucose is filtered and almost completely reabsorbed under normal conditions.\nGlucose is a polar molecule and dissolves in water and blood plasma. It easily passes through the glomerular basement membrane. It is almost completely reabsorbed from the tubules by active transport molecules found in the proximal convoluted tubule (PCT) called sodium-coupled glucose cotransporters (SGLT).\nRecall that a cotransporter moves two molecules: one with its electrochemical gradient and one against its electrochemical gradient. In this case, sodium is moving down its gradient into the cell while glucose is also moving into the cell but against its electrochemical gradient. These transporters are found on the luminal side of the epithelial cells that line the PCT.\nIntracellular sodium concentration is maintained low by an ATP-dependent sodium/potassium pump that actively transports potassium into the cell and sodium out of the cell and into the bloodstream. These pumps are found on the basolateral side of the PCT cells.\nThere are two types of SGLT:\nSGLT2 is found in the first part of PCT (S1 and S2) and are low-affinity, high capacity transport proteins and are responsible for 90 % of reabsorption.\nSGLT1 are high-affinity, low capacity transport proteins, responsible for 10 % of glucose reabsorption and are found in the distal part of the PCT where glucose concentrations are much lower. Recall that affinity is a term that characterizes the interaction between a ligand and its receptor. A high-affinity system requires only a lower concentration to fill all binding sites while a low-affinity system requires a higher concentration. Therefore, SGLT1 (high-affinity transporters) are active even with a low concentration of filtrate glucose.\nOnce inside the cell, the glucose is then passively transported into the bloodstream through the glucose transporter (GLUT)2 found on the basolateral border of the epithelial cells that line the PCT.\n|Transporter||Location on cell||Character of the transporter||Location in nephron|\n|SGLT1||Luminal||High affinity, low concentration cotransporter||S1, S2 of PCT|\n|SGLT2||Luminal||Low affinity, high concentration cotransporter||S3 of PCT|\n|GLUT2||Basolateral||Bidirectional transporter||S1, S2, S3 of PCT|\n|ATP-Na/K Pump||Basolateral||Active transport||S1, S2, S3 of PCT|\nSGLT1 and SLGT2 will absorb filtrate glucose until all of their receptor sites are full. This is called the transport maximum for glucose (TmG). It corresponds to a plasma glucose level of about 200mg/dL. At this point extra glucose will pass into the bladder and will be excreted (glycosuria). Each nephron may reach TmG at different times due to a non-uniform distribution of transporters; therefore, glycosuria may be detected before the tubular maximum is reached. This is called splay.\nSGLT2 inhibitors are a relatively new type of medication to treat diabetes type 2. The gliflozin family of medication prevent glucose reabsorption in the PCT, allowing the excretion of more glucose and lowering plasma glucose levels. Dapagliflozin is the oldest member of this class of drugs. Side effects include increased risk for ketoacidosis, urinary tract infection, hypoglycemia, and candida infection.\nGlucose Homeostasis in the Kidney\nThe kidney can be considered two organs when considering glucose homeostasis. The renal medulla consumes a great deal of glucose. It has the enzymes to store glucose as glycogen and the glycolytic enzymes necessary to use those stores. The renal cortex has the glycos-6-phosphatase, an enzyme that is necessary to release glucose into the bloodstream. The renal cortex works with the liver to regulate glucose hemostasis. The cortex does not have the correct enzymes to store and use glycogen.\nThe answers are below the references.\n1. How is sodium concentration maintained at a low concentration in the tubular epithelial cells that line the proximal convoluted tubule?\n- Basolateral ATP dependent sodium/potassium pump\n- Luminal NKCC transporter\n- Paracellular diffusion\n- Luminal SGLT2\n- Basolateral SGLT1\n2. Tissue that required a great deal of glucose to function (such as the brain) would need which characteristics in a glucose transporter?\n- Low-affinity, high capacity\n- High-affinity, low capacity\n- High-affinity, high capacity\n- Low-affinity, low capacity","Symptoms of Type 1 diabetes\nSymptoms of Type 1 diabetes: Introduction\nType 1 diabetes can strike anyone at any age but most often occurs in children and adolescents. Symptoms of type 1 diabetes generally develop quickly and can include tingling or loss of sensation in hands and feet, excessive thirst, excessive urination, extreme hunger, fatigue, weakness, blurred vision, vomiting, dehydration, and significant weight loss even with the consumption of large amounts of food.\nIf type 1 diabetes is not diagnosed and treated promptly, the body's cells do not receive the glucose they need for energy and the body is forced to burn fat stores for energy. As large amount of fats stores are burned, they substances called ketones are produced. When a large amount of ketones build up in the body, it can lead to a life-threatening condition called diabetic ketoacidosis. High amounts of glucose in the blood can lead to hyperosmolar hyperglycemic nonketotic syndrome, coma, and shock. Long-term complications of type 1 diabetes can also be serious and include kidney failure, diabetic retinopathy and blindness, peripheral neuropathy, kidney failure, serious skin infections, gangrene, cardiovascular disease, stroke, disability, and death....more about Type 1 diabetes »\nType 1 diabetes symptoms:\nThe symptoms of Type 1 diabetes usually arise over\nweeks and months, as compared to those of Type 2 diabetes\nthat may take years.\nAlthough Type 1 diabetics can briefly see mild symptoms similar to\nthe early stages of Type 2 diabetes, the most pronounced symptoms\nof Type 1 diabetes are usually the more severe symptoms\nof very high blood sugars and these progress quickly....more about Type 1 diabetes »\nSymptoms of Type 1 diabetes\nThe list of signs and symptoms mentioned in various sources\nfor Type 1 diabetes includes the 13\nsymptoms listed below:\nResearch symptoms & diagnosis of Type 1 diabetes:\nType 1 diabetes: Symptom Checkers\nReview the available symptom checkers for these symptoms of Type 1 diabetes:\nType 1 diabetes: Symptom Assessment Questionnaires\nReview the available Assessment Questionnaires for the symptoms of Type 1 diabetes:\nType 1 diabetes: Complications\nReview medical complications possibly associated with Type 1 diabetes:\nDiagnostic testing of medical conditions related to Type 1 diabetes:\nResearch More About Type 1 diabetes\nDo I have Type 1 diabetes?\nType 1 diabetes: Medical Mistakes\nType 1 diabetes: Undiagnosed Conditions\nDiseases that may be commonly undiagnosed in related medical areas:\nHome Diagnostic Testing\nHome medical tests related to Type 1 diabetes:\n- High Cholesterol: Home Testing:\n- High Blood Pressure: Home Testing\n- Heart Health: Home Testing:\n- Thyroid: Home Testing:\n- Menopause: Related Home Testing:\n- Vaginal Health: Home Testing:\n- Diet & Weight Loss: Home Testing:\n- Adrenal Gland Health: Home Testing:\n- Breast Cancer: Related Home Tests:\n- Kidney Health: Home Testing:\n- Diabetes: Related Home Testing:\n- more home tests...»\nWrongly Diagnosed with Type 1 diabetes?\nThe list of other diseases or medical conditions\nthat may be on the differential diagnosis list of alternative diagnoses\nfor Type 1 diabetes includes:\nSee the full list of 28\nType 1 diabetes: Research Doctors & Specialists\n- Diabetes & Endocrinology Specialists:\n- Cholesterol Specialists:\n- Cardiac (Heart) Specialists:\n- Pregnancy & Fertility Health Specialists:\n- Womens Health Specialists:\n- Immune-Related Disease Specialists (Immunology):\n- more specialists...»\nResearch all specialists including ratings, affiliations, and sanctions.\nMore about symptoms of Type 1 diabetes:\nMore information about symptoms of Type 1 diabetes and related conditions:\nOther Possible Causes of these Symptoms\nClick on any of the symptoms below to see a full list\nof other causes including diseases, medical conditions, toxins, drug interactions,\nor drug side effect causes of that symptom.\nArticle Excerpts About Symptoms of Type 1 diabetes:\nDiabetes Overview: NIDDK (Excerpt)\nSymptoms of type 1 diabetes usually\ndevelop over a short period, although beta cell destruction can begin\nSymptoms include increased thirst and urination, constant hunger,\nweight loss, blurred vision, and extreme fatigue. If not diagnosed and\ntreated with insulin, a person can lapse into a life-threatening diabetic\ncoma, also known as diabetic ketoacidosis.\n(Source: excerpt from Diabetes Overview: NIDDK)\nEndocrine Diseases: NWHIC (Excerpt)\nIncreased thirst, increased urination, weight loss,\nfatigue, nausea, vomiting, frequent infections.\n(Source: excerpt from Endocrine Diseases: NWHIC)\nType 1 diabetes as a Cause of Symptoms or Medical Conditions\nWhen considering symptoms of Type 1 diabetes, it is also important to consider Type 1 diabetes as a possible cause of other medical conditions.\nThe Disease Database lists the following medical conditions that Type 1 diabetes may cause:\n- (Source - Diseases Database)\nMedical articles and books on symptoms:\nThese general reference articles may be of interest\nin relation to medical signs and symptoms of disease in general:\nFull list of premium articles on symptoms and diagnosis\nAbout signs and symptoms of Type 1 diabetes:\nThe symptom information on this page\nattempts to provide a list of some possible signs and symptoms of Type 1 diabetes.\nThis signs and symptoms information for Type 1 diabetes has been gathered from various sources,\nmay not be fully accurate,\nand may not be the full list of Type 1 diabetes signs or Type 1 diabetes symptoms.\nFurthermore, signs and symptoms of Type 1 diabetes may vary on an individual basis for each patient.\nOnly your doctor can provide adequate diagnosis of any signs or symptoms and whether they\nare indeed Type 1 diabetes symptoms."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:58ee1cf0-d689-4e1f-b802-cae299d16394>","<urn:uuid:5bf868dc-26f1-4e93-b858-a340ab03a557>"],"error":null}
{"question":"What es el main purpose del Exercise Talisman Sabre y how often does it occur?","answer":"Talisman Sabre is a bilateral combined Australian and US training activity that occurs every two years. It is designed to practice the two countries' military services and associated agencies in planning and conducting combined and joint task force operations, and improve combat-readiness and interoperability between Australian and US forces.","context":["image-1 = images/TS4/1.jpg\ndescription-1 = A US Navy MH-60S helicopter lands on HMAS Adelaide flight deck off the coast of Queensland during Exercise Talisman Sabre 2019. (Credit - Defence)\nstatus-1 = 1\nimage-2 = images/TS4/2.jpg\ndescription-2 = HMCS Regina participates in the Exercise Talisman Sabre 2019 submarine familiarisation exercise during Operation Projection / Exercise Talisman Sabre in the Pacific Ocean on 11 July 2019. (Credit - Defence)\nstatus-2 = 1\nimage-3 = images/TS4/3.jpg\ndescription-3 = US Coast Guard, USCGC Stratton conducts manoeuvres with Royal Australian Navy ships, HMAS Canberra and HMAS Parramatta during Talisman Sabre 2019. (Credit - Defence)\nstatus-3 = 1\nimage-4 = images/TS4/4.jpg\ndescription-4 = Aviation Boatswain’s Mate (Equipment) 3rd Class Olivia Fobbs, from Los Angeles, provides signals prior to the launch of an F/A-18F Super Hornet assigned to Strike Fighter Squadron (VFA) 102 aboard the Navy’s forward-deployed aircraft carrier USS Ronald Reagan (CVN 76). Ronald Reagan, the flagship of Carrier Strike Group 5, provides a combat-ready force that protects and defends the collective maritime interests of its allies and partners in the Indo-Pacific region. (US Navy photo by Mass Communication Specialist 2nd Class Tyra M. Campbell)\nstatus-4 = 1\nimage-5 = images/TS4/5.jpg\ndescription-5 = Sailor assigned to Air Department observes flight operations on the flight deck of the Navy’s forward-deployed aircraft carrier USS Ronald Reagan. (US Navy photo by Mass Communication Specialist 2nd Class Tyra M. Campbell)\nstatus-5 = 1\nimage-6 = images/TS4/6.jpg\ndescription-6 = An F/A-18E Super Hornet assigned to Strike Fighter Squadron (VFA) 195 launches off the flight deck of the Navy’s forward-deployed aircraft carrier USS Ronald Reagan. (US Navy photo by Mass Communication Specialist 2nd Class Tyra M. Campbell)\nstatus-6 = 1\nimage-7 = images/TS4/7.jpg\ndescription-7 = Lance Corporal Kevin Swords of the 1st Battalion 1st Marines HWS Company, US Marine Corps, cuts steaks to cook for troops' evening meal at Camp Growl during Exercise Talisman Sabre, Shoalwater Bay, Queensland. (Credit - Defence)\nstatus-7 = 1\nimage-8 = images/TS4/8.jpg\ndescription-8 = The evening meal before the exercise gets into full swing is a time to relax and talk over ideas for the troops living at Camp Growl during Exercise Talisman Sabre, Shoalwater Bay, Queensland. (Credit - Defence)\nstatus-8 = 1\nimage-9 = images/TS4/9.jpg\ndescription-9 = US Army soldiers from the 1-27th Infantry Regiment, 2nd Inf. Brigade Combat Team, 25th Inf. Division from Schofield Barracks, Hawaii arrive in Rockhampton, Queensland, Australia for Talisman Sabre 2019. (Credit - Defence)\nstatus-9 = 1\nimage-10 = images/TS4/10.jpg\ndescription-10 = US Army soldiers from the 1-27th Infantry Regiment, 2nd Inf. Brigade Combat Team, 25th Inf. Division from Schofield Barracks, Hawaii unload their baggage from an airplane after arriving in Rockhampton. (Credit - Defence)\nstatus-10 = 1\nimage-11 = images/TS4/11.jpg\ndescription-11 = Private Charlie Lebois works with the 1st Armoured Regiment during a tactical resupply as part of Exercise Talisman Sabre at the Shoalwater Bay Training Area.(Credit - Defence)\nstatus-11 = 1\nimage-12 = images/TS4/12.jpg\ndescription-12 = Trooper Thomas Laird from the 1st Armoured Regiment stands guard during a tactical resupply during Exercise Talisman Sabre at the Shoalwater Bay Training Area.(Credit - Defence)\nstatus-12 = 1\nExercise Talisman Sabre 19: The road so far – Part 4\nScroll to read and see more\nland & amphibious | 15 July 2019 | Louis Dillon\nWith Exercise Talisman Sabre 19 officially kicking off last week, Defence Connect will provide continuous imagery updates from the biennial exercise.\nTS19 is a bilateral combined Australian and US training activity, and is designed to practice the two countries’ respective military services and associated agencies in planning and conducting combined and joint task force operations, and improve the combat-readiness and interoperability between Australian and US forces.\nOccurring every two years, Talisman Sabre is a major exercise reflecting the closeness of our alliance and strength of the enduring military relationship.\nTS19 is the eighth iteration of the exercise and consists of a field training exercise incorporating force preparation (logistic) activities, amphibious landings, land force manoeuvre, urban operations, air operations, maritime operations and Special Forces activities.\nNearly 35,000 military personnel from the US and Australia are set to take part in what will be the biggest ever Talisman Sabre.\nHistorically, Talisman Sabre exercises have been conducted across northern and eastern Australia, and within Australia’s exclusive economic zone. Additional participants from third-party nations may participate or observe the exercise if invited.\nForces from Canada, New Zealand and the UK have received such an invite, with delegations from India and the Republic of Korea will also observe the exercise, with a total of 18 nations from across the Indo-Pacific invited to an international visitors program.\nPlease scroll through the image gallery above for a look at the exercise so far."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:c3399edd-0afa-4ca2-a0bb-18606edb2f0b>"],"error":null}
{"question":"How do the processes of gas exchange compare between submersed aquatic plants and cyanobacteria?","answer":"Submersed aquatic plants and cyanobacteria have evolved different approaches to gas exchange. Aquatic plants have very thin leaves compared to terrestrial plants and notably lack stomata (the special holes used by terrestrial plants for gas exchange) in their submerged leaves. They often have highly dissected, frilly leaves to create a higher surface area to volume ratio for better CO2 absorption from water. Cyanobacteria, on the other hand, have a simpler structure with a thick, gelatinous cell wall and lack specialized gas exchange structures. They can perform photosynthesis using either water or hydrogen sulfide as electron donors, and some species can even fix nitrogen in aerobic conditions, making them uniquely adaptable to various environments.","context":["Botany: An Introduction to Plant Biology Part 3, How Plants WorkAuthor: Rhonda Wilson\nMy previous two columns looked at the history of botany, the structure of plants, and some of the terminology used within this subject. This month’s installment will examine how the plants actually work.\nBacteria to Plants\nThe first organisms to photosynthesize were bacteria. Very recent studies reveal that the first bacteria to do so were the non-oxygen producing purple and green bacteria, followed by heliobacteria. Later-developing bacteria produced oxygen in photosynthesis, as does the commonly known cyanobacteria which appeared even later, followed by algae and plants.\nThe Discovery of Photosynthesis\nThe discovery of photosynthesis began by with the question of where plants get their mass. Early experiments were carried out in the 1600s to try to find out. The first was by a Flemish scientist, Jan Baptista van Helmont. He grew a willow tree in a pot and noted that, though the plant had grown in mass, the soil hadn’t changed much at all. So he hypothesized that the water had contributed to the increased mass of the plant. Later\nThe next person to contribute to our knowledge of plant photosynthesis and gasses in general was an amazing man, Jason Priestly. Priestly, an Englishman, studied to become a minister. He met Benjamin Franklin in 1766 and a year later began an amazing series of discoveries. His accomplishments ranged from grammar to electricity to discovering gasses, and he was instrumental in our understanding of the composition of water. In 1772 he presented a paper called “On different kinds of air.”\nAt one point in his life he lived next to a brewery and noticed the gas produced by the fermenting process floated near the ground, which indicated it was heavier than air. This was carbon dioxide, and he found a way to make it at home, mixed it with water, and discovered carbonated water. He also conducted experiments with sealed containers, plants, candles, and later mice, which demonstrated that plants somehow changed the air. He believed plants restored the air. His theories as to how and what happened weren’t entirely correct, but led to a clearer understanding later.\nPriestly had very liberal ideas for his day, and he eventually found the need to move to the\nJan Ingenhousz was the next person to make a large contribution to our knowledge of plants. He also used candles and mice to discover that plants needed light to “purify” the air. He also tested aquatic plants in light and darkness and noted the gas bubbles that form when the plant was in bright light.\nFinally, Julius Robert Mayer, a physicist from\nDetails of Photosynthesis\n6H2O + 6CO2 à C6H06 + 6O2\nThis means that six molecules of water and six of carbon dioxide, in plants, with energy from sunlight, will produce one molecule of sugar and six of oxygen. But this is a simple statement for a very complex process.\nIn a typical terrestrial plant, leaves take in oxygen and carbon dioxide, while the roots supply water through the plants vascular system. The leaves must protect themselves from drying out and still be able to exchange gases, so they have special holes called stomata for gases to enter or be released. They still do lose a lot of water, but it usually allows them to retain enough to not dry out.\nPhotosynthesis takes place in chloroplasts, which are organelles, or parts of the plant’s cells. The chloroplasts contain pigments in little stacks of structures called thylakoids. There are many thylakoids in the chloroplast, arranged in pancake-like stacks called granum. These are all surrounded by a liquid called stroma, all of which play a part in photosynthesis.\nPigments give living things their color and are what makes plants green. The pigments absorb some colors of light and reflect others. The colors reflected are what we see. In plants there are several basic types of pigments: chlorophylls, carotenoids, phycobilins, and xanthophylls. The most common chlorophylls are simply labeled chlorophyll a, b, and c. Chlorophyll a is the most important and the one that gives plants their green color by reflecting green while absorbing mostly violet, blue, red, and orange colors. The other pigments in plants are called accessory pigments. They absorb energy from other colors and can help a plant utilize the most energy in its specific surroundings, such as under water, where not all colors of light may penetrate.\nPhotosynthesis takes two different steps. These steps are called light reactions and dark reactions. In the light reaction phase, energy from the sun causes the chlorophyll to lose an electron. It’s going to want to get that electron back though, so it takes it from the water. This is when the oxygen is released as a waste product. The electron that was originally lost from the chlorophyll goes through a series of proteins called the electron transport chain. This process creates chemical energy called ATP and NADPH, which are used in the second step. In the second step, the hydrogen left over from when the oxygen gets released from water is added to carbon dioxide with the energy created earlier to make sugar in a process called the Calvin Cycle. It’s these sugars that everything else in the world needs to feed on, either directly or indirectly through eating the plants.\nPhotosynthesis, Light, Aquatic Plants, and CO2\nIn the air there’s usually a lot more available light and CO2 for plants to utilize, and the levels are more stable. Aquatic plants have developed some different adaptations to deal with this situation.\nSubmersed leaves are different from leaves of plants grown emersed, even in the same plant. The leaves of aquatic plants are usually very thin compared to terrestrials. Many aquatic plants have highly dissected leaves—frilly leaved plants like hornwort is a classic example. They do this to create a higher ratio of surface area to the volume of the plant. This allows them to get more CO2 from the water.\nOther strategies include floating leaves (like water lilies) and sending up emersed leaves, which quite a few of the stemmed plants will do in your tanks if you let them. These leaves can utilize the full sun and CO2 in the air, just like the fully terrestrial plants do.\nThe internal structures of submerged plant leaves are different from terrestrial plants also. The stomata that regulate the passage of gases in terrestrial plant leaves are absent in submerged plants. And while the stomata are on the lower part of terrestrial plant leaves, they are on the upper part of floating plants.\nIn addition to water and carbon dioxide, plants also need other nutrients to live. Nitrogen, phosphorus, and potassium are the other major plant nutrients needed. They also need calcium, magnesium, sulfur, and even smaller amounts of iron, boron, copper, chloride, manganese, molybdenum, and zinc. Some of these are usually found in your aquarium water, but others can be used quickly by your plants.\nThe easiest way to ensure your plants have the appropriate nutrients is to do regular water changes and use fertilizers professionally prepared for the aquarium. For those folks who are a little more adventurous, or want more specific control, there are a number of DIY recipes available. A web page search is a great way to find some of those recipes.\nAn Informative Speech\nI had just finished writing this month’s column on Friday night before going to\nThe differences in other types of nutrients available in natural waters and the aquarium were quite extreme, and Dr. Pedersen showed us some charts that showed that the nutrients most available in the typical tap water and in our aquariums is just about opposite of what occurs in the tropical waters many of our plants come in.\nFor those that have read my column for a while you may remember that I haven’t been using CO2 and very seldom other fertilizers in my tanks. After listening to Dr. Pedersen’s talk I’m finally convinced enough to try a slightly higher-tech tank myself, with CO2.\nKnowing what levels of nutrients occur in natural waters can help us all to create the best environment for the plants and animals that once lived there when we bring them into our homes.","Sometimes the prokaryotic cyanobacteria, given their aquatic and photosynthetic characteristic, have been included among the algae, and have been referred to as cyanophytes or blue-green algae. Recent treatises on algae often exclude them, and consider as algae only eukaryotic organisms.\nCyanobacteria are some of the oldest organisms to appear in the fossil record, dating back about 3.8 billion years (Precambrian). Ancient cyanobacteria likely produced much of the oxygen in the Earth's atmosphere, as they became the dominant metabolism for producing fixed carbon in the form of sugars from carbon dioxide.\nIt is generally considered that the origin of the chloroplasts of plants is cyanobacteria, originating from a symbiotic, mutually-beneficial relationship between cyanobacteria and prokaryotes—a relationship so beneficial to both that the chloroplast became an organelle of the cell.\nCyanobacteria are now one of the largest and most important groups of bacteria on earth. They are found in almost every conceivable habitat, from oceans to fresh water to bare rock to soil.\nCyanobacteria have a prokaryotic cell structure typical of bacteria and conduct photosynthesis directly within the cytoplasm, rather than in specialized organelles. Some filamentous blue-green algae have specialized cells, termed heterocysts, in which nitrogen fixation occurs.\nThey may be single-celled or colonial. Colonies may form filaments, sheets, or even hollow balls. Some filamentous colonies show the ability to differentiate into three different cell types: vegetative cells are the normal, photosynthetic cells that are formed under favorable growing conditions; akinetes are the climate-resistant spores that may form when environmental conditions become harsh; and thick-walled heterocysts are those that contain the enzyme nitrogenase, vital for nitrogen fixation, and that may also form under the appropriate environmental conditions wherever nitrogen is present. Heterocyst-forming species are specialized for nitrogen fixation and are able to fix nitrogen gas, which cannot be absorbed by plants, into ammonia (NH3), nitrites (NO2−), or nitrates (NO3−), which can be absorbed by plants and converted to protein and nucleic acids. The rice paddies of Asia, which feed about 75 percent of the world's human population, could not do so were it not for healthy populations of nitrogen-fixing cyanobacteria in the rice paddy waters.\nEach individual cell typically has a thick, gelatinous cell wall, which stains gram-negative. The cyanophytes lack flagella, but may move about by gliding along surfaces. Most are found in fresh water, while others are marine, occur in damp soil, or even temporarily moistened rocks in deserts. A few are endosymbionts in lichens, plants, various protists, or sponges and provide energy for the host. Some live in the fur of sloths, providing a form of camouflage.\nCyanobacteria have an elaborate and highly organized system of internal membranes that function in photosynthesis. Photosynthesis in cyanobacteria generally uses water as an electron donor and produces oxygen as a by-product, though some may also use hydrogen sulfide, as occurs among other photosynthetic bacteria. Carbon dioxide is reduced to form carbohydrates via the Calvin cycle. In most forms, the photosynthetic machinery is embedded into folds of the cell membrane, called thylakoids.\nThe large amounts of oxygen in the atmosphere are considered to have been first created by the activities of ancient cyanobacteria. Due to their ability to fix nitrogen in aerobic conditions, they are often found as symbionts with a number of other groups of organisms, such as fungi (lichens), corals, pteridophytes (Azolla), and angiosperms (Gunnera).\nCyanobacteria are the only group of organisms that are able to reduce nitrogen and carbon in aerobic conditions, a fact that may be responsible for their evolutionary and ecological success. The water-oxidizing photosynthesis is accomplished by coupling the activity of photosystem (PS) II and I. They are also able to use in anaerobic conditions only PS I—cyclic photophosphorylation—with electron donors other than water (hydrogen sulfide, thiosulphate, or even molecular hydrogen) just like purple photosynthetic bacteria. Furthermore, they share an archaebacterial property—the ability to reduce elemental sulfur by anaerobic respiration in the dark.\nPerhaps the most intriguing thing about these organisms is that their photosynthetic electron transport shares the same compartment as the components of respiratory electron transport. Actually, their plasma membrane contains only components of the respiratory chain, while the thylakoid membrane hosts both respiratory and photosynthetic electron transport.\nAttached to the thylakoid membrane, phycobilisomes act as light harvesting antennae for photosystem II. The phycobilisome components (phycobiliproteins) are responsible for the blue-green pigmentation of most cyanobacteria. The variations to this theme is mainly due to carotenoids and phycoerythrins, which give the cells the red-brownish coloration. In some cyanobacteria, the color of light influences the composition of phycobilisomes. In green light, the cells accumulate more phycoerythrin, whereas in red light they produce more phycocyanin. Thus, the bacteria appears green in red light and red in green light. This process is known as complementary chromatic adaptation and is a way for the cells to maximize the use of available light for photosynthesis.\nChlorophyll a and several accessory pigments (phycoerythrin and phycocyanin) are embedded in photosynthetic lamellae, the analogs of the eukaryotic thylakoid membranes. The photosynthetic pigments impart a rainbow of possible colors: yellow, red, violet, green, deep blue, and blue-green cyanobacteria are known. A few genera, however, lack phycobilins and have chlorophyll b as well as chlorophyll a, giving them a bright green colour. These were originally grouped together as the prochlorophytes or chloroxybacteria, but appear to have developed in several different lines of cyanobacteria.\nRelationship to chloroplasts\nChloroplasts found in eukaryotes (algae and higher plants) are generally thought to have evolved from an endosymbiotic relation with cyanobacteria. This endosymbiotic theory is supported by various structural and genetic similarities.\nLike mitochondria, which are also assumed to result from an endosymbiotic relationship, chloroplasts are surrounded by a double celled composite membrane with an intermembrane space, has its own DNA, is involved in energy metabolism and has reticulations, or many infoldings, filling their inner spaces. The inner membrane of the chloroplasts in green plants is thought to correspond to the outer membrane of the ancestral cyanobacterium. The chloroplast genome is considerably reduced compared to that of free-living cyanobacteria, but the parts that are still present show clear similarities. Many of the assumed missing genes are encoded in the nuclear genome of the host.\nPrimary chloroplasts are found among the green plants, where they contain chlorophyll b, and among the red algae and glaucophytes, where they contain phycobilins. It is speculated that these chloroplasts probably had a single origin, in an ancestor of the clade called Primoplantae. Other algae likely took their chloroplasts from these forms by secondary endosymbiosis or ingestion.\nIt was once thought that the mitochondria in eukaryotes also developed from an endosymbiotic relationship with cyanobacteria; however, it is now considered that this phenomena occurred when aerobic Eubacteria were engulfed by anaerobic host cells. Mitochondria are believed to have originated not from cyanobacteria but from an ancestor of Rickettsia.\nThe cyanobacteria were traditionally classified by morphology into five sections, referred to by the numerals I-V. The first three—Chroococcales, Pleurocapsales, and Oscillatoriales—are not supported by phylogenetic studies. However, the latter two—Nostocales and Stigonematales—are considered to be monophyletic and make up the heterocystous cyanobacteria.\nMost taxa included in the phylum or division Cyanobacteria have not been validly published under the International Code of Nomenclature of Bacteria. Exceptions inclue:\n- The classes Chroobacteria, Hormogoneae, and Gloeobacteria.\n- The orders Chroococcales, Gloeobacterales, Nostocales, Oscillatoriales, Pleurocapsales, and Stigonematales.\n- The families Prochloraceae and Prochlorotrichaceae.\n- The genera Halospirulina, Planktothricoides, Prochlorococcus, Prochloron, Prochlorothrix.\nApplications and biotechnology\nThe unicellular cyanobacterium Synechocystis sp. PCC 6803 was the first photosynthetic organism whose genome was completely sequenced (in 1996, by the Kazusa Research Institute, Japan). It continues to be an important model organism.\nAt least one secondary metabolite, cyanovirin, has shown to possess anti-HIV activity.\nSome cyanobacteria are sold as food, notably Aphanizomenon flos-aquae (E3live) and Arthrospira platensis (Spirulina). It has been suggested that they could be a much more substantial part of human food supplies, as a kind of superfood.\nAlong with algae, some hydrogen producing cyanobacteria are being considered as an alternative energy source.\nSome species of cyanobacteria produce neurotoxins, hepatotoxins, cytotoxins, and endotoxins, making them dangerous to animals and humans. Several cases of human poisoning have been documented but a lack of knowledge prevents an accurate assessment of the risks.\nCertain cyanobacteria produce cyanotoxins, like Anatoxin-a, Anatoxin-as, Aplysiatoxin, Cylindrospermopsin, Domoic acid, Microcystin LR, Nodularin R (from Nodularia), or Saxitoxin. Sometimes a mass-reproduction of cyanobacteria results in ecologically harmful harmful algal blooms.\n- ↑ Cyanobacteria, their toxins and health risks Retrieved December 9, 2007.\n- ↑ Blue Green Algae (Cyanobacteria) and Their Toxins Retrieved December 9, 2007.\n- Giovannoni, S. J., S. Turner, G. J. Olsen, S. Barns, D. J. Lane, and N. R. Pace. 1988. “Evolutionary relationship among cyanobacteria and green chloroplasts.” Journal of Bacteriology, 170:3584-3592.\n- Urbach, E., D. L. Robertson, and S. W. Chisholm. 1992. “Multiple evolutionary origins of prochlorophytes within the cyanobacterial radiation.” Nature, 335:267-270.\n- Wolfe, G. R., F. X. Cunningham, D. Durnford, B. R. Green, and E. Gantt. 1994. “Evidence for a common origin of chloroplasts with light-harvesting complexes of different pigmentation.” Nature, 367: 566-568.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:1820d0c4-9245-4a3f-bf1a-19fbe8d2ca22>","<urn:uuid:a780c530-11b9-444b-9d3f-18af871fedea>"],"error":null}
{"question":"What role does actin play in amoeba phagocytosis, and how can different fluorescent labeling methods visualize it?","answer":"Actin is a key protein present in actively phagocytising amoeba cells. In the phagocytosis process, actin accumulates and dissipates as the cell engulfs its prey. To visualize actin, multiple fluorescent labeling methods can be used. One method involves using fluorescent proteins, such as joining red fluorescent protein to a gene for a protein that binds to actin, making areas of actin expression appear brighter red. Additionally, specific fluorescent dyes like Alexa Fluor 488 can be modified to target actin filaments directly, and the unbound excess can be washed away to achieve high signal-to-background ratio and improved contrast.","context":["Usually, when we think about biotechnology, it’s in the context of agriculture, and occasionally in the context of medicine, but biotechnology is useful for a lot more. It can be used to study complex cellular and developmental processes with results that can be stunningly beautiful, and sometimes silly.\nMargaret Clarke researches the soil amoeba Dictyostelium discoideum using biotechnology. Dr. Clarke is officially retired, but as a dedicated scientist, she’s continuing her work. She visited Iowa State yesterday and today.\nIn particular, Dr. Clarke studies phagocytosis – literally “cell eating”. These amoeba are single celled organisms that eat bacteria (and just about any bacteria-sized particle that might be nutritious). Phagocytosis is the process of forming a cup that engulfs the prey, drawing the prey into the phagocyte, and digesting the prey.\nHer work has important applications in human medicine, as the phagocytosis process takes place in special phagocytic cells that are part of the immune system of humans and other animals. Learning how phagocytosis works in amoeba can help us to understand how it works in the immune system.\nBefore biotechnology came around, Dr. Clarke used biochemical analysis to determine what sorts of compounds were at work in actively phagocytising cells. She was able to find out that both actin and myosin were present. While this was important information (people used to think myosin was only present in muscle tissue), she wasn’t able to get any information about exactly where, when, and how these proteins were produced in the phagocytosis process – especially since it happens so quickly!\nFinally, about 15 years ago, people started using biotechnology to label proteins in living cells with fluorescent proteins. Dr. Clarke saw that she could use these fluorescent labels to actually see where, when, and how actin and other compounds accumulated and dissapated in amoeba that were phagocytising. Dr. Clarke labeled different proteins with either red or green fluorescent proteins, learning much about the phagocytosis process. Even better – she caught them on film!\nThe images above are from Dr. Clarke’s 2006 paper Phagocyte meets prey: Uptake, internalization, and killing of bacteria by Dictyostelium amoebae. This transgenic amoeba are expressing RFP-LimEΔ, which is the gene for red fluorescent protein joined to a gene for a protein that binds to actin. Where ever actin is expressed, the amoeba will be a brighter red. It is eating GFP labeled E. coli.\nAs you can see, Dictyostelium are greedy little guys! They’ll try to eat just about anything, including things that are too large for them, like this amoeba trying to eat a yeast cell to the right (for the video of this frustrated little guy, see Honorable Mention #5 at the Olympus Bioscapes 2009 competition).\nIn addition to its awesome phagocytosis techniques, Dictyostelium has an amazing life cycle. When food becomes scarce, the amoeba will put out cAMP,a chemical that causes them to aggregate. About 100,000 individual amoeba group together and start to form what’s called a “migrating slug.” The “slug” will move through the soil toward the surface where it will develop into a fruiting body and eventually put out spores that will become single celled amoeba again.\nClarke M, & Maddera L (2006). Phagocyte meets prey: uptake, internalization, and killing of bacteria by Dictyostelium amoebae. European journal of cell biology, 85 (9-10), 1001-10 PMID: 16782228","Different Ways to Add Fluorescent Labels\nSee cell structures and proteins more clearly\nViewing cells with white light only allows you to see so much. By selectively labeling proteins, structures, and biological processes with fluorescent proteins, dyes or conjugated antibodies, what you can observe and track increases dramatically.\nLearn about what characteristics of fluorescent dyes are important and see how they can be used in functional and structural cell analysis studies.\nUsing fluorescence to detect your target\nUsing fluorescence provides greater contrast compared to viewing your samples with brightfield microscopy alone. Labeling various targets with separate fluorescent colors allows you to visualize different structures or proteins within a cell in the same experiment. Ways to fluorescently label your target include fluorescent dyes, immunolabeling, and fluorescent fusion proteins—all of which can provide a means to selectively mark structures and proteins within the cell, allowing you to see them more easily when you image.\nSome dyes can be used in live cells, while others have uses in fixed and permeabilized cells. As with any technique, using fluorescent dyes to label your target has limitations. When fluorescent dyes are used in live cells, they can be phototoxic. Sometimes a fluorescent dye that labels exactly what you want to see is not available. If a specific dye is not available, you can try using immunolabeling in fixed cells or fluorescent fusion proteins in fixed or live cells to visualize your target.\nOne fluorescent dye molecule, many different selectivities\nMany fluorescent tools for cell biology are essentially fluorophores that have been modified in different ways or conjugated to various molecules to give them a certain function or allow them to bind to specific organelles or proteins.\nThrough chemical modifications, a single fluorophore can be produced in a number of variant forms, each with a different specificity. For example, the green-fluorescent Alexa Fluor® 488 dye molecule can be modified to target actin filaments (A), can be attached to an IgG for use in immunolabeling (B), or can act as a whole cell stain (C).\nFigure 1. A single fluorophore can be modified to carry out any number of labeling jobs, including functionalized forms for labeling cell structure components such as actin (A) and tubulin (B) and salt forms for whole-cell staining (C).\nFluorescent labeling methods can vary\nAs mentioned above, fluorescent labels usually comprise a fluorophore that is modified in a way that gives them specificity. In addition to linking fluorophores to various molecules, other types of modifications can give fluorophores novel characteristics.\nLinking a fluorophore to a specific molecule, for example, an antibody, can give it selectivity for its target, in this case, an antigen. The advantage to this approach is that the fluorophore is bound to a target, and you wash away any unbound or excess fluorescent dye, resulting in high signal to background and improved contrast.\nFluorogenic dyes start off with dim emissions, but conditions or activity inside the cell trigger an increase in their brightness. For example:\n- Increased fluorescence after binding to target—many nuclear stains like SYTOX® Green and Hoechst show little fluorescence on their own. When these stains bind to nucleic acids, however, the fluorescence intensity is greatly increased, resulting in a very bright signal.\n- Increased fluorescence after intracellular modification—other stains contain a chemically reactive group that becomes modified by cellular activity. For example, calcein AM contains an ester group. Inside live cells, in the presence of active esterases, the ester group is cleaved, which converts nonfluorescent calcein AM to green-fluorescent calcein.\nFluorogenic dyes typically have high signal to background, since the unbound/uncleaved dye has a dim signal compared to the dramatic increase in fluorescence observed when they are bound or activated. Because the unmodified fluorogenic dye is not very bright, you usually don’t have to wash it out.\nUsing fluorescent dyes as functional indicators\nSometimes you don’t just want to label something; you want to know if your cells are healthy, or if they are functioning the way they should be. You may want to know: are my cells alive? are they apoptotic? or are they stressed out? There are many fluorescent dyes available that can act as indicators for various cellular functions and answer these questions.\nFor example, the fluorescent dye tetramethylrhodamine, methyl ester (TMRM) specifically labels mitochondria, and it can also indicate if the mitochondria are healthy. Active mitochondria in healthy cells will maintain a mitochondrial membrane potential, and TMRM is brightly fluorescent in these mitochondria. As mitochondrial membrane potential is lost (in sick or dying cells), TMRM signal is diminished.\nFigure 2. Panel A shows TMRM staining in healthy HeLa cells; panel B shows the loss of TMRM signal concurrent with treatment to destroy the mitochondrial membrane potential.\nImportant characteristics of fluorescent dyes\nWhen people talk about fluorophores, or fluorescent dyes, you may hear them use words like extinction coefficient and quantum yield. These terms are physical properties of the fluorescent dye molecules themselves, and they give chemists an idea of how bright the dyes are. However, there’s more to fluorescent dyes than the extinction coefficient and quantum yield when it comes to biological investigations. For scientists who need to actually use these fluorescent labels in a biological system, there are many other characteristics that should be considered when designing an imaging experiment.\n- Selectivity—you want your fluorescent label to be confined to the molecule or activity you’re interested in. For example, if you want to label actin, you want a fluorescent label that will target only actin.\n- Signal to background—you want something that will give you bright fluorescence with low nonspecific background signal. A high signal with a low background will give you the greatest contrast between what you are interested in seeing and everything else. See Background Fluorescence.\n- Photostability—photostability is an indicator of how well the fluorescent signal is maintained with repeated exposures to illumination light. It is difficult to work with fluorescent dyes that are not photostable, as you can lose your signal in the time that it takes to focus the microscope. Photostability is also important if you want to perform time-lapse imaging.\n- Excitation/emission properties—you want to choose a fluorescent dye that has excitation and emission properties that are compatible with the filter set you are going to use. See Using Filters to Capture Your Signal.\n- Environmental stability—although sometimes inevitable, it can be difficult to work with dyes that are environmentally unstable. Some fluorescent dyes are sensitive to air, light, or temperature, and you’ll want to consider those parameters before you get started."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:45338f65-490e-4bba-80af-dbda8797ded6>","<urn:uuid:0e89dc5f-00c5-46d2-b72d-6ed6f34b2710>"],"error":null}
{"question":"What are the main differences between biological concrete and cellular concrete in terms of their environmental benefits?","answer":"Biological concrete and cellular concrete have different environmental advantages. Biological concrete supports the growth of pigmented organisms, improves thermal comfort in buildings, and helps reduce atmospheric CO2 levels. On the other hand, cellular concrete provides thermal insulation benefits that increase as the material becomes lighter - with R-values ranging from 3-25 per inch depending on density. While both materials offer thermal benefits, biological concrete actively reduces CO2 through organism growth while cellular concrete's environmental impact comes from its insulative properties.","context":["Edison’s idea: a house that could be built with one pour of cement. The process could eliminate not only the traditional work of erecting walls and roof but also much of the labor involved in finishing the interiors. Given the right mold, “stairs, mantels, ornamental ceilings, and other interior decorations and fixtures” would all be formed by the same giant piece of concrete. — slate.com\nAfter 2,000 years, a long-lost secret behind the creation of one of the world’s most durable man-made creations ever—Roman concrete—has finally been discovered by an international team of scientists, and it may have a significant impact on how we build cities of the future. — businessweek.com\nIn case you haven't checked out Archinect's Pinterest boards in a while, we have compiled ten recently pinned images from outstanding projects on various Archinect Firm and People profiles. Today's top images (in no particular order) are from the board Concrete. ↑ Pavilion Siegen in Siegen...\nThe Ping’an Finance Center is planned to top out at 660m, making it not only China’s tallest building but the second-tallest building in the world after the Burj Dubai. 80m has been built so far, but construction has been halted in the wake of the revelation from Shenzhen’s Housing and Construction Bureau that substandard sea sand concrete had been used in its construction. — wired.com\nIn an industry constantly pursuing innovative design that is both environmentally and ethically sound, the implementation of raw materials is directing interior design in 2013.\nNatural materials are being sourced and taking on new forms as designers reinvent familiar items with sustainable credentials. — DesignBuild Source\nThe Structural Technology Group has developed and patented a type of biological concrete that supports the natural, accelerated growth of pigmented organisms. The material, which has been designed for the façades of buildings or other constructions in Mediterranean climates, offers environmental, thermal and aesthetic advantages over other similar construction solutions. The material improves thermal comfort in buildings and helps to reduce atmospheric CO2 levels. — sciencedaily.com\nLocating and patching cracks in old concrete is a time-consuming business, but rebuilding concrete structures is expensive. Jonkers thinks the solution is to fight nature with nature: he suggests combating water degradation by packing the concrete with bacteria that use water and calcium lactate \"food\" to make calcite, a natural cement. — newscientist.com\nThe latest Archinect ShowCase featured Cassia Co-op Training Centre by TYIN tegnestue Architects. The project is located in Sungai Penuh, Sumatra, Indonesia. NewsThe New York Observer reported on Cornell’s plans (unveiled this week) for a brand new 12.5-acre tech campus on Roosevelt Island...\nIn case you haven't checked out Archinect's Pinterest boards in a while, we have compiled ten recently pinned images from outstanding projects on various Archinect Firm and People profiles. Today's top images (in no particular order) are from the board Concrete. ↑ Casa La Punta in Mexico...\nUsing natural soil and sand, the Stone Spray can construct intricate solid structures at almost any location, even on vertical surfaces. The device was developed by architects Petr Novikov, Inder Shergill, and Anna Kulik as a research project to experiment with applying the concepts of digital manufacturing to construction work. — gizmag.com\nIn case you haven't checked out Archinect's Pinterest boards in a while, we have compiled ten recently pinned images from outstanding projects on various Archinect Firm and People profiles. Today's top images (in no particular order) are from the board Concrete. ↑ Residence on the...\nUnlike conventional concrete, Iranian concrete is mixed with quartz powder and special fibers - transforming it into high performance concrete that can withstand higher pressure with increased rigidity.\nDue to its combination, the new Iranian-made concrete is an excellent building material with peaceful applications like the construction of safer bridges, dams, tunnels, increasing the strength of sewage pipes, and even absorbing pollution. — presstv.ir\nWhatever else you might think about it, Boston City Hall is an improbable building. Call it a giant concrete harmonica or a bold architectural achievement, but to walk by this strange, asymmetrical structure in Government Center is to wonder how on earth it landed there. — bostonglobe.com\nThe past 12 months have seen a remarkable number of humanitarian crises with earthquakes in Japan and New Zealand and deadly tornadoes in the southern US being among the most recent.\nAmong new innovations which could help relief efforts is a fabric shelter that, when sprayed with water, turns to concrete within 24 hours. — BBC News\nSUBMIT NEWS: submit in 60 seconds!","Cellular Concrete Mix Design\nWorking with Cellular Concrete\nWhen working with cellular concrete and considering mix designs, a cardinal rule is that as density decreases, so does strength. In some instances, such as the material needing to be excavated at a later time, the loss of strength is a benefit. An additional benefit is that as the material becomes lighter, its thermal and acoustic insulative properties increase as well. A very basic cellular concrete mix design would consist simply of Portland cement, water, and externally generated foam, which is also referred to sometimes as preformed foam.\nThe water-cement ratio can typically vary from .40-.80, and the foam content is commonly as high as 80%, depending on desired density. Typically Type 1 Portland is used, however other Portland types may be used as well. When using other Portland types the benefits for which they’re used in other materials also apply to cellular concrete.\nBeyond Portland cement, there are many other cementitiuous materials that may be used in cellular concrete. Fly ash is very common, but metakaolin, slag, and silica fume are a few others that have also been used in producing cellular concrete. Depending on the application, these alternative materials may be used to help increase the strength of the material, or to further improve the economics of cellular concrete, among other reasons. In addition to cementitious materials, other materials can be used as well, such as fiber.\nTypically when densities are below 50 pounds per cubic foot (PCF) (800.92 kg/m³) there are no fine or coarse aggregates used, as they tend to further decrease the strength. When above 50 PCF (800.92 kg/m³) sand may be introduced, primarily as a measure of economics. Portland is the most expensive component of cellular concrete—and when higher densities are required such as to displace water, but higher strength is not needed—it creates a good opportunity and reason to use a cheap filler such as sand.\nCoarse aggregates won’t typically be introduced until densities are above 100 PCF (1601.85 kg/m³). In applications where cellular concrete is being used in this density range it’s more likely to be a structural or precast application. As with any concrete product, cellular concrete mix designs are particularly important because the mix design is critical to the performance of the material, relative to the application. Once a mix design is decided upon, it’s also critical the density when produced is monitored closely.\nIf the material being produced is too heavy, production yield and money is being lost. If the material is too light, it may not have the required strength for the application.\nThe water-cement ratio of cellular concrete can vary widely. Although most people don’t pay much attention to it, it should be noted that the water-cement ratio of cellular slurry does increase over the base slurry W/C ratio, due to the water in the foam that’s being added. As with any cementitious product, the strength of cellular concrete will increase at any given density when a lower W/C ratio is used. A general range would be .40-.80, with many mix designs falling more commonly between .50 and .65.\nTypically, water-cement ratios should not be lower than .35. When W/C ratios fall below .35 the slurry can pull water from the foam when it’s added, causing the foam bubbles to collapse. However, high shear mixers, such as colloidal mixers, and/or the use of water reducers and superplasticizers can be used effectively to help avoid this problem and allow the use of lower water-cement ratios with good success.\nWhen using water reducers or any type of ad-mixture with cellular concrete, testing must be done to ensure there are no adverse reactions between the foam and ad-mix. A typical result of a reaction would be the ad-mix causing the foam bubbles to collapse.\nExpected Strengths and Insulative Values of Cellular Concrete\nFt³/yd³ (m³/.76 m³) of Slurry\nR value per inch\n(Metrix R Value)\n|30 to 900 (2.07 to 62.05)||12 to 25 (.34 to .71)||.75 to 1.85 (.14 to .33)||Neat Cement|\n|400 to 1500 (27.58 to 103.42)||6 to 10 (.17 to .28)||.25 to .30 (.045 to .054)||Sand Mix|\n|1500 to 4000 (103.42 to 275.79)||3 to 6 (.08 to .17)||.1 to .2 (.018 to .036)||Sand Mix|\nNote: The cellular concrete data above is compiled from industry publications.\nThese are generalized values that should be verified by testing with the use of local materials and equipment for any given project. Local materials, equipment, and slurry preparation—along with processing and quality control—can produce wide variances in the results for any given mix design. The compressive strength for any given density is one of the common topics that people are interested in. Shown above is a table with expected strengths and insulative values for a variety of cellular concrete densities.\nThe strength will vary based upon myriad factors, including final mix design, foam concentrate, foam generator, and base slurry preparation. As with other cementitious materials, cellular concrete typically has compression testing done at 28 days.\nMix Designs Economics\nProportion of Cellular Concrete Mix\nOne of the biggest challenges with cellular concrete mix designs is calculating the proportions for both the base slurry, and the required amount of foam to achieve any given density. An experienced practitioner can do much of the calculations off the top of their head, and exact calculations with scratch paper and a calculator. Over the years, Richway has developed a Mix Design Calculator that makes mix design calculations and proportions a fairly simple process. In addition to calculating necessary batch weights and volumes, another extremely useful feature of the calculator is its ability to also provide cost analysis.\nA simple example of how cellular concrete cost is figured is as follows. Roughly speaking, one yard³ (.76 m³) of finished foam can cost between $10-$15 USD, depending on the water concentrate ratio, foam density, and cost per gallon of foam concentrate. If a 30 PCF (480.55 kg/m³) material starts with one yard of neat cement and has a .50 water cement ratio, it would require 2060lbs (934.4 kg) of Portland and 1030 (467.22 kg) lbs of water. To this we’d add 80 cubic feet (22.65 m³) of foam, to achieve 30 PCF (480.55 kgm³) (wet density) cellular concrete. The total yield would then be 3.75 cubic yards (2.87 m³) of material.\nIf the cost of the base slurry were $175 USD per yard (delivered via local ready mix), we’d be adding $36 of foam [based on $50/gallon (3.79L) foam cost, 3 PCF (48.06 kg/m³) foam density, and 40:1 water concentrate ratio]. The total cost for materials would be $211 USD. This cost divided by 3.75 cubic yards (2.87 m³) total yield, would equal a cost of $56.26 USD per yard of cellular concrete.\nCalculating Batch Amounts\nAs can be seen in the screenshot, the Mix Design Calculator will calculate the required batch amounts based on the desired density and desired volume of material.\nIt’s set up for producing one yard of 30 PCF (480.55 kg/m³) cellular material (wet density). To do this .277 yards³ (.21 m³) of base slurry is needed, requiring 315 lbs (142.88 kg) of Portland, 210 lbs (95.25 kg) of fly ash (40%), and 286 lbs (129.73 kg) of water, for a .55 water:cement ratio.\nApproximately 21 ft³ (.59 m³) of foam is then added to make one yard³ (.76 m³) of cellular concrete. The calculator will also display the required amounts of water and foam concentrate needed, and as already discussed, helps in providing cost analysis for your project."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c38f0856-c90a-4eb1-b3ee-d0ec4a94d120>","<urn:uuid:4eb15a0f-b7b1-48f8-8d31-f1db3db7b0a3>"],"error":null}
{"question":"As a coin collector, I'm curious - what's the difference between how beads and modern US coins are measured?","answer":"Beads are typically measured in millimeters on a metric scale, while modern US coins like quarters and dimes are measured by their metal composition (91.67% copper and 8.33% nickel for both quarters and dimes). This is different from historical US coins, which were measured by their precious metal weight, such as the original silver dollar containing 24.1g of pure silver.","context":["01 of 04\nBead Types Sized in Millimeters\nAlmost all beads are measured on a metric scale in millimeters. One exception is seed beads, which use aught sizes. The millimeter sizing can make it especially difficult to order beads online or shop for them in stores. It can be tricky if you are planning to pair up new beads with beads you already have at home.\nBeads that are measured in millimeters include gemstone, crystal, glass, metal, wood, bone and horn varieties. In most cases, the measurement scale is applied to the actual bead while the strand itself may be sold in inches. Once you have determined the size bead you need, you may want to refer to a bead per inch and beads per strand chart to help you determine how many strands you need for a project or estimate the cost per bead.Continue to 2 of 4 below.\n02 of 04\nComparing Beads to Common Items to Gauge Size\nShort of having a ruler with you at all times, one of the best ways to understand relative bead size is to compare the bead to common objects. Coins are useful everyday objects that work for comparisons. They are easy to find and often found in a purse or pocket.\nOne penny is 1.52 millimeters thick. This gives you a point of reference for bead size. If you stack two pennies together, you can estimate how big a 3 mm sized bead is or use a stack of four to estimate the size of a 6 mm bead. If you are out of pennies, nickels are another good reference. Nickels, at 1.95 mm thick, are just shy of 2 mm. If you are looking at larger beads or pendants, knowing that the diameter of a penny is 19.05 mm can be helpful. The diameter of a nickel is slightly larger at 21.21 mm.\nIf you don't carry a lot of loose change, look for something else that you are always sure to have with you. A favorite pendant, a ring that you always wear, or your watch are potential items. Any of these can help you gauge bead size once you remember their size.Continue to 3 of 4 below.\n03 of 04\nA Bead Sizing Key Chain\nAnother way to keep bead sizing information handy is to make something with different sizes of beads and use it as a reference tool. Labeled keychains are a great option. You can buy key ring jewelry findings and then add your own decoration of hanging beads in the sizes that you need to reference. Label each size with a special tag or numbered bead. Use a strong beading wire or thread to minimize the likelihood of breakage.Continue to 4 of 4 below.\n04 of 04\nBead Size Chart\nA printed bead chart is one of the best ways to compare sizes and avoid purchasing beads that are smaller or larger than you expected. Have a bead chart on hand before making online orders or when planning projects. The bead size chart will help you determine the right size of beads for the length of project you are making or how many beads you will need for specific projects.","The GoldSilver Team\nPeople often wonder about the contents of modern US coins, as well as how those coins have evolved over the past several centuries. It’s a common question: most people know there are no longer any precious metals in our coins, but what exactly are they made of, and how has their composition changed over the centuries? We’ll answer that in a moment, but first a quick look back.\nImage courtesy of The Wall Street Journal\nEstablishing the value and means of production of currency is essential to the conduct of trade within any country, or between nations. The fledgling United States of America, founded in the late 18th century, was no different. The Constitution—which established the federal government in March of 1789—references “money” twice. Article I, Section 8 specifies that Congress “shall have the power to … coin Money, [and] regulate the Value thereof.” And Article I, Section 10 states that “No State shall … make any Thing but gold and silver Coin a Tender in Payment of Debts.”\nNote that, in both cases, money is defined as coins.\nThat continued to be the case in April 1792, when Congress passed The Coinage Act (a/k/a the Mint Act), whose full title is: An act establishing a mint, and regulating the Coins of the United States. The Act established the United States silver dollar as the country's standard unit of money, declared it to be lawful tender, and created a decimal system for U.S. currency (1 dollar = 100 cents). It also established the United States Mint, which would henceforth produce and regulate the country’s coinage.\nThe Coinage Act standardized production of the following coins (“disme” being the original spelling of “dime”), according to the weight of the constituent metal:\n|Coin Name||Face Value||Metal Content|\n|Eagles||$10||16.04 g pure or 17.5 g .920 fine standard gold|\n|Half Eagles||$5||8.02 g pure or 8.75 g standard gold|\n|Quarter Eagles||$2.50||4.01 g pure or 4.37 g standard gold|\n|Dollars or Units||$1||24.1 g pure or 27.0 g .900 fine standard silver|\n|Half Dollars||$.50||12.0 g pure or 13.5 g standard silver|\n|Quarter Dollars||$.25||6.01 g pure or 6.74 g standard silver|\n|Disme||$.10||2.41 g pure of 2.7 g standard silver|\n|Half Disme||$.05||1.2 g pure or 1.35 g standard silver|\n|Cents||$.01||17.1 g of copper|\n|Half Cents||$.005||8.55 g of copper|\nAmong other interesting features of the Act were that citizens “may bring gold and silver bullion [to the Mint], to be coined free of expense”; and, ominously, that the death penalty would be imposed for debasing the gold or silver coins, or embezzlement of the metals for those coins by officers or employees of the Mint.\nThe metals used to make coins were chosen for their chemical and physical properties, particularly as those apply to conditions a currency will encounter. Since coins will be freely circulating, they must have strong wear resistance and anti-corrosive properties. Silver and gold were excellent choices, of course. Copper slightly less so, but they needed something inexpensive for small transactions.\nHowever, there’s a fly in this ointment. In such a coinage system, the underlying value of the metal components could not exceed the putative face value of the coin itself. Otherwise, people would have a significant incentive to melt the coins down into bullion and sell for a profit.\nIn 1933, the U.S. government removed this possibility from the equation by making private possession of gold illegal. The right of citizens to own gold bullion wasn’t restored until 1974. A prohibition on melting silver was put in place in 1967 but the ban was lifted in 1969. It no longer made sense after silver was removed from coinage in 1964.\nToday, gold and silver coins are still minted but not for circulation. They are bought by investors or anyone who desires a store of value. Thus, despite the disparity between a one-ounce gold Eagle’s face value of $50 and its market value of $1300 or so, no one is melting coins down, simply because you can’t buy an Eagle for fifty bucks. Gold and silver are market-priced by the ounce. Whether in the form of bullion or coins, you pay for them by weight. (If you bought your silver coins back in the ‘50s, though, you can now cash them out for about 13 times what you paid.) So, although many people think it’s illegal to melt down gold or silver coins, it isn’t. It’s simply that there’s no benefit to do so.\nWhile gold coins were removed from the system in 1933, silver coins continued to circulate. Eventually, those coins’ underlying value outstripped face value. So, as noted, in order to continue to have coins usable by citizens in transactions, silver was removed from all coinage in 1964 (except for silver-clad half dollars that were minted from 1965-1970).\nToday, as then, quarters and dimes are made of an alloy consisting of 91.67% copper and 8.33% nickel. No one is melting them down, either; a quarter has a market value of around 9 cents; a dime is worth maybe 4 cents.\nGoing to a cupro-nickel alloy solved the problem for formerly-silver coins. But a problem remained with regard to pennies ad nickels.\nThe lowly penny has undergone a lot of transformations. From 1793 to 1837, a one-cent piece was pure copper; from 1837 to 1857, it was made of bronze (95% copper, and 5% tin and zinc); from 1857 to 1864, it was 88% copper and 12% nickel; from 1864 to 1982, it reverted to bronze (except for the 1943 penny, made of zinc-coated steel, and except for the removal of tin in 1962); finally, the two elements essentially flipped in 1982 and the modern copper-plated zinc penny appeared (97.5% zinc and 2.5% copper).\nNickels are less well-traveled, having been standardized at 75% copper and 25% nickel in 1866.\nToday, a penny’s face value lines up pretty well with its intrinsic value; however, the copper content of pre-1982 pennies is around 2.5 cents. The contemporary nickel checks in at about 8 cents. In either case, speculators could pull them from circulation. So, in December of 2006, Congress passed legislation banning the meltage of pennies and nickels. You also can’t export them in quantity.\nNot that this has deterred speculation. In fact, you can find any number of sites on the Internet that recommend stockpiling either nickels or pre-1982 pennies. The logic appears to be that it seems certain the Mint will stop producing pennies someday, and the same may be true of nickels. At that point, restrictions may be lifted (or the composition of the nickel may be debased) and hoarders will benefit. But in the meantime, these pennies and nickels are trading in bulk for more than face value.\nThe bottom line here is that coins with a known gold or silver content will continue to have a fixed value based on metals content (+ any premium commanded by numismatic value, i.e. a given coin’s rarity and desirability to collectors). Spot prices fluctuate, of course, but gold and silver will always serve as an investment, inflation hedge, and insurance against severe monetary disruption.\nNickels and pennies are less of an investment. Pennies, in particular, are so bulky that storage is a problem; and, in addition, if you buy rolls of them from a bank, you’ll have to spend the time to sort them by date. Nickels are less bulky but still require a lot of space. Whether either will someday have a liquid market where they’re worth more than you pay for them remains to be seen."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:3eb9451d-d6aa-4a88-b8b7-d7fa23e7a4e1>","<urn:uuid:318fa115-31b7-4ea9-ae81-b9ff783e0d54>"],"error":null}
{"question":"As a chocolate enthusiast learning about metallurgy, I'm curious how the tempering process in chocolate making compares to stress relieving in steel manufacturing. Could you explain the similarities and differences in these heat treatments?","answer":"Both tempering chocolate and stress relieving steel involve careful temperature control to achieve desired properties, but they work differently. In chocolate tempering, the goal is to create specific beta crystals by following three main temperature stages: first melting completely (up to 120-131°F), then cooling to about 82°F, and finally warming to 89-91°F for dark chocolate. This process ensures the chocolate is shiny, firm, and snaps properly. In contrast, stress relieving steel involves heating to a suitable temperature and holding for a proper time, then cooling slowly to reduce internal residual stresses that may have been induced during casting, quenching, normalizing, machining, cold working, or welding. While both processes aim to improve the material's properties through controlled heating and cooling, chocolate tempering focuses on crystal structure formation for texture and appearance, while steel stress relieving aims to reduce internal stresses for structural integrity.","context":["When you examine a top-quality chocolate bar or a well-made dipped truffle, you'll see that the chocolate is shiny, firm enough to tap with your fingernail, and will break with a sharp snap. That's because it's tempered. Tempering is a process that encourages the cocoa butter in the chocolate to harden into a specific crystalline pattern, which maintains the sheen and texture for a long time.\nWhen chocolate isn't tempered, it can have a number of problems: it may not ever set up hard at room temperature; it may become hard, but look dull and blotchy; the internal texture may be spongy rather than crisp; and it can be susceptible to fat bloom, meaning the fats will migrate to the surface and make whitish streaks and blotches.\nAnytime you need chocolate to be firm at room temperature and to have a glossy sheen and a crisp texture--as you would with chocolate nut clusters, dipped candies or decorations like chocolate leaves or ruffles--you must temper the melted chocolate. For tempering, always use top-quality dark, milk or white chocolate. Compound chocolate, which is a lower-quality chocolate, contains other fats beside cocoa butter, so it often doesn't need tempering. And chocolate that's combined with other ingredients, as in a chocolate cake or mousse, doesn't need to be tempered.\nBeta crystals: the goal of tempering —\nTempering seems like a mysterious process because you can't really tell what's happening; instead, you need to learn to control the process only by temperature, by sight, and by touch.\nWhen chocolate is melted and cooled, it can crystallize into any one of six different forms. Unfortunately, only one of these—the beta crystal, or Form V—hardens into the firm, shiny chocolate that cooks want.\nTraditionally, pastry chefs and chocolate manufacturers use one of several tempering methods that all contain the following stages:\nStage one: Melting the chocolate so that the cocoa butter melts completely. Most cooking literature advises you not to get the chocolate over 120° F for fear of burning the cocoa solids, or causing the chocolate to irreversibly separate into solids and fat. But melting curves of chocolate in the technical literature indicate that most of the fats in cocoa butter aren't melted until 122° F, and some processors recommend heating their chocolate even higher—up to 131° F. If you're serious about perfecting tempering, you should consult the manufacturer of the chocolate you're using for the best temperature. Cocoa beans from different locations vary: at the same temperature, cocoa butter from Malaysian beans grown near the equator will be firm, while cocoa butter from Brazilian beans grown in a cool climate will be very soft.\nStage two: Rapid cooling to about 82° F for dark chocolate (79° F for milk and white chocolates). This gets the crystallization of the good beta crystals started, but it does allow some undesirable beta-primes to form, too.\nStage three: A slight warming, first back to 86° F for dark (84° F for milk and white) where it's held for a few minutes to let the beta crystals continue to form, and then a final warming to 89° to 91° F for dark and 87° to 89° F for milk and white. This final increase in temperature melts the undesirable beta-prime crystals that were formed.\nStage four: Verify that your chocolate is indeed in temper. Smear a thin layer onto a piece of parchment or waxed paper, then wait five minutes and try to peel the chocolate from the paper. If it peels easily and the chocolate is shiny, not blotchy, you're fine. If not, start the tempering process again.\nA radical shortcut to tempering —\nWhen you buy blocks or pastilles of good-quality chocolate from the manufacturer, that chocolate is already tempered. Is there a way to just maintain that temper and avoid going through the whole process again? I learned from an internationally renowned chocolate expert, Dr. Paul Dimick, Professor Emeritus of Food Science at Pennsylvania State University, that this is indeed possible. The good beta crystals don't melt until 94° F, so if you never heat chocolate over 91° to 92° F, you won't lose them and your melted chocolate will remain tempered. The trick is to barely melt the chocolate. Chocolate begins to melt at about 89° F. Start by grating or finely chopping the chocolate so it melts evenly. Put the chocolate in a metal bowl and warm it over very low heat -- an electric heating pad is a neat idea. Stir constantly until about two-thirds of the chocolate is melted. Take the bowl from the heat and continue stirring until all the chocolate is melted. For dark chocolate, you want the whole mass to end up at 89° to 91° F (87° to 89° for milk and white). As long as you haven't exceeded 92° F, your beta crystals should be fine.\nYou do need to be sure that the chocolate you start out with is truly in temper. Chocolate that has been stored improperly or for a long time may look all right, but it could be on its way to losing its temper. If you use this shortcut method, you should still test the chocolate on parchment or waxed paper to make sure that the chocolate sets up hard and shiny.","Glossary of terms used in heat treatment of steel\nGlossary of terms used in heat treatment of steel\nVarious terms used in the heat treatment of steels are described below:\nAgeing – It describes a time temperature dependent change in the properties of certain alloy steels. It is a change in properties that may occur gradually at atmospheric temperature (Natural ageing) and more rapidly at higher temperature (Artificial ageing).\nQuench ageing – It is a change in properties that may occur gradually at atmospheric temperature and more rapidly at higher temperature following rapid cooling (Precipitation hardening).\nStrain ageing – It is a change in properties that may occur gradually at atmospheric temperature and more rapidly at higher temperature following plastic straining.\nAnnealing – It is a term denoting a treatment, consisting of heating to and holding at a suitable temperature followed by cooling at a suitable rate, used primarily to soften but also to simultaneously produce desired changes in other properties or in microstructure. The purpose of such changes may be, but is not confined to:\n- Inducing softness\n- Improving machinability\n- Improving cold working properties\n- Improving mechanical or electrical properties\n- Increasing stability of dimensions\n- Obtaining a desired structure\n- Removing stresses\nThe time temperature cycle used vary widely both in maximum temperature attained and in cooling rate employed, depending on the composition of the steel, its condition, and the result desired. Various types of annealing processes are as follows:\nBright annealing– It is annealing in a protective medium to prevent discoloration of the bright surface.\nCycle annealing – It is an annealing process employing a predetermined and closely controlled time temperature cycle to produce specific properties or microstructure.\nFlame annealing – It is an annealing process in which the heat is applied directly by a flame.\nFull annealing – It is heating to and holding at some temperature above the transformation range, followed by cooling slowly at a rate through the transformation range such that that the hardness of the steel approaches a minimum.\nGraphitizing – In this type of annealing process the annealing of steel is done in such a way that some or all of the carbon is precipitated as graphite.\nIntermediate annealing – It consists of annealing of steel at one or more stages during manufacture and before final thermal treatment.\nIsothermal annealing – It is heating to and holding at some temperature above the transformation range, then cooling to and holding at a suitable temperature until austenite to pearlite (Ferrite – carbide) transformation is complete and finally cooling freely.\nProcess annealing – It is an imprecise term which is used to denote various treatments that improve workability.\nQuench annealing – It is an annealing of austenitic steel by solution heat treatment.\nSpheroidizing – It is heating and cooling in a cycle designed to produce a spheroidal or globular form of carbide.\nSub critical annealing – It is heating to and holding at some temperature below the transformation range, followed by cooling at a suitable rate.\nAustempering – It is quenching from a temperature above the transformation range to some temperature above the upper limit of martensite formation, and holding at this temperature until the austenite is completely transformed to the desired intermediate structure, for the purpose of conferring certain mechanical properties. It is carried out in a medium having a rate of heat abstraction high enough to prevent the formation of high temperature transformation products.\nAustenitizing – It is forming austenite by heating into the transformation range (partial austenitizing) or above the transformation range (complete austenitizing). When used without qualification, the term implies complete austenitizing.\nBlueing – It is a treatment of the surface of steels usually in the form of sheet or strip, on which, by the action of air or steam at a suitable temperature, a thin blue oxide film is formed on the initially scale free surface, as a means of improving appearance and resistance to corrosion. This term is also used to denote a heat treatment of springs after fabrication, to reduce the internal stress created by coiling and forming.\nCarbon potential – It is a measure of the ability of an environment containing active carbon to alter or maintain, under prescribed conditions, the carbon content of the steel exposed to it. In any particular environment, the carbon level attained will depend on such factors as temperature, time, and steel composition.\nCarbon restoration – It is the replacement of the carbon lost in the surface layer from previous processing by carburizing the layer to the original carbon level.\nCarbonitriding – It is a case hardening process in which steel part is heated above the lower transformation temperature in a gaseous atmosphere of such composition (Mixture of carburizing and nitriding gases) as to cause simultaneous absorption of carbon and nitrogen by the surface and, by diffusion create a concentration gradient. The process is completed by cooling at a rate that produces the desired properties in the steel part.\nCarburizing – It is a process in which carbon is introduced into solid steel by heating it and holding it above the transformation temperature range while in contact with a carbonaceous material that may be a solid, liquid, or gas. Carburizing is frequently followed by quenching to produce a hardened case.\nCase hardening – In this treatment the surface layer of steel has been suitably altered in composition and can be made substantially harder than the interior or core. Normally case hardening is followed by suitable heat treatment. It is also used to designate the hardened surface layer of a piece of steel that is large enough to have a distinctly softer core or centre.\nCold treatment – It is exposing the steel to suitable subzero temperature for the purpose of obtaining desired conditions or properties, such as dimensional or microstructural stability. When the treatment involves the transformation of retained austenite, it is usually followed by a tempering treatment.\nConditioning heat treatment – It is a preliminary heat treatment used to prepare a material for a desired reaction to a subsequent heat treatment.\nControlled cooling – It is a term used to describe a process by which a steel product is cooled from an elevated temperature, usually from the final hot forming operation in a predetermined manner of cooling to avoid hardening, cracking, or internal damage.\nCore – It is the interior portion of steel which is substantially softer than the surface layer (case) after case hardening. The term core is also used to designate the relatively soft central portion of certain hardened tool steels.\nCritical range and critical points – It is synonymous with the term transformation range. These are arrest points, critical temperatures, change points and transformation points at which changes occur in the constitution of steels.\nCyaniding – It is the introduction of carbon and nitrogen into the surface of steel by heating to and holding at a suitable temperature in contact with molten cyanides.\nDecarburization: it is the loss of carbon from the surface of steel as the result of heating in a medium that reacts with the carbon.\nDifferential heating – Any method of heating so controlled as to produce a desired non uniform temperature distribution in a steel object. It is a heating process by which the temperature is made to vary throughout the steel object being heated so that on cooling different portions, may have such different physical properties as may be desired.\nDifferential quenching – It is the selective quenching of the different parts of the same steel object.\nDrawing – It is drawing the temper and is synonymous with tempering.\nEutectic alloy – It is the alloy composition that freezes at constant temperature similar to a pure metal. It is the lowest melting (or freezing) combination of two or more metals. The alloy structure (homogeneous) of two or more solid phases formed from the liquid eutectically.\nFlame hardening – It is a process of heating the surface layer of steel above the transformation temperature range by means of a high temperature flame followed by quenching.\nGrain refining – It is heating from some temperature below the transformation range to a suitable temperature above that range followed by cooling at a suitable rate.\nHardenability – It is the property in steel that determines the depth and distribution of hardness induced by quenching.\nHardening – It is a process of increasing hardness of metal by suitable treatment usually involving heating and cooling. This usually implies quenching from a temperature either within or above the transformation range.\nHeat treatment – It is a combination of heating and cooling operations applied to steel to obtain desired conditions or properties. Heating for the sole purpose of hot working is excluded from the meaning of this definition.\nHeat treatment solution: It is a treatment in which an alloy is heated to a suitable temperature and held at this temperature for a sufficient length of time to allow a desired constituent to enter into solid solution, followed by rapid cooling to hold the constituent in solution. The material is then in a supersaturated unstable state which may subsequently exhibit age hardening.\nHomogenizing – It is a high temperature heat treatment process intended to eliminate or to decrease chemical segregation by diffusion.\nInduction hardening – It is heating by means of an alternating magnetic field to a temperature within or above the transformation range followed immediately by quenching.\nIsothermal transformation – It is a change in a phase of steel at constant temperature.\nMalleablizing – It is a process of annealing white cast iron in which the combined carbon is wholly or partly transformed to graphitic or free carbon and, in some cases, part of the carbon is removed completely.\nMaraging – It is a precipitation hardening treatment applied to special steel to precipitate one or more of intermetallic compounds.\nMartempering – It is quenching from a temperature above the transformation range to some temperature above the upper limit of martensite formation, holding at that temperature long enough to permit equalisation of temperature without transformation of the austenite followed by cooling in air. This results into the formation of martensite which may be tempered as desired.\nIt is a hardening procedure in which an austenitized steel object is quenched into an appropriate medium whose temperature is maintained substantially at the Ms temperature of the steel object, held in the medium until its temperature is uniform throughout but not long enough to permit bainite to form, and then cooled in air. The treatment is followed by tempering.\nNitriding – It is a process of case hardening in which a steel of special composition is heated in an atmosphere of ammonia or in contact with nitrogenous material. Surface hardening is produced by the absorption of nitrogen without quenching.\nNormalizing – It is a process in which steel is heated to a temperature above the transformation range and subsequently cooled in still air at room temperature.\nOverheated – Steel is said to have been overheated if, after exposure to an unduly high temperature, it develops an undesirably coarse grain structure but is not permanently damaged. The structure damaged by overheating can be corrected by suitable heat treatment or by mechanical work or by a combination of the two. In this respect it differs from a burnt structure.\nPatenting – It is heating to a suitable temperature well above the transformation range, followed by cooling to a temperature below that range in air or in a bath of molten lead or salt maintained at a suitable temperature to produce a structure which will facilitate subsequent cold working and give the desired mechanical properties in the finished state.\nPrecipitation hardening – It results in hardening due to the precipitation of a constituent from a super saturated solid solution which occurs in certain suitably quenched steel on heating to and holding at some temperature below the transformation range.\nPreheating – It is heating of steel to an appropriate temperature immediately prior to austenitizing when hardening high hardenability construction steel, several types of tool steels and heavy sections.\nQuenching – It is the rapid cooling of steel. When applicable the following more specific terms are used.\nDirect quenching – It is quenching of carburized steel part directly from the carburizing operation.\nFog quenching – It is quenching in an air mist.\nHot quenching – It is a term used to cover a variety of quenching procedures in which a quenching medium is maintained at a prescribed temperature above 71 deg C.\nInterrupted quenching – It is a quenching procedure in which the steel is removed from the first quench at a temperature substantially higher than that of the quenching medium and is then subjected to a second quenching system having a different cooling rate than the first.\nSelective quenching – It is quenching of only certain portion of steel.\nSlack quenching – The incomplete hardening of steel due to quenching from the austenitizing temperature at a rate slower than the critical cooling rate for the particular steel. It results into the formation of one or more transformation products in addition to martensite.\nSpray quenching – It is quenching of steel in a spray of liquid.\nTime quenching – It is interrupted quenching in which the duration of holding of the steel in the quenching medium is controlled.\nSecondary hardening – It is a hardening effect which occurs on cooling certain previously hardened steel from a particular range of tempering temperature.\nSoaking – It is prolonged heating of a metal at a selected temperature.\nSpheroidizing – It is subjecting steel to a selected temperature cycle usually within or near the transformation range in order to produce a suitable globular form of carbide for such purpose as:\n- Improving machinability\n- Facilitating subsequent cold working\n- Obtaining a desired structure for subsequent heat treatment\nStabilizing Treatment – It is treatment applied to stabilize the dimensions of steel or the structure of the steel material. For example\n- Before finishing to final dimensions, heating the steel to or somewhat beyond its operating temperature and then cooling to room temperature a sufficient number of times to ensure stability of dimensions in service\n- Transforming retained austenite in those materials that retain substantial amounts when quench hardened (see cold treatment)\n- Heating a solution treated austenitic stainless steel that contains controlled amounts of titanium or niobium plus tantalum to a temperature below the solution heat treating temperature to cause precipitation of finely divided, uniformly distributed carbides of those elements, thereby substantially reducing the amount of carbon available for the formation of chromium carbides in the grain boundaries on subsequent exposure to temperature in the sensitizing range.\nStress relieving – It is a process to reduce internal residual stresses in steel by heating the steel to a suitable temperature and holding for a proper time at that temperature and then cooling slowly. This treatment is applied to relieve internal stresses induced during casting, quenching, normalizing, machining, cold working, or welding.\nSub zero treatment – It is cooling hardened steel to a temperature sufficiently below zero deg C to promote the transformation of any retained austenite to martensite.\nTemper carbon – It is the free or graphitic carbon that comes out of solution usually in the form of rounded nodules in the structure during Graphitizing or Malleablizing.\nTempering – It is heating hardened, normalized or mechanical worked steel to some temperature below the transformation range and holding for a suitable time at that temperature, followed by cooling at a suitable rate. The process is usually applied for the purpose of producing a desired combination of mechanical properties.\nDouble tempering – It is a treatment in which quenched hardened steel is given two complete tempering cycles at substantially the same temperature for the purpose of ensuring completion of the tempering reaction and promoting stability of the resulting microstructure.\nSnap temper – It is a precautionary interim stress relieving treatment applied to high hardenability steel immediately after quenching to prevent cracking because of delay in tempering them at the prescribed higher temperature.\nTemper brittleness – It is the brittleness that results when certain steel is held within, or is cooled slowly through, a certain range of temperatures below the transformation range. The brittleness is revealed by notched bar impact test at or below room temperature.\nTransformation range or Transformation temperature range: These are those ranges of temperature within which austenite form during heating and transforms during cooling. The two ranges are distinct, sometimes overlapping but never coinciding. The limiting temperatures of the ranges depend on the composition of the alloy and on the rate of change of temperature particularly during cooling.\nTransformation temperature – The temperature at which a change in phase occurs. The term is sometimes used to denote the limiting temperature of a transformation range. The following symbols are used for iron and steel:\nAccm – In hypereutectoid steel, the temperature at which the solution of cementite in austenite is completed during heating\nAc1 – The temperature at which austenite begins to form during heating\nAc3 – The temperature at which transformation of ferrite to austenite is completed during heating\nAc4 – The temperature at which austenite transforms to delta ferrite during heating\nAe1, Ae3, Aecm, Ae4 – The temperatures of phase changes at equilibrium\nArcm – In hypereutectoid steel, the temperature at which precipitation of cementite starts during cooling\nAr1 – The temperature at which transformation of austenite to ferrite or to ferrite plus cementite is completed during cooling\nAr3 – The temperature at which austenite begins to transform to ferrite during cooling\nAr4 – The temperature at which delta ferrite transforms to austenite during cooling\nMs – The temperature at which transformation of austenite to martensite starts during cooling\nMf – The temperature, during cooling, at which transformation of austenite to martensite is substantially completed\nAll these changes except the formation of martensite occur at lower temperatures during cooling than during heating, and depend on the rate of change of temperature."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:96eb7d97-305c-4fee-97bd-e46131f36659>","<urn:uuid:8ffec1a0-d2b4-48b9-9206-1e0a21601df6>"],"error":null}
{"question":"How do the American and German WWII cemeteries in Normandy differ in their memorial styles and gravestone characteristics?","answer":"The Normandy American Cemetery at Colleville-sur-Mer and the German Cemetery at La Cambe show distinct differences in their memorial styles. The American Cemetery features bright white crosses and is situated dramatically on a cliff overlooking Omaha Beach, with a reflecting pool and main memorial. In contrast, the German Cemetery has somber flat brown markers and a central tumulus (burial mound) rising 20 feet high, topped with lava rock statues of Jesus and Mary. While the American graves include information about where the soldiers were from, the German gravestones are simpler, showing just basic information like name and date of death.","context":["|Normandy Places of Interest\nKeep your eyes and ears open during\nyour visit, for the most vivid history of all\nmay be sitting at the next table at dinner.\nWe have met WWII vets on every visit to\nNormandy and many have become\nenduring friends over the years. Be brave\nand strike up a conversation - most are\neager to talk about their experiences.\nAt left is 4th Infantry Division veteran\nIrving Smolens and his daughter Karen on\nUtah Beach in 2004. Over the years our\nfamilies have become close and have\nvisited back and forth several times.\nGold, Juno, and Sword Beaches: These beaches are in the Canadian and British\nsectors of the invasion, around Pegasus Bridge. Small British military cemeteries\nare scattered along the back roads in the area and contrast with the huge American\ncemeteries. They also differ in their personalized inscriptions, e.g. “Rest your head\nawhile, my darling, and I’ll meet you in another place”. Some cemeteries include a\nfew German graves marked with simple black stones.\nNormandy American Cemetery at Colleville-sur-Mer: This cemetery pictured in\n“Saving Private Ryan” is situated on a cliff overlooking the Fox segment of Omaha\nBeach. Check the website for open hours. A footpath descending to the beach is\nworthwhile; this bluff was the site of an intense battle, and the a command post\nwas reputedly situated between the reflecting pool and main memorial.\nThe Back Roads: Scattered along the winding country roads behind Omaha\nBeach are numerous villages with histories of their own. There are also many\nsmall museums which chronicle the human experiences of each place. Watch for\nbullet marks from gunfire, still visible on the stone walls near corners and\nintersections. Hedgerows 10-15 feet high can still be seen on the farm roads; these\nunforeseen obstacles nearly foiled the success of the invasion. The hedgerows are\ndisappearing as farms are consolidated into larger more-easily-plowed fields. We\nrecommend picking up a Michelin map to the area.\nSt-Laurent-sur-Mer, Vierville, and Omaha Beach: Each entry from the sea to\nthe bluffs has a monument, with a small beach road connecting the sites (which\nwasn’t there in 1944). There is also a marked path on top of the bluffs extending\nthe entire length of the beaches; KMC tourists highly recommended the trip but\nsay it makes for a strenuous day. Note the two museums on Omaha Beach, both\nlocated in salvaged WWII buildings. Each one has its own character and is worth a\nvisit. A drive along the beach road is recommended to see where the “shingle”\nused to be, a low wall and stretch of egg-sized stones that provided cover but also\ntrapped troops on the beach. The swampy area which posed so much difficulty to\nthe troops is still visible, though dry now and free of concertina wire and mines.\nLook for the site of the original US cemetery along this road in an area of beach\ncottages. German gun emplacements are clearly visible on the bluffs.\nPointe du Hoc: This area 7 km east of Omaha Beach is a must see on any\nNormandy trip. According to veterans, the area today is much like they found it 6\nJune 44, with huge shell craters, intact German bunkers, and an extensive system\nof trenches and gun emplacements. Unfortunately, deterioration of the cliff area\nprevents visitors from seeing the vertical wall scaled by the Rangers. The La\nCambe German cemetery is not far inland from Pointe du Hoc.\nGuided Tours: Many companies offer DDay tours lasting from a half to two\ndays. If you have the time and money, this is the best way to see it all in a short\ntime. The following have been recommended by friends as well-worth the Euros:\nHistoric WW2 Tours\nFor small group tours, the owner of former Battlebus Tours, Paul Woodadge, has\nunanimous great reviews. He is now working on his own at DDay Historian site.","Our D-Day travels around France continue, this time we’re visiting the Normandy German Cemetery. Holding the remains of the aggressors and, eventually, the losing side of the war makes this cemetery much less popular than the various Allied cemeteries scattered around the area.\nIt is important to visit, I think, and further ponder the war, fascism, these men as human beings, and the war many of them probably didn’t want to fight.\nWith 21,222 total burials (compared to less than 10,000 at the Normandy American Cemetery) would you believe this cemetery is only the second largest World War II German Cemetery in France? And in fact it is just the 6th largest World War II German Cemetery in all of Europe. The point is, a lot of Germans died.\nLa Cambe is cared for by the privately-funded German War Graves Commission (or Volksbund). They care for 2.6 million graves in 832 cemeteries in 45 countries.\nOriginally a military cemetery for Allies and Axis, the Americans removed their dead for the American cemetery 10 miles away after 1945. This cemetery was dedicated in 1961 to the German war dead.\nThere are chapels on either side of the entrance gate.\nOn the wall is the cemetery directory where you can look up your last name to make sure you didn’t have any Nazis in your family. Or maybe that’s just something I would do.\nThere are some very bad people here. Just looking at this page in the Namenbuch, some of these titles are scary.\nRottenfuhrer is an SS rank, one of the worst of the worst Nazi groups, the ones mostly responsible for the Holocaust.\nOne of the worst guys buried here at La Cambe is Sturmbannführer Adolf Diekmann, responsible for the massacre of Oradour-sur-Glane. The SS killed 642 innocent people there in 1944. 247 women and 205 children were locked in a church that the SS then set on fire. So heinous, even the leadership of the SS was to court-martial Diekmann, but he died in battle before he could be punished.\nLeaving the entrance for the open spaces of the cemetery, you feel death like you always do in a cemetery. I’ve just never been in one before with such a Nazi cloud overhead. I didn’t once feel sad here, and I know many of these men, these kids, probably didn’t want anything to do with Hitler or the war. But at least in my case it’s still tough to give them that benefit of the doubt.\nThe tumulus (a burial mound), visible from all areas of the cemetery, rises 20 feet above the flat ground of the cemetery. Under the lava rock statue lies 296 mostly unknown German soldiers buried in a mass grave.\nKnowing that beneath you is a mass grave, it seems odd that you can climb to the top to look out from the top, but the back side of the tumulus has stairs to get you there.\nWaiting for you at the top are the figures of Jesus and Mary.\nA few panorama photos from the tumulus help show the size and scope of the Normandy German Cemetery.\nThe carved crosses are of the same style as the Jesus and Mary statue atop the tumulus.\nThe stones are roughly carved while the gravestones are smooth.\nThe gravestones are somber flat brown markers. Obviously very different from the bright white crosses of the Normandy American Cemetery. Many of the grave stones I encountered were just kids. These two soldiers were only 17 at the time of their deaths. It would have been nice to know where they were from, like the American Cemetery shows. But, as Germans do, they kept it simple.\nWhile not a gleaming museum like it’s American cemetery counterpart, the visitor center here does it’s best to tell the story of the Normandy Invasion. It’s a tough spot to talk about WWII from a German point of view, so the center really doesn’t attempt that.\nThere are some nice artifacts from that time including this German Soldier’s identification book.\nThe cemetery is open daily from 8:00 to 7:00pm. The visitor center is open daily from 8:00am to 12:00pm and 1:00 to 7:00pm. Everyone needs a lunch break.\nA peace garden with 1,200 maple-trees is nearby. It is a pretty sight as you’re driving into the cemetery on a tiny little road. Parking was quite easy, I was one of about 5 cars in the parking lot.\nWithout a doubt the Normandy German Cemetery at La Cambe is worth a visit in person, especially if you’re in the region to see World War II sites.\nthe plaques, we were told, are made of German clay.\nOh very interesting. Thanks for this additional info Allyson!\nYour comments reflect the same feelings Mom and I have."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:13b41c4a-fcff-4e45-9ad6-d941b7fbfedd>","<urn:uuid:f59d8ae5-8dbb-499e-b7d6-f7c5b0ddd16a>"],"error":null}
{"question":"What are the environmental benefits of vertical farming compared to traditional agriculture, and how does Ohio's landscape contribute to bird conservation?","answer":"Vertical farming offers significant environmental advantages over traditional agriculture. It uses up to 95% less water per plant through hydroponic systems and is 350 times more productive per square meter than outdoor farming. It also reduces the need for pesticides, herbicides, and fungicides while freeing up arable land. As for Ohio's landscape, it plays a crucial role in bird conservation through its diverse habitats. The state features prairies, unglaciated Appalachian plateaus, and Lake Erie, providing critical habitats for various bird species. Ohio's hemlock gorges support isolated southern populations of boreal breeders, and its reclaimed surface mines have become key breeding areas for declining grassland species. The state is also vital for migratory birds, with Lake Erie serving as an essential pathway for 19 species of gulls and nearly 50 species of shorebirds.","context":["Ohio Provides Critical Bird Habitat\nOhio is blessed with a geographic orientation that brings us lots of birds—both numbers and species. Our total species list, which includes extinct and extirpated birds, was 431 as of July, 2015. Of the five states which adjoin us, only Michigan’s list exceeds ours though Pennsylvania’s is close behind.\nPerhaps more important than high species lists are the large numbers of certain birds that Ohio plays an integral role in supporting, either by being on their migratory path or by providing breeding habitat. And since our state has strong influences from all directions—prairies from the west, unglaciated Appalachian plateaus on the south and east, and of course our great inland sea to the north, Lake Erie—we are critical to a broad suite of species.\nFrom gulls to sparrows, Ohio harbors some of the best habitats of any Midwestern state for birds. The examples are many. Nineteen species of gulls have been documented, an amazing total for ANY state, and this is in large part due to the importance of Lake Erie as a migratory pathway and wintering area. This fantastic lake also is critical to many species of shorebirds, and it is important that we work to insure that suitable habitats are provided at key periods of their passage. Of the almost 50 species of shorebirds that have occurred here, many have evolved a critical dependence on western Lake Erie habitats as stopover locale to rest and refuel for the incredible trans-global migrations they undertake.\nGrassland Bird Numbers on the Rise\nSurface coal extraction—strip mining—while rightly considered an ecological catastrophe has had an unexpected but fortuitous benefit in southeastern Ohio by creating enormous grasslands during “reclamation.” Many of us are familiar with The Wilds and the adjoining Ohio Power lands in Muskingum County, a good example of this sort of habitat. These grasslands have become key breeding areas for declining species such as the Henslow’s Sparrow, and consequently Ohio now supports some of the best remaining populations of grassland breeders in the Midwest. Of course, these sites are sensational for wintering birds, too, in particular raptors. And the hundreds of thousands of acres of reclaimed surface mines are large enough to have altered the wintering range of Golden Eagles and also attract huge numbers of Northern Harriers, Rough-legged Hawks, and Short-eared Owls.\nProtecting Habitats to Preserve Bird Diversity\nProtection of outlying or edge-of-range habitats is key to preserving genetic diversity, and Ohio has a very significant example of a disjunct habitat. Hemlock gorges such as those found in Hocking County, Mohican State Forest, and far northeastern Ohio are examples of a boreal habitat largely isolated from the core of this plant community far to the north. Coming along with these hemlocks are isolated southern populations of boreal breeders, such as Hermit Thrush; Blue-headed Vireo; Canada, Magnolia, and Blackburnian warblers; and Dark-eyed Junco. Not only are hemlock gorges one of Ohio’s most beautiful habitats, they are also one of the most biologically significant.\nBetween the hemlock gorges, the expanses of eastern deciduous forests that still cloak parts of Ohio, and other habitats, we support about 180 species of breeding birds annually. Of global importance is the group known as neotropical migratory birds. This group includes many perennial favorites such as Scarlet Tanager, Baltimore Oriole, and American Redstart. Over 60 species of neotropicals breed in Ohio and winter well to the south in places like Costa Rica, Nicaragua, and Columbia. Protecting Ohio habitats for these birds—like the icon species of the Society, Cerulean Warbler- becomes vital on a global scale.\nOf course, the Buckeye state is legendary as a migratory pathway for migrants, and the Magee Marsh Wildlife Area bird trail is famous among birders far and wide. On a good May day, the trees and thickets here can seem to drip with birds, and even the mundane becomes extraordinary, such as the massive packs of Blue Jays winging overhead. Thousands of birders converge to observe the spectacular spring migration, and someone often turns up a rarity like Kirtland’s Warbler.\nBirders Bring Sizable Economic Benefits\nBirding brings dollars to the state, too, and our economic impact will only continue to increase. A study conducted by the U.S. Fish and Wildlife Service in 2006 estimated that 2.4 million Ohioans watched birds and that 3.5 million “wildlife watchers” spent $2 billion dollars here for equipment, supplies, and travel. Birders are a large, diverse, well educated, and environmentally aware group as a rule, and our potential collective clout is tremendous.\nOhio Ornithological Society members have contributed to many conservation projects through direct donations and the auctions, raffles, and “buy a bird” poster at OOS events. Major grants have gone for land acquisition at the Edge of Appalachia Preserve in Adams County and Meadowbrook Marsh Preserve in Erie. The Society has distributed over $60,000 from contributions and matching funds.\nThe Importance of Advocation\nOf the roughly 200 species of birds that have been known to nest in Ohio, three are extinct—Passenger Pigeon, Carolina Parakeet, and Ivory-billed Woodpecker. Another five should be considered extirpated (locally extinct), although we once harbored established breeding populations—Greater Prairie-Chicken, Piping Plover, Bewick’s Wren, Golden-winged Warbler, and Bachman’s Sparrow. Merlin and Common Raven have recently bred in the state after long absences, but can hardly be considered established. Most of these species disappeared before there were people advocating their protection and taking steps to insure their survival. While their loss is a tragedy, it should also serve as a lesson as to the importance of birders banding together to form a collective voice that can speak for the birds.\nNews & Events\nUPDATE! Registration Now Open! Join your fellow OOS members at Shawnee State Park for a celebration of Warblers and Wildflowers! Not only will this event give you a chance to head south to meet the first big wave of spring migration, you'll also get to enjoy excellent...read more\nThe OOS will return to The Wilds on January 21, 2017, for the 13th annual trip to find wintering raptors! This event is a THANK YOU to all of the OOS members that help support our great organization! The Wilds is a large reclaimed surface mine grassland area that...read more\nOn November 5, 2016 the Ohio Young Birders Club will be holding their 10th Annual Conference at the Toledo Zoo. Young birders and adults from around Ohio and the nation will gather to enjoy a day full of field trips, presentations made by the young birders...read more","Urban Crop Solutions – sustainability and vertical farms\nBefore you dive into this article too deeply, take a moment to read the following description, and then close your eyes for a second:\nImagine yourself standing inside a climate-controlled, high-ceiling warehouse. In front of you stands a tower with eight irrigated levels, on each of which lettuces, herbs, microgreens, and baby greens grow under LED lights. Robots bring trays with young plants from outside into the right position in the growing tower, while on the other end fully grown crops are taken out, ready to be harvested. Can you see it? You are standing in a plant factory – an indoor vertical farm – a highly engineered manufacturing plant producing not goods, but crops.\nGlobal food and water challenges\nBy 2050, the world will have an additional 2.5 billion mouths to feed – 70% of which will live in urban areas. Meanwhile, 80% of the worlds arable land is already in use, existing agriculture is placing an ever-increasing toll on the environment, and the symptoms of climate change are threatening our food production capacity. You need not look further than the global record-breaking droughts we have witnessed in 2018 to see the strain under which the system finds itself. Moving the production of crops indoors and thinking vertically clears up vast areas of land.\nBy some estimates, indoor vertical farming is 350 times more productive per square meter of footprint than outdoor farming. Moreover, an indoor farm can be located anywhere in the world supplying any area with locally grown crops. This reduces the heavy logistics in the current distribution system and provides higher nutritional value for the consumers by limiting the time between harvest and consumption. As such, a higher degree of food safety can be guaranteed without the use of pesticides, herbicides, or fungicides. Compared to open field farming, a hydroponic vertical farm (plants positioned with roots in nutrient-rich water instead of soil) also uses up to 95% less water per plant.\nUrban Crop Solutions\nUrban Crop Solutions engineers and builds fully automated indoor vertical farms inside large buildings and shipping containers. The farms can be tailored to include carefully selected and tested seeds, substrates, nutrients, and a comprehensive software tool automatically provides the plants with the right combination of climate, lighting, and nutrients throughout their growth cycle. The result is a year-round production of fresh and healthy crops – anywhere.\nTo achieve these results, Urban Crop Solutions' engineers and biologists worked together closely to achieve a deep understanding of what crops need to grow, and then how a system could be engineered that would best meet those needs. An equally important component: they needed to determine how to make said system in such a way that ensured consistently high-quality output at low cost (investment and maintenance).\nHow it works\nThe engineering team focused relentlessly to develop the optimal technology for managing all plant-variables with minimal discrepancies in performance.\n- A HVAC (heating, ventilating, and air conditioning) system creates an airflow that subjects all the plants to the: quasi-identical air movement speed, CO2 levels, relative humidity, and temperature throughout each layer of the system.\n- An LED light solution is tailored to each project to optimize photosynthesis of the selected crops.\n- A nutrient delivery system monitors the different chemical compounds in the water needed by the plants (which changes at different stages of growth) and delivers them directly to the roots.\nOne of the benefits of solutions such as this lies in its automation using robotics. The increasing labour costs form a problem for agriculture just about anywhere in the world, and food security can be achieved, in part, by optimizing the interaction between labour and productivity. Vertical farming is a prime example of sustainable intensification (SI) put into practice – utilising less space and resources while providing significantly more yield.\nFour engineers awarded the world’s most prestigious engineering accolade for the research and development of PERC solar photovoltaic technologyRead more\nKelly Raymont-Osman explains how engineering proved central to the design and manufacture of the Queen’s Baton ahead of the Birmingham 2022 Commonwealth Games.Read more"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:9ed38120-2b55-4d25-8632-3d1a2f48c58d>","<urn:uuid:03f25653-a56b-4064-9207-4df0ab0442d6>"],"error":null}
{"question":"I've heard that income affects life expectancy in the US. Has this relationship gotten stronger over time?","answer":"Yes, the relationship between income and life expectancy has strengthened significantly. In 1995, a 10 percent increase in county income was associated with an increase in average life expectancy of three and a half months. By 2010, the same income increase corresponded to a five and a half month gain in life expectancy. Additionally, income explained 14 percent of life expectancy variation across counties in 1995, but by 2010 it explained nearly one-third of that variation.","context":["Maxim Pinkovskiy Health is an integral part of well-being. The United Nations Human Development Index uses life expectancy (together with GDP per capita and literacy) as one of three key indicators of human welfare across the world. In this post, I discuss the state of life expectancy inequality in the United States and examine some of the underlying factors in its evolution over the past several decades. I will describe evidence in the health literature that first, inequality in life expectancy in the United States is increasing, and second, that life expectancy has become increasingly correlated with income. However, I will also argue that the mechanism at work is more complicated than higher income pushing up life expectancy. In particular, I will cite evidence that\nNew York Fed considers the following as important: inequality\nThis could be interesting, too:\nIMFBlog writes Chart of the WeekThe Threat of Inequality of Opportunity\nIMFBlog writes A Map of Inequality in Countries\nIMFBlog writes The Economic Cost of Devaluing “Women’s Work”\nHealth is an integral part of well-being. The United Nations Human Development Index uses life expectancy (together with GDP per capita and literacy) as one of three key indicators of human welfare across the world. In this post, I discuss the state of life expectancy inequality in the United States and examine some of the underlying factors in its evolution over the past several decades.\nI will describe evidence in the health literature that first, inequality in life expectancy in the United States is increasing, and second, that life expectancy has become increasingly correlated with income. However, I will also argue that the mechanism at work is more complicated than higher income pushing up life expectancy. In particular, I will cite evidence that indicators of health care access do not correlate strongly with life expectancy among lower-income individuals. Instead, I will argue that shifts in the incidence of unhealthy behaviors explain much of the increase in life expectancy inequality. Specifically, I will show that the changing distribution of smoking across the income spectrum explains the strengthening of the relationship between life expectancy and income.\nThe chart below plots estimates of average life expectancy by county (from the Institute of Health Metrics and Evaluation, IHME) against log county income per capita from the BEA for 1995 (in blue) and 2010 (orange). Each dot represents a county, and the size of the dot is proportional to the county’s population. We see that over these fifteen years, life expectancy across U.S. counties has increased markedly—the 25th percentile of the 2010 population-weighted county life expectancy distribution is above the 75th percentile of the 1995 one. However, we also see that the 2010 distribution is more unequal than the 1995 distribution: Many counties enjoy much higher life expectancy than the mean, but a swath of counties—the points that still overlap with the 1995 life expectancy distribution—are falling behind. The interquartile range (the difference between the 75th and 25th percentiles, also a measure of variability of the distribution) grew from 2.38 years in 1995 to 2.73 years in 2010.\nMost strikingly, we see that county life expectancy is much more closely related to county personal income in 2010 than it was in 1995. The blue and orange lines through the clouds of points show the lines of best fit between county life expectancy and log county income in 1995 and 2010. It is immediately obvious that the orange line is steeper than the blue line; life expectancy is more correlated with income in 2010 than it was in 1995. A 10 percent increase in county income was associated with an increase in average life expectancy in 1995 of three and a half months—versus an increase of five and a half months in 2010. It is also obvious that the orange dots (for 2010) fall more tightly around the orange line than do the blue dots around the blue line. Log county income explained 14 percent of the overall variability in average life expectancy across counties in 1995, but by 2010, it explained nearly one-third of that variation.\nLooking at the chart, one may be tempted to conclude that the correlation it plots is causal –having a high income is now more important to accessing quality medical care, which in turn prolongs life. However, the story is more complicated. In a landmark 2016 paper in the Journal of the American Medical Association, Raj Chetty and his coauthors investigate the correlates of the life expectancy of the poor across U.S. commuting zones. They find that the biggest determinants of life expectancy among low-income individuals are behavioral variables, such as smoking, obesity, and exercise rates. Surprisingly, measures of access to health care, such as the fraction of poor people who have insurance, or the provision of preventive care, are not correlated with the life expectancy of the poor, and neither are measures of a county’s economic health, such as its unemployment rate.\nAn even starker illustration of the limits of health care goods as a means to a higher life expectancy comes from recent work by Chen, Persson, and Polyakova (2019), who investigate the mortality-to-income gradient in Sweden. Unlike the United States, Sweden has universal health insurance, guaranteeing access to a baseline level of health care services to everyone regardless of their income. However, Chen et al. (2019) show that although Swedish mortality (the probability of dying in the next year at age 59) is lower than U.S. mortality for both the rich and the poor, the slopes of the two gradients are similar. That is, moving up an extra percentile in the Swedish income distribution would yield a very similar mortality reduction as moving up a percentile in the U.S. income distribution. This suggests that access to care is not the issue, since Swedes’ access is generally unrelated to their incomes.\nI conclude by showing that the strengthening of the relationship between county life expectancy and county income in the United States can be statistically explained by a third variable: the smoking rate. To do this, I use data on U.S. smoking prevalence by county from 1995 to 2010 from IHME. (This dataset runs from 1996 to 2012; I use the 1996 numbers for 1995 to match up with the county income dataset). The next chart shows that the relationship between smoking prevalence and income across U.S. counties has become stronger over time, mirroring the relationship between life expectancy and income. Not only did the smoking rate in the United States decline, on average, over these fifteen years, but it declined much more in high-income places than in low-income places.\nFinally, the last chart compares the residuals of life expectancy and income in 1995 and 2010—subtracting the predictions of the values of life expectancy and income based on the smoking prevalence in each year, county by county. Now, the two scatterplots are right on top of each other, and the lines of best fit through them—the relationships between life expectancy and income, controlling for smoking prevalence, in 1995 and in 2010—are nearly identical. In fact, controlling for the changes to smoking prevalence, income explains nearly the same (low) share of the cross-county variation in average life expectancy in 2010 as it did in 1995. The strengthening of the relationship between income and life expectancy that we observed before can be explained entirely by the intensification of the relationship between income and smoking prevalence.\nWhile income now appears to be more salient to health outcomes than it has been in a generation, paradoxically, redistributing income and the health resources that it buys does not seem sufficient to improve health outcomes for individuals in the bottom of the life expectancy distribution. Instead, a more promising approach might center on reducing adverse health behaviors that used to be widespread but are now increasingly concentrated among the poor. Figuring out how to do this will be the key to reducing health inequality.\nMaxim Pinkovskiy is a senior economist in the Federal Reserve Bank of New York’s Research and Statistics Group.\nHow to cite this post:\nMaxim Pinkovskiy, “Does U.S. Health Inequality Reflect Income Inequality—or Something Else?,” Federal Reserve Bank of New York Liberty Street Economics, October 15, 2019, https://libertystreeteconomics.newyorkfed.org/2019/10/does-us-health-inequality-reflect-income-inequalityor-something-else.html.\nThe views expressed in this post are those of the authors and do not necessarily reflect the position of the Federal Reserve Bank of New York or the Federal Reserve System. Any errors or omissions are the responsibility of the authors."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:cf84ab66-3de5-4c74-a474-399b06cb5bf3>"],"error":null}
{"question":"I'm a surrogacy coordinator. Could you explain what the Triple Stripe pattern indicates during monitoring, and how it relates to FET procedures?","answer":"The Triple Stripe is a specific pattern that appears during a surrogate's transvaginal ultrasound in the monitoring phase. It forms when the uterine lining becomes thick enough that the walls touch in the middle, creating an image showing no empty space between the uterine walls. This relates to FET (Frozen Embryo Transfer) procedures, where embryos that were previously frozen can be thawed and implanted using similar methods to fresh egg transfers. The procedures can be performed weeks, months, or years after the initial freezing.","context":["The Jargon of Surrogacy; Part 1\nThe jargon of gestational surrogacy can be a lot to take in. Surrounded by shorthand abbreviations and phrases that you’re not likely to hear at your local coffee shop, it can often be a bit overwhelming. Here’s a glimpse at some of the phrases you may hear leading up to your first transfer.\nIPs– Intended Parents. The term is broad and can apply to two men, two women, a single parent or a heterosexual couple. In any instance, it is usually these people who submit at least half of the genetic makeup of the child to be.\nIF– Intended Father. In situations of a male-male homosexual couple, IF’s is often used in place of IP’s.\nIM– Intended Mother. In some cases this woman may be providing her own egg to be fertilized. If she is unable to do that she will need the help of an ED, or egg donor.\nGS: Gestational Surrogate. A GS is a person who agrees to carry and birth a child who will bear no biological relation to her.\nHSC: Hysteroscopy. This is a medical procedure used to allow doctors to look at the inside of the uterus. A scope with a light is inserted through the cervix and the images are transferred to an external screen. The procedure can be done to diagnose abnormal bleeding, troubleshoot problems with conception or remove polyps inside of the uterus.\nIVF– In Vitro Fertilization. A common method of producing pregnancy in couples who otherwise struggle with infertility. In this method an egg and sperm are manually combined outside of the body. The embryos are later placed inside the uterus to grow.\nPGD– Pre-implantation Genetic Diagnosis. This is a term that applies to genetic testing completed before the embryos are placed inside of the uterus. The tests can often identify genetic disease and disorder prior to implantation, helping to reduce the instance of selective reduction due to these factors.\nPGS– Pre-Implantation Genetic Screening. Also known as Genesis 24. This procedure notes a biopsy taken at day 3 or 5 of embryo growth to screen for potential chromosomal abnormalities. The test can detect the presence of all 24 chromosomes, including those determining the sex of the embryo. The screening can highlight the presence of abnormal amounts of chromosomes that can suggest occurrence of Downs Syndrome.\nICSI– Intra Cytoplasmic Sperm Injection- This is a method of egg fertilization in which a needle is used to inject single sperm into an egg. This method allows for fertilization in circumstances where there may be difficulty collecting large amounts of quality sperm.\nTriple Stripe: The triple stripe is a coveted image that appears on a surrogate’s transvaginal ultrasound during the monitoring phase. The image is created when the lining of the uterus becomes thick enough to touch in the middle, creating the absence of empty space between the uterine walls.\nFET: Frozen Embryo Transfer. Embryos not needed for an immediate transfer may be frozen. Weeks, months or years later they can be thawed and implanted using similar methods to those used in fresh egg transfers."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d9f96270-8114-4b4a-8d9f-587abadef0e6>"],"error":null}
{"question":"How does the mass extinction event at the end of the Ordovician Period compare to the extinction event that killed the dinosaurs in terms of severity and impact on life forms?","answer":"The Late Ordovician mass extinction was actually more severe than the extinction event that killed the dinosaurs. It was the second-biggest mass extinction event in Earth's history, causing the death of 26% of all families and 60% of all genera worldwide. This event occurred due to rapid climate change that lasted less than 2 million years, causing major sea level changes and alterations to ocean chemistry. One notable impact was the first true 'reef gap' in geological history, with coral reefs disappearing for about 6 million years. In comparison, the Cretaceous-Paleogene extinction event that killed the dinosaurs 66 million years ago, while devastating, was less severe in terms of overall species loss. This later extinction was likely caused by an asteroid strike that created a dust cloud blocking the sun, leading to the extinction of all non-avian dinosaurs but allowing birds to survive as the only remaining dinosaur lineage.","context":["|Carn Owen: how the grey\nrocks of a\nMid-Wales hillside record drastic past climate change!\n& Dyfi Valley landscapes Slide-Library - Click HERE\nversion of this post for\npeople familiar with geology and climate science can be found HERE.\ntimes have you read that? It is usually a sign that the person making\nthe statement has run\nout of other arguments with which to attempt to rubbish climate\nSo: what's the answer?\nSure it has! The rocks here in Mid-Wales were formed at\nwhen a remarkable period of climate\nchange occurred - and they record its passing. However, I should add an\nafternote. The change was accompanied by the second biggest\nmass-extinction in the fossil\nrecord of the last 542 million years - from the Cambrian Period until\nthe present day.\nrolling hills of the Cambrian Mountains that form the backbone of\nMid-Wales are made up of grey rocks that on first appearance are rather\nboring - slates, shales and sandstones. It's therefore surprising to\nknow that they record such a drastic event. Before we go into how they\ndo, let's have a quick look at the geological timescale, so that we can\nset the coordinates on our Tardis for a closer look at that time:\nI've abbreviated the Period names because\nthere wasn't enough room! The relevant ones here are:\nORD = Ordovician Period, 488.3-443.7 and SIL =\nSilurian Period, 443.7-416 million years ago.\nWe'll go up into the mountains, heading for the hill of Carn Owen that\noverlooks the Nant-y-moch reservoir. Carn is Welsh for Cairn, and at\nof Carn Owen there is indeed a Bronze-Age ring-cairn, looking\nout over the vast landscape:\nCarn Owen have been heavily quarried in the past, and two of\nthe smaller quarries reveal something rather interesting. The photo\nbelow shows the western one. Straight away,\nthe rocks can be seen to be forming layers, and the layers are all\ntilted at the same angle to the\nright (eastwards). These are sedimentary rocks - they are made up of\nsediment - sand, silt and mud. They were deposited underwater on an\nancient seabed, in the late Ordovician Period, about 444 million years\nBecause one layer of sediment gets\ndeposited on top of another, the higher layers in the photo are younger\nthan the lower layers. And because the rocks are tilted, if in this\nimmediate area you go from left to right (west to east) you will be\nlooking at younger and younger layers of sedimentary rock.\nThe rocks that form the Cambrian Mountains\nmight all look grey from a distance, but when you get close-up to them,\nthey are a bit more interesting! The thick layers in the quarry are\nsandstone and this is what it looks like in person.\nFull of small grains of white quartz, it is pale, hard and splintery.\nsome other places locally, it also contains rounded\nquartz pebbles. Now, in order to carry coarse-grained\nsediments such as sand and pebbles to their resting-place, you need\nstrong currents. Geologists have worked out that the sandstones were\ndeposited by what are called Turbidity Currents - mixtures of water,\nsand and other debris, that poured from the shallower waters of what is\nnow East Wales, down the slopes into the deep-sea basin that occupied\nMid-Wales back then.\nThe next picture shows the second, or eastern quarry face. As I\nexplained above, we have climbed a little higher up through the\nsequence of rock-strata. In fact this small rock-face marks the\nboundary between the Ordovician and Silurian Periods, 443.7 million\nyears ago. The boundary is at a change in the rocks - can you see it?\nIt's right in the middle of\nthe rock-face. To the left are grey rather\nsolid-looking rocks, from the last few thousand years of the Ordovician\nmiddle they are overlain by rather\nflaky, rusty-coloured rocks. The\nrusty-coloured rocks belong to the lowermost Silurian Period.\nHere is a\nclose-up of the rocks at the top of the Ordovician - they really are\ngrey and boring!\nBut here are the rocks from the Lower\nSilurian - far more to see here, just in terms of colour alone. This\nrock is very fine-grained - it is a mudstone. Although dark grey where\nfreshly broken, it has weathered in many places to\ncolour. That is because the mudstone contains a lot of pyrite - iron\nsulphide - that turns to rusty iron oxides when exposed to air and\nmoisture - just like iron does in time. The technical name for this\nrock is \"hemipelagite\" - translated this means \"deep sea mud\".\nThe Lower Silurian rocks also\ncontain a lot of fossils, which makes them even more interesting\ncompared to the Ordovician sandstones. The commonest fossils are\ngraptolites (below). This specimen, with individuals 4-5cm in length,\nshows how they can be beautifully preserved by pyrite that has\ntheir remains. So what were graptolites? They were free-floating\nthat existed, like modern jellyfish or plankton, close to the sea\nsurface, drifting freely with the currents of the seas.\nPyrite is everywhere in these Lower\nSilurian mudstones, as this photo taken down a special microscope\nshows. Here, the pyrite shows up as bright areas, the biggest of which\nis only a millimetre across. For pyrite to be this abundant, the\nenvironment on the sea-bed must have been one where free oxygen was\nscarce or absent, like the bottom of a stagnant pond, full of stinking,\nsulphurous black mud.\nSo, in a\nsequence of rocks across just a short distance on this mid-Wales\nhave seen a change over less than two million years from\nhigh-energy, turbulent conditions, when sands and pebbles were\ndeposited, to a stagnant, deoxygenated undersea\nplain where fine, muddy sediment only accumulated slowly and fossil\nremains were preserved in pyrite. What happened?\nTHE GEOGRAPHY OF THE LATE ORDOVICIAN, 444 MILLION YEARS AGO\nover the past few hundreds of millions of years\nhow, due to movements of the Earth's tectonic plates,\nhave drifted around, collided and split up through time. The globe\nbelow is such a reconstruction: is shows how things\nwould have looked at the end of the Ordovician Period, when the rocks\nof Carn Owen were being deposited. England, Wales and southern Ireland\nwere part of a block of continental crust known to geologists as\nAvalonia, which lay at a low\nlatitude in the Southern Hemisphere and the South Pole was straddled by\nthe large continent of Gondwana.\nZooming in, this is what the area that is now England and Wales would\nhave looked like at the time of the deposition of the sandstones of\nHow do we know? By geological mapping - of the types of rocks and their\nfossils - we can reconstruct what the environment was like at any given\ntime in the past. It gets much harder the further back you go, because\nthe very old rocks of a billion years and more in age generally don't\ncontain fossils and have been so messed-with by heating and pressure.\nBut in the Ordovician and Silurian Periods, we can get a reasonable\npicture of what it was like. 440 million years ago sounds like a long\ntime, but when the Earth is 4,600 million years old, it doesn't sound\nlike so much!\nThe next map is just a few million years before:\nAnd the next one is the same area in the early Silurian. Look how the\nland of the Midland Platform was submerged, then it emerged from the\nsea, then by the early Silurian it was being flooded again. It has been\nestimated that a sea-level fall-and-rise of at least 80 metres took\nplace to bring this about. What caused it?\nThat's a good question. First let's think about the consequences of the\nThis sudden (in geological terms a couple of million years is sudden)\nretreat and readvance of the sea had\nprofound effects. For a start, the retreat caused extensive areas of\nshallow sea, teeming with life, to become dry land. That\nwas one factor in the great\nmass-extinction that accompanied the changes - loss of habitat.\nlike that of today: the\nfirst primitive land-plants only appeared during the Ordovician and it\nwas only later in the Silurian that they really became widespread. As a\nconsequence, with nothing much holding the ground together, loose\nwas readily eroded from the newly-emerged land by rainfall and rivers,\nswept along to the edge of the deeper water and trundled down\ninto its depths by powerful submarine currents, covering the basin\nfloor with mud, silt, sand and, in places, pebbles.\nThus were the sandstones of Carn Owen deposited.\nWhen sea-levels rose again in the early Silurian, flooding back over\nthe land, the erosion of\nsediments by rainfall and rivers stopped and the deepwater area\nbecame isolated from the sediment source, so that only the\nfinest muds settled out over its depths, onto which the remains of the\ngraptolites and other creatures settled out from the near-surface\nhigh above. In this way, the pyrite-rich fossil-bearing mudstones came\nSo what caused this sudden double-flip in sea-levels? We have to look\nat the rocks that were deposited over the great southern continent,\nGLACIERS IN THE SAHARA? THE REMNANTS OF GONDWANA\nGondwana, the huge continent that\nstraddled the South Pole in the late Ordovician, has long since split\nup into fragments that have drifted away in all directions. Bits of it\nnow make up Africa,\nSouth America, India, Antarctica and Australia. In several of these\nmodern continents, late Ordovician rocks occur that have features that\ncan only have had\none origin - from glaciers and ice-caps. Such \"direct indicators\" of\nancient glaciers that are preserved in the geological record include\nthings like striated (scratched) rock surfaces - ice-sheets full\ndebris, like giant sheets of sandpaper, do a good job of grinding down\nthe rocks underneath. The\nstriking image below shows an\nit illustrates an Ordovician\nglacially-striated rock surface in the\nLibyan part of the Sahara Desert!\nThere are also \"indirect indicators\" of ancient glaciation: the\nfall (and subsequent rise) in sea-levels, recorded on Carn Owen by\nchanges in rock-types described above, is to be found in marine rocks\nof the same age all over the world - therefore the sea-level changes\nwere \"eustatic\" - global - in nature. In other words, there was a\ncooling, leading to a major ice-age, with global sea-level\nfalls due to so much water being locked up in ice, followed by a global\nwarming, melting the ice and bringing worldwide sea-levels back up\nagain. How, then, did it happen?\nTHE DRIVERS OF CLIMATE: TECTONICS, ATMOSPHERE AND SUN\nseveral major drivers of global climate that, working together, could\nhave made this happen.\nThe geographical arrangement of the continents\ndue to plate tectonics affects the flow of air in the atmosphere and\nthe currents of water in the oceans, moving warm and cold air and water\nabout. The composition of\nthe atmosphere, in terms of greenhouse gases like carbon dioxide,\naffects its ability to lose heat to outer space or retain it. The\nenergy output from the sun affects the\namount of energy reaching Earth.\nTaking geography first, having the continent of Gondwana over the South\nPole would be advantageous if you want ice-caps to start forming: it\nwould have played a role similar to that of Antarctica today. The poles\nare always the coldest parts of the planet and are even colder if they\nhave continents stuck over them. So that's\none box ticked.\natmospheric composition, carbon dioxide levels\nduring the Ordovician were much higher than today,\nrunning at several thousand parts per million (ppm). Through\nbut very warm climate compared to today.\nIt seems that a fairly stable carbon-cycle\nexisted, with a balance between carbon sources such as volcanoes and\ncarbon sinks such as oceans.\nHowever, in the late Ordovician, the stability seems to have\nbeen disturbed. It is thought that this was because volcanic activity\ndied away, so there was less carbon dioxide being added to the\natmosphere. At the same time, continental collisions led to new\nmountain ranges being formed in places such as what is now eastern\nNorth America, with erosion and weathering of rocks. Rock-weathering is\na tremendous carbon dioxide sink: the gas dissolves in water, which\nthen falls as rain: rainwater carrying dissolved carbon dioxide is\nweakly acidic and reacts with certain common minerals that make up many\nrocks. The result is solutions carrying carbonate in solution plus\nmetals such as calcium and magnesium: in seawater, these drop out of\nsolution to form limestones. So the carbon dioxide gets locked away in\nlarge amounts. There is a lot of evidence for\nexactly this scenario to have happened in the late Ordovician, in\nextensive areas where sediments from such new mountain ranges would\nhave been deposited, the resultant carbon sink causing\ncarbon dioxide levels to drop from about 5000ppm down to 3000ppm or\nstill sounds high but we haven't looked at all the major drivers of\nclimate yet: we still have one more to think about. What about\nThe Sun is a main sequence star - it behaves in a reasonably\npredictable way over thousands of millions of years. Solar\nenergy output is thought to have\nincreased steadily by about 10 per cent per billion\nyears of Earth's history and it\nin the late Ordovician, it would have been 4-5 per cent dimmer\nthan it is today.\nWhat difference would that shortfall in solar energy make? Global\nclimate models tell us that, at the moment, polar ice can persist\nwhen carbon dioxide\nlevels drop below 500 ppm (of course, they are below that at the moment\nand have been for 25 million years or so).\nThe same models, adjusted for factors like\nthe the dimmer sun of the late Ordovician, predict that for glaciers to\nhave formed at the poles back then, carbon dioxide levels would need to\nbe below 2240-3920 ppm. So: combine a\ndimmer sun, a\nsudden drop in carbon dioxide levels and a large continent over the\nSouth Pole and\nthat appears to have been enough to get the temperature down and allow\nglaciers to develop, due to several major\nclimate drivers working together.\nRAPID ENVIRONMENTAL CHANGE AND MASS-EXTINCTION\nAccording to the fossil record, during this late Ordovician ice-age,\n26 per cent of all\nfamilies and 60 percent of all genera of life\nworldwide died out.\nsecond-biggest such mass-extinction event in the fossil record - there\nbeing five really big ones, including the K-T event that killed off the\ndinosaurs and the end-Permian or P-T event that was the biggest of the\nlot, when life on Earth was almost totally wiped out. The diagram below\nshows how the Late Ordovician event sits with the others.\nWe know that\nthe late Ordovician mass-extinction accompanied a period of rapid\nclimate change - a significant cooling that lasted less than 2 million\nyears before a return to very warm conditions. That caused major\nin sea level and changes to\nocean temperatures and chemistry. Oceanic changes are blamed for\nthe loss of coral-reefs: although a few corals\nsurvived, living reefs themselves disappeared from the face of the\nEarth and it took as much as 6 million years for them to reappear,\nmaking the event the ﬁrst true ‘reef gap’ in the geological record.\nThere is an important lesson to be learned in this story. That is that\nany major and geologically rapid climate shift in either\ndirection - warmer or cooler - from a stable state brings with it\ndrastic environmental changes that can have severe effects on\necosystems and create the danger of extinction\nevents. These may occur due to actual habitat-loss, such as the\ndraining of the Ordovician shelf-seas, or due to changes in, for\nexample, oceanic chemistry that are too rapid for evolution to adapt\nto. That's an important point: life can thrive in both very warm\n(\"Hothouse\") and very cold (\"Icehouse\") climates. It is when the rate\nof transition from one to another is geologically too rapid for\nadaptation that the big problems occur.\nOur burning of the fossil\nfuels on such a massive scale over just a couple of centuries risks\nbringing about such changes unless we change the way we obtain and\nconsume energy, by moving more and more to renewable energy sources and\nin increasing the efficiency and reducing the wastage that is currently\npresent in our energy-use. The \"end-game\" in terms of Earth's\ntemperature is less relevant: it is the speed at which we get there\nthat really matters. Several degrees in a couple of centuries: this is\na serious possibility if we carry on as usual. In geological terms this\nis lightning-like in its speed. It leaves the Ordovician changes stuck\nbehind, still in the starting-blocks - and remember what happened back\nthen. Nobody wants to bring about the sixth mass-extinction, but unless\nwe find a way through this minefield, that scenario will not go away\nIt's surprising what the geology of a couple of small, grey crags on a\nMid-Wales hillside can tell us!\nBACK TO WEATHER-BLOG MENU\nWelsh Weather & Dyfi Valley landscapes Slide-Library - Click HERE","When were dinosaurs alive? On this page you’ll learn about the dinosaur periods of the Mesozoic Era – the ‘Age of Reptiles’.\nWe’ll look at each of the three ‘dinosaur periods’ – the Triassic, Jurassic and Cretaceous – and find out what life on Earth was like in those times.\n- You can find out more about dinosaurs at our main Dinosaur Facts page.\nWhen Were Dinosaurs Alive?\nDinosaurs were alive in the Triassic, Jurassic and Cretaceous periods. Together, these three periods make up the Mesozoic Era. Due to the dominance of the dinosaurs and other reptiles during the Mesozoic Era, it has become known as the ‘Age of Reptiles’.\nMesozoic Era (252.17 to 66 million years ago)\n- Triassic Period (252.17 to 201.3 million years ago): Dinosaurs begin to appear, having evolved from reptiles called Archosaurs.\n- Jurassic Period (201.3 – 145 million years ago): Dinosaurs become the dominant land vertebrates.\n- Cretaceous Period (145 – 66 million years ago): Dinosaurs continue to thrive and diversify.\n- Cretaceous–Paleogene Extinction Event (around 66 million years ago): The dinosaurs are wiped out; only avian dinosaurs (birds) survive.\nFurther down the page you’ll find information on each of the three periods of the Mesozoic Era, including examples of the dinosaurs that existed during each period.\nMesozoic Era: ‘Age of Reptiles’\nThe Mesozoic Era lasted for around 186 million years. It began around 252.17 million years ago, and ended around 66 million years ago. Dinosaurs started to appear around 243 to 231 million years ago, during the Triassic Period.\nDinosaurs are among the most successful groups of animals ever to have lived. They walked the earth for around 170 million years. (Bear in mind that humans have only existed for 2 million years!)\nHowever, the dinosaurs’ successful reign came to an abrupt end.\nDinosaur Periods: Extinction\nThe Mesozoic Era ended with the ‘Cretaceous–Paleogene Extinction Event’, which wiped out the dinosaurs and many other species.\n(An extinction event is a sudden and wide-ranging decrease in the amount of life on Earth.)\nDivisions Of The Mesozoic Era\nAs we’ve seen, the Mesozoic Era is subdivided into three periods: the Triassic, Jurassic and Cretaceous periods. These are ‘geological periods’ of time: each corresponds to a particular layer of rock.\nThe scientific body responsible for measuring the geological timescale is the International Commission on Stratigraphy.\nIn the geological timescale, each period is further divided into ‘epochs’. The Triassic and Jurassic periods are divided into three epochs, the Cretaceous into two.\nMesozoic Era (252.17 to 66 million years ago)\n- Triassic Period (252.17 to 201.3 million years ago)\n- Early Triassic Epoch (252.17 – 247.2 million years ago)\n- Middle Triassic Epoch (247.2 – 237 million years ago)\n- Late Triassic Epoch (237 – 201.3 million years ago)\n- Jurassic Period (201.3 – 145 million years ago)\n- Early Jurassic Epoch (201.3 to 174.1 million years ago)\n- Middle Jurassic Epoch (174.1 to 163.5 million years ago)\n- Late Jurassic Epoch (163.5 to 145 million years ago)\n- Cretaceous Period (145 – 66 million years ago)\n- Early Cretaceous Epoch (145 to 100.5 million years ago)\n- Late Cretaceous Epoch (100.5 to 66 million years ago)\nThe Age of the Dinosaurs\nLet’s find out what the world was like during the three ‘dinosaur periods’ of the Mesozoic Era …\n1: Triassic Period (252.17 to 201.3 million years ago)\nThe Eoraptor is one of the first dinosaurs. It appeared during the Triassic Period.\nThe Triassic Period followed the worst extinction event that the world has ever experienced. (Yes, it was even worse than the later extinction event that would wipe out the dinsoaurs.)\nThe Permian-Triassic Extinction Event, or the ‘Great Dying’ as it is otherwise known, caused between 90% and 96% of all species on Earth to became extinct.\nAt the beginning of the Triassic Period, life on Earth struggled to rebuild itself. At first, the mammal-like reptiles, successful before the extinction event, started to dominate once again.\nMammal-like animals such as the Lystrosaurus – a strange, dog-sized animal with tusks – thrived in the Early Triassic. Mammals themselves would appear towards the end of the Triassic Period.\nHowever, it was another group of animals that would win the race to supremacy: the Archosaurs. Through the course of the Triassic Period, these reptiles became the dominant land vertebrates (animals with backbones).\nAs the Triassic Period progressed, the archosaurs split into two main branches. One of these was the Pseudosuchia, which includes the Crocodilians, and similar animals. The other branch was the Avemetatarsalia, which itself split into Pterosaurs and Dinosaurs.\nThe first dinosaurs were, small, walked on two legs, and ate meat.\nIn the Triassic period, all of the world’s land was joined together in one vast supercontinent called ‘Pangaea’, which means ‘whole Earth’. The climate was hot and dry. The interior of Pangaea was a vast desert.\nPlants such as conifers and ginkgoes flourished. There was no grass, and there were no flowering plants.\nWhen Were Dinosaurs Alive: Triassic Period Dinosaurs\nOne of the earliest dinosaurs was the Eoraptor, a small predator that lived around 228 million years ago (see picture, above). Found in north-west Argentina, it walked on two legs and stood knee-high to a human.\nEoraptor looked like a ‘typical’ dinosaur, with a long tail, long neck and short arms.\nSaurischia and Ornithischia\nEoraptor was a saurischian, one of the two main types of dinosaur. All of the meat-eating dinosuars were saurichians, as were many of the large, four-legged plant eaters.\nThe other main type of dinosaur was ornithischia. The Ornithischians were all herbivores (plant-eaters)\nOne of the earliest known Ornithischians is the Pisanosaurus. This was a small dinosaur that walked on two legs, and outwardly looked much like early bipedal (2-legged) saurichians.\nWhen Were Dinosaurs Alive: Triassic Period Dinosaurs\n2: Jurassic Period (201.3 – 145 million years ago)\nThe Jurassic Period began with another extinction event – the Triassic-Jurassic Extinction Event. While not quite as devastating as the earlier Permian-Triassic Extinction Event, it still led to a substantial decline in both sea and land species.\nThe extinction set the scene for dinosaurs – who were already flourishing by the end of the Triassic – to completely dominate the land.\nPangaea began to break apart in the Jurassic Period, forming two landmasses, Laurasia in the north, and Gondwana in the south. As the continents separated, the climate became wetter and more humid.\nConifer forests covered much of the land, especially near the poles. Ginkgoes, palms, tree-ferns and horse-tails were also present. The first birds appeared. Insects and amphibians continued to develop. Mammals evolved fur coats and started giving birth to live young.\nHuge sauropods such as Brachiosaurus walked the land, feeding on the flourishing vegetation. Large predators, such as Allosaurus also made an appearance.\nWhen Were Dinosaurs Alive: Jurassic Period Dinosaurs\nExamples of Early Jurassic Dinosaurs\nExamples of Mid Jurassic Dinosaurs\nExamples of Late Jurassic Dinosaurs\n3. Cretaceous Period (145 – 66 million years ago)\nDuring the Cretaceous Period, the continents continued to separate. Gondwana broke up to form South America and Africa, and Antarctica and Australia also drifted away. Laurasia began to split into North America, Europe and Asia. The world – as we know it today – began to take shape.\nThe climate became wetter, and the newly formed oceans offered new habitats.\nFlowering plants appeared in Laurasia at the beginning of the Cretaceous Period. By the end of the Cretaceous they were the Earth’s dominant plant type. Oak, maple and beech trees were all present. Insects evolved alongside the new plants.\nWhen Were Dinosaurs Alive: Cretaceous Period Dinosaurs\nAlthough some types of dinosaur, such as the Stegosaurs, died out during the Cretaceous Period, ever-more varied types evolved to take their places.\nDuck-billed dinosaurs such as Iguanodon, armoured dinosaurs such as Polacanthus, and the Ceratopsids, including Triceratops, all thrived during the Cretaceous Period.\nCretaceous–Paleogene Extinction Event\nMany of these dinosaurs existed right up to the end of the Cretaceous Period. However, 66 million years ago, the Earth was rocked by a catastrophic event that wiped out over 75% of all living species.\nThis was the Cretaceous–Paleogene Extinction Event, and it brought the whole Mesozoic Era to a close. At the dawn of the Cenozoic Era, the only dinosaurs still alive were the birds.\nAll of the non-avian dinosaurs had become extinct.\nScientists believe that the Cretaceous–Paleogene Extinction Event (which is also known as the K–Pg extinction event, or Cretaceous–Tertiary (K–T) extinction event) was caused by a large asteroid striking the Earth.\nThe explosion would have thrown up a dust cloud big enough to hide the sun for months, or even years. Plants would have been unable to grow in the darkness, and many of the Earth’s animals – including the dinosaurs – would have been unable to find enough food to survive.\nThe age of the dinosaurs had come to an end.\nExamples of Early Cretaceous Dinosaurs\nExamples of Late Cretaceous Dinosaurs\nDinosaur Periods: When Were Dinosaurs Alive – Conclusion\nThe Mesozoic Era is subdivided into three periods: the Triassic, Jurassic and Cretaceous periods.\nDinosaurs first appeared towards the end of the Triassic Period. Following the Triassic-Jurassic extinction event, the dinosaurs quickly became the dominant land animals on Earth. They would remain so throughout the Jurassic and Cretaceous Periods.\nThe dinosaurs became increasingly diverse throughout the Cretaceous Period, and thrived right up to the Cretaceous–Paleogene Extinction Event.\nThis extinction event, believed to have been caused by a large asteroid striking the Earth, wiped out all of the (non-avian) dinosaurs. Only avian dinosaurs – or birds – survived. (Many scientists today believe that birds are actually dinosaurs, meaning that birds are the only dinosaurs to have survived the extinction event).\nWe hope that you’ve enjoyed reading about the three ‘dinosaur periods’ of the Mesozoic Era.\nOther Relevant Pages on Active Wild\nYou can find more dinosaur information on our main Dinosaur Facts page, or check out these pages for in-depth dino facts:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:026c345f-9ec8-4304-ac3a-c02c3ac5659a>","<urn:uuid:9a6e8fa5-cab4-43f4-9eb6-c7dda4ff9d92>"],"error":null}
{"question":"As a physics student, can you explain how magnetic domains behave differently at high versus low temperatures?","answer":"At high temperatures, magnetic domains move around randomly and point in various directions, making the overall substance appear non-magnetic. However, when the temperature drops, there's a sudden transition where all domains align in a single direction. This alignment happens almost instantly, not one-by-one, as this represents the most stable state, similar to how sugar grains settle in a jar.","context":["In the 1960s the tide in physics flowed towards tackling the strong interactions, as those grappling with the weak interactions had come up against an apparently unshiftable block.\nThe technique that seemed to be the most powerful one was the use of symmetry, and the use of Lie groups enabled a classification to be made that brought the weak interaction into a possible unification with the electromagnetic one. That was truly amazing. The long-distance electromagnetic field that gives us light and radio waves was somehow linked at a deep level with the weak interaction which we only know through its by-products in processes at the quantum level like radioactive decay.\nA second application of the treatment involved another type of symmetry, gauge symmetry, which required the existence of a new type of field, the Yang-Mills-Shaw field, which brought the weak interactions even closer, by giving them a new particle of their own to complement the electromagnetic one (the photon).\nBut – damn and blast – the predicted ‘weak photon’ was massless, which could not be the case in reality. And – damn and blast three times over – Goldstone’s theorem showed that there was no way in which you could generate a mass-possessing weak particle without destroying the symmetry completely.\nFor those few people who refused to abandon the problem, if they had any request from Father Christmas it would be for a symmetry-breaking kit – something that would break the symmetry while preserving it.\nThe situation required the kind of ingenuity of an Orcadian student from Birsay who had happily stuck photos of his native islands on the wall of his Edinburgh digs with Copydex, which at the time was regarded as proof against just about anything – a problem which his landlady encountered when she decided that the pictures infringed the rules of the house. She tried to haul them off the wall and took off a quantity of wallpaper with them, and irately confronted him for his misdeeds. Without the slightest appearance of a pause for thought, he said with a kind of calm authority: ‘Oh, what a pity that you didn’t ask me for the Copydex remover.’\nSomething this was needed with the electroweak interaction – to remove some of the picture-equivalents off the wall without at the same time taking the wallpaper with them.\nThe equivalent of the Copydex remover came indeed not from looking at a wall but from things that are much bigger and solider than the world of very small particles. There were three situations in solid state physics that produced the clues.\nSituation number 1\nWhen atoms join up to form a crystal, their outer electrons loosen and begin to drift. Having lost these outer electrons, the previously electrically-neutral atoms have a net positive charge. The electrons stay in the crystal, not attached to any particular atom, and an electric field can start them moving in a particular direction – this is what forms an electric current. However, the effect of the positive lattice with the little pulls of the various atoms in the structure can be to slow down the electrons – and make them appear to be more sluggish and massive than they would otherwise have been.\nSo one way for a particle to behave as if it has more mass is to put it in some kind of external lattice like this.\nSituation number 2\nA magnetic substance is formed out of many little units which are themselves magnetic – they are called domains. When the substance is very hot, the domains will move around and point in a random mix of ways and from the outside we would never know that there was the potential for magnetism there. But if we let the temperature drop, there comes a point where suddenly the domains click together into pointing in a single direction. Somehow each lines up with its neighbour, but not in a one-by-one fashion but all an overall ‘decision’. We have no way of knowing in advance which direction they will happen to lie along.\nThe reason why they line up in this way is because this is the most stable form they can take. It is as if they have dropped down to the lowest point they can reach. It is a bit analogous to us filling a jar with sugar and tapping it from time to time – to see the level sink as the sugar grains start to fit together better and better. In the case of the magnetic substance, there is no tapping process, and the alignment happens almost in an instant. But there is a parallel in that in each case the little units – whether domains or grains – ‘find’ that by aligning themselves closely a stable ground-state is reached.\nThe magnetic substance when it was hot was very symmetric in that we could turn it around in as many ways as we would like, and in terms of magnetism each direction looked the same. But after we cooled it, we had this sudden transition of the domains into an overall direction. So the individual units still retained their own internal symmetry while the collective result was to make a break.\nSituation number 3\nThis is in a very strange world, the interior of a superconductor. Here electrons can travel through the crystal lattice so smoothly that nothing slows them down in any way and they can travel forever. Here something has happened which is a kind of parallel to the magnet in that there has been a process of internal ordering. The electrons have a kind of overall link between them that enables them to get through the lattice without bumping into any of the lattice atoms.\nIt is a bit like dancing the conga. If a group of people are in a crowded room and all try to make for the door on their own, their will be a lot of bumping into others and an ever-slowing rate of progress, but if they line up in conga formation they can negotiate their way round in a coordinated way and avoid touching anyone.\nWhere does this get us? Well, from situation 1 we see that some kind of background lattice can cause particles within it to slow down and behave as if they have become more massive. So maybe we need to find some kind of background lattice encompassing the whole of space, so that it can somehow interact with the particles we have been studying and cause them to behave ‘massively’. We need in fact a background lattice that consists purely of forces – and that is just a definition of something that we have already become familiar with, a field.\nSo we need an additional field to interact with the field that we already have, in particular the Yang-Mills field.\nBut how will this field get round the constraints of Goldstone’s theorem? – which the Yang-Mills field on its own could not get beyond.\nHere is where situations 2 and 3 come in. This new background field has to have the various symmetries, otherwise we have broken them completely, but since it is a brand-new field with only one task, to help us out of our problem, we do not need to allow it to have all the myriad of options that the more familiar fields do. Indeed, it only requires to have one option, just like the magnetic bar does, and just like the electrons in the superconductor do. There is one single position that the choice of options locks in to – so to that extent it has broken the symmetry – but the break is not so bad that we have lost the symmetry completely.\nIn 1963 Philip Anderson looked closely at what happens inside a superconductor and said that there was something important for particle physicists to be aware of. Anderson was a specialist in the solid state, and went on to share the Nobel Prize in 1977 for his work on the electron interactions in phenomena like magnetism and superconductivity. His studies at Harvard had been interrupted by the war, in which he had been deployed in research on building radio antennas, and after going back to Harvard to complete his training (and where his fellow-students included Tom Lehrer) he went to work at Bell Laboratories, joining a glittering array of researchers there. He found that the sophisticated mathematical techniques of quantum field theory that he had learned from Schwinger at Harvard had practical applications in the advanced radio work. In his background there are professors on both sides of the family, together with a love of the outdoors, and from a sabbatical year in Japan there is a deep interest in Japanese culture and he is a first-degree master of the game of Go.\nThe paper that Philip Anderson wrote in 1963 showed that in a superconductor the ordered cloud of electrons acts like a single quantum system which can be energised, and that the bursts of energy look like particles. The presence of an electromagnetic field affects these excitations in a way that makes them behave as if they had acquired a mass. So, he noted, symmetry was being broken but yet mass was being generated, and so the blocking effect of Goldstone’s theorem had been overcome. The Goldstone difficulty was, he said – ‘not a serious one’. It was this paper that opened the way for the breakthrough of Peter Higgs."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:0f06a0f6-4911-4d04-897e-d5c80bfb89f1>"],"error":null}
{"question":"How can I modify the speed of an effect in EOS without creating a new effect?","answer":"You can modify the speed of an effect using the Rate parameter in the Effect Status Display. A rate of 100 means it's playing at the original speed. For a speed four times faster, enter a Rate of 400. For half the original speed, enter a Rate of 50. You can adjust this either by using the encoder mapped to Rate or, if using Nomad, by clicking on the rate value, entering a number, and pressing Enter.","context":["Over the course of this series, we’ve walked through the how the effects engine is broken into different Types, the parameters of control common to all effects, and then each Type of the engine and how they operate. One would think you were done, but there’s always more. The last thing I will be covering in this series is the Effect Status Display.\nThe Effect Status Display is both a place to see a list of every effect currently running, but also a place where you can alter your effects on a cue-by-cue basis. Let’s get into it. Open Mind the Gap vis file and for the show file, open the version of EOS Family Learning Series Level 4 Complete that you’ve been adding your effects as you did blogs 1 through 8.\nIn Live, let’s get some effects running.\n[Group] 5 [Full] [Full]\n[Focus Palette] 2 [Enter]\n[Effect] 31 [Enter]\n[Group] 22 [Full] [Full]\n[Effect] 21 [Enter]\n[Group] 6 [at] [Full] [Enter]\n[Effect] 41 [Enter]\nTake a look at your stage just so you know what is going on. When you’re ready, clear your command line by pressing [Clear] once, and the press [Effect] just once. Behold, the Effect Status Display!\nLet’s start with what you are looking at. You see the three effects currently running listed and they are in Red. This color of course is showing us these are Manual values, meaning not recorded anywhere. The effects follow the same color rules as cues to indicate a new value, a tracked effect, etc. When I look at the columns, I see two sections. The left side displays what is running and from where. The right side offers things you can customize to record in your next cue or sub.\nEffect- Simply, the number of the effects running.\nSource- Where the currently running effects are from. Right now they are manual because they are in the programmer, but if you recorded a cue or submaster, it would list the number for it.\nPart- If you placed the effect in a part within a cue, it would list that here.\nChannels- Which channels are being affected by each effect.\nThis area is incredibly helpful. If I may become “that old programmer” for a moment, in the past, if you wanted your effect to be a little faster for this cue, you had to create a new effect, alter the tempo and then apply it. Same with if the effect was a little too big or maybe a little too deep or wide on the stage in. Now you can alter these things on the fly as the need comes up without needing to create more and more effects. Let’s step through these options.\nDelay- if you needed to add a delay to the start of the effect without affecting anything else in the cue, you could add that here.\nRate- if you need to speed up or slow down any effect, you can alter that here temporarily. Very handy. Remember that a rate of 100 means that it is playing at the original speed. If you wanted it four times faster, you would enter a Rate of 400. If you wanted it half of the speed of the original, you would enter a Rate of 50. Take a moment to run your encoder mapped to Rate up and down to see how this functions. If you are on nomad, click on the value of rate for the effect you want to change, enter your number and press [Enter].\nBPM- Short for Beats Per Minute, this is the speed used to describe the tempo of music. If you know what the BPM is, you can manually enter it here.\nSize- A way to proportionally increase or decrease the size of an effect. If you wanted the circle effect to be smaller, you would select a Size of 75, for example, and the circle would be tighter around the singer at center. Go ahead and run the Size encoder up and down, or manually enter different values by touching the cell, typing and value and pressing [Enter].\nHForm- a Focus-only parameter, this will alter the wave form of a focus effect. If you wanted the circle effect that is running to be wider, you could enter an HForm value of 125, for instance. One encoder is mapped to HForm.\nVForm- a favorite of mine, for when the effect you otherwise love is hitting the cyc too much, or perhaps the audience too much. Decrease or increase this value to compress or stretch the Focus effect running. If you hold down [Shift], the encoder mapped to HForm switches to VForm.\nAxis- working often with VForm or HForm, though not always, this allows you to rotate the wave form on its axis. Your last encoder is mapped to Axis.\nOne more thing to point out. Between the areas we have spoken of, right above the encoders, there is a slim area that gives you access to edit the effect without exiting to Effect Blind.\nPlease note that changes made here will edit the effect itself, so any cue that uses the effect will be changed. Accessing these controls is easy- simply click in the cell that you want to alter.\nEOS lays out great tools to create, track and modify effects. Hopefully you feel more confident in your ability to receive a creative impulse from your designer or yourself, decide which part of the engine will do it the best and get it up and running quickly. Is there an effect you were always confused by that you either solved or are dying to dig into? Tell us about it below."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:bbd024e1-c92c-403c-83e1-361875fb6d31>"],"error":null}
{"question":"What are the three different types of Pull connections and how do they differ from each other?","answer":"The three types of Pull connections are Continuous Flow, Sequential Pull, and Replenishment Pull. Continuous Flow has the highest level of Pull, featuring one-piece inventory between workstations, fixed production sequence, and no waiting inventory. Sequential Pull allows a defined maximum buffer between workstations while maintaining fixed quantity and sequence, typically using FIFO lanes. Replenishment Pull (supermarket) has a maximum number of waiting products but no fixed sequence of which product will be pulled next, and can be controlled using Kanban.","context":["A Pull process is a process in which a workstation starts to work on his next order only when there is a free slot on the output side. This means the trigger for producing anything on the workstation comes from the customer side, which can be internal as well as external. The customer pulls orders through the process instead of a traditional Push connection in which products are produced no matter what happens on the output side of the workstation.\nBecause inventories between workstations are managed in a pull system, which directly influences and maximizes the Lead Time, Pull always has a preference over Push.\nThree different forms of Pull connections can be identified: a Work cell with Continuous Flow, Sequential Pull and Replenishment Pull.\nThe THREE PULL CONNECTIONS help to achieve different levels of Pull in the system, based on three factors: having a maximum of one piece of inventory between two process steps (1), having a fixed production sequence (2), and having a maximum number of parts waiting (3).\nThe Continuous flow connection has the highest level of Pull, since all three factors are included. Products are worked on one by one and with a maximum inventory of one between the workstations, a workstation can only work one the product that is waiting in front of it at that time (the fixed sequence), and can only produce when the inventory behind it is less than one (no inventory). multiple workstations with continuous flow between them are also known as Work cells or U-cell.\nThe Sequential Pull connection is the second best possible Pull connection, in which the fixed quantity is determined, the sequence of product is defined, but a buffer with a defined maximum is allowed between workstations to buffer for variance. This is usually implemented using First-In-First-Out lanes (FIFO) .\nReplenishment pull, the supermarket, is the third and last option, in which a maximum number of products is waiting to be worked on, but it is unknown which type of product will be pulled out next. This type of inventory is also known as a supermarket and can be controlled using Kanban.\nThe differences between the three types of pull connections can be summarized in the following table:\nIllustration 1: Difference between Continuous Flow, Sequenced Pull and Replenishment Pull\nCHOSING THE RIGHT TYPE OF PULL CONNECTION is one of the steps in designing a Future State VSM. This decision depends on a few process- and product variables. Illustration 2 shows a discussion table with variables which might influence the decision for a type of connection. In the discussion it is advised to follow the topics from left to right, and move from top to bottom. The goal of the table is not to discourage the use of a certain Pull system, but to give direction in pointing out what variables need to be worked on to be able to implement a higher level of pull within a specific situation.\nIllustration 2: Discussion table, when to use what type of connection?\nProcess reliability is the first factor that influences the type of Pull connection which can be used. When the reliability of the output of the upstream workstation is low, a buffer is needed to prevent the entire flow from stopping. Since in this case all products need to be buffered, the sequence cannot be held anymore which results in the Supermarket being the only viable option at this point.\nChange-Over-Time is the second variable. In situations where more than one product is produced within a family, short change-over-times are needed to produce in sequence and per order on a workstation. Whenever Change overs are high, it would make more sense to batch products, which means a buffer is needed in front of the workstation. This buffer would be a supermarket, because the products are sorted by type, and therefore sequence cannot be kept.\nLead times also influence the type of Pull connection can be used. Continuous Flow can only be kept if the upstream process can respond to downstream changes. When the Lead time is higher than the cycle time of the machine, a buffer is needed to prevent starvation. The same holds for Takt Time. Only when the upstream machines can deliver on all Takt times (which can vary, for instance with seasonal products), continuous flow can be implemented. When some Takt levels cannot be achieved upstream, a buffer is needed.\nThe fourth variable is variation in demand of products, which limits the options for a pull connection the other way around. The higher the demand variation or product mix is the less desirable a supermarket is, because the number of buffers and the number of products per buffer increase. This variation should be minimized using Heijunka.\nThis also holds for the price of the part or product, the fifth factor. The higher the part costs, the less desirable the Supermarket option becomes. This is a tough one, because it might be necessary to have a supermarket to keep the pull system running even though the costs are high.\nThe interesting thing about these five factors is that there are situations where an unstructured Push connection might be the only option left and having Push connections in a Future State Value Stream Map is really not desirable. In these situations where Push seems to be the only option, kaizen events need to be defined to change on or more of the above five factors to make it possible to implement on of the three Pull connections.\nThe table shown in illustration 2 is meant for the purpose of discussion only. It is not black and white and does not include all possible factors that might influence the choice of connection type. It is the task of the Team that develops the Future State Value Stream Map to decide what connection type is suitable for each specific situation."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:6b0c426a-91a2-4833-8819-aafde127a636>"],"error":null}
{"question":"What role do variables play in mathematical expressions, and how do these concepts relate to monitoring environmental impacts in digital systems?","answer":"In algebra, variables represent unknown quantities that haven't been counted yet, commonly denoted as 'x', and are used to write expressions where the variable appears on one side of an equals sign with a number on the other side. Variables can also represent sets of numbers. In the context of environmental monitoring, distributed ledger technologies (DLT) use similar mathematical concepts to track and verify data. The Environmental Protection Agency and Department of Energy recommend collecting and analyzing variables such as mining energy usage, fuel mix, and environmental justice implications in a privacy-preserving manner to enable evidence-based decisions on energy and climate implications of digital systems.","context":["- 1 Introduction\n- 2 Neolithic Algebra\n- 3 Using A Variable\n- 4 Simplifying Equations\n- 5 Manipulation of Radicals\n- 6 Basic Laws In Algebra\n- 7 Proportions or Ratios\n- 8 Solving Equations\n- 9 Lesson Review\n- 10 Lesson Quiz\nWe've seen that an equation is like a balance. When two quantities are on either side of an equal sign in a mathematical statement, we are saying that the statement is only true under conditions that make the quantities the same. For instance, when we see the statement we now know that we are asking what number can we substitute for in the equation to make this statement true? One way you could work this out is by trying out different values of until you get one that works. This is called guess-and-check. Alternatively you might know the answer intuitively (by thinking What do I need to add to 2 to get 3?).\nHowever, if you have a more complicated problem such as you are likely to have trouble solving this problem intuitively or by guess-and-check. Because of this, mathematicians worked out a technique to solve this type of problem easily. This technique is the fundamental basis of algebra.\nSince the equal sign means that both sides of the equation are the same (Same value, different appearance), and that if you manipulate (using addition, multiplication, etc.) the values on both sides of the equal sign in the same way, then they will still be equal. This is also how a mechanical balance works. As long as you do the same thing to whatever is in both pans they stay level. If we have a value of 3 = in one pan and two things with the value of 2 and 1 in the other pan the balance is level. If we multiply both sides by 5 we get,\nNotice how the equality still holds.\nNeolithic Functions Imagine you are the neolithic shepherd. What changes can you make to your bag of rocks?\n- Addition - When new sheep are added to your flock you would use addition. For instance in the spring you would have a \"lamb born\" function in which you add a rock to your bag. You might also have a \"multiple birth\" function in which you invoke the lamb born function once for each lamb born.\n- Subtraction - When you decide that a sheep is no longer part of your flock you would use a \"remove sheep function\" to keep your bag of rocks in balance with your herd. You might do different things with the rock for a missing sheep, a sheep you traded for something you needed, or because it became dinner one night. You might anticipate using these functions to manage your herd. If you know that you need to remove five rocks from your bag in the fall to buy firewood you might examine your bag carefully to know how many times you can invoke the \"sheep dinner\" function over the summer.\n- Multiplication - Imagine you are a shepherd just starting out. If your neighbors had a big enough flock they might promise you a number of sheep per month as payment for watching their flock. For instance if they promised you two sheep a month for tending their flock then you would know that after 1 month you would have two sheep, after two months you would have four sheep, and after an entire year you might have enough sheep to start your own operation.\n- Division - It is said that two things are inevitable: death and taxes. Division is the operation we use to make the inevitable fair. When a shepherd died his heirs could use division to divide up his flock fairly. They might use a function that allocated one sheep per heir until the flock was distributed. In what ways could the heirs distribute extra sheep if the flock was not evenly divisible? Similarly, when collecting taxes it is more fair to take a portion of the flock rather than a single amount. The clan leader would be better served taking 1/12th of a flock from each shepherd every year. If the clan leader asked for a specific number of sheep every year they might find that their clan became smaller every year just before tax time as new shepherds took their sheep to a new clan.\nVideo Games have an attribute called game play. Look at a video game you enjoy playing and try to define how you interact with the environment using addition, subtraction, multiplication, and division.\nUsing A Variable\nWith algebra we use variables to represent things we haven't been able to count. Since we can't count the number the variable represents the number is called an \"unknown\". The most common variable is denoted . To use a variable we write an expression which we know is true and has on its own on one side of an equals sign and a number on the other side of the equals sign.\nSometimes a variable can represent a set of numbers. In this case the number can be represented with set notation. We generally use a letter that reminds us what the variable represents.\nHere are some examples of manipulating equations to get the on its own,\nExample 1: How many dollars do I need to see a movie and buy popcorn? If we let equal the dollars that I need to get into the theater, then we could use the variable set to represent a discount admission, a matinee admission, a regular admission, or an Imax admission. If we assume that popcorn costs 3 dollars at all the theaters we will go to then we can write the equation to represent the money we need. We could add an A to both sides to get the equation . Plugging the values for A into this equation we find that .\nExample 2: What is the value of in ?\nSolution: We can do the same in this case by dividing it by 2 (as is (remember that is the same as 1 because it has implied coefficient of 1)), but again to keep both sides of the equation equal we'll need to divide the other side by 2 as well to get,\nExample 3: What is the value of in\nSolution: Here we first need to subtract 1 from both sides,\nThen we divide both sides by 3 to get,\nAlthough in this case we chose to do the subtraction first and then the division, we could have done it the other way around, doing the division first followed by the subtraction, as follows,\nWhen working problems it often helps to do additions/subtractions first and then multiplication/divisions, as this lets us avoid having to add or subtract fractions. However, both ways are equally valid.\nSometimes you'll come across equations which have a variable on both sides, for instance , where x can be found on both sides of the equation.\nWe solve this type of equation in much the same way as we've solved the previous problems, but only this time you have to first make sure all of the variables are on the same side. The easiest way to see how to do this is by example:\nExample 1: How do you find the value of in the equation, ?\nFirst of all you need to choose which side you want the variable to be on, the left or the right of the equals sign, in this case we'll choose to have the on the left hand side.\nTo do this we first have to look at where occurs on the right hand side; in this case it only appears in the term . As we don't want on the right hand side we need to get rid of it, and we can do this by subtracting from the right side. Remember that for the equality to still be accurate we need to do the same on the left side as well.\n- (subtracting 2x from both sides)\nNow the equation is in a form which you are familiar with from the last chapter so hopefully you should now be able to solve this problem and get the answer .\nManipulation of Radicals\nLet's say that we have a number . The square root of is the number that, if multiplied by itself, equals . Since there are two numbers which satisfy that condition, we usually specify the positive value. For example, the square root of 4 could be 2 (because ) or it could be -2 (because ). We use the symbol to indicate the positive square root of .\nThe cube root of is the number that, if multiplied by itself three times, equals . We use the symbol to indicate the cube root of .\nWe use the symbol to indicate the number, which when multiplied times is equal to . Or in symbols: if then .\nSolving for (Variable)\nWhen solving an equation, you usually solve for a specific variable. To do so, you have to get all instances of that variable on one side of the equals sign, and everything else on the other.\nProperties of Equality\nThe equal sign that depicts the fact that both sides of it are equal is a very strange symbol with many properties. It tells you various traits of each side, and it allows you to manipulate each side in specific ways. Here are the different properties of that sign:\n|Reflexive||a = a||8=8|\n|Symmetric||If a = b, then b = a||If (3)(2) = 6, then 6 = (3)(2)|\n|Transitive||If a = b & b = c, then a = c||If 8 = (4)(2) and (4)(2) = (2)(4), then 8 = (2)(4)|\n|Substitution||If a = b, then one can replace a with b or vice versa||If a = b and 1 + a = 3, then 1 + b = 3|\n|Addition||You can add one number to both sides of the equation.||\n|Subtraction||You can subtract one number from both sides of the equation.||\n|Multiplication||You can multiply both sides of the equation by a number.||\n|Division||You can divide both sides of the equation by a number.||\nDecide whether these following problems are expressions or equations.\nIdentify which properties are being used in the following problems.\n1. and , so .\n2. , then .\n3. , then .\n1. Substitution Property of Equality\n2. Subtraction Property of Equality\n3. Addition Property of Equality (combine like-terms!)\nBasic Laws In Algebra\nIn algebra we are working with the set of real numbers. We talked about the properties of real numbers with respect to mathematical operations in the section Real Numbers. If you don't remember the Commutative Property, the Associative Property, the Distributive Property, and the Identity Property go back to the real numbers section to review them.\nThere are several basic laws in algebra. Understanding these will help you to manipulate and solve equations, and to understand algebraic relationships.\nProportions or Ratios\nRatios or proportions can be expressed as an equation of fractions\n(for example ),\nor they can be expressed as a relationship Q : R = S : T , (expressed in words “ ‘Q’ is to ‘R’ as ‘S’ is to ‘T’ ”).\nUsing the words \"is to\" help us understand the physical relationship between the values, while the fractional representation can help us deal with the mathematical relationship.\nFor instance in America we know that 3 feet is to 1 yard as 6 feet is to 2 yards, but expressing this as a math equation: helps us see this is true because all we have done is double the original ratio.\nConsider the relation 3*4 = 2*6 = 1*12. Does this mean something more than the obvious fact that 12=12=12? How many ways could you package 12 items?\nIn general, these relationships can be described by a general equation like\nDividing each side of the equation by gives\n, which simplifies to\nIf we divide by instead the results simplify to\nOne needs to be careful since each term also has a proportional relation with the two other variables.\nIn our examples all of the following are also valid Q : S = R : T ,“ ‘Q’ is to ‘S’ as ‘R’ is to ‘T’ ”. R : Q = T : S ,“ ‘R’ is to ‘Q’ as ‘T’ is to ‘S’ ”.. S : Q = T : R ,“ ‘S’ is to ‘Q’ as ‘T’ is to ‘R’ ”.\nSince 2*6=3*4 2:3 = 4:6, 2 is to 3 as 4 is to 6, and 2/3 = 4/6. 4:2 = 6:3, 4 is to 2 as 6 is to 3, and 4/2 = 6/3. 3:2 = 6:4, 3 is to 2 as 6 is to 4, and 3/2 = 6/4. 2:4 = 3:6, 2 is to 4 as 3 is to 6, and 2/4 = 3/6.\nAlthough we have already solved a few equations, we will now discuss the formal idea of solving equations. To solve an equation, you are finding the value of any variables within the equation. To find the value of a variable, you have to manipulate the equation to state . Then you know the value of the variable! You will use the Properties of Equality to manipulate the equation into the desired form.\nSolve for x in the following equations.\n1. Subtract 7\n2. Subtract 9, then multiply by -3/2 (Inverse Property of Multiplication)\n3. Subtract 3x, then divide by 5\n4. No Solution, because there are too many variables to find a single number for x.\n5. All Real Numbers. Perform Distributive Property, and you'll get the same equation on both sides. Thus, any number would work!\nEquations are two expressions that are equal to each other, and they are expressed by putting each of them on one side of the equal sign. You can add, subtract, multiply, or divide both sides of an equation while keeping it equal (For example, we know that 7 = 7, correct? What if we subtracted 2 from each side? We'd still have a true statement: 5 = 5). There are other properties of equality, such as the Reflexive, Symmetric, Transitive, and Substitution. You will be using all of these properties to solve (find the value of) variables in equations.\n1. What property is expressed here? and , then .\n2. If I divided both sides of an equation by 4, would it still be equal on both sides? If so, why?\n3. Solve for y.\n1. Transitive Property of Equality\n2. Yes, due to the Division Property of Equality.","FACT SHEET: Climate and Energy Implications of Crypto-Assets in the United States\nClimate change is one of the most pressing problems confronting our nation and our world, and President Biden has taken bold steps to address it with legislation and policy. Among the President’s commitments are: protecting communities from pollution, reducing greenhouse gas emissions by 50% by 2030, achieving a carbon pollution-free electricity grid by 2035, and reaching net-zero greenhouse gas emissions no later than 2050.\nTo achieve these ambitious goals, we must ensure that emerging technologies contribute to a net-zero, clean energy future. The use of digital assets based on distributed ledger technology (DLT) is expanding. Digital assets are a form of value, represented digitally. As an emerging technological innovation, digital assets have provided some benefits and value for some residents and businesses in the United States, and have the potential for future benefits with emerging uses.\nCrypto-assets are digital assets that are implemented using cryptographic techniques. Crypto-assets can require considerable amounts of electricity usage, which can result in greenhouse gas emissions, as well as additional pollution, noise, and other local impacts to communities living near mining facilities. Depending on the energy intensity of the technology and the sources of electricity used, the rapid growth of crypto-assets could potentially hinder broader efforts to achieve U.S. climate commitments to reach net-zero carbon pollution.\nIn March, in Executive Order 14067 on Ensuring the Responsible Development of Digital Assets, President Biden made clear that the responsible development of digital assets includes reducing negative climate impacts and environmental pollution. The Executive Order directed the White House Office of Science and Technology Policy (OSTP), in coordination with other federal agencies, to produce a report on the climate and energy implications of crypto-assets in the United States. OSTP assembled an interdisciplinary team of experts to assess and extend existing studies with new analysis, based on peer-reviewed studies and the best available data.\nToday, OSTP published its report, examining the challenges and opportunities of crypto-assets for the United States’ clean energy and climate change goals, and providing a set of recommendations to further study and track impacts of the sector, develop potential performance standards, and provide tools and resources to reduce negative impacts. This report’s assessment and recommendations align with federal actions that reduce greenhouse gas emissions to protect public health and welfare, grow a clean energy economy with good-paying jobs, and improve environmental justice.\nCrypto-Assets Can Be Energy-Intensive, and the United States Has a Major Crypto-Asset Sector\nFrom 2018 to 2022, annualized electricity usage from global crypto-assets grew rapidly, with estimates of electricity usage doubling to quadrupling. As of August 2022, published estimates of the total global electricity usage for crypto-assets are between 120 and 240 billion kilowatt-hours per year, a range that exceeds the total annual electricity usage of many individual countries, such as Argentina or Australia. This is equivalent to 0.4% to 0.9% of annual global electricity usage, and is comparable to the annual electricity usage of all conventional data centers in the world.\nNearly all crypto-asset electricity usage is driven by consensus mechanisms: the DLT used to mine and verify crypto-assets. The dominant consensus mechanism is called Proof of Work (PoW), which is used by the Bitcoin and Ethereum blockchains. Bitcoin and Ether combined represent more than 60% of total crypto-asset market capitalization. The PoW mechanism is designed to require more computing power as more entities attempt to validate transactions for coin rewards, and this feature helps disincentivize malicious actors from attacking the network. As of August 2022, Bitcoin is estimated to account for 60% to 77% of total global crypto-asset electricity usage, and Ethereum is estimated to account for 20% to 39%.\nThe energy efficiency of mining equipment has been increasing, but electricity usage continues to rise. Other less energy-intensive crypto-asset ledger technologies exist, with different attributes and uses. Switching to alternative crypto-asset technologies such as Proof of Stake could dramatically reduce overall power usage to less than 1% of today’s levels.\nThe United States is estimated to host about a third of global crypto-asset operations, which currently consume about 0.9% to 1.7% of total U.S. electricity usage. This range of electricity usage is similar to all home computers or residential lighting in the United States. Crypto-asset mining is also highly mobile. The United States currently hosts the world’s largest Bitcoin mining industry, totaling more than 38% of global Bitcoin activity, up from 3.5% in 2020. Despite the potential for rapid growth, future electricity demand from crypto-asset operations is uncertain, demonstrating the need for better data to understand and monitor electricity usage from crypto-assets.\nCrypto-Assets Can Have Significant Environmental Impacts\nGlobal electricity generation for the crypto-assets with the largest market capitalizations resulted in a combined 140 ± 30 million metric tons of carbon dioxide per year (Mt CO2/y), or about 0.3% of global annual greenhouse gas emissions. Crypto-asset activity in the United States is estimated to result in approximately 25 to 50 Mt CO2/y, which is 0.4% to 0.8% of total U.S. greenhouse gas emissions. This range of emissions is similar to emissions from diesel fuel used in railroads in the United States.\nBesides purchased grid electricity, crypto-asset mining operations can also cause local noise and water impacts, electronic waste, air and other pollution from any direct usage of fossil-fired electricity, and additional air, water, and waste impacts associated with all grid electricity usage. These local impacts can exacerbate environmental justice issues for neighboring communities, which are often already burdened with other pollutants, heat, traffic, or noise. The growth of energy-intensive crypto-asset technologies, when not directly using clean electricity, could hinder the ability of the United States to achieve its National Determined Contribution under the Paris Agreement, and to avoid the most severe impacts of climate change. Broader adoption of crypto-assets, and the potential introduction of new types of digital assets require action by the federal government to encourage and ensure responsible development. This includes minimizing negative impacts on local communities, significantly reducing energy intensity, and powering with clean electricity.\nDistributed Ledger Technologies May Help with Climate Monitoring or Mitigation\nDLT may have a role to play in enhancing market infrastructure for a range of environmental markets like carbon credit markets, though other solutions might work as well or better. The potential benefits of DLT would need to outweigh the additional emissions and other environmental externalities that result from operations to merit broader use, relative to the markets or mechanisms that DLT displaces. Use cases are still emerging, and like all emerging technologies, there are potential positive and negative use cases yet to be imagined. Responsible development of this technology would encourage innovation in DLT applications while reducing energy intensity and minimizing environmental damages.\nKey Recommendations of the Report\nTo help the United States meet its climate objectives, crypto-asset policy during the transition to clean energy should be focused on several objectives: reduce greenhouse gas emissions, avoid operations that will increase the cost of electricity to consumers, avoid operations that reduce the reliability of electric grids, and avoid negative impacts to equity, communities, and the local environment.\nTo ensure the responsible development of digital assets, recommendations include the following actions for consideration:\n- Minimize greenhouse gas emissions, environmental justice impacts, and other local impacts from crypto-assets: The Environmental Protection Agency (EPA), the Department of Energy (DOE), and other federal agencies should provide technical assistance and initiate a collaborative process with states, communities, the crypto-asset industry, and others to develop effective, evidence-based environmental performance standards for the responsible design, development, and use of environmentally responsible crypto-asset technologies. These should include standards for very low energy intensities, low water usage, low noise generation, clean energy usage by operators, and standards that strengthen over time for additional carbon-free generation to match or exceed the additional electricity load of these facilities. Should these measures prove ineffective at reducing impacts, the Administration should explore executive actions, and Congress might consider legislation, to limit or eliminate the use of high energy intensity consensus mechanisms for crypto-asset mining. DOE and EPA should provide technical assistance to state public utility commissions, environmental protection agencies, and the crypto-asset industry to build capacity to minimize emissions, noise, water impacts, and negative economic impacts of crypto-asset mining; and to mitigate environmental injustices to overburdened communities.\n- Ensure energy reliability: DOE, in coordination with the Federal Energy Regulatory Commission, the North American Electric Reliability Corporation and its regional entities, should conduct reliability assessments of current and projected crypto-asset mining operations on electricity system reliability and adequacy. If these reliability assessments find current or anticipated risks to the power system as a result crypto-asset mining, these entities should consider developing, updating, and enforcing reliability standards and emergency operations procedures to ensure system reliability and adequacy under the growth of crypto-asset mining.\n- Obtain data to understand, monitor, and mitigate impacts: The Energy Information Administration and other federal agencies should consider collecting and analyzing information from crypto-asset miners and electric utilities in a privacy-preserving manner to enable evidence-based decisions on the energy and climate implications of crypto-assets. Data should include mining energy usage and fuel mix, power purchase agreements, environmental justice implications, and demand response participation. OSTP could establish a National Science and Technology Council subcommittee to coordinate with other relevant agencies to assess the energy use of major crypto-assets.\n- Advance energy efficiency standards: The Administration should consider working with Congress to enable DOE and encourage other federal regulators to promulgate and regularly update energy conservation standards for crypto-asset mining equipment, blockchains, and other operations.\n- Encourage transparency and improvements in environmental performance: Crypto-asset industry associations, including mining firms and equipment manufacturers, should be encouraged to publicly report crypto-asset mining locations, annual electricity usage, greenhouse gas emissions using existing protocols, and electronic waste recycling performance.\n- Further research to improve understanding and innovation: For improved analytical capabilities that can enhance the accuracy of electricity usage estimates and sustainability, the National Science Foundation, DOE, EPA and other relevant agencies could promote and support research and development priorities that improve the environmental sustainability of digital assets, including crypto-asset impact modeling, assessment of environmental justice impacts, and understanding beneficial uses for grid management and environmental mitigation. Research and development priorities should emphasize innovations in next-generation digital asset technologies that advance U.S. goals in security, privacy, equity, and resilience, as well as U.S. climate goals."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:51ccfe97-7f16-4ffd-bf66-9be679ed800c>","<urn:uuid:c2b3e9e1-0ef3-465d-9a23-92fdbe2309a6>"],"error":null}
{"question":"As a cinema technology enthusiast, I'm curious about the relationship between the technical implementation and commercial success of 3D in Avatar versus The A-Team, both released around 2009. What were their different approaches and outcomes?","answer":"Avatar and The A-Team took notably different approaches to filming technology and achieved very different outcomes. Avatar was acclaimed as the full realization of 3D's capabilities and commercial potential, earning three Oscars and becoming the highest-grossing film of its time with 70% of its box office revenue coming from 3D screenings. The A-Team, on the other hand, opted for traditional 35mm anamorphic format to create an organic look, specifically choosing different formats for different scenes - using Super 35mm for certain sequences and focusing on creating specific environmental looks rather than utilizing 3D technology.","context":["Mauro Fiore ASC / The A-Team\nMauro Fiore ASC / The A-Team\nAvatar took audiences on a surrealistic journey to a future world where humans invade an alien planet and engage the natives in a desperate battle for survival. The film broke box office records when it was released in 3D format on IMAX and traditional cinema screens in 2009.\nAvatar earned three Oscars and six other nominations. Mauro Fiore ASC claimed one of the Oscars, for best cinematography, along with outstanding achievement award nominations from BAFTA and his peers at the ASC. So what did Fiore do for an encore? The A-Team, which premiered on cinema screens around the world this summer. It’s another action-adventure story, only this time the main characters are empathetic human beings who make emotional connections with audiences.\nThe A-Team is an adaption of a mid-1980s television series about four veterans of the Vietnam war who were accused of a crime they didn’t commit. They banded together and performed heroic deeds in a quest for redemption. In the contemporary film, the main characters are veterans of the war in Iraq who join forces as mercenaries and perform dare devil deeds for the right reasons.\n“It’s not a simple story,” Fiore says. “Each of their missions has sub-plots augmenting the main story about these four men who are trying to redeem their reputations by fighting bravely in the face of overwhelming odds.”\nThe A-Team is Fiore’s second collaboration with writer/ director Joe Carnahan on a long form narrative film following in the wake of Smokin’ Aces in 2006. Ridley Scott is among the executive producers, with Tony Scott and Iain Smith amongst the producers. Fiore and Carnahan have also worked together on various commercials during the past half a dozen years. They agreed that 35mm anamorphic was the right format for creating an organic look and feeling that visually augments the emotional tone as the drama heightens. They also discussed creating specific looks for different environments.\nCarnahan cast Liam Neeson in the role of Col. “Hannibal” Smith. Bradley Cooper, Sharlto Copeley and Quinton “Rampage” Jackson were brought onboard to portray Lt. “Faceman” Peck, Capt. “Howling Mad” Murdock and Sgt. B.A. Baracus.\nThey originally intended to film scenes at practical locations around the world. However, a decision was made to produce The A-Team in and around Vancouver, Canada as a concession to the budget and schedule. Fiore and production designer Charles Wood scouted locations in and around Vancouver searching for the right environments for different parts of the film. They found a location in Kamloops, a region just outside of Vancouver, which has similar landscapes and mountain ranges as the dry, arid environment characteristic of the Baja desert in Mexico. Two blocks in a warehouse district convincingly served as alleys in Mexico when they were dressed with signs and extras wore the right costumes.\nOther scenes are staged in an army Iraqi camp, where they are printing American money with plates stolen from the mint, on downtown streets in Frankfurt, Germany, and on the harbor in Long Beach, California.\n“We created specific looks for each environment,” Fiore says. “The Mexican section was treated very warm and sun-lit with a saturated golden tone, almost like reversal film. The audience isn’t always able to see the shadow side of people’s faces. The Iraqi section of the film was produced in a field outside of Vancouver where we built a set for an army base. We decided on a desaturated look with very little color in it. Most of the backgrounds were dusty. We used physical special effects to create a blown out sort of look with details in backgrounds not decipherable.”\nCarnahan created storyboards for action sequences, which provided guidelines for covering scenes. He and Fiore agreed to cover the action with two cameras. “The story called for scope,” Fiore says. “The 2.4 aspect ratio was really important, and I like the effect of anamorphic lenses optically.”\nThey made several exceptions. Second unit cinematographer Larry Blanford suggested shooting a night exterior scene that is set on a dock in Long Beach in Super 35mm. Fiore and Carnahan embraced that idea. “We wanted the ability to be able to open up to T- 2.0 or 2.8 and enable the second unit to use zoom lenses at night in that scene and few others,” Fiore says.\nOther scenes filmed in Super 35mm included a helicopter sequence set in Mexico, a night chase scene in the Iraqi segment, and a huge stunt set in Frankfurt, where people seem to be falling off 200 to 500 foot high skyscrapers.\n\"The Mexican section was treated very warm and sun-lit with a saturated golden tone, almost like reversal film.\"\n- Mauro Fiore ASC\nFiore generally covered scenes with A and B cameras. He used the A-camera for close-ups during dialogue scenes, and the B camera for wide-angle coverage of environments and for visual subtext, including close-ups of hands and faces. Panavision provided the camera package – a Millennium XL was the A-camera, a Panavision Platinum the B-camera, and Panavised ARRI 435 and 235 cameras were used for high-speed cinematography. Fiore carried a combination of anamorphic C, E and G series lenses, and the second unit used Primo prime and zoom lenses.\nFiore had two filmstocks on his palette. He used Kodak Vision3 5205 250 D film for daylight exteriors, because it renders “a bit” more contrasty images than the 100-speed negative, and Kodak Vision 3 5219 500T negative for night and darker interior scenes.\n“My relationship with my gaffer (David Tickell) and rest of the crew was very important, because this is a collaborative process, and you need everyone’s best efforts,” he emphasises. Dave Kurvers, a digital imaging technician, was in charge of taking stills of scenes, which were colour corrected and sent to Technicolor, in Vancouver, as visual references for the dailies timer. Final touches were rendered on the look of the film during DI timing with colourist Yvan at EFilm, in Los Angeles. The A-Team was produced and distributed by 20th Century-Fox.","The slow, quiet death of 3D cinema\nAlthough we consider 3D a recent phenomenon in cinema, its history began all the way back in the early 1800s, coming to cinemas by the early 20th century.\nIndeed, even in the German film industry of the 1910s, 3D was seen as an expensive, cumbersome format but one that could potentially draw in audiences for its technical marvels. By the '20s, the movies were produced sparingly, often at huge expense, and the subtlety of circus attractions.\nThe golden age, if one ever really existed, came in the '50s, but it soon fell out of favour with audiences and cinema exhibitors alike. The reasons? Cost, eye strain of audiences, the general headaches of trying to make it work, and an overall lack of ingenuity in the films made with 3D as a component.\nWhen it was tried again in the '80s, with hopes of capitalising on the renewed interest in sci-fi from the likes of 'Star Wars', it was equally troublesome. 1983's 'Spacehunter: Adventures in the Forbidden Zone' was made on a budget of $14.4 million and recouped only $16.5 million at the US box office. 'Return of the Jedi', released a week later, was made on a budget of $32 million and made $309 million in the US and Canada.\nOne of the big proponents of 3D during this time was the Walt Disney Company, who had set the likes of Francis Ford Coppola and George Lucas to work on making it more commercially viable. Projects like 'Captain EO' were made specifically for Disney theme parks, while competing attractions in Universal Theme Parks included James Cameron and 'T2 3-D: Battle Across Time'.\nEven though they had name-recognition directors attached, the charm of 3D was confined to theme parks and very often combined with other effects, such as vibrating seats and in-theatre pyrotechnics. By 1996, 'Captain EO' had run its course while 'T-2 3-D: Battle Across Time' only closed in 2017, although it featured live performers interacting in part with the 3D movie.\nIt wasn't until 2003, however, that 3D left theme parks and returned to cinemas. That year saw James Cameron once again trying to break 3D to audiences with 'Ghosts of the Abyss', which was made specifically for IMAX 3D theatres and 35mm theatres that were outfitted with the technology. Although made on a budget of $13 million, the box office return was minimal - $28.7 million.\nThe first commercial success didn't come until 'The Polar Express', directed by Robert Zemeckis and starring Tom Hanks in multiple roles. Earnings on 3D screenings compared to regular 2D screenings were 14 times higher. Across 59 IMAX theatres in the US on its opening weekend, it earned an average of $35,593 per screen.\nWhile it was a commercial success, 'The Polar Express' was an expensive movie to make - $165 million, an unprecedented amount for any movie at the time, let alone an animated one. Overall, the movie claimed $313 million across its various iterations and releases. Yet, for all the groundbreaking success, the critical reception to the movie was quite cool; it currently sits at 61 out of 100 on Metacritic and 56% on Rotten Tomatoes.\nThe commercial and critical breakthrough for 3D didn't come until 2009's James Cameron's 'Avatar'. Nominated for nine Academy Awards including Best Picture and Best Director, the movie was acclaimed as the full realisation of its capabilities and commercial potential. The movie was, until very recently, the highest-grossing movie of all time. It was the first movie to cross $2 billion at the box office.\nTen years on, the reality is very different.\nIn the Republic of Ireland, there are 267 screens capable of showing 3D movies. Of the total box office revenue for 2019 across all formats, 3D screenings accounted for just 2% of revenue. Despite this, movies screened in 2D and 3D accounted for 48% of all box office revenue. In fact, according to Wide Eye Media, 3D screenings tend to cease within the first three weeks of a movie's release, depending on popularity.\nWhat's true in the Irish market is true elsewhere. Worldwide, there are close to 75,000 screens capable of showing 3D. In 2016, 3D accounted for just 20% - $7.8 billion - of the global box office revenue. 'Avatar' made up 70% of its box office revenue from 3D screenings in 2009. In the US, 3D screenings accounted for 21% of total revenue. By 2016, that figure had fallen to 14.1%.\nSo what happened in those ten years, from 'Avatar' being hailed the future of cinema to the quiet death of 3D?\nThe short answer is over-saturation. The joke doing the rounds was always that if a studio couldn't make a good movie, they could make it in 3D. Whether this mantra was actual official studio dogma or not is unclear, but one thing's for certain - studios were latching on to the format for dear life.\nA common strategy deployed by studios was to upscale movies to 3D in order to shore up potential losses. In some cases, movies that were never at all intended for 3D were given releases with rudimentary 3D effects applied. Likewise, some movies had their releases specifically adjusted to ensure there were more 3D screenings than 2D screenings, thus earning more money per screen on average.\nAudiences have more or less been rejecting 3D since the middle of the previous decade. A quick question via Twitter on the last time someone paid for a 3D screening showed up only a handful of recent movies. By and large, people pointed to the likes of 2013's 'Gravity', 2008's 'Up', 2012's 'The Life of Pi', with a couple of exceptions pointing to 'Bumblebee', 'Star Wars: The Force Awakens' or 'Star Wars: The Rise of Skywalker'. Generally, people will only choose it as a last resort if all other screenings are full. Most people, again from anecdotal sources, would opt for 2D over 3D given the choice between the two.\nSo who weeps for 3D? Certainly not audiences. At a panel at CineEurope in 2016, Jeffrey Katzenberg, co-founder of DreamWorks Animation and ex-CEO of the Walt Disney Company, declared that when it came to 3D, \"we blew it.\"\n\"It was a game-changing opportunity for the industry. When you gave them an exceptional film that artistically, creativity embraced and celebrated the uniqueness of that experience, people were happy to pay the premium,\" Katzenberg stated. His argument was those producers who had taken the low road and \"gimmicked\" the technology had ultimately cost the industry the goodwill of audiences.\nSome would argue that it was always a gimmick, no different from Smell-O-Vision or any other hokey special effect deployed in a cinema for cheap thrills. Yet, for every technological leap in cinema, the initial impact is always dicey at best. In 1928, the reviews for 'The Terror', an all-talkie murder mystery starring May McAvoy and Louise Fazenda, were skeptical of the inclusion of audio. 'The Terror' was one of the very first movies to feature all-talking throughout its runtime.\nThe writer of the movie, Edgar Wallace, wasn't impressed by the technological advances. In a piece in the New York Times, Wallace commented that he \"never thought the talkies would be a serious rival to the stage.\" In her review, Observer critic CA Lejeune decried the inclusion of talking in movies, arguing that \"(we) may deplore limitations of language for the hitherto universal cinema... I cannot think that, once having heard the voice, we shall ever be satisfied with the dumb figures of our favourites again.\"\nIt may yet be that the technology still hasn't found the right combination of artistic integrity, convincing storytelling, and an audience willing to accept it. Or it could just be that audiences simply don't care enough about it in the first place. Major studios and filmmakers have largely abandoned 3D, except for a lonely handful.\nThe next major release for the format is, funnily enough, James Cameron's sequel to 'Avatar'.\nSet for release in December 2021, which includes more recent technological advancements in filmmaking such as underwater motion-capture for actors, the movie could potentially be the first to feature autostereoscopic imagery. In plain English, it means 3D without the use of glasses.\nAt the Vivid Light Festival in Sydney, Australia, Cameron explained his vision. \"We need to see the roll-out of these laser projection systems so that we can fully appreciate 3D through glasses in cinemas. Then, we need the roll-out of autostereoscopic screens, large panel displays, where you don't need glasses at all. You have multiple discreet viewing angles and all that sort of thing. Anybody that's geeking out on 3D knows what I'm talking about,\" he enthuses.\n\"It's all possible. It's just a question of will it happen or not.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:34f70826-c61b-4686-8222-f88906563eda>","<urn:uuid:9d8af920-ce33-434e-a68b-534556a8b2b3>"],"error":null}
{"question":"As a guitarist looking to simulate a 12-string guitar sound, how can I achieve this effect using the Micro POG pedal?","answer":"To simulate a 12-string guitar sound with the Micro POG, you need to turn the SUB OCTAVE knob all the way down, then adjust the DRY and OCTAVE UP knobs. The dry level should be slightly higher than the octave up level for the best sound. This works because a 12-string guitar doubles each string with another string an octave higher (except for the high E and B strings).","context":["Electro-Harmonix Micro POG Review\nThe Micro POG (Polyphonic Octave Generator) has a simple job: it generates a two separate signals: one octave lower than your original guitar and other one octave higher than your original guitar. We’ll look at how useful this really is later, but let’s start with the basic features offered by the Micro POG:\nThis is a very important feature for any pitch shifting pedal. There are pedals out there only designed for monophonic playing which means if you strike a chord or play an arpeggio, you will hear glitches. The Micro POG was designed for polyphonic (more than one note at a time) so you can rest assured that it will work with whatever riffs or ideas you have.\nSeparate control for each signal\nThere are three main knobs on the pedal which gives you complete control over the mix. The first knob controls your dry tone’s level. The second controls the level of the tone one octave lower and the third controls the level of the tone one octave higher. As you will read later, this simple setup allows you to produce a wide range of sounds.\nTwo separate outputs\nYou have the option for two outputs from the Micro POG. One output bypasses the pedal and passes through your dry tone even when the pedal is activated and the other output sends out the effect. This means you could add the Micro POG to your signal chain and send it to a separate amp or effects chain without it affecting your main rig. I’ll go through this scenario later because it’s worth looking into.\nEase of use\nThis really is a simple pedal and they couldn’t have made it easier to use. You simply adjust the three level knobs depending on what type of effect you’re after and that’s it. Let’s go through a few different scenarios to see how easily different sounds can be produced:\n12 string guitar effect\nA 12 string guitar doubles each string with another string an octave higher (except the high E and B strings). This means you can use the Micro POG to simulate the same sound by adding a signal an octave higher into the mix. To do this you turn the SUB OCTAVE knob all the way down because you don’t want an octave lower to be present. Then you adjust the DRY and OCTAVE UP knobs to achieve a mix you’re happy with. You want the dry level to be slightly higher than the octave up level to achieve the best sound.\nIf you want to simulate a bass guitar you simply turn the DRY and OCTAVE UP knobs all the way down, then turn the SUB OCTAVE knob up. You will end up with a sound one octave than your original sound. The main riff to Seven Nation Army by The White Stripes is achieved with this same basic effect.\nIf you want to achieve a great organ tone, then I highly recommend checking out the EHX B9 Organ Machine. But before the Organ Machine was available, guitarists would sometimes use the Micro POG as a simple way to simulate a basic organ-like tone. Don’t expect anything as good as the B9, but if you want something similar this is a simple starting point. To achieve a decent sound, you want to play around with all three knobs so you have three tones for every note you play on guitar: one octave lower, original, and one octave higher.\nSplitting your signal\nOne of the best uses I’ve seen for the Micro POG was a guitarist in a two-piece band who used the Micro POG to split his signal. He sent his main guitar signal to his amp using the DRY OUT output. Then he also used the EFFECT OUT with the DRY and OCTAVE UP knobs all the way down (to produce a bass tone) and sent it to a bass amp. The result was he could play riffs and achieve both a guitar and a bass tone without muddling up his tone with one amp output. He could then ‘turn the bass guitar’ on and off as he pleased by simply hitting the footswitch on the Micro POG. This also meant he could continue to use other effects for his main guitar sound.\nFor some riffs, he used a wah while other sections he used delays and fuzz pedals while still retaining a basic bass tone. The Micro POG was the first pedal (after his tuner) in his rig so the bass tone was unaffected by other pedals.\nThe above example demonstrates the potential use for this pedal. Of course, there are other creative ways you can use it, but being able to instantly double your tone with a simulated bass is a powerful way to thicken up your tone – especially if you’re in a two-piece band or play solo.\nUsing a bass guitar\nIt’s also worth mentioning that the Micro POG works extremely well with a bass guitar. I experimented with a bass and adding an octave up to the mix gave me plenty of inspiration for different riffs. You could even experiment with using an octave lower as a way to reach really low tones normally only available on five-string bass.\nThis effect produces a very basic sound and thankfully the tracking is spot on. You’re not going to notice latency while playing with the Micro POG. The best tones I managed to get out of the Micro POG was when I used it in combination with different effects. I’ll go through some of the interesting combinations to give you an idea of how versatile this pedal can be.\nMicro POG (with one octave lower) & Big Muff Pi\nI set the Micro POG to mix between the dry signal and an octave lower. Then I added a Nano Big Muff Pi (read my review on the Nano Big Muff Pi here) to add some fuzz to the mix. The results were gritty and gnarly. With a gritty gain on the Big Muff it was great fun to rip into leads and play thick chords. When I dialed the gain back a bit it came closer to the Super Colossal sound you hear Joe Satriani use (he actually uses a Micro POG in his live rig as of 2018). I recommend you listen to that song for a good idea on how to use the Micro POG to play both riffs and leads. Read through my guide on Joe Satriani here to learn more about the effects he uses.\nMicro POG (with one octave higher) & Big Muff Pi\nThis time I turned the sub-octave almost all the way down and turned the one octave higher up in the mix. The intention was to create an effect similar to the Octavia which Hendrix used extensively throughout his career (read my Hendrix guide here for details on the Octavia). The Octavia basically mixes in a signal an octave higher and adds some fuzz distortion so using the Big Muff Pi with the Micro POG was a great way to achieve the same effect. Personally, I feel this combination is better than the Octavia simply because you have far more control over both the fuzz and octave mixing. If you’re a Hendrix fan, this is something worth experimenting with.\nMicro POG & Eventide H9\nThere are some great effects on the Eventide H9 (read my review on the H9 here) and a lot of them worked really well with the Micro POG. From simple reverb effects to dramatic modulations, playing around with the three knobs gave me plenty of inspiration for songwriting and just jamming. If you ever feel like you’re stuck for ideas then this combination is a really quick and easy way to get the creative juices flowing. It doesn’t hurt that the H9 has incredible sound quality either!\nMicro POG & Ditto Looper\nThis is a basic setup but very useful for songwriting or jamming on your own. Set the Micro POG to only play one octave lower then record a loop of yourself playing a bass line. Then bypass the pedal and you have a bass track you can jam over using your normal guitar tone. It’s a very simple way to brainstorm ideas. I used the Ditto Looper (read my review of the Ditto here) but you can use any looper.\nMicro POG & Wah\nMixing in an octave lower with some overdrive and a wah is a great way to create a gnarly tone that feels incredible on simple riffs. I used the Original Crybaby wah (read my review of the Crybaby here) and experimented with slightly mixing in an octave higher to add some extra harmonics and range. If you’re looking for a way to spice up your tone when using a wah, using the Micro POG to subtly (or not so subtly) enhance your tone is a great way to bring something new to your tone.\nThere are plenty of other combinations to experiment with, but hopefully you can see why the Micro POG is a worthwhile buy thanks to it’s versatility.\nSometimes pitch based pedals have a horrible digital feel to them. Fortunately, the Micro POG doesn’t give you a digital feel. Of course, if you crank the higher octave up you might feel like it sounds digital, but that’s simply because shifting the pitch up an octave is always going to sound a bit unnatural.\nThe pedal has a nice strong chassis and the three knobs feel very secure. Unlike the POG2 which has small sliders, I don’t feel like this one is going to cause any problems even after years of abuse.\nThe Electro-Harmonix Micro POG is a simple pedal with a simple effect. To be honest I didn’t really think that I would find many uses for this effect in my playing but after experimenting with it and using it in combination with other pedals, I realized the POG’s potential. It’s an incredibly useful and versatile effect that can be used in a wide range of styles and situations. Whenever I feel like I’m running out of ideas or feel low on creativity, now I can tweak the three knobs and straight away come up with something new.\nThere are a lot of pitch based effects out there and I really feel this simple effect is worth its place on most guitarist’s pedalboards. It won’t suit every guitarist but if after reading the above examples you feel you could make use of the Micro POG, then it’s definitely worth the money.\n- Split output options allow you to run two separate rigs\n- Polyphonic tracking means you can play anything without having to worry about glitches\n- Simple controls\n- Fantastic quality sound\n- Fairly expensive for a basic effect\n- Doesn’t offer detuning options like other pedals\nWho is it for?\nI recommend this pedal for most guitarists simply because it can be used in a wide range of situations. Being able to simulate a bass guitar on demand is a great option every guitarist who writes music should have. Laying down your own backing bass track with a looper pedal can really give you freedom in songwriting and jamming. There are plenty of songs that use this effect and it’s a great way to try something creative so I really recommend the Micro POG for most guitarists.\nWho isn’t it for?\nThe only time I wouldn’t recommend the Micro POG is for any guitarist looking at using it to produce an organ-like tone. You will read and hear that a lot of people do use the POG for that use, but that was before the B9 Organ Machine came out. If you want a pedal to create an organ tone, the B9 Organ Machine should really be the option you consider. While the Micro POG can achieve that sound more or less, it’s nowhere near as good as the Organ Machine.\nHow to get the most out of the EHX Micro POG\nExperiment with combining the Micro POG with different pedals. I had so much fun using the POG with different effects and tones and that’s how I learned to get so many different sounds out of it. You’re going to get the most out of the POG by learning how to use it in different situations.\nFor example, start out with a dry signal and slightly mix in an octave higher just enough that it’s subtly noticeable. Now connect a wah pedal after the POG and listen to how the wah responds to your tone now that you have mixed in something an octave higher. You’ll be impressed with the difference it makes to your tone.\nAs another example turn the higher octave off and mix in an octave below but keep it fairly low in the mix. Use a heavy distortion pedal and listen to how the octave below gives you a wider tone. Adjust the lower octave mix up and down and find the sweet spot that suits you.\nMessing around with different options like the two examples above is how you will get the most out of the Micro POG.\nAlternatives to the Micro POG\nThere are plenty of similar pitch based effect pedals out there, but here are two good alternatives to consider.\n- Electro-Harmonix POG2 – this gives you far more flexibility and options (eg: also has an option for 2 octaves down)\n- Electro-Harmonix Nano POG – Same features as the Micro POG in a tiny pedal and lower price\n- DigiTech Whammy DT Drop Tune – the Digitech Whammy isn’t quite the same as the POG, but it does offer a great pitch based effect with an expression pedal (something missing from the Micro POG)\nIf you’re looking for a cheaper alternative, I strongly recommend avoiding any pitch based pedals that are offered at a price lower than the Micro POG. I’ve used quite a lot of pitch based effects over the years and almost every time I use a cheaper pedal, I’m horribly disappointed with the end result. Think of the Micro POG as the lower limit to getting a quality tone for a cheap cost. Pick up a cheap pitch based pedal and you’ll immediately hear why I recommend avoiding them as soon as you strum a chord.\nLast Updated on"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:1ca1cf07-5cb9-4228-b67d-b71ec1d43cc5>"],"error":null}
{"question":"How do muscle mass maintenance and mental exercises contribute to preventing cognitive decline in older adults? Which specific strategies are recommended for each aspect?","answer":"For muscle mass maintenance, key strategies include consuming protein every 2.5-3 hours, performing weight-bearing exercise every other day, and supplementing with muscle-enhancing aids like whey protein, creatine monohydrate, and branched-chain amino acids. Maintaining muscle mass is crucial as research shows it's linked to better memory and similar cerebrospinal fluid levels as younger people. For mental exercise, several approaches can boost cognitive function: reading, doing crossword puzzles, playing board games, enrolling in school, or learning a new language. These mental exercises can help prevent or delay the development of Alzheimer's or other forms of dementia. Regular mental stimulation is essential since brain functions, like muscles, can diminish if not used regularly.","context":["The body fat-disease link\nEverybody wants to live a long and healthy life. But, if you carry excess body fat on your frame, then you may be setting yourself up for a shorter lifespan than the one you were aiming for.\nExcess body fat is directly linked to so many diseases that it can no longer be thought of as solely a cosmetic issue. Cardiovascular disease, diabetes, stroke, numerous cancers, gallbladder disease, immune dysfunction, sleep apnea, infertility, and osteoarthritis (degeneration of the cartilage and bone in the joints) are just the tip of the iceberg. The more body fat you have on your frame, the higher your risk of checking out well before your time.\nSo why do the majority of us seem to find it so easy to pack on the pounds after age 35, all the while losing the features that defined our youth—strength, vitality, immunity, sexual function and desire, skin tone, and memory?\nKeep it long and lean\nLean body mass (muscle) to a very large extent controls the overall metabolic rate of the body. In fact, a study of 84 men and women aged 90 to 106, presented in the Journal of the American Geriatrics Society in 1997, showed that loss of muscle is the primary longevity factor.\nWhen you grow or reactivate muscle metabolism, you actually enhance overall energy production, burn more calories and in the process, more fat. Enhanced muscle mass has also been linked to lower levels of stress hormones. Elevated stress hormones are a contributing factor to muscle breakdown. Research indicates that stress levels (cortisol) in older people are typically two to four times those of younger ones—both at rest and with exercise.\nMany studies have shown that the maintenance of muscle mass is not only associated with lower body fat levels but also increased energy, better mood, stronger connective tissue and bones, better immunity and according to Japanese researchers, possibly even enhanced memory. Researchers have discovered that low cerebrospinal fluid (CSF)—well documented in the elderly—is a possible risk factor for Alzheimer's disease. It turns out that elderly people who maintain muscle mass through full range-of-motion exercises have the same level of CSF as younger people.\nThe take-home message is that healthy aging is controlled to a large extent by your metabolism and, in turn, your metabolism is controlled by your muscle mass and activity.\nHow to maintain muscle mass\n- Consume sufficient protein every two and a half to three hours (high-alpha whey protein shakes are a great substitute for one or two protein meals).\n- Perform weight-bearing exercise every other day. Resistance exercise stimulates new muscle growth and helps maintain metabolism.\n- Supplement with ergogenic (muscle-enhancing) aids like high-alpha whey protein, micronized creatine monohydrate, and branched-chain amino acids (BCAAs), before or immediately after training.\n- Reduce stress. Excess stress interferes with DHEA (dehydroepiandrosterone), one of your body's most powerful anabolic hormones. Research shows that lowered stress levels cause an increase in DHEA levels. For starters, try reducing your caffeine consumption.\n- Ravaglia G, et al. Determinants of functional status in healthy Italian nonagenarians and centenarians: a comprehensive functional assessment by the instruments of geriatric practice. J Am Geriatr Soc. 1997 Oct;45(10):1196-202.\n- Ziegler, MG, Lake, CR, Kopin, IJ (1976) Plasma noradrenaline increases with age. Nature 261,333-335\n- Mitsui S, Okui A, Uemura H, Mizuno T, Yamada T, Yamamura Y, Yamaguchi N. Decreased cerebrospinal fluid levels of neurosin (KLK6), an aging-related protease, as a possible new risk factor for Alzheimer's disease. Ann N Y Acad Sci. 2002 Nov;977:216-23.\n- Cherniske S. The Metabolic Plan. Ballantine Books, New York, 2003. Pg. 23-24.\n- Bassit RA, Sawada LA, Bacurau RF, Navarro F, Costa Rosa LF. The effect of BCAA supplementation upon the immune response of triathletes. Med Sci Sports Exerc. 2000 Jul;32(7):1214-9.\n- Cruess DG, et al. Cognitive-behavioral stress management buffers decreases in dehydroepiandrosterone sulfate (DHEA-S) and increases in the cortisol/DHEA-S ratio and reduces mood disturbance and perceived stress among HIV-seropositive men. Psychoneuroendocrinology. 1999 Jul;24(5):537-49.\n- Smaller Small Medium Big Bigger\n- Default Helvetica Segoe Georgia Times\n- Reading Mode\nBrad King, MS, MFS\nBrad King is a highly sought after authority on nutrition, obesity, longevity and one’s health and he has been touted as one of the most influential health mentors of our time.\nHe is an award winning nutritional formulator and was honored with the Best in Canada Award for Health Motivator/Educator and Public Speaker in 2010, was inducted into the Canadian Sports Nutrition Hall of Fame in 2003 and sits on the board of Directors for CHI the premiere sports nutrition education center.\nBrad is the author of 10 books including the international best seller, Fat Wars 45 Days to Transform Your Body, the award winning Beer Belly Blues: What Every Aging Man and the Women in His Life Need to Know and his newly released The Ultimate Male Solution.\nBrad is a leading health expert having appeared on NBC, ABC and CBS, some of these interviews include: The Today Show, Canada AM, Balance TV, Macleans, Oxygen, The National Post, Chatelaine and Live with Marcus and Lisa.","Decades ago, people believed that senility or frequently forgetting a lot of things is a fact of life for old people. For them, there is nothing that one can do but gracefully accept that at a certain age, probably 70 or 80, one will start to show signs of cognitive function decline. In recent years, however, researchers have discovered that it is never too late to start to boost your brainpower.\nPhysical Exercise Is Also Good For The Mind\nA study published in the Journal of the American Medical Association in 2004 showed that people aged 71 to 93 who walked over 3 kilometres a day were twice more unlikely to develop dementia than those who walked less than a kilometre every day. Furthermore, women between 71 and 80 who clocked 90 minutes of walking a week did a lot better on cognitive examinations than those who had a sedentary lifestyle. Several studies have shown that increased exercise, even just walking, is crucial in keeping a healthy mind. Experts are not sure how much exercise and physical activities old people need to perform to have a boost on their cognitive functions. It is important to note, however, that regular exercise aids in keeping a healthy blood flow. These activities may also help in creating and maintaining new nerve connections in the brain.\nAnother benefit of exercise is that it aids in keeping glucose levels normal. Glucose is important for the proper functioning of the brain, so even a slight decline in the level of glucose may have a great impact on brain functions of an old person. Rresearch conducted by the New York University’s Center for Brain Health showed that people who have problematic glucose tolerance did not only perform badly in memory tests but also showed signs of brain shrinkage compared to those who had normal glucose tolerance or control. Exercise does not have to be boring or confined in a gym. Old people can find physical activities that they like, such as strolling in the park with colleagues, wading in the pool for a couple of hours, and playing bowling or badminton. See more.\nMind Aerobics Is Essential\nAnother crucial factor in keeping the brain healthy is through mental exercises. If you don’t bother about using your brain, then there is a big likelihood that you will lose it. Our brain functions are impaired or greatly diminished if not used regularly, much like our muscles atrophy when not utilised or exercised.\nNowadays, there are many ways in which you can improve your memory and brain functions. One of which is by enrolling in a brain booster class. There are many institutions in the US that are offering classes specifically designed for old people who want to keep their cognitive functions at their peak. Reading, doing crossword puzzles, playing board games, enrolling in school, or learning a new language is just some ways wherein you can help boost your mental functions. It is never too early or too late to start doing mental exercises.\nExperts believe that doing mental aerobics can help prevent or delay the development of Alzheimer’s or other forms of dementia. If your family has a history of dementia, there is a big chance that you will also develop this disease. However, mental exercises can significantly delay the development of such condition for weeks, months and even boost your brainpower."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:eea71dc8-1d16-4201-887e-943208f4dd3f>","<urn:uuid:3da78e32-ca2e-44e6-9a93-64910bacdd18>"],"error":null}
{"question":"How does linguistic precision affect environmental discourse, and what are its practical implications?","answer":"Linguistic precision in environmental discourse is crucial - just as Fowler warned against using varied terms that might confuse readers (like switching between 'Emperor', 'His Majesty', and 'Monarch'), environmental communication requires clear, consistent terminology. This is especially important when discussing deforestation, where precise language helps convey specific impacts: trees remove carbon dioxide from the atmosphere and release oxygen (providing 20% of Earth's oxygen in the Amazon's case), prevent soil erosion through their root systems, and provide habitat for 80% of animal and plant species. Using clear, consistent terminology helps readers understand these concrete environmental effects without the confusion that could arise from unnecessarily varied vocabulary.","context":["Many of us, while we're writing, go out of our way to not repeat a word or phrase within a sentence or a paragraph because we imagine, understandably perhaps, that to do so would make the sentence or paragraph awkward and monotonous. But trying to solve the problem by replacing these words or phrases with synonyms can lead to ambiguity, and often disrupts the flow of the sentence. \"Elegant variation\" is the phrase coined by English lexicographer Henry Watson Fowler to refer to these \"substitutions of one word for another for the sake of variety\", as the writer conspicuously strains to avoid harmless repetition. (When Fowler coined the term in the 1920s, the word \"elegant\" had a negative connotation of precious overrefinement that it no longer has today, and in \"The Oxford Dictionary of American Usage and Style\", author Bryan Garner unambiguously renamed the term \"inelegant variation\".)\nIn \"Modern English Usage\" (1926) Fowler wrote:\n\"It is the second-rate writers, those intent rather on expressing themselves prettily than on conveying their meaning clearly, & still more those whose notions of style are based on a few misleading rules of thumb, that are chiefly open to the allurements of elegant variation. ... The fatal influence ... is the advice given to young writers never to use the same word twice in a sentence – or within 20 lines or other limit.\"\nIn \"The King's English\" (1908), he gives as one of his several examples this passage from The Times:\n\"The Emperor received yesterday and to-day General Baron von Beck... It may therefore be assumed with some confidence that the terms of a feasible solution are maturing themselves in His Majesty's mind and may form the basis of further negotiations with Hungarian party leaders when the Monarch goes again to Budapest.\"\nFowler objected to this passage, because \"The Emperor\", \"His Majesty\", and \"the Monarch\" all refer to the same person. \"The effect\", he pointed out, \"is to set readers wondering what the significance of the change is, only to conclude that there is none.\"\nTake this sentence: \"Four of the defendant’s witnesses were women, while all of the plaintiff’s witnesses were ladies\". In trying not to repeat the word \"women\", the writer implies that there is something different about the plaintiff’s group of witnesses. Another example of this would be a document continually referring to a person as a \"landlord\", then suddenly switching to referring to this same person as a \"lessor\" - the reader may then be led to believe that the \"landlord\" is not the same person as the \"lessor\". These examples illustrate why we should be careful about varying for the sake of variety in this way.\nWhile we would understandably want to avoid awkward repetition, using a thesaurus to find synonyms to use as replacement is often not the answer. Synonyms listed in the thesaurus often have different shades of meaning, and cannot be used as straightforward replacements for each other. And when the synonyms we choose are uncommon words, they tend to leap off the page at the reader, attracting their attention to how something's being said and distracting them from what is being said.\nThe solution is often to restructure the sentence or paragraph so as to eliminate the problem entirely. Where there is no change in meaning, either repeat the word, or, if it sounds awkward, change the structure and omit the word altogether. Sometimes we can overcome the problem by using a pronoun or some other means of reference.\nIn his Dictionary of Modern English Usage, Fowler said of elegant variation, \"There are few literary faults so widely prevalent, and this book will not have been written in vain if the present article should heal any sufferer of his infirmity.\"\nHis advice? \"When the choice lies between monotonous repetition on the one hand and clumsy variation on the other, it may fairly be laid down that of two undesirable alternatives the natural is to be preferred to the artificial.\"\ncanedit.ca newsletter, \"Monthly Info about Editing, Writing, Style, Words...\", October 29, 2001 - http://www.canedit.ca/newsletter-20011029.htm\nArticle on effective legal writing and clarity - http://tech.clayton.edu/mshapiro/Para%201105%20Legal%20Research%20&%20Writing/Clarity.htm\nDeccan Herald, 19 Feb 2004 article on elegant variation -http://www.deccanherald.com/deccanherald/feb192004/edu5.asp\nWikipedia article on elegant variation - http://en.wikipedia.org/wiki/Elegant_variation\nOnline version of \"The King's English\" - http://www.bartleby.com/116/302.html\nColumbia Journalism Review, Language Corner article - http://www.cjr.org/tools/lc/elegant.asp","Deforestation: Causes, Effects, and Solutions\nClimate change and global warming are hot topics of discussion today in almost every global forum and these problems are to a large extent caused by deforestation. The fact is that humans and animals cannot survive if deforestation continues the way it is at present. At present, there seems to be no end in sight and deforestation is capable of altering the future of the whole earth. That’s why it’s important to understand what deforestation is, what is causing deforestation, its effects on our life and planet and how we can together solve this potentially disastrous situation.\nWhat is Deforestation\nChildren often ask, “What is deforestation?” and the answer is simple enough! It’s the cutting down of trees by humans or by natural causes which lead to reduced forest areas. More often than not, humans are the problem with their increased requirement of wood for construction, urbanisation and a host of other reasons and thus cause widespread deforestation across the world. Deforestation is converting wooded areas into farms, ranches or cities. Most deforestation activities benefit the production of other commodities like soy, palm oil, and other lucrative commodities. As of now, the most widespread deforestation is in the Amazon rainforest of South America.\nWhat are the Causes of Deforestation\nDeforestation can happen wherever there are dense trees and it is caused either deliberately, naturally or by accident. The common causes of deforestation are volcanic eruptions, avalanches, hurricanes, climate or temperature change, drought, severe insect infestation, disease, residential, agricultural, commercial or industrial land development, strip mining, logging, unsustainable forest management and human activities. The increased population is also a major factor as the more people, the more are the resources required. This is a very dangerous trend because deforestation is self-perpetuating which results in further deforestation. While it may appear that most deforestation occurs due to natural or accidental reasons, it is actually human activity that causes the most deforestation worldwide.\nEffects of Deforestation\nThe consequences of deforestation have been very far-reaching and impacted the environment and livelihood of many.\n- Forests take in carbon dioxide from the atmosphere and release oxygen with the Amazon rainforests of South America being responsible for 20 percent of Earth’s oxygen that we breathe in. When trees are cut down, the carbon dioxide is released back into the atmosphere, increasing the greenhouse gases in our atmosphere.\n- Trees help to control the level of atmospheric water and when they are cut down, there is less water in the air to return to the soil disturbing the water cycle. This results in dryer soil which will eventually stop supporting agriculture or ranching.\n- 80 percent of the animal and plant species inhabit forests and deforestation results in loss of habitat for many animal and plant species. Forest trees provide shelter and regulate light and temperatures. Cutting of trees results in temperature changes and increased sunlight which can prove disastrous for many plant and animal species as it results in endangerment or extinction of species.\n- The roots of various plants prevent soil erosion. Deforestation degrades the quality of soil as fertile topsoil gets washed away leaving the land barren and prone to flooding. Soil erosion perpetuates deforestation as new land is deforested to support crop growth or cattle grazing.\n- A lot of indigenous people rely completely on forests for their livelihood and they are adversely affected due to deforestation. Deforestation kills plants they may use for medicine and sustenance, drives away the animals, leaves them vulnerable to the elements and disrupts their life.\n- One of the major consequences of deforestation is that due to the absence of sufficient trees, severe climatic changes are visible.\nMeasures to Curb Deforestation\nThere are certain measures which can be adopted by people to reduce deforestation and reverse its effects, the obvious solution being to stop the human activities that are causing it. However, the increasing population of people has caused an increase in demand for the things grown on deforested land. If deforestation continues as it is at present, global warming and climate change will continue unabated and eventually lead to the destruction of life on earth.\n- Plant a tree whenever and wherever possible. Every tree that is planted reduces the effects of deforestation.\n- Start practising the concept of reducing, reusing and recycling in your daily lives and encourage others to do the same.\n- Since paper comes from trees, try reducing the use of paper in your daily lives. Don’t discard paper that has only been used on one side.\n- Spread awareness about the importance of afforestation and the dangers of deforestation.\n- Promote products which ensure reduced or no deforestation.\n- Do some research and find out about organisations which are actively fighting deforestation and its effect globally. This will give you an idea on ways to help slow and gradually stop deforestation. Organisations like WWF, IUCN and the Pachamama Alliance have been working with government officials, companies and communities to promote responsible forest management practices, reforming trade policies, combating illegal logging and protecting forests. They have been providing public and private organisations with tools and information that simultaneously enable human progress, economic development and protection of forests. They offer people the opportunity to learn, engage, travel, and cherish life so that there is a sustainable future for all.\n- Become an advocate of afforestation and become a subscriber or donor of an accredited organisation that is actively and responsibly working towards stopping unnecessary deforestation.\nDeforestation happens when large areas of forest lands are cleared for a variety of human activities. This is mainly because of the increasing population which requires more and more resources and commodities. Deforestation is mainly carried out for monetary gains and man’s greed but we pay a very heavy price with it. Around 18 million acres of forests are cleared each year for various reasons. Deforestation has disastrous effects on our environment like loss of habitat for many animal and plant species, global warming and climate change. But it’s not too late and there are several measures that can be taken to curb deforestation and its effects. If you are looking for more information on what are the causes of deforestation, its effects and the measures to curb it, check out the EuroKids website."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:2f4dd492-4e29-403c-8764-f0f8aa86e1a9>","<urn:uuid:41af8bea-27c3-46e2-ab90-a1d4c368d926>"],"error":null}