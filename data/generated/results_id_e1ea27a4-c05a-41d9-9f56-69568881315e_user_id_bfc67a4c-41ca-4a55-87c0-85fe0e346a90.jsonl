{"question":"I'm interested in the achievements of women pilots during WWII - what were their main responsibilities and accomplishments with the WASP program?","answer":"The Women Airforce Service Pilots (WASP) achieved remarkable results during WWII. Their primary responsibility was ferrying military aircraft between locations, but their duties expanded to include flying high-performance planes, fighters, and bombers. They also flew damaged combat planes needing repairs, conducted target practice missions by towing targets for ground soldiers' live ammunition training, transported secret documents, and carried special parts for the first atomic bomb. By December 1944, WASP pilots had flown 60 million miles of operational flights in 78 different types of aircraft, completed 12,650 ferrying operations, and delivered over 10,000 P-47s that were crucial to winning the war. They operated in extreme conditions, sometimes flying open-cockpit aircraft at 13,000 feet with temperatures as low as minus 45 degrees. The program maintained impressive safety records, though 38 WASP pilots lost their lives in service.","context":["Women in aviation were virtually unheard of in the early 1900s. Women’s primary role at that time was to take care of their families. Few considered leaving their traditional roles to take on new ones; less than 25 percent of American women worked outside the home. It required supreme sacrifice to rise above the common expectations of others, and reach to fulfill their dreams — some of which included flying.\nIn the midst of the world’s greatest depression, the world’s greatest war began and things changed dramatically. Suddenly there was a need for female workers in all aspects of society. Few hesitated to answer the call, wherever that might lead. Several brave women stepped forward to fly airplanes. Much to their dismay, it was not an easy venture.\nFlying was a province reserved solely for men, who believed that women were not emotionally stable enough to pilot airplanes. They were proved wrong. When America entered World War II with the Japanese bombing of Pearl Harbor, the need became great for everyone to support the war effort. The nation's sudden engagement in the war meant more opportunities for women.\nIn 1941 the federal government initiated a special educational program designed to increase the number of pilots. Through university-sponsored civilian flying schools and the Civil Aeronautics Authority (CAA), students could now learn to fly — with a ratio of 10 men to every woman. As the war intensified, more women signed up to fly airplanes, and later became known as \"Women of Courage.\"\nThe famous aviatrix, Nancy Love, organized a group of 28 experienced female pilots at New Castle Air Base in Wilmington, Delaware. Their title was the Women’s Auxiliary Ferrying Squadron (WAFS). At the same time, Jackie Cochran, winner of the 1938 Transcontinental Race, was in England with a team of 25 women flying for the British Ferry Command and the Air Transport Auxiliary (ATA). Most of those women were professional pilots and civilian flight instructors with more than 500 hours of experience.\nThe two groups merged in the summer of 1943 and were then called Women Airforce Service Pilots (WASP). It was not easy to become a WASP. The training program was rigorous and had high standards. After 24 weeks of stringent training, with tests in physics, math, aircraft, navigation, engines, and Morse code, the women trainees — if they endured — would receive their silver wings.\nOf the more than 25,000 women who applied, 1,830 were accepted, and only 1,074 received their pilot’s license. Although WASP was not officially military, it adopted the military lifestyle. Later, the WASPs were the first women to fly high-powered military aircraft.\nStill unrecognized by the public, women pilots were sometimes mistaken for mail carriers, doormen, stewardesses, or members of another country’s military. Despite the fact that they wore the official Air Force patch and their cuffs indicated the lieutenant's rank, it was hard for many to relate to a woman pilot.\nWomen pilots were mainly responsible for ferrying needed military aircraft from one point to another. That was extremely challenging because there were many different types of aircraft to fly. With the war in full force, the military now took a closer look at the WASP. Because of its competence and low accident rates, the government now expanded its duties.\nSoon the WASPs were not only delivering light-weight aircraft, but they were also flying the high-performance planes, including fighters and bombers, to where they were needed most. In addition, they were expected to fly combat planes in need of repair — some of which were unsafe to fly. The WASPs also provided target practice maneuvers by towing targets behind their planes while ground soldiers shot live ammunition at them. Because of the high risk, few male pilots took the target-towing jobs.\nTheir tasks also included transporting top-secret documents and special parts for assembling the world’s first atomic bomb. They also flew at night to help with tracking and searchlight missions. Even though they were not asked to go into combat, nearly all would have, if given the opportunity.\nAmong the dangers women pilots encountered was flying in open-cockpit aircraft, where temperatures would sometimes plummet to minus 45 degrees below zero at a harrowing 13,000 foot altitude. It was recorded that when they arrived at their destinations, they were so cold and stiff, they would have to be virtually lifted out of their planes. They earned tremendous respect as pilots.\nAs the war drew to an end, the goals of the WASP were evidently well met. By December 1944 it had flown an incredible 60 million miles of operational flights in 78 different types of aircraft, and had completed 12,650 ferrying operations. It had delivered more than 10,000 P-47s, which were instrumental in winning the war. President Roosevelt presented the Air Medal to WASP Barbara London for her exceptional ferrying skills.\nBy late 1944, veteran male pilots were returning home and the WASPs had compiled an outstanding record. Civilian male pilots felt threatened by what they saw was both groups' direct competition for civilian jobs. They lobbied hard for their views in Congress and invariably won their plea. The WASPs actually wanted to become assimilated into the regular air force, but they were summarily disbanded by Congress just prior to Christmas 1944.\nUnfortunately, the WASPs were not even thanked for their wartime contributions. Thirty-eight of their comrades perished in service to their country. It was a sad chapter in U.S. history; the government would not even pay for their bodies to be shipped home, or allow the American Flag to be placed on their coffins.\nIt was 34 years later when newspapers began to feature articles about how the Air Force would soon be training women to fly military aircraft for \"the first time in history.\" All those who had served as WASPs set out to correct the record, and achieve long-awaited and well-deserved recognition. Enlisting the support of Senator Barry Goldwater, a veteran pilot who had witnessed some of the feats by the WASP in World War II, and with the help of others, corrective congressional legislation was finalized in 1979. The WASP were granted military recognition and veteran status. America had finally recognized the exceptional women of courage.\nSee Important and Famous Women in America .\n---- Selected Quotes ----\nQuotes regarding Women Pilots.\nBy Amelia Earhart\nThe time to worry is three months before a flight. Decide then whether or not the goal is worth the risks involved. If it is, stop worrying. To worry is to add another hazard. It retards reactions, makes one unfit. Hamlet would have been a bad aviator. He worried too much.\n- - - Books You May Like Include: ----\nPearson Field: Pioneering Aviation in Vancouver and Portland by Bill Alley.\nPearson Field, part of the Vancouver National Historic Reserve, is one of the oldest continually operating airfields in the United States. From the fi..."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:3dd23633-9936-4469-87ec-17f04c351e77>"],"error":null}
{"question":"Why are portable web documents inherently limited in their security capabilities compared to hosted web content?","answer":"Portable web documents are inherently limited because the web's security model gives capabilities based on how trusted a document's origin is. Since portable documents don't have a trusted origin, they must be given extremely limited capabilities - even more limited than regular unencrypted websites - to maintain security. This cannot be bypassed without compromising the security model.","context":["(I’m largely thinking out loud with this and noting this down for myself, so feel free to ignore. Also, most of the following is extremely simplified. The actual security issues involved can get quite a bit more complicated.)\nTo prevent the spread of malware modern client systems refuse to run code without safety assurances.\n- Web pages: “Does this code come from a URL in the browser’s list of unsafe URLs?” (Origin blacklist.)\n- Web pages, again: “Does this code come from the same origin as the assets and code it is trying to access?” (An origin whitelist: ‘just this source’.)\n- Web pages, yet again: “Does this code come over an encrypted connection so I can be sure it hasn’t been tampered with?” (A whitelist of origins that are more trusted.)\n- Browser extensions and mobile apps: “Does this code come from the system app/extension store?” (Origin whitelist.)\n- Mac OS X with gatekeeper: “Is this code signed with a key from a trusted central authority.” (Developer whitelist.)\n- Desktop OS with anti-malware software: “Does this code look and behave like malware?” (Heuristics, leads to frequent false positives. Anti-malware software also uses whitelists and blacklists and often several different heuristic engines and malware still gets through.)\n- AMP CDNs: “Does this only use the pre-approved and vetted code I provided?” (Code whitelist.)\n- Legacy desktops: “Whatever. Just run it.” (Extremely unsafe, obviously.)\n(Historically, well-implemented whitelist or whitelist-oriented security models tend to be safer than blacklists or heuristics. All else being equal.)\n1. User overrides\nIn most cases the end user can override the system and run the code regardless. Some only with great effort (iOS jailbreaking). Some with relative ease (Android side-loading). Some with no difficulty at all (Windows XP everything).\nThe easier it is for the user to force unsafe code to be run, the more likely the system is to become infected. This has solidly been confirmed through observation and data.\nThe countervailing force is the platform’s median level of user expertise. The higher it is, the less precautions the system has to take and the more options it can offer for overriding the precautions. And since ecosystems that require high expertise tend to be small, the gain in targeting them often does not offset the difficulty for malware makers. This is not a panacea. As soon as the perceived value of the ecosystem as a target increases, the whole thing becomes a sitting duck.\nTying overrides to expertise in some way isn’t as useful as you’d think since people can memorise complex activities without understanding them. That is, if the user thinks they’re being inconvenienced by a security precaution, they can usually find step-by-step guidelines to how to override them, provided the overrides are available, and with use those overrides become muscle memory or habit. No understanding or expertise is required. This means that a system that intentionally allows the user to override precautions (as opposed to unintentionally as with iOS jailbreaks) will become more and more porous over time.\nIf the balance of security precautions and user expertise is wrong the system’s ecosystem gets hit with a ‘malware tax’ which is the cost the users incur in making their systems safe. If that malware tax is too high, large parts of the system’s ecosystem become unsustainable which results in a loss of diversity and leaves a lot of potentially valuable use cases unaddressed.\n2. The web’s security is built on origin\nThe web’s basic model of safety is this:\nDoes this come from a location I trust and, if so, how much can I trust it?\nIt measures that trust in multiple ways:\n- Same origin.\n- Extension store whitelists.\n- Browser malware blacklist.\n- Encrypted connections are trusted more as they prevent man-in-the-middle attacks and have limited ownership assurances based on the admittedly flawed SSL certificate system.\nThe only way to get the full set of capabilities is to come from an origin with the highest level of trust. Extensions can access more capabilities than encrypted websites which in turn have more than unencrypted websites. Even when a site is run offline (e.g. with service workers), it is strongly bound to its origin (origin is a basic part of the service worker security model).\nOrigin as a measure of security is a basic assumption of most, if not all, web standards.\n3. Publishing is a low expertise environment\nThis is both in terms of in-house technical expertise and—more importantly—in terms of end-user expertise. Publications should require even less expertise of the end-user than an iOS app. Expecting your average reader to have more technical know-how and a better understanding of software security than your average iOS user is a non-starter in multiple ways.\nJust as important is the lack of technical expertise within the publishing industry. An extremely common pattern among publishers and imprints is to only have a single in-house digital staff member who manages and oversees the outsourcing of all work related to digital.\nWhich leads us to the tech brain-drain the industry is experiencing at the moment. A very solid proportion of the tech people I knew in the industry a year ago are no longer working in publishing proper. And flow keeps increasing. Publishers are reacting to the ebook slowdown by cutting down on digital staff.\nAdd to that the universally lower salaries throughout the publishing industry and you get an environment where technical expertise is limited to much smaller and less influential clusters than in comparable industries.\nThis isn’t an environment that’s conducive for making nuanced decisions on software security, especially given the possible repercussions of implementing an unsafe ecosystem. A ‘malware tax’ could easily kill off the entire digital publishing ecosystem, especially when its nearest competitors (non-portable websites and mobile phone apps) have a relatively low ‘malware tax’.\n4. Portable web publications are… difficult\nPortable Web Publications, the idea that a web page can be seamlessly transformed into document with the same portability characteristics as that of an ebook (i.e. a packaged file that users can give to each other), is not-so-subtly being presented as the next generation saviour of the obviously broken and dysfunctional ebook ecosystem.\nThe problem with this vision is this:\nIt assumes a symmetry in capabilities and behaviour between the hosted document (i.e. with a HTTP or HTTPS origin) and the portable document. The web’s inherent security model dictates that the only safe portable document is one that’s severely restricted in its capabilities. And the publishing industry’s user expertise requirements dictate that there cannot be any exceptions to these restrictions.\nThe web’s model of security is to give documents capabilities in proportion to how trusted their origin is. For documents whose origin cannot be trusted because it is a fully portable document, the only safe and compatible option is to give it extremely limited capabilities, much more limited than even those given to a regular unencrypted website.\nThere aren’t many ways around this and none that seem viable to me.\n- A new security model from scratch, making sure that it slots neatly in as a replacement for the origin model.\n- An app store-style whitelist: only publications from specific sources get full capabilities. This is what browsers do for extensions.\n- A publisher whitelist (the Mac OS X gatekeeper model): only publications signed by a key from a central authority have full capabilities.\n- An origin whitelist: only give full capabilities to portable documents whose code is identical in every way to code that’s available from a trusted source.\nThe first option is impractical given the sheer amount of work involved and the high potential for error (a single design mistake dooms the entire system to be unsafe forever).\nThe second and the third are impractical as limiting capabilities to central authorities completely destroys the utility of such documents and ties the ecosystem to silos; it becomes no better than Apple’s app store or Amazon’s Kindle store.\nThe last option turns portability into a joke; it means you can only get full capabilities if there’s a network connection and accessible origin during install which makes them functionally indistinguishable from progressive web apps, except more complicated and more error prone.\nWhich leads us to…\n5. What model would work?\nIf full and unrestricted capabilities are not an option for portable documents the question becomes one of how we should restrict them. You’ll note that solutions 1-3 are the ones ebook reading systems have tended to opt for (ebooks are, in essence, a dysfunctional subset of portable web documents).\n- Block all network access. No cross-origin requests. No WebRCT. No image hot-linking. No nothing. EPUB3’s current model of only allowing an immutable whitelist of media assets (nothing dynamic) would work. Given how easy it is to get data out of a container like this using nothing but social engineering, combining this solution with solution 2 might be necessary.\n- Limit storage. Depending on what other restrictions are in play this could be anything from ‘no persistent storage’ (like a browser’s incognito mode) to storage that is either limited severely in size or duration or both.\nI personally favour option 4 since it is implicitly evolvable and postpones standardisation and the bureaucracy that involves indefinitely. It’s a model that allows for plurality (each niche/market can defined its own set of custom elements for behaviour) and explicitly does away with the idea that one solution can possibly fit all. It would inevitably lead to a diverse set of pseudo-formats. Each of which would target a specific use case but could still be run in a browser. And the browser would not have to implement any support or understand the format itself as anything other than standard HTML.\nOptions 1-3 are still viable but need to be clearly defined within the spec (which took ages for EPUB3 to clear up, for example) and require proper implementation by the client systems themselves, most of which are going to be outside of the publishing ecosystem’s sphere of influence (browser vendors, OSes, major reading systems). Also, experience with EPUB3 reading systems should show us that implementing those options is error- and bug-prone.\nThe state of implementation in publishing both on the publisher side and on the reading system side is abysmal. Publisher ebooks barely use EPUB3 features and certainly aren’t idiomatic HTML (constructs like\n<p class=\"h1\"> are the norm). Reading system EPUB3 support is spotty and very, very buggy. Therefore any path that requires pre-existing actors to suddenly become much better at their job than they have been to date has a much lower chance of success than a path that does not.\nThe only path that has a realistic chance of long-term success is community- and implementation-driven code where each use case is addressed separately by the people who need it.\nPremature standardisation, i.e. standardising when the community isn’t yet (or shouldn’t yet be) sure what needs to be done or how, is an extremely risky path to take. Especially when you consider that ebook standardisation has, to date, been nothing short of an abject failure.\n(The above is just off the top of my head. Notes for myself hammered out over coffee this afternoon. Feel free to ignore or disregard.)\nIf you care about portable web documents working in web-based systems solutions 1-3 are not viable.\nAnd we all saw how well not caring about implementation worked for EPUB3?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e9c8561b-78b1-4bc0-b30b-33c4ca169cec>"],"error":null}
{"question":"What challenges do transit systems face in implementing technological features versus passenger amenities?","answer":"Transit systems face complex technological implementation challenges, with thousands of unique communications points requiring integration, including train tracking, signaling, emergency communications, gas monitors, equipment failure monitors, ventilation control, fire alarms, and CCTV. While individual components aren't expensive, their installation, integration, and testing can significantly extend project timelines. Additionally, specifications often become obsolete during long construction periods. On the passenger amenity side, even simple improvements like seating at bus stops face challenges - for instance, the RTS network has thousands of bus stops with limited resources to provide seating. Reconnect Rochester addressed this by developing bus stop cubes, but implementation required years of testing and development, moving from seasonal wooden models to permanent fiberglass solutions.","context":["Anyone who has ever used public transportation in Rochester is painfully aware of two things: At some point you will have to wait for your bus, and when you do, you will probably be standing.\nFor senior citizens, people with disabilities, and parents with young children, being made to stand for any length of time can be less than ideal. Even for those passengers who are physically capable of standing, having no place to sit while waiting on the side of a busy roadway can cause anxiety and discomfort.\nWhy is our bus system the only transportation mode that requires its passengers to stand while waiting for the service? The single biggest issue is the sheer scale of the system. There are thousands of bus stops in the RTS network, and the resources of the transit authority are already spread thin.\nIf this issue could be remedied, not only would we make the lives of current riders a little easier, but we might also encourage more people to use public transportation. This is why Reconnect Rochester has decided to make bus stop seating a priority for our community.\nIn 2014, Reconnect Rochester set out to find a solution. What we came up with was a design for a bus stop seat that is a simple 2’x2’x2’ cube. Our bus stop seating cube comes in 4 primary colors (red, green, yellow, and blue) that add beautification and brightness to the street landscape. The compact size allows the seat to fit easily within areas where space is at a premium – such as tree lawns or that little bit of space between the street curb and sidewalk.\nIn 2017, after 3 years piloting seasonal bus stop cubes made from high-pressured wood, Reconnect Rochester set out to find a permanent, year-round amenity for bus riders. In our research, we came upon a local manufacturer of fiberglass — a nearly indestructible, weather resistant material that was perfect for the job!\nIn September 2020, Reconnect Rochester installed the first 15 fiberglass cubes on Parsells, Lyell and Monroe Avenues (read more in this blog post). Stay tuned to our blog and social media for updates on our current efforts.\nCubes for Your Community\nReconnect Rochester will continue to work with RTS, local municipalities and community organizations throughout Monroe County to add bus stop cubes at stops that are well utilized but lack seating.\nWould you like to see bus stop cubes at stops in YOUR neighborhood or community? Contact us and we’ll do our best to work with you to secure funding and make it happen.\nAre you from outside the Monroe County area and interested in purchasing bus stop cubes for your town or city? Contact us and we’ll put you in touch with the manufacturer. Reconnect Rochester receives a sales commission that helps fuel our effort to put more bus stop cubes on the ground locally.\nThe Bus Cube Birth Story\nThe bus cube was born in 2014, when Reconnect Rochester set out to come up with a temporary solution to the dearth of seating at local bus stops. Here’s how we did it…\nWe spent countless hours brainstorming. We scoured the internet. And we even met with a local furniture designer, Staach (we really admire the way those guys balance form, function, and sustainability). But we needed something that would be relatively inexpensive and easy for regular people like us to build and duplicate. It would also need to be compact, sturdy, and weather resistant.\nWe could have simply taken a page from the guerilla bus stop seating playbook and chained a plastic patio chair to a bus stop sign, but to be honest, we’re not fans of plastic furniture. And we really didn’t think the neighbors would appreciate this look very much.\nThen one day, almost like it happens in the movies, the solution hit us like a lightning bolt…good old-fashioned children’s blocks! It’s amazing how sometimes the best ideas are inspired by the simplest things. Children’s blocks. Durable, easy to use, easy to construct – and what could possibly be more fun? Quite fitting for Rochester, the home of the National Toy Hall of Fame!\nWe put pencil to paper and designed a simple 2’x2’x2’ cube. The compact size allows the seat to fit easily within areas where space is at a premium – such as tree lawns or that little bit of space between the street curb and sidewalk. Our prototype was constructed using pressure-treated lumber and decking materials for a total cost of about $100 per cube.\nWe tested the prototypes at two locations within the city of Rochester: The PriceRite at Dewey & Driving Park and N. Union St. at the Public Market. The results were very positive. Interviews with transit riders and passersby can be viewed in this video.\nThe idea quickly won community support as well as accolades from RTS which encouraged the effort. Over the next three years (2014 – 2017), in partnership with the City of Rochester, Flower City Habitat for Humanity and many neighborhood and community organizations, we built and placed a fleet of over 30 bus stop cubes at bus stops all around the city.\nThe seasonal cubes go out on the street in May and are brought back in and stored in October. As the fleet grew, the job performed by Reconnect Rochester volunteers of placing, removing and storing the cubes each season, became harder to manage. That’s when we decided it was time for a permanent, year-round solution.\nIt took about three years (2017 – 2020) of stops-and-starts to research, design and manufacture the fiberglass model that you see today. But we’ll save THAT story for another day.","Technological advances have improved the ability to monitor, control and manage operational and safety performance of transit systems. However, they have significantly added to the complexity of projects, particularly tunneling projects. For example, a transit line project can have thousands of unique communications points that are transmitted and report in some manner to a remote location, such as a control center. These include train tracking, signaling, emergency communications devices, intrusion alarms, gas monitors, failure monitors on myriad types of equipment, ventilation control and monitoring, fire alarms, and CCTV.\nWhile not expensive as stand-alone elements, their installation, integration, and testing can add significant time to rail projects. Many of these subsystems, like fire alarms, must be connected in order to work. Activation of a fire alarm in a station affects operational functions of elevators, escalators, messaging systems, station ventilation and alarm reporting. Each interface must be tested for each alarm, of which an underground station can dozens.\nFurther, given the long time to complete rail projects, specifications for advanced communication systems are often obsolete by the time they are ready to be installed toward the end of a project. This can result in change orders with schedule impacts if upgrading to a more modern standard. Many of these technological requirements are driven by fire and safety codes that are unique to rail projects, discussed in detail below.\nFire Safety Standards\nRail transit stations, particularly below ground, are also subject to safety regulations. The U.S.-based National Fire Protection Association (NFPA), an independent global trade group, publishes safety and fire codes for a range of facilities, including rail transit systems under NFPA 130 Standard for Fixed Guideway Transit and Passenger Rail Systems.258 NFPA 130 is not federal law, but it has been formally adopted by many jurisdictions and agencies as part of their fire safety codes for rail transit construction. While some countries like Spain, France, Japan, Italy, Germany and Austria have their own fire safety standards for transit, most agencies around the world follow NFPA 130.259\nNFPA 130 largely consists of performance-based criteria for ventilation, fire endurance and spread, and evacuation, but also include specific provisions for materials, distances between exits, spacing of stations and cross-passageways, and doors, among others. For example, one part of the code that has direct implications for the scope of subway stations, and thus costs, is riders standing on a platform must be able to evacuate the station within four minutes and reach a safe location within six minutes.260\nThe code also sets parameters for modeling evacuation scenarios. These evacuation times are based on peak service, with trains one headway behind schedule, resulting in twice the normal passenger load on vehicles and twice as many passengers on a platform.261 Additionally, evacuation scenarios assume that one escalator on each station level is out of service, and that the escalator chosen must be the one that would most negatively impact passenger exit capacity.262 Escalators generally cannot make up more than half of a station’s egress capacity on each level.263 This is intended to ensure that evacuation can be completed even in a worst case scenario.\nOne of the more significant determinants of station platform size are NFPA 130 requirements on the number and width of stairs, as well as the maximum permissible distance from the most remote points of the platform to the nearest exit.264 As a result, station and platform sizes often comfortably exceed the levels that would be necessary to handle normal passenger flow rates. While intended to ensure space for evacuation, meeting these strict standards can lead to a more comfortable passenger experience.265\nOther standards that may impact station costs or elements include provisions for the inclusions of cross-passages to allow for passengers to move between tunnels in case of emergency and, for example, if one tunnel has smoke. According to NFPA 130, if the distance between two stations is greater than 2500 feet, cross passages must be built between the tunnels at 800-foot intervals if there are no intermediate shafts to the surface.266 According to one analysis, cross passages are rare in Europe as well as in Japan.267 This is likely in part due to the relatively close spacing an d travel time between stations that may allow passenger to walk a short distance to evacuate, and reducing the likelihood that a train would get caught in the middle of a tunnel and unable to drive to the next station.268 Constructing cross-passages can require additional excavation and complexity that may affect construction costs.\nVentilation systems that can bring fresh air to underground passengers during a safety incident is also a major element of underground metro systems. NFPA 130 requires mechanical and passive ventilation systems to become fully operational within 180 seconds, and maintain airflow rates for at least one hour to allow for evacuation of vehicles.269 Design of ventilation systems also accommodate the maximum number of trains possible between ventilation shafts during an emergency.270\nTransit systems in earthquake prone areas also must comply with seismic safety guidelines. At and above ground systems are particularly vulnerable to ground movement from earthquakes while underground transit systems largely move with soil in the event of an earthquake and are generally safer.271\nSeismic codes for transit are largely handled at the local or a gency level, though there are certain statewide and federal guidelines that agencies may incorporate into their design standards.272 For example, Seattle’s Sound Transit adopted agency-wide seismic standards that take a hazard-based approach to earthquake resilience. These approaches include planning for an Operating Design Earthquake (ODE) this strength over a facility’s 100 year design life. The other is a Maximum Design Earthquake (MDE), which would be expected to occur once every 2500 ye ars, with a 4 percent chance of an earthquake exceeding this level during a facility’s design life. Sound Transit’s guidelines require light rail facilities to withstand ODE’s and resume operations in a “reasonable amount of time,” and withstand a MDE without collapsing or risking lives.273\nMeeting such standards can vary depending on the seismic profile of varying regions. For example, San Francisco’s Bay Area Rapid Transit (BART) strengthened its standards over the past decades and are undertaking vulnerability analyses and retrofitting key facilities to enhance their earthquake resilience. These measures include enlarging tunnels that cross through faults to account for potential displacement and incorporating concrete-encased steel ribs.274 Aerial structures are reinforced with stronger foundations or columns to withstand collapse or poor soil is replaced with non-liquifiable soil to prevent collapse or damage.275\nTransit stations are also subject to accessibility requirements under the Americans with Disabilities Act of 1990 (ADA). Design specifications for accessibility are outlined under Title II and III of the ADA, also known as ADA Accessibility Guidelines. Enforced by both the federal departments of Justice and Transportation, these guidelines cover vehicles, buildings, transportation facilities, and many other types of facilities. The U.S. Access Board, a federal government agency, writes all code/guidance and has issued supplements to cover different facilities. The ADA guidelines were last updated in 2004 to address usability and format issues, as well as cover new types of facilities. The U.S. DOT formally adopted these new standards in 2006.\nAmong the DOT-specific guidelines for transit include locating accessible routes in the same area as general circulation paths, including detectable warnings on curb ramps and along platforms that do not have screen doors or platform guards, minimum platform heights, and maximum rail platform slopes.276 DOT has added to these standards over time. For example, in September 2011, DOT added a provision mandating that individuals with disabilities, including wheelchair users, “must have access to all accessible cars available to passengers without disabilities in each train using the station”, to prevent segregating disabled riders in separate vehicles.277 These standards apply to all new construction, as well as alterations to existing facilities.\nThe ADA requires that any alterations to existing facilities make them fully ADA compliant, or to the maximum extent feasible in cases where full accessibility is not possible. If making a facility fully accessible would exceed 20 percent of the alteration cost, agencies are only required to incorporate accessibility elements that would not result in a disproportionate cost (under 20 percent).278\nA U.S. DOT 2016 ruling clarified that any alterations to existing transportation facilities that can impact their usability must incorporate accessibility, including for wheelchair users.279 The ruling also clarifies that the ADA requirement to incorporate accessibility to the maximum extent possible is primarily intended for rare cases where it is impossible to make an existing facility fully ADA compliant. In these cases, agencies cannot cite disproportionate cost as a limiting factor preventing incorporation of accessibility. The disproportional cost provision applies only in instances where a primary function area of a station (such as a platform) is being renovated.\nCoverage of the impact of ADA compliance on construction costs has largely revolved around elevator retrofits on older subway systems. The cost of retrofitting elevators has gained particular attention in New York City. Only 23 percent of New York MTA’s subway stations are accessible, and the agency has retrofitted several stations without installing elevators or ramps.280 A 2019 lawsuit ruled that the agency violated the ADA by not installing elevators as part of a 2013 subway station renovation in the Bronx, and must make stations accessible when renovating future stations.281 The agency announced a $5.5 billion capital program in 2019 to install elevators in 70 stations in five years.282 The plan received increased scrutiny for its cost—nearly $78 million per elevator, in contrast to examples from European cities, where station upgrade costs per elevator are as low as $22 million.283 These costs are also lower in other North American cities like Boston, where the MBTA installed three new elevators and two escalators at a Red Line station for $36 million, and Chicago, where a new station with four elevators cost $75 million ($19 million per elevator).284\nAccessibility regulations abroad are largely handled at the country level, but generally all stations built in recent decades are designed to be accessible. Transportation systems in Canada are governed by the newly enacted Accessible Transportation for Persons with Disabilities Regulations (ATPDR), as well as the 2018 Accessible Canada Act, which is the first nationwide accessibility act.285 Provinces also have their own accessibility regulations that apply to public entities, like the Accessibility for Ontarians with Disabilities Act.286 Public transportation in Australia is similarly governed by the national Disability Discrimination Act of 1992, which includes design and service standards for public transport similar to the ADA.287\nThere are no European Union-wide accessibility standards comparable to the ADA, but rather individual member state regulations. The European Accessibility Act, passed by the European Parliament in 2019, largely focuses on fare payment systems and does not explicitly address system design.288 Accessibility on European transit systems can vary significantly. In Barcelona, 143 out of 158 metro stations (81 percent) are accessible, while just under 20 percent of stations on the London Underground are accessible.289 Just three percent of stations on the Paris Metro, for example, are accessible to passengers with disabilities, while the much newer tram system is fully accessible.290 While France passed a law in 2005 to improve accessibility in public spaces, Paris’ Metro was exempt, and its operator has argued that the system’s age would make retrofitting stations extremely costly.\nDesign and architecture can be significant cost drivers for transit projects in three ways: poor management of the design processes, project design itself, and design standards. Lack of oversight of the design process can result in accepting inadequate or faulty designs that result in issues during construction and require change orders. The design of transit projects themselves, particularly on underground stations, can also raise construction costs. Deep, extravagant stations and the use of bespoke materials have been cited as major cost drivers in cities like New York and Toronto. Lastly, select safety standards can require more complex system design to make a project resistant to natural disasters like earthquakes. Stringent evacuation standards in fire safety codes like NFPA 130 can also result in large subway s tations, while the need to install cross-passages and ventilation systems can be an additional source of costs. Accessibility standards, on the other hand, do not appear to be a particularly significant cost driver for new construction, though accessibility retrofits of older station in New York City have received scrutiny for th e high costs of elevator installations compared to other cities."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:8c0f0c5d-b054-4819-bc2a-31c2ee4f7f40>","<urn:uuid:26460705-36ea-4986-9de6-6b55a86b4778>"],"error":null}
{"question":"How do lysosomes and vacuoles differ in their waste management functions within cells?","answer":"Lysosomes and vacuoles have distinct waste management roles. Lysosomes act as cellular recycling centers, containing digestive enzymes that break down large molecules like proteins, polysaccharides, fats, and nucleic acids, as well as failing organelles. They provide a safe compartment for digestion without releasing enzymes into the cell itself. Vacuoles, on the other hand, are sacs that bud from the ER, Golgi, or plasma membrane, coming in different sizes with varied functions. In plant cells, the central vacuole can occupy over half the cell volume and primarily stores organic nutrients like proteins, rather than focusing on waste breakdown.","context":["Home > Preview\nThe flashcards below were created by user\non FreezingBlue Flashcards.\n-characterize the organisms of the domains Bacteria and Archaea, known as Prokaryotes\n-Organisms of the domain Eukarya-Protists, plants, fungi, and animals\n- Unlike Prokaryotic Cells:\n- -have organelles\n- -more complex\n- -A thin outer membrane that bounds the cell.\n- -Regulates the traffic of molecules between the cell and its surroundings\n- -composed of mostly lipids and proteins\n- -All cells have this\n- -Tiny structures that build proteins according to instructions from DNA\n- -responsible for protein synthesis\n- -in the eukaryotic cells, the components of ribosomes are made in the nucleus, then transported through the pores of the nucleus into the cytoplasm, making proteins that remain in the fluid\n- -other ribosomes are attached to the outside of an organelle called the Endoplasmic Reticulum, making proteins that are incorporated into membranes or secreted by the cell.\n- -\"little organs\"\n- -membrane-enclosed structures that perfrom specific functions.\n- -Most important is the nucleus\n-Houses most of a Eukaryotic Cell's DNA and its surrounded by a double membrane.\n- -The entire region of the cell between the nucleus and plasma membrane\n- -In a Eukaryotic cell it consists of various organelles suspended in fluid\n- -Only in plant cells\n- -organelles that convert light energy to the chemical energy of food\n- -two fatty acid tails and a phosphate head (combo of phosporus and oxygen)\n- -phosphate head makes it hydrophilic on the head part only and it is still hydrophobic on the fatty acid tails.\n- -makes phospholipid bilayer when it forms two layers of phospholipid on the cell wall (inner and out)\n-a membrane is a fluid mosaic because the molecules can move feely past one another and a mosaic becaue of the diversity of proteins that float like icebergs in the phospholipid sea.\n- -the sticky coat most animal cells secrete\n- -this layer holds cells together in tissues\n- -Most animal cells structures contain these\n- -structures that connect to other cells\n- -allow cells ina tissue to function in a coordinated way\n- Double membrane that borders the nucleus\n- -each membrane of the nuclear envelope is similar in structure to the plasma membrane\n- -pores in the envelope allow certain materials to pass btwn the nucleus and the cytoplasm.\n- -fibers made from long DNA molecules and associated proteins within the nucleus\n- -each chromatin fiber constitues one chromosome\n- -make up chromatin\n- -the number of chromosomes ina cell depends on the species (ex: humans= 46)\n- -prominent structure within the nucleus\n- -site where the components of ribosomes are made\nHow DNA directs Protein Production\n1. Dna programs protein production in the cytoplasm by transferring its coded information to a molecule called messenger RNA (mRNA). Like a middle manager, the RNA molecule then carries the order to \"build this type of protein\" from teh nucleus to the cytoplasm.\n2. The mRNA exits through pores in teh nuclear envelope and travels tot he cytoplasm, where it then binds to ribosomes.\n3. As a ribsome moves along the mRNA, the genetic message is translated into a protein with a specific amino acid sequence\n-this system includes the nuclear envelope, the endoplsmic reticulum, the Golgi Apparatus, lysosomes, and vacuoles.\nEndoplasmic Reticulum (ER)\n- -one of the main manufacturing facilities wihtin a cell\n- -produces an enormous variety of molecules\n- -connected to the nuclear envelope\n- -2 components make up the ER: rough ER and smooth ER\n- -roughness is due to ribosoms that stud the outside of the ER membrane.\n- -these ribosomes produce membrane protiens and secretory proteins.\n- -some newly manufactured membrane proteins are embedded right in the rough ER membrane, so it creates new membrane and also secrete protein outside of hte cell\n- -some products manufactured by the rough ER are dispatched to other locations in the cell by means of transport vesicles (membranous spheres that bud form the rough ER)\n- -this organelle lacks the ribosomes that populate the surface of rough ER\n- -performs many functions because of the diversity of enzymes build into the smooth ER\n- -refinery, warehouse, and shipping center\n- -recieves, refines, stores, and distributes chemical products of the cell\n- -products made in teh ER reach the Golgi in transport vesicles\n- -a sac of digestive enzymes found in animal cells\n- -absent from most plant cells\n- -develop from vesicles that bud off from teh Golgi\n- -Enzymes within a lysosome can break down large molecules such as protiens, polysaccharides, fats, and nucleic acids.\n- -the lysosome provides a compartment where the cell can digest these molecules safetly, without unleashing these digestive enzymes ont he cell itself\n- -sacs that bud from the ER, Golgie, or plasma membrane\n- -come in different sizes and have variety of functions\n- -central vacuole: can account for more than half the volume of a plant cell\n- -cv of a plant cell stores organic nutrients, such as proteins stockpiled in the vauoles of seed cells. also contributes to plant gorwth by absorbing water and causing cells to expand.\n- -unique to photosynthetic cells of plants and algae\n- -perform photosynthesis\n- -partitioned into 3 major compartments by internal membranes\n- -one compartment is grana: thick fluid within the chloroplasts, chloroplats solar packs, trap light energy and convert to chemical energy.\n- -sites of cellular respiration, a process that harvests energy from sugars and other food molecules and converts it to another form of chemical energy called ATP.\n- -cells use molecules of ATP as the direct energy source for most of their work.\n- -found in all eukaryotic cells\n- -an envelope of 2 membranes encloses the mitochondrion\n- -inner membrane has numerous infoldings called cristae many enzymes and other molecules that function in cellular respiration are built into the innermembrane so by increasing the surface area of this membrane, the cristae maximize ATP output.\n- -cells infrastructure\n- -network of fibers extending throughout the cytoplsm.\n- -serves as both skeleton and \"muscles\" for the cell, so support and movement\n- -contains several types of fibers made from diff types of protein like microtubles: straight, hollow tubes composed of proteins.\n- also intermediate filaments and microfilaments that are thinner and solid.\n- -eukaryotic flagella propel the cell by an undulating whiplike motion.\n- -ex is sperm tail\n- -generally shorter and more numerous than flagella\n- -promote movement by a coordinated back and forth motion\n- -like flagella, propel various protists through water and also core of microtubles wrapped in an extension of the plasma membrane.\n- -in your windpipe to clean mucus trapped in your lungs","Lysosomes could be called cells’ recycling centres because they digest and recycle waste inside the cells.\nWhy are lysosomes often referred to as the recycling centers of the cell?\nWhy are lysosomes often referred to as the “recycling centers” of the cell? The enzymes within the lysosome break down failing organelles and other structures within the cell. What is the function of plant vacuoles?\nDoes lysosome recycle materials?\nAs most high schoolers learn, the lysosome carries out waste disposal and recycling. In a process known as autophagy (meaning “self-eating”), it takes in old cellular components and unneeded large molecules, such as proteins, nucleic acids and sugars, and digests them with the help of enzymes and acids.\nHow lysosomes function as a waste disposal system explain?\nAnswer: Lysosomes break down and enzymes are released freely in damaged cells, old cells, dead cells, or cell organelles that do not work to digest them. In these processes, they remove cellular debris. Therefore, they are also called Cellular Waste Disposal Systems.\nWhat organelles are the recycling center of the cell?\nCells also have to recycle compartments called organelles when they become old and worn out. For this task, they rely on an organelle called the lysosome, which works like a cellular stomach.\nWhich are recycling centers for the cell quizlet?\nThe Golgi apparatus packages molecules processed by the endoplasmic reticulum to be transported out of the cell. These organelles are the recycling center of the cell. They digest foreign bacteria that invade the cell, rid the cell of toxic substances, and recycle worn-out cell components.\nWhy is recycling important in cells?\nRecycling, the reuse of material, saves energy and resources. … FERARI distributes the recyclable molecules, mainly transport proteins and receptors, and reintroduces them into the cellular cycle. In this way, valuable cell components do not have to be constantly produced anew, which not only saves energy but also time.\nHow do lysosomes break down materials?\nLysosomes break down macromolecules into their constituent parts, which are then recycled. These membrane-bound organelles contain a variety of enzymes called hydrolases that can digest proteins, nucleic acids, lipids, and complex sugars. The lumen of a lysosome is more acidic than the cytoplasm.\nWhy do we need lysosomes?\nA lysosome is a membrane-bound cell organelle that contains digestive enzymes. … They break down excess or worn-out cell parts. They may be used to destroy invading viruses and bacteria. If the cell is damaged beyond repair, lysosomes can help it to self-destruct in a process called programmed cell death, or apoptosis.\nWhat do lysosomes look like and do?\nLysosomes appear initially as spherical bodies about 50-70nm in diameter and are bounded by a single membrane. Several hundred lysosomes may be present in a single animal cell. Recent work suggests that there are two types of lysosomes: secretory lysosomes and conventional ones.\nWhy are lysosomes known as a waste disposal system of the cell B suicide bags of the cell?\nLysosomes are known as suicide bags of the cell because they contain lytic enzymes capable of digesting cells and unwanted materials. … This causes the hydrolytic enzymes to be released. The released enzymes then digest their own cell, causing the cell to die.\nWhere does lysosome waste go?\n(Otherwise, if a lysosome were to leak or burst, the enzymes could kill everything in the cell.) When a lysosome comes across cellular debris it can’t reuse, it fuses with the cell membrane and dumps the waste out of the cell in a process called exocytosis.\nWhich is known as waste disposal of cell?\nLysosome. Lysosomes act as the waste disposal system of the cell. They have hydrolyzing enzymes which can digest biological substances.\nDo lysosomes destroy and recycle old organelles?\nLysosomes are organelles that digest and dispose of unwanted protein, DNA, RNA, carbohydrates, and lipids in the cell. … Aside from breaking down unwanted molecules, and even other organelles, its recycling function is at the center of a process called autophagy, in which the cell digests itself.\nWhat type of cells contain lysosomes?\nlysosome, subcellular organelle that is found in nearly all types of eukaryotic cells (cells with a clearly defined nucleus) and that is responsible for the digestion of macromolecules, old cell parts, and microorganisms."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:360e727c-6f7e-4179-860b-73331ea796f6>","<urn:uuid:f163f9b3-b051-4780-a57f-db3ab6e34db2>"],"error":null}
{"question":"Compare the challenges of atmospheric distortion for ground-based telescopes versus spacecraft re-entering Earth's atmosphere.","answer":"Ground-based telescopes face atmospheric distortion that causes stars to twinkle and blocks certain wavelengths like ultraviolet, gamma and X-rays. For spacecraft re-entry, the atmosphere presents different challenges - the craft must enter at a precise angle (not too steep or shallow) and requires thick heat shields to avoid burning up due to atmospheric drag. While newer ground telescopes use technology to correct atmospheric distortion, spacecraft like the Space Shuttle required specific thermal protection systems and trajectories to safely navigate atmospheric re-entry.","context":["Can a glider be dropped from geosynchronous or other orbits and safely land?\nIt depends what you mean by \"glider\".\nIf you mean any craft that trades height for horizontal movement while flying without engines then yes. See the space shuttle.\nIf you mean a craft designed to fly without engine power while losing as little height as possible and exploiting thermals then no.\nThe problem is as you lose height you gain speed and when there is no significant atmosphere you have no good way of getting rid of that speed. So you inevitably hit the atmosphere extremely fast. The difficult part of coming back from space is losing speed to atmospheric drag without your craft burning up or ripping itself apart in the process.\nTo reenter in one peice requires thick heat shields to avoid burning up and a compact shape and robust construction to avoid breaking up.\nTo fly for a long time without engine power while losing as little height as possible requires a lightweight construction with long slender wings.\nThese requirements are mutually exclusive.\nI did some quick searches to put this in perspective. Afaict a modern glider has a glide ratio of about 50, a 747 has a glide ratio of about 15, concorde has a glide ratio of about 9 and the space shuttle has a glide ratio of about 1.\nThat's pretty much how space shuttles and other orbiting craft work.\nThey are dropped from orbit, they do calculations to enter the atmosphere at a velocity and attitude that doesn't burn up the aircraft, then they glide to their landing airport.\nThe entire journey is controlled, but unpowered flight. The flight surfaces are there to allow the pilots the ability to reduce the velocity and attitude of descent in order to get controlled descent into the atmosphere.\nSo a glider can also land; assuming of course it had some way of slowing down and is sufficiently hardened and shielded against the atmospheric effects. I am not too familiar with gliders but from what I know they don't have any spoilers or other such controls.\nThis is more applicable to space exploration really as the problem is de-orbiting and surviving re-entry rather than gliding.\nIf you are in geosynchronous orbit then you have a lot of velocity, if a glider was suddenly dropped into that orbit it would continue to orbit unless some sort of propulsion was used to slow it down enough to re-enter the atmosphere.\nIf it was to survive re-entry from orbit it would need a Thermal Protection System (TPS), or heat shield. Without one it would fry.\nPossibly you are asking what would happen if you dropped a glider from a geosynchronous orbit height but with no orbital velocity. If that happened then it would start to fall towards the earth because of gravity, and by the time it hit the atmosphere it would be travelling well in excess of Low Earth Orbital speed, meaning it would still burn up unless it had a TPS.\nCan a glider re-enter the Earth's atmosphere and land? Sure. As pointed out above, the Space Shuttle was one of the best examples of this but other types e.g. X-37, etc exist as well.\nAlso an orbiter doesn't exactly 'drop' from geosynchronous orbit; its in an energy state which allows it to remain there perpetually; the energy must be bled off via a de-orbit burn using rocket engines to alter its orbital trajectory to make contact with the atmosphere. The burn has to be precise to place the craft on s specific trajectory to contact the exosphere at a specific angle. Too steep, the craft will be incinerated and break up, too shallow and it can skip off the atmosphere like a stone skipping across a pond.\nIn addition to a heat shield, the shape of the craft must be a blunt object for initial re-entry and aerodynamic enough to provide an acceptable glide ratio to a runway. The shuttle did this via its de-orbit burn while orbiting facing aft and inverted(!) thence tipping its nose forward and entering the atmosphere belly first and then transitioning to more of s traditional glider attitude once it had slowed considerably and most of the kinetic energy had bee dissipated.","Presentation on theme: \"The suitability of optical instruments\"— Presentation transcript:\n1 The suitability of optical instruments What do I already know?I know what I can see in space using binoculars.I know what a telescope is.The suitability of optical instrumentsProduce an information leaflet or poster which identifies at least THREE DIFFERENT methods (land and space based) that scientists use to observe and find out about the Universe. Describe the features each has to make them suitable for their purpose.\n2 Naked eye Binoculars Telescopes What can you see in space just using your eyes?Describe some of the objects you can see.What can you see in space using telescopes AND space based telescopes?Describe some of the objects you can see.What can you see in space using binoculars?Describe some of the objects you can see.Use pictures!!Then:Discuss on the suitability of each:Which is the best to use? When? Why?Think about poor weather conditions, daylight and electromagnetic waves.\n3 Jupiter and its four largest moons roughly as they will appear in a small telescope Lunar map showing the major features of the Moon's surfaceOrion's belt and sword. The bright fuzzy spot on the sword is the Orion Nebula, a diffuse nebula that appears about twice the size of the full moonThe Pleiades photographed using a 90 mm (3.5 in) telescope\n4 The Andromeda galaxy, M31, also showing the satellite galaxies M32 and M110 Close-up view of the Trapezium asterism within M42The Large and Small Magellanic Clouds. Note the enormous NGC104 globular cluster to the left of the SMCM33, the Triangulum galaxy, clearly showing the face-on spiral structure\n5 Jupiter as captured by the Cassini probe Apollo 15 picture of Messier and Messier A craters, clearly showing the long parallel ejecta plumesJupiter as captured by the Cassini probehttps://www.astronomics.com/what-can-you-expect-to-see-in-a-telescope_t.aspxThe four largest moons of Jupiter, as imaged by the Galileo spacecraft\n6 Hubble mosaic image of the Great Orion Nebula Hubble's view of the Pleiades including star namesClose-up of the Large Magellanic Cloud, showing the reddish Tarantula Nebula above the left endAstrophoto of the Tarantula Nebula taken by a robotic telescope with an aperture of 24 in (0.6 m)\n7 The core region of globular cluster NGC104 Close-up of the Small Magellanic Cloud. Note the globular cluster NGC104 at the bottom of the framePuppis A is a supernova remnant located about 7,000 light years from Earth.This new image includes data from Chandra and XMM-Newton and is the most complete and detailed X-ray view of Puppis A to date.The combined dataset reveals a delicate tapestry of X-ray light left behind by the supernova explosion.The core region of globular cluster NGC104\n8 A new composite of NGC 4258 features X-rays from Chandra (blue), radio waves from the VLA (purple), optical data from Hubble (yellow and blue), and infrared with Spitzer (red).NGC 4258 is well known to astronomers for having \"anomalous\" arms that are not aligned with the plane of the galaxy, but rather intersect with it.Researchers are trying to understand how the giant black hole in the center of NGC 4258 is affecting the rest of the galaxy.NGC 4258, also known as Messier 106, is located about 23 million light years from Earth.\n9 The Hubble Space Telescope's launch in 1990 sped humanity to one of its greatest advances in that journey. Hubble is a telescope that orbits Earth. Its position above the atmosphere, which distorts and blocks the light that reaches our planet, gives it a view of the universe that typically far surpasses that of ground-based telescopes.Hubble is one of NASA's most successful and long-lasting science missions. It has beamed hundreds of thousands of images back to Earth, shedding light on many of the great mysteries of astronomy. Its gaze has helped determine the age of the universe, the identity of quasars, and the existence of dark energy.WHY A SPACE TELESCOPE?The Hubble Space Telescope is the direct solution to a problem that telescopes have faced since the very earliest days of their invention: the atmosphere. The quandary is twofold: Shifting air pockets in Earth's atmosphere distort the view of telescopes on the ground, no matter how large or scientifically advanced those telescopes are. This \"atmospheric distortion\" is the reason that the stars seem to twinkle when you look up at the sky.The atmosphere also partially blocks or absorbs certain wavelengths of radiation, like ultraviolet, gamma- and X-rays, before they can reach Earth. Scientists can best examine an object like a star by studying it in all the types of wavelengths that it emits.Newer ground-based telescopes are using technological advances to try to correct atmospheric distortion, but there's no way to see the wavelengths the atmosphere prevents from even reaching the planet.The most effective way to avoid the problems of the atmosphere is to place your telescope beyond it. Or, in Hubble's case, 353 miles (569 km) above the surface of Earth."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:66e7a91e-96b6-4e32-b5da-dc38893e45b8>","<urn:uuid:5e9332c1-d758-4b61-8f78-26683f061bc6>"],"error":null}
{"question":"How does a mining company get approval to use a lake as a tailings dump in Canada?","answer":"A mining company must follow a specific process to have a lake reclassified as a tailings impoundment area. They need to submit their proposals to an environmental assessment process and provide a plan for habitat compensation. This is done through Schedule 2 of the Fisheries Act's Metal Mining Effluent Regulations, which allows natural fish-bearing lakes, streams and wetlands to be reclassified into tailings impoundment areas, removing the Act's protections.","context":["The Sandy Pond Alliance\nThe Sandy Pond Alliance to Protect Canadian Waters was founded in response to the imminent threat to Sandy Pond, a unique and pristine lake ecosystem that transnational mining giant Vale plans to use as a waste dump. If the company proceeds it will use the lake to dispose of solid wastes from the Long Harbour nickel processing facility. The Alliance is led by residents of Newfoundland and Labrador, including scientists and activists who are supported by regional and national organizations the Council of Canadians, MiningWatch Canada, Nature Canada, Nature Newfoundland and Labrador, and Sierra Club Atlantic.\nThe Fisheries Act and Schedule 2\nOver the years the Fisheries Act has been one of Canada’s most important pieces of environmental legislation. Though recent changes have weakened it Section 36 of the Act, which prohibits the deposit of harmful substances into fish habitats, remains in place.\nThere is however, a significant exception to the protections of Section 36. Schedule 2 of the Act’s Metal Mining Effluent Regulations gives mining companies the opportunity to have natural fish-bearing lakes, streams and wetlands reclassified into “tailings impoundment areas” removing all the Act’s protections. Schedule 2 came into effect in 2002 with the listing of five lakes. All five were previously used for tailings disposal by operating mines. Since then, nine other lakes and streams have been added. Six of these, including Sandy Pond, were pristine water bodies with healthy aquatic ecosystems.\nUsing Lakes as Mine Waste Dumps\nUsing a lake, stream or wetland for a dumpsite saves the industry millions of dollars - the main motivation for this practice. The Canadian mining industry, Environment Canada, and DFO argued that using lakes as mine waste dumps can be a responsible alternative to other waste management techniques. This is because natural lake basins provide a geologically stable container to hold the potentially toxic wastes. The basins are not, however, watertight and contaminants may leach into groundwater.\nUnder millions of tonnes of mine waste, the native ecosystem of the lake itself is of course smothered. The long term biological fate of contaminants in the lakes has not been well researched. A recent assessment by a federal review panel concluded there were no good examples of compensation for the loss of an entire, productive lake ecosystem.\nFrom Lake to Waste Dump\nIn order for a lake to be reclassified as a tailings impoundment, mining companies have to submit their proposals to an environmental assessment process and provide a plan for habitat compensation. The environmental assessment for the Long Harbour facility and tailings area was, however, done as a screening – the lowest possible degree of federal scrutiny. When lake-dumping proposals have been submitted to more rigorous panel review processes they have been rejected on two separate occasions, both in British Columbia – the Kemess North project that proposed the destruction of Amazay (Duncan Lake) and the Prosperity project that proposed to drain Teztan Biny (Fish Lake) and fill in Y’anah Biny (Little Fish Lake) with tailings.\nDuring the limited public review of the Long Harbour project, weaknesses in baseline data were identified but never addressed. The destruction of Sandy Pond was also portrayed as the only viable alternative, playing off jobs and economics with environmental protection. In fact, the construction of an impoundment would have created more jobs while saving Sandy Pond.\nVale has begun “habitat compensation” activities that are meant to achieve “no net loss” of fish habitat. The plan, however, does not and cannot address the loss of Sandy Pond, a unique and poorly studied lake ecosystem. If history is any indication there is little reason for confidence in the success of the plan. In 2009 the federal Office of the Auditor General determined that the Department of Fisheries and Oceans has a poor track record of achieving the goal of no net loss. Retired DFO fisheries biologist and Sandy Pond Alliance member, Dr. John Gibson puts it this way “Science is being bent for the benefit of mining companies with the loss of natural heritage for future generations.”\nWater bodies now classified as Tailings Impoundment Areas:\n- Albino Lake Lake, BC, 2002\n- Anderson Lake, MB, 2002\n- King Richard Creek, and tributaries to Alpine Lake, BC, 2010\n- Flora Lake, NL, 2009\n- Garrow Lake, NWT, 2002\n- Mallard Lake, SK, 2011\n- Second Portage Lake (Northwest Arm), NU, 2008\n- South Kemess Creek, BC, 2002\n- Sandy Pond, NL, 2009\n- Tail Lake, NU, 2008\n- Tom MacKay Lake, BC, 2002\n- Trout Pond and Gills’s Brook tributary, NL, 2006\n- Un-named muskeg pond and stream, ON, 2012\n- Wabush Lake, NL, 2009\nWater bodies proposed for reclassification:\n- Bird Brook, NB – Sisson Brook Tungsten and Molybdenum Project\n- Bucko Lake, MB – Bucko Lake Nickel Project\n- Clary Creek Watershed Headwaters, BC – Kitsault Molybdenum Project\n- Clet Creek, QC – Arnaud Apatite Project\n- Davidson Creek and tributaries, BC - Blackwater Gold Project\n- Lac Hessé, QC – Mont Wright Iron Ore Mine\n- Y’anah Biny (Little Fish Lake), BC – New Prosperity Copper-Gold Project\n- Quarry and Trail Creeks, BC – Red Chris Copper-Gold Project\n- Treaty and Teigen Creeks, BC – KSM Gold-Copper Project\n- Various ponds and streams, ON – Marathon Platinum-Copper Project\n- Various ponds and streams, ON – Josephine Cone Iron Project\n- Winter Lake, NWT – Yellowknife Gold Project"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:dcf64b7c-4704-46fa-8657-c34392abd69a>"],"error":null}
{"question":"How do Cotazym and MS1819 differ in their source materials and potential side effects for treating pancreatic insufficiency?","answer":"Cotazym and MS1819 differ significantly in their sources and side effects. Cotazym is derived from pig pancreas and carries potential risks including viral infections from pork sources, fibrosing colonopathy, allergic reactions, and irritation of the mouth. MS1819, on the other hand, is a synthetic lipase derived from Yarrowia lipolytica yeast and does not contain any animal products, thus eliminating the risk of animal pathogen transmission or fibrosing colonopathy. MS1819 has shown no serious adverse events in trials, while Cotazym's side effects can include blood sugar changes, abdominal pain, abnormal bowel movements, sore throat, and cough.","context":["Сotazym - Consumer Medicine Information\n|Manufacture:||Merck and Co., Inc.|\n|Condition:||Chronic Pancreatitis, Pancreatic Exocrine Dysfunction, Pancreatitis|\n|Ingredients:||lipase, amylase, protease, mixed conjugated bile salts, cellulose, print ink, cellulose acetate phthalate, colloidal silicon dioxide, cornstarch, diethyl phthalate, gelatin, magnesium stearate, povidone, precipitated calcium carbonate, pregelatinized starch, propylene glycol, silicon dioxide, sodium lauryl sulphate, sucrose, talc, titanium dioxide, red food dye (FD&C Red No. 40), yellow food dye (D&C Yellow No. 10)|\nAbout this Medication\nWhat the Medication is Used for\nCOTAZYM is used for the treatment of pancreatic insufficiency attributed to cystic fibrosis, chronic pancreatitis, or any other medically defined pancreatic disease that might require pancreatic enzyme therapy, as determined by the doctor.\nWhat it does\nCOTAZYM is a prescription medicine used to treat people who cannot digest food normally because their pancreas does not make enough enzymes due to cystic fibrosis, swelling of the pancreas that lasts a long time (chronic pancreatitis), removal of some or all of the pancreas (pancreatectomy), or other conditions.\nCOTAZYM may help your body use fats, proteins, and sugars from food. It contains a mixture of digestive enzymes including lipases, proteases, and amylases from pig pancreas.\nWhen it Should not be Used\nCOTAZYM should not be used if you:\n- have known hypersensitivity to porcine protein, pancreatic enzymes or any excipients; and/or during acute pancreatitis or the acute exacerbation of chronic pancreatitis\nWhat the Medicinal Ingredient is\nLipase, amylase, protease, mixed conjugated bile salts and cellulose.\nWhat the Important Nonmedicinal Ingredients are\nCapsule print ink, cellulose acetate phthalate, colloidal silicon dioxide, cornstarch, diethyl phthalate, gelatin, magnesium stearate, povidone, precipitated calcium carbonate, pregelatinized starch, propylene glycol, silicon dioxide, sodium lauryl sulphate, sucrose, talc, titanium dioxide, red food dye (FD&C Red No. 40), yellow food dye (D&C Yellow No. 10).\nWhat Dosage Forms it comes in\nCapsules containing 10,000 units of lipase, 40,000 units of amylase and 35,000 units of protease (Cotazym).\nEnteric coated capsules containing 10,800 units of lipase, 42,000 units of amylase and 45,000 units of protease (Cotazym ECS 8).\nEnteric coated capsules containing 25,000 units of lipase, 100,000 units of amylase and 100,000 units of protease (Cotazym ECS 20).\nWarnings and Precautions\nSerious Warnings and Precautions\nCOTAZYM may increase your chance of having a rare bowel disorder called fibrosing colonopathy. This condition is serious and may require surgery. The risk of having this condition may be reduced by following the dosing instructions that your doctor gave you.\nTalk to your doctor about all the drugs you are taking before taking COTAZYM.\nBEFORE you use COTAZYM talk to your doctor or pharmacist if you:\n- Are allergic to pork (pig) products\n- Have a history of intestinal blockage or scarring or thickening of your bowel wall (fibrosing colonopathy)\n- Have gout, kidney disease, or high blood uric acid (hyperuricemia or hyperuricosuria)\n- Have trouble swallowing capsules Have any other medical condition\n- Are pregnant, plan to become pregnant, are breast-feeding or plan to breast-feed. It is not known if COTAZYM will harm your unborn baby or if COTAZYM passes into your breast milk. Enzymes such as COTAZYM are broken down in your gastrointestinal tract, and thus are not absorbed into the body as intact enzymes.\nTell your doctor all the medications that you take, including prescription and nonprescription drugs, natural health products, vitamins and herbs.\nTalk to your doctor if the following occurs while taking COTAZYM:\n- stomach area (abdominal) pain\n- trouble passing stool (having bowel movements)\n- nausea, vomiting, or diarrhea\nTake COTAZYM exactly as prescribed. Do not take more or less COTAZYM than directed by your doctor.\nInteractions with this Medication\nAs with all other pancrelipase lipase preparations, interactions with COTAZYM is possible. Therefore, tell your doctor all the medications that you take, including prescription and nonprescription drugs, natural health products, vitamins and herbs.\nProper Use of this Medication\n- Take COTAZYM exactly as prescribed by your healthcare provider.\n- You should not switch COTAZYM with any other pancreatic enzyme product without first talking to your doctor.\n- Do not take more capsules in a day than the number your doctor tells you to take (total daily dose).\n- Always take COTAZYM with a meal or snack and enough liquid to swallow COTAZYM If you eat a lot of meals or snacks in a day, be careful not to go over your total daily dose.\n- If the capsules are opened, try to avoid sprinkling them on dairy products such as milk, custard or ice cream.\n- Your doctor may change your dose based on the amount of fatty foods you eat or based on your weight.\n- Do not crush or chew COTAZYM capsules or its contents, and do not hold the capsule or capsule contents in your mouth. Crushing, chewing or holding the COTAZYM capsules in your mouth may cause irritation in your mouth or change the way it works in your body.\nTake 1 to 3 capsules with each meal and 1 capsule with each snack as directed by your doctor.\n|In case of drug overdose, contact a health care practitioner, hospital emergency department or regional Poison Control Centre immediately, even if there are no symptoms.|\nSide Effects and what to do about them\nThe most common side effects of pancreatic enzyme preparations including COTAZYM include the following:\n- Blood sugar increase (hyperglycemia) or decrease (hypoglycemia)\n- Pain in your stomach (abdominal area)\n- Frequent or abnormal bowel movements\n- Soar throat and cough\nOther Possible Side Effects\nCOTAZYM and other pancreatic enzyme products are made from the pancreas of pigs, the same pigs people eat as pork. These pigs may carry viruses. Although it has never been reported, it may be possible for a person to get a viral infection from taking pancreatic enzyme products that come from pigs.\nTell your healthcare professional if you have any side effect that bothers you or does not go away.\nSerious Side Effects, how often they Happen and what to do about them\nCOTAZYM may cause serious side effects including:\n- Irritation of the inside of your mouth. This can happen if\nCOTAZYM is not swallowed completely.\n- Increase in blood uric acid levels. This may cause worsening of swollen, painful joints (gout) caused by an increase in your blood uric acid levels.\n- Allergic reactions, including trouble with breathing, skin rashes, or swollen lips.\nCall your doctor right away if you have any of these symptoms.\nThis is not a complete list of side effects. For any unexpected effects while taking COTAZYM, contact your doctor or pharmacist.\nHow to Store it\n- Store COTAZYM at room temperature below 25°C. •Keep COTAZYM in a dry place and in the original container.\n- After opening the bottle, keep it closed tightly between uses to protect from moisture.\nKeep COTAZYM and all medicines out of the reach of children.\nReporting Suspected Side Effects\nYou can report any suspected adverse reactions associated with the use of health products to the Canada Vigilance Program by one of the following 3 ways:\n- Report online at www.healthcanada.gc.ca/medeffect Call toll-free at 1-866-234-2345\n- Complete a Canada Vigilance Reporting Form and:\n- Fax toll-free to 1-866-678-6789, or\n- Mail to: Canada Vigilance Program\nPostal Locator 0701E\nOttawa ON K1A 0K9\nPostage paid labels, Canada Vigilance Reporting Form and the adverse reaction reporting guidelines are available on the MedEffect Canada Web site at www.healthcanada.gc.ca/medeffect.\nor at Merck Canada Inc. by one of the following 2 ways:\n- Call toll-free at 1-800-567-2594\n- Complete a Canada Vigilance Reporting Form and:\n- Fax toll-free to 1-877-428-8675, or\n- Mail to: Merck Canada Inc.\nMedical Information Center\n16750, route Transcanadienne\nKirkland QC H9H 4M7\nNOTE: Should you require information related to the management of side effects, contact your health professional. The Canada Vigilance Program does not provide medical advice.\nThis document plus the full product monograph, prepared for health professionals, can be found at:\nwww.merck.ca or by contacting the sponsor, Merck Canada Inc. at: 1-800-567-2594.","Get inside Wall Street with StreetInsider Premium. Claim your 1-week free trial here.\nAzurRx BioPharma, Inc. (NASDAQ: AZRX), a company specializing in the development of targeted non-systemic, recombinant therapies for gastrointestinal (GI) diseases, today announced the completion of patient enrollment in the first cohort of the Phase 2b OPTION 2 extension study evaluating immediate-release capsules of MS1819 for the treatment of exocrine pancreatic insufficiency (EPI) in patients with cystic fibrosis (CF).\nThe Phase 2b OPTION 2 trial extension arm is currently dosing patients at clinical trial sites in the U.S. and Europe who have participated in the previous arms of the OPTION 2 trial. The goal of the MS1819 clinical program is to provide cystic fibrosis patients with a safe and effective therapy to control EPI, a debilitating gastrointestinal condition common to patients with cystic fibrosis that can lead to a chronic nutritional deficiency. For cystic fibrosis patients, maintaining proper nutritional levels is essential to ensure healthy growth, weight management and good lung function.\nJames Sapirstein, President and Chief Executive Officer of AzurRx BioPharma, stated, “The extension arm of the Phase 2b OPTION 2 trial is providing us with an opportunity to determine both the optimized dosage and delivery mechanism of MS1819. We have fully enrolled the most important cohort of the extension study and would like to thank both our clinical collaborators and patients for their continued participation in the study. We continue to be encouraged by the absence of any reports of serious adverse events to date, and eagerly look forward to reporting topline results from this trial in Q1 2021.”\nOPTION 2 Extension StudyThe Phase 2b OPTION 2 study is an open-label, multi-center, crossover clinical trial designed to investigate the safety, tolerability, and efficacy of MS1819 in a head-to-head comparison against the current standard of care for EPI, porcine pancreatic enzyme replacement therapy (PERT). The primary efficacy endpoint is the coefficient of fat absorption (CFA). AzurRx BioPharma initiated the additional study arm, the Phase 2b OPTION 2 extension trial, to identify both the optimal dose and the optimal delivery method for MS1819 using immediate-release capsules at higher dose levels than previous early-stage clinical trials.\nTo date, there have been no serious adverse events reported during either the Phase 2b OPTION 2 trial or its extension arm. The Phase 2b OPTION 2 extension trial is enrolling volunteers 18 years or older at trial sites in the U.S. and Poland who have completed the OPTION 2 crossover trial at higher dose levels relative to the previous OPTION 1 trial. Trial participants will be treated with MS1819 over a two-week-period. AzurRx BioPharma expects to report topline data by the end of first quarter 2021.\nAbout MS1819MS1819 is a recombinant lipase enzyme for the treatment of exocrine pancreatic insufficiency associated with cystic fibrosis and chronic pancreatitis. MS1819, supplied as an oral, non-systemic biologic capsule, is derived from the Yarrowia lipolytica yeast lipase and breaks up fat molecules in the digestive tract of EPI patients so that they can be absorbed as nutrients. Unlike the standard of care, porcine pancreatic enzyme replacement therapy (PERT), the MS1819 synthetic lipase does not contain any animal products. The global market for PERT was estimated to be approximately $1.4 billion in the U.S. and more than $2 billion globally in 2019. There currently is no non-animal-based enzyme replacement therapy in the market for the treatment of exocrine pancreatic insufficiency. AzurRx believes that MS1819 has the potential to provide a safe and effective non-animal derived, or synthetic, alternative to PERT, without the risk of animal pathogen transmission or fibrosing colonopathy. In addition, we believe that MS1819 has the potential to improve patient compliance and quality of life given anticipated reductions in pill burden and pill size relative to PERT.\nAbout Exocrine Pancreatic InsufficiencyEPI is a condition characterized by a deficiency of the exocrine pancreatic enzymes, resulting in a patient’s inability to digest food properly, or maldigestion. The deficiency in this enzyme can be responsible for greasy diarrhea, fecal urge, and weight loss.\nThere are more than 30,000 patients in the U.S. with EPI caused by cystic fibrosis according to the Cystic Fibrosis Foundation and approximately 90,000 patients in the U.S with EPI caused by chronic pancreatitis according to the National Pancreas Foundation. Patients are currently treated with porcine pancreatic enzyme replacement pills."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9893fb2e-6a40-4998-afa7-7149c25636ba>","<urn:uuid:3b710308-c2a0-45cc-a851-d7cf8689c7c4>"],"error":null}
{"question":"Was ist der Latin inscription und meaning of the text 'Germinet terra herbam virentem' on Michelangelo's fresco?","answer":"The Latin inscription 'Germinet terra herbam virentem' comes from Genesis Chapter 1, Verse 11, and means '[God said] Let the earth bring forth green grass [and it came to pass]'.","context":["Thursday, 11 May 2017\nDomenico Cunego’s etching (1781) of Michelangelo’s fresco, “Creation of the Sun, Moon and Plants” (1511)\nDomenico Cunego (1727–1803) et al.\n“Creation of the Sun, Moon and Plants” or as inscribed on the plate: “Germinet terra herbam virentem” (Gen. Cap. I. Vers 11: [God said] “Let the earth bring forth green grass” [and it came to pass]) and “Fecitque Deus duo luminaria magna” (Gen. Cap. I. Vers.16: “God made the two great lights” [the greatest to rule in the day, the least to rule in the night, and the stars]), 1781, from the series of 40 plates first published by Gavin Hamilton (1723–98) in “Schola Italica Picturae” after Michelangelo Buonarroti’s (1475–1564) fresco by on the ceiling of the Sistine Chapel. The British Museum advises that the plates for Hamilton’s publication were later sold to Piranesi in 1780 and passed for publication to Giovanni Volpato (1740–1803). As this plate is dated 1781, I presume that it is from the Volpato edition. (http://www.britishmuseum.org/research/search_the_collection_database/term_details.aspx?bioId=122151)\nEtching and engraving on laid paper trimmed with margins around the image borderline.\nSize: (sheet) 29.1 x 46 cm; (image borderline) 25.3 x 43.3 cm\nLettered below the image borderline: (left) 'Michael Angelus Bonarota in Sacetto Sistine /\nGerminet terra herbam virentem Gen. Cap. I Vers 11”; (centre) “Romae apied Dom Curego; (right) “Dom. Cunego sculp. Romae 1781 / Fecitque Deus duo luminaria magna Gen. Cap. I, Vers.16”\nThe Harvard Art Museums offer a description of this print from a later (?) state (note my attribution to a later state is based entirely upon the font used for the publication details): http://www.harvardartmuseums.org/collections/object/276144?position=4\nCondition: Excellent impression trimmed with margins around the image borderline. The sheet has light age-toning, but is otherwise in good condition (i.e. there are no tears, holes, folds, losses or significant stains).\nI am selling this strong etching by one of the great masters of interpretative printmaking for the total cost of AU$194 (currently US$142.92/EUR131.45/GBP110.60 at the time of posting this listing) including postage and handling to anywhere in the world.\nIf you are interested in purchasing this beautifully executed print based on Michelangelo’s famous fresco, please contact me (firstname.lastname@example.org) and I will send you a PayPal invoice to make the payment easy.\nThis image fascinates me for all the wrong reasons and I had to check the source image—Michelangelo’s fresco—to see whether Cunego had made a mistake … but he hadn’t.\nFor those who believe that the legendary Michelangelo could never make a mistake, please forgive me, but from my way of looking he has made an error of judgement in this composition.\nLet me explain …\nOne of the fundamental rules of achieving spatial depth in a composition is to ensure that key subjects overlap each other to ensure that a viewer “knows” where each subject lies in relation to the others. For instance, Michelangelo presents the figure of God the Father with his hands gesturing in two different directions as being clearly in front of his accompanying angels by God’s form overlapping the small helpers. Beyond this example of overlapping, however, I see many curious points on silhouette edges where forms are tangentially, or almost tangentially, abutted (i.e. lightly “touching” each other) rather than the subject in front overlapping the one further behind. Note for instance how the head of the closer angel accompanying God “touches” the circle of the sun and how God’s hand almost touches the sun. Note also how the closer angel’s finger tangentially abuts the right foot of the flying figure shown on the left—a second view of God the Father. Actually the more that I look at this composition the more uneasy I am with it regarding tangential junctions, as I now see that the tail end of the left figure’s drapery almost touches the sun and that God’s outstretched left arm tangential connects with the head of the angel in shadow on the right.\nOf course, Michelangelo would have very good reasons to dispose the figures in this arrangement anyway that he liked. Moreover, “rules” are meant to be broken (or so I’m told) but, for me, this composition has me wondering what Michelangelo was thinking."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cf82b494-65ca-48be-994f-0b7880882c91>"],"error":null}
{"question":"What are payment tokens and what security measures are required for their trading platforms?","answer":"Payment tokens, also known as virtual currencies or cryptocurrencies, are crypto tokens intended to serve as alternative means of payment. While their issuance generally doesn't require a license or prospectus, trading platforms for payment tokens are subject to BaFin supervision and must implement specific security measures. These platforms must secure all points of entry, deploy identity and access management controls with encryption, and provide security awareness training for employees. Additionally, as credit or financial service institutions, they must comply with the German Anti-Money Laundering Act. The platforms must also ensure protection against manipulation of data and transactions, secure the infrastructure, and address potential vulnerabilities in smart contracts.","context":["On 16 August 2019 the German Federal Financial Supervisory Authority (BaFin) published a new guidance note regarding crypto tokens (GZ: WA 51-Wp 7100-2019/0011 and IF 1-AZB 1505-2019/0003). The guidance note dealt with prospectus and licence requirements in connection with crypto tokens and is intended to complement the BaFin’s explanatory letter regarding the supervisory classification of crypto tokens as financial instruments in the area of securities supervision (GZ: WA 11-QB 4100-2017/0010). BaFin felt the publication of the guidance note was in order as it had received numerous – often unclear content-wise – inquiries regarding the legal classification of crypto tokens.\nNot all crypto tokens are the same\nBaFin reaffirms its ongoing administrative practice that crypto tokens are subject to existing financial market regulation depending on their concrete form. In BaFin’s administrative practice, a number of token categories have developed that allow an initial assessment of the regulatory classification. Broadly speaking, a distinction can be made between utility tokens, payment tokens and security tokens, whereby utility tokens are largely unregulated and security tokens are subject to the most stringent regulation. In this context, BaFin emphasises that in view of the numerous forms of crypto tokens that exist in the market, an assessment will need to be made in each individual case.\nUtility tokens are crypto tokens that give access to certain services or products of the issuer. This type of token can be compared to a voucher. Utility tokens are generally not subject to financial market regulation as they are neither classed as securities under the Prospectus Regulation nor as investments under the German Asset Investment Act (VermAnlG) nor as financial instruments under the German Banking Act (KWG). Therefore, in general neither the issue nor the sale of such utility tokens is subject to a licence requirement. There is also no obligation to publish a prospectus.\nPayment tokens (also known as virtual currencies or cryptocurrencies) are crypto tokens that are intended to serve as alternative means of payment. To this end, it is sufficient if the crypto tokens are easily transferable between the relevant users and if the issuer intends for them to include a payment function in their respective areas of use. BaFin qualifies such payment tokens as financial instruments, namely as units of account pursuant to Section 1 para. 11 sentence 1 no. 7 KWG. This results in the following consequences:\n- The issuance of payment tokens is generally not subject to a licence requirement. There is also usually no obligation to publish a prospectus as payment tokens in general do not constitute securities or investments.\n- However, companies trading payment tokens commercially are subject to supervision by BaFin. Depending on the structure of the exchanges, the following may be of particular relevance: proprietary trading, financial commission business, the operation of a multilateral trading system, investment brokerage, contract brokerage, issuing or placement business as well as deposit business. In addition, as exchanges are classed as credit or financial service institutions, they are subject to the German Anti-Money Laundering Act (GwG), requiring them to comply with anti-money laundering obligations.\nSecurity tokens are characterised by the fact that they convey to their holders certain asset-related membership rights or rights under the law of obligations (e.g. right to dividend-like payments, co-determination rights, repayment rights, right to interest). Such security tokens are regularly considered to be financial instruments within the meaning of the German Banking Act (KWG) and generally also fall under the definition of securities within the meaning of the Prospectus Regulation or of investments within the meaning of the German Asset Investment Act (VermAnlG) or of investment assets within the meaning of the German Investment Act (KAGB). This results in the following consequences:\n- Whether or not the issuance of security tokens is possible without a licence depends on the form of the relevant security token. It is conceivable that a licence may be required as a result of there being deposit business, e-money or investment business. In addition, any prospectus obligations under the Prospectus Regulation or the German Asset Investment Act (VermAnlG) must be observed in the context of initial security offerings.\n- In addition, companies trading payment tokens commercially are subject to supervision by BaFin. Depending on the structure of the exchange, the following may be of particular relevance: proprietary trading, financial commission business, the operation of a multilateral trading system, investment brokerage, contract brokerage, issuing or placement business as well as deposit business. In addition, as exchanges are classed as credit or financial service institutions, they are subject to the German Anti-Money Laundering Act (GwG), requiring them to comply with anti-money laundering obligations.\n- Additionally, further laws such as the German Securities Trading Act as well as the EU Market Abuse Regulation may apply.\nClarification of the legal classification applied by BaFin\nIf there are uncertainties about the regulatory classification of specific crypto tokens, for example in the run-up to an initial coin offering or an initial security offering, these can be clarified with BaFin. In order to reduce the processing time for such enquiries, BaFin has defined in its guidance note the minimum content of such enquiries which it requires to be able to answer enquiries regarding possible prospectus and authorisation obligations promptly and in a timely manner.\nWith its additional guidance, BaFin provides further details on its current administrative practice and thereby provides further clarity on the regulatory framework relevant to crypto tokens. However, it has also become clear that due to the variety of different forms crypto tokens can take as well as the complexity of financial market regulation, a detailed case-by-case assessment is still necessary.\nCover picture: Copyright © fotolia / Syda Productions","Recent research revealed that blockchain is set to become ubiquitous by 2025, entering mainstream business and underpinning supply chains worldwide.\nThis technology is set to provide greater transparency, traceability and immutability, allowing people and organizations to share data without having to be concerned about security. However, blockchain is only as strong as its weakest link. Despite the hails surrounding blockchain’s immutable security, there are still risks surrounding it that organizations must be aware of – and mitigate – prior to implementation.\nIt is important to understand that there are two types of blockchain – permissionless and permissioned. The most prominent example of permissionless blockchain is Bitcoin – a public blockchain network that anyone can participate in. Cryptocurrencies like bitcoin favor this type of blockchain technology because it enables all users to track, verify and confirm transactions, regardless of whether users choose to be anonymous or not.\nThe other blockchain model is permissioned (also known as private blockchain) – and is mainly used for business applications. These networks are only accessible to known entities such as partners, suppliers or customers. With permissioned blockchain, a company establishes protocols to achieve consensus, and verify and assemble blocks. This set up can deliver thousands of transactions per second and provide granular management and control over who sees and accesses the transactions.\nIn both cases, the main benefit is the trust and transparency that blockchain brings – all parties involved in the network have total visibility into the transactions recorded in the blockchain ledger and each block is tied to the block before it.\nThis transparency makes blockchain extremely difficult to manipulate at scale. While the blockchain platform itself may be secure, there is still some work to be done to ensure organizations are equipped to make their networks secure end to end. For true security, organizations must focus on the last mile connection between a physical event and the digitized record of this event.\nIf these points of entry to the platform are tampered with, the blockchain is rendered worthless. It is therefore imperative that organizations secure all points of entry, and assess the risks, before they consider deploying blockchain on a broad scale. They will need to consider security at all layers, most importantly:\nThis starts with ensuring data and transactions entered in the blockchain ecosystem are adequately protected from manipulation. The infrastructure these networks resides on must also have the necessary protections in place. With blockchain, you are only as strong as your weakest link.\nIf integration points are compromised, the entire blockchain ecosystem could be at risk, meaning that blockchain credentials and data could be exposed to unauthorized users.\nIdentity and access management\nTo prevent unauthorized parties from accessing blockchain data, a combination of encryption and identity management tools are needed. Stolen credentials could potentially allow a cybercriminal to access the blockchain platform, regardless of how secure it is. Organizations must deploy identity and access management controls. Encryption should also be deployed to ensure that data is not stolen, manipulated or leaked in transit.\nThe insider threat should be a focal concern when it comes to blockchain too. Organizations must consider that employees, partners and suppliers – be it unintentionally or maliciously – can cause security incidents that impact the blockchain.\nTo mitigate this, organizations should deploy security awareness training for employees and outline clear security parameters and responsibilities with partners. This will stop employees from making careless mistakes and may also ward off malicious insiders. In line with these requirements, blockchain can provide advanced security controls – for example, leveraging the public key infrastructure (PKI) to authenticate and authorize parties, and encrypt their communications.\nBlockchain-based networks are built on shared business interests creating a system of trust. However, as the network grows, participating entities could leave the network and new ones may join, leading to ambiguities around operational considerations around data sharing and data ownership. These could result in serious regulatory and reputational repercussions for organizations as data owners, unable to secure the customer data.\nOrganizations are multi-faceted and have multiple revenue streams, often linked to each other. One of the major challenges to blockchain adoption has been a lack of interoperability across different blockchain networks. There have been recent developments, with major players embarking on developing interoperable networks, which could boost blockchain interest to a different level, at the same time introducing additional levels of vulnerability.\nA key component of blockchain networks is the Smart Contracts, which are developed using different languages on the platform being used, like Solidity being used in Ethereum. These languages allow developers to make changes to the underlying blockchain networks, causing vulnerabilities. However, from an enterprise blockchain perspective, a solid governance mechanism using permissioned chain can establish a secure system in place to restrict the privileges to governing body.\nTo achieve the most value from blockchain, both now and in the future, organizations must take responsibility for their safety and security at all levels – application, Infrastructure, data and partners.\nBy conducting a blockchain risk assessment and addressing key risks, organizations can make sure they are well positioned to leverage the efficiencies, transparency and cost-effectiveness provided by blockchain without opening themselves up to unexpected risks. The most pragmatic way for organizations interested in blockchain is to test the concept through pilot programs. Pilots should be focused on the areas that offer organizations the most control and companies should take these weak links into consideration.\nUltimately, blockchain has the ability to solve business issues relating to traceability, responsiveness, and trust. By taking a carefully planned approach to implementation, and understanding blockchain’s weak links, organizations can unlock the true value of blockchain, creating new opportunities and reducing inefficiencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:11c258b2-2b97-48a8-9dfd-5947d73b32dd>","<urn:uuid:aa3cc425-6945-4d18-8ba5-24e7cdc911d7>"],"error":null}
{"question":"What evidence confirms that our universe is matter-dominated, and how did this matter-antimatter asymmetry emerge after the Big Bang?","answer":"The evidence for a matter-dominated universe comes from multiple observations: Neil Armstrong's safe moon landing, solar cosmic rays being matter, space probes surviving visits to other planets, and cosmic rays containing 10,000 protons for every antiproton. Additionally, the absence of gamma ray emissions from matter-antimatter annihilation in nearby galaxy clusters suggests they are matter-dominated. As for the asymmetry's origin, in the first microsecond after the Big Bang, there were approximately 30 million antiquarks for every 30 million and 1 quarks - a tiny imbalance. Over time, most matter and antimatter annihilated, leaving the small excess of matter to dominate. This asymmetry likely emerged through baryogenesis, requiring three conditions: baryon number violation, C and CP violation, and thermodynamic nonequilibrium. The Standard Model satisfies these conditions, though it may have insufficient CP-violation to fully explain the observed asymmetry.","context":["The Dirac equation is a relativistic wave equation derived by Paul Dirac in 1928. In its free form or including electromagnetism interactions, it describes all spin-1/2 massive particles such as electrons and quarks for which parity is a symmetry. The equation also implied the existence of a new form of matter. antimatter, previously unsuspected and unobserved and which was experimentally confirmed several years later.\nAlthough Dirac did not at first fully appreciate the importance of his results explanation of spin as a consequence of the union of quantum mechanics and relativity - and the eventual discovery of the positron - represents one of the great triumphs of theoretical physics. The accomplishment has been described as fully on par with works of Newton, Maxwell, and Einstein.\nAccording to Schrödinger Equation the Hamiltonian operator and momentum operator may be written as (𝑉: potential energy) \n(3.25) 𝑖ℏ ∂𝜓/∂𝑡 = Ĥ𝜓.\n(3.26) Ĥ = -(ℏ2/2𝑚) ∂2/∂𝑥2 + 𝑉\n(3.29) 𝑃^ = -𝑖ℏ ∂/∂𝑥.\nThe operator version of classical total-energy equation 𝐸 = 𝑇 + 𝑉 = 𝑝2/2𝑚 + 𝑉: (𝑇: kinetic energy)\n(3.30) Ĥ = (𝑃^)2/2𝑚 + 𝑉\n(3.31) (𝑃^)2𝛹 = 𝑃^(𝑃^𝛹) = -𝑖ℏ ∂/∂𝑥(-𝑖ℏ ∂/∂𝑥) = 𝑖2ℏ2 ∂2𝛹/∂𝑥2 = -ℏ2 ∂2𝛹/∂𝑥2.\n(3.32) (𝑃^)2 = -ℏ2 ∂2/∂𝑥2\nHence the equation is written formally by\nĤ𝜓(𝑟̄) = 𝐸𝜓(𝑟̄)\nwhere 𝐸 is called the energy equivalence of the Ĥ-operator. Without potential energy Ĥ-operator can be written\nĤ = -(ℏ2/2𝑚) 𝛻2.\nAccording to special relativity the relativistic energy is given by (refer to 'MC Forum 61')\n𝐸2 = 𝑝2𝑐2 + 𝑚2𝑐4 or 𝐸 = ∓𝑐√(𝑝2 + 𝑚2𝑐2)\nwhere the case of 𝐸 < 0 will be for the positron which is the antimatter counterpart of the electron!\nDirac derived a realistic wave equation  which has the form for kinetic energy (operator) \n(2.2) 𝑖ℏ ∂𝜓/∂𝑡 = [-𝑖ℏ𝑐(𝛼^1∂/∂𝑥1 + 𝛼^2∂/∂𝑥2 + 𝛼^3∂/∂𝑥3) + 𝛽^𝑚𝑐2]𝜓 ≡ Ĥ𝜓,\nwhere 𝛼^𝑗 (𝑗 = 1,2,3) and 𝛽^ cannot be a simple number, therefore matrices can be solutions.\nWe consider a particle, such as an electron with mass 𝑚 in field-free region of space here now.\n(14.5) 𝑖ℏ ∂𝜓/∂𝑡 = √ ∣𝑃^2𝑐2 + 𝑚2𝑐4∣ 𝜓\nIf we are to make the standard replacement 𝑃^𝑥 = -𝑖ℏ∂/∂𝑥, etc., folllowing Dirac, we write\n(14.6) 𝑖ℏ ∂𝜓/∂𝑡 = [𝑐𝛼^1𝑃^𝑥 + 𝑐𝛼^1𝑃^𝑦 + 𝑐𝛼^1𝑃^𝑧 + 𝛽^𝑚𝑐2]𝜓\n(14.7) [𝑐𝛼^1𝑃^𝑥 + 𝑐𝛼^1𝑃^𝑦 + 𝑐𝛼^1𝑃^𝑧 + 𝛽^2𝑚𝑐2]2 = 𝑐2𝑃^2 + 𝑚2𝑐4\nMultiplying out the lef-hand side of (14.7) and equating the corresponding terms leads to\n(14.8) 𝛼^12 = 𝛼^22 = 𝛼^32 = 1\n𝛼^1𝛼^2 + 𝛼^2𝛼^1 = 𝛼^2𝛼^3 + 𝛼^3𝛼^2 = 𝛼^3𝛼^1 + 𝛼^1𝛼^3 = 0\n𝛼^1𝛽^ + 𝛽^𝛼^1 = 𝛼^2𝛽^ + 𝛽^𝛼^2 = 𝛼^3𝛽^ + 𝛽^𝛼^3 = 0\nWe call them the Dirac matrices which Dirac found.\n(14.9) 𝛼^1 = 4⨯4 matrix [row-1 (0 0 0 1), row-2 (0 0 1 0), row-3 (0 1 0 0), row-4 (1 0 0 0)],\n𝛼^2 = 4⨯4 matrix [row-1 (0 0 0 -𝑖), row-2 (0 0 𝑖 0), row-3 (0 -𝑖 0 0), row-4 (𝑖 0 0 0)],\n𝛼^3 = 4⨯4 matrix [row-1 (0 0 1 0), row-2 (0 0 0 -1), row-3 (1 0 0 0), row-4 (0 -1 0 0)],\n𝛽^2 = 4⨯4 matrix [row-1 (1 0 0 0), row-2 (0 1 0 0), row-3 (0 0 -1 0), row-4 (0 0 0 -1)],\nOne remarkable feature of the expression is that the matrices are all made from 2⨯2 Pauli spin matrices as\n(14.10) 𝜎^𝑥 = 2⨯2 matrix [row-1 (0 1), row-2 (1 0)], 𝜎^𝑦 = 2⨯2 matrix [row-1 (0 -𝑖), row-2 (𝑖 0)], 𝜎^𝑧 = 2⨯2 matrix [row-1 (1 0), row-2 (0 -1)].\nwhere 𝛽^ can be expressed in terms of unit matrix 𝐼. Thus\n(14.11) 𝛼^𝑗 = 2⨯2 matrix [row-1 (0 𝜎^𝑗), row-2 (𝜎^𝑗 0)], (𝑗 = 1,2,3) and 𝛽^ = 2⨯2 matrix [row-1 (𝐼 0), row-2 (0 -𝐼]\nwhere 𝜎^1 ≡ 𝜎^𝑥 etc.\nThe fact that the Dirac equation is a matrix equation implies that 𝜓 is a now a vector formed out of four functions\n(14.13) 𝜓 = column matrix [𝜓1, 𝜓2, 𝜓3, 𝜓4] ≡ column matrix [𝜓+, 𝜓-]\nwhere 𝜓+ and 𝜓- are two-component vector defined by (14.13).\nThe Dirac equation is studied such that a Lorentz covariant form is given by using 𝛾𝜇 matrices instead of 𝛼^ and 𝛽^ matrices.\n(3.9) 𝑖ℏ(𝛾0∂/∂𝑥0 + 𝛾1∂/∂𝑥1 + 𝛾2∂/∂𝑥2 + 𝛾3∂/∂𝑥3)𝜓 - 𝑚𝑐𝜓 = 0,\nwhere 𝛾0 = 𝛽^, 𝛾𝑖 = 𝛽^𝛼^𝑖 (𝑖 = 1,2,3). When we use Einstein summation convention, it can be written in a short form as follows\n(3.16) (𝑖ℏ𝛾𝜇𝜕𝜇 - 𝑚𝑐)𝜓 = 0,\nwhere 𝜕𝜇 = ∂/∂𝑥𝜇. And if we use natural units such that ℏ = 𝑐 = 1, the equation takes the simpler form as\n(𝑖𝛾𝜇𝜕𝜇 - 𝑚)𝜓 = 0.\nwhich can be written the simplest form using Feynman slash notation, that is, (𝜕/) ≡ 𝛾𝜇𝜕𝜇 as follows\n[𝑖(𝜕/) - 𝑚]𝜓 = 0.\nAccording to the principle of relativity 𝜓'(𝑥') must be a solution which has the form (3.16)\n(3.19) (𝑖ℏ𝛾'𝜇∂/∂𝑥'𝜇 - 𝑚𝑐)𝜓'(𝑥') = 0, ※\nin the primed system, too. And the Dirac equation satisfies the form invariance for the relativity.\nFinally, it is worth noting that that all the coordinates 𝑥0, 𝑥1, 𝑥2, 𝑥3 have the symmetric positions and roles!\nHole Theory \nThe negative 𝐸 solutions to the equation were problematic, but mathematically there is no reason for us to reject the negative-energy solutions. To cope this problem, Dirac introduced hypothesis, known as hole theory, that the vacuum is the many-body quantum state where all the negative-energy electron eigenstates are occupied. It is called Dirac sea. Dirac reasoned that if the negative-energy eigenstates are incompletely filled, each unoccupied eigenstate - called a hole - would behave like a positively charged particle. the hole possesses a positive energy since energy is required to create a particle-hole pair from vacuum. The hole was eventually identified as the positron, experimentally discovered by Carl Anderson in 1932.\nHowever in quantum field theory, a Bogoliubov transformation on the creation and annihilation operators (turning an occupied negative-energy electron state into an unoccupied positive energy positron states and an unoccupied negative-energy eclectron state into an occupied positive energy positron state) allows us to bypass the Dirac sea formalism, formally it is equivalent to it.\nAntimatter of the Utmost Gravity \nDirac gave his Nobel lecture on 12 December 1933 and suggested that Universe could contain both matter and antimatter without us knowing. If Dirac were right, the whole Universe should be a uniform mix of matter and antimatter. Where is this antimatter? Certainly there is no accumulated antimatter on Earth, nor even in the Solar System. If the Universe contains antimatter, examples being the positron and the kaon, then cosmic ray should also contain in cosmic rays.'Fountains' of positrons have been seen by satellite-borne detectors peering into the centre of our Galaxy, but there are no fountains of any other sort of antiparticle.\nWhere has all the Big Bang antimatter gone? If we cannot see any antimatter, perhaps matter and antimatter are separated into distinct domains. The Universe we know is a matter domain. Maybe somewhere else there is a corresponding antimatter domain. Wherever and whenever the boundaries of the domain and antidomain briefly touched, pieces of matter and antimatter would have mutually annihilated to give powerful bursts of radiant energy - gamma rays. As the Universe subsequently cooled down, these gamma rays would have cooled down too and produced a dim but uniform cosmic gamma-ray signal all over the sky. In 1991, the Space Shuttle Atlantis placed into orbit a new eye, the Gamma Ray Observatory (GRO). GRO clearly saw bursts from out space against a faint but uniform gamma-ray backdrop. The gamma-ray background shows no sign of matter-antimatter annihilation processes ever having taken place on a large scale. the Universe we can see looks to have been eternally free of nuclear antimatter.\nThe evolution of the whole Universe has been and still is controlled by the all-pervading forces of gravity and gravity will ultimately seal its fate. If a certain probability of finding a transient quantum pin-point is plugged into Einstein's equation of general relativity, the equation reveal that the quantum bubble expands faster even than the speed of light, doubling its size in just 10-34 of a second. Through called 'inflation' by cosmologist, the matter and antimatter had to resolve their differences, This tug-of-war between the initial expansion phase of gravity and the subsequent pull of aggregate matter has been continuing eve since. However Black hoes caused by the collapse of an antimatter star would look just the same as any other black hole. Moreover, the expansion of the Universe now appears to be gently speeding up rather than slow down. To allow for these effects, some bold theorists suggested that the new face face of gravity is repulsive between matter and matter or antimatter, and antimatter, but is attractive between matter and antimatter. Our understanding of cosmology and the origin of the Universe would require a major rethink, a Copernican revolution for the twenty-first century.\nThe Mystery of the Missing Antimatter \nThe established wisdom is that the energetic fireball of the Big Bang fourteen billion years ago spawned matter and antimatter in perfect balance. If in the first moment matter and antimatter emerged equally from the Big Bang, an instant later they should have annihilated one another. The mystery is more a question of why has matter survived? Searches for antimatter in the rays above the atmosphere are being made by the AMS (Anti Matter Spectrometer) satellites and so on. However none has been seen, not even anything as simple as antihelium but the abundance of individual positrons and antiprotons. All of the evidence suggests that, with the exception of transient antiparticle, everything within several hundred million light years of us is made of matter. Everything that we know about the early universe, from theory, observation and the results of experiments at LEP (Large Electron-Positron Collider), suggests that in the hot aftermath of the Big Bang those numbers would have been ten billion quanta of radiation, ten billion antiprotons, and ten billion and one protons. The inference is that one of the first acts after creation was a Great Annihilation such that the matter-dominated universe today is made from the surviving one out of ten billion protons.Somewhere in the first moments of the universe, earlier than the billionth of a second that was studied by the experiments LEP, an imbalance between matter and antimatter must have emerged.\n* Reference:  T. Reding Dirac equation (Wikipedia 10 March 2021)\n D. A. Fleisch A Student's Guide to the Schrödinger Equation (Cambridge University Press 2020)\n P. A. M. Dirac The Principles of Quantum Mechanics (4th edition, Oxford University Press 1958)\n W. Greiner Relativistic Quantum Mechanics - Wave Equations (3rd edition, Springer 2000)\n Rae Alastair I. M; Napolitano J. Quantum Mechanics (6th edition CRC Press 2016)\n G. Fraser Antimatter - The Ultimate Mirror (Cambridge University Press 2000)\n F. Close Antimatter (2nd edition, Oxford University Press 2018)\n※ attention: the detailed rigorous derivations are omitted here","[Physics FAQ] -\nOriginal by David Brahm.\nBaryogenesis: Why Are There More Protons Than Antiprotons?\nHow do we really know that the universe is not matter-antimatter\n- The Moon: Neil Armstrong did not annihilate, therefore the moon\nis made of matter.\n- The Sun: Solar cosmic rays are matter, not antimatter.\n- The other Planets: We have sent probes to almost all. Their survival\ndemonstrates that the solar system is made of matter.\n- The Milky Way: Cosmic rays sample material from the entire galaxy.\nIn cosmic rays, protons outnumber antiprotons 104 to 1.\n- The Universe at large: This is tougher. If there were antimatter\ngalaxies then we should see gamma emissions from annihilation. Its absence\nis strong evidence that at least the nearby clusters of galaxies (e.g., Virgo)\nare matter-dominated. At larger scales there is little proof.\nHowever, there is a problem, called the \"annihilation catastrophe\"\nwhich probably eliminates the possibility of a matter-antimatter symmetric\nuniverse. Essentially, causality prevents the separation of large chucks\nof antimatter from matter fast enough to prevent their mutual annihilation\nin the early universe. So the Universe is most likely matter dominated.\nHow did it get that way?\nAnnihilation has made the asymmetry much greater today than in the\nearly universe. At the high temperature of the first microsecond, there\nwere large numbers of thermal quark-antiquark pairs. Kolb and Turner\nestimate 30 million antiquarks for every 30 million and 1 quarks during\nthis epoch. That's a tiny asymmetry. Over time most of the antimatter has\nannihilated with matter, leaving the very small initial excess of matter to\ndominate the Universe.\nHere are a few possibilities for why we are matter dominated today:\n- The Universe just started that way.\nNot only is this a rather sterile hypothesis, but it doesn't work under\nthe popular \"inflation\" theories, which dilute any initial abundance.\n- Baryogenesis occurred around the Grand Unified (GUT) scale (very early).\nLong thought to be the only viable candidate, GUT's generically have\nbaryon-violating reactions, such as proton decay (not yet observed).\n- Baryogenesis occurred at the Electroweak Phase Transition (EWPT).\nThis is the era when the Higgs first acquired a vacuum expectation value\n(vev), so other particles acquired masses. Pure Standard Model physics.\nIn 1967 Sakharov enumerated 3 necessary conditions for baryogenesis:\n- Baryon number violation. If baryon number (B) is conserved in all\nreactions, then the present baryon asymmetry can only reflect asymmetric\ninitial conditions, and we are back to the first case in the previous list.\n- C and CP violation. Even in the presence of B-violating\nreactions, without a preference for matter over antimatter the B-violation\nwill take place at the same rate in both directions, leaving only a very\ntiny statistical excess, perhaps only enough matter to make one star\nin the observable universe.\n- Thermodynamic Nonequilibrium. Because CPT guarantees equal\nmasses for baryons and antibaryons, chemical equilibrium would drive the\nnecessary reactions to correct for any developing asymmetry.\nIt turns out the Standard Model satisfies all 3 conditions:\n- Though the Standard Model conserves B classically (no terms in\nthe Lagrangian violate B), quantum effects allow the universe to tunnel\nbetween vacua with different values of B. This tunnelling is very\nsuppressed at energies/temperatures below 10 TeV (the \"sphaleron mass\"),\nmay occur at future supercollider energies (controversial), and\ncertainly occurs at higher temperatures.\n- C-violation is commonplace. CP-violation (that's \"charge\nconjugation\" and \"parity\") has been experimentally observed in kaon\ndecays, though strictly speaking the Standard Model probably has\ninsufficient CP-violation to give the observed baryon asymmetry.\n- Thermal nonequilibrium is achieved during first-order phase\ntransitions in the cooling early universe, such as the EWPT (at T = 100 GeV\nor so). As bubbles of the \"true vacuum\" (with a nonzero Higgs vev)\npercolate and grow, baryogenesis can occur at or near the bubble walls.\nA major theoretical problem, in fact, is that there may be too\nmuch B-violation in the Standard Model, so that after the EWPT is\ncomplete (and condition 3 above is no longer satisfied) any previously\ngenerated baryon asymmetry would be washed out.\n- Kolb and Turner, The Early Universe\n- Sakharov, JETP, 5, 32 (1967)\n- Dine, Huet, Singleton & Susskind, Phys.Lett.B257:351 (1991)\n- Dine, Leigh, Huet, Linde & Linde, Phys.Rev.D46:550 (1992)."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:90d0b256-699f-46c1-9b8d-7bbfe730bf6a>","<urn:uuid:a5dd0b9c-02c6-4fec-8989-520f8894d3d9>"],"error":null}
{"question":"What's the difference between rigid orthotics and ankle braces in terms of their primary purpose and construction?","answer":"Rigid orthotics and ankle braces serve different primary purposes and have distinct constructions. Rigid orthotics are designed to control foot function and motion in two major foot joints below the ankle joint. They are made of firm materials like plastic or carbon fiber, extending from the heel to the ball or toes of the foot, and are mainly used in walking or dress shoes. In contrast, ankle braces are medical pieces of fabric wrapped around the ankle to immobilize the joint, provide heat and compression for healing. They can be made of stretchable fabric or include metallic plates for severe injuries, and are primarily used for injury prevention and rehabilitation, particularly in sports-related ankle injuries.","context":["Information From The American Podiatric Medical Association\nWhat are Orthotics?\nOrthotics are shoe inserts that are intended to correct an abnormal, or irregular, walking pattern. Orthotics are not truly or solely “arch supports,” although some people use those words to describe them, and they perhaps can best be understood with those words in mind. They perform functions that make standing, walking, and running more comfortable and efficient by altering slightly the angles at which the foot strikes a walking or running surface.\nDoctors of podiatric medicine prescribe orthotics as a conservative approach to many foot problems or as a method of control after certain types of foot surgery; their use is a highly successful, practical treatment form.\nOrthotics take various forms and are constructed of various materials. All are concerned with improving foot function and minimizing stress forces that could ultimately cause foot deformity and pain.\nFoot orthotics fall into three broad categories: those that primarily attempt to change foot function, those that are primarily protective in nature, and those that combine functional control and protection.\nThe so-called rigid orthotic device, designed to control function, may be made of a firm material such as plastic or carbon fiber and is used primarily for walking or dress shoes. It is generally fabricated from a plaster of paris mold of the individual foot. The finished device normally extends along the sole of the heel to the ball or toes of the foot. It is worn mostly in closed shoes with a heel height under two inches. Because of the nature of the materials involved, very little alteration in shoe size is necessary.\nRigid orthotics are chiefly designed to control motion in two major foot joints, which lie directly below the ankle joint. These devices are long lasting, do not change shape, and are usually difficult to break. Strains, aches, and pains in the legs, thighs, and lower back may be due to abnormal function of the foot, or a slight difference in the length of the legs. In such cases, orthotics may improve or eliminate these symptoms, which may seem only remotely connected to foot function.\nThe second, or soft, orthotic device helps to absorb shock, increase balance, and take pressure off uncomfortable or sore spots. It is usually constructed of soft, compressible materials, and may be molded by the action of the foot in walking or fashioned over a plaster impression of the foot. Also worn against the sole of the foot, it usually extends from the heel past the ball of the foot to include the toes.\nThe advantage of any soft orthotic device is that it may be easily adjusted to changing weight-bearing forces. The disadvantage is that it must be periodically replaced or refurbished. It is particularly effective for arthritic and grossly deformed feet where there is a loss of protective fatty tissue on the side of the foot. It is also widely used in the care of the diabetic foot. Because it is compressible, the soft orthotic is usually bulkier and may well require extra room in shoes or prescription footwear.\nThe third type of orthotic device (semirigid) provides for dynamic balance of the foot while walking or participating in sports. This orthotic is not a crutch, but an aid to the athlete. Each sport has its own demands and each sport orthotic needs to be constructed appropriately with the sport and the athlete taken into consideration. This functional dynamic orthotic helps guide the foot through proper functions, allowing the muscles and tendons to perform more efficiently. The classic, semirigid orthotic is constructed of layers of soft material, reinforced with more rigid materials.\nOrthotics for Children\nOrthotic devices are effective in the treatment of children with foot deformities. Most podiatric physicians recommend that children with such deformities be placed in orthotics soon after they start walking, to stabilize the foot. The devices can be placed directly into a standard shoe or an athletic shoe.\nUsually, the orthotics need to be replaced when the child’s foot has grown two sizes. Different types of orthotics may be needed as the child’s foot develops and changes shape.\nThe length of time a child needs orthotics varies considerably, depending on the seriousness of the deformity and how soon correction is addressed.\nOther Types of Orthotics\nVarious other orthotics may be used for multidirectional sports or edge-control sports by casting the foot within the ski boot, ice skate boot, or inline skate boot. Combinations of semiflexible material and soft material to accommodate painful areas are utilized for specific problems.\nResearch has shown that back problems frequently can be traced to a foot imbalance. It’s important for your podiatric physician to evaluate the lower extremity as a whole to provide for appropriate orthotic control for foot problems.\n- Wear shoes that work well with your orthotics.\n- Bring your orthotics with you whenever you purchase a new pair of shoes.\n- Wear socks or stockings similar to those that you plan on wearing when you shop for new shoes.\n- Return as directed for follow-up evaluation of the functioning of your orthotics. This is important for making certain that your feet and orthotics are functioning properly together\nThe doctors at the Foot & Ankle Clinics of Utah have been trained specifically and extensively in the diagnosis and treatment of all manner of foot conditions. This training encompasses all of the intricately related systems and structures of the foot and lower leg including neurological, circulatory, skin, and the musculoskeletal system, which includes bones, joints, ligaments, tendons, muscles, and nerves.\nIf you are are experiencing foot pain, contact one of our Doctors at the Foot & Ankle Clinics of Utah.","Ankle sprains are the most common sports related injuries among athletes. According to statistics, nearly two million ankle sprain injuries are reported every year in the United States. When an athlete experiences an ankle injury, he is put to bed for several weeks and is suggested to refrain from games during this entire time frame.\nThis kind of break can be excruciating for someone who lives to play. Many athletes have known to suffer major trauma because of these injuries and not because they get hurt, but because they lose major opportunities that come their way while they are on bed rest.\nSeeing this, the concept of ankle wrapping was introduced more than 60 years ago. This was a practical method for decreasing the pain and number of injuries caused during extreme sports and training exercises. As the time went by, ankle wrapping turned into the manufacturing of professional ankle braces that have now made life easier of all the runners and athletes.\nWhat is an ankle brace?\nAn ankle brace is a medical piece of fabric that is wrapped around the ankle to immobilize the joint and provide heat and compression to the bones in order for it to heal. Ankle brace is more like a walking boot for sprained ankle. These are sturdy preventative measures that are suggested by all the physicians for anyone who is involved in running or impact sports.\nThere are different types of ankle braces available in the market for all kinds of sports. You will easily find football ankle braces, volleyball ankle braces, soccer ankle brace, athletic ankle brace and even ankle brace for running, as these sports are the most common ones to cause ankle injuries.\nThere are specific ankle braces available in the market that will cater to your needs regarding these sports while some are usually used for healing process.\nMost of these ankle braces are commonly used for rehabilitation purposes. Some ankle braces are made up of stretchable fabric while some are constructed with metallic plates for better immobilization in severe injuries.\nThere are several benefits of using ankle braces. They will effectively speed up your healing process and also prevent you from getting future injuries.\nConditions That Require The Use of Ankle Braces\nThere are mainly two reasons for using an ankle brace continuously, a knee injury or a high ankle sprain.\nAccording to the American Orthopedic Foot and Ankle Society, “a high ankle sprain is an injury to the ligaments above the ankle that connect the tibia to the fibula.”\nThis injury is usually caused by rotating or twisting of the foot while the ankle stays in one position and is unable to move freely. You end up tearing your high-ankle ligaments and this is where you need a high ankle sprain brace.\nThis was one medical condition while there are several others, some of them we have mentioned below.\nThe most common injury among athletes and even non-athletic people is an ankle sprain. It can occur just by walking and doesn’t really need you to indulge in impact sports.\nWhen ankle sprains happen, your foot goes into an unnatural movement and causes ligaments to stretch that result in inflammation and pain. The ligament damage can range from complete rupture to a small tear.\nYou will need a sprained ankle brace to heal from this injury. You may also require a complete bed rest for a few days as well.\nRunning athletes and basketball players are the most vulnerable to a condition known as Achilles Tendonitis. The Achilles tendon connects the calf muscles to the heel bone and is prone to rupture.\nThis tendon is basically used during running and jumping. When you play basketball for continuous and repetitive hours, it becomes vulnerable to injury and may become inflamed. You will need to use the best ankle brace for basketball in order to prevent this from happening.\nThis condition causes a lot of pain and may even cause you to leave your game for several weeks. You will also need to use a basketball knee brace as soon as you resume playing again for preventative reasons.\nChronic Ankle Instability\nChronic ankle instability is a condition that needs an active ankle brace. In this condition, the outer side of the ankle feels like it may give out and the ankle itself feels highly unstable.\nThis occurs when you have had several ankle sprains and they haven’t had enough time and support to heal completely. In this condition, you need the best ankle support braces that you can find and stay off your feet for a while.\nThis is a highly dangerous condition that may result in a broken ankle if it prolongs. Chronic ankle instability also occurs because of running on a sprained ankle without any support. So, if you feel you have just experienced an ankle sprain, it is highly recommended by the physician that you take your time to heal properly before getting back to your routine.\nOnce healed, find yourself the best ankle braces for running as a prevention from future ankle sprains.\nThe Plantar Fasciitis is another common condition among athletes. It occurs when the Plantar Fascia, a tissue that runs from the front of the foot to the heel, gets ruptured due to acute pressure and muscle stress.\nThe ruptured can be caused by an unnatural landing or quick cut on the court.\nMost of the people who suffer from this condition have this misunderstanding that using an ankle brace will decrease their performance however, that isn’t the case. If you wear an ankle brace properly and make use of specially designed shoes for Plantar Fasciitis, you can prevent re-injury and heal yourself without minimizing your movement.\nAnkle Stress Fractures\nAnkle stress fractures are known to be tiny cracks in the bones caused by overusing tired muscles. When a person is suffering from ankle stress fractures, his bones lose the capability to absorb shock from repetitive impacts.\nThe bones try to absorb the effects of the impact and that’s when you may feel pain, swelling and tenderness around your ankle. You can find the best ankle brace for sprain or you can even use ankle stabilizers to cope with this condition.\nAnkle stress fractures may last for a week or two during which you must take complete bed rest.\nSo, these are the few conditions in which you need to use an ankle brace. You also need to consult with your physician or an orthopedic for a professional advice regarding the entire healing process.\nSuggested to Wear Ankles Braces When:\nThe conditions that we have mentioned above are when you can use an ankle brace. Other than those conditions, following are a few scenarios where you should always take precautionary measures and keep yourself safe from ankle injuries.\nGoing for a run\nFrequent running may causes ankle sprain. You never know when you may twist your foot and end up with an excruciating pain. Wear an ankle brace for running to keep yourself safe from ankle sprains.\nPlaying extreme sports\nImpact sports such as volleyball, football, baseball, basketball and squash will make you more vulnerable to knee, ankle or elbow injuries.\nIt is highly recommended by the doctors that you always look for an ankle brace that will cater to your needs for a specific sport. For instance, if you play volleyball, you will easily find volleyball knee brace available in the market that is designed exclusively for the specific game.\nThe same way, you can easily find some of the best volleyball ankle brace or a baseball ankle brace that will cater to the needs of their respective sports.\nOther than that, you need to wear an ankle brace when you are exercising. If you are a gym freak and have a fitness regime under an experienced trainer, you are in good hands. But if you’re not exercising under supervision, always wear ankle braces for exercises that have an impact on your feet and legs.\nSo, these are a few times when you should wear an ankle brace and be vigilant about it. Make sure to do not overuse an ankle brace because that is harmful as well. Your rehabilitation process matters the most and you need to make sure you do not overuse your tired muscles."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ef8f2d45-52cb-4f1b-92ec-84dfefbf14b3>","<urn:uuid:11b8cd36-69d6-4796-b4fe-d542ec0a38d9>"],"error":null}
{"question":"What role does surgical timing play in treating eyelid trauma versus appendicitis? What are the potential complications if surgery is delayed in each case?","answer":"In eyelid trauma, surgery within 12-24 hours offers the highest chances of effective treatment without future complications. If delayed, complications may include ptosis if the levator muscle injury isn't repaired, and infection risks from foreign debris. For appendicitis, delayed surgery can lead to a progression from early catarrhal stage to more severe complications: phlegmon state, gangrenous stage with necrosis, perforation, and ultimately peritonitis - first localized then generalized, which can be fatal. The appendix can progress to perforation with abscess formation and generalized peritonitis, particularly rapid in children. This progression can lead to death within a short time if emergency surgery isn't performed.","context":["Trauma to the any body parts can pose a lot of challenges to a surgeon. Specifically, trauma of the eyelids can be quite complex in nature and may become a test of skill of an ophthalmologist. An ophthalmologic surgeon not only needs expertise in ocular as well as orbital anatomy but also great precision and skill as far as the trauma surgery of the eyelids is concerned.\nBefore choosing any surgical approach for the management of an eyelid trauma surgery, it is important that the entire ocular system is thoroughly checked. Both minor as well as major trauma can cause a lot of ocular problems which may cause retinal detachment, angle recession etc. the globe of the eye structure is dealt with before attending to the condition of the eyelids so that no undue pressure is applied to the globe. It is easier to make a surgical evaluation of the globe before the eyelids are treated.\nTo begin treatment of the trauma of the eyelids, it is very important to know how the injury was caused in the first place. It is crucial to know the nature of the injury, the time when the injury happened, if the patient was wearing any safety goggles or contact lenses and if there could be any foreign debris in the eyes. If the injury involves any chemicals it is important that the eyes are flushed out immediately. To understand the extent and the type of injuries, some surgeons compare the old picture of the patient like the one that may be present on an ID with current condition. This can give the surgeon an idea about how far the injury has been sustained. Before beginning the treatment the medical history of the patient including any present medical conditions and allergies should be communicated to the surgeon,\nBroadly, eyelid trauma injuries can be classified as blunt trauma injuries and penetrating trauma injuries. Though the classification of the injuries is exclusive, many injuries can have common characteristics.\nPenetrating trauma injuries can run deeper and cause much more damage. The levator muscles and the canthal tendons may get detached or tissue oedema may be caused due to blunt trauma injuries. If the eyelid skin has been penetrated there are chances of the presence of foreign bodies in the eyes. In some cases, the foreign particles may be present in the outer orbital area but it may not be evident during the testing.\nTo test for any ocular injury, a detailed step by step examination is done. The eyes are checked for vision, reaction of pupils to light, pressure in the ocular area and ocular movements. For evaluating the condition of the eyelids, both the nervous and the muscular condition of the eyelids are analyzed. The functions of the nervous system may be disrupted because of the trauma. The condition of the cornea is affected by the functioning of the nerves. In many cases, if the levator muscle is injured due to trauma and is not repaired it can lead to ptosis after the trauma surgery. If there is swelling present in the eyelids but the lid seems to function properly, this could mean that the integrity of the levator muscle has not been compromised. If the injury is caused by massive blunt trauma and foreign debris has entered the eyes, it may lead the eyelid tissues unusable. Therefore, great care is taken by the surgeon to assess what type of tissue and how much of the tissue is usable.\nSurgical repair of the eyelid trauma\nIf the eyelid trauma is addressed within 12 – 24 hours of the injury, there are very high chances of the treating the eyelid trauma effectively and without any future complications. It is very important to completely remove any foreign debris from the eyes before beginning the surgical treatment to eliminate any possibility of infection. The process of removal of foreign debris from the eyes is often done along with saline irrigation. If the eyelid injury is caused due to animal bites it can be repaired well, due to good blood supply. Before the wounds are sutured, irrigation is done on them to clean them and excess of blood clot is also removed. It is necessary to perform this to allow better healing of the wound. The edges of the skin tissues should be handled with great care by the surgeon so that healing is accelerated. The types of sutures that are placed are chosen by the surgeon. The sutures that are placed on the skin of the eyelid are removed within four to five days of the surgery. The total times for the wounds of an eyelid trauma take around a complete year to heal. Even after the surgery great care should be taken by the patient to avoid any complication.","Appendicitis, Acute appendicitis is the most common abdominal surgical emergency. This condition commonly occurs between the ages of 10 and 30 years, but its occurrence is possible at any age, are known cases of particular events in elderly patients (pseudotumor forms or pseudoocluziva).\nAcute appendicitis is a disease in which there is an inflammation of the appendix. Once on the inflammatory process can not be stopped medication, treatment of this condition is surgical. As surgical intervention is faster, the patient’s pain is lower and more easily realizable surgery complications as reduced frequency and severity. I\nn If surgery is delayed, the disease may be complicated: the catarrhal stage (early), the state phlegmon, gangrenous later (necrosis body) with its perforation and peritonitis initially localized and then generalized and can cause the patient’s death.\nVermiform appendix is a tubular structure attached to the first portion of the large intestine (colon), section called check. Appendix base is implanted in the cecum, the most common medial and inferior to the union of the three tapeworms (smooth muscle strips), the most common being mezocecala position.\nBut also may have atypical positions (subhepatic, pelvic, etc.). The abdominal wall, it is projected on the right side of the abdomen in the lower, right iliac fossa region called.\nIs lymphoid structure (hollow organ abundant lymphatic tissue), but its function is not well known and is considered primarily a vestige embryo. Removal to the appendectomy (surgery that is extracted when it is inflamed appendix) does not alter digestive function.\nBecause this disease can be, in many cases, obstruction of the lumen, the body cavity or due to bacterial proliferation stimulated lymphatic or small foreign body (stone fruits, seeds).\nThis blockage of the lumen causes an increase in intraluminal pressure (inside the body), a disorder of blood circulation and inflammation in the body walls it can evolve without treatment until gangrene and perforation (rupture).\nCan be discussed and a genetic predisposition to obstruction causing appendicitis appendicular lumen, starting from the observation that there are families with more subjects apendicectomizati (surgery for appendicitis) than others.\nThese are varied but have certain characteristics:\n– Pain – is initially located in the epigastrium (the chest) and around the navel, and later to modify based right iliac fossa, often radiating to the right lower limb, pain intensity is different and variable from one stage to another; if initially looks like a discomfort, then it can become intense and even defensive muscle (abdomen becomes hard to the touch)\n– Inappetence (lack of appetite), nausea and vomiting, constipation or diarrhea are other symptoms that may occur\n– Subfebrilitatea is possible – not very high fever.\nIn some cases, symptoms can develop very quickly without representative, therefore presenting to an emergency doctor, these cases are the immunosuppressive therapies used in patients with organ transplants and certain diseases, patients with HIV, patients with diabetes mellitus, neoplastic disease treated with chemotherapy obese patients.\nIf pelvic appendix to these symptoms can be associated rectal tenesmus (cramps) with diarrhea.\nAlso pregnant women, young children and the elderly may have particular forms of manifestation of this disease. Women during pregnancy often have symptoms such as pain, nausea and vomiting, but when they have special shapes and intensities, the patient must be present to emergency surgeon for examination.\nAppearance of the disease raises special concerns because of their inability to communicate pain doctor. They may have atypical symptoms – just vomiting, drowsiness, difficulty in feeding, constipation, etc.. Also, specialized medical consultation should be done as quickly, especially if small children are often quick evolution, often even without intermediate phases.\nManifestations may differ, diagnosis is more difficult. May take the form pseudoocluzive – with symptoms like intestinal obstruction and pseudo – Ceca tumor-like symptoms. To them, a feature represents comorbidities (diseases associated with old age Typical): ischemic heart disease, hypertension, type 2 diabetes, kidney failure and so on, conditions that worsen the prognosis of the disease and that can and they decompensate during disease evolution .\nIn the diagnosis of this disease, an important role is occupied by history, medical history and physical examination.\nPhysician should carefully formulate questions, questions of which to deduce the nature of symptoms, when they occur and time evolution, their location, severity of symptoms. Also the patient will be asked by the existence of other diseases in history (personal or family), any ongoing treatments, allergies to medications or substances (elements to be taken into consideration in drug therapy during hospitalization), alcohol consumption, smoking and possible drug use must be mentioned, which is important information.\nThis involves inspection and can uncover a possible rash, swollen lymph (lymph increase), other skin lesions, respiratory movements mobility with the wall (peritonitis important for diagnosis, stage the patient shows a rigid abdomen movements without mobility Respiratory – abs “wood”).\nPalpation can detect right iliac fossa pain that radiates frequently right leg, pain intensity is variable from discomfort to push deep muscle pain with defense. Entire abdomen should be palpated.\nThere are certain signs with more specificity:\n– Sign Blumberg, push deep and then immediate release of the abdominal wall sign followed by a short-lasting pain at this level, which is evidence of peritoneal irritation, acute abdominal pain or implicitly a Heightened\n– Psoas sign – palpation of the iliac fossa and the patient’s recommendation to raise the right leg stretched, noticing an increase in pain at this level.\nPercussion and auscultation are not highly specific in diagnosing this disease. Percussion can detect an area of dullness in the iliac fossa where a plastron timpanism up or if CECA stasis in various stages of the disease. Auscultation may reveal one abdominal silence in appendicular peritonitis established.\nIt will measure the patient’s temperature, heart rate (pulse), respiratory rate and blood pressure.\nSpecific for acute appendicitis are:\n– Moderate leukocytosis – increase in blood leukocytes (white blood cells), usually reaching values 10.000/mmc but can get to 20-30.000/mmc in severe cases\n– Can be highlighted changes in ionogramei in cases of vomiting and dehydration\n– Urinalysis – laboratory examination is common for differential diagnosis of a urinary tract disorder\n– Test task for young women presenting these symptoms.\nCommon imaging tests are:\n– Simple abdominal radiography or dye (Radiography or gastrointestinal radiography)\n– Abdominal ultrasound\nThese tests are not specific for acute appendicitis.\nSimple abdominal radiography may show a certain level of aerocolie (distended loops in the abdominal flank – the region check) in early stages and in cases with appendicular peritonitis due to bowel distension loops around the abdomen may reveal levels hidroaerice – mark functional occlusion.\nRadiography is contraindicated in acute appendicitis, appendiceal perforation can trigger. It is only useful for differentiating elderly cecal neoplasm.\nAbdominal ultrasound also is not very specific exploration, but due to low costs and lack of harm is common practice especially for the differential diagnosis of acute appendicitis with other types of diseases: the female genital (ovarian cyst, ectopic pregnancy, uterine fibroids with / without necrobiosis, Annex Acute right, and so on – which is why you should be consulted sick and gynecological), diseases of the urinary tract (renal colic right ureteral calculus, cystitis, etc..), other diseases of small pelvis or intestinal (mesenteric lymphadenitis common in children, Meckel diverticulum), etc..\nCT scan is not usual in diagnosing acute appendicitis, being reserved for cases with uncertain diagnosis.\nAs was mentioned above, treatment of appendicitis is surgical and consists of surgery called appendectomy. This is performed in adults in most cases under spinal anesthesia, sometimes under general anesthesia, general anesthesia in children usually is.\nIncision in usual cases, uncomplicated right iliac fossa is small, – 1.5 to 3 cm and can be extended if intraoperative difficulties (atypical positioning, associated pathology – ovarian cysts, adhesions, and so on).\nThere are cases when appendectomy can be achieved through an incision of only 1 cm (if then will practice and intradermal suturing, the scar will be very aesthetic, resulting appreciated especially young patients).\nCan practice surgery and laparoscopy, which involves three small incisions.\nSurgical treatment should be associated and appropriate medical treatment: broad-spectrum antibiotic, analgesic, sometimes gastric antisecretory (patients complaining of postoperative pain in the epigastrium which is due to ligation meso appendicular) administered by infusion during the first 24 hours and then orally.\nAt 12-24 hours after surgery the patient can mobilize can begin to feed on clinical and typically 48-72 hours at the hospital is considering releasing surgeon recommendations: overall diet, avoid exercise 4-6 weeks, return the patient to control and remove wires from 1 week after surgery in general.\nTreatment for complications\nIn more severe cases or complicated (gangrenous acute appendicitis with abscess periapendicular, appendicular peritonitis) incision can be enlarged or median incision can be practiced even subombilicala. Antibiotic treatment will be much stronger and for a longer period and also the recovery will take longer. The surgeon will decide intraoperatively and need drainage peritoneal cavity lavage and frequently practicing the Douglas bag bottom drainage (region in which fluid accumulates collections) with drainage tubes, which will be suppressed from 48-72 hours, depending on the evolution patient.\nComplications of the disease:\n– Appendiceal perforation with abscess\n– Localized and then generalized peritonitis.\nThe evolution of this complication in children is more rapid. This is an extremely serious complication of the disease that can lead to death within a short time without emergency surgery.\nEarly postoperative complications:\n– Parietal suppuration (common complication in complicated cases)\n– Residual abscess (occurs after generalized peritonitis is a serious complication and requires reintervention)\n– Postrahianestezie headache often associated with nausea and vomiting, etc..\nLate postoperative complications:\n– Eventration postapendicectomie (in obese patients do not follow doctor’s recommendations or treadmill exercise after surgery and consists of a parietal defect that can be solved through surgery to restore it)\n– Peritoneal adhesions (after severe cases) that can lead to opportunities volvulari with intestinal occlusion.\n– Appendix is a small tubular structure attached to the first portion of the colon\n– Appendicitis is inflammatory disease of the body and is considered a surgical emergency\n– Symptoms are abdominal pain (usually on the right, but may initially be located in “the chest”), inappetence (lack of appetite), nausea and varstaturi, fever (but not too high) but have retained , not always acute appendicitis has all the symptoms and also may present with atypical symptoms\n– Diagnosis is blamed on personal history, clinical examination (performed by surgeon), laboratory tests and imaging laboratory investigations\n– Disease treatment is surgical and consists of appendectomy\n– Is a disease whose complications (appendicular perforation with generalized peritonitis) are of extreme gravity and may become sick."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7960e105-5fa0-4c33-84c9-ea540ef22d0b>","<urn:uuid:4e73427d-db29-4eb7-a19a-1ce4ef260fa3>"],"error":null}
{"question":"Can green manure affect both soil structure and organic carbon levels?","answer":"Yes, green manure impacts both soil structure and organic carbon levels. When incorporated into the soil, green manure crops improve soil structure by enhancing moisture-holding capacity, water infiltration, and reducing erosion. As the green manure decomposes, it adds organic matter which contains approximately 58% carbon. This organic carbon serves as a food source for soil microorganisms, whose activity further improves soil conditions. The green manure also produces organic acids during decomposition that enhance the availability of nutrients and help build soil aggregation, leading to better overall soil structure.","context":["Introduction to Green Manure in Organic Farming\nOrganic farming depends on organic manures such as farmyard manure, compost, and green manure, etc. Therefore, green manure is one of the most important types of manure used in organic farming. Green manure crops broadly defined as crops grown for the benefit of the soil. The green manuring crops improve the humus, organic carbon, and nitrogen and soil microbial growth. Green manure crop leads to the addition of organic matter to the soil.\nA Step by Step Guide to Green Manure in Organic Farming\nGreen manures are a key part of organic farming, and they serve many different purposes. They prevent soil erosion, improve soil structure, can control weed growth, and most importantly, increase the soil’s fertility. Green manure plays an important role in sustainable annual cropping systems. This method of organic fertilizing has several benefits for the home gardener as well. It is used to define specific crop varieties that are grown into the soil to improve its overall quality. Green manure crops will give a huge range of benefits to future crops and your soil. They are grown completely for the benefits and not for grazing or harvest. Green manure is generally used to improve the soil, for organic matter, nutrients, or to control weeds. Instead of harvest the crop, the green manure is left on the soil surface or worked into the soil. These crops incorporated into a rotation add important benefits but they are a low-cost addition. The advantages of a green manure crop far outweigh the nutrients that they add back into the soil. Also, they feed the soil which feeds the plant rather than the other way around. In feeding the soil a green manure crop feeds all the soil organisms and also improves the fertility of the soil.\nBy growing green manure crops you provide soil microbes with a boost as well as food and an environment they can thrive in. Then, these microbes will convert the nutrients in the green manure into available nutrients for crops. By stimulating these soil organisms and proving a food source, they convert the unavailable nutrients in the soil into available nutrients. By improving the biological activity in the soil you will also improve the soil conditions.\nGreen manures are plants that are grown to benefit the soil nutrients. Green manures are the organic way to;\n- Improve soil fertility, including adding valuable nitrogen levels\n- Develop the soil structure, giving better drainage or water retention\n- Suppress weeds\n- Attract beneficial insects and other predators\nCrop rotation with Green Manure in Organic Farming\nGrowing green manure crops as part of a crop rotation is an important part of organic farming. They are useful when grown before crops that need a lot of nutrients.\nGreen manures can be used in rotation;\n- Whenever there is no crop in the ground, rather than leaving the land bare and allowing weeds to grow and crop nutrients to leach out of the soil.\n- As break crops, when there is a short time between main crops.\n- The timing of sowing is also important. The green manure should be ready to dig in before the crop next is sown. Then, there should not be a long gap between digging in the green manure and planting the next crop. This is to prevent essential nutrients from the green manure from leaching out of the soil, before being taken up by the next crop.\nAdvantages of Green Manure in Organic Farming\nThe purpose of green manure varies depending on each situation but some of the benefits are;\n- The deep-penetrating roots of these crops break open the deep layer of a hardpan of the soil.\n- Brings up the nutrients from the deeper soil layer and make it available to the shallow-rooted crops upon its decomposition.\n- Enrich the soil with biologically fixed nitrogen, adds organic matter, and other macro and micronutrients to the sail.\n- The organic matter added to the soil through green manures acts as food for microorganisms.\n- Green manure protects the soil from erosion and they absorb nutrients from the deeper soil layers.\n- The green plant material buried stimulates the activity of the micro-organisms inhabitant the soil. Then, they respire and decompose the organic matter CO2, which helps in producing carbonic acid. The carbonic acid decomposes the soil essential minerals to release plant nutrients that bind in them.\n- Green material on decomposition produces certain organic acids that enhance the availability of certain plant nutrients such as phosphorus, calcium, potassium, magnesium, and iron.\n- The green manuring crop absorbs soil nutrients and protects them against leaching losses.\n- It improves the soil structure, moisture-holding capacity, and infiltration of water, thus decreasing the runoff and erosion.\n- Increasing organic matter and soil humus\n- Increased Nitrogen fixation\n- Protection of the soil surface\n- Prevention of erosion\n- Maintaining or improving soil structure\n- Reduced susceptibility to leaching\n- Provide readily obtainable nutrients to the next crop\n- Reducing leaching losses\n- Suppressing weeds\n- Reducing pest and disease problems\n- Providing supplementary animal forage\n- Drying and warming the soil\nTypes of Green Manure Crops\n- Bahar Treatment in Pomegranate for High Quality and Yields: A Step-by-Step Guide to Implementation\n- Mobile Veterinary Units in India: Implementation in States\n- Moringa as Feed for Livestock: Moringa Fodder Crop Yield Per Acre\n- National Beekeeping and Honey Mission (NBHM): Features, Schemes, and Benefits\n- Management of Cutworms in Chilli: Prevention and Control With Organic, Chemical, Cultural Practices\n- Best Fertilizer for Tinda: Organic, Natural, Homemade, NPK Ratio, When and How to Apply\nGreen manures are mainly two types;\nGreen manures can be legumes or non-legumes.\nLegumes (clover family)\nLegumes develop on their roots (in association with special bacteria) nodules that can take nitrogen from the air and fix it into a form that the plant can use. Though, this can be utilized by crops grown after the legume has been ploughed and incorporated into the soil.\nLegumes are considered to be nitrogen-fixing but this will only happen in the presence of correct strains of Rhizobium bacteria. These crops make excellent green manures as they have low carbon-to-nitrogen ratios (C: N), which results in a quick release of nitrogen. Because of this, they add nitrogen quickly to the soil but the amount of organic matter contributed to the soil is limited over the long-term.\nNon-legumes do not fix nitrogen but can provide useful organic matter that might otherwise be leached. Non-legumes are primarily used to increase biomass. Some green manure crops like winter wheat and winter rye can also be used for grazing. Some non-legume green manures are quick growing and can be incorporated within gaps in production during the growing season.\nCharacteristics of an Ideal Green Manure Crop\nIdeal green manure crops have the following characteristics;\n- The green manure has low water and nutrient requirement.\n- It should be quick farming to produce abundant biomass.\n- These crops have a deep rooting system, facilitating nutrient mining from subsurface soil.\n- The biomass produced has low fibrous material to facilitate quick decomposition.\n- It has a high capacity to fix atmospheric nitrogen.\n- Capable of establishing and growing quickly.\n- Tolerant to adverse climatic conditions like drought, waterlogging, high temperature and low temperature, etc. and tolerant to pests and diseases.\n- It should possess adequate Rhizobium nodulation potential and must be an effective nitrogen fixer.\n- Green manure should be capable of growing fast and capable of accumulating sufficient fixed Nitrogen in 4 to 6 weeks.\n- Easy to incorporate and quickly decomposable.\nSowing and Fertilization of Green Manure in Organic Farming\nThe green manure crop seeds can be sown from May to June and ploughed down in July. Wheat-fields in north India can be green manured with sun hemp, cowpea, dhaincha, green gram, and black gram, etc. Normally, a higher seed rate is recommended for green manuring. Organic fertilization of green manures with phosphatic fertilizers can be done by broadcast because it improves the availability of phosphorus to the succeeding crop as compared to phosphorus applied to the succeeding crops.\nSelection of Suitable Crop for Green Manure in Organic Farming\nThe selection of crop sown for green manure must be done by the land, climate, and the objective. For green manure, crops have the following properties;\n- Early growing crop.\n- The green manure crop must have a soft stem, branches and leaves are soft and more in number so that it can decompose and the soil can get more and more organic matter.\n- The crop must be of tap root so that it can absorb the nutritive elements from the deep. In alkaline and saline land, the deep-rooted crop is necessary for drawing internal water.\n- The symbiotic bacteria present in the roots fix the free nitrogen obtainable in the atmosphere and provides it to the plants.\n- The green manure crop should be drought resistant. It should also bear a water logging situation. The green manure crop is disease and insect resistant and the capacity of seed production is more.\nOrganic Matter Management with Green Manures\nWhen green manure crops are incorporated into the soil they break down to form soil organic matter. This is important as green manures act as a store for nutrients that are released when they are returned to the soil. Cereals and grasses are high in carbon-containing material and young growth is slightly carbonaceous. As the crop ages and develops more fibrous, the carbon-to-nitrogen (C: N) ratio increases and the material becomes more carbonaceous.\nGenerally, the practice of green manuring in India can be classified in two ways.\nA) Green leaf manuring\nWhere the application of green leaves and twigs of trees, shrubs, and herbs collected from plants grown in field bunds, wastelands, degraded lands, and nearby forest. They are turned down or mixed into the soil 15 to 30 days before sowing of the crops mainly depending on the tenderness of the foliage or plant parts are known as green leaf manuring.\nB) On-farm green manure\nOn-farm green manure is also called legume green manuring. In this system, the short duration legume crops are grown and buried in the same site when they attain the age of 60 to 80 days after seed sowing. This system of on-site nutrient resource generation is prevalent in northern and southern parts of India, where rice is the main crop in the existing cropping systems. Almost any crop can be used for green manuring, but legume crops are preferred because of their ability to fix nitrogen from the air. Green manuring with legumes such as peas, clovers, and lentils, etc. is called legume green manuring. These crops must be turned into the soil before the setting of seeds. It can be profitably used on lands where it was not possible to add animal manures.\nOrganic Nitrogen Management with Green Manures\nSeveral factors influence the release of nitrogen from green manures including soil temperature and moisture. Normally, the nutrient release will be slower at lower soil temperatures because the soil organisms that breakdown organic matter has lower biological activity or work slower at lower temperatures.\nShort term soil nitrogen improvement – Fast cultivated green manure crops such as crimson clover, Persian clover, and fenugreek can be grown in short breaks between cash crops to boost nitrogen levels in the soil. These annual legume crops are used in intensive horticultural systems between vegetable crops. As legumes will fix nitrogen when the soil is above 8C they are effective between April and August.\nLong term soil nitrogen enhancement – Slower growing perennial legumes such as red and white clover, and alfalfa are used to add nitrogen to the soil over a long period. These crops are slow to establish but are persistent, so reduce the need for resowing. They are most sown in a mixture with grasses and are used for silage or grazing in extensive livestock systems while they improve the soil.\nPreventing nitrogen leaching – If soil is left bare for any length of time, rainfall will leach (or wash) nitrogen and other nutrients out, particularly on lighter ground. In many situations reducing leaching is important in maintaining soil fertility than fixing nitrogen. Then, this is true during the winter season, when legumes are slow to establish and fix little nitrogen. Fast-growing crop species with a deep root system are best for preventing leaching.\nYou should not miss this: Clay Soil Treatment Methods.\nMajor Role of Green Manure in Organic Farming\n- Green manure is a type of organic fertilizer where an entire fresh plant or plant part is directly used as manure in agricultural lands, without any prior composting.\n- It increases the soil fertility and productivity by the direct addition of nitrogen and also improves the soil structure, water-holding capacity, and microbial population of soil by the addition of organic matter. Green manuring is practiced based on the suitability of soil and climatic conditions.\n- Green manuring is natural farming. Green manuring helps to improve the soil properties such as physical and chemical properties. It is a method of substituting a basket of compost with a handful of seeds.\n- To improve soil health green manuring is one of the best alternatives and meets the nutritional requirement of the following crop. A periodical application of organic matter is, more important to replace the loss of humus, which is essential for maintaining the soil health in good condition by promoting the growth of microorganisms and by enhancing the supply of nitrogen.\n- Green manuring crops increase the biological activity in the soil and these crops improve soil structure. Green manure crops help in reducing soil erosion. Green manure crops help to increase the supply of nutrients available to plants. These crops help in reducing leaching losses. Also, green manuring crops help to suppress weeds, reducing pest and disease problems, providing supplementary animal forage.\nProblems and Disadvantages of Green Manures\nThere are potentially some problems with the use of green manures in organic farming. These are;\n- Tilling in a heavy nonleguminous crop with a high C: N ratio can result in depressed nitrogen uptake by the next following crop.\n- In low rainfall areas, green manure crops can deplete soil moisture to the point that the succeeding main crop will suffer from drought.\nIn case if you are interested in this: Organic Gardening Questions and Answers.","Decomposition | Forms and Function | Management of Soil Organic Matter | Carbon Cycle\nBut, what is it?\nOrganic matter is critical for soil health and for soil productivity. It:\nOrganic matter derives from the growth and death of organisms. Soil organic matter is:\nOrganic matter is constructed from cellulose, tannin, cutin, and lignin and various proteins, lipids and sugars. These are all based on chains of carbon molecules which mean that a measure of soil organic carbon can give an indirect measure of soil organic matter.\nDecomposed organic matter has a black or dark brown colour and will darken soil colour.\nHow does it get into soil?\n- The living component of the soil (roots, micro-organisms, animals and plants);\n- Exudates from living organisms; and\n- Dead, decaying and highly decomposed materials.\nPlant growth is the primary source of soil organic matter. Photosynthesis converts sunlight, carbon dioxide and water into plant material. On death, the plant material is steadily decomposed and progressively incorporated into the soil.\nWhat is its fate in soil?\nOrganic matter is a dynamic component of soil. Plant and animal debris is regularly added and carbon dioxide is routinely lost as soil organisms use organic matter as an energy source. This is the soil carbon cycle. If the rate of incorporation is low, or the rate of respiration is high, soil organic matter levels will decline. Thus the level of organic matter in soil is dependant on the balance between inputs and losses of soil carbon.\nSoil organisms rely on organic matter as their food source. In doing this some of the carbon chains of the organic matter are converted to carbon dioxide (termed respiration). Organic matter is consequently decomposed. This process of organic matter decomposition is discussed further on following pages.\nHow is it measured?\nThe amount of organic carbon present in the soil is used to estimate organic matter. Various experiments have shown that organic matter contains about 58% carbon. On this basis, the following relationship can be used to estimate levels of organic matter.\nWhy does it matter to soil health?\n(% by weight)\n(% by weight)\nThe primary value of organic matter to soil health is in providing the mechanism for fuelling the soil with energy and nutrients. It provides a reservoir of metabolic energy that enables biological processes to occur.\nAs plant remains steadily decompose in this part of the carbon cycle, secondary benefits occur. Nutrients are mobilised, soil mixing occurs, and soil structure is improved and strengthened as decomposition products adhere to physical soil particles and build aggregation.\nWhat options are there to change organic matter in soil?\nThe key to good management of soil organic matter is in the balance between decomposition rate and replenishment rate. Soil organic matter management is discussed further on following pages.\nA Practical Note on soil organic matter and a Quick Reference Guide on how to measure Active Carbon in your soil are available.\n|Practical Note: Soil Organic Matter|\nSoil organic matter (SOM) is made up of living plants and animals (roots, fungi, bacteria, macro fauna and micro fauna), plant litter, and all the degraded material from decomposing plant and animal material.\n|Quick Reference Guide: Potassium Permanganate Test for Active Carbon|\nActive carbon is an indicator of the fraction of soil organic matter that is readily available as a carbon and energy source for the soil microbial community (i.e., food for the soil food web). The soil is mixed with potassium permanganate (deep purple in colour) and as it oxidizes the active carbon the colour changes (becomes less purple), which can be observed visually, but is very accurately measured with a spectrophotometer."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3e8e5def-578b-4d46-86ae-e3c82827522a>","<urn:uuid:150fc8d4-b707-4741-ab57-9ce02cbe147b>"],"error":null}
{"question":"What are the key differences between anxiety-induced panic attacks and bipolar mood episodes in terms of duration and onset patterns?","answer":"Panic attacks and bipolar episodes differ significantly in their duration and onset patterns. Panic attacks reach maximum intensity within 1-2 minutes and diminish over 30 minutes to several hours. They occur suddenly without warning and without obvious reason. In contrast, bipolar disorder episodes, which involve periods of mania and depression, typically last longer - episodes of mania and depression usually last for one to two weeks in Bipolar I Disorder, and about two weeks in Bipolar II Disorder.","context":["Anxiety and Panic Attacks\nAnxiety is a normal reaction to stress. It helps one deal with tense situations in the\noffice, study harder for an exam, keep focused on an important speech. In general, it\nhelps one cope. But when anxiety becomes an excessive, irrational dread of everyday\nsituations, it has become a disabling anxiety disorder.\nAnxiety can be accompanied by physical effects such as heart palpitations, nausea, chest\npain, shortness of breath, stomach aches, or headaches. Physically, the body prepares to\ndeal with what it perceives as a threat. Blood pressure and heart rate are increased,\nsweating is increased, blood flow to muscle groups increases and immune and digestive\nsystem functions are inhibited (the fight or flight response). External signs of anxiety\nmay include pale skin, sweating, trembling and many others. Someone suffering from anxiety\nmight also experience it as a sense of dread or panic.\nPanic attacks are sudden surges of overwhelming fear that that comes without warning and\nwithout any obvious reason. It is far more intense than having anxiety or the feeling of\nbeing 'stressed out' that most people experience. One out of every 75 people worldwide\nwill experience panic attacks at one time in their lives.\nPeople who have full-blown, repeated panic attacks can become very disabled by their\ncondition and should seek treatment before they start to avoid places or situations where\npanic attacks have occurred. For example, if a panic attack happened in an elevator,\nsomeone with panic disorder may develop a fear of elevators that could affect the choice\nof a job or an apartment, and restrict where that person can seek medical attention or\nPanic attacks are not dangerous, but they can be terrifying, largely because it feels\n'crazy' and 'out of control.' Panic disorder is frightening because of the panic attacks\nassociated with it, and also because it often leads to other complications such as\nphobias, depression, substance abuse, medical complications, even suicide.\nFacts About Anxiety and Panic Attacks\nIn any given year, approximately 40 million American adults 18 years and older are\naffected by anxiety disorders. This startling data means that anxiety disorders cost the\nUnited States more than $42 billion dollars a year. According to \"The Economic Burden of\nAnxiety Disorders\", a study commissioned by the ADAA and published in The Journal of\nClinical Psychiatry, Vol. 60, No. 7 July 1999 this is almost 1/3 of the country’s $148\nbillion total health bill. Those with anxiety disorders are three to five times more\nlikely to seek medical treatment and six times more likely to be hospitalized for\npsychiatric disorders than people that do not suffer from anxiety.\nApproximately 6.8 million are affected by Generalized Anxiety Disorder, 6 million by\nanxiety attacks and panic attacks, 7.7 million by Posttraumatic Stress Disorder, 15\nmillion by social anxiety disorder, 2.2 million by OCD, and 19 million by Specific Phobia.\nAnxiety attacks and panic attacks are the most common emotional disorders and are more\ncommon than bipolar disorder, schizophrenia, alcohol abuse or depression. People with\nAnxiety and panic attacks seek relief for symptoms that mimic physical illnesses which\ntotal more than $22.84 billion and are associated with repeated use of health care\nWhat A Panic Attack Feels Like\nThe main symptom of a Panic Anxiety Disorder is the panic attack itself. Panic\nAnxiety Disorder is a medical disorder characterized by severe and sudden episodes.\nIt is important to mention that sudden episodes of the symptoms listed above caused by\nanother reasonable cause are not panic attacks. Two such reasonable causes would be (1) a\ncertain medical ailment that might mimic a panic attack, or (2) a life threatening\nexperience immediately preceding the attack. If these reasonable causes are found not be\nthe cause of the problem then there is the possibility of a Panic Disorder.\nPanic attacks reach maximum intensity within a minute or two once they begin. They\ndiminish slowly over the next 30 minutes or the next several hours. It is common for the\nfirst attack to cause a person to go to an emergency medical facility. Subsequent attacks\noccur several times a month and are often as severe as the initial attack.\nAbout three fourths of Panic Disorder patients are women. Panic Anxiety Disorder begins\nmost often when people are 20-30 years old. It begins less often in teenagers or persons\nin their forties. It is uncommon for the disorder to appear in the elderly for the first\nIt is important to note that although a few experts say it is more common in persons who\nexperienced a separation experience as a child, many of experts feel that Panic Anxiety\nDisorder afflicts emotionally healthy people. Persons having Panic attacks are no more\nlikely than the average American to have suffered from emotional problems at the time the\nSymptoms of a Panic Attack\n- raging heartbeat\n- difficulty breathing, feeling as though you 'can't get enough air\n- terror that is almost paralyzing\n- nervous, shaking, stress\n- heart palpitation, feeling of dread\n- dizziness, lightheadedness or nausea\n- trembling, sweating, shaking\n- choking, chest pains, distress\n- fear, fright, afraid, anxious\n- hot flashes, or sudden chills\n- tingling in fingers or toes ('pins and needles')\n- fearful that you're going to go crazy or are about to die\nTreatments for Panic Attacks\nSelf-Care at Home\nTaking care of panic attacks at home is possible, but be careful not to mistake another\nserious illness (such as a heart attack) for a panic attack. In fact, this is the dilemma\nthat doctors face when people experiencing panic are brought to a hospital's emergency\ndepartment or the clinic.\nThere are things that people with panic disorder can do to assist with their own recovery.\nSince substances like caffeine, alcohol, and illicit drugs can worsen panic attacks, those\nthings should be avoided. Other tips for managing panic attacks include engaging in\naerobic exercise and stress-management techniques like deep breathing and yoga on a\nregular basis, since these activities have also been found to help decrease panic attacks.\nAlthough many people breathe into a paper bag in an attempt to alleviate the\nhyperventilation that can be associated with panic, the benefit received may be the result\nof the individual thinking it will help (a placebo effect). Unfortunately, breathing into\na paper bag while having trouble breathing can worsen symptoms when the hyperventilation\nis caused by a condition associated with oxygen deprivation, like an asthma attack or a\nIf a person has been diagnosed with panic attacks in the past and is familiar with the\nsigns and symptoms, the following techniques may help the person stop the attack. You may\nalso try these tips for overcoming the symptoms of a panic attack.\n- First, relax your shoulders and become conscious of any tension that you may be\nfeeling in your muscles.\nThen, with gentle reassurance, progressively tense and relax all the large muscle groups.\nTighten your left leg while taking a deep breath in, for example, hold it, then release\nthe leg muscles and the breath. Move on to the other leg. Move up the body, one muscle\ngroup at a time.\nSlow down your breathing. This may best be done by blowing out every breath through pursed\nlips as if blowing out a candle. Also, place your hands on your stomach to feel the\nrapidity of your breathing. This may allow you to further control your symptoms.\nTell yourself (or someone else if you are trying this technique with someone) that you are\nnot \"going crazy.\" If you are concerned about not being able to breathe, remember that if\nyou are able to talk, you are able to breathe.\nIf a person is diagnosed with any medical illness, especially heart disease, home\ntreatment is not appropriate. Even if the person has a history of panic attacks, home care\nis not appropriate if there is any new or otherwise worrisome symptom.\nPanic Attacks Medical Treatment\nGenerally, panic attacks are treated with reassurance and relaxation techniques. By\ndefinition, panic attacks last less than an hour, so many times a person already feels\nmuch better by the time he or she makes it to the doctor's office. Nevertheless, because\nthe diagnosis is made by excluding more dangerous causes, people may be given medications\nduring their attack.\nIf the doctor is suspicious of a cardiac (heart) cause, then the person might be given\naspirin and various blood pressure medicines. An IV line may be started and fluids given.\nSome doctors will prescribe various antianxiety medicines such as\nduring the evaluation.\nOnce the diagnosis of panic attack is made, however, the person may be surprised that no\nmedicines are prescribed. Before medications are started, the person requires further\nevaluation by a mental-health professional to check for the presence of other\nmental-health disorders. These may include anxiety disorders, depression, or panic\ndisorder (a different diagnosis than panic attack).\nIf medications are prescribed, several options are available. Selective serotonin reuptake\ninhibitors (SSRIs), selective serotonin and norepinephrine reuptake inhibitors (SSNRIs),\nand the benzodiazepine families of medications are considered to be effective treatment of\npanic disorder. SSRIs include:\nClinical trials have shown SSRIs reduce the frequency of panic attack up to 75%-85%. SSRIs\nmust be taken three to six weeks before they are effective in reducing panic attacks and\nare taken once daily.\nBeta-blocker medications like propranolol are sometimes used to treat the physical\nsymptoms associated with panic.\nBenzodiazepines are often used to provide short-term relieve of panic symptoms.\nand lorazepam (Ativan)\nare examples of this group of medications.\nAlthough another benzodiazepine, alprazolam (Xanax)\n, is often used to treat panic attacks, the\nshort period of time that it works can cause the panic sufferer to have to take it\nmultiple times each day. Benzodiazepines tend to be effective in decreasing panic attacks\nby up to 70%-75% almost immediately; however, they sometimes require taking up to four\ntimes per day to be effective. Additional drawbacks include sedation, memory loss, and\nafter several weeks, tolerance to their effects and withdrawal symptoms may occur.\nTricyclic antidepressants such as imipramine (Tofranil)\nand MAO inhibitors such as\nhave also been used, but many individuals\nexperience side effects that are difficult to tolerate.\nThe person being treated will be closely monitored for the possibility of side effects\nthat can range from minor to severe and can sometimes even be life-threatening. Because of\nthe possible risks to the fetus of a mother being treated with medications for panic\nattacks, psychotherapy continues to be the treatment of first choice when treatment of\nthis symptom is given during pregnancy.\nPsychotherapy is at least as important as medication treatment of panic disorder. In fact,\nresearch shows that psychotherapy alone or the combination of medication and psychotherapy\ntreatment are more effective than medications alone in overcoming panic attacks. To\naddress anxiety, cognitive behavioral therapy is widely accepted as an effective form of\npsychotherapy. This form of therapy seeks to help those with panic disorder identify and\ndecrease the self-defeating thoughts and behaviors that reinforce panic symptoms.\nBehavioral techniques that are often used to decrease anxiety include relaxation and\ngradually increasing the panic sufferer's exposure to situations that may have previously\ncaused anxiety. Helping the anxiety sufferer understand the emotional issues that may have\ncontributed to developing symptoms is called panic-focused psychodynamic psychotherapy and\nhas also been found to be effective.\nOften, a combination of psychotherapy and medications produces good results. Improvement\nis usually noticed by about three months. Thus, appropriate treatment for panic disorder\ncan prevent panic attacks or at least substantially reduce their severity and frequency,\nbringing significant relief to up to 90% of people with panic disorder.\nResearchers have explored a number of natural remedies as possible treatments for anxiety\ndisorders, including panic disorder. Studies to date have concluded that two alternative\ntherapies, in particular, have potential in the treatment of panic disorder.\nRelaxation training. Relaxation techniques include deep breathing, yoga, meditation and\nprogressive muscle relaxation, which is accomplished by tensing one muscle at a time, and\nthen completely releasing the tension, until every muscle in the body is relaxed. Studies\nhave found that these techniques may be as effective or nearly as effective as cognitive\nbehavioral therapy for some people with panic disorder.\nThe nutritional supplement inositol. This oral supplement, which influences the action of\nserotonin, may reduce the frequency and severity of panic attacks.\nTalk with your doctor before trying any natural therapies. These products can cause side\neffects and may interact with other medications. Your doctor can help determine if they\nare safe for you.\nLifestyle and Home Remedies\nWhile panic attacks and panic disorder benefit from professional treatment, you can also\nhelp manage your symptoms on your own. Some of the lifestyle and self-care steps you can\nStick to your treatment plan. Facing your fears can be difficult, but treatment can help\nyou feel like you're not a hostage in your own home.\nJoin a support group for people with panic attacks or anxiety disorders so that you can\nconnect with others facing the same problems.\nAvoid caffeine, alcohol and illegal drugs, all of which can trigger or worsen panic\nPractice stress management and relaxation techniques. Meditation, yoga and guided imagery\nmay be good options.\nGet physically active, since aerobic activity may have a calming effect on your mood.\nGet sufficient sleep — enough so that you don't feel drowsy during the day.\nThere's no sure way to prevent panic attacks or panic disorder. However, getting treatment\nfor panic attacks as soon as possible may help stop them from getting worse or becoming\nmore frequent. Sticking with your treatment plan can help prevent relapses or worsening of\npanic attack symptoms. Practicing relaxation and stress management techniques may be","Bipolar disorder is a serious mental illness that can affect people of any age, but it’s especially common in teenagers. If you’re a teen and you’re experiencing signs of bipolar disorder, there are some things you should know about how to get help.\n- 1 What is Bipolar Disorder In Teens?\n- 2 Types of bipolar disorder in teens\n- 3 Symptoms of bipolar disorder in teens\n- 4 Diagnosis of Bipolar Disorder In Teens\n- 5 Treatment for bipolar disorder in teens\n- 6 Prevention of bipolar disorder in teens\n- 7 Conclusion\nWhat is Bipolar Disorder In Teens?\nBipolar disorder is a mental health condition that affects teenagers as well as adults. It is a serious illness that causes people to experience episodes of extreme happiness (mania) and depression (manic-depression).\nIn teens, bipolar disorder can cause mood swings that can be hard to control. Teenagers with bipolar disorder may become irritable, have difficulty sleeping, or engage in risky behaviors. They may also exhibit signs of psychosis, such as believing things that are not true.\nIf you are a teenager and you are experiencing signs of bipolar disorder, please talk to your doctor. There is help available, and your doctor will be able to diagnose and treat your condition.\nTypes of bipolar disorder in teens\nBipolar disorder is a mental disorder that affects a person’s mood, emotions, and behavior. It can be mild or severe and can last for months or years. In teens, bipolar disorder is more common than in adults. There are three main types of bipolar disorder in teens: bipolar I Disorder, bipolar II Disorder, and cyclothymia.\nBipolar I Disorder is the most severe type of bipolar disorder in teens. It is characterized by episodes of mania (a high or excessive mood) and depression (a low mood). In some cases, people with bipolar I Disorder experience psychosis (a loss of touch with reality). In this type of bipolar disorder, the episodes of mania and depression usually last for a week or two and come back often.\nBipolar II Disorder is less severe than bipolar I Disorder. People with bipolar II Disorder experience episodes of mania and depression, but they don’t have episodes of psychosis. They may also have hypomania (a milder form of mania) and dysthymia (a milder form of depression). In BipolarII Disorder, the episodes of mania and depression usually last for about two weeks.\nCyclothymia is a type of bipolar disorder that is characterized by alternating periods of high and low moods. people with cyclothymia may have episodes of mania, depression, hypomania, or dysthymia, but they don’t have episodes of psychosis. This type of bipolar disorder usually lasts for about six months.\nSymptoms of bipolar disorder in teens\nBipolar disorder in teens is a serious mental illness that can severely impact the teen’s life. Bipolar disorder is the most common mental illness in teenagers and young adults. Symptoms of bipolar disorder in teens can include:\nOne of the most common symptoms of bipolar disorder in teens is mood swings. Teenagers with bipolar disorder often experience sudden changes in their moods, which can be very intense and unpredictable. This can make it difficult for them to function normally and can lead to problems at school, at home, and in relationships.\nAnother common symptom of bipolar disorder in teens is depression. Teens with bipolar disorder often experience a deep sense of sadness and loneliness combined with a strong sense of hopelessness. This can make school, activities, and everyday tasks extremely difficult to complete.\nFeeling out of Control\nTeenagers with bipolar disorder often feel incredibly out of control. This can lead to feelings of anxiety and fear, as well as problems sleeping and concentrating. It can also be difficult for them to make decisions or handle stress in any way.\nSuicidal Thoughts and Attempts\nTeenagers with bipolar disorder are at an increased risk for suicidal thoughts and attempts. This is due to the severe mood swings and depression that are common symptoms of the condition. If you notice any signs of suicide or self-harm in a teen with bipolar disorder, please contact a mental health professional immediately.\nTeenagers with bipolar disorder often exhibit very unusual behavior. This can include a sudden increase in spending money, binge eating, drug use, or reckless behavior. It is important to watch for any changes in a teen’s behavior that could be indicative of the bipolar disorder. If you think your teen may have the condition, please seek professional help.\nDiagnosis of Bipolar Disorder In Teens\nBipolar disorder in teens is a very common condition. It is also one of the most difficult to diagnose and treat. Only about one-third of teenagers with bipolar disorder are diagnosed with it by the time they reach adulthood.\nThe diagnosis of bipolar disorder in teens can be tricky because many of the symptoms — such as mood swings, difficulty concentrating, and impulsive behavior — are common to many other conditions, such as teenage angst, depression, or bipolar disorder.\nTo make a diagnosis of bipolar disorder in teens, your pediatrician will first need to rule out other possible causes of your child’s symptoms. For example, your pediatrician may ask you about any history of depression or bipolar disorder in your family. He or she also may perform a physical exam and order blood tests to check for signs of bipolar disorder.\nIf your pediatrician determines that your child has bipolar disorder, he or she will likely recommend treatment with medication. In some cases, the child may also need therapy to help manage his or her symptoms.\nTreatment for bipolar disorder in teens\nBipolar disorder in teens is a serious condition that requires immediate treatment. If left untreated, bipolar disorder can lead to a host of negative effects. Some of these treatment options are medication, therapy, and counseling.\nIf medication is considered as a treatment option for bipolar disorder in teens, there are a few things to keep in mind. First, it is important to identify which type of medication is best suited for the individual. Second, it is essential to monitor the patient’s medications regularly to ensure that they are taking them correctly and at the right dosage. Third, it is important to make sure that the teen receives support from a team of healthcare professionals while taking medications.\nTherapy can be an invaluable tool in treating bipolar disorder in teens. In particular, therapy can help teens learn how to manage their symptoms and live more fulfilling lives. Therapy can also provide support during times of transition or when life gets complicated.\nMany people turn to counsel when seeking treatment for bipolar disorder in teens. Counseling can provide support during times of difficult change or when life gets overwhelming. Counseling can also guide dealing with daily challenges and developing healthy relationships. Also, be sure to ask about any insurance coverage that may be available for counseling.\nPrevention of bipolar disorder in teens\nPrevention of bipolar disorder in teens starts with early recognition and diagnosis. To prevent a child or teenager from developing bipolar disorder, parents and guardians need to be aware of the signs and symptoms of the disorder. Some of these ways to recognize the bipolar disorder in a child or teenager include:\nCreate a Routine\nOne of the most important things that parents and guardians can do to help prevent bipolar disorder in teens is to create a routine. This includes establishing regular sleep and wake times and establishing a consistent daily schedule. This will help to ensure that the child is staying on track, and will reduce the risk of them becoming overwhelmed or stressed.\nEncourage Social Participation\nOne of the best ways to prevent bipolar disorder in teens is to encourage social participation. This means providing opportunities for the teen to meet new people, participate in extracurricular activities, and engage in other forms of activity. This will help to increase their socialization skills, which can be important for preventing the development of the bipolar disorder.\nIncrease Exercise Levels\nAnother important way to prevent bipolar disorder in teens is to increase their exercise levels. This will help to increase their overall physical health and well-being, as well as their moods. Exercising regularly can also help to reduce stress levels, which can be another key factor in the development of bipolar disorder.\nBipolar disorder is a serious mental illness that affects approximately 1 in every 100 children and adolescents. It is important to remember that bipolar disorder is not just an adult problem – it can also affect teenagers at any stage of their development. If you or your child has symptoms of bipolar disorder, you must seek professional help as soon as possible. There are many resources available to you and your loved one, both locally and online, so please don’t hesitate to get started on the healing journey.\nHope this article was of help to you! If you are suffering from mental health disorders, you may seek help from Therapy Mantra. We have a team of highly trained and experienced therapists who can provide you with the tools and skills necessary for overcoming mental health disorders. Contact us today to schedule an online therapy or download our free Android or iOS app for more information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f8445b21-188d-4286-9077-6fe665cfeb6f>","<urn:uuid:14853296-364a-48d7-b22d-a46fec46487f>"],"error":null}
{"question":"How should I install a new bathtub drain basket?","answer":"To install a new drain basket, seal the flange with plumbers putty by rolling it into a 'snake' shape and placing it around the underside of the flange. Then screw the basket down carefully with pliers or the appropriate tool. Tighten it snugly but don't over-tighten.","context":["The drain we're working on is pretty straightforward. Rather than messing with a tub shoe and linkage, the garden tub in your average trailer uses a rubber stopper to hold the water in. The chrome basket is pretty durable for the most part, but they won't hold up well to chemical drain cleaners. They'll corrode and fracture; eventually disintegrating and making a mess under the floor. The best time to replace these is before they completely fall apart and you can get a tool on them to screw them out. Unfortunately, by the time we get around to changing it out the insides are too brittle to be of much use. In this article, I'll give you a few ideas you might be able to use. As always, you have control over your work and you alone must decide what to do to fix this right. Take your time, as you can wreck some pretty expensive fixtures if you mess up. Being down one of anything when it comes to plumbing will not endear you to the rest of your family. I will not settle domestic disputes, so do this and all other repairs at your own risk.\nMy favorite method for getting the basket out is with a pair of pliers stuck into it. The idea is to snag them on anything that will hold the basket to screw it out. I tried this and managed one turn before what was left of the pieces inside broke off. Time for plan \"b\". Normally, I would get a Dremel and cut it out, but mine was dead on arrival. The next best thing is a metal cutting jigsaw blade to score along the inside of the basket, as well as a flat screwdriver and some painters tape. Take the painters tape and mask off around the drain basket. The idea is protect the finish of the tub. It also wouldn't be a bad idea to line the tub with an old blanket or some cardboard, but you knew that.\nTake the screwdriver and CAREFULLY pry up around the basket flange until you can get a pair of pliers underneath. Go ahead and break off as much of that flange as you can , preferably all of it. You should be able to see where the basket threads into the tub shoe or plumbing. Once that's done, take your blade and score straight up and down where the flange is broken off. The idea is to cut through the basket until you reach the shoe, but don't cut through the shoe. I would cut and break this in three spots so you can break out one piece and than other two without damaging the tub or plumbing. You can pry between the basket and the plumbing to work this out without any trouble. If there's any doubt, score the basket a little more and try again. Eventually, you should be rewarded with the basket in several pieces and your tub and plumbing intact. This is a bit tricky to do, but if you take your time can be done relatively easily and inexpensively. It takes me about 30 minutes or so get it out with this method. Installing the new basket is pretty easy and just requires you seal the flange with plumbers putty, screwing it down carefully with the pliers or the appropriate tool. Roll the putty into a \"snake\" and around the underside of the flange. Tighten it snugly, but don't wreath on it. From now on, don't use any chemical drain cleaner and use a plumbers snake to remove the clogs. This will help insure your drain basket lasts for a couple decades or more. Even if the basket you install is plastic, chemical drain cleaners are NEVER a good idea. You can lose eyes, skin, clothing and the social graces these things afford. If you must, try vinegar and baking soda and then a plunger, graduating to the aforementioned snake. Better yet, try cleaning the screen each time you shower; problem solved. Maranatha!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d285a3ba-9a5c-46a2-9b33-79c084344db1>"],"error":null}
{"question":"For animal nutrition specialists: Looking at feeding behaviors and nutrient absorption, how do the bonnethead shark's omnivorous adaptations differ from the elephant's specialized herbivorous traits?","answer":"The bonnethead shark shows unique omnivorous adaptations where it digests seagrass while primarily hunting for animal prey, though this plant consumption appears incidental. In contrast, elephants have evolved highly specialized herbivorous traits, including a muscular trunk for gathering plant matter, specialized grinding teeth that move forward as they wear down, and a complex digestive system with microorganisms specifically adapted for breaking down plant fiber. The elephant's digestive process is more sophisticated for plant matter, featuring anaerobic bacteria and protozoa in both the small intestine and colon to break down cellulose and hemicellulose, while also requiring specific ratios of nutrients like protein, vitamins, and minerals for optimal health.","context":["A shark’s diet is typically a cornucopia of meats: seals, rays, squids, and krill. The idea that “fish are friends, not food” may be charming in Finding Nemo but a bloody bucket of chum is more of a shark’s style. That is, unless the shark in question is a bonnethead shark, a small member of the hammerhead shark genus. According to a new study, this broad, smooth fish is the only shark species known to be an omnivore.\nIt is, however, unclear if the bonnethead shark means to be an omnivore. In the study, released Wednesday in the Proceedings of the Royal Society B, scientists determined that these sharks are able to digest the copious amounts of seagrass they consume. Previous studies established that up to 62 percent of the shark’s gut content mass could be seagrass, but it was unknown if the sharks were actually digesting the plant matter. Now, it’s clear that they are digesting it, but like a weekend warrior downing a garnished Bloody Mary, the ingested greens are likely a byproduct of wanting something else.\n“It has mostly been assumed that they could not digest the seagrass and it was just incidental from scooping up blue crabs,” study co-author and Florida International University assistant professor Yannis Papastamatiou, Ph.D., tells Inverse. “We show that they can quite efficiently digest seagrass and get nutrients and energy from seagrass. I still believe they are getting it incidentally while chasing crabs, but they are getting some energy from the seagrass nonetheless.”\n“I still believe they are getting it incidentally while chasing crabs, but they are getting some energy from the seagrass nonetheless.”\nIn this case, digestion is key: What an animal ingests and what they digest are not always the same thing. Just because you can swallow a small battery doesn’t mean your body is going to break it down into nice, nutritious molecules your body can use. The fact that previously dissected bonnetheads appeared to have degraded seagrass — not fresh seagrass — in their bellies was the first clue that their bodies were breaking the plants down and gaining nutrients.\nIn their study, Papastamatiou and his colleagues fed captive bonnethead sharks a diet that was 90 percent seagrass and 10 percent squid. When they subsequently analyzed the animal’s digestive system and the biochemistry of food particles that passed through them, they determined that the sharks were able to digest both fiber and soluble carbohydrates from the seagrass.\n“The fact that this is the first known species of omnivorous sharks is very exciting!” Samantha Leigh, a Ph.D. candidate at the University of California, Irvine, and co-author on the study tells Inverse. “This means that we need to completely re-evaluate their role in crucial and fragile seagrass meadow ecosystems, as they likely play a different role in ecosystem dynamics than we had believed.”\nPapastamatiou says that people typically focus on what a predator eats, but neglect to think about what happens after that food is consumed. Digestion, he reasons, “is a fascinating but often neglected process.” It’s what happens after an animal eats that allows for nutrients to spread, disperse, and prompt growth. Bonnethead sharks might be after crabs, shrimps, and mollusks, but their seagrass snacks give them a boost of necessary energy as well.","- Elephants are the largest extant herbivores on earth.\n- The average weight of adult Asian elephants (Elephas maximus) tends to be less than that of adult African elephants (Loxodonta africana), in the wild their weight ranges overlap (Asian, 1,800-5,000 kg [4,000-11,000 lb]; African, 1,800-6,000 kg [4,000-13,000 lb])\nELEPHANTS FEEDING IN THE WILD\nNumerous studies on feeding habits of African and Asian elephants indicate that they are generalist feeders, consuming a large number of plant species but with wide variations regionally and seasonally in the proportions of grasses, sedges, forbs (herbaceous flowering plant), shrubs, and trees. Fruits, bulbs, plant bases, and roots also are consumed.\nBoth browsing and grazing are practiced, but elephants tend to take plant types in proportion to their availability. Browsing is commonly defined as consumption of forbs and the tender shoots, twigs, and leaves of trees and shrubs, whereas grazing is the consumption of grasses and sedges. Free-ranging African elephants select relatively young plant parts if food supplies are not excessively restricted by drought, overpopulation, or habitat degradation.\nOne of the most obvious and unique physical features of the elephant is its elongated upper lip and nose, forming a muscular trunk capable of reaching from ground level to high branches in its search for food. Although strong enough to lift an entire tree, the trunk is delicately prehensile and can be used to pick a single grass inflorescence. Food is transferred to the mouth where there is a large grinding tooth in wear on each side of the upper and lower jaw.\nUnlike the horse, the elephant has no canines or lower incisors, and the upper incisors, when present, have been modified to form tusks. The grinding teeth do not succeed each other vertically, as in most mammals, but migrate forward from the back of the jaw. As the foremost tooth wears down, it is pushed out, often breaking off in transverse plates. Transverse ridges on the teeth produce an occlusal grinding surface that is particularly important for reducing siliceous or highly lignified foods to a more digestible particle size. If a tooth erupts at an improper angle, or wears unevenly, grinding surfaces may not meet, and the physical form of the diet may need to be altered to assist in the particle size reduction normally accomplished by chewing.\nChewed food, mixed with saliva, passes down the esophagus to a simple stomach, connected, in turn, to the small intestine, which then joins with the colon. At this latter juncture there is a cecum of considerable proportions. The colon is sacculated but not compartmentalized, and the majority of the digesta is located in the proximal two-thirds. Digestion of protein begins in the stomach and continues in the small intestine, where fat and carbohydrate are also being broken down to absorbable forms. The cecum and colon are inhabited by anaerobic bacteria and protozoa similar to those found in the rumen and reticulum of the ruminant. Anaerobic bacteria and protozoa have also been found in the small intestine, although concentrations of protozoa are lowest in the duodenum and increase in the more distal jejunum and ileum. These microorganisms digest plant fiber (principally cellulose and hemicellulose) that otherwise could not be used, since elephants, like other herbivores, have no fiber-digesting enzymes of their own. Microbial fermentation of fiber, other incompletely digested compounds, and lactic acid formed in the upper tract results in production of volatile fatty acids that can be absorbed and used for energy.\nTHE IMPORTANCE OF FIBER\nIn animals that have evolved as herbivores, the physicochemical characteristics of dietary fiber play an important role in normal gastrointestinal function. The amount and form of digesta reaching the lower gut of the elephant influences the character of the fermentation occurring there, and may affect the rate at which fermentation products are produced and absorbed and the rate at which undigested residue is excreted. When dietary fiber concentrations are low and concentrations of rapidly fermented materials, like starch, are high, fermentation rates will be accelerated, gut motility will change, the bowel can become distended with gas, and abdominal pain may result.\nWHAT TO FEED ELEPHANTS\nHays of various types generally constitute the foundation of feeding programs for elephants. Grass hays are commonly used, although they vary greatly in nutrient content, and many elephants may benefit from a combination of grass and legumes. Hays that are dusty, moldy, or infested with blister beetles, poisonous plants, or other dangerous substances should never be used.\nThe composition of grass hays can vary widely, dependent largely upon soil fertility and stage of maturity when cut. For example, bermudagrass hay that has not been fertilized with nitrogen and is cut when mature, may have only 4% CP, whereas properly fertilized bermudagrass cut at the early heading stage, or before, may have up to 14% CP. Perennial legumes, such as alfalfa (Medicago sativa) and common red clover (Trifolium pratense) have relatively high CP concentrations because of the nitrogen-fixing bacteria that live in their root nodules.\n- The nutrient requirements of the elephant have not been defined, however, the similarities in digestive strategies of the elephant and the horse suggest that the nutrient needs of the former might reasonably be compared to the nutrient requirements of the latter.\nWhen nutrient analysis is done for typical hays, deficiencies of several nutrients are apparent. Protein concentrations are low in many grass hays compared to the needs of growing young, pregnant and lactating females, and breeding males. Some of the poorer grass hays are too low in protein for maintenance of adults, but these conclusions should be based on analysis. Calcium concentrations tend to be high in legumes and low in grass hays, whereas Phosphorous and Sodium tend to be low in both grass and legumes. Dependent upon the region where grown, low concentrations of Iodine, Cobalt, Selenium, Zinc, and Copper may be seen. Vitamin A (from beta-carotene) and vitamin E can be inadequate if the hay has few leaves or is badly weathered, and concentrations of both nutrients decline with storage time.\nAlthough daily digestible energy (DE) requirements are commonly met by allowing elephants to consume hay until their appetites are satisfied, fiber concentrations in mature grass hays may be so high and DE concentrations so low that gut fill will physically restrict intakes below the needs of young, growing elephants or of lactating females. Thus, supplements to hay must be formulated to ensure that intakes of specific nutrients and of digestible energy (DE) will be sufficient. Proposed minimum nutrient concentrations (DM basis) in elephant diets, based largely on extrapolation of nutrient requirements of horses, are presented in the following table:\nProposed minimum nutrient concentrations (DM basis) in elephant diets based largely on extrapolation from nutrient requirements of horses.\nBreeding, early pregnancy\nGrowth of juveniles\n|Crude protein, %||8-10a||12||12-14b||12-14c|\n|Vitamin A, IU/kg||3,000||3,000||3,000||3,000|\n|Vitamin D, IU/kg||800||800||800||800|\n|Vitamin E, IU/kg||100||100||100||100|\na.Adult maintenance, 8% CP; breeding bull, pregnant cow (1st two-thirds of pregnancy), 10% CP.\nb.First yr of lactation, 14% CP; 2nd yr of lactation, 12% CP.\nc.Weanling, 14% CP; 3-yr-old, 13% CP; 4-yr-old to 12-yr-old, 12% CP.\nPelleted feeds can be useful in correcting the inadequacies of hay. To avoid digestive upsets, the introduction of either pellet into the diet should be gradual (increasing slowly over 2 wk), and the amount fed should be appropriate to need but should not exceed 50% of total dietary DM (dry matter basis).\nphoto credit: AZA Elephant TAG and SSP\nThus, it is apparent that appropriately formulated pellets can be used in a variety of ways to correct the deficiencies of forage. However, if those deficiencies are extreme, it may be more cost-effective to buy and use only hays that meet Quality Standard 4 or higher of the Hay Market Task Force of the American Forage and Grassland Council (see NAG Fact Sheet 001, 1997, Hay Quality Evaluation). See my post about new research being done by Disney’s Animal Kingdom on the herbivore diet.\nAssuming that pellets are compounded and used as described, other nutrient supplements should not be needed. However, it has been suggested that biotin supplements improve foot health, and that zinc supplements may be needed to ensure normal immune function. Also suggested is that large doses of a water-soluble derivative of vitamin E protect more effectively, than the usual vitamin E compound added to feeds (D,L-alpha-tocopheryl acetate), against myopathies and other expressions of oxidative damage to cells.\nAnecdotal reports of improvements in foot health associated with biotin supplementation of horses or elephants are almost invariably confounded by improvements in foot husbandry. Considering the evidence of biotin synthesis in the digestive tract of horses, the similarity of the elephant digestive tract, and the difficulty in demonstrating biotin deficiency in any species fed natural diets without use of an antibiotic or sulfa drug to limit microbial synthesis in the intestine, it is surprising that responses to biotin supplements have been noted. It is doubtful that this issue will be resolved until an adequate double-blind study is conducted, in which the person administering oral biotin or a placebo does not know which is which, and likewise, the person scoring foot health does not know which animals received biotin and which received the placebo. It is implausible that the application of biotin-containing products (ointments) to the foot or toenails will result in improvement.\nWith respect to vitamin E requirements, serum concentrations of alpha-tocopherol in captive elephants responded markedly (rising from 0.1 to 0.4 mg/ml) to a large oral dose (4.8 IU/kg body weight) of water-soluble vitamin E (D-alpha-tocopheryl polyethylene glycol succinate [TPGS]) compared to little response from equal or higher doses of D-alpha-tocopherol or D-alpha-tocopheryl acetate. When TPGS was provided at 6.6 IU/kg body weight, serum alpha-tocopherol concentrations rose to about 1 mg/ml.25,26 Death of a 17-mo-old Asian elephant was previously reported when plasma concentrations of alpha-tocopherol in this animal and others in the herd were undetectable (<0.1 mg/ml).\nVitamin E Levels in Elephants\nAppropriate levels of Vitamin E are essential to muscle function in elephants. Low Vitamin E can lead to muscle weakness and an inability of animals to recover from anesthesia, have appropriate birth contractions and other medical related issues. It is generally considered that elephants should have Vitamin E levels in the blood of around .5 -1.0 micrograms/ml.\n|Species||N||Vitamin E (µg/ml)|\n|African (North American)||223||0.47 +/- 0.75|\n|African (European)||57||0.33+/- 0.46|\n|Asian (North American)||945||0.50 +/- 0.36|\n|Asian (European)||20||0.24 +/- 0.16|\n|African||38||0.50 +/- 0.25|\n|Asian||12||0.27 +/- 0.0|\n|Asian||26||0.77 +/- 0.05|\nTable found in: Fowler, M.E., and Mikota, S.K. (2006). Biology, Medicine, and Surgery of Elephants. Ames, IA: Blackwell Publishing.\nA captive Asian elephant fed timothy hay, oat grain, carrots, lettuce, and pellets was reported to exhibit hyperkeratosis and a poor inflammatory response in infected vesicles above the toenails. A presumptive diagnosis of immunodeficiency, secondary to zinc deficiency, was made when an improvement was noted following administration of a zinc carbonate supplement. The concentration of zinc in dietary DM (dry matter) was reported to be 22 mg/kg before supplementation and 54 mg/kg afterward. Unfortunately, it isn’t clear whether these were analyzed or calculated values, and if calculated, whether consideration was given to missing values for zinc in some dietary ingredients. In addition, dietary zinc concentrations given in the text of this report differed from those in the tables.\nIt is noteworthy that 88 young African elephants (<2-9 yr initially), housed in dirt lots with small areas of mud (man-made in dry weather), fed bermudagrass hay, alyceclover (Alysicarpus vaginalis) hay, and ADF16, and subjected to regular veterinary examination for 6 yr, grew normally and showed no signs of biotin, vitamin E, or zinc deficiency.\nFOR FUN: Predicting Elephant Birth Date at Disney’s Animal Kingdom\nIt has been suggested that use of bran may prevent colic in elephants. Horses and elephants are not “meal-eaters”, but have evolved to consume large amounts of food throughout the day. Bran, when offered along with pellets as part of a meal, provides a bolus of high fiber material to the gut. By contrast, consumption of properly selected hay throughout the day better simulates the natural feeding strategy of elephants and provides fiber continuously over a longer period.\nHOW MUCH TO FEED\n- Estimates of daily dry matter intake by adults of about 1.0-1.5% of body weight. Using similar methods with wild adult Asian elephants, daily dry matter intakes have been estimated to be 1.5-1.9% of body weight. Daily dry matter intakes of timothy hay by two captive 6-yr-old female African elephants, measured for one 7-day period in summer and a second 7-day period in winter, were 1.4-1.6% of body weight. In another study, daily dry matter intakes of captive Asian and African elephants fed grass hays were 1.3 and 1.7% of body weight, respectively.\nWHEN AND HOW TO FEED\n- Varied feeding schedules dispersed both spatially and temporally throughout the day and night are required\n- Mechanisms to deliver food to elephants during the day and night should be implemented (e.g., changing animal care staff schedules, automated feeders, hanging feeder nets, etc.). Feeders should be located in multiple locations to discourage undue competition or aggression over feed items\nWATER AND MOATS\n- While outdoors and weather permitting, elephants must have regular access to a water source, such as a pool, waterfall, misters/sprinklers, or wallow that provides enrichment and allows the animals to cool and/or bathe themselves. Standing water in indoor floor areas can cause foot problems and become a breeding ground for bacteria. Floors must therefore be impervious to water, quick to dry, and sloped to a drain. Floor surfaces must be relatively smooth, but not enough so that they become slippery when wet. Conversely, very rough surfaces may cause excessive wear or irritate footpads. When water containers are used, drinking water must be cleaned and refreshed at least twice a day. Containers must also be cleaned daily.\n- Elephants should be given ample access to fresh, potable water daily. Water free-choice is not a requirement; however, offering water at least twice a day and more frequently depending on temperature, humidity, and the amount of exercise the elephant receives is recommended.\nAdult elephants consume an average of 140 to 200 L of water per day.\n- Many zoos continue to offer produce to large herbivorous animals. When used in small amounts, fruits and vegetables are not harmful and may help when shifting or medicating elephants. However, other palatable products, such as leaf-eating primate biscuits can be used in small amounts (a few cubes), as an enticement to shift/move, at appreciably less cost. At one major U.S. zoo that still feeds apples, carrots, and leafy greens, 49% of the cost of the elephant diet was due to produce, whereas those high-cost, high-moisture foods contributed 5% of diet dry matter.\nKEEPING TRACK AND RECORD KEEPING\n- Nutritional content is a critical tool for assessing overall nutritional wellbeing. Daily intake records may also be valuable to maintain. Overall energy content of the diet must be assessed in relation to the body condition scores for each elephant and diet composition adapted as needed.\nHOW TO ENCOURAGE NATURAL FEEDING BEHAVIORS\n- Opportunities must be provided for elephants to acquire food using multiple foraging behaviors. Food must be provided in areas where it is less likely to be soiled. Excess or waste food must be removed daily.\n- Opportunities for searching, browsing, grazing, reaching, opening, etc. can be provided by scatter-feeding, hiding foods in crevices and substrates around the exhibit, or by using elevated feeders such as hanging hay nets that encourage an elephant to reach for and manipulate its trunk to gain access to the food. Mechanisms that promote physically active feeding behaviors can be incorporated into a comprehensive enrichment plan for the elephants.\n- Because exhibit areas are small, and food requirements can generally be met by short bouts of eating, there are long periods of inactivity during which aberrant behavior may develop in a few elephants. This may include consumption of sand or other exhibit substrates, resulting in intestinal impaction. Some zoos have used decomposed granite on ground surfaces which, when consumed, tends to form a stable concretion that is more likely than sand to obstruct the intestinal tract, because it does not break apart readily. It may be most appropriate to thoroughly review and correct husbandry, restrict access to consumable substrates, if feasible, and observe elephants regularly for signs of abnormal behavior. Depression, failure to eat, changes in the odor, appearance (e.g., presence of substrate), and volume of feces, changes in the pattern of defecation (commonly about every 2 hr, but individuals have a characteristic pattern), stretching in a “saw-horse” stance, elevation of the head, and rolling, are colic danger signs. Forced exercise may be helpful in restoring normal gut motility and relieving pain.\n- BROWSE: Providing browse for elephants increases foraging time, can add important nutritional benefits, and can promote dental health. As with other food items offered to elephants, it is important to have browse nutritionally analyzed.\nSummary of Recommendations\n1. Analyze hays for dry matter, crude protein, neutral detergent fiber, acid detergent fiber, lignin, calcium, and phosphorus in a laboratory with demonstrated expertise.\n2. Use grass hay or a mixture of grass and legume hays, of known composition, as the base for the diet.\n3. Using analyzed and published nutrient concentrations, determine the need for supplemental sources of energy, protein, minerals, and vitamins.\n4. When hay mixtures are not adequate to meet digestible energy, protein, mineral, and vitamin needs, consider adding a formulated pellet.\n5. When hay mixtures are principally deficient in minerals and vitamins, consider adding a formulated pellet similar to the Herbivore Supplement.\n6. Note that grass hay to be fed ad libitum should have 30% or more ADF (acid detergent fiber) to avoid problems with colic.\n7. Make all dietary changes gradually (over 1-2 wk) to avoid digestive upsets.\n8. Observe elephants regularly and conscientiously.\nReferences (click to view source document):"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cfc3a41c-9a2c-41dc-b40a-1e7ce7bb8a11>","<urn:uuid:52a2f042-4d7e-49f7-905e-b1f42b139b9d>"],"error":null}
{"question":"How can I plan a diving trip to Marsa Shagra and Wadi Lahami villages within an 8-day holiday?","answer":"For an 8-day holiday, it is recommended to book maximum two locations. Marsa Shagra, the northernmost village, can accommodate 220 guests and offers unlimited diving on the house reef plus shore and off-shore dive sites. Wadi Lahami, the southernmost village, accommodates 80 guests and offers up to 4 dives per day by RIB to 27 different offshore reefs. Both are full-board villages where you only pay extra for alcoholic drinks, shop items, or dives outside your pre-booked diving package. Remember to bring your passport, diving license(s), and a recent medical statement, plus any applicable vouchers for diving package, course, and transfers.","context":["Find answers to our most frequently asked questions here\nMost major European countries have direct flights to Marsa Alam international Airport, which is only 35 kilometers north of Marsa Shagra Village, 75 kilometers from Marsa Nakari Village and 150 kilometers from Wadi Lahami Village. A private bus will take you from the airport to the location of your choice and back. It is also possible to fly to Hurghada, or Luxor to transfer to any one of our villages.\nMarsa Shagra is our northernmost village and can accommodate a maximum of 220 guests. It is our gateway to the south boasting one of the most beautiful house-reefs in the Red Sea region. Marsa Shagra’s most attractive appeal is its mix of modern luxury and natural simplicity – the best of both worlds. It is centrally located just 38km south of the Marsa Alam airport and 20km north of Marsa Alam city. This is the ideal starting point for exploration into the deep-south or vacationers looking for a mix of enjoyment and tranquility.\nMarsa Nakari is our second village which can accommodate a maximum of 120 guests. Nakari is the perfect choice for people who prefer to enjoy their holiday in a personal atmosphere. It also offers a beautiful house reef with entrance via its sandy bay. Marsa Abu Dabbab and boat diving to offshore reefs like Shaab Nakari, Habili Nakari and Dolphin House are only a few of the great dive sites around Nakari Village.\nWadi Lahami is our southernmost village in one of the most pristine regions of the southern Red Sea. It can accommodate a maximum of 80 guests and you can enjoy a truly personal atmosphere due to its remoteness. A natural atmosphere combined with RIB diving to 27 different offshore reefs (Fury Shoals) makes this a divers dream holiday.\nShagra and Nakari offer unlimited diving on the house reefs plus a variety of shore and off-shore dive sites, while Wadi Lahami offers a maximum of 4 dives per day (all by RIB). Please see our diving page for full details of the diving concept in each village. All dive centers offer PADI Courses from DSD to Assistant instructor, plus we have an annual PADI IDC course in Marsa Shagra Village.\nEco-diving Village Shagra, Nakari and Wadi Lahami are full-board villages. You only pay for extras such as alcoholic drinks and shop items, or dives outside your pre-booked diving package.\nRed Sea Diving Safari offers accommodations for every taste and budget.\nIf you have very young children (0-8 years old) we would advise you to go to Marsa Shagra Village, where our sandy shallow bay offers safe entry for children and our playground on the beach. For older children who like to play on the beach, swim or even learn to snorkel or dive, Marsa Shagra and Nakari Villages are suitable. Children aged 8 years or older can take the PADI Bubblemaker course. If you would like to spend a day at the pool, Kahramana Resort is located only 10 minutes walking from Marsa Shagra Village and has a standard pool or Aquapark ('Aquatico'), for which tickets are available at Marsa Shagra reception.\nYou can create whatever combination of villages or diving packages as you like (subject to availability and your diving experience). For an 8-day holiday we recommend you book maximum two locations.\nYou need to bring (a copy of) your passport, your diving license(s) and a recent medical statement, plus if applicable, vouchers for your diving package, diving course and transfers.\n|High Temperature (C)||22||23||24||27||30||32||33||33||32||29||25||23|\n|High Temperature (F)||71.6||73.4||75.2||80.6||86||89.6||91.4||91.4||89.6||84.2||77||73.4|\n|Low Temperature (C)||14||14||16||20||23||25||26||26||25||22||18||16|\n|Low Temperature (F)||57.2||57.2||60.8||68||73.4||77||78.8||78.8||77||71.6||64.4||60.8|\n|Sea Temperature (C)||21||22||23||24||25||27||28||29||28||28||23||22|\n|Sea Temperature (F)||69.8||71.6||73.4||75.2||77||80.6||82.4||84.2||82.4||82.4||73.4||71.6|\n|Cloud Cover (%)||10||7.5||8.75||8.75||7.5||1.25||1.25||1.25||2.5||5||7.5||11.25|\n|Sunshine Hours (hrs)||9||10||10||11||12||13||13||12||11||10||10||9|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4631eca8-04ab-4022-b87c-ce90c71e87f1>"],"error":null}
{"question":"With my background in cognitive psychology, I've noticed parallels between market disruption theories and attention processing - how does Christensen's disruption theory in publishing relate to Miller's findings about cognitive limitations?","answer":"Christensen's disruption theory and Miller's cognitive findings both demonstrate fundamental limitations in their respective domains. In publishing, Christensen shows how established firms lose power to disruptors who enter at the low end and gradually move upmarket, as seen with traditional publishers facing competition from e-books and Amazon. Similarly, Miller's research proves the brain has inherent cognitive limitations - it can only process a small amount of information at once, typically seven items, and cannot truly handle multiple tasks simultaneously. Both theories reveal natural constraints: businesses cannot indefinitely maintain market dominance against disruptive innovations, just as the human brain cannot expand beyond its natural capacity to focus on multiple stimuli at once.","context":["Publishing is undergoing disruption.\nHarvard Business School Professor Clayton Christensen introduced the world to his analysis of technological changes in his seminal “The Innovator’s Dilemma,” where be coined the word (and idea) of “disruption.”\nLong standing firms (incumbents) suddenly lose the power and dominance to disruptors who enter the market either at the low end, or in places that the incumbent does not serve; then the disruptor gets a toe-hold, and starts to move up the food chain where the profits are better.\nIt’s happening in publishing, just as it happened in automobiles when the Japanese entered the car market at the low end and pretty much overturned Detroit’s dominance.\nPeter Drucker tells us about how products work in markets. First, there is the product itself; second, the channel of distribution; and third, the market itself (i.e., how the product is consumed). Managers can order a product change more easily than a distribution change or a market change.\nConsider motion pictures, going back to the days of silent movies. It was a very cumbersome process—requiring consumers to go to a theater at set times to sit before a screen as a group and watch what the film maker had to show.\nToday, while theaters are still a factor, movies are streamed directly to homes and watched commercial free. Odd, the place we see commercials now is in theaters. Moreover, we watch the films whenever we want.\nAt one time television almost drove movie theaters into oblivion. Television monetized itself based on the concept of the commercial—radio had been doing that for some time. Yet, with the advent of the DVR, such as TiVo, commercials could be skipped by a process of fast-forwarding.\nAnd of course, Netflix takes it to a whole other level, not to mention series such as Game of Thrones, True Blood, and True Detectives.\nBack to Drucker. The product has changed a bit, yet if someone from 1920 were to see a current movie, they’d pretty much get that that’s what it was…a movie.\nBut the channels of distribution, and how we consume the movie, have changed quite a bit and the old business models have been disrupted.\nPeople got rich off those business models and anyone who’s seen Hollywood mansions knows that for a time the model worked well for the owners.\nPublishing is going through much of the same—disruption.\nLike the theaters of old, until recently books had to be “consumed” in special place—bookstores and libraries. With Amazon and Kindle and other electronic means, publishing is shaking out and the publishers largely do not like what is happening.\nDetroit did not make long-term headway by pointing out how “crummy” the first Japanese cars were, though students of the early 1970s can go back and see there were all sorts of articles out there to prove just that.\nHowever, with time, the Japanese automobile manufacturers came up-market all the way to the Lexus.\nPublishers are trying to use the same argument about the e-press. “There are a lot of crummy books out there,” they say, and they’d be right, but anyone who’s spent any time at an antiquarian books store will probably say that it has always been so.\nIt’s not whether a book is good or bad, but how does it navigate the channels of distribution and how is it consumed? The existing channels are being disrupted, just like movie theaters went to their nadir as television made distributed home-viewing more popular than centralized theater-viewing. Everyone’s said, “I’ll wait for it to come out on DVD,” or is that Blu-ray or Netflix now?\nAt bottom, the artist (the writer) has not changed, though we’ve given up our candle-lit rooms with inkwells and quills for the halo light of laptops with word processors integrated with grammar and spellcheckers, dictionaries, and thesauruses.\nWas it Marx who said that the Workers would do well to seize the means of production? Those of us who were early adopters of word processors, decades ago, whether we knew it or not, were doing exactly that—seizing the means of production.\nBut the other part of equation—distribution and consumption—are now being disrupted as other industries were in other eras.\nWe are now seizing the means of distribution and consumption, and like Detroit, there may be no way to stop this.\nIndeed, Elvis has left the building.","Multitasking: The Modern Myth\nYou are busy. Regardless of your profession, on a daily basis you are inundated with a seemingly endless stream of phone calls, emails, text messages and meetings. Because of the rapid pace and high demands of the modern workplace, everyone is in search of ways to increase their productivity. One popular technique that many utilize in an attempt to execute numerous tasks quickly is multitasking. When a person attempts to multitask, he or she is trying to accomplish two or more tasks at the same time. Many professionals claim that their ability to multitask is one of their most essential skills. These workers contend that multitasking allows them to remain productive even when overwhelmed with activity. Yet, there is one problem with multitasking… it is impossible.\nResearch scientists have made some startling discoveries regarding the capabilities of the human brain. Science has confirmed that though the brain is an extraordinary organ, it has limited cognitive abilities. George Miller, the great cognitive psychologist, wrote about the brain’s limited capacity to be attentive to and process information in his famous article, “The magical number seven, plus or minus two: some limits of our capacity for processing information” which was published in the Psychological Review. Miller demonstrated how the brain can only grasp a small amount of information at one time. This is why phone numbers, excluding area codes, are only seven digits. Scientists maintain that if phone numbers were more than seven numerals they would be forgotten with far greater frequency.\nThe limitations of the brain to concentrate upon multiple stimuli are why multitasking is an impossibility. The brain can only contemplate one idea at a time. As Dr. Pierce Howard, Director of Research for the Center of Applied Cognitive Studies affirmed, “Notwithstanding teenagers’ claims that they can do homework in front of the television set, the brain cannot focus on more than one stimulus at a time.” Neuroscientist John Medina echoes this assertion when he declared, “research shows that we can’t multitask. We are biologically incapable of processing attention-rich inputs simultaneously.” Nobel Prize winning economist Herbert Simon emphasized that human beings consciously “operate largely in serial fashion. The more demanding the task, the more we are single-minded.” Psychologist Edward Hallowell aptly summarizes the mental impossibility of multitasking by comparing it to playing tennis with numerous tennis balls.\nWhat You Are Doing When You Think You Are Multitasking\nWhen many people learn about the human brain’s inability to multitask they become perplexed. This confusion is derived from the experience doing what they deem multitasking. So the question is, if the brain cannot multitask, what is it doing when people believe they are multitasking? Scientists have identified that when the brain attempts to multitask it is actually diverting its focus from one task and giving it to another. As well-known research scientist Mihaly Csikszentmihalyi affirms, “Humans cannot really successfully multitask, but can rather move attention rapidly from one task to another in quick succession, which only makes us feel as if we were actually doing things simultaneously.” Though your brain can maintain a basic awareness of its surroundings, it can only thoughtfully deliberate one idea at a time. So as you read this article and ponder the concepts it is espousing, your brain is unable to simultaneously contemplate what you had for lunch yesterday. Your brain can only think about this article or yesterday’s lunch, but not both.\nThe reality is that those who believe that they are skilled at multitasking simply have good memories that allow them to remember the thoughts they had before they jumped to the other activity. Nevertheless, regardless of the strength of one’s memory, juggling multiple tasks concurrently will hinder productivity. There is a vast amount of scientific research which has verified that bouncing back and forth between tasks lengthens the time needed to complete the tasks and reduces one’s effectiveness in the execution of the tasks.     For example, a research study which examined the effects of talking on a cell phone while driving found that by focusing on a phone call, the driver’s impairment was nearly equal to being drunk. In addition, the Harvard Business Review published the results of a study that analyzed the behaviors of daily workers. The research found that the more the workers moved back and forth from task to task, the less they accomplished. The conclusion of the research was that workers should “stick to one thing at a time.” \nThough the notion of multitasking may seem alluring, science has proven it to be a modern myth. The human brain can only concentrate on one idea at a time. Consequently, by focusing on one task at a time you will improve your productivity. As legendary management expert, Peter Drucker stated, “If there is any one ‘secret’ of effectiveness, it is concentration. Effective executives do first things first, and they do one thing at a time.”\nClick here to download this article in pdf.\n G. A. Miller. “The magical number seven, plus or minus two: some limits of our capacity for processing information.” Psychological Review, 63, 1956. p. 81 – 97.\n Pierce J. Howard. The Owner’s Manual for The Brain. (Austin: Bard Press, 2006). p. 497.\n John Median. Brain Rules. (Seattle: Pear Press, 2008). p. 85.\n Herbert Simon. Administrative Behavior, 4th edition. (New York: Simon & Schuster, 1997). p. 90.\n Edward Hallowell. Crazy Busy: Overstretched, Overbooked, and About to Snap! (New York: Ballantine Books, 2007). p. 19.\n R. Rogers and S. Monsell. “The costs of a predictable switch between simple cognitive tasks.” Journal of Experimental Psychology: General, 124, 1995. p 207-231.\n R. Meuter and A. Allport. “Bilingual language switching in naming: Asymmetrical costs of language selection.” Journal of Memory and Language, 40, 1999. p. 25-40.\n J.S. Rubinstein, D.E. Meyer and J.E. Evans. “Executive Control of Cognitive Processes in Task Switching.” Journal of Experimental Psychology: Human Perception and Performance, 27, 2001. p. 763-797.\n N. Yeung and S. Monsell. “Switching between tasks of unequal familiarity: The role of stimulus-attribute and response-set selection.” Journal of Experimental Psychology-Human Perception and Performance, 29, 2003. p. 455-469.\n Harold Pashler. “Dual-Task Interference in Simple Tasks: Data and Theory.” Psychological Bulletin, vol. 16, no. 2, 1994. p. 241.\n David Strayer, Frank Drews and Dennis Crouch. “A Comparison of the Cell Phone Driver and the Drunk Driver.” Human Factors, vol. 48, no. 2, summer 2006. p. 381 – 391.\n “The Multitasking Paradox.” Harvard Business Review, March, 2013. p. 30.\n Peter Drucker. The Effective Executive. (New York: Harper Collins, 2006). p. 100."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:76d268d6-be78-478c-849e-65e3d7b55c1b>","<urn:uuid:660da43e-81da-42e4-b51f-c968c422b6b5>"],"error":null}
{"question":"Can you explain how the building maintains optimal thermal comfort during summer through its various architectural features?","answer":"The building maintains optimal summer thermal comfort through three main features: strong thermal inertia, suitable solar protection (which includes overhanging roof and sun screens), and nightly over-ventilation that doubles the normal ventilation flow. Additionally, the South-facing roof gradually becomes slimmer and ends in a large brim-like cantilever to protect from sunshine. The Eastern and Western façades are protected by perforated white vertical Trespa panels that can be automated to control the building's permeability to sun rays, even allowing complete closure.","context":["Architects: ARTUR Architectes Mandataire\nLocation: Dordogne, France\nProject area: 6,771 sqm\nPhotographs: Courtesy of ARTUR Architectes Mandataire\nThis rehabilitation is a clear opportunity for a new deal and provides the chance to offer all users a new working and living environment. The architectural challenge is to create a core, re-orient the polarities of the school, harmonise a high school made of heterogeneous buildings. The entire restructuring of the teaching building (externat) and the addition of a school library on the restaurant’s terraced roof therefore allow students to find a coherence in its use. The coherence of the architectural design results from the use of white Trespa panels. The white facing of the teaching building becomes the scenic background for the white-striated homogenous block of pure volumes which hosts the school library and the restaurant on the schoolyard level.\nThe school library (Centre de Documentation et d’Information) reigns on a pedestal in the middle of the schoolyard. Built on the model of a peristyle room, it hosts the reading room in its centre. This superimposition, built from dry construction materials with a structure in galvanized steel, unifies the school library and the restaurant and gives to the whole the look of a white-striated homogenous block of pure volumes. The roofing rests upon a peripheral line of very thin galvanized steel posts, which encloses the central volume.\nFinally, the floor has undergone a complete restructuring, through the creation of a concrete base and the integration of both stairs and access ramps; it sets the whole composition, prioritizes circulation, and leads naturally to the doors of the various buildings. To protect from the sunshine, the South-facing roof becomes gradually slimmer and ends up in a sizeable brim-like cantilever.\nThe Eastern and Western façades are protected from low-angled sun rays by perforated white vertical Trespa panels, running at right angles from the glass surface, mounted on pivots and linked together to an automated system. This system allows to control the building’s permeability to the sun’s rays for each façade, all the way to complete closure. In this ultimate position, the panels form a second skin 60cm from the façade. These vertical panels are also found at the lower level on the school restaurant’s façades.\nThe teaching building went through a rehabilitation process. A covering of white Trespa panels on the façade, which had been isolated beforehand, as well as a double cap of perforated white Trespa panels (Trespa panels that match the coverings and the sun protections of the library) were put in place in addition to the new layout of the inside volumes.\nEfficiency requirements in terms of materials and construction standards, energy management, isolation, summer thermal comfort, low pollutant-emission and building site management have been reached.\nHeating consumption is 45.8 kWh/m²/year (average for the buildings). Before the rehabilitation, this consumption was 96.8 kWh/m²/year. This new consumption level is the combined result of the isolation efficiency of the outer layer, and of the energy-saving system. The energy-saving system includes the existing gas-powered boiler room, low-temperature heaters, double-flow ventilation for the library, and highly energy-efficient lamps.\nThe strong thermal inertia combined with suitable solar protection (overhanging roof and sun screens) and a nightly over-ventilation (doubled the normal ventilation flow) ensure an optimal thermal comfort during the summer.\nThe building site also followed the recommendations of the « low-pollution building site » charter."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fd6819b7-3a29-4da2-9258-996bee96eb7d>"],"error":null}
{"question":"How does the crossover frequency setting in a surround sound system affect speaker size and performance?","answer":"The crossover frequency affects both speaker size and performance in several ways. For spatial speakers, keeping the crossover frequency low requires the speakers to be of a certain size to provide acceptable audio quality and sound pressure towards lower frequencies. However, having too high a crossover frequency between the subwoofer and spatial speakers can degrade the spatial experience, as the subwoofer's presence tends to obscure or confuse the spatial cues perceived by the listener. The proper crossover setting depends on each speaker's capacity to produce bass, which manufacturers typically specify in their documentation.","context":["FIELD OF THE INVENTION\n- Top of Page\nThe invention relates to a speaker system and method of operation therefor and in particular, but not exclusively, to a speaker system for a rear channel of a surround sound system.\n- Top of Page\nOF THE INVENTION\nIn recent years, spatial sound provision has become increasingly popular such as e.g. evidenced by the wide popularity of various surround sounds systems. For example, the increased popularity of home cinema systems has resulted in a surround sound systems being common in many private homes. However, a problem with conventional surround sound systems is that they require a high number of separate speakers located at specific positions.\nFor example, a conventional Dolby 5.1 surround sound system requires right and left rear speakers, as well front centre, right and left speakers. In addition, a low frequency subwoofer may be used.\nThe high number of speakers not only increases cost but also results in reduced practicality and increased inconvenience to users. In particular, it is generally considered a disadvantage that loudspeakers at specific positions in front as well as to the rear of listeners are needed. The rear loudspeakers are particularly problematic due to the required wiring and the physical impact they impose on the interior of the room.\nIn order to mitigate this problem, research has been undertaken in order to generate speaker sets that are suitable for reproducing or emulating surround sound systems but using a reduced number of speaker positions. Such speaker sets use directional sound radiation to direct sounds in directions that will result in them reaching the user via reflections from objects in the sound environment. For example, audio signals can be directed so that they will reach the listener via reflections of sidewalls thereby providing an impression to the user that the sound originates to the side (or even behind) the listener.\nHowever, such approaches of providing virtual sound sources tend to be less robust than real sources positioned to the rear of the listener and tend to provide reduced audio quality and a reduced spatial experience. Indeed, it is often difficult to accurately direct audio signals to provide the desired reflections that achieve the desired virtual sound source position. Furthermore, the audio signals intended to be received from the back of the user also tend to reach the user via direct paths or alternative unintended paths thereby degrading the spatial experience.\nIndeed, it has been identified that one of the highest preferences of consumers of e.g. home cinema- and surround systems is that of obtaining a convincing surround experience with as few and small loudspeaker units as possible. Preferably, consumers would like to be able to have a great immersive experience using only a single compact system. In order to address such preferences loudspeaker arrangements have been developed where a plurality of spatial channels can be generated from a single loudspeaker box. This is typically achieved by the loudspeaker box comprising a plurality of speaker drivers that are individually driven with different weights for each speaker driver. This allows directional audio beams to be formed and may e.g. be used to direct surround sound channels towards the side so that they will reach the listening position from the side or back due to reflections of walls.\nHowever, although such approaches are often able to create a pleasant wide, spacious sound experience, they do tend to be suboptimal in providing a spatial surround sound experience. For example, they tend to be dependent on the specific audio environment and e.g. the presence of suitable walls to reflect sound of. As a consequence, such systems may in some scenarios tend to not provide an accurate and highly realistic impression of sound reaching the listener from behind.\nTherefore, it is generally the case that in order to obtain an optimal spatial user experience, the use of loudspeakers located to the side or rear of the user is typically desired. However, whereas improved performance may often be achieved by positioning of surround speakers e.g. to the side or behind the listening position, such speakers tend to be considered undesirable. Therefore, it is desired that speakers of e.g. a surround sound system are as small as possible and this has for example led to the typical arrangement of relatively small spatial (satellite) speakers combined with a single subwoofer. However, such an approach tends to not provide optimal sound quality. In addition, the spatial experience tends to be degraded as the presence of the subwoofer tends to obscure or confuse the spatial cues perceived by the listener. Furthermore, in order to provide a reasonable sound quality and spatial experience, the cross-over frequency between the subwoofer and the spatial speakers must be kept relatively low. This results in the spatial speakers needing to be of a certain size in order to provide acceptable audio quality and sound pressure towards the lower frequencies.\nHence, an improved speaker system would be advantageous and in particular a system that will allow facilitated implementation, facilitated setup, a reduced number and/or size of speakers, an improved spatial experience, improved audio quality and/or improved performance would be advantageous.\n- Top of Page\nOF THE INVENTION\nAccordingly, the Invention seeks to preferably mitigate, alleviate or eliminate one or more of the above mentioned disadvantages singly or in any combination.\nAccording to an aspect of the invention there is provided a speaker system comprising: a first speaker arranged to reproduce sound in response to a first drive signal, the first speaker being arranged to reproduce sound to arrive at a listening position; a second speaker arranged to reproduce sound in response to a second drive signal; a driving circuit comprising: a receiver for receiving an audio signal for reproduction, a first drive circuit for generating the first drive signal in response to a first filtering of the audio signal, the first filtering having a first pass band, a second drive circuit for generating the second drive signal in response to a second filtering of the audio signal, the second filtering having a second pass band, the second passband comprising a frequency band below the first frequency band; a delay for delaying the second drive signal relative to the first drive signal; and wherein the speaker system is arranged to directionally radiate sound from the second speaker with an directional radiation pattern having a notch towards the listening position.\nThe inventors' have realized specific characteristics of human perception of direction for audio signals that may be used to provide a speaker system allowing improved audio performance using smaller and/or fewer speakers. In particular, an accurate spatial sound source localization may be achieved using a very small speaker while at the same time providing a sound quality which is not limited to the characteristics of the very small speaker.\nSpecifically, in many embodiments the directional cues provided to a user may be dominated by the spatial position of the first speaker while allowing a large part of the audio quality to be provided by the second speaker. The system seeks to concentrate significant human spatial cues at the first speaker while providing significant audio quality cues from the second speaker.\nSpecifically, the system may use psycho acoustic phenomenon known as the so-called “precedence effect” (or Haas effect) in combination with an increased diffused audio perception of sound from the second speaker to concentrate spatial cues to the first speaker.\nThe precedence effect represents the phenomenon that when the same sound signal is received from two sources at different positions and with a sufficiently small delay, the sound is perceived to come only from the direction of the sound source that is ahead, i.e. from the first arriving signal. Thus, the psychoacoustic phenomenon refers to the fact that the human brain derives most spatial cues from the first received signal components. The inventor\\'s have realized that the precedence effect may also be used for scenarios where different speakers do not radiate the same signal but radiates different frequency bands of the same signal.\nThe use of directional lower frequency sound provision increases the strength of the precedence effect and allows the relative weight of the second speaker to be increased substantially while still maintaining a desired spatial perception. For example, it may allow the second speaker to cover a larger frequency range and/or to be used at higher relative levels thereby providing an improved sound quality. The reduced frequency range that needs to be covered by the first speaker may allow a substantial reduction in size and power. The first speaker may for example be a very small tweeter.\nThe first and/or second speaker may comprise a plurality of speaker elements or drivers.\nThe system may for example allow very small rear loudspeakers in a surround sound setup while still providing high audio quality and an accurate spatial experience.\nIn accordance with an optional feature of the invention, an angle between a direction from the listening position to the first speaker and a direction from the listening position to the second speaker is no less than 60 degrees.\nThe invention may reproduce audio using two different loudspeakers while only requiring one loudspeaker to be placed to provide desired spatial cues. Thus, the invention may in many embodiments allow a high degree of flexibility in positioning of speakers and may in particular allow the two speakers to be positioned at substantially different directions from the listening position while still allowing a single sound source to be perceived.\nIn some embodiments, the angle may advantageously be no less than 90 degrees.\nIn accordance with an optional feature of the invention, the audio signal is a signal of a surround channel of a surround sound multi-channel audio signal and the first speaker is arranged such that the sound from the first speaker arrives at the listening position from a non-frontal direction.\nThe invention may provide an advantageous speaker system for a surround channel of a surround sound system and may in particular allow accurate spatial surround reproduction while only requiring that very small speakers are positioned to provide the required spatial cues.\nA non-frontal direction may specifically be a direction which is no less than 60 degrees offset relative to a direction from the listening position to a center front position of the surround sound system setup.\nIn accordance with an optional feature of the invention, the first speaker is part of a surround sound system and is positioned outside a front direction angle interval for the surround sound system, the front direction interval comprising angles less than 60 degrees offset relative to a direction from the listening position to a surround sound center channel audio source.\nThe invention may provide an advantageous speaker system for a surround channel of a surround sound system and may in particular allow accurate spatial surround reproduction and high audio quality while requiring only very small speakers to be positioned to provide the required spatial cues.\nIn accordance with an optional feature of the invention, an intensity of audio from the second speaker in the direction of the listening position is no less than 10 dB below a maximum intensity of the audio from the second speaker.\nThis may provide an advantageous effect and may in particular provide a suitable attenuation of the direct path for the second speaker to suitably enhance the precedence effect. In some embodiments, the intensity may advantageously be no less than 20 dB below the maximum intensity.\nIn accordance with an optional feature of the invention, the first pass band has a lower 3 dB cut-off frequency that belongs to a frequency range of 400 Hz to 1 kHz.\nThis may in many embodiments provide an improved performance. In particular, an advantageous trade-off between audio quality and spatial perception may be achieved. In some embodiments, the lower 3 dB cut-off frequency may advantageously be no less than 600 Hz, 700 Hz or 800 Hz.\nIn accordance with an optional feature of the invention, the first pass band has a lower 3 dB cut-off frequency of no more than 1000 Hz. This may allow an improved precedence effect and reduce the risk of the first speaker not providing enough signal to provide the desired spatial cues.\nIn accordance with an optional feature of the invention, the second pass band has a higher 3 dB cut-off frequency of no less than 500 Hz.\nThis may in many embodiments provide an improved performance. In particular, an advantageous trade-off between audio quality and spatial perception may be achieved. In some embodiments, the higher 3 dB cut-off frequency may advantageously be no less than 600 Hz, 700 Hz or 800 Hz.\nIn accordance with an optional feature of the invention, the second pass band has a higher 3 dB cut-off frequency of no more than 1000 Hz. This may allow an improved precedence effect and reduce the risk of the first speaker not providing enough signal to provide the desired spatial cues.","It goes without saying that along with world-class picture quality, incredible sound quality is equally important to an immersive home theatre experience. While most home theatre speakers are equipped with auto-setup features, small adjustments in your system can help you enjoy the highest quality sound.\nFor that calibrating your system accurately is most significant, as failing this will lead to speakers delivering a sub-par performance.\nFeel daunted by the task of calibrating your home theatre speakers?\nBe sure you read our easy-to-follow guide which covers every detail about the calibration of your surround sound system for the most satisfying sound experience. (Especially, if you are a home cinema enthusiast!)\nHere’s everything you need to know on how to calibrate your home theatre system\nEnsure the visibility of your receiver’s on-screen display on your TV\nMost modern-day receivers will put out the on-screen display via HDMI, but some earlier models of the receivers that date back to five years need a conventional video connection.\nDon’t see a menu pop up on your TV when you press the menu/setup button on the receiver’s remote? It would be helpful to keep an RCA cable to connect the receiver’s composite video (yellow RCA jack) output to your TV’s composite video input.\nGet a hands-on experience with your receiver’s menu for significant benefits\nGet to know your receiver’s menu as there are a lot of menus. Having navigated them before will help you remember the several menu options. Do not get overwhelmed if you are not familiar with the names, initially, as we will have them covered too, in this plain-language guide especially curated for you.\nFollow the speaker placement rules\nUsually, the placement of the speakers varies as per the types of speakers. If you have a specific entertainment area in mind where you plan to place your speakers, try to follow these speaker placement rules.\nDistance between each speaker and your central listening position\nMeasure the distance from the front of each speaker to where your head would reside when in the central listening spot, then note down the measurement. Once all the measurements are made, add them to the receiver. Speaker by speaker the receiver will prompt you to input a distance measurement in increments of feet, half-feet or sometimes down to 1/12 of afoot.\n- The centre channel should be placed directly below or above your television. As the dialogue comes from this speaker, it should be in close proximity to the TV so there is an illusion of the voices coming from the picture itself.\n- Place your left and right speakers at a distance from each other as your central listening position is from your TV, building an equilateral triangle between your head and the two speakers. Usually, the listening position is the seat in front of the TV. Else you can select the spot where most of the listening will be done. Try to place the top of the speaker at about 40 inches off the floor.\n- Place your surround speakers above ear level, between 50 to 70 inches and just behind your listening position. If you have the surround back speakers, mirror them in the same positions as the front speakers on the rear wall in the room.\n- If you’re adding Dolby Atmos/DTS:X speakers position upward-firing speakers on top of each of the front left and right speakers. It is also recommended that you assure the speakers are at/ above ear level.\n- Adding a subwoofer? Place them a third of the way into the room from the side and front, or the back wall. Be mindful of placing them at least 3 feet away from your head.\nThe speaker crossover setting\nThe term “speaker crossover” means the point at which your receiver stops sending bass to each of your speakers and begins sending it to your subwoofer. The accurate crossover setting would be contingent upon your speakers’ capacity to produce bass. Most speaker manufacturers give clear specs that inform you where your speakers stop producing bass.\nBalance the volume level of each speaker concerning your seating position\nIf you want to hear each speaker at the proper level, use a decibel meter to get the most accurate outcome.\nYou can find them online or download apps for both Android and iOS devices. Be seated in your central listening position, turn on the meter and set the dial to 70 dB. Set the weighting to “C” and the response to “slow.” Hold the meter facing you directly with the microphone end pointed at the ceiling. You can select any level of your choice. The popular levels are 70, 75 and 80 dB.\nReady for the balanced and natural-sounding performance of your speaker system and receiver? Hopefully, our guide has some useful tips for you. Alongside, we would be happy to help you achieve the best outcome from your home theatre system."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2c70bf80-7324-4177-b1fe-6735071ca03d>","<urn:uuid:925a080f-da5b-4455-a526-4a960fd0cd50>"],"error":null}
{"question":"How do astronomers detect and study both Neptune and massive black holes, given that they're difficult to observe directly?","answer":"Both Neptune and black holes require indirect methods of observation. Black holes are detected by measuring the speed of stars orbiting very close to them - faster moving stars indicate stronger gravity and thus more massive black holes. This requires state-of-the-art telescopes like Gemini, Keck, and McDonald Observatory. Neptune, while also requiring telescopes for observation, was first discovered through mathematical predictions when astronomers noticed irregularities in Uranus's orbit. Johann Galle finally spotted Neptune using these calculations, though it cannot be seen with the naked eye. Today, Neptune can be observed with small telescopes or binoculars in the constellation Aquarius.","context":["GWEN IFILL: Finally tonight, they’re big, they’re scary, and, lucky for us, they’re about 300 million light years away.\nWe’re talking about the largest black holes in the universe, and nothing, not even light, can escape their gravitational pull. Astronomers recently discovered two black holes, each one 10 billion times the size of our sun. Their findings are being reported this week in the journal “Nature.”\nJoining me now is Chung-Pei Ma. She is a professor of astronomy at the University of California, Berkeley, and led the team that published the study.\nWelcome to the program.\nDescribe for us how massive, really, these holes are that we’re talking about.\nCHUNG-PEI MA, Professor of Astronomy, University of California, Berkeley: Yes.\nEach one has the mass of about 10 billion times that of the sun. And we can also think about their sizes, that is to say, the region of space within which light cannot even escape. And that size for these black holes are about five times that of our solar system, so within which these objects are dark.\nGWEN IFILL: What are black holes really, and how does one go about finding them?\nCHUNG-PEI MA: Right.\nSo, black holes are these regions of space where the gravity is just enormous. And so, by that definition, they are dark. So, we have to use things we can obviously see. So, in this case, we used stars that are orbiting very close to the black holes. And we measure their speeds.\nAnd the faster the stars are moving, that indicates they’re feeling stronger gravity, and, therefore, the more massive the black hole would be.\nGWEN IFILL: Now, you didn’t obviously see these with the naked eye. How did you find them? How did you even know where to look?\nCHUNG-PEI MA: Yes, they are hard to find.\nThey are hiding at the centers of very massive galaxies. So, in order to measure these stars, we relied on state-of-the-art telescopes, such as the Gemini and the Keck telescopes in Hawaii and the McDonald Observatory in Texas. And we used the spectrographs on them, very sensitive ones, to measure the orbits of these stars.\nGWEN IFILL: We — you have found black holes — in fact, your team has found them before. Why are these so big, so huge? What is it about the ones you found that’s different from what you have seen before?\nCHUNG-PEI MA: Yes.\nThe previous record-holder had a mass of about half of the ones we saw. And they were done by our — two of our team members, Karl Gebhardt in Texas and Tod Lauer at NOAO. And these two we found, however, dwarfed them by a factor of two.\nGWEN IFILL: Now, you describe them as — absence of light is what a black hole is by definition. So, how do you know that what you’re looking at when you find it is actually a black hole, and not just space?\nCHUNG-PEI MA: Yes.\nSo, what we’re able to do is, we are able to estimate the amount of mass that is enclosed within the orbit of these stars. And since there’s an enormous amount of mass concentrated at a very small region in space, we believe the best candidate for the thing that is pulling on the stars would be a black hole.\nGWEN IFILL: So, we know that they’re — we have an idea — I don’t know if we can actually — we can see, though, how big they are, but how far away are they from us? Is this something that seems closer than what you have seen before, or is it farther away?\nCHUNG-PEI MA: Yes.\nSo, the previous record-holder was in a galaxy that’s much, much closer to ours, about a factor of 20 closer. And these ones we saw are at — in a galaxy that’s about 300 million light years away. And that sounds like a very large distance, but for galaxies — we know about many, many galaxies — these are actually quite nearby.\nGWEN IFILL: Nearby, but not a threat? I mean, we’re not — you’re talking about black holes that suck in light and gases and everything in its path, but we’re not in its path?\nCHUNG-PEI MA: That’s right.\nSo, what made this observation very difficult is, like you said, we are observing the stars near the black holes. But we needed to look at stars very close to the black holes. So, in this case, we’re looking at stars within about 1,000 light years of the black holes. That sounds like a large distance, but that region is at 300 million light years away, so it required very sharp eyes.\nThat is, you need to have very good glasses to — in order to see these stars.\nIn our case, we relied on the state-of-the-art telescopes.\nGWEN IFILL: So, tell us about what — why these black holes matter to us? Are they — does that say something about the galaxies that we know of, that we live in? Does it say something about the universe?\nCHUNG-PEI MA: Yes, absolutely.\nAstronomers have known for some time that bigger galaxies seem to have bigger black holes. That is to say, the black holes seem to know about the big environment that they live in. They live at the centers of these galaxies.\nAnd this correlation is very important, because it indicates that, when we learn about these black holes, when we do these measurements, that can also help us understand how their parent black holes were assembled. And galaxies are, after all, the building blocks of the universe. And this will allow us to understand how the universe evolved.\nGWEN IFILL: Well, thank you for allowing us to understand the building block, Professor Chung-Pei Ma of Berkeley.\nCHUNG-PEI MA: Thank you.","The gas giant planet Neptune takes center stage in a series of sharp new photos snapped by the Hubble Space Telescope in honor of the blue-green world's first Neptunian year around the sun since it was discovered in 1846.\nYesterday, Neptune completed its first trip around the sun since being discovered nearly 165 Earth years ago — on Sept. 23, 1846, to be exact, by German astronomer Johann Galle.\nNeptune takes about 165 years to complete one orbit around the sun. It is about 30 times farther from the sun than Earth and typically orbits at a distance of about 2.8 billion miles (4.5 billion kilometers). [Photos of Neptune: Latest Hubble and Voyager Views]\nBig, blue Neptune\nThe four new Hubble photos show Neptune in stunning detail.\nThe images were taken about four hours apart and show the planet as it appeared between June 25 and 26 over the course of a single Neptunian day, which lasts about 16 hours, yielding a complete view of the distant world.\nIn a photo description, scientists said the new Hubble photos revealed more high-altitude clouds on Neptune than those seen in recent observations within the last few Earth years.\nThe clouds are composed of methane ice crystals and hover over parts of Neptune's northern and southern hemisphere, Hubble scientists said.\nLike Earth, Neptune spins on a tilted axis, which gives the planet its own set of seasons. Earth's axis is tilted about 23 degrees, but Neptune has a more pronounced 29-degree tilt.\nWhile Earth's seasons tend to last a few months each, a single season on Neptune runs for about 40 Earth years, scientists explained. Currently it is early summer in Neptune's southern hemisphere and winter in the north, they added.\nThe large temperature differences between Neptune's warm interior and its super-chilly cloud tops, which can reach minus 260 degrees Fahrenheit (minus 162 Celsius), may be responsible for large-scale weather changes, Hubble scientists said.\nDiscovering the eighth planet\nThe story of Neptune's discovery in the 19th century is unique among the planets of the solar system. It was the first planet to be discovered using mathematics, with astronomers predicting Neptune's position after observing that the orbit of Uranus — the seventh planet from the sun — did not exactly match up with Newton's theory of gravity.\nAccording to NASA, it was the French astronomer Alexis Bouvard in 1821 who first speculated that another planet was tugging on Uranus and tweaking its orbit. By 1841, mathematician-astronomers Urbain Le Verrier of France and John Couch Adams of England had each independently predicted the location of this mystery planet.\nLe Verrier passed the information on to the man who would actually discover Neptune — German astronomer Johann Gottfried Galle of the Berlin Observatory — and Galle spotted the planet less than a degree from its predicted location during a two-night campaign in September 1846.\n\"The discovery was hailed as a major success for Newton's theory of gravity and the understanding of the universe,\" Hubble scientists said.\nStill, while Galle is credited as Neptune's discoverer, he wasn't the first person to actually see the planet.\nIn December 1612, the famed astronomer Galileo Galilei recorded an observation of Neptune, which he described as a \"star\" in his notebook while making observations using a handmade telescope, Hubble officials said. Later, in January 1613, Galileo noticed that the so-called \"star\" had shifted position in relation to other stars, but he never actually identified the object as a planet or followed up that initial observation.\nThat leaves the honor of Neptune's discovery securely with Galle.\nNeptune is so far away that it cannot be seen with the unaided eye. A small telescope or binoculars can resolve the planet. Currently, Neptune can be found in constellation Aquarius, close to the boundary with Capricorn, Hubble officials said.\nPhoto: NASA, ESA, and the Hubble Heritage Team\nThis article was reprinted with permission from SPACE.com.\nRelated on SPACE.com:\n- Top 10 Extreme Planet Facts\n- Perplexing Neptune: Our Outermost Major Planet\n- Solar System Explained From the Inside Out\nAlso on MNN:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:8aa98d75-e168-44c0-bae7-d766d422afcf>","<urn:uuid:f39dc8ed-678a-461b-8c43-24fff47a28f1>"],"error":null}
{"question":"As a gamer concerned about security, which provides better protection - a Moderate NAT type or email encryption with S/MIME?","answer":"Moderate NAT type and S/MIME email encryption serve different security purposes. Moderate NAT acts as a firewall with only one or two ports open to transmit data, restricting connections from interfering with your network, though it may cause some gaming lag. S/MIME email encryption, on the other hand, specifically protects email content by scrambling the message using public and private keys, making it unreadable to anyone without the proper decryption key. While Moderate NAT provides general network security, S/MIME offers targeted protection for email communications.","context":["There are times where you had to change the NAT type on your PC but weren’t able to do it. Today, we will learn how to change the Network Address Translation type on your pc most easily. Before we go ahead, let us first understand what NAT is and where it is used.\nMethods to change NAT type on PC\nHere are the simple methods to change NAT type on a PC device:\nMethod 1- Port Forward\nIt is the most common way to change your NAT type without going through additional hassles. You need to check your router model and the ports of the game that you want to play.\n- You need to go to your router’s homepage and find the Port Forwarding option from the list.\n- Enter the game’s ports in the first section and enter your IP address in the next column.\n- You will be asked to select the device where you want to point. After that, select UDP and TCP options.\n- You can now save the settings and reboot it for immediate results.\nLearn more about Port Forwarding.\nMethod 2- Use a VPN\nUsing a VPN is the best way to bypass a NAT connection as there are no data restrictions on a VPN. If you are using a VPN, All the data transmitted through your computer is encrypted. It will help you protect your online identity from the physical network, and you can access the network without any problem.\nThe biggest advantage of using a VPN is that it can restrict your internet service provider from monitoring your data and putting any restrictions on it, saving you from ISP throttling.\nMethod 3- Through Network Discovery\nIf you are a windows user, you can change NAT Type using Network discovery settings:\nFollow these steps to turn ON the network Discovery:\n- Open the start menu\n- Search & go to settings\n- Click Network and Internet\n- Then, click Network and Sharing Center\n- Choose Change advanced sharing options on the left\n- Turn on the network discovery option and check the box for the connected devices too.\nNow, you can save the settings, and you will be able to turn on your windows device’s network discovery.\nWhat is NAT?\nNetwork Address Translation is a process that involves the mapping of an IP address to a different address by modifying the network address information. It would help if you changed them in the network packets while transiting through a traffic routing device.\nIt looks like a simple process, but you may encounter many complex issues while changing NAT. It happens because there are some limitations of NAT by the manufacturers, and you have to take care of them by yourself.\nTypes of NAT\nYou can set different types of setting for your NAT. However, these are the most preferred NAT settings for optimal online experience:\nAs the name suggests, this is the open Type NAT that allows all the devices connected through the network to send and receive data. All the shared data is unrestricted, and there are no firewall configurations in place.\nWhile this will help you get a better experience, it does attract a lot of risks. Any hacker can easily get into your local area network as your ports are open.\nThis is more secure than the open NAT and has only one or two ports open to transmit data. In this setting, NAT will act as the firewall and restrict the connections from interfering in your network. However. You may experience a little lag in your online gaming experience when in the moderate NAT setting.\nThis is the safest type of NAT and does not let any data transmitted through the local network. It can protect you from most types of attacks, but you may face difficulty in joining other networks as most of the data is restricted in this network.\nYou may also face some lags in your gaming experience while being connected through a strict NAT.\nLearn about NAT filtering.\nThere are many other methods to change NAT type on PC, but these 4 methods are the easiest ways to get the work done.","Does a VPN protect email? Well, unfortunately, the answer is no, a VPN (Virtual Private Network) does not provide any protection for your email messages, but that’s not to say you can’t protect sensitive, personal, private email messages. In order to protect email you can use encryption, there are many ways this can be done, as a matter of fact, you can encrypt, Gmail, Outlook, iOS, OSX, Yahoo, Android, and Webmail. You can encrypt email messaging and depending on your email service provider, it can be enabled through the settings or the use of 3rd party apps. You can also use free/paid mail encryption services, which we will discuss later.\nFirst, we will look at how encryption works and the types of encryption used, to encrypt your email messages.\nHow Email Encryption Works\nEmail encryption is basically scrambling the contents of an email message making it a puzzle, which only you have the key to solve. The PKI ( Public Key Infrastructure ) is used to encrypt and decrypt email messages. Each person is assigned a public and private key, this is in the form of a digital code.\nThe public key is stored on a key server, along with the persons’ name and email address. This public key is used to encrypt email messages, for example, if someone sends you an encrypted email message they would use your public key to encrypt. Your private key is used to decrypt the email message. The private key is stored on your computer in a secure and private place in which you are the only one that has access.\nYour key can also be used as a digital signature as proof that an email message was sent by you.\nTypes Of Email Encryption\nThe two main types of email encryption are S/MIME and PGP/MIME. S/MIME stands for ( Secure Multipurpose Internet Mail Extensions ) it is built into most iOS and OSX devices. S/MIME relies on a central authority to choose the encryption algorithm. S/MIME is most often used because it is built into larger web-based email companies such as Outlook and Apple.\nThe other type of email encryption used is PGP/MIME which stands for ( Pretty Good Privacy Multipurpose Internet Email Extensions ). PGP/MIME uses a decentralized trust model and was developed to address security issues that are inherent in plain text messages. PGP/MIME provides more control and flexibility over the encryption used in your email messages. PGP/MIME encryption requires 3rd party encryption tool app.\nSome various email services and the encryption they use are :\n- Gmail – Has S/MIME built into the app. Encryption only works if both the sender and receiver have enabled.\n- Outlook – Uses S/MIME, but requires additional user setup.\n- iOS, OSX – Uses as default the built-in S/MIME encryption.\nEmail service providers that require 3rd party encryption tools :\n- Yahoo – Uses 3rd party app to encrypt with either S/MIME or PGP/MIME. Yahoo also uses as an extra layer of protection that is known as SSL ( Secure Sockets Layer ).\n- Android – can use both S/MIME and PGP/MIME, but requires extra user setup and 3rd party app.\nEmail Encryption Services\nAs mentioned earlier, they are a variety of free/paid email encryption services you can use, some of them are :\n- Cipher Mail – is a free app for use with Android devices. It uses S/MIME encryption.\n- Mail Envelope – This is a free app used on Chrome and Firefox browsers, compatible with Gmail, GMX, Outlook, and Yahoo. It uses open PGP/MIME encryption\n- Enig Mail – Is free extension used on Mozilla Thunderbird. It uses open PGP public key email encryption and decryption, it is compatible with Microsoft Windows, Unix-like, and Mac OSX operating system.\n- Gpg4win – Is a free email and file encryption package. It uses GnuPG public-key encryption for data and digital signatures. It is compatible with most versions of Microsoft Windows.\n- Proton Mail – Has free and paid plans, it allows end-to-end encryption with PGP/MIME compatibility. The paid service price levels are dependent on how many domains and messages are sent per day.\n- Vitru – Provides open end-to-end encryption, used on Chrome and G Suite browsers and is compatible with Gmail, Outlook, Hotmail, and Yahoo.\n- Send 2.0 – Provides military-grade encryption. It works with Outlook plug-in and is compatible with Outlook and Gmail.\nThese are only a few of the service providers, they are more that you could research and choose one that would fit your specific needs.\nFake, Disposable, 10 Minute Email Services\nThese are just a few of the names to describe these types of email services. Fake email services will provide you with a fake email address from which you can send, receive email messages. These email services allow you to send encrypted emails anonymously. The fake email address will protect your primary mailbox from spam, junk email, etc. Most of these services are free to use, there are many to choose from, so you may want to research and find the one that will fit your needs. Here are a few :\n- Guerrilla Mail\n- Temp Mail\n- Email Fake\n- Trash Mail\n- Dead Fake\nIn The End\nSo again a VPN does not protect emails, but if you are worried about email privacy, and or your emails being seen and read by other than the intended recipients, you can protect by means of encryption.\nThe good thing about using email encryption, for most users it is free, with either user enabled, or 3rd party downloadable app or web browser extensions. You can safely secure your email content, you can ensure your privacy.\nWith the varied means to protect yourself from viruses, malware, spam, junk email, hackers, email encryption will help with the whole internet, online security needed when navigating the web today. There are so many unforeseen threats that lurk on the internet, waiting for an unsecured connection, to steal, harvest and sell your information. Email encryption is just another tool to totally shut out these threats.\nI hope you enjoyed and found this post informative and if you have any questions regarding VPN’s please leave me a comment, and I will get back to you soon, Thank you\nStay secure and always practice\nSafe Text !!!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:df52b81e-2db0-4497-9fc7-0a4a312621be>","<urn:uuid:3dc92e47-9e6d-4f1b-9334-ba6127f15151>"],"error":null}
{"question":"How did Caravaggio and Carracci's approaches to Baroque painting compare?","answer":"Caravaggio and Carracci represented contrasting approaches within Baroque painting. Caravaggio was known for his dramatic use of light and chiaroscuro to create spatial effects, using unsophisticated people as models and abandoning detailed depiction. He was considered a 'naturalist' who wanted to break from stereotypes. In contrast, Carracci diligently practiced classical beauty, as seen in works like 'The Dead Christ Mourned', where he used simple, harmonious compositions that were somewhat sentimental while avoiding explicit depictions of horror and suffering. Both artists worked within the Baroque style but approached it with different artistic philosophies.","context":["Rome: From the “Whore of Babylon” to the Resplendent Bride of Christ\nWhen Martin Luther tacked his 95 theses to the doors of Wittenburg Cathedral in 1517 protesting the Catholic Church’s corruption, he initiated a movement that would transform the religious, political, and artistic landscape of Europe. For the next century, Europe would be in turmoil as new political and religious boundaries were determined, often through bloody military conflicts. Only in 1648, with the signing of the Treaty of Westphalia, did the conflicts between Protestants and Catholics subside in continental Europe.\nMartin Luther focused his critique on what he saw as the Church’s greed and abuse of power. He called Rome, the seat of papal power, “the whore of Babylon” decked out in finery of expensive art, grand architecture, and sumptuous banquets. The Church responded to the crisis in two ways: by internally addressing issues of corruption and by defending the doctrines rejected by the Protestants. Thus, while the first two decades of the 16th century were a period of lavish spending for the Papacy, the middle decades were a period of austerity. As one visitor to Rome noted in the 1560s, the entire city had become a convent. Piety and asceticism ruled the day.\nBy the end of the 16th century, the Catholic Church was once again feeling optimistic, even triumphant. It had emerged from the crisis with renewed vigor and clarity of purpose. Shepherding the faithful—instructing them on Catholic doctrines and inspiring virtuous behavior—took center stage. Keen to rebuild Rome’s reputation as a holy city, the Papacy embarked on extensive building and decoration campaigns aimed at highlighting its ancient origins, its beliefs, and its divinely-sanctioned authority. In the eyes of faithful Catholics, Rome was not an unfaithful whore, but a pure bride, beautifully adorned for her union with her divine spouse.\nThe Art of Persuasion: to Instruct, to Delight, to Move\nWhile the Protestants harshly criticized the cult of images, the Catholic Church ardently embraced the religious power of art. The visual arts, the Church argued, played a key role in guiding the faithful. They were certainly as important as the written and spoken word, and perhaps even more important, since they were accessible to the learned and the unlearned alike. In order to be effective in its pastoral role, religious art had to be clear, persuasive, and powerful. Not only did it have to instruct, it had to inspire. It had to move the faithful to feel the reality of Christ’s sacrifice, the suffering of the martyrs, the visions of the saints.\nThe Church’s emphasis on art’s pastoral role prompted artists to experiment with new and more direct means of engaging the viewer. Artists like Caravaggio turned to a powerful and dramatic realism, accentuated by bold contrasts of light and dark, and tightly-cropped compositions that enhance the physical and emotional immediacy of the depicted narrative. Other artists, like Annibale Carracci (who also experimented with realism), ultimately settled on a more classical visual language, inspired by the vibrant palette, idealized forms, and balanced compositions of the High Renaissance. Still others, like Giovanni Battista Gaulli, turned to daring feats of illusionism that blurred not only the boundaries between painting, sculpture, and architecture, but also those between the real and depicted worlds. In so doing, the divine was made physically present and palpable. Whether through shocking realism, dynamic movement, or exuberant ornamentation, seventeenth-century art is meant to impress. It aims to convince the viewer of the truth of its message by impacting the senses, awakening the emotions, and activating, even sharing the viewer’s space.\nThe Catholic Monarchs and Their Territories\nThe monarchs of Spain, Portugal, and France also embraced the more ornate elements of seventeenth century art to celebrate Catholicism. In Spain and its colonies, rulers invested vast resources on elaborate church facades, stunning, gold-covered chapels and tabernacles, and strikingly-realistic polychrome sculpture. In the Spanish Netherlands, where sacred art had suffered terribly as a result of the Protestant iconoclasm (the destruction of art), civic and religious leaders prioritized the adornment of churches as the region reclaimed its Catholic identity. Refurnishing the altars of Antwerp’s churches kept Peter Paul Rubens’ workshop busy for many years. Europe’s monarchs also adopted this artistic vocabulary to proclaim their own power and status. Louis XIV, for example, commissioned the splendid buildings and gardens of Versailles as a visual expression of his divine right to rule.\nThe Protestant North\nIn the Protestant countries, and especially in the newly-independent Dutch Republic (modern-day Holland), the artistic climate changed radically in the aftermath of the Reformation.\nTwo of the wealthiest sources of patronage—the monarchy and the Church—were now gone. In their stead arose an increasingly prosperous middle class eager to express its status, and its new sense of national pride, through the purchase of art.\nBy the middle of the 17th century a new market had emerged to meet the artistic tastes of this class. The demand was now for smaller scale paintings suitable for display in private homes. These paintings included religious subjects for private contemplation, as seen in Rembrandt’s poignant paintings and prints of biblical narratives, as well as portraits documenting individual likenesses.\nBut, the greatest change in the market was the dramatic increase in the popularity of landscapes, still-lifes, and scenes of everyday life (known as genre painting). Indeed, the proliferation of these subjects as independent artistic genres was one of the 17th century’s most significant contributions to the history of Western art. In all of these genres, artists revealed a keen interest in replicating observed reality—whether it be the light on the Dutch landscape, the momentary expression on a face, or the varied textures and materials of the objects the Dutch collected as they reaped the benefits of their expanding mercantile empire. These works demonstrated as much artistic virtuosity and physical immediacy as the grand decorations of the palaces and churches of Catholic Europe.\n“Baroque”—the Word, the Style, the Period\nIn the context of European history, the period from c. 1585 to c. 1700/1730 is often called the Baroque era. The word “baroque” derives from the Portuguese and Spanish words for a large, irregularly-shaped pearl (“barroco” and “barrueco,” respectively). Eighteenth century critics were the first to apply the term to the art of the 17th century. It was not a term of praise. To the eyes of these critics, who favored the restraint and order of Neoclassicism, the works of Bernini, Borromini, and Pietro da Cortona appeared bizarre, absurd, even diseased—in other words, misshapen, like an imperfect pearl.\nBy the middle of the 19th century, the word had lost its pejorative implications and was used to describe the ornate and complex qualities present in many examples of 17th-century art, music and literature. Eventually, the term came to designate the historical period as a whole. In the context of painting, for example, the stark realism of Zurbaran’s altarpieces, the quiet intimacy of Vermeer’s domestic interiors, and restrained classicism of Poussin’s landscapes are all “Baroque” (now with a capital “B” to indicate the historical period), regardless of the absence of the stylistic traits originally associated with the term.\nScholars continue to debate the validity of this label, admitting the usefulness of having a label for this distinct historical period, while also acknowledging its limitations in characterizing the variety of artistic styles present in the 17th century.","Baroque is an art style that flourished in Europe between 1600 and 1750. It originated in Italy during the Counter-Reformation period and developed in most parts of Europe that believe in Catholicism. Later, with the spread of Catholicism, its influence went far to Latin American and Asian countries. As an artistic style with far-reaching influence in time and space, the rise of Baroque was closely related to the religion of that time. Not only in painting, Baroque is represented in the whole field of art, including music, architecture, and decorative arts, and its connotations are also very complex. The most fundamental characteristic of Baroque is to break the seriousness, subtlety, and balance of the Renaissance period, advocating luxury and grandeur and focusing on expressing strong emotions. Baroque artworks are warm and tense, with a piercing and exciting artistic effect.\nOrigin and Influence\nIn the seventeenth century, Europe expanded, plundered overseas colonies to accumulate wealth, and advocated luxurious enjoyment in life. Hence, architecture, music, and art also required luxury and vividness, rich in a passionate mood. The word \"baroque\" is derived from the Spanish and Portuguese word for \"deformed pearl\" (Barroso). As an adjective, it means \"vulgar and messy.\" Europeans used the word to refer to \"works lacking the balanced character of classicism.\" It was initially a derogatory term for people who advocated classical art in the 18th century and was different from the Renaissance style in the 17th century. Today, the word has lost its derogatory implication. Baroque emerged in Italy in the second half of the 16th century, reached its heyday in the 17th century, and gradually declined in the 18th century. Baroque positively influenced Rococo in the 18th century and Romanticism in the 19th century. The Baroque style was strongly supported by the Church and was mainly popular in Italy, Flanders, Spain, and other countries where Catholicism was prevalent. The works of Italian master Bernini and Flemish painter Rubens reflect the most brilliant achievements of Baroque art in the 17th century.\nCharacteristics of Baroque\n1. Luxury and Opulence\nBaroque architecture combines art and color boldly and uses various precious materials. Baroque buildings are glorious, luxurious, and beautiful, have a strong artistic atmosphere, and are breathtaking.\nBaroque architecture is gorgeous in appearance and ingenious and unique in design. Designers used a lot of curved surfaces, breaking the dignified and rigorous approach of conventional buildings.\n3. Rich Imagination\nBaroque broke the rational tranquil harmony. It had a strong romantic color and emphasized the artist's rich imagination.\n4. Close to Nature\nIn the late Baroque era, more architects tended to incorporate natural elements. They built a lot of villas, gardens, and many city squares. The buildings became more spacious, and the use of natural elements increased. The buildings were more inclined to pursue the harmony between man and nature.\n5. Space and Perspective\nBaroque paid much attention to the sense of space and three-dimensionality of works.\nReligious themes occupy a dominant position in Baroque art.\n7. Away from Life and Times\nMost Baroque artists tended to stay away from life and times. For example, in some zenith paintings, the figures are insignificant, like some patterns.\nBaroque paintings are magnificent, full of movement, and have great perspective variation (such as foreshortening), dramatic composition, and contrast of ideal light. Dramatic and stage-like are characteristics of Baroque paintings.\nCaravaggio (1573-1610), a pioneer of Baroque painting, is characterized by his use of light to obtain a dramatic effect on the painting, as well as the use of chiaroscuro to set off a real sense of space, and abandoned the depiction of details. His approach to nature was intuitive, using brutish or unsophisticated people as models for his portraits. He took a different approach to still life painting. His masterpieces include The Supper at Emmaus, The Entombment of Christ, Basket of Fruit, and Bacchus. Wild, irascible, irritable, and short-lived, Caravaggio wanted to break away from stereotypes and rethink art and was stylistically known as a \"naturalist.\"\nCarracci (1560-1609), an Italian painter of the Baroque style, diligently practiced classical beauty. His masterpiece includes the altarpiece, The Dead Christ Mourned, in which the light shines on the Christ, and the overall technique of arousing the viewer's emotion, which is of Baroque style. The composition is simple and harmonious, somewhat sentimental, but avoids reminding people of the horrors of death and the pain of suffering."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a31020d0-ed9a-464f-920c-6a8b7575b091>","<urn:uuid:f5198023-ae62-4faa-b0ac-904f30736f87>"],"error":null}
{"question":"What are the similarities between net zero building design principles and accessible parking space requirements in terms of their approach to environmental sustainability?","answer":"While both topics address different aspects of building design, there is very limited overlap in terms of environmental sustainability. Net zero building focuses on minimizing energy use and carbon emissions through strategies like efficient building envelopes and renewable energy generation. In contrast, accessible parking requirements focus primarily on functionality for users with disabilities through specifications like size, gradient, and placement near entrances. The only minor environmental consideration mentioned for accessible parking is maintenance requirements like removing debris and maintaining surrounding vegetation. The documents do not indicate any explicit environmental sustainability focus in accessible parking design.","context":["This 12-hour package contains:\n5 HSW hour Audio Course – Moving Toward Net Zero | #AIABLTI518\n5 HSW hour Audio Course – CA Building Code Division 2: Accessibility | #AIABLTI331.23\n2 HSW hour Audio Course – Speaking of Older Buildings\nMoving Toward Net Zero\nInstructor: Paul Spite\nSustainability is defined by one source as “meeting current needs, without compromising the ability of future generations to meet their needs, in the environment we will leave them.” Net zero refers to achieving an overall balance between carbon-based emissions produced and the removal or offsetting of the same amount of carbon from the atmosphere. Taken together, striving to meet these two objectives can result in choices made in project design and project use, resulting in the least harm possible being done to our environment.\nWe will examine a few key principles involved in moving toward ‘net zero:\n- Designing buildings that use as little energy as possible, once occupied as intended\n- Generating as much power on-site, or the equivalent thereof, to supply the power still needed and used by the facility\n- Look at carbon emissions and ways to lower or offset greenhouse gas emissions as a result of our activities\nThis course teaches the following knowledge and skills:\n- An understanding of what is being referred to, when ‘net zero’ is being discussed\n- An overview of broad strategies used in designing sustainable buildings\n- Identifying sources of more environmentally friendly materials for incorporation in projects\n- Why buildings renovated for adaptive reuse are arguably the greenest buildings.\n- How incorporating passive design principles can reduce future energy use.\n- Use of the earth’s temperature below the frost line to preheat and precool incoming air\n- A good understanding of different options for insulating building envelopes\n- How LEED certification can be sought and obtained\n- Different ways in which green power can be purchased\n- Criteria that can be used in purchasing Renewable Energy Certificates\n- The various technologies used to generate power on site from renewable resources\n- Steps to be taken in planning for an on-site renewable generation project\n- The primary differences between renewable energy certificates and carbon offsets\n- How clean water state revolving funds are used to finance conservation projects\nThis Course Covers:\nOverall Steps in Reaching Net Zero\nPart One – Minimizing Energy Use in Buildings\n- Broad Design Concepts for Energy Conservation\n- Energy Audits\n- Passive Site and Building Design\n- Designing for LEED Certification\n- Recycling Buildings\n- Energy Efficient Building Envelopes\n- Energy Efficient Building Components\n- Lifestyle Choices to Conserve Energy and Other Resources\n- Creating Replacement Power\n- Summary of Minimizing Energy Use\nPart Two – Minimizing Carbon Output\n- Purchasing Green Power\n- Purchasing Carbon Offsets\n- Funding Land Conservation Projects with the Clean Water State Revolving Fund\n- Summary of Reducing Carbon Output\nCalifornia Building Code: Division 2: Accessibility\nInstructor: Rodger B. Peck\n(If you are being audited by the Board, please print out this section It contains the information needed for the continuing education audit.)\nRodger has over 25 years experience building, teaching and consulting with individuals in the construction industry. He has conducted extensive research and study in developing, compiling and writing Americans with Disabilities Act courses for architects, engineers, and commercial and residential builders. Rodger holds a vast amount of certifications and approvals, both State specific and nationally:\n- Michigan Residential Builders License #2101137251\n- American Institute of Architect (AIA) approved instructor\n- International Distance Education Certified (IDECC) Qualified #67861\n- Lead Paint Safety (RRP) Certified Instructor through the Environmental Protection Agency (EPA)\n- Alabama Licensing Board approved instructor\n- Florida Department of Business and Professional Regulation (DBPR) approved instructor\n- Georgia Board for Residential and General Contractors (BRGC) approved instructor\n- Massachusetts Department of Public Safety (DPS) approved instructor\n- Michigan Licensing and Regulatory Affairs (LARA) approved instructor\n- Minnesota Department of Licensing and Industry (DLI) approved instructor\n- Oregon Construction Contractor Board (CCB) approved instructor\n- Utah Department of Occupational and Professional Licensing (DOPL) approved instructor\n- Virginia Department of Professional and Occupational Regulation (DPOR) approved instructor\nThe ADA is one of America’s most comprehensive pieces of civil rights legislation that prohibits discrimination and guarantees that people with disabilities have the same opportunities as everyone else to participate in the mainstream of American life — to enjoy employment opportunities, to purchase goods and services, and to participate in State and local government programs and services. The standards set minimum requirements – both scoping and technical – for newly designed and constructed, or altered state and local government facilities, public accommodations, and commercial facilities to be readily accessible to and usable by individuals with disabilities.\nThis five-hour audio course covers the second chapter of the ADA Standards for Accessible Design and includes the California modifications found in Division 2, Chapter 11B of the California Building Code including the most recent changes. While an individual designer may be able to use the ADA for personal or generic residential design, the California code should be used by registered design professionals and enforcement officials and applied when the project is for public use within the state of California. In this video course, the material is presented by simple narration and power point video presentation, as well as on-site, real-world video examples in various accessible buildings, used by both the public and private businesses.\nAfter completing this course, participants will be able to:\n- Recognize how the overall philosophy of the Americans with Disabilities Act can and does create an environment of opportunity and non-discrimination.\n- Be able recognize what facilities can and should comply with Chapter 11B Accessibility of the California Building Code.\n- Outline at least one design strategy based on accessibility standards for the construction of either a public or private building.\n- Summarize the options available to the design or building professional when designing a facility per the requirements of Chapter 11B.\nThis Course Covers:\n- Existing Buildings and Facilities\n- General Exceptions\n- Protruding Objects\n- Operable Parts\n- Accessible Routes: Work areas, Amusement parks, Recreational areas, Entrances, Lifts\n- Accessible Means of Egress\n- Parking Spaces\n- Passenger Loading Zones and Bus Stops\n- Drinking Fountains\n- Kitchens, Kitchenettes, and Sinks\n- Toilet Facilities and Bathing Facilities\n- Washing Machines and Clothes Dryers\n- Fire Alarm Systems\n- Transportation Facilities\n- Assistive Listening Systems\n- Automatic Teller Machines and Fare Machines\n- Assembly Areas\n- Dressing, Fitting, and Locker Rooms\n- Medical Care and Long-term Care Facilities\n- Transient Lodging Guest Rooms\n- Dining Surfaces and Work Surfaces\n- Sales and Service\n- Depositories, Vending Machines, Change Machines, Mailboxes and Fuel Dispensers\n- Two-Way Communication Systems\n- Judicial Facilities\n- Detention Facilities and Correctional Facilities\n- Residential Facilities\n- Recreational Boating and Golf Facilities\n- Exercise Machines and Equipment\n- Play Areas\n- Saunas and Steam Rooms\n- Swimming Pools, Wading Pools and Spas\n- Shooting Facilities with Firing Positions\nSpeaking of Older Buildings\nInstructor: Paul Spite\nWhen the question of what to do with older buildings arises, it usually resolves itself to choices of reusing them as is, repairing them, restoring them, repurposing them or replacing them.\nThere is an old saying regarding existing buildings that goes like this. “It has good bones.” Any facility, free of structural defects and doing a reasonably good job of keeping water out, represents a tangible asset. The trick to maximizing the value of existing structures, especially in areas where changing economic factors have also resulted in changing market demands, is to approach their reuse from a different point of view. The judicious employment of renovation funds should not be based on restoring them to a previous use, but making them suitable for other markets in which the existing bones might enable a whole new purpose.\nThis presentation proposes criteria that may prove valuable in determining which option For the disposition of older buildings represents the best value for the owner, the buyer or the community in which the structure is located. It will also briefly examine a potential business opportunity in a collaboration between architects and commercial real estate agents.\nFor the benefit of everyone involved, any reuse of older buildings is better than none. Some thought just needs to be put into the options of how to do so, before any decision is made on to the best way to proceed.\nBy the end of this course, the design professional will be able to:\n- Understand the various stakeholders seeking to provide input into the end use of a newly acquired aged structure.\n- Readily identify system components needing to be included in the assessment of an older or historic building, as part of determining how to proceed with its further use.\n- Be able to explain what determine historic significance and how cultural significance plays a role in the practicality of restoring all or part of a heritage building.\n- Realize principles inherent in a decision to move forward with adaptive reuse, both the benefits and the difficulties encountered.\n- Develop a solid rationale for why or why not an older building should be demolished and replaced with a more modern structure on the same site.\n- Have a grasp of what can be accomplished through a collaboration between architects and commercial realtors, in regards to the reuse of older buildings.\nThis course covers:\n- Assessing Options\n- Realtor/Architect Collaboration\nCredit(s) earned on completion of this course will be reported to AIA CES for AIA members. Certificates of Completion for both AIA members and non-AIA members are available to print upon completion of the course.\nThis course is registered with AIA CES for continuing professional education. As such, it does not include content that may be deemed or construed to be an approval or endorsement by the AIA or any material of construction or any method or manner of handling, using, distributing, or dealing in any material or product.","Contributed by Anthea Skinner\nIf you have limited mobility as a result of a disability or impairment, a Disability Parking Permit can allow you to use accessible parking spaces. Accessible parking is designed to make it easier for people with disabilities to visit schools, hospitals, shopping centres and other everyday services and activities. In Australia, it is a legal requirement for councils and businesses to supply accessible parking, however, finding an empty accessible parking space can be difficult and demand often seems to outstrip supply. This article explains all the features of accessible parking so that you can get full use from your Disability Parking Permit.\nAccessible Parking – Where is It?\nWhen you arrive in a new carparking facility, the first thing you’ll need to do is find the accessible parking bays. Generally speaking, accessible parking should be placed close to the accessible entrance of a building. This allows people with limited mobility to access the premises without expending too much energy. If you are in an underground or multi-story parking facility, accessible parking will usually be close to the elevators. If you’re in a hilly area, look for the nearest flat ground to an entrance, accessible parking spaces should always be situated on level ground, with less than 1:40 gradient, and should have an equally level adjacent space to allow you to easily enter and exit your vehicle.\nAccessible parking spaces are clearly recognisable and are labelled with a white international access symbol painted on a blue rectangle. This access symbol highlights the fact that parking spaces are reserved only for vehicles displaying valid Disability Parking Permits. If you park in an accessible space without a valid permit you will be fined.\nHow Big Should Accessible Parking Spaces Be?\nAccessible parking spaces are larger than regular parking spots. There are a variety of reasons why people with disabilities can require extra space when parking. If you drive or travel in your car while still seated in your wheelchair, you’ll need extra room for ramps and lifting equipment to enter and exit your vehicle safely. You’ll also need extra space if you transfer between your wheelchair and the car, or if you use bulky mobility equipment like walking frames or crutches. Even if you don’t use large mobility aids, limited or unpredictable movement might mean that you need extra space to get in and out of the car. If you require help from a carer to access your vehicle or to use safety equipment like seat belts, extra space can also useful.\nFor all these reasons, the size and placement of accessible parking in Australia is regulated by law. This states that parallel accessible parking spaces should be 780cm long and 320cm wide, with an extra 160cm of space beside them to allow for loading and unloading. Angle parking spaces should be 540cm long and 240cm wide with a 240cm2 space behind for rear-loading vehicles. Angle spaces should also have a 540cm long by 240cm space next to them to allow easy access to side doors. You will notice that loading spaces next to accessible parking these spaces are often painted with diagonal stripes. These stripes show that the area needs to be kept clear to provide people with disabilities room for to safely enter and exit their vehicles. It is illegal to park in these striped areas, even if you have a Disability Parking Permit.\nWhat About the Areas Around Accessible Parking Spaces?\nThere are also regulations covering the areas immediately surrounding accessible parking spaces. Both they, and their designated loading spaces should be placed on flat ground and any ramps accessing the space should have a gradient of no more than 1:10. These are important safety requirements designed to avoid falls on uneven or steep ground. Accessible parking should have an overhead clearance of at least 250cm, allowing room for vans and roof mounted wheelchair storage and reducing overhead hazards for people with vision impairment. Line makings in and around the parking space should be painted with a non-slip surface.\nWhile the regulations listed above are legal requirements, there are other things that can help to make parking truly accessible for all. All line markings should be clearly visible, and repainted when they start to fade. Any gardens immediately surrounding parking spaces should be kept well maintained as overhanging branches can be dangerous to people with vision impairments and overgrown grass or shrubs can be a tripping hazard. Fallen debris like litter and leaves can be slippery and should be removed regularly. Accessible parking spaces should also be kept clear of obstacles like rubbish bins and discarded trolleys. Any curbs adjacent to the spaces should have ramps to allow wheelchair users to access the footpath safely.\nHow Many Spaces Should There Be?\nIt can sometimes seem like there just isn’t enough parking to go around. However, there are minimum legal requirements for the numbers of accessible parking spaces that need to be provided. These requirements differ depending on the type of business or organisation that you are visiting, but are usually either one space in every 50 or 100 parking spaces. The minimum requirement for a range of businesses is outlined below:\nThe Accessible Parking in My Area Isn’t Up to Scratch… What Should I Do?\nEven though accessible parking is required by law, sometimes it can be poorly maintained or out of date with current legal requirements. If you are concerned about the quality of accessible parking in your area you should first take the matter up with the management in charge of the parking lot. Explain the problem that you are having, and that it is making it difficult for you to access their premises. Most business owners understand that good accessibility ensures that more customers can use their services and will try to help. They may not realise that things like overgrown trees or faded line markings can limit people’s access and will often be eager to assist once it is politely pointed out to them. Make a note of the time and date of any correspondence, the name of who you speak to and be sure to save copies of any letters or emails. If this doesn’t resolve the matter then contact your local council with the details of your concerns and they will be able to advocate on your behalf."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:98b811de-99c8-4ac9-9138-c1b2a4481b7e>","<urn:uuid:d7c886a0-c81e-48eb-a262-4f22fc2a0b2b>"],"error":null}
{"question":"What are the technical principles behind radar-based detection systems used in both military survey operations and weather monitoring, and how do they process electromagnetic signals for different purposes?","answer":"In military survey operations, as seen at the Battle of Alamein, survey troops used sound ranging and flash spotting sections to detect enemy positions, requiring regular calibration every 3 days and meteorological data updates every 2 hours. For weather monitoring, Doppler radar systems use electromagnetic signals that are sent toward targets and analyzed based on the returned signal's frequency changes. The Doppler effect, named after Christian Doppler, shows how a wave's frequency changes relative to observer movement. While true Doppler radar (used in police speed guns) sends direct beams at precise frequencies, Pulse-Doppler radar used in meteorology sends pulses of radiation in a circular motion to calculate radial velocity, enabling the tracking of precipitation, wind patterns, and even bird migration.","context":["This episode looks at the Counter Battery battle at 2nd Alamein and is part of our Battles of Alamein series. It is part of the RAAHC commemorations for 150 years of Australian Artillery and a part of our 10 decisive battles for Australian Artillery series. These are the show notes – for the full details, listen to the podcast on your favourite podcast player.\nThe is far, far better to give than to receive – the key principle in the counter battery battle.\nSurvey Troops supporting the battle\nHow did they do it and what role did the 4th Survey (Durham) Regiment play, along with the Desert Air Force and Engineers in constructing a process to find and neutralise Axis Artillery? The Regiment established 2 sections for flash spotting and 2 sections for sound ranging. The Survey troops supported the calibration of the guns, determining the muzzle velocity for each of the guns in the 8th Army. Calibration shoots were conducted every 3 days, such was the wear that some of the guns were experiencing during the heavy firing.\nThe Survey Troops also maintained the meteorological data, updating it every 2 hours, providing corrections of the moment for the batteries.\nThe Counter Battery Office\nThe CBO was responsible for maintaining the HBL – the Hostile Battery List. It deter\nThe 9th and 51st Divisions sent out infantry patrols to draw artillery fire. The sound ranging and flash spotting sections were forewarned, enabling them to update the Hostile Battery List and improve it’s accuracy.\nAs 9th Division took on a larger part of\nRamsay and Kirkman denied Axis Artillery the ability to interfere with the commencement of Op Lightfoot. Before Op Lightfoot commenced, Kirkman instigated a silent counter battery policy for the last 2 weeks before the offensive. This meant that no HBs were to be fired on. This decreased the need for the Axis artillery to move positions. Kirkman wanted all enemy batteries to remain in place right up until their neutralisation at the start of H-Hour. In XXX Corp the the Counter Battery bombardment started with batteries receiving between 20:1 and no less than 10:1. Kirkman ensured that the Allies had achieved moral superiority over the Axis artillery. This was supported by the work of the gunners, firing 24 hours a day and an almost unlimited supply of 25 pounder ammunition.\nThe Desert Air Force also worked to assist in the in locating the batteries. It flew photo reconnaissance mission and the Aerial Photographic Interpretation Section kept track of each of the batteries.\nThe main CB serial last for 15 minutes, commencing at 21:40.\nOnce the infantry cross the LD, the static battle finished and a battle of manoeuvre commenced. This made it more difficult to track the HBs and as a consequence\nHow was the fire plan developed to support the infantry as they crossed the LD?\nWe discuss the C2 arrangements for the artillery as tasks changed from CB at 21:40 to neutralisation of targets to support the advance of XXX Corps troops at 22:00. This was important because of the depth and extent of the ‘devil’s garden’ and the fact that Montgomery did not have confidence in his X Corps Armour to support the infantry.\nThe first day saw 80% of the objectives being obtained. On D+1, Monty had the 9th Division swing north towards the coast, just 6,000 meters away and cut of the Axis units to the East. The 26th BDE commenced the advance. They had 7 field and 2 medium Regiments in support. We see here the flexibility of artillery. The 9th Division becomes the main effort.\nRommel counter attacks with the 90th Light Division and commits the 21st Panzer Division, his operational reserve. This is a decisive event for Montgomery. The 21st Panzer was already out of position due to the deception plan, Op Bertram. He commits not only his physical reserve, but this will also consume his fuel reserve.\nOn the 28th the 20th BDE takes over the offensive for 9th Division. BRIG Ramsay, the 9th Division CRA (Commander, Royal Artillery), now has 13 field regiments and 3 medium regiments under command, a total of 360 guns.\nThe 20th BDE culminates and 26th BDE is recommitted to the battle. It will be supported by a fireplan created extremely quickly by the Brigade Major Royal Artillery, MAJ Hylton Williams. It was this fireplan that was promulgated by Montgomery of an example of best practice staff work. The fireplan was particularly complicated with some Regiments firing directly into the axis of advance of the advancing 26th BDE.\nThe 24th BDE conducts a relief in place, at night, in contact at The Saucer. Ramsay used the ‘Stonk’ to support the 24th in their defensive battle. This is a Standard Concentration of 72 guns – effectively a divisional fire mission. This were able to get predicted fire in sometimes as little time as 2 minutes – all this with just slide rules.\nOne of the critical lessons from 2nd El Alamein was that infantry alone, without armour, could achieve their objectives against stiff resistance if supported with copious amounts of artillery.\nWe discuss a clever German ruse aimed at undermining the infantry confidence in the supporting artillery, which, thankfully, was discovered after some careful crater analysis.\nFor the rest of the war, an “Alamein Barrage” was the term used to describe a barrage of particular ferocity.","Doppler radars help us get ready in the morning. They tell us whether to pack an umbrella or leave the coat at home. And they help us navigate conversations with anyone — because we all have a strange fascination with weather. But Doppler radars also used for more than just weather tracking — doppler radars can be found in aviation, police speed guns, radiology, health care and missile systems.\nHow does this invisible phenomenon work? In short, Doppler radars send beams of electromagnetic signals toward a desired target. The signals use sound reflection to analyze how the returned signal's frequency has been altered. But there's more to it.\nThe Doppler Effect\nThe Doppler effect (or shift) is named after the Austrian physicist Christian Doppler, who in 1842 proposed that a wave's frequency changes for an observer moving close to its source. The frequency is higher during its initial transmission, identical at the moment of passing, and lower during the recession.\nIt's commonly experienced when a car siren or horn passes, which is why the sound is always higher when the vehicle is approaching than when it is receding. With the naked ear, you can get a general idea of how far and fast the vehicle is traveling.\nSince light waves have no medium, we use the Doppler effect in terms of the motion of the source relative to its observer (the radar). Obviously, this requires a much more complex and calculated formula than what's audible to the human ear.\nTrue Doppler radar, in its simplest form, is found in police officer's radar guns, which determine the speed of a motor vehicle. The electromagnetic radiation is sent out at a precise frequency, then the radar uses the frequency of the wave on the return path to calculate a targeted object's velocity.\nHowever, one of the most common known examples of Doppler use, meteorology, uses a different technique called Pulse-Doppler radar.\nGIF courtesy of Tumblr, accuweather\nThis sends the beam in a swirling, circular motion, which we often see on TV, shown in the GIF above. When a beam strikes a raindrop, hailstone or other precipitation, the doppler can calculate the distance, intensity and direction from the reflected beam. This radar can be so sensitive that it can track wind-blown particles and bird migration.\nWhile True Doppler sends a direct beam, Pulse-Doppler sends pulses of radiation that allow for calculations of radial velocity. It can be sent out with varying frequency and carrier cycles, using the formula below to track.\n33 Stunning Weather Photos From Readers Around the World\nThe U.S. Navy used continuous-broadcast (or FM) radar during World War II so that aircrafts could perform night combat operation, such as approaching target ships and training guns on enemy aircraft. They were also used as navigation aids, helping pilots accurately determine wind speed.\nThese early Dopplers relied on analog filters to calculate velocity, which were riddled with inaccuracies, due to the extra weight. It wasn't until the 1970s, when modern microprocessors were invented, that Doppler radar became more practical for weather and air traffic control.\nToday, this technology is so advanced that we can determine weather complications weeks in advance. So next time you're stuck at the airport due to adverse weather when the skies appear to be clear and sunny, you can thank your nearest Doppler radar.\nWhy the Weatherman Is Wrong\nSo if Doppler is right, why is the weatherman so often wrong? For one thing, he's not always wrong, we just remember the times when he is (see also: Snowquester). The other thing to keep in mind is that the weather is very dynamic and constantly changing. There's a lot of data in weather, and forecasts have actually gotten far better over the years, though they're clearly not perfect. Newer prediction models are incorporating these massive amounts of data to help meteorologists understand dynamic weather patterns, which will lead to more accurate forecasts to help us get dressed for day — and know whether to grab the umbrella.\nImage courtesy of iStockphoto, Anthro"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cc4eddfb-4b2a-4c61-b7db-546386042fa1>","<urn:uuid:3955ae10-2af6-4e37-bd01-7031752d7275>"],"error":null}
{"question":"Which material is more durable and resistant to environmental damage: slate roofs or limestone gravestones?","answer":"Slate roofs are more durable and resistant to environmental damage. Slate roofs can last for a hundred years or more with proper maintenance, and they don't warp, corrode, or attract mold. They are also fire resistant and don't absorb water. In contrast, gravestones, particularly those made of natural stone, are highly susceptible to damage from environmental factors including flora, fauna, contaminants in the atmosphere, and weather conditions, which can make them unreadable over time.","context":["Have you spent any time walking through old cemeteries? If so, you know the effects the elements can have on headstones. Damage from surrounding flora and fauna, contaminants in the atmosphere, animals, and people can cause the stones to become unreadable over time.\nJump ahead to these sections:\n- Step 1: Obtain Permission\n- Step 2: Make Sure the Stone is Secure\n- Step 3: Check the Outdoor Temperature\n- Step 4: Look for Plants Stuck to the Stone\n- Step 5: Inspect the Stone\n- Step 6: Wet the Stone with Water\n- Step 7: Gently Scrub the Stone\n- Step 8: Rinse the Stone\n- Special Considerations for Each Gravestone Type\n- What Not to Use When Cleaning Gravestones\nWant to preserve your great-grandparents’ legacy or fix a community treasure? Kudos to you for researching this subject before you gather your household cleaners and other materials — which, as we’ll cover, can often do more harm than good.\nKnow this for sure — it’s not going to be anywhere near the cost of a funeral to fix one. Here are some basic steps and special instructions for each type of stone.\nPlease note that these are basic instructions, and each stone should be individually evaluated. Treat aged stones delicately. Cleaning them requires the skill of an expert who has been appropriately trained by a conservationist.\nStep 1: Obtain Permission\nYou might be most interested in learning how to clean the gravestone of your loved ones or ancestors. You only have to ask the permission of your other family members to clean these headstones if this is the case.\nOn the other hand, are you interested in cleaning stones for historical or altruistic reasons? Make sure you get permission from the deceased’s descendants. They may be impossible to locate, so in that case, obtain approval from the cemetery superintendent or the municipal employee who oversees the cemetery. Know ahead of time that these individuals may not feel comfortable with an untrained person cleaning the stone.\nStep 2: Make Sure the Stone is Secure\nOld gravestone bases may be unstable after hundreds of years. Make sure the marker you’re working with is secure and isn’t going to fall. Remember — you want to remain safe throughout the process.\nStep 3: Check the Outdoor Temperature\nTouch the surface of the gravestone with your bare hand. Is the stone hot to the touch? If so, do not clean it with cold water. This could cause cracks.\nConsider covering the stone until the temperature lowers. You could use warm water to clean it if this isn't an option.\nAlso, don’t attempt to clean a stone when there’s any chance of freezing temperatures in the forecast. Water expands when it freezes, so the water could get inside the minuscule cracks in the stone and grow.\nStep 4: Look for Plants Stuck to the Stone\nYour natural instinct might be to pull any plants off a stone as soon as you see one — but expert stone cleaners caution against this practice.\nInstead, cut the plants, particularly ivy, at the root and periodically along the vine. Some plants’ suckers can damage stones. You can remove the suckers but it’s best to wait until the plant is dead to complete the cleaning.\nStep 5: Inspect the Stone\nStop and scrutinize the stone before you begin the cleaning process. There may be indicators that you need to call in a stone preservationist to handle the job of cleaning the monument. Here are some things to look for on the stone:\n- Delamination: Some stones are layered. Do not begin the cleaning process if you see that the stone’s layers are starting to separate. This is a job for a professional.\n- Hollow sounds: Do you hear a hollow sound when you tap lightly on the stone? If so, don’t attempt to clean it.\n- Large cracks: Whether the stone is cracked from an intrusive tree root or another source, do not attempt to clean the stone. The fragile stone may chip easily and should be repaired before cleaning begins.\nStep 6: Wet the Stone with Water\nFirst, it’s worth noting that you should never use a power washer to clean a headstone. This would do irreparable damage. Instead, use a pump sprayer set on a mist setting.\nGently wet the stone with clean water and watch as it dries. Look for stress cracks on the stone. If there are many cracks or they seem unusually wide, don’t continue the cleaning process.\nStep 7: Gently Scrub the Stone\nHave you determined that the stone is in good enough shape to be cleaned by an amateur? If so, you can move on to the next step. Soak the stone and wait for a few minutes. Scrape the moss and other plant growth off of the stone with a wooden or plastic scraper.\nRinse the gravestone of any debris that you brought to the surface from your gentle scrubbing. You will notice that there was no mention of any cleaning products in this article. Most sources recommend not using any cleaning products until you have received training on which products will not harm the stone or the surrounding vegetation.\nSpecial Considerations for Each Gravestone Type\nThese instructions can be used to clean most gravestones but there may be special instructions to consider for specific materials.\nGranite can become very hot in the summer and cleaning it on a hot day may be problematic. It’ll be difficult to keep the surface wet when water evaporates so quickly. The monument could end up looking streaky.\nMarble is especially susceptible to damage when you use cleaning agents. Take care when cleaning a marble surface. Do not use wire brushes or aggressive scrubbing methods.\nBronze is different than cleaning granite, slate, or other natural stone. You can use the same steps when you clean a bronze monument — just add an extra step at the end.\nSome experts recommend adding a protective coating to a newly-cleaned bronze monument. Some websites recommend adding a thin layer of wax paste. Buff the surface to a smooth finish once the surface dries.\nAvoid vinegar solutions when you clean slate gravestones. The method listed in the steps above is appropriate when working with this natural stone.\nSandstone is soft enough to carve and hard enough to last for centuries. The problem with using sandstone is that delamination often occurs and it crumbles when moisture gets into the layers.\nBe very careful when you work with sandstone headstones.\nWhat Not to Use When Cleaning Gravestones\nProfessionals in the past used the following products and tools to clean monuments. Unfortunately, experts today agree that you should never use these products to conserve gravestones.\nPeople have cleaned gravestones with bleach for years. In fact, you can still find some websites that recommend using this household product to clean marble and other stones. This is a mistake — bleach leaves salt deposits on headstones, which can break down stones’ surfaces and cause them to crumble.\nAvoid using wire brushes. A wire brush will remove any algae that covers a stone but it will also remove parts of the stone as well.\nDo not use power washers to clean gravestones. Power washers will remove contaminants from the surface of stones but will damage headstones in the process.\nFinally, avoid using any power tools to clean a gravestone. Some companies use nylon wheels attached to a drill to clean the surfaces of stones. This does irreparable damage and can render stones unreadable.\nMost people have good intentions when they clean a gravestone. Just make sure your good intentions don’t cause more harm than good. Follow the best practices when you clean your loved one’s stone.\nAre you interested in preserving gravestones in your local community? If so, look for preservation groups that work in your area. Some groups host classes to teach others how to properly clean old monuments.","Whether it’s time to replace your roof or you’re building a new home, the roof you choose is essential to the longevity and visual appeal of your home. There are many roofing materials to choose from, including asphalt, which is the cheapest and most common, to tile and slate, which are more expensive.\nIn this article, we look at the slate roof and why it’s a good value for the price, and why you should consider this option when choosing a new roof for your home.\nWhat is a Slate Roof?\nSlate tiles are constructed from rock which comes from volcanic ash and clay. Slate roofs are among the oldest used and date back centuries because of the availability of the material and the longevity of the tiles. There are slate roofs built hundreds of years ago that still hold up to this day.\nThere are two types of slate roofs available — hard and soft. The hard variety is strong and durable, making it ideal for areas that experience harsh weather. Hard slate tiles are also fire resistant and don’t absorb water, which makes them less prone to warping.\nThe soft slate tile isn’t as durable as the hard slate, but it does retain the fire and water resistance of hard slate. People may opt for the soft variety because it is somewhat less expensive while providing much of the same advantages.\nHow Long Do Slate Roofs Last?\nAs mentioned, slate roofs can last for a hundred years or more with proper care and maintenance, and many factors come into play when discussing the longevity of a slate roof. On average, a well-maintained slate roof in ideal conditions lasts about 60 years.\nWeather plays a significant factor in the longevity of slate roofing tiles, and you will have to replace the underlay about every 30-40 years to maintain the health of the roof and the integrity of the tiles. Also, the longevity is mostly dependent on the maintenance you do, taking care to fix small problems before they turn into large ones down the road.\nWhat is the Cost of a Slate Roof?\nOne reason many people steer clear of slate roofs is the cost, and to be fair, it is one of the more expensive roofing tiles you can buy. If your current home has a slate roof, the cost to upkeep it is minimal, and you likely won’t have any large expenditures during ownership.\nHowever, if you’re installing a brand new slate roof, you can expect to pay about $1,500 per square, and in roofing terms, a square is a 10×10 foot area. By way of comparison, asphalt shingles typically cost about $200 per square, so you can see why so many people opt for asphalt over slate.\nAnother thing to consider when talking about the cost is the installation. Asphalt shingles are ubiquitous, and almost every contractor is familiar with this material. Because it’s so widely available, and because of its familiarity, the labor cost to install an asphalt roof is relatively cheap compared to more exotic materials such as slate or concrete.\nAlso, since not every roofing contractor is skilled at installing slate, you need to hire one who has the experience, which means higher labor costs. You don’t want to cut corners on a slate roof installation because doing so will cost you more money in repairs due to a shoddy job down the road.\nCommon Issues with Slate Roofs\nWhile slate roofs sound like the perfect option, if you want a long-lasting roof that offers protection against fire and the elements, there are some disadvantages to installing these as well.\nThe major disadvantage, as mentioned above, is the cost of the tiles themselves and the cost of labor for installation. Depending on the type of slate you decide to install, the roof can cost anywhere from $1,000 to $6,000 a square installed. Also, as mentioned, not every roofing contractor has experience installing slate roofs, which makes it more difficult to find a qualified installer in some areas.\nThickness of Tiles Can Be Uneven\nAnother disadvantage to slate tiles it that they’re sometimes not gauged, which means the thickness varies from tile to tile.\nSlate Tiles are Heavy\nThe weight of the roof is also something to consider. Slate tiles are heavy, and the deck of the roof needs reinforcing to deal with the weight. Slate roofs can weigh up to 1,500 pounds, and you’ll need an inspection of your roof’s support system to make sure it can handle the weight before you go down that road.\nDifficult to Find Tile Replacements\nFinally, slate roofing tiles can become damaged if you have roofers who have to go up and work on the roof. If tiles get broken, it can be challenging to find a replacement that matches the exact color.\nDo you Need a Contractor to Fix your Slate Roof?\nThe best advice if you want to keep your slate roof in tip-top condition is to hire a professional when you need fixing or maintenance; however, because of the high cost, you can usually handle minor repairs and fixes yourself if you know what you’re doing. When you have your slate roof installed, ask the roofing contractor if they offer a maintenance package or maintenance services to keep the roof in shape.\nThe good news is because slate doesn’t warp, corrode, or attract mold, it doesn’t require as much maintenance like other roofing materials. Make sure you keep trees that overhang the roof trimmed to avoid any damage caused by falling branches and be aware of animals that may climb up on the roof and crack the tiles. Other than that, you should be fine.\nSlate is a gorgeous and durable roofing material that is sure to give your home value and curb appeal. If you’re willing to shell out the high up-front expense and take care of it, you will have a roof that will likely last your entire lifetime."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9dada518-f9a1-40ed-9094-9165229022b3>","<urn:uuid:4a7f4ee5-bd06-4f31-80cc-073061e12bc2>"],"error":null}
{"question":"Who is the statue in the front yard of the Mason farmhouse meant to represent?","answer":"The statue represents Amos Steele, a prominent Mason resident who died at the Battle of Gettysburg.","context":["Cool spaces: Century-old Mason farmhouse is haven for war memorabilia\nMASON - Five generations of Scott Shattuck's family have lived in his 131-year-old farmhouse.\nThe white house, with its wrap-around porch sits off West South Street on more than 12 acres. Shattuck's grandfather and father were born there. He grew up there, and raised his kids there.\nThe property is a family legacy, renovated carefully two decades ago to maintain as much of its past as possible.\nThe 3,000-square-foot home is steeped in local history.\nBut you have to look closer to see what else Shattuck has turned the property into – a haven for local military history.\nThe only clue from the outside of the home is a statue of Amos Steele, a prominent Mason resident who died at the Battle of Gettysburg. It sits in Shattuck's front yard, greeting cars as they pull in the home's driveway.\nFor more than 25 years Shattuck has been collecting war memorabilia and historical items that have deep ties to Ingham County.\nA trip out to the carriage house on his property won't turn up horses, straw or antique transportation.\n“I keep stories of our military alive out there,” Shattuck said.\nStarting a collection\nShattuck was a teenager when he found a box filled with his own family's Civil War legacy. It was tucked away in an upstairs closet on the second floor of the home.\nThe items inside it – including knives, artillery fuses, artwork, a spoon, a pair of spectacles and buckles and buttons from a war uniform – belonged to Shattuck’s great-great-great grandfather.\nShattuck never forgot about the collection and, years later, he asked his grandparents if he could have it.\nThose odds and ends became Shattuck’s first war memorabilia display.\nThey sat in his home’s front room when he moved into the family home in 1999. Today they are part of an extensive collection that spans more than 150 years of U.S. conflicts, and requires its own space.\nShattuck's 57. He's been collecting historical pieces of Ingham County's veteran history since he was 25.\nThe collection is broad, and most of it is housed inside the property's carriage house, built in 2009 to model one that might have been constructed a century ago.\nThere are antique windows from a Grand Army of the Republic hall, weapons and journals, letters and medals. Military uniforms from nearly every conflict in the country’s history are displayed on mannequins.\nPhotos, Civil War drums and other items are displayed on the walls, on shelves and in glass cases.\nSome of the items are more than 150 years old, and date back to the Civil War. Others belonged to local veterans who served in World War I and II, and the Vietnam War.\n“When you ask me how many pieces I have, I don’t really know,” Shattuck said on a recent tour of the space. \"It’s something I evolved and grew. What you pay attention to grows, and so I really kind of leaned into the stories of our past, and that just became more and more a mantra, something I was really interested in.”\nA caretaker of history\nOver the years Shattuck has connected with countless residents who have entrusted him with their own family’s war memorabilia.\n“You should never say no,” he said, of the historical items that have come his way.\nHe’s also returned items to family members who didn’t know they existed, when he finds them.\nCarl Woodard of Dansville is a friend and a Navy veteran who served during the Vietnam War.\nThe collection is \"100% in keeping\" with Shattuck's true passion for history, he said.\n\"Most of these things, if not all of the items, were things local men and women had,\" Woodard said. \"This is a personal collection. These were all Ingham County residents, and you just couldn't find a better caretaker.\"\nShattuck said he cares about every item he's collected, and often shares them when asked.\n“Vets come and visit,” Shattuck said. “The American Legion post tries to do an annual meeting here. Family members come to see their loved one’s items on display. It’s about keeping the story alive, preserving a piece of history, using it for the betterment of society.”\nSomeday Shattuck envisions establishing a public museum in downtown Mason, but until then he said he's simply watching over pieces of history.\n“All those things in the past create a mosaic that represents who we are today, the good and the bad,\" Shattuck said. \"I think the more we understand the past, the more we understand who we are, and I think that helps us move forward.”\nSEE MORE AT LSJ.COM:\nContact Rachel Greco at 517-528-2075 or at email@example.com. Follow her on Twitter @GrecoatLSJ."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5f61ab41-91f9-48af-97f8-e27d0b285eb7>"],"error":null}
{"question":"Working on disaster preparedness in informal settlements - what are the innovative early warning systems available for fire detection in these areas, and how does institutional funding support such safety initiatives in Africa?","answer":"For informal settlements, the Lumkani detector is a specialized $7 device that monitors temperature increases rather than smoke, creates a 60-meter radius mesh network for early warnings, and can send alerts to emergency responders. Regarding institutional support, major financial institutions like the World Bank and African Development Bank provide billions in annual funding to African countries, with significant portions devoted to rural community development and infrastructure, though the specific allocation for safety initiatives varies by country and project type.","context":["Millions of people around the world still lack consistent access to the basics of modern life. They also lack resources to build conventional infrastructure in order to obtain essentials such as water and a consistent supply of electricity.\nEnter frugal innovation—a process for simplifying complex technologies so they are less expensive to produce and operate. Two startups have devised affordable systems that give people access to essential utilities.\nWaterpoint Data Transmitter\nAbout 780 million people, mainly in rural locations, don’t have indoor plumbing. Instead, they rely on hand pumps to access groundwater. Sooner or later, these hand pumps break and often aren’t fixed due to lack of parts and know-how. By some estimates, one-third of pumps aren’t functioning at any given time.\nOxWater, a startup launched from Oxford University, has a solution that incorporates basic cell phone technology. The Waterpoint Data Transmitter is a monitoring device that communities deploy to track pump usage. If a pump stops working, a local, trained repair team receives a notification to fix it. The device also provides predictions of which pumps are likely to break and reports low water levels. A pilot project in Kenya showed a dramatic reduction in repair times, from an average of 37 days down to just two.\nSolar power has become an important technology for people living in off-the-grid rural environments. But once the sun goes down, or during spells of cloudy days, the solar panels may not generate enough electricity. That often means a return to inefficient and unsafe solutions, such as kerosene lamps for lighting.\nAzuri Technologies has developed a simple, independent system that enables solar users to adapt the amount of power they use according to the amount of energy they generate. The Quad is a small wall-mounted unit that’s wired to a solar panel that comes with a USB port for mobile phone charging. The system uses the company’s HomeSmart technology to monitor local weather patterns and learn consumers’ energy usage. Then, based on available energy, it automatically regulates the amount of power used for lighting (by, for example, adjusting brightness) and battery charging.\nA 5-watt system costs about US$156, which users can pay off weekly using a mobile money account. Once they own the unit, they can generate power at no cost. Since its launch in Kenya in 2011, 90,000 Quads have been purchased in 12 African countries.\nPreventing disasters and delivering aid when they do hit are difficult in isolated locations, where there aren’t enough services that enable quick reaction. Complexity and cost can also keep aid from reaching its targets. These startups are using frugal technology in imaginative ways to issue alerts of impending problems and deliver help to people in need.\nDisaster relief is an uphill race against the clock. Whether responding to a natural disaster, war, or famine, aid workers must assemble and deliver supplies, navigate around natural obstacles, avoid thieves, and stay safe. Windhorse Aerospace has developed POUNCER, a disposable drone, to address these problems.\nDesigned for takeoff from a C-130 Hercules military transport plane and guided using a built-in GPS, POUNCER can be launched from up to 40 kilometers from its destination, with a landing accuracy of within seven meters. The drone can carry enough food and water rations for 50 people. What’s more, every part is reusable and disposable. For example, the frame, which has a three-meter wingspan, can be used for shelter or burned for fuel (Windhorse is meanwhile looking to develop an edible frame). Because the entire unit is designed for on-site use, there’s also no cost or peril involved in recovering it from the disaster area.\nMany of the world’s poor live in shacks that are built very close together, and they lack electricity. As a result, they rely heavily on open flames for light, heat, and cooking, creating a high risk of fire. But conventional smoke detectors can’t be relied on in places that are already smoky. One devastating fire in Cape Town, South Africa, prompted a group of local university students to design a fire detection device specifically for these environments.\nThe Lumkani detector is a small wall-mounted unit that runs on batteries and, instead of being triggered by smoke, detects fires by monitoring temperature increases. The detectors use basic radio frequency technology to link all units within a 60-meter radius to a mesh network, which enables early warning alerts for the surrounding inhabitants. The $7 device also stores GPS coordinates, sends warning texts to residents, and can self-monitor the operating health of the whole linked system. Lumkani is working on a way to send real-time data to local emergency response units.\nData at the Digital Frontier\nDo you own the land you’re farming? When will the next rainstorm hit? These are basic questions, but for some people living in emerging economies, they’re not so easy to answer. Startups are using clever designs and simple interfaces to provide the information that rural communities need to thrive.\nFor millions of small landowners around the world, verifying a legal claim to their land is a complex, expensive, and practically insurmountable process. And without documentation that proves that they own their land, protecting their property rights is nearly impossible, as is getting loans to expand their land holdings and businesses.\nLandmapp, based in Amsterdam and operating in Ghana, has developed a mobile platform to make mapping and filing claims accessible to small landowners. The company educates farmers about property rights and then, for a small fee, uses its own platform to record and legally validate land ownership. Landmapp uses geospatial technology and cloud data on a tablet, meaning they don’t need fancy and expensive surveying equipment. FarmSeal, Landmapp’s first product, serves farmers; the company is also launching HomeSeal, for homeowners, and CropSeal, for sharecroppers and landowners. The startup’s platform incorporates local government, legal, and traditional community agreements, and is customizable for different locales.\n3D-Printed Weather Stations\nWeather data drives numerous economic and public safety decisions. But in many countries, a scarcity of weather stations means no data about vast geographic areas. Unfortunately, conventional weather stations are expensive, costing upwards of $20,000 per unit. In emerging economies, governments and rural communities don’t have the resources or training to buy and maintain them.\nAt the nonprofit university consortium University Corporation for Academic Research, researchers are leveraging 3D printing to fill the weather gap. They’ve devised a weather station that local government agencies can install in rural communities. The units use off-the-shelf, basic sensors, store data on a small computer, and run on energy generated by a single solar panel. The local agencies have 3D printers to create other parts, including the frame and wind gauges, which can be easily customized or replaced.\nThe cost? About $300. And beyond letting communities know when, for example, rain is on the horizon, the unit can also be a first alert for natural disasters, like floods.","Types of Research\nIn this dataset, we compile current project data from three major international financial institutions (or IFIs) - the World Bank, African Development Bank, and the International Fund for Agricultural Development - to understand\n- how much countries are borrowing from each institution. and\n- how much of that funding is devoted to small scale producer agriculture.\nWe begin by gathering publicly accessible data through downloads and webscraping Python and R scripts. These data are then imported into the statistical software program, Stata, for cleaning and export to Excel for analysis. This dataset contains rich information about current projects (active, in implementation, or recently approved), such as project title, project description, borrowing ministry, commitment amount, and sector. We then code relevant projects into two categories: On Farm (projects pertaining directly to small scale producer agriculture) and Rural/Agricultural Economies (inclusive of On Farm, but broader to include projects that impact community livelihoods and wellbeing). Finally, we annualize and aggregate these coded projects by IFI and then by country for analysis. Bilateral funding, government expenditures on agriculture, and development indicators are also included as supporting data to add context to a country's progress towards agricultural transformation.\nThe primary utility of this dataset is having all projects collected in a single spreadsheet where it is possible to search by key terms (e.g. commodity, market, financial, value chain) for lending by IFI and country, and to get some level of project detail. We have categorized projects by lending category (e.g. irrigation, livestock, agricultural development, research/extention/training) to aggregate across IFI so that the total funding for any country is easier to find. For example, Ethiopia and Nigeria receive the most total lending from these IFIs (though not on a per capita basis), with each country receiving more than $3 billion per year on average. Ethiopia receives the most lending devoted to On Farm projects, roughly $585 million per year. Overall, these data provide a snapshot of the magnitude and direction of these IFI's lending over the past several years to sub-Saharan Africa.\nFigone, K., Porton, A., Kiel, S., Hariri, B., Kaminsky, M., Alia, D., Anderson, C.L., and Trindade, F. (2021). Summary of Three International Financial Institution (IFI) Investments in Sub-Saharan Africa. EPAR Technical Report #411. Evans School of Public Policy & Governance, University of Washington. Retrieved <Day Month Year> from https://epar.evans.uw.edu/research/tracking-investment-landscape-summary-three-international-financial-institutions-ifis\nRecent research has used typologies to classify rural households into categories such as “subsistence” versus “commercialized” as a means of targeting agricultural development interventions and tracking agricultural transformation. Following an approach proposed by Alliance for a Green Revolution in Africa, we examine patterns in two agricultural transformation hallmarks – commercialization of farm output, and diversification into non-farm income – among rural households in Ethiopia, Nigeria, and Tanzania from 2008-2015. We classify households into five smallholder farm categories based on commercialization and non-farm income levels (Subsistence, Pre-commercial, Transitioning, Specialized Commercial, and Diversified Commercial farms), as well as two non-smallholder categories (Largeholder farms and Non-farm households). We then summarize the share of households in each of these categories, examine geographic and demographic factors associated with different categories, and explore households’ movement across categories over time. We find a large amount of “churn” across categories, with most households moving to a different (more or less commercialized, more or less diversified) category across survey years. We also find many non-farm households become smallholder farmers – and vice versa – over time. Finally, we show that in many cases increases in farm household commercialization or diversification rates actually reflect decreased total farm production, or decreased total income (i.e., declines in the denominators of the agricultural transformation metrics), suggesting a potential loss of rural household welfare even in the presence of “positive” trends in transformation indicators. Findings underscore challenges with using common macro-level indicators to target development efforts and track progress at the household level in rural agrarian communities.\nIn many countries in Sub-Saharan Africa and South Asia smallholder farmers are among the most vulnerable to climatic changes, and the observed shocks and stresses associated with these changes impact agricultural systems in many ways. This research brief offers findings on observed or measured changes in precipitation, temperature or both, on five biophysical pathways and systems including variable or changing growing seasons, extreme events, biotic stressors, plant species density, richness and range, impacts to streamflow, and impacts on crop yield. These findings are the result of a review of relevant documents cited in Kilroy (2015), references included in the IPCC draft Special Report on Food Security, and targeted searches from 2015 - present for South Asia and Sub-Saharan Africa.\nThis report reviews and summarizes the existing evidence on the impact of access to financial services/products on measures of production, income and wealth, consumption and food security, and resilience for smallholder farmers and other rural customers and their households in Sub-Saharan Africa. This study covers four main types of financial products/services: 1) credit; 2) savings; 3) insurance; 4) transactional products. We also review the very limited evidence on the effectiveness of bundling these products/services together and of combining them with other offerings such as trainings or support for access to markets, and of providing them via digital channels. We note when financial products/services have been specifically designed to serve the needs of rural customers or smallholder farmers, since the needs of these groups are often very different from those of other stakeholders.\nThis brief reviews the evidence of realized yield gains by smallholder farmers attributable to the use of high-quality seed and/or improved seed varieties. Our analysis suggests that in most cases, use of improved varieties and/or quality seed is associated with modest yield increases. In the sample of 395 trials reviewed, positive yield changes accompanied the use of improved variety or quality seed, on average, in 10 out of 12 crops, with rice and cassava as the two exceptions.\nCassava production is prone to many constraints throughout the production cycle, including biotic, abiotic, and management constraints. This brief reviews the literature on the production impacts of two key cassava stressors: cassava bacterial blight (CBB) and postharvest physiological deterioration (PPD). We summarize available estimates of the frequency and magnitude of these constraints relative to other drivers of cassava production losses that affect smallholder farmers in Sub-Saharan Africa (SSA), review the control strategies proposed in the literature, report on the views of several experts in the field, and identify research gaps where relatively little appears to be known about CBB or PPD yield impacts or best practices for CBB or PPD management.\nWater is a critical input for significantly enhancing smallholder farmer productivity in Sub-Saharan Africa (SSA) where less than 5% of farm land is irrigated, and in India where 42% of farm land is irrigated. For many years, donors have invested in human-powered treadle pump technologies as a point of entry for smallholder farmers unable to afford motorized pumps. In spite of some successes in treadle pump promotion, however, there is a widespread perception that as soon as smallholder farmers can afford to they quickly transition to motorized diesel- powered pumps. While diesel pumps substantially ease farmers’ workload, they pollute excessively (both in terms of local air quality and greenhouse gas emissions), pump excessive amounts of water, and put farmers at the mercy of cyclical spikes in fuel prices. This brief provides an overview of state-of-the-art alternative energy pumps, including technologies available and implementation lessons learned from China, India, Africa, South America and other regions. Through a literature review, written surveys and phone interviews with water pump producers and non-governmental organizations (NGOs) we evaluate the availability, affordability, and adoption rates of alternative energy technologies in developing countries. Our findings suggest that no single alternative energy water pumping system is a “silver bullet” for rural smallholder irrigation needs. Biofuels may prove a successful short- to intermediate-term solution for farmers who already have access to diesel pumps, but other problems associated with diesel engines, including high maintenance costs and excessive water use remain even when biofuels are used. Solar systems eliminate pollution almost entirely, reduce water consumption, and eliminate the need to purchase fuels. However solar systems are typically prohibitively expensive for smallholder farmers. Wind powered pumping solutions have not proven successful to date, with high costs and irregular wind patterns (either too little or too much wind) proving substantial barriers to widespread adoption.\nMarket-oriented agricultural production can be a mechanism to increase smallholder farmer welfare, rural market performance, and contribute to overall economic growth. Cash crop production can allow households to increase their income by producing output with higher returns to land and labor and using the income generated from sales to purchase goods for consumption. However, in the face of missing and underperforming markets, African smallholder households are often unable to produce efficiently or obtain staple foods reliably and cheaply. This literature review summarizes the available literature on the impact of smallholder participation in cash crop and export markets on household welfare and rural markets. The review focuses exclusively on evidence from Sub-Saharan Africa regarding top and emerging export crops, with the addition of tobacco and horticulture due to the volume of research relevant to smallholder welfare gains from the production of these crops. It includes theoretical frameworks, case studies, empirical evidence, and historical analysis from 42 primary empirical studies and 112 resources overall.\nIn recent years, product supply chains for agricultural goods have become increasingly globalized. As a result, greater numbers of smallholder farmers in South Asia (SA) and Sub-Saharan Africa (SSA) participate in global supply chains, many of them through contract farming (CF). CF is an arrangement between a farmer and a processing or marketing firm for the production and supply of agricultural products, often at predetermined prices. This literature review finds empirical evidence that demonstrates that the economic and social benefits of CF for smallholder farmers are mixed. A number of studies suggest that CF may improve farmer productivity, reduce production risk and transaction costs, and increase farmer incomes. However, critics caution that CF may undermine farmers’ relative bargaining power and increase health, environmental, and financial risk through exposure to monopsonistic markets, weak contract environments, and unfamiliar agricultural technologies. There is consensus across the literature that CF has the best outcomes for farmers when farmers have more bargaining power to negotiate the terms of the contract. In reviewing the literature on CF, we find a number of challenges to comparing studies and evaluating outcomes across contracts. This literature review summarizes empirical findings and analyses regarding contract models and best practices to increase farmers’ bargaining power and decrease contract default.\nEPAR’s Political Economy of Fertilizer Policy series provides a history of government intervention in the fertilizer markets of eight Sub-Saharan African countries: Côte d’Ivoire, Ghana, Kenya, Malawi, Mozambique, Nigeria, Senegal, and Tanzania. The briefs focus on details of present and past voucher programs, input subsidies, tariffs in the fertilizer sector, and the political context of these policies. The briefs illustrate these policies’ effect on key domestic crops and focus on the strengths and weaknesses of current market structure. Fertilizer policy in SSA has been extremely dynamic over the last fifty years, swinging from enormous levels of intervention in the 1960s and 70s to liberalization of markets of the 1980s and 1990s. More recently, intervention has become more moderate, focusing on “market smart” subsidies and support. This executive summary highlights key findings and common themes from the series."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fd7c0adc-2d64-47c3-b707-e4a2a55fc93f>","<urn:uuid:38688b95-8675-4e04-9d59-be9d09c96aff>"],"error":null}
{"question":"How do St. David's Cathedral and Prague's Sacred Heart Church demonstrate different approaches to addressing architectural problems over time?","answer":"These buildings show contrasting approaches to architectural challenges. St. David's Cathedral faced recurring destruction and reconstruction over centuries, with successive bishops adding improvements and extensions until Cromwell's devastation, followed by John Nash's restoration which quickly deteriorated, requiring George Gilbert Scott's later intervention. The Sacred Heart Church, on the other hand, demonstrates more modern challenges - its concrete and brick oculus has experienced ongoing issues due to thermal effects, concrete shrinkage, and brick expansion, leading to multiple unsuccessful repairs and current monitoring and modeling before attempting another fix. This shows the evolution from historical cycles of destruction and rebuilding to modern scientific approaches to preservation.","context":["Where do you begin when you have to write about a cathedral? Each chapel, every corner, every nook and cranny is replete with art and history. The sensory input is so much that all you can do is walk and attempt to catch the aura of the place and snippets of information as you marvel at how much of the past is preserved in wood, stone and colour. For me, outside is a good place to start. Apart from places like Lincoln where the outside is overwhelming as what lies within the walls. It gives a chance to get a feel for the place and seems to put what you will find into context.\nNot that we had a lot of choice… the mayor-making was taking its time and, until they were finished, we were left with the refectory and the exterior to play with. The refectory is a lovely space, with high, clear glass windows overlooking the cloisters. It now holds a cafe and an art exhibition was in progress while we were there, but instead of mangling the ancient architecture, a floating, self-contained mezzanine has been installed that barely touches the old walls. It does spoil the proportions of the lofty space… but it is a practical and not unattractive compromise that allows modern usage of an otherwise impractical height. It also serves tea and, while several of the party lingered over that welcome beverage, I wandered off to look at the walls.\nNot just the cathedral walls, but the remnants of the old town walls built by the Norman lords not long after the Conquest. The walled enclosure dates back to at least the 12th century, and the contours of the earthworks still remain visible. The tower that now houses the bells was added a century later and was once the consistory court of the bishops and also houses the bishops’ dungeon. The gatehouse and taller south tower came a hundred years after that.\nThe first religious community here was founded by St David himself. The saintly bishop died in 589, which makes this one of the oldest known religious sites of Christianity in Britain. Between 645 and 1097 the monks faced the incursions of raiders, seeing their brothers and bishops killed, one after the other. A beautifully carved stone that marries the older, Celtic art with the newer, Roman iconography is now housed in the gate-tower, that once marked the grave of the sons Bishop Abraham, Hedd and Isaac, who were killed by the Vikings in 1080.\nYou can see the antiquity of the place… but only at close quarters. The lower courses of the walls show their earliest origins in places. It has seen much destruction during the past thousand years. The cathedral building itself was begun in 1181. In 1220, the new tower collapsed and less than thirty years later, an earthquake caused further damage. Relics of St David and St Justinian were inhumed in jewelled shrines… until in 1538 Bishop Barlow, seeing their veneration as superstition, had the relics removed and the shrines stripped of their riches. For the next few hundred years, successive bishops added chapels, improvements and extensions, making the building reflect its importance at the heart of the religious and political life of the area…until the forces of Oliver Cromwell devastated the building during the years of the Commonwealth of England.\nIt was not until 1783 that John Nash, best known in his later role as architect to the Prince Regent, was commissioned to restore and rebuild the cathedral. He re-used medieval traceries for his great west front and the cathedral rose again. Even this was not the end of its troubles. Nash’s work was not up to standard and began to deteriorate rapidly. It was to be George Gilbert Scott who would finally restore the building in the 19th century; born in a vicarage not far from my home and whose work we have seen at so many places we have visited.\nThe great west front now looks back to an older age, wearing the same style as the Norman archways of the north and south doors. It is a curious thing to see the shapes and symbols copied. From one perspective, their uneroded state gives a glimpse of what the older stonework may have looked like…yet it is as if a child has tried to copy one of the Old Masters. The form is there… but no more than that. the soul is missing, the understanding of the symbolism not present….or, if present, it is an intellectual comprehension rather than the knowing of the heart. The carvings do not speak, they merely show.\nAfter the wonders of the carved doorway and corbels of Kilpeck just a few days before, the contrast is stark. Beautifully executed by their craftsmen, they lack heart. They tell no stories, inspire no affirmation. There is no life, no joy and no humour in the reproduction of these symbols., just the posturing of form that mimics a fluid expression of faith.\nThe 12th century south door and its northerly counterpart are completely different from the west front. In spite of centuries of weathering, they are alive with movement, detail and expression. The contrast between the old and the new makes me wonder about how we have chosen to live, pursuing the outward forms of integration and conformity, while leaving little place for joy. In the organised forms of religion, we now see mostly the rigidity of ritual. Through the years that we have been wandering ancient sites and visiting the holy places of our forefathers, we have seen how the representations of an inclusive reverence for nature and the life of both body and soul, have given way gradually to a culture of sin and the need for repentance, illustrated by many of the medieval wall paintings depicting hell and damnation in no uncertain terms and with graphic detail for the sinner to contemplate.\nI do not subscribe to the doctrine of eternal condemnation for sin. Of the saints whose stories we know form their own writings, few have entered into beatitude without struggling with their very human demons. We do not see the whole story of humanity, so cannot see how our little fragment of the tale fits into the whole, taking its unique place in the completion of a greater picture than we can perceive. We do not always see the good that may come from the seemingly bad, even if it serves only as a contrast by which we can appreciate the difference between light and shade, rigidity and movement. We make mistakes, take a wrong path, commit harm and it is from having made such choices that we can choose to learn and grow. We do repent. Not in the mundane sense of the word, where we admit guilt and say we are sorry, but in the true sense of turning ourselves around with a change of heart and mind. A change of consciousness. Like the cathedral, we live an uneasy peace, ever poised on the edge of change, but like the cathedral, the light shines within, opening the doors to a promise of beauty.","Editor’s Note: This unmodified edition of Marvelous Masonry originally ran in MASONRY Magazine. We hope you enjoy the historic information and specifics on architecture and construction provided by David Biggs as much as the readers of MASONRY did.\nFrom Egypt last month, we now move up to the Czech Republic in Central Europe. As with most European countries, it has an amazing history including being part of various empires. During World War II, it was part of Czechoslovakia. However, Czechoslovakia was peacefully dissolved in 1993 and the independent states of the Czech Republic and Slovakia were formed. In the past year, the country has adopted the short name of Czechia in addition to the formal name of Czech Republic.\nCzechia is one of Europe’s 16 land-locked countries (Figure 1). In size, it is slightly smaller than South Carolina but with more than double the population (10+ million). It is ranked 6thin the world as a peaceful country (the US is ranked 103).\nDespite its size, Czechia (Figure 2) has 12 UNESCO (United Nations Educational, Scientific and Cultural Organization) World Heritage sites and another 19 that are being considered for designation. A common thread throughout these sites is masonry! Let’s visit some of these.\nThe most famous city is the capital of Prague (#1 on the map). The city is a gem of Europe. There are numerous historic buildings throughout Prague which was mostly spared during the war except for one bombing in 1945 by Americans with malfunctioning radar who thought they were over Dresden, Germany. Film makers have used Prague (Amadeus, Mission Impossible, Les Miserables, and xXxto name a few) for its beauty, historic sites and low cost.\nSince 2010, I have been fortunate to be an invited lecturer at the Czech Technical University (CTU) in Prague as part of an international advanced masters course in Structural Analysis of Monuments and Historical Constructions. My primary topic is repair and strengthening of masonry structures. During this time, I have able fortunate to visit numerous historic sites and work with students on the evaluation and restoration of various projects.\nFigure 3 shows the St. Vitus Cathedral on the hill in Prague within the walled area of the Prague Castle. This UNESCO site has to be the highlight of any visit to Prague. The Gothic construction was begun in 1344 and wasn’t completed until 1929! There were wars and changes in government that caused the construction to stretch out nearly 700 years.\nFigure 4 shows a Google Earth view of the castle (white outline) with the cathedral. Figure 5 shows an enlargement of the cathedral. Anyone can take a virtual tour of the castle from the web site https://www.hrad.cz/en/prague-castle-for-visitors/virtual-tour.\nLike many of the cathedrals of Europe, St, Vitus is an amazing masonry structure with its flying buttresses and soaring towers. While Gothic, it has touches of other influences due to the length of construction. Figure 6 shows the south side of the cathedral which was constructed from the right to left. We see the original entrance in Gothic style from the 1300s (a), Renaissance tower from 1409 (b), and Baroque top rebuilt 1600s (c). There are also Romanesque elements that were introduced after a fire in 1541. The current western entrance and towers (d) were the last portion completed (1929).\nFigures 7 and 8 show interior views of the marvelous masonry construction of St. Vitus. The stone is primarily sandstone and the original stone joints from the 1300s are filled with lead, not mortar. There are no records as to why they used lead. Many believe the builders were concerned about high compressive loads from the stone weight. Imagine no repointing in over 700 years! The 20thcentury construction at the west end used lime mortar.\nFigure 7 is the view every tourist gets. Notice the iron bars tying the arches together. Throughout Europe you will see some roof arches with ties and many without. This building is a bit unusual in that it has both tie bars for the arches and flying buttresses on the exterior.\nFigure 8 is taken from an elevated part of the cathedral that is not open to tourists. I was fortunate to be given a VIP tour of the building arranged by my Prague colleagues through the office of the President of the Czech Republic. This was very special because the professors who went on the tour had lived in Prague their entire lives and never had this opportunity.\nThe historical city of Prague (Lesser Town) is on the same side of the river as the castle. Walking down the hill from the castle, you’ll find numerous stone buildings, churches and government offices. On the way, you pass one of the student work sites (Figure 4, red oval; Figure 9). This building was used as a case study to evaluate masonry, timber and material restoration.\nConstructed after a fire in 1541 that destroyed the previous house as well as damaging much of the castle, this building has had many uses but is most well known as a Theatine monastery. Despite the building’s age, written records were available that documented its major renovation campaigns. There is a pattern of significant work every 100-125 years; only possible with masonry buildings. Since 2012, it has been undergoing another restoration and renovation into a hotel.\nThe monastery was constructed with both sandstone and Opuka, a locally available marl. Opuka was plentiful and used throughout Prague. However, its low quality has created enormous restoration problems. Replacement stone is available in small quantities but often it is simply replaced with sandstone.\nFigure 10 shows one side of the building; the Opuka stone is covered with a rendering (plaster). Below several windows, cracks in the underlying stone have been stitched. Figure 11 shows the stitching before the rendering was completed. Helical rod was used. This procedure was used throughout the exterior. In the US, stitching cracks in brick walls is common, but it can be done in stone as well. While steel rods are normally used, a thin cable is often placed in the irregular joints of ashlar stone.\nAs the St. Vitus Cathedral was under construction, Charles IV began a building spree. He engaged the cathedral architect for several other projects including the Charles Bridge (Figure 13) crossing the Vltava River. St. Vitus Cathedral is visible in the background. The bridge opened up expansion of Prague to New Town (founded 1348) across the river from the castle.\nBegun in 1357, the bridge was originally called the Stone Bridge (constructed of sandstone) and was renamed the Charles Bridge in 1870. The bridge was preceded by a 21 arch span stone bridge that collapsed in 1347 (built 1158) due to flooding. Another UNESCO site, the Charles Bridge has 16 spans of between 50 to 70 feet. There are large stone towers at each end to symbolize Prague’s growing power and influence at the time they were built (Figure 14). In 1978, the bridge was converted to a pedestrian way and is always one of the busiest spots in Prague. The 30 Christian statues that adorn the bridge above the arch piers were added in the 1700s; they are now replicas.\nThe river has a long history of almost yearly flooding that has damaged the bridge several times, including destroying some arches. Each time the bridge was repaired or rebuilt. On March 4, 2017, 50,000 people were evacuated from the historical area of Prague as well as many other riverside cities.\nThe most recent restoration was begun in 2008. In 2010, UNESCO criticized the quality of the work stating “the restoration of Charles Bridge was carried out without adequate conservation advice on materials and techniques”. Critics felt that original features were being removed and replaced unnecessarily. So, even in Europe with a long history of preservation, there are disagreements over methodology just as we have in the US.\nMoving on into Old Town (founded 1348), we find more and more examples of marvelous masonry. Get an aerial view of this area and more of Prague at http://www.charming-prague-hotels.com/articles/prague-views-and-towers.\nIn 1348, New Town was first surrounded by a fortification wall and towers (Figure 15). Masonry was always the choice! Much of these brick and stone walls still exist.\nA main visitor attraction is the Old Town Square. Any direction you turn, you’ll be amazed at the masonry structures. Figure 16 is the sandstone north elevation of the Old Town Hall from 1364. It is one of the most famous tourist spots in Europe because of the astronomical clock and calendar clock on the east elevation (Figure 17); the masonry ornamentation is amazing as well. The clock was added in 1410. Tourists mob the area to see moving statues come out above the clock each hour\nLooking north from the clock tower, we see the Church of Our Lady before Týn (begun c.1365 and completed 1511). The sandstone Gothic church (Figure 18) has spires over 260 feet tall. Legend has it that this church gave Walt Disney the inspiration for the Sleeping Beauty Castle.It is surrounded by other buildings and finding the entrance can be a challenge.\nTo this day, the Tyn church along with most churches from its era, remain unheated. Thermal mass is very useful in summer to maintain a cool interior temperature, but a winter visit can be bitterly cold. Plan to sit near a portable infrared heater and still be cold.\nThe number of masonry sites in Prague you could visit is amazing, but let’s look at a couple newer examples. Figures 19 and 20 show the Sacred Heart Church (1929-1932). The unique brick construction is topped with a white cast stone. The building has been nominated for UNESCO monument recognition as part of the body of work by the architect, not as a singular structure.\nFigure 21 shows a close-up of the oculus with the Czech Republic’s largest church clock. For some reason the architect chose to construct the oculus with concrete and face it with brick. Thermal effects, concrete shrinkage and brick expansion have resulted in masonry cracking of the oculus. Several previous repairs have been unsuccessful; the oculus is being monitored and modeled before starting another repair.\nAnother more modern building is shown in Figure 22. It houses the Architecture Department at the Czech Technical University (built 2011). This concrete frame building has an exterior cavity wall of clay tile infill with brick veneer. I work a couple buildings away in the Civil Engineering building.\nThe reason for mentioning this building is highlighted in Figure 23. The veneer was constructed while I was back in the US. Seeing it for the first time in 2012, I was taken by the lack of movement joints. I had to almost get within touching distance of the building to notice there were movement joints but they were zipper shaped. These are rarely used in the US due to difficulty and cost. On this building they are expertly done. The sealant color match is perfect; only the thickened joint gives it away. I have been watching this building for five years now and there has been no splitting of the sealants.\nGoing back to historical masonry, let’s take a look at the Summer Palace of King Ferdinand II (Figure 24). This building was constructed in 1698 as a hunting retreat for the king in the Baroque style.\nIt became a restaurant in 1820s and remained one of the finest in Prague until 1968. At that time, the Communist government nationalized this and many other buildings in the country.\nIn the 1850s, the building was transformed into a Neo-Gothic structure with numerous additions and modifications (Figure 25). The main interior still maintains the baroque appearance. The loadbearing walls are constructed of Opuka. All the exterior walls are rendered with a lime plaster.\nThe front arcade was added using brick vaulting for the roof and the exterior spandrels (Figure 26). The columns of the arcade are constructed of Opuka and some brick. In the vaulting, a wrought iron tie rod was embedded into the brickwork at each rib to connect the exterior columns supporting the newer vaulting back to the existing building. Water penetration from the roof deck has corroded the tie rod and spalled the plaster rendering as visible in the photograph.\nA longitudinal crack runs along the length of the arcade (see dashed line). Analyses indicated this crack was based upon the original construction. Restoration plans include new waterproofing on the roof deck and repairs to iron rods.\nThe restaurant closed after being nationalized in 1968 and has fallen into disrepair. The grounds of the hunting retreat have been transformed into an English park. Since 2007, the city has been slowly trying to save and restore the building. Figure 27 shows the building surrounded in scaffolding covered with a wrap that depicts the appearance proposed after restoration. Many European cities use the wrap strategy to stimulate public interest in a restoration by showing what could be. In the US, you may remember the scaffolding of the Washington Monument was similarly wrapped during its restoration.\nOne major problem stalling restoration, other than funding, is flooding. Since 2002, the river has risen eight to 12 feet above the ground floor. Until Prague solves the flooding problem, city officials are hesitant to invest in restoration; so, most efforts have been to stabilize the building (see shoring on Figure 25) and protect the original ceiling murals inside. Prague consultants and CTU students have studied the materials extensively, modeled the masonry structure, proposed a new roof, and offered restoration plans.\nTime to leave Prague! Of the hundreds of historic masonry sites to visit, I’d like to highlight St. Barbara’s Church in Kutná Hora (#2 on the map). About an hour from Prague, the city had major silver mines and the mint for the country. I’ve been there at least four times and would go again tomorrow if I could. Plus, it is another UNESCO site.\nConstruction on the church (Figures 28 and 29) began in 1388. After a quick start, there was a time period of about 500 years where no work was done. For that time, the roof was not completed and it rained into the building. Imagine if you were asked to complete the construction of a building that had been sitting for 500 years and the roof was open. How much rework would you expect to be needed on the existing? Was there are protection on the walls? Ultimately, construction restarted in 1905 and was finally completed in 1908.\nImagine how many full generations of masons worked on this church and never saw it completed. Despite all that time, the church is only one-half the size that was originally intended. Figure 30 shows the floor plan.\nThe sandstone used to build the church was quarried locally. Despite being a land-locked country, the stone has seashells that are readily visible. The marvelous masonry of the main nave (Figure 31) is braced by amazing flying buttresses.\nEven though the church was begun in 1388, the buttresses are likely some of the last elements to be constructed making them just over 100 years old. From 2003 to 2011, the recent restoration needed to strengthen several buttresses and interior arches that had cracked and spread from thermal stresses and overload.\nA flying buttress is a delicate structural element. It has two major components, a) the diagonal element which is often a shallow arch and b) the vertical pier. As seen in Figure 32, the weight of the roof causes the arch to spread putting the diagonal in compression. That diagonal remains in compression provided there is sufficient mass of the vertical pier to resist it from overturning.\nFor the flying buttress restoration, the contractor had to repoint and stiffen the vertical pier, jack open the crack in the diagonal element and fit a new stone to reestablish the compression, and tie several interior arches with iron bars. Figure 33 shows one of the iron bars after installation. The workmanship is superb.\nAs I said before, there are countless more examples of marvelous masonry, but it’s time to end this month’s article. I thought the last features I would show are several gargoyles. They are part of the great masonry structures all over Europe. Often they are carved stone of grotesque figures. Figures 34 and 35 are two examples. We have them in the US also and you’ve likely seen some. Ever wonder what they look like from above?\nFigure 36 is one gargoyle viewed from above and we see a built-in copper gutter. This one also has modern heating wires to prevent ice buildup. Not all gargoyles have an open gutter, some have an enclosed pipe. These gargoyles are actually downspouts to divert roof runoff water away from the masonry walls that could deteriorate the mortar.\nWe started at St. Vitus Church at the Prague Castle and end back there with the gargoyles spitting out water on a rainy day (Figure 37).\nIf you get a chance to visit the Czech Republic and Prague, you’ll enjoy these sites and the many more examples of marvelous masonry."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bfe249a5-9a5c-49e3-8c38-32601c9fe924>","<urn:uuid:675b4b42-9c1b-4aab-9f0f-07022e50606e>"],"error":null}
{"question":"How does the air temperature in Cool, California compare to the surface water temperature in the Bering Sea during summer months?","answer":"The air temperature in Cool, California reaches its highest point in July with an average high of 93°F, while the surface water temperature recorded in the Bering Sea in early August was significantly colder at 47°F (8.3°C). This shows a dramatic temperature difference of about 46°F between these two locations during the summer season.","context":["NOAA Teacher at Sea\nAboard NOAA Ship Oscar Dyson\nJuly 23 – August 11, 2012\nMission: Alaskan Pollock Mid-water Acoustic Survey\nGeographical Area: Bering Sea\nDate: August 2, 2012\nLatitude: 61°12’61” N\nLongitude: 178°27’175″ W\nShip speed: 11.6 knots (13.3 mph)\nWeather Data from the Bridge\nWind Speed: 11 knots (12.7 mph)\nWind Direction: 193°\nWave Height: 2-4 ft (0.6 – 1.2 m)\nSurface Water Temperature: 8.3°C ( 47°F)\nAir Temperature: 8.5°C (47.3°F)\nBarometric Pressure: 999.98 millibars (0.99 atm)\nScience and Technology Log\nAt the end of last blog, I asked the question, “What do you do with all these fish data?”\nThe easy answer is… try and determine how many fish are in the sea. That way, you can establish sustainable fishing limits. But there is a little more to the story…\nHistorically, all fisheries data were based on length. It is a lot easier to measure the length of a fish than to accurately determine its weight on a ship at sea. To accurately measure weight on a ship, you have to have special scales that account for the changes in weight due to the up and down motion of the ship. Similar to riding a roller coaster, at the crest of a wave (or top of a hill on a roller coaster), the fish would appear to weigh less as it experiences less gravitational force. At the trough of a wave (or bottom of a hill on a roller coaster), the fish would experience more gravitational force and appear to weigh more. Motion compensating scales are a more recent invention, so, historically, it was easier to just measure lengths.\nOne of the motion-compensating scales onboard the Oscar Dyson.\nFor fisheries management purposes, however, you want to be able to determine the mass of each fish in your sample and inevitably the biomass of the entire fishery in order to decide on quotas to determine a sustainable fishing rate. So, you need to be able to use length data to estimate mass. Here is where science and math come to the rescue! By taking a random sample that is large enough to be statistically significant, and by using the actual length and weight data from that sample, you can create a model to represent the entire population. In doing so, you can use the model for estimating weights even if all you know is the lengths of the fish that you sample. Then you can extrapolate that data (using the analysis of your acoustic data – more on this later) to determine the entire size of the pollock biomass in the Bering Sea.\nHow do they do that? First, you analyze and plot the actual lengths vs. weights of your random sample and your result is a scatter-plot diagram that appears to be an exponential curve.\nScatterplot showing observed Walleye pollock weights and lengths for a sample of the population.\nThen you create a linear model by log-transforming the data. This gives you a straight line.\nLinear regression of the Walleye pollock length and weight data.\nNext, you back-transform the data into linear space (instead of log space) and you will have created a model for estimating weight of pollock if all you know are the lengths of the fish. This is close to a cubic expansion which makes sense because you are going from a one-dimensional measurement (length) to a 3-dimensional measurement (volume).\nObserved weight and length data showing the model for predicting weight if all you know are lengths.\nScientists can now use this line to predict weights from all of their fish samples and then extrapolate to determine the entire biomass of Walleye pollock population in the Bering Sea (when combined with acoustic data… coming up in the next blog!) when the majority of the data collected is only fish lengths.\nAnother interesting question… How does length change with age? Fish get bigger as they get older, all the way until they die, which is different from mammals and birds. However, some individual fish grow faster than others, so the relationship between age and length gets a little complicated. How do you determine the age distribution of an entire population when all you are collecting are lengths?\nSeveral age classes of Alaskan pollock (Theragra chalcogramma). Can you tell which one is youngest? Are you sure???\nJust like weight, you can determine the age from a subset of fish and apply your results to the rest. This works great with young fish that are one year old. The problem is… once you get beyond a one-year-old fish, using lengths alone to determine age becomes a little sketchy. Different fish may have had a better life than others (environmental/ecological effects) and had plenty to eat, great growing conditions, etc and be big for their age relative to the rest of the population. Some may have had less to eat and/or unfavorable conditions such as high parasite loads leading them to be smaller… There are also other things to consider such as genetics that affect length and growth rate of individuals. Here is where the collection of otoliths becomes important. By collecting the otoliths with the lengths, weights, and gender data, the scientists can look at the age distributions within the population. The graph below shows that if a pollock is 15 cm long, it is clearly a 1 year old fish. If a pollock is 30 cm long, it might be a 2 year old, a 3 year old, or a 4 year old fish, but about 90% of fish at this length will be 3 years old. If a fish is 55 cm long, it could be anywhere from 6 to 10+ years old!\nGraph showing age proportions of the Walleye pollock population when compared to length data.\nCollection of otoliths is the only way to accurately determine the age of the fish in the random sample and be able to extrapolate that data to determine the estimated age of all the pollock in the fishery. Here is a photo comparing otolith size of Walleye pollock with their lengths.\nA comparison of otolith sizes. These otoliths were taken from fish that were 12.5cm, 24.5cm, 30.5cm, 39.0cm, 55.5cm, and 70.0cm counter clockwise from top, respectively.\nIf we wanted to find out exactly how old each of these fish were, we would need to break the otoliths in half to look at a cross section. Below is what a prepared otolith looks like (courtesy of Alaska Fisheries Science Center). You can try counting rings yourself at their interactive otolith activity found here.\nCross section of Walleye pollock otolith after being prepared (courtesy of the Alaska Fisheries Science Center).\nAll of these data go into a much more complicated model (including the acoustic-trawl survey walleye pollock population estimates) to accurately estimate the total size of the fishery and set the quotas for the pollock fishing industry so that the fishery is maintained in a sustainable manner.\nNext blog, we will learn about how the various ways acoustic data fit into this equation to create the pollock fishery model!\nOk, so here is a long overdue look at the NOAA Ship Oscar Dyson that I am calling home for three weeks. I was pleasantly surprised when I saw my state room. It is bigger than I thought it would be and came with its own bathroom. I was also pleasantly surprised to learn I would be sharing my state room with Kresimir Williams, one of the NOAA scientists and an old college friend of mine! Here is a picture of our room.\nMy state room on the Oscar Dyson. The curtains around each bunk help block out light.\nThe room has a set of bunk beds. Thankfully, my bed is on the bottom. I do not know how I would have gotten in and out of bed in the rough seas we had over the last couple of days. If I do fall out of bed, at least I will not have far to fall. Last year, the ship rocked so hard in rough seas that one of the scientists fell head first out of the top bunk! The room also had two lockers that serve as closets, a desk and chair, and our immersion suits (the red gumby suits). The bathroom is small and the shower is tiny! Notice the handles on the wall. These are really handy when trying to shower in rough seas!\nThe bathroom in my state room. Notice the essential handles.\nNext, we have the Galley or Mess Hall. This is where we have all of our meals prepared by Tim and Adam. Notice that all of the chairs have tennis balls on the legs and that each chair has a bungee cord securing it to the floor! There are also bungee cords over the plates and bowls. Everything has to be secured for rough seas.\nThe Mess Hall, also known as “The Galley.”\nThe chairs in the galley have tennis balls on their feet and have bungee cords holding them down so they will not move during high seas.\nThe coffee bar and snack bar in the galley.\nThe Mess Hall also has a salad bar, cereal bar, sandwich fixings, soup, snacks like cookies, and ice cream available 24 hours a day. No one on board is going hungry. The food has been excellent! We have had steaks, ribs, hamburgers and fish that Tim has grilled right out on deck. Here is a picture of my “surf and turf” with a double-baked potato.\n“Surf and Turf” meal, courtesy of Stewards Tim and Adam. Yummy!\nMost of my work here on board (other than processing fish) has been in the acoustics lab, also known as “The Cave” since it has no windows. This is where the NOAA scientists are collecting acoustic data on the schools of fish and comparing the acoustic data with the biological samples we process in the fish lab.\nThe acoustics lab, also known as “The Cave” since it has no windows.\nI also spend some time up on the Bridge. From the Bridge, you can see 10 to 12+ nautical miles on a clear day. This morning, we saw a couple of humpback whales blowing (surfacing to breathe) about 1/4 mile off our starboard side! A couple of days ago (before the weather turned foul), we spotted an American trawler.\nAn American Trawler spotted in some foggy weather.\nToday, we got close enough to see the Russian coastline! Here is a picture of a small tanker ship with the Russian coastline in the background!\nLand Ho! A small tanker off the Russian coastline.\nHere are some pictures of the helm and some of the technology we have onboard to help navigate the ship.\nThe “helm” of the Oscar Dyson.\nRadar showing numerous Russian fishing vessels near the Russia coastline.\nI have also spent some time in the lounge. This is where you can go to watch movies, play darts (yea, right! on a ship in rough weather???), or just relax. The couch and chairs are so very comfy!\nThe Lounge aboard the Oscar Dyson.\nWhen you have 30 people on board and in close quarters, you better have a place to do laundry! Here is a picture of our very own laundromat.\nThe onboard laundry facilities.\nAll for now. Next time, I will share more about life at sea!","Average Weather in Cool California, United States\nIn Cool, the summers are hot, arid, and mostly clear and the winters are long, cold, wet, and partly cloudy. Over the course of the year, the temperature typically varies from 39°F to 93°F and is rarely below 33°F or above 101°F.\nThe hot season lasts for 3.2 months, from June 14 to September 20, with an average daily high temperature above 85°F. The hottest day of the year is July 28, with an average high of 93°F and low of 67°F.\nThe cool season lasts for 3.8 months, from November 16 to March 10, with an average daily high temperature below 62°F. The coldest day of the year is December 30, with an average low of 39°F and high of 54°F.\nAverage High and Low Temperature\nThe figure below shows you a compact characterization of the entire year of hourly average temperatures. The horizontal axis is the day of the year, the vertical axis is the hour of the day, and the color is the average temperature for that hour and day.\nAverage Hourly Temperature\nIn Cool, the average percentage of the sky covered by clouds experiences significant seasonal variation over the course of the year.\nThe clearer part of the year in Cool begins around May 21 and lasts for 5.1 months, ending around October 23. On July 21, the clearest day of the year, the sky is clear, mostly clear, or partly cloudy 90% of the time, and overcast or mostly cloudy 10% of the time.\nThe cloudier part of the year begins around October 23 and lasts for 6.9 months, ending around May 21. On February 21, the cloudiest day of the year, the sky is overcast or mostly cloudy 59% of the time, and clear, mostly clear, or partly cloudy 41% of the time.\nA wet day is one with at least 0.04 inches of liquid or liquid-equivalent precipitation. The chance of wet days in Cool varies significantly throughout the year.\nThe wetter season lasts 5.6 months, from October 29 to April 16, with a greater than 17% chance of a given day being a wet day. The chance of a wet day peaks at 34% on February 20.\nThe drier season lasts 6.4 months, from April 16 to October 29. The smallest chance of a wet day is 0% on July 19.\nAmong wet days, we distinguish between those that experience rain alone, snow alone, or a mixture of the two. Based on this categorization, the most common form of precipitation throughout the year is rain alone, with a peak probability of 34% on February 20.\nDaily Chance of Precipitation\nTo show variation within the months and not just the monthly totals, we show the rainfall accumulated over a sliding 31-day period centered around each day of the year. Cool experiences extreme seasonal variation in monthly rainfall.\nThe rainy period of the year lasts for 8.5 months, from September 22 to June 6, with a sliding 31-day rainfall of at least 0.5 inches. The most rain falls during the 31 days centered around February 17, with an average total accumulation of 5.5 inches.\nThe rainless period of the year lasts for 3.5 months, from June 6 to September 22. The least rain falls around July 30, with an average total accumulation of 0.0 inches.\nAverage Monthly Rainfall\nThe length of the day in Cool varies significantly over the course of the year. In 2017, the shortest day is December 21, with 9 hours, 26 minutes of daylight; the longest day is June 21, with 14 hours, 54 minutes of daylight.\nHours of Daylight and Twilight\nThe earliest sunrise is at 5:38 AM on June 14, and the latest sunrise is 1 hour, 57 minutes later at 7:34 AM on November 4. The earliest sunset is at 4:41 PM on December 7, and the latest sunset is 3 hours, 52 minutes later at 8:33 PM on June 27.\nDaylight saving time (DST) is observed in Cool during 2017, starting in the spring on March 12, lasting 7.8 months, and ending in the fall on November 5.\nSunrise & Sunset with Twilight and Daylight Saving Time\nThe perceived humidity level in Cool, as measured by the percentage of time in which the humidity comfort level is muggy, oppressive, or miserable, does not vary significantly over the course of the year, remaining a virtually constant 0% throughout.\nHumidity Comfort Levels\nAverage Wind Speed\nThe predominant average hourly wind direction in Cool varies throughout the year.\nThe wind is most often from the south for 8.4 months, from February 3 to October 16 and for 1.0 months, from November 1 to December 1, with a peak percentage of 56% on August 12. The wind is most often from the east for 2.3 weeks, from October 16 to November 1 and for 2.1 months, from December 1 to February 3, with a peak percentage of 36% on October 29.\nThe average daily incident shortwave solar energy experiences extreme seasonal variation over the course of the year.\nThe brighter period of the year lasts for 3.5 months, from May 10 to August 24, with an average daily incident shortwave energy per square meter above 7.2 kWh. The brightest day of the year is July 1, with an average of 8.5 kWh.\nThe darker period of the year lasts for 3.6 months, from November 2 to February 20, with an average daily incident shortwave energy per square meter below 3.4 kWh. The darkest day of the year is December 20, with an average of 2.0 kWh.\nAverage Daily Incident Shortwave Solar Energy\nFor the purposes of this report, the geographical coordinates of Cool are 38.887 deg latitude, -121.015 deg longitude, and 1,381 ft elevation.\nThe topography within 2 miles of Cool contains very significant variations in elevation, with a maximum elevation change of 1,257 feet and an average elevation above sea level of 1,431 feet. Within 10 miles contains very significant variations in elevation (2,464 feet). Within 50 miles contains large variations in elevation (9,915 feet).\nThe area within 2 miles of Cool is covered by grassland (53%), shrubs (26%), and trees (20%), within 10 miles by shrubs (39%) and grassland (30%), and within 50 miles by trees (35%) and cropland (21%).\nThis report illustrates the typical weather in Cool, based on a statistical analysis of historical hourly weather reports and model reconstructions from January 1, 1980 to December 31, 2016.\nTemperature and Dew Point\nThere are 3 weather stations near enough to contribute to our estimation of the temperature and dew point in Cool.\nFor each station, the records are corrected for the elevation difference between that station and Cool according to the International Standard Atmosphere , and by the relative change present in the MERRA-2 satellite-era reanalysis between the two locations.\nThe estimated value at Cool is computed as the weighted average of the individual contributions from each station, with weights proportional to the inverse of the distance between Cool and a given station.\nThe stations contributing to this reconstruction are: Auburn Municipal Airport (85%, 10 kilometers, northwest); Placerville Airport (9%, 30 kilometers, southeast); and Sacramento Mather Airport (6%, 43 kilometers, southwest).\nTime zones for aiports and weather stations are provided by AskGeo.com ."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a6b6dbd5-3118-4965-8b34-0e759543fbce>","<urn:uuid:3b0d8333-9c9a-489c-ad21-c7f85abdbebc>"],"error":null}
{"question":"Comparing technological integration in American classrooms with Germany's higher education modernization efforts since the 1990s, how do their approaches to educational innovation differ?","answer":"The approaches show distinct differences in focus and implementation. In American classrooms, technological integration has been limited, with tools like interactive whiteboards and computers being grafted onto the existing factory-model classroom structure without fundamentally changing the educational model. Despite significant investment, there is little research showing meaningful impact on student learning. In contrast, Germany's higher education system has undergone comprehensive reorganization since the early 1990s, focusing on systemic changes by introducing internationally comparable Bachelor, Master, and Ph.D. programs, granting institutions more organizational freedom, and encouraging competition and individual institutional profiles. While America struggles to integrate technology within traditional classroom constraints, Germany's approach has involved restructuring the entire higher education system to align with international standards and enhance efficiency.","context":["A technology and education entrepreneur gazes into the future of the classroom\nMore than 150 years ago, Massachusetts became the first state to provide all of its citizens access to a free public education. Over the next 66 years, every other state made the same guarantee. The result was a publicly-funded system where, in every American classroom, groups of about 28 students of roughly the same age are taught by one teacher, usually in an 800 square-foot room. This model has been the dominant archetype ever since.\nIt's a factory-model classroom. Inspired in part by the approach Horace Mann saw in Prussia in 1843, it seemed to adequately prepare American youth for the 20th century industrialized economy. But in 1983, the federal government declared in A Nation At Risk that our system was starting to slide.\nThe year 1983 was also seminal for the technology industry. Microsoft released MS Word and Apple introduced the new Apple IIe. Some predicted that the demand for better schools, coupled with the supply of computers and new software, would soon revolutionize our nation's classrooms.\nIt didn't quite happen.\nSchools did move to adopt new technologies -- computers and software, increased bandwidth, and infrastructure. But there is scant research-based evidence that these tools have had the exponential impact on public education many anticipated.\nGiven the enormous impact that technology has had on nearly every other aspect of our society, how can that be?\nWITH LOVE FROM PRUSSIA\nPerhaps it is because educational tools that have come into our classrooms over the last couple of decades, whether technology or otherwise, continue to be used within a school structure that is virtually unchanged since the mid-nineteenth century.\nThat model was imported from Prussia with a different purpose in mind. Horace Mann's free school movement stemmed less from a belief in the economic or moral imperative of education for all children and more from a desire to simply create a tolerant, civilized society.\nMann grew up in Massachusetts during the early part of the 19th century, where religious tension between Protestants and Catholics dominated public life. Parochial schools, in his view, only reinforced these divisions. The Prussian model, on the other hand, was designed to build a common sense of national identity.\nApplied back home, Mann thought, large groups of students learning together would help to blur the divisions among religious groups and establish a more unified and egalitarian society. And as that model became the American blueprint, Mann's vision ultimately became the foundation for our national system of schooling.\nMann's vision also made sense for the industrial age in which he lived. The factory line was simply the most efficient way to scale production in general, and the analog factory-model classroom was the most sensible way to rapidly scale a system of schools. Factories weren't designed to support personalization. Neither were schools.\nTOOLS AREN'T ENOUGH\nToday our collective vision for education is broader, our nation is more complex and diverse, and our technical capabilities are more powerful. But we continue to assume the factory-model classroom and its rigid bell schedules, credit requirements, age-based grade levels, and physical specifications when we talk about school reform.\nThat's why the promise of educational innovation is less about processing power and software code and more about the opportunity to release ourselves from general assumptions regarding how instruction is organized and delivered. It's why our collective charge in K-12 innovation today should go beyond merely designing and producing new tools. Rather, our focus should primarily be to design new classroom models that take advantage of what these tools can do.\nAbsent new models, many of our technological capabilities (which can now support both scale and personalization) are either inaccessible or clumsily grafted on. Three computers added to the back of a classroom may look like a positive step toward bringing that classroom into the advanced technological age. However, smoothly integrating three computers into a daily lesson is not always easy when a teacher has to consider the needs of 28 students all learning at the same time. Software programs that enable students to learn at their own pace can be powerful, particularly for students who are at an academic level far above or below the rest of the class. But this type of software is often not readily compatible with a teacher's need to cover a grade-level scope-and-sequence for all students.\nAPP FOR TEACHER\nOf course, some new technology tools have been useful in the classroom. There are many schools where interactive whiteboards have replaced chalkboards, computers support research in libraries, and electronic grade-books have supplanted spiral notebooks. These are the kinds of tools that can be readily integrated into a traditional classroom environment. But different teachers use these kinds of tools in different ways and their use does not facilitate a pivot from the rigidity of the factory model classroom. As a result, there is little research to show that investment in these kinds of tools has a meaningful impact on student learning.\nNew classroom delivery models allow us to re-imagine new combinations of educator expertise, time, instructional materials, research, physical space, parental support, and (yes) technology in ways that achieve optimal outcomes for students. They begin not by assuming the current model but rather by understanding what it is we want students to be able to do, the measures of success, the resources we have to work with, and our own sense of possibility.\nDifferent schools may take different approaches to combining these components, depending on their educational philosophies, available teaching resources and student needs. For example, some might offer science through a combination of in-class activities, collaborative lab periods in the evening, and online coaches who work in a scientific industry. Others might teach a foreign language through the combination of in-class dialogue, web-based software, and online activities with students in other countries. Still others, like New Classrooms, use a combination of teacher-led instruction, student collaborative activities, software, virtual instructors, and a complex scheduling algorithm to enable each student to move through an individualized learning progression at his or her own pace.\nImportantly, model providers also do not need to be directly managing the school. While some providers (e.g. Charter Management Organizations) may chose to both design new models and directly manage schools, others providers may design models to work within existing schools and with faculty who remain on the district's payroll.\nBut in either case, model providers would begin to share in the accountability for student outcomes at the school level. State or districts that currently adopt textbooks would instead certify a number of model providers who would then pair off with schools (on a mutual selection basis) to support the implementation and customization of their model in a particular subject area. Over time, as models begin to mature, states and districts would be able to analyze the academic impact of the model providers, rewarding those that are most successful and decertifying those that are not.\nThe Information Age has facilitated a reinvention of nearly every industry except for education. It's time to unhinge ourselves from many of the assumptions that undergird how we deliver instruction and begin to design new models that are better able to leverage talent, time, and technology to best meet the unique needs of each student. In doing so, we can put Mann's innovation in its proper context: as the foundation for our commitment to a public education but not as the blueprint for how to deliver it.","WHY STUDY IN GERMANY\nEducation system in Germany\nFirst and Secondary Education\nCompulsory education in Germany is from the age of 6 to 15 years. School children are in primary school (Grundschule) for four years in most of the federal states, apart from Berlin and Brandenburg where primary school finishes after grade 6. There are different types of secondary schools, starting with grade 5 or 7 and finishing with grade 10 or 12 with different school leaving certificates.\nThe Hauptschule (lower secondary school) is till grade 9 and with the certificate for taking a vocational apprenticeship. Upper secondary school (Realschule) completes with grade 10, students can then chose to continue with the Oberschule and take the Abitur (A-Level/baccalaureate) or they start a vocational apprenticeship. At a Gymnasium, students finish after grade 12 (until recently, it used to be after grade 13) with their Abitur which is the entry qualification for higher academic education at universities or colleges. The Fachabitur is a special version of the Abitur, taking more practical subjects and already qualifying oneself in a practical field, for example social studies. Students with a Fachabitur can study at colleges and polytechnics, but not at universities.\nThere is also a mixed version, the Gesamtschule (secondary state school) which includes all three school types, depending on the development of the student's qualifications. So, a student with improving grades at the end of a school year can change from Hauptschule to Realschule or to Gymnasium. The education plan can vary between the Ministries of Education of each federal state, but generally, secondary education is compulsory and free.\nThere are also different types of higher education institutions divided into: universities (Universitäten, Technische Hochschulen/Technische Universitäten, Pädagogische Hochschulen), and colleges of art and music (Kunsthochschulen and Musikhochschulen), and Fachhochschulen (colleges of applied sciences). All these institutions are undergoing a reorganisation since the early 1990s. With the introduction of the internationally comparable Bachelor, Master and Ph.D. programmes, the qualification of a new generation of academics and scientific study is the focus of future development.\nMoreover, the institutions of higher education are made more efficient by granting them further freedom in organisational matters and the chance to shape an individual profile and evoking more competition. As the primary and secondary education system, the higher education system is also subject to decisions and programmes of the governments of the federal states.\nFachhochschulen (colleges of applied sciences), offer a range of practically oriented study courses such as Engineering, Economics, Social Work, Public and Legal Administration and Health and Therapy. A Diplom degree can be achieved after 8 semesters of studying and a finally successful examination. Students then get the title of Diplom-Ingenieur (FH). The initials \"FH\" are added to the Diplom degrees pointing to the diploma of a Fachhochschule.\nUniversities in Germany\nUndergraduate studies were until recently the basic studies (Grundstudium) of a Diplom or Magister programme, generally taking four semesters (2 academic years) and finishing with an intermediate examination (Diplom-Vorprüfung, Zwischenprüfung). Students are then enabled to follow their studies in the second stage of Hauptstudium, taking another 4 semesters with the 5th being the preparatory semester for taking the final exams, the Diplomprüfung or the Magisterprüfung or State Exam (for Law and subjects for becoming a teacher).\nThe new graduation system of the Bachelor as an undergraduate program instead of the basic studies program has already been introduced in Germany with the aim of achieving an internationally competitive degree and studying in a condensed, shortened time of 3 years.\nThe advanced studies (Hauptstudium) form the second stage to the final examination, takes five semesters at least. The final exams still are the Diplom and Magister, but they are slowly replaced by the Master degree. The Magister study involves either two equally weighed major subjects or a mixture of one major and two minor subjects. According to the new graduation system, after having completed the Bachelor's studies, a Master of Arts/Science is the successfully achieved title after two years of studying.\nA Doctoral degree can only be achieved at universities. The time of doctoral studies, the Promotion, has duration of 2 to 4 years of independent scientific research, the public presentation and defence of the thesis. The Diplom/Erstes Staatsexamen/Magister Artium/Master of Arts/Science are the preconditions for taking Doctoral studies.\nThe closing dates for applications to the International offices are usually July 15 for the following winter semester and January 15 for the following summer semester. If your application is late even by one day, it is not going to be processed.\nAs soon as you have received your notification of admission and passed the language test, you need to register at your chosen university or college. Please direct your queries to the Registrar's office (studentensekretariat) in good time so as to wrap up the formalities and paperwork long before the session is due to start.\nYour first port of call upon arrival in Germany must necessarily be the International office which will provide you with all the basic information you need to arrange the initial days of your stay in that country. Subsequently you must get yourself registered at the Resident Registration office and finally with the Alien'sRegistration Authority.undefined\nWe investigate the relationship between remittances and migrants' education both theoretically and empirically, using original bilateral remittance data. At a theoretical level we lay out a model of remittances interacting migrants' human capital with two dimensions of immigration policy: restrictiveness, and selectivity. The model predicts that the relationship between remittances and migrants' education is ambiguous and depends on the immigration policy conducted at destination. The effect of education is more likely to be positive when the immigration policy is more restrictive and less skill-selective. These predictions are then tested empirically using bilateral remittance and migration data and proxy measures for the restrictiveness and selectivity of immigration policies at destination. The results strongly support the theoretical analysis, suggesting that immigration policies determine the sign and magnitude of the relationship between remittances and migrants' education."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1efad6f4-df32-4a91-93af-b6bc5e7ad709>","<urn:uuid:d65c5c70-6d94-48a8-814b-93d133c166d6>"],"error":null}
{"question":"What materials are used for providing stretchability in both the anterior trunk support vest and the direct-coated sponge abrasive, and how do they differ in their stretching properties?","answer":"The anterior trunk support vest uses elastomeric foams like chloroprene and EPDM, or spandex/stretchable mesh fabric specifically in the shoulder straps area, with the lower portion being either non-stretchable or stretchable only in the width dimension. In contrast, the direct-coated sponge abrasive uses open-cell polyester-urethane foam with a density of 50-100 kg/m3 as its substrate, providing overall resilience and flexibility rather than directional stretch. The vest's design intentionally incorporates different materials for controlled movement, while the sponge abrasive's foam provides uniform resilience throughout.","context":["|20070074991||Ear plug package and method of manufacture||April, 2007||Heisserer|\n|20040007235||Intake valve||January, 2004||Rafoss|\n|20100071694||Nitrous Oxide Anesthetic Administration System||March, 2010||Ahearn|\n|20090205653||Breathing System||August, 2009||Wisniewski|\n|20040221841||Breathing apparatus for reducing fogging of lenses caused by breath vapor||November, 2004||Herschel|\n|20100059049||Dry-Powder Inhaler||March, 2010||Genosar|\n|20060254581||Dose counting in metered dose inhaler||November, 2006||Genova et al.|\n|20040231678||Adjustable autofixing sling for treatment of urinary incontinence||November, 2004||Fierro|\n|20090205668||Condom with anti slippage feature||August, 2009||Morissette|\n|20070215157||Rebreather Setpoint Controller and Display||September, 2007||Straw|\n|20030024536||Anatomical device||February, 2003||Bagby|\nThis application claims the benefit of U.S. Provisional Application No. 60/579,471, filed Jun. 14, 2004, entitled “Anterior Trunk Support or Harness,” and incorporated herein by reference.\nThe present invention relates to body harnesses and supports, and more particularly an anterior trunk support or harness.\nIn general, an anterior trunk support is a device that provides upper trunk control or support. Most prior art anterior trunk supports utilize a belt, strap, panel, or harness. Each of these is formed of a flexible material and is designed to comfortably fit around a user and to hold that user in an upright position, for example connected to a wheelchair.\nAlthough prior anterior trunk supports work well for their intended purpose, often they rigidly hold the user in position, allowing little to no movement from the upright position. Although this function is desired in some applications, some users may have upper body movement, and may prefer limited support instead of restrictive control.\nConversely, some anterior trunk supports are formed entirely of a soft, elastic material such as neoprene. Such designs allow a great deal of movement but sacrifice the support needed by users with diminished muscle tone or spastic movement. Furthermore, because the lower portion of these entirely elastic anterior supports is able to stretch, the device rides up when the user leans forward, rubbing against the user's neck and creating a choking hazard.\nThe following presents a simplified summary of some embodiments of the invention in order to provide a basic understanding of the invention. This summary is not an extensive overview of the invention. It is not intended to identify key/critical elements of the invention or to delineate the scope of the invention. Its sole purpose is to present some embodiments of the invention in a simplified form as a prelude to the more detailed description that is presented later.\nIn accordance with an embodiment, an anterior trunk support is provided. The anterior trunk support is shaped like a vest and is formed of multiple materials, for example stretch and non-stretch or limited stretch materials.\nIn accordance with an embodiment, non-stretch or limited stretch material extends over the upper belly and lower ribs of a user, and stretch material extends from the non-stretch material to over the shoulders of the user. In this manner, a wearer may lean slightly forward, with the stretchable shoulder portion flexibly permitting such movement. The stretchable material stretches with the user's shoulders, preventing the lower, non-stretch or limited stretch material from riding up the user's chest, and providing maximum comfort. Thus, unlike some prior art anterior trunk supports, a user is not rigidly locked in place. And unlike other prior art anterior trunk supports, the user is stabilized, but is still provided movement.\nIn accordance with an embodiment, the lower section of the anterior trunk support may be stretchable in a width dimension only. As such, the lower section provides movement for user's respiration, thus providing additional comfort. By not stretching in the vertical direction, the anterior trunk support does not slide up the wearer's chest when tension is applied to the upper stretchable material. Thus, support is maintained, and the lower section does not approach the neck of the user.\nThe anterior trunk support may be used as a support (i.e., to position or hold an individual). Alternatively, the anterior trunk support may be used as a load-bearing harness, for attaching an item such as a backpack to a user.\nOther features of the invention will become apparent from the following detailed description when taken in conjunction with the drawings, in which:\nFIG. 1 is a side perspective view of an anterior trunk support vest in accordance with an embodiment of the invention, with the anterior trunk support vest shown on a user;\nFIG. 2 is a side view of the anterior trunk support and user of FIG. 1, with the user leaning slightly forward;\nFIG. 3 is a front view of the anterior trunk support of FIG. 1, with the anterior trunk support extended flat;\nFIG. 4 is a side perspective view of a swivel buckle that may be used with the anterior trunk support of FIG. 1, with the swivel buckle shown detached; and\nFIG. 5 is a top view of the swivel buckle of FIG. 4, with the swivel buckle shown attached.\nIn the following description, various embodiments of the present invention will be described. For purposes of explanation, specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However, it will also be apparent to one skilled in the art that the present invention may be practiced without the specific details. Furthermore, well-known features may be omitted or simplified in order not to obscure the embodiment being described.\nReferring now to the drawings, in which like reference numerals represent like parts throughout the several views, FIG. 1 shows an anterior trunk support vest 20 in accordance with an embodiment of the invention. In the drawings, the anterior trunk support vest 20 is worn by a user U sitting a wheelchair 22. However, the anterior trunk support vest 20 may be used for a number of other uses, for example for supporting a user against another device, or as a harness for attachment to a backpack or other item.\nAs can be seen in FIG. 3, the anterior trunk support vest 20 includes shoulder straps 24 that are attached to top straps 26. The shoulder straps 24 extend downward to a convergence zone 30 where the shoulder straps 24 are attached to a lower portion 32 of the anterior trunk support vest 20. The convergence zone 30 is the area at which or line along which the lower portion 32 attaches to the shoulder straps 24.\nThe lower portion 32 is adapted and configured to fit around a trunk of a user U, and more specifically an upper portion of the belly of the user U up to and covering a lower portion of the ribcage of the user. The lower portion 32 includes two extensions 34, 36 that extend outward to sides of the user U. Swivel buckles 38 are attached by loops 50 at the ends of these extensions 34, 36. Lower straps 40 are attached to opposite sides of the swivel buckles 38 from the loops 50.\nThe top straps 26 are attached to a structure, for example to a top of the back of the wheelchair 22. The lower straps 40 are also attached to a structure, for example to a bottom of the back of the wheelchair 22, or to another suitable structure.\nIn the embodiment shown, the shoulder straps 24 are located on opposite sides of a zipper 42. The zipper 42 also divides the lower portion 32. The zipper 42 provides an easy way for a user, such as the user U, to install and remove the anterior trunk support vest 20.\nIn accordance with an embodiment, the shoulder straps 24 are formed of a stretchable material, such as elastomeric foams, also known as foamed rubber. Examples are chloroprene and Ethylene Propylene Diene Monomer (EPDM). Other stretchable materials may be used, including, but not limited to, spandex fabric and/or stretchable mesh fabric.\nIn accordance with an embodiment, the lower portion 32 is formed of a different material than the shoulder straps 24, and may, as an example, be formed of a non-stretchable material, such as a mesh material. In accordance with an alternate embodiment, the lower portion 32 is formed of a material that stretches in one direction indicated by the arrow 44 in FIG. 3: generally in the width dimension of a user.\nThe shoulder straps 24 and the lower portion 32 may be padded or insulated as desired to provide comfort and/or warmth for a wearer. However, such padding or insulation preferably would not hinder the stretchability of the shoulder straps 24 (and the lower portion 32, if stretching is provided).\nIn use, the anterior trunk support vest 20 is installed on a user, such as the user U, and provides support for the user U. The shoulder straps 24 fit on opposite sides of the users neck, and the lower portion 32 is positioned on the user's chest. The shoulder straps 24 and the lower portion 32 are adjusted, for example by cinching the top straps 26 and the lower straps 40. Alternatively, one of the lower straps 40 and top straps 26 may be fixed and the other cinched to adjust the anterior trunk support vest 20 against the user U. An end fitting buckle (not shown) may be used for cinching of each of the lower straps 40 and/or the top straps 26. An example of such an end fitting buckle is shown in U.S. Pat. No. 6,665,913, owned by the assignee of the present invention.\nThe anterior trunk support vest 20 fits against the user U to support the user in an upright position. In accordance with an embodiment, however, the user U is free to lean forward against the resistance of the installed anterior trunk support vest 20, and in particular against the resistance of the stretchability of the shoulder straps 24. Because the shoulder straps 24 are stretchable, the user U can lean forward such as is shown in FIG. 2, without the lower portion 32 crawling upward on the torso of the user U. This feature permits some forward movement by the user U without the lower portion 32 crawling upward to the user U's neck. The amount that a user may lean forward is based upon the stretchability of the shoulder straps 24. A person of skill in the art may chose a suitable material for the shoulder straps 24 based upon the range of motion or level of function of the user U and a particular application.\nAs can be seen in FIG. 4, the swivel buckles 38 each include a male connector 52 and a female receiver 54. As is known, the male connector 52 may be inserted and snapped into the female receiver 54. The male connector 52 is then free to rotate relative to the female receiver 54, as shown in FIG. 5. The male connector 52 may be removed from the female receiver 54 by depressing a tab at the center of the male connector 52.\nThe swivel buckles 38 provide convenience in that when the user U moves, such as leaning forward as is shown in FIG. 2, the lower straps 40 pivot to the most direct line of tension and avoid bunching of the anterior trunk support vest 20, which would cause discomfort. The swivel movement of the swivel buckles 38 is shown exaggerated in phantom in FIG. 5. This action can also be achieved by sewing the strap loosely to a D-ring or other suitable connection structure, but the use of the buckles 38 allows the lower straps 40 to be separated for removal.\nIf unidirectional stretchable material is used for the lower portion 32, the unidirectional material can provide ease in breathing for the user U, because it allows the chest of the user U to expand during respiration. An example of a unidirectional stretchable material is leno weave material, also known as gauze or doup weave. This material stretches primarily in one direction, but allows very limited stretching in another. Other suitable materials may be used. The material may be arranged so that the stretchable direction is aligned to extend the width of a user, such as the user U, and the limited stretch direction of the material is aligned vertically.\nThe convergence zone 30 is shaped to take maximum advantage of the stretchable qualities of the shoulder straps 24. As can be seen in FIG. 3, the convergence zone 30 is rounded into an S-shape so that at its outer portions it is perpendicular to the direction of extension of the shoulder straps 24, but at its central portion it approaches horizontal, or perpendicular to the zipper 42. The overall rounded shape of the convergence zone 30 is perpendicular to the extension of the shoulder straps 34. This configuration permits tension applied through the shoulder straps 24 to stretch and pull uniformly on the lower portion 32.\nAlthough the anterior trunk support vest 20 has use for supporting a person, such as a seated person upright in the wheelchair 22, the anterior trunk support vest 20 may be used in other applications, such as for a load-bearing harness in backpacks, military equipment carrying vests, toolbelt supports, and in other applications where there is a need for a supportive harness which allows comfortable movement with minimal loss of control. The anterior trunk support vest 20 may include pockets or other structures for convenience of the particular application. For example, an anterior trunk support vest 20 used on a backpack for bicyclists may include pockets for a drinking tube and an anterior trunk support vest 20 designed for a fly fisherman's backpack may include pockets for various hooks and other fishing paraphernalia. In addition, the anterior trunk support vest 20 may be attached in a different manner, such as by being sewn directly onto a backpack or being attached directly to a toolbelt in the style of suspenders.\nOther variations are within the spirit of the present invention. Thus, while the invention is susceptible to various modifications and alternative constructions, a certain illustrated embodiment thereof is shown in the drawings and has been described above in detail. It should be understood, however, that there is no intention to limit the invention to the specific form or forms disclosed, but on the contrary, the intention is to cover all modifications, alternative constructions, and equivalents falling within the spirit and scope of the invention, as defined in the appended claims.\nAll references, including publications, patent applications, and patents, cited herein are hereby incorporated by reference to the same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.\nThe use of the terms “a” and “an” and “the” and similar referents in the context of describing the invention (especially in the context of the following claims) are to be construed to cover both the singular and the plural, unless otherwise indicated herein or clearly contradicted by context. The terms “comprising,” “having,” “including,” and “containing” are to be construed as open-ended terms (i.e., meaning “including, but not limited to,”) unless otherwise noted. The term “connected” is to be construed as partly or wholly contained within, attached to, or joined together, even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range, unless otherwise indicated herein, and each separate value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples, or exemplary language (e.g., “such as”) provided herein, is intended merely to better illuminate embodiments of the invention and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non-claimed element as essential to the practice of the invention.\nPreferred embodiments of this invention are described herein, including the best mode known to the inventors for carrying out the invention. Variations of those preferred embodiments may become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventors expect skilled artisans to employ such variations as appropriate, and the inventors intend for the invention to be practiced otherwise than as specifically described herein. Accordingly, this invention includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover, any combination of the above-described elements in all possible variations thereof is encompassed by the invention unless otherwise indicated herein or otherwise clearly contradicted by context.","|Publication number||US7311591 B2|\n|Application number||US 10/625,353|\n|Publication date||Dec 25, 2007|\n|Filing date||Jul 23, 2003|\n|Priority date||Oct 11, 1994|\n|Also published as||US20050059329|\n|Publication number||10625353, 625353, US 7311591 B2, US 7311591B2, US-B2-7311591, US7311591 B2, US7311591B2|\n|Original Assignee||3M Innovative Properties Company|\n|Export Citation||BiBTeX, EndNote, RefMan|\n|Patent Citations (32), Non-Patent Citations (2), Referenced by (11), Classifications (11), Legal Events (3)|\n|External Links: USPTO, USPTO Assignment, Espacenet|\nThis application is a continuation-in-part of U.S. patent application Ser. No. 08/540,674, filed Oct. 11, 1995 now abandoned which claimed priority from United Kingdom Patent Application No. 94 205 09, filed Oct. 11, 1994. This application also claims priority from UK Patent Application No. 94 205 09, filed Oct. 11, 1994.\nThis invention relates to abrasive materials and in particular to direct-coated sponge abrasive materials.\n“Direct-coated” sponge abrasives are materials in which abrasive mineral is coated on the surface of a resilient, cellular (sponge) material, such as foamed plastic, together with the relevant adhesives and binders. Although one or more resin layers may be coated on the foam prior to coating the abrasive, it is the foam itself which provides the overall structural integrity of the finished article, and largely determines its bulk physical properties such as tensile strength.\n“Laminated sponge” abrasives are materials in which a conventional substrate, such as paper, cloth, etc. is coated on one side with abrasive grains and laminated by the other side to a sponge backing. Such laminated sponge abrasive materials are disclosed in DE 39 03 204; U.S. Pat. No. 4,263,755; and EP 0 578 865; and exemplified by the product sold under the tradename “SIASOFT” by SIA. Although the presence of the sponge layer confers useful properties such as resilience and insulation, the overall properties are determined to a significant extent by the substrate on which the abrasive is actually coated.\nDirect-coated sponge abrasives show several advantages over their laminated sponge counterparts, including reduced raw material costs and an improved performance in terms of the smoothness of finish for a given rate of cut. Direct-coating of abrasive on to sponge materials is disclosed in various patents, e.g. U.S. Pat. Nos. 4,966,609; 4,629,473; 4,038,047; and 3,607,159; and UK Patent Nos. 1 597 455 and 1 472 087. Commercially available direct-coated sponge abrasive products are sold by 3M Company under the product numbers 03808, 03809 and 03810.\nSuch direct-coated sponge abrasives are generally intended for hand sanding where they are grasped by the user and rubbed against the workpiece with a suitable application of pressure. Thus, the user's hand and arm must supply a gripping action, a pressing action and a push-pull action, with the gripping action in particular potentially imposing strain and fatigue on the muscles and joints of the fingers when sanding is carried out over extended periods of time.\nA variety of materials have been laminated to the reverse (non-abrasive coated) side of direct-coated sponge abrasive articles.\nU.S. Pat. No. 4,966,609 discloses the use of a textile reinforcement (which may be any knit, woven or non-woven fabric) to provide increased strength and restrict extensibility. U.S. Pat. Nos. 4,629,473; 4,038,047; and 3,607,159 all disclose direct-coated sponge abrasive laminated to a backing member. The backing members include cloth, paper, plastic film, etc., and the backing member apparently contributes most of the mechanical strength of the finished article. UK Patent No. 1 472 087 discloses a foam pad having a coating of abrasive on one side and a coating of adhesive on the reverse. The adhesive enables the pad to be fixed to a suitable surface so that surgical tools or other implements may be cleaned by rubbing against the abrasive coating.\nHook and loop attachment systems are known and have been used for the attachment of abrasive articles to back-up pads, including the attachment of laminated sponge abrasive articles, as disclosed in EP 0 578 865 and DE 39 03 204. Hook and loop systems have also been used for the attachment on non-abrasive-coated foam buffing pads, as disclosed in U.S. Pat. Nos. 5,123,139 and 4,962,562.\nU.S. Pat. No. 4,263,755 discloses a laminated sponge abrasive article in which the sponge layer itself functions as part of a hook and loop attachment system. This places severe restrictions on the type of sponge material that can be used, as only very open-textured materials give adequate engagement with the hooks or other protuberances on the back-up pad. Such materials do not function well as the self-supporting backing for a coated abrasive layer.\nVarious patents, e.g., UK 2 113 977; WO 87/04061; and WO 86/01090, disclose articles in the form of a glove having abrasive and/or cleaning elements releasably attached thereto. The hook and loop attachment system sold under the trade designation “VELCRO” attachment system is suggested as a possible attachment means, and the cleaning/abrading elements may comprise cellular materials.\nIt is known to use a strap or similar device to secure a sanding pad or block to a user's hand, the pad or block being releasably attached to an abrasive sheet, e.g., U.S. Pat. Nos. 5,222,331 and 4,202,139 and the hand sanding pad sold under the trade designation “EASYGRIP” hand sanding pad supplied by NicSand Inc. of Cleveland, Ohio.\nThe present invention provides direct-coated sponge abrasive materials adapted for being releasably secured to a hand strap, back-up pad, etc.\nAccording to the present invention, there is provided a direct-coated sponge abrasive material directly bearing a releasable securing means comprising one part of a two part hook material and loop material attachment system.\nThe term “direct-coated sponge abrasive material” is intended to include materials in which abrasive material is coated on the surface of a resilient, cellular sponge material together with relevant binders and to exclude sponge materials that are impregnated throughout with abrasive particles. That is, a direct coated abrasive sponge has only a surface coating of abrasive material.\nThe invention provides direct-coated sponge abrasive materials which may be used in combination with a strap, back-up pad, block, handle, etc. bearing the other half of the hook and loop attachment system. The resulting products retain the outstanding performance of direct-coated abrasive sponges and possess one or more of the advantages of convenience, simplicity to use and reduction in strain and fatigue during hand sanding. The invention allows the sponge abrasive material to be secured flat against the user's palm so that the gripping action is largely eliminated, thereby reducing strain on the hand.\nIn one embodiment of the invention, a layer of loop material is secured to the sponge surface and enables temporary attachment to a layer of hook material which forms part of a handle, hand strap, back-up pad, etc.\nIn an alternative embodiment, a layer of hook material is secured to the sponge surface to enable attachment to a layer of loop material which forms part of a handle, hand-strap, back-up pad, etc.\nLoop materials suitable for use in the invention include brushed nylon, such as the material sold under the trade designation “VELL STRAPP RESINATO” by Sitip SpA of Italy, and other commercially available materials such as those supplied under the trade designations “KANEBO 2A3,” “KANEBO 2K3,” “VELCRO,” etc.\nHook materials suitable for use in the invention have a multiplicity of hook-shaped or burr-like protuberances capable of engaging the loops of the loop material, and are available commercially under the trade designations “VELCRO,” “KANEBO,” etc.\nThe loop material or hook material may be secured to the sponge surface by any conventional means, such as flame lamination or adhesive, especially a hot melt adhesive. Suitable hot melt adhesives under the trade designation “SHARNET 4200,” are available from Sharnet Corp., Ward Hill, Mass. Alternatively, a pressure-sensitive adhesive may be used.\nAbrasive articles in accordance with the invention comprise a sponge substrate bearing on at least one surface thereof a coating of abrasive particles and on at least one other surface thereof a layer of hook or loop material. The layer of hook or loop material may extend over the entire area of the surface(s) to which it is secured, or may extend over only part of the surface(s). The substrate may comprise any sponge material suitable for use as an abrasive backing, including both open-cell and closed-cell materials, such as those disclosed in U.S. Pat. Nos. 4,966,609; 4,629,473; 4,038,047; and 3,607,159; and UK Patent Nos. 1 597 455 and 1 472 087. Abrasive particles are coated on at least one surface of the substrate by any conventional method, such as the methods disclosed in the aforementioned US and UK patents, but a particularly suitable method employs a moisture-curable hot melt adhesive as the make adhesive, as disclosed in UK Patent Application No. 9316715.3\nIn a particularly preferred embodiment of the invention, the substrate comprises a sheet of open-cell polyester-urethane foam having a density of about 50 to 100 kg/m3 and a thickness of about 2 to 15 mm coated with abrasive particles on one major surface, the other major surface being bonded to a layer of hook or loop material, which preferably extends over the entire area of said other major surface. In this embodiment, the abrasive materials may be releasably attached to the back-up pad of a machine sander where back-up pad is equipped with the complementary half of the hook and loop attachment system. This enables the outstanding performance of direct-coated sponge abrasives to be realized in machine sanding operations such as rotary or random orbital sanding. The back-up pads and abrasive materials may have any desired shape, e.g., circular, rectangular, etc.\nIn this embodiment, abrasive materials of the invention, in the form of an elongate strip, may also be used to form an abrasive belt by attachment to a backing belt bearing the appropriate part of the hook and loop attachment system.\nAbrasive materials in accordance with this embodiment of the invention are equally suited to hand sanding applications. For example, they may be releasably attached to a block or hand pad having an attachment surface equipped with the appropriate part of the hook and loop attachment system. The block or hand pad may be made of any of the conventional materials (wood, plastic, rubber, etc.) and is preferably molded or otherwise shaped so as to be grasped comfortably by the user, and/or is equipped with a strap with which to secure it to the user's hand.\nAlternatively, the invention enables a hand-securing strap to be attached directly to the abrasive material. To this end, the strap comprises an elongate strip of material, e.g. about 50 mm wide and about 200 mm long having on at least the end portions of one of its surfaces the part of a hook and loop attachment system that is complementary to that present on the abrasive material of the invention. The strap may comprise any suitable material such as rubber, plastic film or textiles (woven or nonwoven). Elasticated or otherwise resilient materials may be used advantageously. At least the end portions of one of the surfaces of the strip are equipped with one half of the hook and loop attachment system. Preferably, an area of at least 500 mm2 (more preferably at least 1000 mm2) at each end is so equipped. Optionally, the entire surface may be so equipped. By attaching the ends of the strip to the abrasive material, it is possible to form a loop which accommodates the user's hand.\nTwo possible modes of attaching the ends are shown in\nIn the case of direct attachment of a hand-securing strap as described above, it is unnecessary for the entire surface of the abrasive material to be equipped with hook or loop material. For example, a central strip would normally be all that is required, thus saving on the cost of raw materials, although this may be outweighed by other considerations, such as the desirability of generating products for a wide range of end uses from a single manufacturing operation.\nThe arrangement shown in\nIn another preferred embodiment of the invention, the abrasive material comprises a foam block (e.g., of about 25 mm thickness and square or rectangular in shape) having one pair of parallel opposed major surfaces, and two pairs of parallel opposed minor surfaces. At least one, but preferably both, of the major surfaces are coated with abrasive particles, and preferably one pair of parallel opposed minor surfaces are also coated with abrasive particles. Different grades of abrasive particles may be coated on different surfaces. Such abrasive materials are known, and are disclosed, for example, in UK Patent No. 1 328 292. In accordance with the present invention, two parallel opposed minor surfaces not having a coating of abrasive particles are each equipped with one part of a hook and loop attachment system. This enables the temporary attachment of a hand strap of the type described above, which may be used to secure the sanding block to the user's hand and hence reduce or eliminate the gripping action required.\nVarious modifications and alterations of this invention will become apparent to those skilled in the art without departing from the scope of this invention, and it should be understood that this invention is not to be unduly limited to the illustrated embodiments set forth herein.\n|Cited Patent||Filing date||Publication date||Applicant||Title|\n|US2780533 *||Mar 7, 1950||Feb 5, 1957||Rexall Drug Company||Abrasive article and method of making|\n|US3607159||May 12, 1967||Sep 21, 1971||Norton Co||Saturated, resilient, flexible and porous abrasive laminate|\n|US4038047||Aug 13, 1971||Jul 26, 1977||Norton Company||Method of making a flexible resilient abrasive|\n|US4202139||Apr 6, 1978||May 13, 1980||Minnesota Mining And Manufacturing Company||Conformable hand sanding pad|\n|US4263755||Oct 12, 1979||Apr 28, 1981||Jack Globus||Abrasive product|\n|US4593427||Aug 20, 1984||Jun 10, 1986||Ortolivo Thomas V||Waterproof scouring glove|\n|US4621388||Dec 10, 1984||Nov 11, 1986||Ortolivo Thomas V||Waterproof scouring glove with flange|\n|US4629473||Jun 26, 1985||Dec 16, 1986||Norton Company||Resilient abrasive polishing product|\n|US4962562||Jan 18, 1989||Oct 16, 1990||Minnesota Mining And Manufacturing Company||Compounding, glazing or polishing pad|\n|US4966609||Apr 7, 1989||Oct 30, 1990||Uniroyal Plastics Co., Inc.||Conformable abrasive article|\n|US5054248||Sep 3, 1989||Oct 8, 1991||Thayer Donald R||Four-way hand sander|\n|US5123139||Jan 16, 1991||Jun 23, 1992||Meguiar's, Inc.||Buffing pad assembly|\n|US5127976||Sep 27, 1989||Jul 7, 1992||Tescona Pty. Limited||Method of making a scrubber glove|\n|US5170595||Dec 19, 1990||Dec 15, 1992||Wiand Ronald C||Pull tab for velcro backed marble grinding pad and method for removal|\n|US5220752||Jul 22, 1991||Jun 22, 1993||Christopher Cheney||Conformable sanding device incorporating a flexible attachment means|\n|US5222331||Apr 10, 1990||Jun 29, 1993||Minnesota Mining And Manufacturing Company||Abrading assembly|\n|US5309681||Jun 21, 1993||May 10, 1994||Christopher Cheney||Conformable sanding assembly|\n|CH650441A5||Title not available|\n|DE3903204A1||Feb 3, 1989||Feb 15, 1990||Peter Joest||Abrasive element with temperature barrier|\n|DE9407622U1||May 6, 1994||Aug 4, 1994||Joest Peter||Schleifkörper mit einer Kontaktfläche zur Adaption mit einem Werkzeug|\n|EP0112405A1||Dec 24, 1982||Jul 4, 1984||Hans J. Fabritius||Attaching element to be employed in grinding and polishing machines|\n|EP0578865A1||Dec 21, 1992||Jan 19, 1994||Norton Company||Abrasive tool|\n|FR1411309A||Title not available|\n|GB1328292A||Title not available|\n|GB1472087A||Title not available|\n|GB1597455A||Title not available|\n|GB2113977A||Title not available|\n|GB2143720A||Title not available|\n|GB2260889A||Title not available|\n|GB2282144A||Title not available|\n|WO1986001090A1||Dec 24, 1984||Feb 27, 1986||Kato Products Corporation||Waterproof scouring glove|\n|WO1987004061A1||Jan 14, 1987||Jul 16, 1987||Juliana Mcleish||Scrubber glove|\n|1||3M product brochure entitled \"New abrasive pads for easy sanding of awkward areas;\" 3M, Berks, United Kingdom; 2 pgs. (Dec. 1992).|\n|2||3M product brochure entitled \"Softback Sanding Sponges;\" 3M, St. Paul, Minnesota; 1 pg. (1993).|\n|Citing Patent||Filing date||Publication date||Applicant||Title|\n|US8210910 *||Nov 18, 2008||Jul 3, 2012||Lake Country Manufacturing, Inc.||Multi-faceted sanding/finishing tool|\n|US8566998 *||Feb 23, 2012||Oct 29, 2013||James A. Gavney, Jr.||Absorbent structures with integrated contact elements|\n|US8950638 *||Sep 25, 2013||Feb 10, 2015||Loopy Cases Llc||Finger loop for portable electronic device case|\n|US9155377 *||Jan 2, 2015||Oct 13, 2015||Loopy Cases Llc||Finger loop for portable electronic device case|\n|US20080207099 *||Feb 26, 2008||Aug 28, 2008||Brown John E||Resilient abrasive article and method of manufacture|\n|US20100124873 *||Nov 18, 2008||May 20, 2010||Lake Country Manufacturing, Inc.||Multi-Faceted Sanding/Finishing Tool|\n|US20110177761 *||Jan 19, 2011||Jul 21, 2011||Ronald Mastro||Sanding Tape Clip and Methods of Use|\n|US20110183589 *||Jan 25, 2010||Jul 28, 2011||Samuel Yu||Universal interface pad conversion band|\n|US20120151699 *||Feb 23, 2012||Jun 21, 2012||Gavney Jr James A||Absorbent structures with integrated contact elements|\n|US20120309273 *||Jun 6, 2011||Dec 6, 2012||Popov Georgi M||Hand-powered polishing apparatus and kit for stainless steel sinks|\n|US20160274312 *||Oct 16, 2014||Sep 22, 2016||Afl Telecommunications Llc||Attachable Disposable Fiber Optic Cleaning Pad|\n|U.S. Classification||451/538, 451/524, 451/539, 451/523|\n|International Classification||B24D13/14, B24D11/00, B24D15/04|\n|Cooperative Classification||B24D15/04, B24D13/147|\n|European Classification||B24D13/14D, B24D15/04|\n|Nov 24, 2004||AS||Assignment|\nOwner name: 3M INNOVATIVE PROPERTIES COMPANY, MINNESOTA\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:STUBBS, ROY;REEL/FRAME:015407/0984\nEffective date: 20031216\n|May 25, 2011||FPAY||Fee payment|\nYear of fee payment: 4\n|Jun 10, 2015||FPAY||Fee payment|\nYear of fee payment: 8"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:46844553-925d-4c10-a71a-5ee80af2c9c9>","<urn:uuid:f4942f34-8c03-41c9-b070-a5784fb08ab9>"],"error":null}
{"question":"What are the key steps in preparing for home construction, and how do siding costs factor into the overall budget planning?","answer":"The key steps in preparing for home construction include deciding to build, choosing a home plan, modifying the plan if needed, hiring a contractor, adhering to local laws, choosing construction materials, and setting a timeline. When considering siding costs specifically, they vary significantly by material type - vinyl ranges from $3-$11 per square foot, brick costs $7-$15 per square foot, and stone can cost $10-$45 per square foot including labor. Factors affecting siding costs include the size of your home, complicated house shapes, local labor costs, and whether old siding needs removal. It's important to calculate total square footage needed and account for special areas like gables, windows, doors, and soffits when budgeting for siding.","context":["What to Think About When Building a Home\nThe process of building a house is one of the most exciting, frustrating, and rewarding experiences that anyone can ever go through.\nYou get to see your dreams come to life right in front of your eyes, but as with any major construction project, building a new house can also have its share of setbacks, difficulties, and problems.\nWith proper planning and attention to detail, these problems can be kept to a minimum, and you can instead focus on the excitement of spending many happy years living in your dream home. We’ve put together a step-by-step guide full of tips to help you understand what to expect throughout the entire process.\nThe Process of Building a House\nHome construction from start to finish is a long, drawn-out endeavor that can be broadly divided into three different stages: pre-construction, during construction, and post-construction. It pays to be organized, so here’s a rough outline of the home building process.\nBefore You Start Construction\nThese are the choices you’ll need to make before the day that you break ground on your new home.\nDecide to Build\nOne of the most important parts of a new home is deciding whether or not to buy new, remodel your current home, or build a home to match your needs.\nThis is a big decision. Things like your budget, the value of your home, the real estate market, and your specific needs and desires are all factors that come into play.\nIf you decide building is for you, many people opt to hire a real estate agent to help find the perfect piece of land. The lot that you purchase will factor into the size and layout of the floor plan.\nChoose a Home Plan\nNext, once you’ve chosen to build a new home, you’ll need to choose a plan. When looking for a home construction plan, it’s a good idea to think about the kinds of features that drew you toward building your own home in the first place.\nThis beautiful Transitional Farmhouse style home with 4 bedrooms and 3.5 bathrooms in 2926 sq. ft. of living space is one of the designs you can choose to build from (Plan #142-1220).\nDo you want a ranch style home? Or a Dutch Colonial? Or maybe a Craftsman style home instead?\nThink about things like how many bedrooms you would need, whether or not you want an attached garage, the layout of your kitchen, if you want your living room to be part of an open floor plan, and how you would like to incorporate the geography of your lot into your home.\nReview the cost to build report for your favorite plans to make sure you stick within your budget, and always plan for some added building costs that could pop up along the way and increase your total cost.\nModify the Plan\nUsually, you’re not going to be able to find a home that completely fits every single need that you have.\nBecause of this, you might find a plan that has most of the features you’re looking for and then modify it slightly (add or subtract a fireplace, add a walk-in closet, add a modest amount of square footage, etc.). It’s also important to learn how to read home plans so you fully understand the layout and what changes you’ll need made.\nDepending on the plan, the changes you have in mind, and the company you get it from, alternations likely come at a cost in addition to the plan price. Alternatively, you can discuss these “remodels” directly with your contractor. He or she may be able to do them on-site if they’re not structural in nature.\nHire a Contractor\nThis is possibly the most important step of new home construction because the person you choose is going to be your go-to for all questions throughout construction. They are also in charge of helping you approve your final design choices and making sure you have a beautiful and safe home.\nThere are plenty of online databases for hiring independent contractors, as well as local companies that specialize in building houses. Whatever you choose, make sure that you’ve read plenty of reviews on their previous work, and even contact the home builder in person, prior to signing any contracts with them.\nAdhere to Local Laws\nThis is a step of new home construction that you can partner with your contractor on. You want to make sure that you are following local laws every step of the way during construction. Because of this, you should even do your own research on acquiring building permits, gaining access to city water, building codes, etc.\nSometimes working with local governments is a tedious process, so be sure to build time for “the unexpected” into your construction timeline.\nChoose Construction Materials\nChoosing the construction materials for your home is another big choice to make, and it’s one that shouldn’t be made without the help of your contractor.\nWhile home-building materials can be adjusted depending on personal taste and budget, there are certain structural elements that are best left to the opinion of a professional.\nMake sure that you speak to your contractor about the following types of things:\nFoundation - Choosing the new foundation for your newly built home is likely one of the most important choices that you’ll make throughout the construction process. There are many different types of foundations, including a basement, slab, or crawl space. There are also many different types of tests to run beforehand to ensure the soundness of a particular foundation. These tests help determine the presence of groundwater and other soil quality considerations.\nSiding - Another one of the bigger choices to make when building a new home is thinking about the kind of siding that you want to use. There are so many different options to choose from, all depending on your personal preferences and the local environment. You can use wood, brick, vinyl, fiber cement, rock, stone, or a veneer, aluminum, or even log.\nRoofing - There are many types of roofing materials available to choose from, as well as colors. Your contractor will be able to help you choose which type of material is best for both the look of your home, as well as local weather patterns and durability.\nSet a Timeline for Construction\nIt’s important to have a schedule for construction when building a new home. Yes, problems will arise, but you can account for those in your schedule. You don’t want to spend months waiting around when you could be living in and enjoying your new home.\nWork out a rough timeline (including certain progress benchmarks) with your contractors and be sure to stop in regularly during construction to check in on things. Be sure to include time for site preparation at the beginning and final touches at the end.\nHere are the kinds of things you should expect once construction on your new home has started:\n- During construction, you should expect to stop by the building site for both planned and unplanned inspections and walk-throughs.\n- Will you want a house wrap? This is a popular option and can help increase energy efficiency.\n- You should be in constant contact with your builder to give the final okay on many aspects of your home’s design and safety features.\n- You should make yourself available to answer any questions or concerns as they arise.\n- Be ready to make quick decisions like changes to the HVAC system, choosing different light fixtures, or anything else that could pop up.\n- If needed, create a punch list of items you notice that must be completed before construction is done.\nAfter New Home Construction\nOnce your new home is finished being built, here are the kinds of things that you can expect to do.\n- A Deep Clean - It’s pretty standard when purchasing a home to have it professionally cleaned before you move in. While this might not cross your mind when you are creating a custom home, it’s still always a good idea to hire a cleaning company to come through before you start decorating or moving items into the home. This way, there isn’t any residual construction dust or debris left in the home and you can truly move in with a fresh start. Once you move in, it can’t hurt to vacuum the carpeting again, wipe down the countertops, and give the home another quick clean.\n- Decorating Fun - One of the most exciting parts about building a new home is not only getting to customize the design and structure of the home but also to have complete control over its aesthetics as well. So have fun! Whether you choose to partner with an interior decorator or rely on your own eye for style, your newly built home is truly the best kind of blank canvas for you to play with.\n- Moving In - Since you are moving into a brand new home, this might be a good time to pause and take inventory on all the things that you own to see if you want to bring them into your new home with you. You might want to consider discarding older, outdated, or broken items in favor of updating them for newer versions for your new home.\nEnjoy the Process\nMost of all, remember to enjoy the process of building a new home. This is a once in a lifetime event. It’s not every day that someone gets to have complete creative control over the place where they spend the majority of their lives – the place where family memories are made.\nWhile it might seem a bit stressful at times, remember to take a step back to soak it all in.\nTo help build your knowledge as you prepare to embark on this journey, be sure to read our ebook The Ulitmate Beginner's Guide to Building Your Home. This free, step-by-step guide provides you with the essential information you need to know when building your own home.\nBuilding a new home is a journey. You’ll need to make big decisions, face setbacks, and oversee a major but ultimately amazing process. This is because, at the end of it all, you will get to enjoy a custom-built, beautiful home.","House siding cost doesn’t come down to a few simple line items. When it’s all said and done, it’s a project with countless moving parts, the likes of which can become confusing.\nBetween materials, labor, permits and the numerous other factors that often go forgotten, an accurate estimate may seem impractical.\nThere’s still several ways to get an idea of the overall project cost before getting started.\nTake a look at the approaches below to gather a shortlist of the major line items that go into a house siding project.\nCost of Siding by Square Foot\nCalculating house siding cost by square foot is the most comprehensive option. Given that it takes into consideration everything from specific house siding measurements, soffits, trim and more, there won’t be much left to account for.\nIf you’re struggling to figure out how to measure for siding or how much siding you need, no worries.\nHere’s a thorough siding calculator method by Lowe’s with a step-by-step guide:\n- Start by measuring the height and width of each side of the home, multiplying the two numbers to get each side’s surface area. Now add all the sums of each side to get the total in square feet for your siding.\n- Take note of special areas not measured in the siding itself such as gables. For triangular measurements such as this, measure from bottom to top, multiplying that by half the base length. Now add those measurements.\n- Now, measure all windows and doors by multiplying height and width to find the surface area as done to calculate the siding. Add all of the sums of each surface area together for a total.\nWith these three measurements in mind, add the totals from the first two lines and subtract the total from the third line. Now you have your total square footage needed in siding. There’s still a few more materials to consider though.\n- Soffits, the underside of a roof’s overhang, will also need to be worked into the equation. Again, measure each section of soffits, multiplying height and width to find the surface area.\n- The last measurement is for trim. Measure each area you’d like to add trim and record the amount you’ll need in feet.\nIf doing the totals on paper seems intimidating, use the Lowe’s link above for their siding estimate calculator that lets you enter your numbers and tabulates them for you in real-time.\nCost by Siding Material\nHere are some of the house siding costs for the most common siding types per square foot. This cost includes both materials and labor.\n|Material||Total average range cost per square foot for labor and materials|\n|Vinyl||$3-$11 per square foot|\n|Wood||$4-$13 per square foot|\n|Aluminum||$3-$11 per square foot|\n|Engineered Wood||$3.50-$8.50 per square foot|\n|Fiber Cement||$5-$19 per square foot|\n|Brick||$7-$15 per square foot|\n|Stucco||$4-$8 per square foot|\n|Steel||$3-$10 per square foot|\n|Stone||$10-$45 per square foot|\n|Cedar Shake||$6.50-$13.50 per square foot|\nSiding Factors to Consider\nThere are many considerations when thinking about what kind of siding would work best for you.\n- Style Considerations – You should consider the siding type that you like the most when you are deciding which siding to buy. If you like your siding, you will value it more. It is also important to understand how the siding you like best compares to other options.\n- Size of Your Home – The larger size house you have, the more expensive siding will cost.\n- Complicated House Shapes – If your home has intricate bends, curves, and cut-outs, the labor costs for siding these houses will be more than for simple styles.\n- Local Labor Costs – Depending on available labor in your area, the costs will be higher in city areas.\n- Removal of Old Siding – If you have old siding that needs to be removed, it will increase your labor costs.\n- Architectural Style of Your Home – You should consider the style of your home when thinking about which siding to choose. If you have a large, historic home, the best kind of siding would be in keeping with the style of the home.\n- Climate – Some homes will need moisture barriers if they are in wet areas that will increase the cost. Also, they will need a siding like fiber cement that stands up well to this. Others in hot climates will function best with steel or aluminum siding.\n- Maintenance – Decide how much maintenance you are willing to do before you decide on your siding. For example, wood siding requires painting or staining on a regular basis in order to keep looking good. Metal sidings require little in the way of maintenance.\nHouse Siding Cost by Types\nHouse siding costs vary by type according to the cost of the materials and the kind of labor and supplies needed to add the siding.\nVinyl siding is a popular siding material in the United States and abroad. It is made from polyvinyl chloride and is made to mimic other materials like wood. It is the most cost-effective siding type and functions well in terms of durability. Vinyl siding prices differ depending on the thickness of the vinyl panel.\nAlong with the vinyl siding itself, you need to plan for the trim including the undersill, the starter strip, and J-channels. Here is a handy vinyl siding cost calculator to help you calculate the full cost for your home.\n- Vinyl Siding Cost – $1.50-$8.50 per square foot\n- Installation Costs for Vinyl Siding – $.90-$1.20 per square foot\nWood siding has a natural and textured look that many sidings try to replicate, but it also has higher maintenance than siding like vinyl or metal. This includes regular painting or staining and termite management.\nOne of the most popular types of natural wood siding is clapboard siding. Manufacturers use wood such as oak, pine or spruce. Pine is the least expensive and redwood the most expensive type of wood siding. If you maintain it, natural wood siding will last up to 40 years.\n- Cost of Wood Siding – $2-$15 per square foot (depending on the type of wood)\n- Installation Costs for Wood Siding – $1.06-$2.50 per square foot\nAluminum siding is one of the most cost-effective metal siding options available. It has been a popular siding choice because it is lightweight, eco-conscious, and easy to work with. It is also durable. It will withstand pests, harsh weather, and resists corrosion from moisture. Aluminum siding comes in the form of flat or corrugated sheets.\n- Cost of Aluminum Siding – $1.75-$7 per square foot\n- Installation Costs for Aluminum Siding – $4.40 per square foot\nEngineered Wood Siding\nEngineered wood is composite siding that manufacturers create from wood particles mixed with resin and plastics. They mold this into siding boards. These can have an embossed texture that resembles natural wood texture or can have a smooth finish. These boards are durable, inexpensive, and will last between 20-30 years.\n- Cost of Engineered Wood Siding – $2-$4 per square foot\n- Installation Costs for Engineered Wood Siding – $1-$6 per square foot\nFiber Cement Siding\nFiber cement siding, also known as Hardie board, is a siding made from a composite of sand and cement. It is very durable and rot, fire, and insect resistant. It comes in many different forms including shingles, boards, and siding. Plank siding is the least expensive and boards are the most expensive. Most fiber cement siding lasts for up to 50 years.\n- Cost of Fiber Cement Siding – $1-$15 per square foot (depending on which form of siding you choose)\n- Installation Costs for Fiber Cement Siding – $4-$8.50 per square foot\nBrick is a classic siding type that comes in various colors and depths. You can have full-sized bricks installed or use thinner brick veneer which is more cost-effective. Brick is durable and resistant to rot and weather. Brick is also a natural insulator and will help keep your home more energy efficient. There are also minimal maintenance concerns for around 25 years. After this, you will need to inspect the masonry joints to see if they require any repairs.\n- Cost of Brick Siding – $3-$10 per square foot\n- Installation Costs for Brick Siding – $3-$20 per square foot\nStucco is a sand and cement based siding that is ideal for homes in dry climates. It is a durable, energy-efficient, and eco-conscious choice. It also has an old-world look that works well on homes with Mediterranean styles. It is not ideal for homes in wet and humid climates.\n- Cost of Stucco Material – $5-$6 per square foot\n- Installation Costs for Stucco Siding – $2.50 per square foot\nSteel siding is a durable metal siding that looks great in industrial or rustic settings. While it is a low maintenance siding, it does help to coat it with a rust-resistant layer. Steel siding is more durable than aluminum but also more expensive. It is crafted to mimic the look of wood, but it will still look like steel.\n- Cost of Steel Siding – $4-$6 per square foot\n- Installation Costs for Steel Siding – $3.50 per square foot\nStone siding is one of the most classic types of siding available. You can choose either natural stone or stone veneer for a less expensive option. This type of siding requires little to no maintenance and will last over time. It also resists pests, rot, and fire. The cost depends on the type of natural stone you choose. Slate is the least expensive and granite is one of the most expensive. =\n- Cost of Veneer Stone for Siding – $4-$21 per square foot\n- Installation Costs for Steel Siding – $6 -$24 per square foot\n- Cost of Natural Stone for Siding – $4.50-$30 per square foot\n- Installation Costs for Steel Siding – $3 -$15 per square foot\nCedar Shake Siding\nCedar is one of the best quality wood siding options for anyone who loves the look and texture of natural wood. Cedar shakes have a rustic quality with gorgeous color variations. This wood is insect and rot-resistant. Cedar is an eco-conscious choice as it is biodegradable and sustainable. You can paint or stain your cedar shakes or even leave them untreated if you prefer a more natural look.\n- Cost of Cedar Shake Siding – $4-$8 per square foot\n- Installation Costs of Cedar Shake Siding – $1.70 -$5 per square foot\nCost of Repair vs Replacement\nIf the siding on your home has yet to reach its lifespan, repairing it may still be an option depending on the extent of the damage. While the cost to repair averages around $500, that hinges on the type of siding and what issues you’re dealing with. Here’s some of the most common repairs made to siding and their average cost:\n- Mold ranges from $500 to $700.\n- Cracks range from $50 to $150.\n- Water damage ranges from $500 to $600.\n- Dents range from $100 to $300$.\nFrequently Asked Questions (FAQ)FAQ\nWhen does siding need to be replaced?\nSiding that needs replacing will start to show obvious signs, some with more impact than others. For example, fading, rotting, and bubbling are common signs your siding may need replacing. Likewise, warping or loose boards aren’t unusual when siding is older. One of the biggest signs though is a higher electric bill. This is a tell-tale sign your siding isn’t doing its job.\nWhat’s the most popular siding material?\nWhile some siding selections are more popular region to region, it’s safe to say vinyl siding is most popular overall. Being that it’s one of the most affordable options, it comes as a first choice to most. Not to mention, it’s a low maintenance siding, which is without doubt an attractive quality to homeowners.\nWhat does it mean to purchase a square of siding?\nWhen purchasing siding you’ll hear the term “square” used a lot. In short, it’s how siding is measured. One square of siding is 100 square feet of siding material. Though the term siding square is more common to use when referring to how to measure for vinyl siding, it’s still used among other materials as well. Using a house siding calculator like the Lowe’s siding calculator will provide you with how many squares your project will need.\nWhich siding is easiest to maintain?\nThere’s several siding options that are easy to maintain but vinyl and metal appear to be the easiest by far. Both resist common siding problems like mold and pests, making for less repairs. They also require little in the way of cleaning. An annual power wash with mild soap and a soft brush for spot treatment on areas that may need it will keep your siding looking good as new.\nHow long will it take to replace siding?\nThe timeline to re-side a house seems to range based on who you talk to. Factors such as demand, weather, removal and material selection can play a role in this timeframe too. Regardless, the average project ranges from one to two weeks.\nHouse siding cost can be easy to estimate with the right resources. With several methods at your disposal, there’s no doubt you can get an accurate price without missing any major components.\nLikewise, certain factors like material and design can offer flexibility in price, making even the most modest of budgets simple to work with.\nTake the first step toward your dream home exterior and start calculating your cost today."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:271794c7-bc84-467d-9f5d-a64b70be49a9>","<urn:uuid:2a234d3b-cefc-4646-95e9-b2f1800a207f>"],"error":null}
{"question":"What are the key structural differences between current compensated chokes and copper ferrite nanocomposites?","answer":"Current compensated chokes and copper ferrite nanocomposites have distinct structural characteristics. Current compensated chokes are built with different ferrite geometries, primarily using toroidal ring cores or ribbed cores, and feature two counteracting windings that create opposing magnetic fluxes. They can be manufactured in various forms including SMD types with ring cores and multi-chamber designs. In contrast, copper ferrite nanocomposites, synthesized through co-precipitation, have a face-centered cubic (FCC) structure with specific lattice constants (around 8.24-8.31 Å). The copper ferrite nanoparticles show uniformity and homogeneity in their structure, with their crystallite size being calculable using Scherrer's formula from XRD peaks, particularly the strongest diffraction peak (311).","context":["Coils wound on toroids have due to the closed magnetic circuit, a high inductance and thus a good attenuation effect. If they are connected into a mains network in order to block or suppress interference of any frequency the choice of inductor must be determined by the kind of interference that applies. This article discuss construction and its impact of toroidal inductors, current compensated chokes, common mode chokes and beads.\nWe have to deal with either common mode / asymmetrical interference or differential mode / symmetrical interference.\nIf we have a noise or interference source in a network that produces interference currents via short circuiting parasitic capacitance in, for example, the load, the two interference types may be illustrated with Figure 1.\nThe cure for this kind of interference current might be use of chokes combined with X- and Y- safety capacitors, i.e., capacitors intended for interference suppression in the mains. Asymmetrical interference is best rectified by a so called current compensated choke, where two counteracting windings cause the load current to create two opposing magnet fluxes. Those two fluxes cancel each other out. Any saturation risk is out of the question.\nThis method allows high permeability ferrites and subsequent high inductance. For asymmetric interference signals the inductance of the two windings co-operate and produce approximately 3.5 times higher inductance than a separate winding.\nNote that the opposing fluxes from the load current create certain magnetic stray fluxes around the coils. This is illustrated in following figure 3.\nWhen there is a symmetrical interference (Figure 4.) we can’t use a current compensated choke. The fluxes of the load and interference currents will co-operate inside the toroid. In order not to come near magnetic saturation ferrites, with low to medium high permeability have to be used.\nCurrent Compensated Chokes\nSaturation effects caused by high signal currents, or DC current super imposed on the signal, reduce the effectiveness of the choke. The use of standard inductors in the signal path adversely impairs the useful signal. Current compensation circumvents these disadvantages. In current compensation, the “useful return current” must be passed through the choke. In this way, the useful current does not contribute to the magnetization of the core. See Figures 2. and 5.\nCurrent-compensated chokes can be manufactured with different ferrite geometries; the best known are toroidal ring core and ribbed core. Different core materials enable their use in various frequency ranges. A very well known component, but one not designed as a common mode choke, is the snap ferrite or the split ferrite sleeve.\nThe effect of current-compensated chokes on coupled interference is, above all, used for data and signal lines. They are often the only option to avoid the interference suppression component affecting the useful signal.\nCurrent Compensated Choke Types\nThere is a wide range of current compensated choke types. Here is an overview of the most common types.\nSMD Common Mode Noise Suppressor\nThe SMD Common Mode Noise Suppressor is usually not based on a ring core, either in size 0805 or 1206. Only for this reason it is possible to achieve such a compact current-compensated component. However, as it is a closed ferrite material system, the stray field remains negligibly small. Typical applications for the SMD common mode noise suppressors are USB, Firewire or High Speed Data Lines.\nCurrent-Compensated SMD filter\nIn contrast to SMD common mode noise suppressors, the current-compensated SMD line filters include ring cores. As a result, stray fields can almost be excluded. The different geometries and, above all, the very flat package heights of the various types, offer potential solutions for every application. High current ratings, as used in low-voltage applications, are also available.\nDespite their compact construction, the current-compensated SMD line filters can also offers 4x current-compensated versions.\nNiZn Core Current-Compensated SMD Filter\nThe NiZn current-compensated SMD line filters offers high in common mode impedance with a smaller footprint than MnZn types. NiZn ferrite base material provides a wider working temperature range.\nAt the same time, the leakage inductance is lower so the signal is less affected. The NiZn core current-compensated SMD line filters can therefore also be used at high signal frequencies.\nMnZn Current-Compensated SMD Filter\nThe MnZn current-compensated SMD line filter (such as Würth Elektronik WE-SL1 series) all excels by virtue of its low space requirement, both in terms of package height as well as footprint.\nThe manganese-zinc basic material provides adequate balances attenuation values.\nMnZn Current-Compensated SMD Filter with Separated Construction\nThe MnZn current-compensated SMD line filters can be prepared also with separated construction and both sectional and bifilar winding technology.\nThe separated construction of the sectional winding allows both the attenuation of high frequency symmetrical frequency components, as well as the suppression of asymmetrical interference components. However, if the quality of the useful signal is too greatly affected, the original bifilar winding technology should be chosen.\nHigh Density MnZn Current-Compensated SMD filter with Separated Construction\nThe MnZn current-compensated SMD line filter with separated construction can be also made in high density version that represents an advancement of the MnZn current-compensated SMD line filters. Despite the halved package height, almost the same performance can be attained, at least for low inductance values. Additionally a 3x current-compensated version has been developed, which is mainly used for low voltages.\nHigh Frequency MnZn Current-Compensated SMD Filter\nA special high frequency design of manganese-zinc allows the frequency band in the single and double figure megahertz range to be covered.\nCurrent Compensated Choke for Mains Voltage Applications\nSpecial construction of the current-compensated chokes allow the reduction of undesirable parasitic effects. Careful selection of core/winding relationship allows a very high current for a comparable footprint.\nHowever, if conducting components or packaging parts are placed in the immediate vicinity, the required safety separation must be ensured on some types, as enameled wires are not considered to be insulated components. In most cases, the use of this design, however, unproblematic as insulated components, such as capacitors, provide the necessary separation.\nMulti-chamber Current-Compensated Power Line Chokes\nMultichamber current-compensated choke has roughly twice the leakage inductance relative to comparable toroidal chokes. The effect on symmetrical interference increases without having to use an additional inductor. Parasitic parallel capacitance is reduced as a result of the construction with a multi-chamber coil body.\nThe impedance profile is raised at higher frequencies. At the same time, resilience against burst and surge pulses is improved.\nA special variant of the toroidal inductor is the so called attenuation bead that becomes effective in the MHz range. It consists in its most simple form of a miniature toroid in low or high permeability ferrite or sometimes of iron powder. In alternative designs the “bead” becomes a tube or, depending on application, a double or multiaperture ferrite core. In all these variants the lead passes through the toroid. In designs where all space in the construction is consumed and we find afterwards that interference suppression measures are needed a small ferrite bead may be the solution.\nIt increases the inductance of the lead/wire in the RF range and functions as an energy absorber, for example, transients. A simple formula for estimating the induction factor AL of the bead is Equation  below.\nAL= µ x 0.4π/c » µ x 0.2 x h x ln(D/d) \nh is height, D is outer diameter, d is inner diameter of the toroid, The measures are expressed in mm.\nExample: µ = 750, h = 2.5 mm, D = 6.3mm, d=3.8 mm gives AL = 750 x 0.2 x 2.5 x ln(6.3/3.8) = 190 nH.\nFerrite beads also are manufactured as chips. Examples of ferrite chip characteristics of two different materials are stated in the following table 1.\nNote how the flux density B has been reduced considerably at 100 °C, though the Curie temperature TC is situated considerably higher. How the different materials may depend on frequency is shown in following diagram.\nNote how lower permeabilities improve the impedance at higher frequencies as well as how DC load, increasing the flux density B, lowers the impedance. In order to cover a broader frequency band it might be necessary to connect in series ferrite beads with different materials.\nFinally we should observe that ferrites have a certain conductance. In sensitive applications it might be necessary to use isolated/lacquered beads.","STRUCTURAL CHARACTERIZATION OF CuFe2O4 NANOCOMPOSITES AND SYNTHESIS BY AN ECONOMICAL METHOD.\nABSTRACT: Copper ferrites were prepared through co-precipitation technique. The aim of this effort was to present a novel and economical method of preparation of copper ferrites via co-precipitation technique. Structural properties were studied with the help of XRD technique while micro structural study of the samples was carried out by SEM. The particle size was calculated with the help of Scherrer's formula using characteristic peaks. The SEM images showed uniformity and homogeneity of the synthesized CuFe2O4 particles.\nKey words: Copper ferrite, Low cost ferrites, Co-precipitation\nFerrites (CuFeO4) have paramount advantages over other types of magnetic materials like high electrical resistivity and resultant low eddy current losses over a wide range of frequency. For the most favorable combination of low cost, high quality, high stability, and lowest volume, ferrites are considered to be the best core material choice for frequencies from 10 KHz to 50 MHz. Ferrites offer an unmatched flexibility in magnetic and mechanical parameters . The development and consistent successes in switch-mode power supplies is continuously encouraging the ferrite industry to produce new, high quality ferrite cores capable of operating at increasingly higher frequency . The ferrite particles can be used for the synthesis of temperature sensitive magnetic fluid with higher magnetization and magnetization-temperature gradient .\nOf all magnetic materials, ferrites are the most useful because in addition to many magnetic properties, they are also good electrical insulator, unlike the ferromagnetic me als. Thus losses due to free electrons are eliminated. The crystallography, electrical and magnetic properties of ferrites depend upon the chemical composition as well as on the various heat treatments during the course of preparation . Different studies have been made to synthesize copper ferrites by using different techniques. These studies revealed that the magnetic performances and microstructure depend considerably on chemical composition and sintering temperature of samples . Ferrite shrinks when sintered, depending on the specific ferrite, this shrinkage can range from 10% to 17% in each dimension. The grain size significantly increases with increasing copper contents and sintering temperature Ts also affects the densification, grain growth and initial magnetic permeability of the samples .\nHankare and co-workers  used oxalate precipitation method to synthesize single-phase Cu-Co ferrite. Wang et al.  used NaOH as precipitant to fabricate Cu-substituted NiZn ferrite and confirmed the formation of cubical spinel structure at all the temperatures for calcinations and sintering. Yang et al.  synthesized copper ferrite (CuFe2O4) by using three different techniques, citric acidassisted sol-gel method, solid-state reaction and co-precipitation method. Conventional oxide ceramic process have been applied by Ahmed et al.  to synthesize nano-crystalline copper ferrite and indicated that with a firing temperature of 1100 degC, the samples have higher bulk density (3.93 g/cm3), whereas at 1200 degC higher saturation magnetization (45.2 emu/g) and lower co-ercivity (6.13 Oe). Various publications also explain successful synthesis of nanosized copper ferrites, and their characterizations [11, 12, 13].\nThe current effort has been focused to synthesize a low cost copper ferrite via co-precipitation tchnique. Of all these techniques, chemical co-precipitation seems to be the most convenient to synthesize nanoparticles because of its simplicity and better control over crystallite size and other properties of the materials . Characterization has been done using X-ray diffraction and scanning-electron microscope.\n2. EXPERIMENTAL SECTION\n2.1 Chemicals. Ferrous Sulphate anhydrous (FeSO4), Citric Acid (C2H4O2), and Ethylene Glycol (C6H8O7) were purchased from Merck where as Copper Sulphate (CuSO4) and Sodium Hydroxide (NaOH) were purchased from Fischer and Sigma Aldrich respectively.\n2.2 Instruments. X-ray diffraction (XRD) experiments were performed with PANanalytical, Philips and scanning electron microscopic (SEM) images were captured by using JEOL-JSM 5910. For SEM each sample was prepared and coated by gold in Spi-Module Sputter Coater because Scanning Electron Microscopy requires conductor material to analyze the morphology of samples.\n2.3 Preparation of Copper Ferrites. Cu ferrites (CuFe2O4) were prepared according to the method adopted by Shin et al.  with minor changes. They synthesized CuFe2O4 with metallic chlorides and used KOH (5N) as precipitant by maintaining pH 10 but we used NaOH (1N) as precipitant and metallic sulphates by maintaining pH 12. The total molarity of the metallic ion solution was kept constant. The samples of CuFe2O4 were prepared by co-precipitation from CuSO4 and FeSO4 salts and citric acid was added to act as complexent. Shin and co-workers mixed the chloride salts with KOH at temperature of 353K but we mixed at room temperature. The bath temperature was maintained at 85 oC. The blackish brown precipitates so obtained were washed several times with de-ionized water till filtered water of precipitates attained pH 7 and finally washed by acetone several times. The bath temperature was maintained at 85 oC.\nThe final product was dried in oven for 6 to 8 hrs at 50 oC and grinded to fine powder with the help of pestle and mortar. The micro structural characteristics were examined via XRD technique and Scherrer's formula was used to find crystallite size (t = 0.9 ? / B cos ?B, , where ? is the wavelength of the X-rays used. B is deducted from the characteristic peak at full width half maximum (FWHM).\n3. RESULTS AND DISCUSSION\nWe have successfully employed co-precipitation technique and synthesized the Copper ferrites; results were compared with that of Shin et al's  results and were found in a close agreement. In the present work, synthesized copper ferrite showed all the peaks of XRD pattern in close agreement with JCPDS card no. 00-025-0283. There were additional peaks corresponding to extra phases such as a-Fe2O3 which showed incomplete reaction. Crystallite size was estimated in nm . The analysis of the peak positions and the relative intensities of the diffracted lines of the synthesized copper ferrites were compared with XRD pattern of Gomes et al. . By comparing the XRD patterns of the prepared samples, it was found that these were not indexed to single phase of spinal structure. Particle size of each sample was calculated with strongest diffraction peak (311), Fig.1. It was observed that broadening of diffraction peak (311) was directly relating to the particle size.\nSo in this regard Scherrer's formula was used to find the crystallite size. The previous study clearly depicts that the pH of the solution has important factor regarding the ferrite formation. In the present study we maintain the pH of the solution at 12.\nIn sample K1A four peaks out of seven major peaks have been matched with the standard pattern of CuFe2O4. The peaks with miller indices (222), (400), (533) did not match with the standard pattern of CuFe2O4. The following miller indices (422), (333) and (622) were not present in our synthesized CuFe2O4. The synthesized CuFe2O4 was found out with FCC structure. Lattice constants have been presented in table 1.\nIn sample K2A six peaks out of eight major peaks were found out in close agreement with the standard patterns of CuFe2O4. The peaks with miller indices (111) and (422) did not match with the standard pattern of CuFe2O4, Fig. 2. The following miller indices (400) and (300) were not present in the synthesized CuFe2O4. The structure CuFe2O4 was found to be FCC. Lattice constants have been listed in table 2.\nTable 1: Peak analysis of XRD Pattern of Sample K1A\n2 (deg)###(deg)###I/Imax %###(hkl)###d-value###a (A)\nThe above table confirmed the miller indices and lattice constants. The average lattice constant was 8.24(?). The X-ray density was calculated 5.67g/cm3!.\nTable 2: Peak analysis of XRD Pattern of Sample K2A\n2 (deg)###(deg)###I/Imax %###(hkl)###d-value###a (A)\nThe above table confirmed the miller indices and lattice constants. The average lattice constant was 8.31(?). The X-ray density was calculated 5.53g/cm3!.\nHomogeneous CuFe2O4 nanoparticles were prepared by simple and economical chemical co-precipitation method at room temperature. It is reported that the water bath temperature, 85 oC, is the temperature where the most suitable results can be obtained.\nSamples Particle size (XRD)\nTabTable 3: Particle sizes obtained from X-ray diffraction technique.\nThe authors would like to acknowledge Department of Physics University of Agriculture Faisalabad for technical assistance and greatly indebted to the University Sains Malaysia (USM) for providing state of the art facilities and graduate assistantship (GA).\n1. Z. Huang, G. Yin, X. Liao, Y. Yao and Y. Kang. Preparation and magnetic properties of Cu-ferrite nanorods and nanowires. Journal of Colloid and Interface Science, 317, 530-535(2008)\n2. C. R. Hendries and W. R. Amarakoon. Processing of Maganese Zinc Ferries for High-Frequency Switch-Mode Power Supplies. C. Bulliten, 70, 890-896(1991)\n3. B. Jeyadevan, C. N. Chinnasamy, K. Shinoda and K. Tohji. Mn-Zn ferrite with higher magnetization for temperature sensitive magnetic fluid. Journal of Applied Physics, 93, 450-455(2003)\n4. H. Y. Luo. Z. X. Yue and J. Zhou. Synthesis and high-frequency magnetic properties of sol-gel derived Ni-Zn ferrite for sterite composites. Journal of Magnettism and Magnetic Materials, 210,104-108(2000)\n5. M. Feder, O. Caltuns and V. Valceanu. Microstructure and magnetic properties of ni-zn-cu ferrites sintered at different temperatures. Fizica Starii Condensate, 98-103(2001)\n6. M. M. Haque. M. Huq and M.A. Hakim.. Influence of CuO and sintering temperature on the microstructure and magnetic properties of Mg-Cu-Zn ferrites. Journal of Magnetism and Magnetic Materials, 320, 2792-2799(2008)\n7. P. P. Hankare, P.D. Kamble, M.R. Kadam, K.S. Rane and P.N. Vasambekar. Effect of sintering temperature on the properties of Cu-Co ferrites prepared by oxalate precipitation method. Materials Letters, 61, 2769-2771(2007)\n8. H. Wang, J. Liu, W. Li, J. Wang, L. Wang, L. Song, S. Yuan and F. Li. Structural, dynamic magnetic and dielectric properties of Ni0.15Cu0.2Zn0.65Fe2O4 ferrite produced by NaOH co-precipitation method. Journal of Alloys and Compounds, 461, 373-377(2008)\n9. H. Yang, J. Yan, Z. Lu, X. Cheng and Y. Tang. Photocatalytic activity evaluation of tetragonal CuFe2O4 nanoparticles for the H2 evolution under visible light irradiation. Journal of Alloys and Compounds, 33, 568-575(2008)\n10. Y. M. Z. Ahmed, M. M. Hessien, M. M. Rashad and I.A. Ibrahim. Nano-crystalline copper ferrites from secondary iron oxide (mill scale). Journal of Magnetism and Magnetic Materials, 321, 181-187(2009)\n11. D. Gingasu, I. Mindrua, L. Patrona and C. Cizmas. Tetragonal copper ferrite obtained by self-propagating combustion. Journal of Alloys and Compounds, 460, 627-631(2008)\n12. T. Liu, L. Wang, P. Yang and B. Hu. Preparation of nanometer CuFe2O4 by auto-combustion and its catalytic activity on the thermal decomposition of ammonium perchlorate. Materials Letters, 62, 4056-4058(2008)\n13. W. Lv, Bo Liu, Z. Luo, X. Ren and P. Zhang. XRD studies on the nanosized copper ferrite powders synthesized by sonochemical method. Journal of Alloys and Compounds, 465, 261-264(2008)\n14. I.H. Gul, W. Ahmed and A. Maqsood. Electrical and magnetic characterization of nanocrystalline Ni-Zn ferrite synthesis by co-precipitation route. Journal of Magnetism and Magnetic Materials, 320, 270-275(2008)\n15. H. C. Shin, S. C. Choi, K. D. Juung and S. H. Han. Mechanism of M ferrites (M = Cu and Ni) in the CO2 Decoposition Reaction. Chem. Mater, 13, 1238-1242(2001)\n16. B. D. Cullity. Elements of x-ray diffraction. 2nd edition. Addison wesley publishing company, canada(1978)\n17. R. K. Selvan, C. O. Augustin, V. Sepelak, L. J. Berchmans, C. Sanjeeviraja and A. Gedanken. Synthesis and characterization of CuFe2O4/CeO2 nanocomposites. Materials Chemistry and Physics, 112, 373-380(2008)\n18. J.A. Gomes, M.H. Sousa, G. J. da Silva, F.A. Tourinho, J. Mestnik-Filho, R. Itrid,G.de M. Azevedo, J. Depeyrot. Cation distribution in copper ferrite nanoparticles of ferrofluids:A synchrotron XRD and EXAFS investigation. Journal of Magnetism and Magnetic Materials, 300, 213-216(2006)\nDepartment of Physics, University of Agriculture, 38000 - Faisalabad, Pakistan\nSchool of Physics, Universiti Sains Malaysia (USM), 11800-Penang, Malaysia\nSchool of Chemical Sciences, Universiti Sains Malaysia (USM), 11800-Penang, Malaysia"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2a2cde69-a4a1-4292-a7fd-7540cb6b4c24>","<urn:uuid:3f529b7f-20e8-40bf-aa91-d7536ba87103>"],"error":null}
{"question":"How do the water treatment goals differ between the citrus juice company in Zimbabwe and the Government Analyst Laboratory's water analysis unit?","answer":"The citrus juice company's water treatment focuses on meeting environmental discharge requirements and making wastewater suitable for irrigation, achieving an average pH of 7.6 that meets legal disposal standards. In contrast, the Government Analyst Laboratory's water unit has broader objectives - they chemically analyze potable water samples, effluent and waste water both to determine suitability for human consumption and to control pollution in public waterways, while also providing technical advice to clients.","context":["Redesigning Waste Water Treatment Process in View of Utilising the Water: A Case Study at a Citrus Juice Processing Company in Zimbabwe\nInternational Journal of Nutrition and Food Sciences\nVolume 3, Issue 6-1, November 2014, Pages: 15-21\nReceived: Sep. 9, 2014;\nAccepted: Sep. 15, 2014;\nPublished: Sep. 17, 2014\nViews 3011 Downloads 150\nBruce Chitunhu, Department of Food Processing Technology, School of Industrial Science & Technology, Harare Institute of Technology, Ganges Rd, Box BE 277, Belvedere, Harare, Zimbabwe\nRaphael Kwiri, Department of Food Processing Technology, School of Industrial Science & Technology, Harare Institute of Technology, Ganges Rd, Box BE 277, Belvedere, Harare, Zimbabwe\nPerkins Muredzi, Department of Food Processing Technology, School of Industrial Science & Technology, Harare Institute of Technology, Ganges Rd, Box BE 277, Belvedere, Harare, Zimbabwe\nDisposal of industrial wastewater from a local citrus juice processing company was found to be resulting in negative environmental impacts on the vegetation surrounding the disposal site and the treated wastewater did not meet the minimum purity requirements stated in the Zimbabwe Environmental Management Act (Chapter 20:27). This project was aimed at redesigning the process so that wastewater which meets required legal requirements for quality is discharged and made available for different purposes. Samples of wastewater were collected at the company’s effluent processing plant and experiments were conducted to assess its overall quality. The process was redesigned based on the shortcomings found. The redesigned wastewater treatment prototype gave water that was legally fit for disposal, with an average pH of 7.6 and could be used for irrigation purposes.\nRedesigning Waste Water Treatment Process in View of Utilising the Water: A Case Study at a Citrus Juice Processing Company in Zimbabwe, International Journal of Nutrition and Food Sciences. Special Issue: Optimizing Quality and Food Process Assessment.\nVol. 3, No. 6-1,\n2014, pp. 15-21.\nAhmed A.H, Moustafa M, T.A, 1986, Growth and survival of Yersenia enterocolitica in yoghurt, J. Fd Prot 49\nAnonymous, 2003, Analysis of microbial hazards related to time/temperature\nAkuzuoOfoefule, 2011,Waste Water: Treatment Options and its Associated Benefits, Waste Water - Evaluation and Management, SebastiÃ¡nGarcÃa\nAlissa Hamilton, 2010,Squeezed: What You Don't Know about Orange Juice , Yale Agrarian Studies,\nAmerican Society for Horticultural Science,2008, \"Reclaimed Wastewater Benefits Florida's Citrus Orchards.\nBuchanan, Richard, 2000,\"Good Design in the Digital Age.\" GAIN: AIGA Journal of Design for the Network Economy.\nCandela L, Fabregat S, Josa A, Suriol J, Vigues N, Mas J, 2007,Assessment of soil and groundwater impacts by treated urban wastewater reuse. A case study: Application in a golf course, Girona, Spain, 374:26-35.\nChang, E.-W. Lee, S. Oh, Y. Kim, 2005, Comparison of SAR (sodium adsorption ratio) between RO and NF processes for the reclamation of secondary effluent Water Science and Technology, pp. 313–318\nDuncan RR, Carrow RN, Huck MT, 2009,Turf grass and Landscape Irrigation Water Quality \"Assessment and Management\". New York: CRC Press;\nEnvironmental Management (Water Quality Standards) Regulations, 2007.\nEui-Chan Jeon, Hyun-Keun Son, and Jae-Hwan Sa, 2009, Emission Characteristics and Factors of Selected Odorous Compounds at a Wastewater Treatment Plant\nHeidarpour M, Mostafazadehfard B, Abedikoupai J, Malekian R, 2007,The effects of treated wastewater on soil chemical properties using subsurface and surface irrigation methods. Agricultural Water Management , 90:87-94.\nJ Hazard Mater M:, 2004 , Water strategies and potentional of water reuse in south Mediterranean countries.Desalination , 165:31-41.\nKansel, B.D. and Singh, J. (1983), Influence of the municipal wastewater and soil properties on the accumulation of heavy metals in plants. In: Proceedings of International Conferences of Heavy Metals in the Environment, Heidelberg, Germany, CEP Consultants, Edinburgh, 413–416.\nKhan, A.R, 1995, Appropriate Wastewater Treatment Processes Pakistan, Master of Science Research of Loughborough,\nKolko, Jon, 2007, Thoughts on Interaction Design, Brown Bear LLC,\nKothari, C. R, 2004 ,Research Methodology; Methods and Techniques, WishwaPrakashan, India.\nLoetscher T, 1998,SANEX Sanitation Expert Systems, Milan\nLevine, 2004, \"Peer Reviewed: Recovering Sustainable Water from Wastewater\". Environmental Science & Technology\nLudwig, S, 1998,Decentralized Wastewater Treatment in Developing Countries Bremen Overseas Research and development Association, Bremen.\nLevine A, Asano T (2004). Recovering sustainable water from wastewater. Environ SciTechnol; 201A\nMann, H.T., Williamson, D, 1982,Water Treatment and Sanitation, Intermediate Technology, England, Nottingham,\nMara, D, 1991, Sewage Treatment in Hot Climates, John Wiley and Sons. Neptune Pacific Ltd\nMcyintre, J.P, 2006, Industrial Water Reuse and Wastewater Minimization, Trevose: GeWater\nMorgan, Kelly T., Wheaton, T. Adair, Parsons, Larry R., Castle, William S, 2008, Effects of Reclaimed Municipal Waste Water on Horticultural Characteristics, Fruit Quality, and Soil and Leaf Mineral Concentration of Citrus. HortScience, 43: 459-464\nMujeriego R, Sala L (1991). Golf course irrigation with reclaimed wastewater. Water Sci Technol;24(9):161–72 p.\nMaczulak, Anne Elizabeth (2010). Pollution: Treating Environmental Toxins. New York: Infobase Publishing. p. 120.\nOnyechere L.N., U. Ngodi and M.N. Ezike, 2007, Anaerobic biotechnology for sustainable waste treatment. A review. J. Res. Bioscience\nPickford, J, 1991, the Worth of Water. Technical Briefs on health, water and sanitation Russell Press Ltd., Nottingham.\nQadir M, Oster JD, 2004,Crop and irrigation management strategies for saline-sodic soils and waters aimed at environmentally sustainable agriculture.Sci Total Environ 323:1-19.\nRenzetti, S. &Dupont, D, 1996, \"Water Use in the Canadian Food and Beverage Industry,\" York (Canada)\nRossi F,Mapei,LBonomo, 1991, Textile water reuse: Ecological bulletin e in northern Italy,Milan\nShah, K.L,1994, An Overview of Physical, Biological, and Chemical Processes for Wastewater Treatment, In: Process Engineering for Pollution Control and Waste Minimization, New York.\nSozanskyy S, 1998,Two-level clearance of wastewater ; Food and processing industry, York (Canada)\nStandard Methods for the Examination of Water and Wastewater, 1998, (20th ed.)American Public Health Association/American Water Works Association/Water Environment Federation, Washington DC, USA.\nTchobanoglous, G., 1995. Decentralized systems for wastewater management. Decentralized systems for wastewater management. In: 24th Annual WEAO Technical Symposium, Toronto, Canada.\nUNEP, 2007, Utilization in African Beverage Industries: Current Practices and Prospects. Nairobi\nWakefield, J. M, 1953,The results of research on citrus waste disposal. Fla. State Hort.\nWalker, Andrea ,2009, \"Ask an Academic: Orange Juice\". The New Yorker.\nXu J, Wu L, Chang AC, Zhang Y, 2010,: Impact of long-term reclaimed wastewater irrigation on agricultural soils: A preliminary assessment\nYacyk A.V , 2007,Water resources in the context of ecological safety and sustainable development of a state","THE GOVERNMENT ANALYST LABORATORY\nThe Government Analyst is a national laboratory under the Ministry of Health and Child Care. It is mandated to analyse samples for medico-legal purposes and in support of government programmes, to co-ordinate the national food control program and food standards development work and to provide technical advice and information on issues pertaining to food quality and standardization, water treatment and industrial processing and environmental pollution. The laboratory is also the National Codex Contact Point and the secretariat to Food Standards Advisory Board.\nTo be the best Regulatory /Analytical Laboratory for Food, Water, Toxicology/Clinical and Industrial inputs/product samples analysis and information sharing.\nTo promote health for all Zimbabweans through the provision of laboratory analysis, scientific and technical information and to participate in health preventive and support programmes.\n- To analyse all submitted samples (Food, Water, Toxicology/Clinical and Industrial inputs or products timely.\n- To proffer technical advice to assist clients’ decision making with reference to above.\n- To review the Food and Food Standards Regulations as necessary and appraise the Minister of Health and Child Care, Top Management Team and the general public accordingly.\n- To verify and certify products hence facilitate fair food trade and access to safe food and water by the public.\n- To procure equipment and instruments meeting ISO 17025 standards and train users to improve laboratory capacity.\n- To compile and share information of non-complying sample analyses results with stakeholders for necessary intervention.\n- To implement ISO 17025 Quality Standards for QI/QA/TQM and sustain access by clients to quality analytical service as and when required.\n- To house and offer secretariat service to the National CODEX Committee.\n- To house and offer secretariat service to the Food Standards Advisory Board (FSAB) See Below for TOR for the FSAB.\nQuality Policy Summary\nWe are a laboratory committed to the provision of quality analytical services in the areas of Food, Water, Clinical, Toxicology and Industrial inputs/products. We aim to satisfy stated and implied need and requirements of our customer through the implementation of ISO 17025 Quality Management Systems. The Laboratory offers affordable, accurate and timely service. Our objective is to continually improve efficiency and effectiveness of our operations and information sharing to protect public health. The personnel working at the laboratory is trained to be familiar with the Quality Assurance system.\nConfidentiality, Honest, Affordable, Client focus, Result oriented, Accountable and Timeliness.\nGOVERNMENT ANALYST LABORATORY UNITS\nThe unit chemically and microbiologically analyse samples for compliance with national and international standards. It proffers technical information to clients on matters related to food composition and legislation governing the quality and safety of food.\nThe unit chemically analyses potable water samples, effluent and waste water to determine their suitability for human consumption and /or discharge into public water ways for pollution control respectively. It also offers technical advice to clients.\nToxicology and Clinical Unit\nThe unit analyses post mortem specimens for toxic substances to establish the cause of sudden death cases as a service to the Ministry of Home Affairs and give evidence in the court of Law to finalise deceased estate cases. It also handles clinical samples for Therapeutic Drug Monitoring, Emergency Toxicology and Heavy metal analysis as well as monitoring occupational exposure to hazardous substances as a service to health care institutions, companies and individuals.\nIndustrial Chemistry Unit\nThe unit analyses industrial inputs and products for compliance with the Hazardous Substances and Articles Regulations. It also analyses malaria spraying chemicals for efficacy, residual pesticides if fruits and vegetables and alcoholic beverages for alcohol content.\nIodine Deficiency Disorders (IDD) Unit\nThe unit analyses iodine in salt to asses adequate iodization and urine samples as biological indicators for iodine deficiency disorders.\nLegislation, Regulations Guideline and Policies\n- Constitution of Zimbabwe Amendment (No.20 Act 2013 Section 77.\n- Public Health Act\n- Food and Food Standards Act(CAP 321)\n- Hazardous Substances Control Act (2000)\n- Drugs and Allied Substances Act\n- Dairy Act\n- Water Act\n- Health Services Regulations(2006)\n- Guidelines for Drinking Water Quality WHO(2008) Regulations\n- Water(Effluent and Waste Water Standards) Regulations/(2000)\n- Codex Alimentarius Commission Guidelines\n- Food Security and Nutrition Policy\n- Environmental Health Department\n- Local Authorities\n- Government Departments\n- Private Clients\n- Non- Governmental Organizations\nTHE ZIMBABWE FOOD STANDARDS ADVISORY BOARD (FSAB).\nThe Food and Food Standards Act Chapter 321 was promulgated in 1971. The purpose was to provide for the sale, importation for sale of food in a pure state; to prohibit the sale, importation and manufacture for sale of food which is falsely described; and to provide for the fixing of standards relating to food and matters incidental hereto.\nThe Act was amended in 1973, 1976, 1979, 1981, 1988 and 1994.\nThe latest version is the Food and Food Standards Act Chapter 15:04. Section 18 of this Act provides for the FSAB and its membership.\nIn accordance with section 27 of the FFA Chapter 15:04 the Minister gazetted Statutory Instrument 322 of 1995 which provides for the FSAB.\nFFSA Chapter 18 subsection 1(a) to 1(l) provides for members of the FSAB and the institutions they come from. The Secretary of Health is a member of the FSAB. Subsection (6) provides for the secretary to the board who is usually the Deputy Director-Food Control in accordance with Public Service job description as advised by the Minister of Health.\nAll members are appointed on a three year term by the Minister of Health.\nTERMS OF REFERENCE OF THE FSAB\nIn accordance with Section 18 subsection 5 of the Act, the broad function of the FSAB is to advise the Minister of Health on all maters relating to Food and Food Standards.\nSPECIFIC TERMS OF REFERENCES ARE:\nNational Food Regulations, Food Standards Laws\nand Food Safety and Quality Policy.\n1. To formulate food safety and quality policy and advise the Minister of Health accordingly.\n2. To amend and revise food laws and regulations then advise the Minister of Health.\n- To consider applications for use or introduction of new food products, ingredients or food articles on the market and advise the Minister accordingly.\n- To certify on behalf of the Minister, food products for purposes of local sale, export and import.\n- To carry out risk analysis for food contaminants and advise the Minister accordingly.\n- To coordinate food safety control programme along the supply chain (farm to table approach)\n- To assist in the education of consumers on food safety and quality issues.\n- To make regulations to provide for all matters relating to the Board, Including:\n- Remuneration, allowances, and conditions of office of members of the Board and its committees.\n- The vacation of office by members of the Board and its committees.\n- The procedure of the Board and its committees\n- The establishment of committees by the Board and generally provide for the efficient carrying out of the objects and purposes of this Act.\n- To function as the National CODEX Committee and to deliberate on issues to be discussed at international CODEX meetings and come up with Zimbabwe position on food standards issues.\n- To create awareness of CODEX recommended codes of practices, guidelines and standards HACCP, GMP, GHP and GAP among food producers.\n- To make food regulations that provide standards for:\n(a) The composition, strength, potency purity, quality or other property of any food or of any ingredient or component part thereof.\n(b) The nature or proportion of any foreign matter, which may be present in any food as a result of unavoidable or necessary admixture therewith during collection, preparation or manufacture.\n(c) The composition of any mixed or compounded food.\n(d) the proportions in which any substance or ingredient may be contained in food or which may be added to or mixed or diluted with food or prescribe any substance or ingredient which may not be contained in, added to or mixed or diluted with food.To make regulations for the methods, appliances and processes to be used in or applied to or not to be used in or applied to the manufacture, preparation, preservation or packing of food or any of its substances or ingredients.\n(e) food, if in the Minister’s opinion:\n- The nutritional value of the food will be improved by the addition of the substance or the application of the process.\n- It is in the interests of the health of the public for the substance to be added or the process to be applied to the food.\n(f) to provide for the inspection and detection or removal for analysis or examination of any package, wrapper, container, appliance or other article used in connection with any food.\n(g) control of food import and export.\n(h) the minimum requirements in respect of;\n- Premises used for;\n- Employees engaged in the sale, manufacture, production, processing and treatment of foods.\n(i) to provide for the method of testing, examining or analysing any food in terms of this Act and the form of affidavit to be used by an analyst in making a report upon such examination or analysis.\n(j) to provide for the mode of labelling food or packages containing the same, or bulk stock from which food is taken for sale, and the matter to be contained or not to be contained on such labels.\n(k) the control and regulation of advertisements relating to any food and, in particular, the prohibition of any such advertisement, which is false or misleading.\n(l) preservative to be used by inspectors for preventing decomposition or other change in samples taken under this Act.\n(m)Prescription of the duties of analysts’ and inspectors under this Act.\n- To act as National CODEX Committee (NCC) and coordinate work of the 9 Technical Sub Committees of\n- TC 1 High Level Issues\n- TC 2 Fresh Produce Items\n- TC 3 Processed Food Items\n- TC 4 Hygiene Matters-\n- TC 5 Food Contaminants-\n- TC 6 Nutrition-\n- TC 7 Food Inspection-\n- TC 8 Methods of Analysis and Sampling\n- TC 9 Miscellaneous Items\n- Documents, Reports, Strategies\n- Composition of FSAB\n1. Chairman: A member of the law society of Zimbabwe,\n2. Secretary: Gvt Analyst Lab Deputy Director –Food Control,\n3. Other members: Director - Government Analyst Laboratory\n4. Secretary of Health appointee Environmental Health\n5. Secretary of Health appointee Nutrition Department\n6. Principal Director: Veterinary Services or appointee.\n7. Principal Director: Agriculture Research and Extension services. 8. Representative: Ministry of Industry & International Trade.\n9. Representative: Biosafety Authority of Zimbabwe.\n10. Representative: Farmers Organisations.\n11 Representative: Retailers Association of Zimbabwe.\n12. Representative: Consumer Council of Zimbabwe.\n13. Representative: Municipal Health Officers Association of Zimbabwe. 14. Representative: Standards Association of Zimbabwe.\n15. Representative: Food Manufacturers, Zimbabwe.\n16. Representative: Food Manufacturers, Zimbabwe.\n2. Deputy Director- Food Control\n3. Deputy Director- Toxi/Clinical and Admin\n5. Further information on the NCC can be found on its website www.zimcodex.gov.zw."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4d0efa0f-7541-4603-92e0-448fd24b6bd2>","<urn:uuid:2a5de2c2-7e56-4b31-92e5-9c71ad84ba13>"],"error":null}
{"question":"What are the key similarities between how Alzheimer's disease and ALS spread through the brain?","answer":"Both Alzheimer's disease and ALS show a pattern of regional spread through the brain, though through different mechanisms. In Alzheimer's, the spread occurs through the buildup of plaques (beta-amyloid) and tangles (tau proteins) that eventually cause neurons to die. In ALS, the spread occurs in a contiguous pattern through the motor neuron network, with protein aggregates like TDP-43 and FUS forming in affected neurons and potentially spreading in a prion-like fashion. In both conditions, the disease typically starts in a discrete location and progressively spreads to adjacent anatomical regions.","context":["Alzheimer’s disease is a degenerative neurological condition that is known for its debilitating symptoms, such as memory loss, confusion, and loss of identity. The two hallmarks of Alzheimer’s disease are the formation of plaques (beta-amyloid) and tangles (tau proteins)in the brain.\nOver time, these build up to a point where symptoms begin to develop and, eventually, cause the neurons in our brains to die. While it may be a disease of the brain, researchers are currently investigating how its development could be related to the rest of the body.\nHeart disease has historically been directly related to gum disease (aka chronic periodontitis). It appears that it also correlates to brain health; more specifically, to Alzheimer’s disease. Research seems to be indicating that Alzheimer’s may be the result of an infection caused by a particular type of bacteria usually implicated with periodontal disease (also known as gum disease).\nThe bacteria in question? Porphyromonas gingivalis. It’s not only found in the mouth but in the respiratory tract, gastrointestinal tract, colon, and vagina when suffering from bacterial vaginosis.\nSeveral scientific studies (including the examples below) back up the hypothesis that the two are indeed linked; while some researchers are trying to work out the exact mechanisms of the illness:\nA study of 597 men by Elizabeth Krall Kaye et. al, showed a correlation between the appearance of gum disease, tooth loss and a greater risk of cognitive decline. Another study found that tooth loss also appeared to be linked to Alzheimer’s and that the more teeth that fell out, the more the illness increased.\nThis evidence implies greater importance to keeping an optimal level of oral health than originally imagined.\nIn 2017 a study conducted in Bristol (UK) led scientists to believe that the pathogen was the cause for the neural inflammation previously associated with the disease. “We now have strong evidence connecting P. gingivalis and Alzheimer’s pathogenesis, but more research needs to be done before P. gingivalis is explicitly implicated in the causation or morbidity of AD (Alzheimer’s Disease),” reported David Emery.\nThe proof Mr. Emery was hoping for may have been discovered when a more recent study was published early this year (2019) by microbiologist, Jan Potempa. It helps to solidify that the bacteria implicated, Porphyromonas gingivalis, is also found in the brains of deceased Alzheimer patients. To put it simply, the more chronic exposure a person has to this bacteria, the higher the risk of experiencing cognitive in later life.\nAuthor of the latest, 2019 study, Stephen Dominy concurs that its findings have linked the likelihood of the cause-effect relationship shared by gum disease and Alzheimer’s. While not definitively proven, the findings certainly add to the weight of evidence already discovered: “Now, for the first time, we have solid evidence connecting the intracellular, Gram-negative pathogen, P. gingivalis, and Alzheimer’s pathogenesis while also demonstrating the potential for a class of small molecule therapies to change the trajectory of disease.”\nThere is no cure for Alzheimer’s disease, but these new findings add to the evidence that it is linked to and, at least partially caused by P. gingivalis. By understanding some of the root causes of the disease, this could help with the development of new preventative measures.\nThere are numerous preventative actions. These include getting good nights sleep, developing your brain by constantly learning and developing new skills, exercising regularly, eating well.\nIn conclusion, Alzheimer’s still has some way to go in terms of a cure, but these new findings give new hope that its development can be averted. At the very least, this study gives us all more of incentive to maintain good oral health and to visit the dentist regularly.","| Article Access Statistics|\n| Viewed||2961 |\n| Printed||83 |\n| Emailed||2 |\n| PDF Downloaded||62 |\n| Comments ||[Add] |\n| Cited by others ||1 |\nClick on image for details.\n|Year : 2013 | Volume\n| Issue : 2 | Page : 107-110\nProtein aggregates and regional disease spread in ALS is reminiscent of prion-like pathogenesis\nDepartment of Neurology, University of Miami Miller School of Medicine, Clinical Research Building, 1120 NW 14 Street, Suite 1317, Miami, FL 33136, USA\n|Date of Submission||27-Feb-2013|\n|Date of Decision||08-Mar-2013|\n|Date of Acceptance||05-Mar-2013|\n|Date of Web Publication||29-Apr-2013|\nDepartment of Neurology, Clinical Research Building, 1120 NW 14 Street, Suite 1317, Miami, FL 33136\nSource of Support: None, Conflict of Interest: None\nAmyotrophic lateral sclerosis (ALS) typically commences in a discrete location in a limb or bulbar territory muscles and then spreads to the adjacent anatomical regions. This pattern is consistent with a contiguous spread of the disease process in motor neuron network resulting in progressive motor weakness. The etiology of ALS onset and the mechanism of the regional ALS spread remain elusive. Over the past 5 years, identification of mutations in two RNA binding proteins, trans active response (TAR) DNA-binding protein (TDP-43) and fused in sarcoma (FUS), in patients with familial ALS has led to a major shift in our understanding of the ALS disease mechanism. In addition to their role in RNA metabolism, TDP-43 and FUS form protein aggregates in the affected neurons. More recent findings demonstrating that both TDP-43 and FUS contain glutamine/asparagine (Q/N) residue-rich prion-like domains have spurred intense research interest. This brief review discusses the prion-related domains in TDP-43 and FUS and their implication in protein aggregate formation and disease spread in ALS.\nKeywords: ALS, aggregates, FUS, prion, Q/N domain, TDP-43\n|How to cite this article:|\nVerma A. Protein aggregates and regional disease spread in ALS is reminiscent of prion-like pathogenesis. Neurol India 2013;61:107-10\n| » Introduction|| |\nFirst described in clinical detail by Charcot  in 1869, amyotrophic lateral sclerosis (ALS or motor neuron disease, MND) is one of the earliest known neurodegenerative diseases. Following Charcot's initial description, Gowers  in 1886 commented on variable anatomic sites of disease-onset and independent and variable abnormalities of the upper motor neuron and lower motor neuron in ALS, thus broadening the clinical spectrum of the disease. Gowers particularly emphasized focal onset and contiguous spread of the disease process in his description, which is still a classic: …from the part first affected the disease spreads to other parts of the (same) limb. Before it has attained a considerable degree in one limb, it usually shows itself in the corresponding limb on the other side (homologous part)…\nThis original and astute clinical observation by Gowers has been repeatedly witnessed by practicing neurologists well over a century. The classical focal and asymmetric onset of ALS in spinal or bulbar region and then its spread to contiguous group of motor neurons is now so well known that an experienced physician can often predict the 'logical' spread in a particular case.  Recent detailed autopsy studies of ALS patients have confirmed that loss of motor neurons is most pronounced at the site of onset and diminishes in a gradient fashion with further distance from that site.  The pathophysiologic mechanism underlying the focal onset and regional spread of ALS is unclear. Over the past two decades, several aberrant phenomena including excessive oxidative stress, excitotoxicity, mitochondrial dysfunction, inflammation, and altered axonal transport have been implicated in ALS pathogenesis. However, it is not easily discernable how any of these generalized processes could by itself explain the focal initiation or the progressive spread of the disease through the motor neuronal pool.\nApproximately 10% of ALS cases are familial (fALS), mostly autosomal dominant, and the rest are sporadic (sALS). For almost 15 years, the only gene clearly associated with fALS was the Cu-Zn superoxide dismutase 1 (SOD1) gene, which accounts for 20% of fALS cases. The identification of SOD1 mutations  in 1993 ushered in the molecular era of ALS research, and significant insight into ALS pathogenesis has been gained through identification of pathways directly affected by the toxicity of mutant SOD1. Two main discoveries in mutant SOD1-mediated ALS in rodent models are the demonstration that the SOD1 protein aggregates produce a toxic gain of function that causes neuronal loss and the disease can spread in a noncell autonomous fashion in the nervous system. ,\nA major shift in our understanding of ALS pathogenesis occurred in 2006 with the identification of a 43-kDa transactive response (TAR) DNA-binding protein (TDP-43) as a key pathological substrate of cellular inclusions in sALS and non-SOD1 fALS, and frontotemporal lobar degeneration with ubiquitinated inclusions (FTLD-U).  It was soon followed in 2008 by the successful discovery of dominant mutations in the TDP-43 gene as a primary cause of ALS,  thus providing the proof of principle that aberrant TDP-43 can trigger neuronal degeneration and cause ALS. The identification of TDP-43 mutations was soon followed by the discovery of mutations in another RNA/DNA-binding protein, fused in sarcoma (FUS) in 2009, as a primary cause of fALS.  Together, TDP-43 and FUS gene mutations account for approximately 10% of the fALS cases. Both in TDP-43- and FUS-linked ALS, the age and site of disease-onset and clinical progression are variable, as in sporadic ALS, and both show characteristic pathological features of the sALS. Incomplete penetrance has been documented for several of TDP-43 and FUS mutations, accounting for some sALS cases.\nBoth TDP-43 and FUS are predominantly nuclear proteins involved in diverse aspects of RNA metabolism;  however, in ALS disease tissue both are observed as aggregates in the cytosol of affected neurons. ,,, This finding suggests that aberrant protein aggregation and its cytosolic accumulation may play a key role in ALS pathogenesis, akin to the central role of protein misfolding and aggregations observed in other neurodegenerative diseases.  Interestingly, both TDP-43 and FUS contain 'prion-related' glutamine/asparagine (Q/N) rich domains and in the case of TDP-43, almost all the ALS-associated mutations occur in this region. \nIn living organisms, proteins are regularly synthesized and degraded each day. During protein synthesis, nascent polypeptide chain exits from ribosomal tunnel and passes through a highly organized and precisely monitored protein quality control systems (see review , ). The newly formed protein is folded in its three dimensional structure, steered by molecular chaperons, and transported to appropriate subcellular site(s) where it executes its physiological functions. In prokaryote and eukaryote cells, an equally efficient and cooperative protein clearance system known as ubiquitin-proteasome system (UPS) and lysosomal autophagy also exists to clear the abnormally misfolded proteins and protein aggregates in order to maintain cellular proteostasis and normal health (reviewed elsewhere  ). For clearance through UPS, misfolded peptide is first targeted and tagged to ubiquitin, a process known as ubiquitination. Ubiquitination requires an isopeptide bond between the targeted protein and ubiquitin; the covalent isopeptide bond is generated between the glycine and lysine aminoacids of the protein moieties. In this context, it is interesting that most pathogenic ALS-associated TDP-43 and FUS mutations occur in the glycine-rich regions of these proteins. Also interestingly, rare mutations in ubiquilin2  and SQSTM1 (sequestone1/p62),  which are involved in the clearance of misfolded and aggregated proteins, are shown to cause TDP-43 positive cellular aggregates and ALS in rare fALS cases. , Excess of misfolded proteins and uncleared aggregates are inherently toxic to cell, a phenomenon well demonstrated in in-vitro cell models.  Why do certain proteins, such as TDP-43, FUS, amyloid-β, tau, etc., aggregate, how these aggregates cause cell toxicity and whether such aggregate-related toxicity is transferrable from cell to cell is an area of intense current research in neurodegenerative diseases.\nCurrently, prion protein is the only known example of a protein capable of propagating a self-replicating conformation that can spread like infectious particles (transmissible spongiform encephalopathies) across cells and individuals. However, additional proteins are now recognized that can exhibit 'prion-like' behavior under certain circumstances. Such proteins with more than one conformation are known to exist in yeast, invertebrate and mammalian cells.  In these cases, unlike in human prion diseases, the adoption of an alternate protein conformation and template-based spreading of this altered conformation by additional conversion of normal forms, appear not to be deleterious and cause disease, but instead regulates the physiological function of the aggregated protein.\n'Prion-like' behavior of proteins is best characterized in yeast Sup35, Ure2, and Rnq1 proteins.  For example, the Sup35 protein is normally required for stop-codon recognition and translational termination; however, under certain stressful conditions it can form a self-propagating fibrillar-b-sheet conformation transmissible to offsprings. Such self-propagating fibrillar-b-sheet conformation seems to be dependent on the N-terminal region, which is characteristically rich in Q/N residues. Because this Q/N rich region is required for prion-like propagation, it is referred to as the 'prion domain'. Induction of the yeast Sup35 prion state leads to loss of Sup35 function and thereby widespread read through of stop codons, allowing the rapid emergence of novel phenotypes, a molecular adaptive strategy for yeast survival under stressful conditions. \nOther examples of prion-like behavior and Q/N rich proteins include CREB (an RNA binding protein) protein in Aplysia  and Pumilio protein in Drosophila.  Q/N rich CREB and Pumilio proteins regulate synaptic activity  and postsynaptic translational suppression,  respectively. Finally, the mammalian proteome contains several Q/N rich prion domains that may similarly use self-aggregation to modulate their activity. A well-studied example is the RNA binding protein TIA1, which is a key component of stress granules, cytoplasmic RNA-protein complexes formed under conditions of cellular stress, which mediates mRNA translational suppression.  The prion-related Q/N domain of TIA1 is essential for self-aggregation and stress granule formation. Thus, one consistent theme for proteins containing prion-related Q/N rich domains from yeast to mammals is stimulus-induced (environmental stress-induced) conformational change leading to self-aggregation, which then alters protein function to orchestrate an adaptive response. Several algorithms based on the amino acid sequence have been used to predict proteins that bind to DNA/RNA and also contain prion-related domains in both yeast and human proteomes. Using one recent algorithm, Markov Model algorithm, trained on known yeast prion-domain containing proteins, FUS and TDP-43, were predicted as the 15 th and 69 th most likely to contain prion-related domains out of nearly 30,000 proteins in the human proteome.  Although the existence of the prion-related domains in TDP-43 and FUS appears convincing, investigations into their role in the normal and pathological functions of these proteins warrants further research.\nNeuronal cytoplasmic inclusions of TDP-43 and FUS are observed in cases of sporadic and familial ALS. For TDP-43, the C-terminal region is highly prone to aggregation, both as purified protein in vitro or when expressed as a fragment in yeast or cultured mammalian cells. ,, This strong tendency of the Q/N rich C-terminus of TDP-43 to self-associate and form aggregate is likewise consistent with the behavior of other prion-related Q/N domain containing proteins. Curiously, TDP-43 pathology (nuclear clearing and cytoplasmic aggregates) is observed not only in ALS, but also in affected brain regions in FTLD-U, Alzheimer disease, Parkinson disease and chronic traumatic encephalopathy, and in inclusion body myopathies.  This is quite consistent with the possibility that TDP-43 aggregation is part of a general response to cellular stress, and is likely mediated by the prion-related domain.\nAlthough wild-type TDP-43 and FUS readily aggregate in vitro, this generally does not occur in normal cells (without pathogenic mutation); the intracellular array of protein folding chaperones probably inhibits this phenomenon. In this context, it is known that the \"yeast prion\" domains can switch their conformation between two states: an intrinsic unfolded state and an aggregated state that can impose its conformation on its unfolded counterpart.  If such a phenomenon can occur with wild-type TDP-43 and FUS, then whether it indeed occurs in sALS and what tips the initiation and accumulation of aggregates remain important unresolved questions.\nRecently, attention has been focused on the concept that misfolded proteins involved in neurodegenerations (such as amyloid-β, tau, and synuclein) could be propagated from cell to cell in a prion-like fashion. , Although experimental evidence for this hypothesis is not robust yet, it is attractive as a potential explanation for the clinically observed spread of neurodegenerative disease through particular neuronal network. Given that prion-related Q/N domains are capable of developing altered conformers, which then can recruit aggregation of the native proteins, the TDP-43 and FUS provide a potential molecular substrate to study the spread of de novo and seeded aggregates in recently created TDP-43 and FUS animal models.\n| » Conclusion|| |\nThe identification of mutations in DNA/RNA binding proteins TDP-43 and FUS as causative of ALS, and the demonstration of TDP-43 as a major constituent of cytoplasmic inclusions in patients with sALS and non-SOD1 fALS, has been a major step toward understanding the pathobiology of the disease. Both these proteins appear to be aggregation-prone, and their ALS-linked mutants appear to exhibit greater degree of aggregation-propensity. TDP-43 and FUS also harbor prion-like domains, raising a tantalizing possibility that focal accumulation of cellular aggregates, and their progressive spread into the motor neuron pool, could be the underlying mechanism of the temporo-spatial spread of the disease. It will be interesting to see if seeded aggregates of these proteins are found to spread into the neuronal network of the nervous system. The emergence of protein aggregates as pathognomonic feature in ALS opens unparalleled opportunities toward therapeutic targets and drug discovery strategies based on the pathways of cellular protein aggregate formation and disease spread in ALS.\n| » References|| |\n|1.||Charcot JM. Sclerose laterale amyotrophique. Oeuvres Completes. Vol 2. Paris: Bureux du Progres Medical; 1897. p. 249-66. |\n|2.||Gowers WR. Manual of diseases of the nervous system. London: Churchill; 1886-8. |\n|3.||Verma A, Tandan R. RNA quality control and protein aggregates in amyotrophic lateral sclerosis: A review. Muscle Nerve 2013;47:330-8. |\n|4.||Ravits J, Laurie P, Fan Y, Moore DH. Implications of ALS focality: Rostral-caudal distribution of lower motor neuron loss postmortem. Neurology 2007;68:1576-82. |\n|5.||Rosen DR, Siddique T, Patterson D, Figlewicz DA, Sapp P, Hentati A, et al. Mutations in Cu/Zn superoxide dismutase gene are associated with familial amyotrophic lateral sclerosis. Nature 1993;362:59-62. |\n|6.||Boillée S, Yamanaka K, Lobsiger CS, Copeland NG, Jenkins NA, Kassiotis G, et al. Onset and progression in inherited ALS determined by motor neurons and microglia. Science 2006;312:1389-92. |\n|7.||Yamanaka K, Chun SJ, Boillee S, Fujimori-Tonou N, Yamashita H, Gutmann DH, et al. Astrocytes as determinants of disease progression in inherited amyotrophic lateral sclerosis. Nat Neurosci 2008;11:251-3. |\n|8.||Neumann M, Sampathu DM, Kwong LK, Truax AC, Micsenyi MC, Chou TT, et al. Ubiquitinated TDP-43 in frontotemporal lobar degeneration and amyotrophic lateral sclerosis. Science 2006;314:130-3. |\n|9.||Sreedharan J, Blair IP, Tripathi VB, Hu X, Vance C, Rogelj B, et al. TDP-43 mutations in familial and sporadic amyotrophic lateral sclerosis. Science 2008;319:1668-72. |\n|10.||Kwiatkowski TJ Jr, Bosco DA, Leclerc AL, Tamrazian E, Vanderburg CR, Russ C, et al. Mutations in the FUS/TLS gene on chromosome 16 cause familial amyotrophic lateral sclerosis. Science 2009;323:1205-8. |\n|11.||Verma A. RNA processing errors in amyotrophic lateral sclerosis. Ann Indian Acad Neurol 2011;14:231-6. |\n|12.||Walker LC, Diamond MI, Duff KE, Hyman BT. Mechanisms of Protein Seeding in Neurodegenerative Diseases. Arch Neurol 2012:70:304-10. |\n|13.||Lagier-Tourenne C, Polymenidou M, Cleveland DW. TDP-43 and FUS/TLS: Emerging roles in RNA processing and neurodegeneration. Hum Mol Genet 2010;19:R46-64. |\n|14.||Chhangani D, Mishra A. Protein Quality Control System in Neurodegeneration: A Healing Company Hard to Beat but Failure is Fatal. Mol Neurobiol 2013 [Epub ahead of print]. |\n|15.||Chen B, Retzlaff M, Roos T, Frydman J. Cellular strategies of protein quality control. Cold Spring Harb Perspect Biol 2011;3:a004374. |\n|16.||Sherman MY, Goldberg AL. Cellular defenses against unfolded proteins: A cell biologist thinks about neurodegenerative diseases. Neuron 2001;29:15-32. |\n|17.||Deng HX, Chen W, Hong ST, Boycott KM, Gorrie GH, Siddique N, et al. Mutations in UBQLN2 cause dominant X-linked juvenile and adult-onset ALS and ALS/dementia. Nature 2011;477:211-5. |\n|18.||Fecto F, Yan J, Vemula SP, Liu E, Yang Y, Chen W, et al. SQSTM1 mutations in familial and sporadic amyotrophic lateral sclerosis. Arch Neurol 2011;68:1440-6. |\n|19.||Bucciantini M, Giannoni E, Chiti F, Baroni F, Formigli L, Zurdo J, et al. Inherent toxicity of aggregates implies a common mechanism for protein misfolding diseases. Nature 2002;416:507-11. |\n|20.||Udan M, Baloh RH. Implications of the prion-related Q/N domains in TDP-43 and FUS. Prion 2011;5:1-5. |\n|21.||Halfmann R, Alberti S, Lindquist S. Prions, protein homeostasis, and phenotypic diversity. Trends Cell Biol 2010;20:125-33. |\n|22.||Si K, Lindquist S, Kandel ER. A neuronal isoform of the aplysia CPEB has prion-like properties. Cell 2003;115:879-91. |\n|23.||Salazar AM, Silverman EJ, Menon KP, Zinn K. Regulation of synaptic Pumilio function by an aggregation-prone domain. J Neurosci 2010;30:515-22. |\n|24.||Harrison PM, Gerstein M. A method to assess compositional bias in biological sequences and its application to prion-like glutamine/asparagine-rich domains in eukaryotic proteomes. Genome Biol 2003;4:R40. |\n|25.||Cushman M, Johnson BS, King OD, Gitler AD, Shorter J. Prion-like disorders: Blurring the divide between transmissibility and infectivity. J Cell Sci 2010;123:1191-201. |\n|26.||Furukawa Y, Kaneko K, Watanabe S, Yamanaka K, Nukina N. A seeding reaction recapitulates intracellular formation of Sarkosyl-insoluble transactivation response element (TAR) DNA-binding protein-43 inclusions. J Biol Chem 2011;286:18664-72. |\n|27.||Lee SJ, Desplats P, Sigurdson C, Tsigelny I, Masliah E. Cell-to-cell transmission of non-prion protein aggregates. Nat Rev Neurol 2010;6:702-6. |\n|This article has been cited by|\n||Prion-like Mechanism in Amyotrophic Lateral Sclerosis: are Protein Aggregates the Key?\n| ||Shynrye Lee,Hyung-Jun Kim |\n| ||Experimental Neurobiology. 2015; 24(1): 1 |\n|[Pubmed] | [DOI]|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b184c309-daf2-4dda-9736-1c755166d63a>","<urn:uuid:29c876d3-a025-4c61-bf30-f025525334c8>"],"error":null}
{"question":"作为一名新手父母，我想了解婴儿出牙期间的症状以及牙齿护理方法。请问婴儿在出牙过程中会有哪些常见症状，以及如何正确护理他们的牙齿？","answer":"During teething, babies typically show several symptoms: increased drooling, restlessness or difficulty sleeping, and a desire to chew on everything. They may also have swollen, tender gums where new teeth are emerging. These symptoms usually begin 3-4 days before a tooth appears and last 2-3 days after. For dental care, it's recommended to start brushing at least once daily at bedtime using a soft-bristled toothbrush with a small head designed for infants. This helps remove plaque bacteria that can lead to decay. Additionally, parents should establish a dental home by 12 months of age and assist with brushing until the child reaches third grade. It's important to note that while teething may cause discomfort, fever has not been proven to be associated with teething, and any fever should be evaluated by a doctor.","context":["All Smiles: Tips for Easing Teething\nYour baby has been fussing a bit more than usual the past couple days, and you notice that you're suddenly awash in drool. Could your little one be teething?\nBeginning anywhere from ages four to six months, your baby will likely get her first new tooth. The first teeth to appear are usually the two front bottom teeth, called the central incisors. The four upper front teeth, also incisors, are normally the next to erupt. The remaining teeth, including molars and eyeteeth, will break through periodically until your child is about 2 1/2 years old and has 20 teeth all together.\nExpect your baby to experience some discomfort and side effects during teething. While many people believe that a host of symptoms are associated with teething, the most widely recognized by doctors are:\n- restlessness or difficulty sleeping\n- increased saliva, drooling\n- a desire to chew on everything within grasping distance.\nSigns of teething in a baby’s mouth can also include swollen, tender gums where a new tooth is coming through. These symptoms may begin three to four days before a tooth pushes through and last two to three days after it makes its appearance. Fever, even low grade, has never been proven to be associated with teething. So if your baby has a fever, please consult your doctor.\nIf your baby is showing discomfort from teething, there are things you can do to help him feel better:\n- Try simple distractions such as cuddling, rocking, or walking around with your baby.\n- Use your fingers to massage irritated or swollen gums for two-minute intervals as often as necessary.\n- Wrap a piece of ice in a wet cloth and rub the spot. Be careful that the ice doesn’t slip out of the cloth and into baby’s mouth, since this is a choking hazard.\n- Allow your child to massage her own gums by gnawing on a teething ring. A chilled (but not frozen) teething ring or wet washcloth can be extremely comforting. Avoid using teethers once baby’s teeth appear, since the teeth could puncture a soft teething ring.\n- Offer chilled baby foods such as applesauce or pureed fruit to a child already accustomed to eating solids. Do not use popsicles, frozen bananas, carrots, or any other non-pureed food as these pose a choking hazard.\n- If your baby has excessive drooling, avoid dehydration by replacing lost fluids with diluted juice or water. You will also want to keep your baby’s face and clothes dry to prevent rashes or irritation. A bib may help protect clothing from wetness.\n- If the above methods don’t seem to work, ask your child’s physician about using medicated ointments such as Zilactin Baby medicated gel, Hyland’s Teething Gel or Tablets, Baby Orajel Teething Pain Medicine, or Baby Anbesol.\n- If nothing seems to work, consult with your pediatrician about trying systemic analgesics such as infant acetaminophen or ibuprofen. Do not use baby aspirin, which has been linked to Reye’s syndrome, a serious and potentially life-threatening disease.\nAs with so many aspects of parenting, teething can be a difficult stage for you and your baby to endure. But remember, this too shall pass and before you know it, your little one will have a beautiful, toothy grin.\nYOU MIGHT BE INTERESTED IN","Dental Care for Your Baby\nHow should I clean my baby’s teeth?\nA toothbrush with soft bristles and a small head, especially one designed for infants, is the best choice. Brushing at least once a day, at bedtime, will remove plaque bacteria that can lead to decay.\nAt what age should my child have his/her first dental visit?\nThe AAPD encourages parents and other care providers to help every child establish a dental home by 12 months of age. The AAPD recognizes a dental home should provide: comprehensive oral health care including acute care and preventative services in accordance with AAPD periodicity schedules.\nWhat is baby bottle tooth decay and how can I prevent it?\nBaby bottle tooth decay is a pattern of rapid decay associated with prolonged nursing. It happens when a child goes to sleep while breastfeeding and/or bottle feeding. During sleep, the flow of saliva is reduced and the natural self-cleansing action of the mouth is diminished. Avoid nursing a child to sleep or putting anything other than water in their bedtime bottle. Encourage your child to drink from a cup as they approach their first birthday. He or she should be weaned from the bottle at 12-14 months of age.\nCan thumb sucking be harmful for my child’s teeth?\nThumb sucking and pacifier habits over a prolonged period of time can create crowded, crooked teeth or bite problems. If your child is still sucking on thumbs or fingers when the permanent teeth arrive, a mouth appliance may be recommended by your pediatric dentist. However, most children stop these habits on their own.\nEducating parents and the community on cavity prevention is central to our practice. There are three key topics we address with each new patient to ensure good at-home care for teeth.\nWe call them the “Three Things”\n- Help your child with brushing and flossing. Assist your children with brushing and flossing rituals at least until the third grade. This is when children are developing their fine motor skills and are capable of adopting this responsibility on their own.\n- Ensure your child is getting enough fluoride. Fluoride works both topically and systemically to make tooth enamel stronger and more resistant to the acid attacks that cause cavities. Topical fluoride soaks into the very outer layer of the teeth (from toothpaste or mouthwash) while systemic fluoride works on developing teeth and is incorporated into the entire thickness of the tooth (from drinking water treated with fluoride). Systemic fluoride is most beneficial for developing teeth.\n- Ensure your child is brushing after frequent snacks or meals. Eating habits are crucial to reducing your child’s cavity risk. We are less concerned about what your child eats and more concerned with how often your child is eating. Snackers and grazers are at a greater risk of cavities than those who eat at designated feeding times. The more frequent the snack, the more acid attack the mouth experiences! Brush often to decrease the bacteria growth caused by the breakdown of sugars.\nPediatric Dental Emergencies\nBegin by cleaning around the sore tooth meticulously. Using warm salt water, rinse the mouth to displace any food trapped between teeth. UNDER NO CIRCUMSTANCES should you use aspirin on the aching tooth or on the gum. In the event of facial swelling, apply a cold compress to the area. For temporary pain relief, acetaminophen is recommended. See a dentist as soon as possible.\nCut or Bitten Tongue, Lip or Cheek\nIce can be applied to any bruised areas. For bleeding, apply firm (but gentle) pressure with sterile gauze or a clean cloth. If the bleeding does not stop with pressure or continues after 15 minutes, go to an emergency room.\nBroken Braces and Wires\nRemove a broken appliance only if it comes out easily. If it is lodged or painful to remove, cover any protruding edges with wax, cotton balls, gauze or chewing gum. DO NOT REMOVE any wire caught in the gums, cheek or tongue; see a dentist immediately. Emergency attention is usually not required for loose or broken appliances that cause no discomfort.\nRinse the area with warm water. Put a cold compress over the facial area of the injury. Recover any broken tooth fragments. Get immediate dental attention.\nKnocked Out Permanent Tooth\nRecover the tooth, making sure to hold it by the crown (top) and not the root end. Rinse, but do not clean or handle the tooth more than necessary. Reinsert the tooth in the socket and hold it in place using a clean piece of gauze or cloth. If the tooth cannot be reinserted, carry it in a cup containing milk or water. Because time is essential, see a dentist immediately.\nOther Emergency Conditions:\nPossible Broken Jaw\nIn the event of jaw injury, tie the mouth closed with a towel, tie or handkerchief. Go immediately to an emergency room.\nBleeding After a Baby Tooth Falls Out\nFold a piece of gauze and place it (tightly) over the bleeding area. Bite down on the gauze for 15 minutes; if bleeding continues, see a dentist.\nCold or Canker Sores\nPediatric Dental FAQs\nIf my child gets a toothache, what should I do?\nTo comfort your child, rinse his/her mouth with warm salt water and apply a cold compress or ice wrapped in a cloth on your child’s face if it is swollen. Do not put heat or aspirin on the sore area, but you may give the child acetaminophen for pain. See our staff as soon as possible.\nHow safe are dental X-rays?\nWith contemporary safeguards, such as lead aprons and high-speed film, the amount of radiation received in a dental X-ray examination is extremely small. Even though there is very little risk, pediatric dentists are particularly careful to minimize the exposure of child patients to radiation. In fact, dental X-rays represent a far smaller risk than an undetected and untreated dental problem.\nMy child plays sports. How should I protect my child’s teeth?\nA mouth guard should be a top priority on your child’s list of sports equipment. Athletic mouth protectors, or mouth guards, are made of soft plastic and fit comfortably to the shape of the upper teeth. They protect a child’s teeth, lips, cheeks and gums from sports-related injuries. Any mouth guard works better than no mouth guard, but a custom mouth guard fitted by our doctor is your child’s best protection against sports-related injuries.\nWhen do the first teeth start to erupt?\nAt about 6 months, the two lower front teeth (central incisors) will erupt, followed shortly by the two upper central incisors. The remainder of the baby teeth appear during the next 18 to 24 months but not necessarily in an orderly sequence from front to back. At 2 to 3 years, all of these 20 primary teeth should be present.\nWhat should I do if my child knocks out a permanent tooth?\nFirst of all, remain calm. If possible, find the tooth and hold it by the crown rather than the root. Replace the tooth in the socket and hold it there with clean gauze or a washcloth. If you can’t put the tooth back in the socket, place the tooth in a clean container with milk and take your child and the glass immediately to the pediatric dentist. The faster you act, the better your chances of saving the tooth.\nHow can I help my child through the teething stage?\nSore gums when teeth erupt are part of the normal eruption process. The discomfort is eased for some children by use of a teething biscuit, a piece of toast or a frozen teething ring. Your pharmacy should also have medications that can be rubbed on the gums to reduce discomfort.\nI noticed a space between my child’s two upper front teeth. Is this cause for concern?\nUsually, the space will close in the next few years as the other front teeth erupt. We can determine via a consultation whether there is cause for concern.\nIf my child gets a cavity in a baby tooth, should it still be filled?\nPrimary or baby teeth are important for many reasons. Not only do they help children speak clearly and chew naturally, they also aid in forming a path that permanent teeth can follow when they are ready to erupt. Some of them are necessary until a child is 12 years old or longer. Pain, infection of the gums and jaws, impairment of general health and premature loss of teeth are just a few of the problems that can happen when baby teeth are neglected. Tooth decay is really an infection that will spread and could cause decay of permanent teeth. Proper care of baby teeth is instrumental in enhancing the health of your child.\nWhat causes tooth decay?\nFour things are necessary for cavities to form: a tooth, bacteria, sugars or carbohydrates, and time. Dental plaque is a thin, sticky, colorless deposit of bacteria that constantly forms on everyone’s teeth. When you eat, the sugars in your food cause the bacteria in plaque to produce acids that attack the tooth enamel. With time and repeated acid attacks, the enamel breaks down and a cavity forms.\nIs my child getting enough fluoride?\nFluoride dramatically decreases a person’s chances of getting cavities by making teeth stronger. Fluoride in the drinking water is the best and easiest way to get it, but to make sure your child is getting enough fluoride, have your pediatric district evaluate the fluoride level of your child’s primary source of water. If your child is not getting enough fluoride via water (especially in communities where the water district does not fluoridate the water or if your child drinks bottled water without fluoride), your pediatric dentist may prescribe fluoride supplements.\nWhen should my child start using toothpaste?\nDo not use fluoridated toothpaste for your kids until age 3. Earlier than that, clean your child’s teeth with water and a soft-bristled toothbrush. After age 3, parents should supervise brushing. Use no more than a pea-sized amount of toothpaste and make sure children do not swallow excess toothpaste."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:491ac706-3ee6-4347-a422-cf58210cfe9c>","<urn:uuid:08b88a53-d16d-4eba-8bcd-950f67156a20>"],"error":null}
{"question":"Being interested in family traditions, how do Korean and Chinese New Year celebrations differ in terms of showing respect to elders and ancestors?","answer":"In Korean Seollal, younger family members perform specific bows called 'sebae' to their living elders, getting down on both knees and bending toward the floor. The elders then give their young relatives New Year's money and best wishes. Additionally, families perform 'charye,' traditional rites paying respect to ancestors through food offerings. In Chinese tradition, ancestor worship involves offering various dishes, fruits, and burning incense, as they believe deceased family members continue to exist in heaven and can influence the fortune of the living. Chinese elders also give monetary gifts in red envelopes to younger family members. While both cultures emphasize respect for elders and ancestors, the specific rituals and customs differ in their execution.","context":["Soellal, Lunar New Year, Customs\nLunar New Year, known as Seollal (also spelled Seolnal) is one of Korea’s major holidays. Since Koreans also mark January 1st as a holiday, the country celebrates two New Year holidays, almost in a row, since Soellal fall in either February or mid to late January. The actual New Year’s Day is a statutory holiday as are the day before and the day after – this was to allow people to travel to their hometowns for the traditional rites honouring the family’s ancestors. Many of the time-honoured rituals are gradually disappearing or being modified in modern Korea. However, the majority of people still practice most, if not all, of them.\nBefore Seollal – Shopping.\nLunar New Year is usually one of the most profitable times for department stores and fresh markets. No one wants to come empty-handed to visit family and friends so they shop for gifts such as meat, fish, fruit as well as Korean traditional snacks, tteokguk (rice cake soup), and various types of wild vegetables that are a required part of the ancestral rites. Department store gift certificates and cash are also popular with most people. The older generation likes to receive ginseng, honey, and other health / massage products. Daily necessities gift sets such as shampoo, soap, and toothpaste and food gift sets of ham, tuna, Spam, Korea’s traditional snack ‘Hangwa’, dried fish or fruit baskets are commonly given as well.\nIt is customary also to wear new clothing on New Year’s Day, usually the traditional hanbok. (Note: the actual new year's day is the 2nd day of the 3-day holiday)\nGiven that only a fraction of the population still lives in their ancestral villages and that the majority of Koreans are now in and around Seoul, the highways become one huge parking lot at the beginning and end of the Seollal period. At least a month or so prior to the holidays, train seats have all been booked as have domestic flights. Those who waited too long or who prefer going on their own can look forward to extremely long drives – a normally four hour drive can take up to four times longer.\nThings are changing, however. An increasing number of parents are choosing to come to their children’s homes in the city, since travel in the reverse direction is much easier. Also, with the increase in evangelical Christianity, some families are no longer practicing all of the traditional ancestor rites. Moreover, when the holiday covers more than the usual three days (near a weekend, etc) some people are opting to travel outside the country, often to warmer climates or to visit relatives living abroad. Whereas Seoul used to be almost traffic-free over Seollal, that is no longer the case and the majority of retail and service businesses remain open, sometimes even on New Year’s Day itself.\nThe day before Seollal, family members gather together to prepare the dishes required for the ancestral rites dishes, which must not only taste good but also to look perfect. Seollal’s most important dish is tteokguk (rice cake soup) but 20 other dishes such as wild vegetables, Korean style pancakes, various types of fish, galbijjim (rib stew), japchae (noodles with meat and vegetables), and more are also prepared for the ancestral rites. Preparing all this food has long been the duty of women, especially the elder son’s wife. A wife could be judged on the quality, appearance and amount of food she prepared.\nFortunately for some women, changes are happening here as well. Rather than have the elder son’s wife assume the entire burden, relatives are sharing the duties and bringing food with them…again prepared by the women of the family. Also, shops are offering ‘catering’ services of holiday foods that can be delivered on the day prior or on New Year’s Day for W200,000 and up. The men of the family are also being encouraged to participate in the process, as opposed to just eating, gaming and watching TV.\nSeollal Day Rituals\nCharye: On the morning of Seollal, people get up early and put on their ‘Seolbim (new clothes prepared for Seollal)’, often Hanboks still today. The family then gathers to perform the traditional rites, paying respect to their ancestors through food offerings and special rituals. After the rites have been performed, everyone partakes of the prepared foods.\nThe ritual includes a special bow to the ancestors. This bow, called sebae, involves getting down on both knees and bending toward the floor.\nThose families that no longer perform the ancestral rites still get together to share food and enjoy the other traditional activities.\nDDeokguk (aka Tteokguk): All Koreans eat ddeokguk, a soup made of thinly sliced ddeok (rice cakes) cooked in beef broth, on New Year’s Day. According to tradition, eating ddeokguk on Seollal adds one year to one’s age. So, if someone asks you how much ‘rice cake soup’ you had on Seollal, they are really asking your age. Because the slices of ddeok (rice cake) resemble coins, they symbolize wealth while the long rice cake role from which the slices are cut is symbolic of long life.\nChildren Bowing (jeol): After finishing their meal, the younger generations pay their respects to the elders of the family (grandparents, older family members, parents) by bowing (Keun Jeol) to them. The elders, in turn, offer their young relatives their best wishes for the year (for example. ‘have healthy year’ or ‘meet someone nice’) along with a gift of ‘New Year’s money’ (usually new bills placed in an envelope).\nSebae is another bowing custom wherein younger family members perform a deep bow to their elders and wish them a happy new year.\nThe most common game is yut nori - a fun and easy-to-learn game that requires a certain degree of teamwork. Players sometimes make bets with extra cash from their ‘New Year’s money’ they received. Another popular game, especially for the men, is Gostop. Board games are also popular, with games like Zenga among the children’s favourites. Palaces, parks and theme villages offer the chance to participate in some of the old traditional outdoor games, such as jegichagi, neoltwiggi, tuho, or kite flying. Modern families often go to the movies making Seollal a very profitable season for movie theatres.\nVisiting the Wife’s Parents Home\nBecause of the ancestral rites duties, families generally go to the home of the husband’s parents or elder brother for Seollal. In the past, they did not visit the wife’s family at all. However, in recent years, that has changed and it is now increasingly common to spend time with both sides of the family...although, many women still feel that the division time is still not very equal, things are changing.\nK4E Editor: Korea4Expats want to provide the most accurate and complete information possible, so if you notice an error or omission in the contents above, please contact us at firstname.lastname@example.org\nLast Updated on 2013-01-04\n|In the same header|\n|Chuseok Events and Activities||Daeboreum - First Full Moon Festival|\n|Dongji – Winter Solstice||Other Lunar Folk Celebrations|\n|Soellal, Lunar New Year, Customs||Translation - Chuseok Expressions|\nSchools Nursery to University\nSeoul Foreign British School, EYFS to Year 9, Seodaemun, Seoul\nKorean Language and Culture (translation, orientations, classes, etc)\nKoreanClass101 - An online Korean learning system\nSchools Nursery to University\nKorea International School-KIS, PK-12 English, Pangyo, Gyeonggi-do\nDental Clinics incl. Dentists, Orthodontists\nA Plus Dental Clinic, Orthodontics , Gangnam, Seoul\nAccounting, Tax Filing Services\nTieTax, US Taxes and Financial Planning, Korea\nBusiness and Networking Associations\nAsia Society Korea Center, Seoul\nShopping, Food, Clothing, New and Used Stuff, etc.\nSalvation Army Family Stores, Secondhand Goods, Seoul\nEnglishClass101 - An online English learning system","Top Chinese New Year's Eve Traditions\nChinese New Year's eve is the big day for ringing out the old and ringing in the new. With the development of the economy, some traditional customs have gradually disappeared, some new ways of celebration appeared. Here are the top 8 traditions of Chinese New Year's eve.\n1. Having a big dinner with Family\nChinese New Year's eve is the reunion day for every Chinese family. With a 7-day public holiday, all Chinese people would try their best to go back home no matter how far they live.\nAll family members will gather together, enjoying a grand family reunion dinner, chatting, singing, and laughing to celebrate this special day.\n2. Cleaning Houses\nChinese families will clean their houses: sweeping the floor, washing clothes, cleaning spiders' webs, and dredging ditches on Chinese New Year's Eve.\nIt is traditionally believed that dust represents \"old\" things, so cleaning houses means doing away with the \"old\" and preparing for the \"new\"; with the intention of sweeping all the rotten luck from the previous year out the door.\nInterestingly, one of the taboos of Chinese New Year is avoiding cleaning houses on Chinese New Year Day. People believe that if they do sweep or dust on New Year's Day, their good fortune will be swept away.\n3. Decorating House\nApart from cleaning houses, Chinese people will also decorate their houses on Chinese New Year's eve.\nPeople believe that auspicious decorations such as Spring Couplets, Door-God pictures, New Year's paintings, paper-cuts, and lanterns can drive away evil spirits and bring good luck.\n4. Worshiping Ancestors\nWorshiping ancestors is an old custom dating back thousands of years. People believe that the deceased family members have a continued existence in heaven, their spirits will look after the family and have the ability to influence the fortune of the living. So people offer sacrifices to ancestors at traditional festivals and hope the deceased ancestors would bless the whole family.\nOn Chinese New Year's eve, people will offer various dishes, fruits, and burn incense to worship ancestors.\n5. Watching CCTV New Year's Gala\nCCTV's New Year Gala, also known as the Spring Festival Gala, is called Chunwan ( chūn wǎn) in Chinese. The TV show is broadcast annually on Chinese New Year's eve at 8:00 PM.\nSitting together and watching the Spring Festival Gala has become a popular way to celebrate Chinese New Year's eve since the 1980s.\nThe show includes many different kinds of performances such as singing, dancing, comic dialogue, sketch comedy, magic, and acrobatics.\n6. Setting off Fireworks\nAncient Chinese people believed that fire could dispel bad luck, sparks could bring good luck, loud noise could scare away evil spirits and smoke made Yang energy (a kind of positive life-energy) rise. Fireworks produce such effects as fire, sparks, sound, and smoke when they are set off. It naturally combines with people's good wishes and becomes an ideal product for celebrations.\nIn ancient times, people would set off fireworks\nat 12:00 PM on Chinese New Year's Eve to celebrate the coming of the new year. Nowadays, many cities across China have imposed bans or restrictions on the use of fireworks for the sake of safety and environmental protection.\n7. Giving Red Envelopes\nThe red envelope or red packet is used as a monetary gift from the older generation to the younger generation during festivals or special occasions in China. Through giving red envelopes, parents send their good wishes to the children.\nCurrently, giving digital red envelopes via Wechat or Alipay app becomes a trend among young people.\n8. Staying up Late\nStaying up late or all night on Chinese New Year's Eve. After the grand reunion dinner, all family members will sit together, chatting, playing cards or mahjong, watch the gala to welcome the arrival of the New Year.\nPeople believe that staying up late can delay the aging process of the elder family member and prolong their life. The longer children stay awake, the longer their parents will live. You will find many families will keep their light on all night on Chinese New Year's eve.\nTop Traditional Dishes to Eat on New Year's Eve\nThe family reunion dinner held on the evening of Chinese New Year's eve is one of the major events of the Spring Festival. In ancient times, it was the biggest meal of the whole year. People could eat a lot of food that they wouldn't normally eat.\nAlthough there are many choices for food in modern times, traditional Chinese New Year's Eve dinner is also a very important meal for every Chinese family. Every dish has a different meaning.\n1. Fish: Always Have More Than You Wish For\nThe Chinese word for \"fish (鱼 yú)\" has the same pronunciation as the word for \"surplus (余yú)\". Chinese people always like to have a surplus at the end of the year, people think that saving things helps to make more in the next year. Eating fish stands for \"always have more than you wish for\".\nWhat's interesting is that people will not finish all the fish as leaving some on the table signifies having everything in abundance for the coming year.\n2. Chicken: Good Luck!\nThe Chinese word \"Chicken (鸡 jī)\" sounds like the word \"吉jí\", which means good luck and auspice in Chinese culture. In southern China, people will serve a whole chicken on the dinner table to show happiness and reunion.\n3. Nian Gao: Wish You a Prosperous Year\nChinese New Year Cake in South China\nNian gao (年糕 nián gāo), also known as \"rice cake\" or \"Chinese New Year cake\", is a traditional food made from glutinous rice flour. People believed nian gao carries auspicious meaning.\nFor old people, nian gao expresses the wish for longevity. For young people, it expresses the wish for\npromotion and high income. For kids, it expresses the wish to\n4. Dumplings: Change from the Old and to the New\nDumplings, in Chinese, sound like the Chinese expressions of changing from the old to the new.\nThe shape of a Chinese dumpling is similar to a gold ingot, people believe that eating dumplings on Chinese New Year's Eve means \"ushering in wealth and prosperity\".\nWhat's interesting is that people who eat the dumplings occasionally stuffed with special materials like a coin, a candy, a peanut, or a red date, will have the best luck in the new year.\nClick Chinese New Year Food to find out more lucky food eaten during Chinese New Year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:ea16cfb0-1156-48fa-9410-a26d41549b1a>","<urn:uuid:4b0fac7f-102a-48ee-b41e-e95518ebc889>"],"error":null}
{"question":"What are the key career paths available in visual communication, and how do fine art careers differ in terms of work environments?","answer":"Visual communication professionals can work in television, print publishing, and web-based organizations, taking roles such as creative art director, web designer, marketing director, photographer, and industrial designer. In contrast, fine artists typically work in different settings - they often work from lofts, artists' studios or their homes, with about 60% being self-employed. While visual communication professionals focus on creating designs for commercial purposes and websites, fine artists display their work in galleries and museums, though some may need additional employment since they can't always make their entire living from art.","context":["Visual communications involves the use of all types of devices in order to present information, ideas, and concepts across a variety fields. Whether looking to publish an advertisement in a magazine, design a website, or start out a career in photography, an education in a visual communication degree program can be a beneficial and necessary start.\nWhat Communicating Visually Can Do\nA well-developed visual design is able to reach out to an audience and accomplish one of several purposes. Most often, visual designs are created in order to inform and educate, persuade, or entertain. Visual communication professions are in demand because knowledge in this field combines an understanding of how to accomplish these purposes along with the computer and technology skills needed to put ideas into everything from a commercial to a new website.\nReaching an audience is dependent upon the combination of using text and graphic images appropriately and effectively. The most commonly used visual types of communication include graphs and charts, pictures, diagrams, advertisements, and printed periodicals or other publications. The key to creating effective graphics and other types of visual designs is the ability to understand the target end user’s needs.\nCurriculum and Courses\nA curriculum focused on the visual presentation of any type of communication begins with basic courses that help students build a foundation for creating designs that are appealing and functional for an audience. In most beginning courses for these programs, fundamentals such as images, colors, and typography are reviewed. This helps students to gain an understanding of the concepts that create appealing images.\nAnother important component of a curriculum in visual communication is the inclusion of courses that introduce the tools with which to develop the creations. These programs might include CAD, InDesign, and other design software. Additionally, many programs include courses in marketing trends, psychology, human behavior and learning, drawing, and linguistics.\nUpper level courses will likely require a more specific focus on a future career. After beginning to understand concepts such as the ways in which color affects mood, students might continue on to take courses in photography, graphic design, website usability, fashion design, industrial design, or advertising. Additional information on courses and curriculum elements of visual arts and design programs can be found at the International Graphic Arts Education Association.\nCareers in Visual Communications Fields\nInformation is disseminated everywhere, and the industries that rely most heavily on qualified, skilled visual arts professionals include television, print publishing, and web-based organizations. Graduates with visual arts or communication credentials might pursue positions developing printed publications or advertising, working in television or film, or creating websites and web advertisements for technology companies. Some examples of careers that a graduate with a degree in visual arts or communications might pursue include creative art director, print artist, web designer, print production manager, marketing director, web developer, photographer, and industrial designer.\nCreativity, critical thinking, and knowledge of everything from psychology to marketing to computers come together in careers in all types of industries to jobs where visual arts and design are highly valued. As more and more information is created for online mediums and with computer-based tools, the necessity for an in-depth understanding of visual communications continues to grow.","Aspiring artists have more career options today than they ever have before. The digital age has opened up not only the way art can be taught, but also the types of art that can be created. Each type of art degree requires that the artist master some basic skills sets such as sketching, drawing and painting with each taking on a slightly different skill set as the artist’s education progresses. Additionally, the type of degree depends on the end career goal of the artist. While some artists want to work in a gallery or museum, others have a more commercial venue in mind or even art therapy.\nFine Art (Drawing, Painting, Sculpting)\nArtists who study fine art display their work in galleries and museums, according to the Bureau of Labor and Statistics. They make their work environments in lofts, artists’ studios or their homes. Artists work in a variety of media, including paper, canvas, sculpture, art metals or ceramics. Up to 60 percent of fine artists are self-employed, although not all can make all of their living from art. While some of these artists take up art-related careers such as museum curating, teaching or fine art publishing, others work other jobs not related to art.\nAnimation careers become the outlet for many aspiring artists working in the field. With the proliferation of video games the career prospects for many aspiring artists has improved dramatically. Artists looking to become animators can find animation degrees online and on traditional campuses. According to All Art Schools, would-be animators learn a variety of skills in animation school including how to paint and draw, how to draw storyboards and how to create characters. They also become acquainted with the latest animation software.\nWith the pioneering work of photographer Alfred Stieglitz, photography became a fine art in its own right during the 20th century. Aspiring photographers take their place in the ranks of many of the finest art schools in the country and learn basic art skills like drawing and art history in addition to their photography related studies. Additionally, while many photographers work in still photography, some get degrees in photography related art degrees, which include film and videography as well as digital photography, according to the Bureau of Labor and Statistics.\nIllustration and Design\nIllustrators and designers work in complementary fields to one another, and designers as well as illustrators often do the same core course work in their art degree programs. These artists create graphics that offer insight into a client or organization’s mission, advertising campaign or other business related topics that need visuals in order to get the message across.\nFor those artists who are equally concerned about creating art as they are about helping people, there is the profession of art therapy. The foundations of art therapy rely on the use of the visual arts-drawing, painting, sculpture and other forms of visual art-to access the inner world of the therapy patient. Through the use of art and counseling, an art therapist helps treat many issues such as depression, anxiety or even pain therapies like migraine relief. To prepare for a career in art therapy, aspiring art therapists should take at least 15 credits of art plus 12 credits of psychology before going on to get a Master’s degree in art therapy.\nAllied Health Profession, Art Therapist, AHP\nBureau of Labor and Statistics, Artists and Art Related Workers, BLS\nBureau of Labor and Statistics, Graphic Designers, BLS\nBureau of Labor and Statistics, Photographers, BLS\nMasters of Photography, Alfred Stieglitz, MOP\nAll Art Schools, Animation Career, AAS\nArt Study, Choosing Art School: Type, AS"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4fe0b353-d1e0-4ab9-95a7-ae1e1fe19040>","<urn:uuid:0364b8e6-9b51-421d-9d1c-38fe1d7658d3>"],"error":null}
{"question":"What challenges do natural resource extraction companies face in indigenous territories, and how do these issues affect their financial performance?","answer":"Natural resource extraction companies face significant challenges when operating in or near indigenous territories. Companies often inadequately consider indigenous rights when launching exploration ventures, with only 5 out of 52 surveyed U.S. companies having specific Indigenous Peoples policies. This oversight can lead to community protests and operational disruptions, resulting in substantial financial losses. For instance, when operations are halted due to community protests, a world-class mining operation can lose between $20-30 million per week. This situation reflects the broader historical pattern of state-sanctioned encroachment of multinational companies on indigenous lands, leading to contradictions and violence in resource extraction efforts.","context":["Demand for natural resources such as oil, water, and land remain undiminished at the start of the 21st century despite growing public anxiety about their depletion. As key resources become scarce, new resources come into existence. Across the globe, states and corporations have redoubled efforts to extract conventional and unconventional resources in an attempt to deliver ongoing prosperity to citizens and shareholders. The contradictions and violence of these endeavors are most apparent in state-sanctioned encroachment of multinational companies on indigenous and other rural lands.\nThe resurgence of anthropological research on natural resources, a field with a long and continuous trajectory, stems from the recognition of these dilemmas and their growing impact on the peoples and places anthropologists study. Until the past decade and a half, anthropological studies tended to focus—with some noteable exceptions—on agriculture, hunting, fishing, foraging, and similar activities involving the exploitation of so-called renewable resources. However, more and more anthropologists have turned their attention to the study of natural resources per se. They have produced studies of water, sapphires, gold, oil, coltan, forests, and biodiversity (Acheson 2006, Behrends et al. 2011, Mantz 2008, Orlove and Caton 2010, Whiteford and Whiteford 2005), of specific extractive regions such as Australia and Papua New Guinea (Rumsey and Weiner 2004), of modes of engagement with resources such as [End Page 5] extraction and conservation (Ballard and Banks 2003, Carrier and West 2009), and of conceptualizations of specific resource processes such as adaptation and commoditization. The common refrain of these studies is that while natural resource exploitation continues to play a critical part in shaping the human condition, it does not do so in a uniform or environmentally deterministic manner. However, with the exception of Ferry and Limbert’s (2008a) stimulating edited volume, little effort has been made to examine resources as a theoretical and comparative problem in a way that would conceive of their “resourceness” as going beyond their status as particular kinds of commodities.\nThis special collection explores questions in the anthropology of natural resources that have thus far remained implicit, including questions about resources’ specific characteristics and capacities, the processes through which they come into being, and how such processes of resource making can be studied ethnographically. We suggest that placing these kinds of questions—questions of an ontological bent—at the center of inquiry can enhance the possibilities for a comparative ethnographic analysis. They also help interrogate the logics that perpetuate natural resource exploitation and specify an anthropological intervention in cross-disciplinary debates. Our questions arise out of broader intellectual trends in the social sciences and philosophy to which anthropologists have contributed that probe the legacies of modernist divisions between human and nonhuman, the social and the material, and what is active and what is acted upon in the environment. Terms such as “socionature” (Swyngedouw 1999), “natureculture” (Haraway 1997, Latour 1993), “nature regimes” (Escobar 1999), or “second nature” (Biersack 2006) have been developed to convey the sense that nature “is humanly produced (through conceptualization as well as activity) and that [it] therefore partakes, but without being entirely, of the human” (2006:14).\nNatural resource exploitation—as a sustained project of abstracting substances identified as useful, valuable, and natural in origin from their environment—has long played a central role in that continuing human effort to become “modern.” It is a process of boundary making par excellence—of distinguishing subject from object, nature from culture, and science from politics (Latour 1993). The intellectual agenda of scholars mentioned above, sometimes dubbed “posthumanist” or “new materialist,” has been partly driven by ethical concerns about climate change and ecological disasters, and the exploitation of resource environments. [End Page 6] One of anthropology’s key contributions to these discussions stems from research about the differences in how people relate to their surroundings and about worlds premised on principles other than modernist ones (Descola and Pálsson 1996, Ingold 2000, Strathern 1980). Curiously, geographers have been quicker than anthropologists to import concepts of nonhuman or material agency, which have also emerged from anthropological work, back into the study of resources as such (Bakker and Bridge 2006, Bridge 2009, Kaup 2008). In this special collection, we draw...","Oil and gas companies often don’t consider indigenous rights strongly enough when launching exploration ventures, says First Peoples Worldwide (FPW), a Virginia-based organization that has just released a survey on risks and challenges that extractive industries may face as they pertain to indigenous rights and properties.\nThe result says the study, exposes company investors to potential losses when disagreements or community protests arise.\nThe report, which was released at the SRI (Sustainable, Responsible Impact Investing) conference in October analyzed 52 U.S.-based companies involved in 370 extractive industry operations throughout the world. The operations were reviewed because of their presence on, near or affecting indigenous lands.\n“The results are eye opening. Ninety-two percent of the sites posed a medium to high risk to shareholders,” FPW says in its October 30 press release. “This is concerning given that only five of the 52 companies had an Indigenous Peoples policy for productively engaging indigenous communities, leaving shareholders exposed to considerable risk.”\nAnd the problem isn’t limited to today’s natural gas and hydraulic fracturing ventures. Indigenous rights and land ownership have often been overlooked or underestimated in industrial development. Examples stretch back as far as the 1800s when Native American lands were converted to producing oil fields in what later became the state of Oklahoma. But more recent legal battles at Split Rock, Maine and Black Mesa, Ariz. show there are oftentimes still misunderstandings about the legal processes and impact of exploration and extraction of natural resources on indigenous lands.\nThe result, says the FPW, isn’t just a loss of time while equipment and workers side idle during a protest. The financial cost is often significant.\nJohn Ruggie, United Nations Special Representative, has studied the toll that is often borne by company investors when there is a shutdown due to “non-technical” issues like a protest or roadblock.\n“For a world-class mining operation, which requires about $3-5 billion capital cost to get started, there’s a cost somewhere between $20 million and $30 million a week for operational disruptions by communities,” Ruggie stated in a 2011 interview with Business Ethics Magazine.\nFPW’s Director of Communications, Dan Morrison, said that lesson came home this week for the U.S. company Southwestern Energy (Southwestern) when its crew was stopped from carrying out testing for hydraulic fracturing in New Brunswick, Canada because of angry protests at the company’s entrance.\nMorrison said that the Southwestern site had already been rated has having a high risk for problems when the local First Nations band filed for an injunction.\n“The big bellwether for us was the fact that Southwestern stated itself that it was costing them $60,000 a day with this protest,” said Morrison, who noted that Southwestern’s dilemma is a classic example of the risks that companies often face when they don’t take a community’s concerns into consideration before launching oil and gas exploration – particularly those of indigenous communities that may have historic and cultural ties with the area that go back thousands of years.\nMorrison said that highlighting Southwestern’s difficulties in the report as a way to illustrate investor risks helped to get their point across.\nI couldn’t help wonder, however, why FPW chose this particular community conflict to highlight its point. The story of the Elsipogtog First Nation’s efforts to stop oil and gas exploration in New Brunswick is complex and perhaps not as clear as some U.S.-based incidents where protests have garnered success in the courts. The conflict isn’t happening on First Nations treaty lands but adjacent to it. And Canada’s indigenous rights protections are some of the most developed in the world, with nation status accorded to Canada’s Aboriginal Peoples that provides greater autonomy and speech than in the United States and many other democracies. So why would this be an ideal example by which to highlight the possible costs to investors?\nMorrison said the complexity of the New Brunswick site, in fact, made it a prime example of the kinds of risk that companies – and investors – can face when doing their research\n“It is exactly why we did the report,” said Morrison. “(It) doesn’t matter (whether the site) is on indigenous lands or not. The fact is that indigenous peoples can block the road and make their claim and halt the operations.”\nAnd the fact that the conflict that Southwestern is now embroiled in is occurring in Canada, said Morrison, where First Nations rights are given strong consideration in the courts only helps to illustrate further the need for oil and gas companies to do their homework about the risks they may encounter before considering a project for development.\nTo aid oil and gas companies in learning about such issues, First Peoples Worldwide plans to publish regular updates to its risk report. Interested companies and individuals can register for email updates on the FPW website.\nImage by R.A. Paterson"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:339b376b-01fb-42ed-a8c0-652a3b23a417>","<urn:uuid:21f5a56c-ffa7-4a94-8aeb-113ba4990987>"],"error":null}
{"question":"What are the challenges in patient adherence to chronic disease treatment plans, and how effective is social media as a healthcare communication channel?","answer":"The main challenges in patient adherence to chronic disease treatment plans include financial issues forcing choices between medical costs and living expenses, adverse side effects leading to treatment discontinuation, knowledge deficits about care plans, and psychosocial challenges like depression. Regarding social media's effectiveness as a healthcare communication channel, data shows it's highly effective - at least 30% of consumers use social media to communicate with healthcare companies, and nearly three-quarters of marketers report social media has been effective for their business. Social media platforms provide trackable results for marketing efforts and allow healthcare practices to monitor engagement through metrics like user reach, clicks, and appointment bookings. Additionally, 90% of Instagram users follow businesses, making it a valuable channel for healthcare communication.","context":["Education and team-based care: The two essentials to providing effective chronic care.\nToday, more Americans than ever are living and struggling with chronic health conditions. According to the CDC, roughly 70 percent of the population will die from a chronic disease and caring for those patients accounts for over 86 percent of the nation’s healthcare spending.\nA chronic condition, by its very definition, requires ongoing and attentive treatment by healthcare providers to attain optimal outcomes for their patients. This is no easy task, as healthcare resources are stretched to their breaking point and the prevalence of chronic diseases are rising at an alarming rate. The market is at the tip of the proverbial iceberg of what is undoubtedly an epidemic, as chronic diseases account for the majority of deaths in the United States and globally. However, there are best practices for managing the vast population of chronic diseases such as diabetes.\nThere are two approaches for caring for chronically ill patients: a team-based strategy involves a cross-departmental collection of providers caring for one patient while a patient-centered method brings the patient into the decision-making process of creating an individualized plan for care. These two strategies are not mutually exclusive, but they aren’t correlated either.\nCompliance vs. Adherence\nA common challenge for providers and educators is engaging patients to proactively participate in their care and adhere to the designated plan of action. It is reasonable to assume that most patients inherently want to be well, but they can become frustrated and exhausted in dealing with the daily grind that is diabetes care and often give up.\nProviders continue to struggle with the terms compliance vs. adherence when describing a patient’s ability to follow a plan of care. The distinguishing factor is that adherence allows the patient to participate in the self-management of their disease rather than yielding themselves to a proposed regimen from the healthcare provider. When a patient adheres to a treatment plan they are collaborating with their provider to develop and execute next steps together, increasing engagement and employing a team-based and patient-centered approach to care.\nIn order to enhance care quality, providers and educators need to address the challenge of improving adherence and therefore outcomes. It is not enough to continue approaching diabetes care, or any chronic illness, with traditional episodic one-on-one care as these types of diseases need the collaborative assistance of an interdisciplinary team, calling for a different strategy such as a team-based and/or patient-centered approach to care.\nDespite an expanding portfolio of diabetes medications available and strong efforts to expand diabetes education programs, the outcomes for these patients remain relatively poor. When caring for members of the chronic care population, there needs to be a shift in the care model to involve the patient, educators, and additional team members, which will impact treatment plans, patient engagement, and ultimately outcomes.\nContinuing with the example of diabetes, the disease is extremely personal beyond the differences between type 1 and type 2 diabetes, each person has different treatment goals, medication needs, and required insulin doses. As such, effective care depends upon a patient’s adherence to their customized and recommended plan of action. When patients are not adhering to their plan, it is imperative to identify the root cause and combat it to begin re-engaging with them.\n- Financial issues often force patients to choose between medical costs and other living expenses.\n- Adverse side effects can include hypoglycemia, swelling, nausea, etc. and may result in ED visits or hospitalization. These side effects can cause patients to discontinue treatment.\n- Knowledge deficits regarding the multiple aspects of their care plan, including medications, exercise, diet, hypoglycemia treatment, etc. also hinder one’s ability to adhere.\n- Psychosocial challenges such as depression, a common comorbid condition in people with diabetes, can deter patients from continuing with their plan.\nOnce the barrier is identified, team-based care and disease education are instrumental in engaging patients as well as improving adherence. Understanding new concepts, technology and benefits of medication regimens are essential to this strategy. Education is best delivered by a team of providers and Certified Diabetes Educators across multiple disciplines, but is often underutilized, as it can be costly and difficult to find local experts. Since people learn in a variety of ways: reading, writing, observing, listening, etc., providers need to utilize a form of education that best suits their patient’s learning style to increase their likelihood of self-management of diabetes. Embracing social media platforms (Twitter, Blogs, Facebook, etc.) is an effective manner for patients to communicate with their peers and share their struggles in diabetes management, fostering community. These touch points also serve as a launching point for long-term diabetes self-management support as patients connect with one another.\nStandardization of Chronic Care Processes\nClinicians and experts are often asked, “How can healthcare providers standardize the care of patients with chronic diseases?” Standardization of care processes for patients with chronic diseases can result in improved care quality, increased drug adherence, enhanced patient outcomes and reduced costs. It is important for providers to individualize therapy and maintain a patient-centered approach, as recommended by clinical guidelines published by numerous associations around the world to ensure optimal outcomes throughout standardization.\nAs value-based payment models begin to take effect, it is imperative that providers identify a consistent workflow to engage patients in their individualized care. The allocation of resources, both electronic and human, containing strategies to successfully manage a large population of patients is essential to the success of any chronic disease management program. In that same vein, overall positive population health management is a key to successful chronic disease management. Providers need to be able to identify, engage, and communicate successfully with patients who are struggling with their care – chronic or not. Going back to the diabetes example, providers must standardize a process to utilize electronic health records (EHRs) to mine the data and identify patient populations with diabetes and flag those who are at-risk by missing appointments, not achieving A1c, blood pressure, or lipid targets, frequently visiting the emergency room, requiring recurrent hospitalizations, not refilling their medications, and who are missing their specialty appointments.\nThere must be a team-based approach to care in place to engage these patients once they are identified by working with care coordinators, educators, providers, etc. These struggling patients may require frequent communications, at times daily, in order to keep these individuals out of the hospital and encourage their adherence to the personalized care plan. This requires both manpower and technology, but providers should be vigilant about engaging with patients through their preferred methods, such as text messages, e-mail, phone calls, telehealth visits or even “snail” mail. Successful chronic disease management includes standardizing a process to identify the at-risk patients, and then individualizing their engagement, education, and treatment through a team-based and patient-centered approach.","Social media is a powerful communication tool, as demonstrated by its ability to influence, educate, and inform prospective customers across a variety of industries. From consumer products and real estate to political campaigns, technology, and beyond, social media has become an integral tactic in any marketer’s toolbox.\nWhat about medical marketing? Medical and dental practices use social media to inform patients and attract new ones, reinforce their brand, boost web presence, build credibility, and enhance online reputation.\nIf you’re skeptical about how social media can support your healthcare practice’s marketing strategy, here are some powerful statistics to consider.\n9 social media statistics for healthcare practices\n- More than 75 percent of people use at least one social media platform.\nThe verdict is in: Social media isn’t going anywhere. According to Statista, nearly 80 percent of Americans use at least one social media platform. Facebook and Twitter have now been around for more than 15 years and have become part of our daily lives. Add in massively frequented outlets like YouTube and Instagram, and practices have a variety of channels to connect with patients in meaningful and engaging ways.\n- Facebook is the most widely used social media platform.\nAs a market leader, Facebook draws nearly two-thirds of all U.S. adults. While checking their feed for the latest on family and friends, users often spend time engaging with the more-than 200 million small businesses that use Facebook. A report published by Datareportal says Facebook users spend nearly 20 hours per month on the platform, providing healthcare practices with numerous opportunities to inform, educate, and connect with both current and prospective patients.\n(source: Pew Research)\n- Nearly three-quarters of marketers feel social media has been effective for their business.\nIf your practice is working with a limited marketing budget, it’s critical to know which of your marketing tactics are the most effective. One of the many benefits of social media marketing over traditional channels is the ability to track your campaigns’ results and progress — this offers greater insight into your social media return on investment.\nPractices using social media can track the number of users reached with each post, their level of engagement (clicks, “likes”), how many people visited their website or booked an appointment as a result, and much more. While data review takes a bit of time, Buffer’s 2019 State of Social Media Report reaffirms that most marketers are pleased with the performance of their social media marketing efforts.\n- Eighty percent of social media time is spent on a mobile platform.\nThis highlights the importance of creating mobile-friendly social media content. That means making your posts that short, sweet and to the point. Smartphone and tablet users also enjoy dynamic content like video — short videos can be among the most successful communication on mobile devices.\nFinally, since the goal of most social media posts is to convert readers and followers into patients via your website, this reaffirms the need for a website that is either mobile-optimized or mobile-responsive. That’s the only way to ensure a clear, satisfying experience for patients who visit your site via their mobile device.\n- Ninety percent of Instagram users follow a business.\nIf your practice is not participating on Instagram, you risk losing prospective patients to the social media-savvy practice down the road. While you may think of Instagram as a newer platform, it’s been around for more than a decade, and is drawing more businesses using the channel as part of an integrated marketing strategy.\nIn the popular PatientPop social media 101 webinar, 8 of 10 live attendees told us they already use Instagram at their practice — second only to Facebook. Think of Instagram almost like a rotating billboard, with people spending a lot more time “driving” their mobile devices than cars these days. Within your Instagram account, you can feature physicians, facilities, services you offer, and any visually appealing content that won’t play as well on other channels.\nIf your specialty lends itself to visual storytelling — aesthetics, plastic surgery, dermatology, weight loss — then Instagram is a must. Its light tone and quick video snippets make it easy to engage with your followers, and can help prospective patients get to know your brand and services in an interactive way.\n- Adults 18 and older watch an average of 4.9 hours of YouTube videos each week.\nThat’s more than 41 minutes a day, of what people too often perceive as time-wasting videos of silly animals, pranks, and stunts. But with a staggering variety of videos, YouTube can be a source of education. Why not bring meaningful information to your prospective patients? Healthcare practices that use YouTube connect with followers, patients, and prospective patients on a more personal level.\nCommon healthcare practice uses for YouTube videos include:\n- “meet the provider”-style videos\n- office and facility tours\n- “day in the life” physician or provider stories\n- patient stories or testimonials\n- educational videos or vlogs that answer patients’ most frequently asked questions\nFor providers who offer unique or highly specialized services, YouTube can be a powerful tool to introduce your practice to patients who may not otherwise make their way to your website.\n- More than 70 percent of consumers who have a positive brand experience on social media are likely to recommend that brand to friends and family.\nYou know the old mantra “Experience is everything?” It translates well to the world of social media. Healthcare practices that engage with social media followers, respond to questions and feedback promptly, and offer a positive patient experience are introducing themselves to a much broader audience than most are used to.\nTake that opportunity to show off your patient-focused approach for all to see. Treat social media comments, questions, direct messages, reviews, and feedback just as you would when speaking with a patient in person or on the phone. This increases your chances of scoring a personal recommendation, and helps continually improve your online reputation.\n(Source: Forbes, via Oberlo)\n- The number of daily active Stories users on Instagram and Facebook hit 500 million in 2019.\nJust when healthcare practices figured out the social media basics, the Stories format has taken over and users can’t get enough. Stories has played an integral role on Instagram for more than a decade — now, on Facebook, the format is growing 15 times faster than news-feed sharing.\nHealthcare practices can use the fast-moving Stories option to give followers a quick look within the practice, feature patients (with permission, of course!) or providers, or highlight new services or equipment. When it comes to Facebook or Instagram Stories, creativity is the name of the game. But remember that Stories disappear after 24 hours unless you add them to your highlights reel, on Instagram and Facebook.\n- At least 30 percent of consumers use social media to communicate with a company.\nThe rise of direct messaging on social media has created yet another avenue for patients and prospective patients to connect with your healthcare practice. Don’t ignore them.\nCheck your messages regularly and respond promptly, just as you would after receiving a voicemail or patient portal message. As with the tips in number 7, this level of connection can help you improve the patient experience and maintain a positive online reputation.\n(source: Drift, via Oberlo)\nSince most experts assume the use of social media as a business communications channel will increase, you may find your practice receiving more patient inquiries with time. Consider delegating the responsibility of monitoring and responding to comments to a practice staff member with a great track record for delivering exceptional customer service and prompt response times."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7bd78991-554d-4a4c-89e0-f2f273f5e389>","<urn:uuid:d384ea22-1c21-41bc-9bba-17274b6da38f>"],"error":null}
{"question":"How did the use of mosaics differ between Roman and Byzantine religious buildings?","answer":"Roman mosaics typically featured geometric patterns, mythological scenes, and sporting events, as seen in places like Pompeii and Ostia Antica. In contrast, Byzantine mosaics were characterized by glittering gold backgrounds, precious stones, and religious imagery. While early Byzantine churches like San Vitale included recognizable human figures, later Byzantine religious art became more stylized, focusing on conveying spiritual messages rather than precise human forms.","context":["10 Awesome Roman and Byzantine Mosaics\nUpdated: Feb 19\nAs a lover of art and history, mosaics have always caught my attention throughout my travels. While existing on a continuum, Roman mosaics and Byzantine mosaics evolved to become quite different from each other. Ancient roman mosaic patterns included recurring geometric motifs as well as elaborate mythologic, hunting, and sporting scenes. Byzantine mosaics, which succeeded the Roman era, were bedazzled with glittering gold, incorporated precious stones, and often centered on imperial and religious figures. This blog shares five fantastic ancient Roman mosaics and five spectacular Byzantine empire mosaics. Check them out!\nThe Alexander Mosaic in Pompeii, Italy (Roman)\nContributed by Maggie at www.pinkcaddytravelogue.com\nPompeii is a city frozen in time. Thanks to the 12 meters of ash and lava that buried the city in 79 AD, a window into ancient Roman life was preserved for millennia until archaeologists unearthed the city. The most well-known artifact uncovered from the Pompeii excavations are the plaster casts of bodies, but one of the greatest mosaics in the Roman empire was also preserved under all that ash – the 'Alexander Mosaic.'\nThe huge (8 x 16ft!) mosaic once decorated the floor of what is now known as the House of the Faun in Pompeii, the largest home in the city. Today, the original hangs in the National Archaeology Museum of Naples, but a replica mosaic is located at its original location in Pompeii.\nThe incredibly detailed and artistic art piece depicts Alexander the Great and his troops battling those of the Persian King Darius III, at the Battle of Issus (333 BC). Alexander himself is shown to the left, riding into battle on his horse. The Persian King is shown fleeing on his chariot, with his forces in disarray.\nThe mosaic stands out because, from a distance, it looks like a painting. It is uncharacteristically complex for a Roman mosaic, using foreshortening and shadowing effects. One and a half million tiles were used to create the intricate scene. It is truly a masterpiece and worth seeing on a trip to Pompeii.\nThe replica in Pompeii, Italy:\nThe original displayed in the National Archaeological Museum in Naples, Italy:\nBaths of Neptune Mosaic in Ostia Antica, Italy (Roman)\nContributed by Mary at www.wanderu.com\nOstia Antica, an expansive archeological site in what used to be Rome’s seaport, is home to the remains of 24 bath complexes. Ostia Antica is about 15 miles from Rome, and it’s worth visiting to see one of the more impressive bathhouses, the Baths of Neptune.\nThe thermal baths of Neptune were built near the end of Emperor Hadrian’s rule in the 2nd century AD. Baths in the Roman Empire served a social function along with a hygienic one. Bathers used the space to conduct business, eat a meal, or seduce a romantic partner.\nThe Baths of Neptune included a gymnasium, hot, cold, and warm baths, dressing rooms, and an entrance hall with two magnificent mosaics on its floor. One depicts Neptune, the god of the sea, riding a chariot drawn by seahorses and surrounded by various marine animals. The other, in the adjacent room, shows his wife, Amphitrite, riding a seahorse.\nThe mosaics are created from white and black tiles, in a style that was used throughout many spaces in Ostia. These mosaics are particularly well-preserved, and the images depict active scenes of gods and animals from the sea. As Ostia was a harbor town, the use of marine imagery seems especially appropriate!\nFor visitors to Ostia Antica, there is an elevated platform adjacent to the Baths of Neptune, from which you can look down on the floor mosaics. Informational plaques identify the figures in the mosaic and share information about the function of a Roman bathhouse.\nAldborough Fort Mosaic in Yorkshire, UK (Roman)\nContributed by Coralie at www.greyglobetrotters.com\nTucked away in North Yorkshire in the UK is a tiny village with the remains of an impressive Roman Fort. Far from the well-trodden tourist trail, the Aldborough Roman site is a modest and unassuming hidden gem, well worth a trip from York. Once the capital for Britain’s largest tribe - the Romanized Brigantes – Aldborough now showcases that past. It houses a small museum with archaeological finds from 1,800 years of history. Its beautiful grounds contain the remains of the original town walls. In addition, two incredible 2nd-century Roman mosaic pavements can be viewed in their original positions. The beautifully preserved Star Mosaic features an 8-pointed star surrounded by interlaced geometrical patterns. It’s housed inside a small building towards the end of the fort. The second mosaic, the Lion Mosaic, was discovered in the garden of a local pub and while it has suffered some damage to the central area, it’s still stunning. The best way to visit Aldborough- known to the Romans as Isurium Brigantum - is to drive to the pretty nearby town of Boroughbridge and then walk the mile from the public car park. The only parking at Aldborough is for disabled drivers. The Aldborough Roman site is managed by English Heritage. The admission charge is £5.60 for adults.\nVilla of the Birds Mosaic in Alexandria, Egypt (Roman)\nContributed by Joanna from www.theworldinmypocket.co.uk\nAlexandria is one of the most surprising cities in Egypt as the metropolis combines the remains of ancient Egypt's history with the vibrancy of its Mediterranean port. A particularly impressive site to see in Alexandria is the Kom el-Dikka Archaeological Park, home of the only still-intact Roman amphitheater in Egypt.\nWithin the Kom el-Dikka complex, you will also find the Villa of the Birds, which is the ruins of an ancient Roman mansion that still has some intact mosaics on the walls and floors. It is believed that the mosaics have been laid down in the 1st century AD and they are in very good condition. The colorful Birds Mosaic represents different species of birds, as well as geometric motifs.The bird mosaic could have been one of the bedrooms of the villa. It is adjacent to the dining room, the biggest space in the villa. One of the floors has a mosaic with a panther centerpiece. The villa was discovered in 1998, beneath the wall of a Byzantine building. The Villa of the Birds is a fantastic example of ancient Roman design and style. It is one of the few remaining structures from Alexandria’s Roman era.\nBikini Girls Mosaic in Villa Romana del Casale in Sicily, Italy (Roman)\nContributed by Ildiko at www.indulgewithildi.com\nThe 3rd century Villa Romana del Casale in Piazza Armerina, a rural town in the center of Sicily, is a site to behold. Proposed to be the potential “retirement villa” of Emperor Valerianus Maximianus, the villa is lush with numerous exquisite and very well-preserved mosaics. Gorgeous, elaborate, and colorful mosaics throughout the villa feature complex hunting scenes, animals being loaded onto boats, transported and disembarked to Rome, destined for the Roman amphitheater, chariot-racing scenes, many mythological scenes, geometric and floral motifs, intimate scenes, and even a fun mosaic showcasing ‘Bikini Girls”.\nIntended as a room for the servants of the residence’s matriarch or Domina (Eutropia), the rectangular ‘Bikini Girls’ cubicle interestingly had two overlapping mosaic floors, for even in ancient times women seemingly loved redecorating! The original mosaic, as can be seen in the top left-hand segment of the flooring, was a simple geometric motif dated to the 3rd c. AD. Once the owner tired of that pattern and wanted a change in the decor, craftsmen from the Constantinian period (320 AD) laid a new and fun mosaic atop the geometric one, featuring ten bikini girls!\nThe girls actually represent serious athletes engaged in a pentathlon which was comprised of five sports: spoked wheel, hand weight jump, discus throw, free running, and handball competitions. Note that in the second register, the girl in the center has already been awarded a victory palm and crown of flowers, while the girl holding the spoked wheel is about to be rewarded a victory palm and crown of flowers by the young women wearing the gold cloak.\nArmenian Bird Mosaic in Jerusalem (Byzantine)\nContributed by Ildiko at www.indulgewithildi.com\nThis is a rare Byzantine mosaic in that it is not open to the general public, but can only be viewed with the permission of the Armenian Patriarch in Jerusalem. I feel very fortunate to have had that opportunity on one of my visits to Jerusalem.\nThe mosaic is a stunning, colorful floor mosaic showing medallions of vines and grapes, and 39 various birds, all bordered by a geometric braid. The inscription along the top of the mosaic reads, 'To the memory and salvation of the souls of all Armenians whose names are known by God alone.'\nThe mosaic floor is the remains of an old Armenian chapel constructed sometime between the 5th and 6th centuries AD, based on the style of the motifs and the shape of the inscription. It was built in honor of St. Polyeucte who was an Armenian from Malatia and an officer in the Roman Army. He was martyred in 260 AD for his deep Christian faith. Consequently, the mosaic is considered to be a monument erected in memory of a fallen Armenian soldier and lays overtop a grave full of bones belonging to Armenian soldiers.\nThe ruins of this chapel, discovered in 1894, are hidden behind an obscure run-down entrance located just outside of the Damascus Gate of Jerusalem's Old City.\nBasilica of San Vitale Mosaics in Ravenna, Italy (Byzantine)\nContributed by Dhara from www.notaboutthemiles.com\nIf you love art history then the Basilica di San Vitale in Ravenna, Italy is a must-visit. While all of the Ravenna mosaics are spectacular, the Basilica di San Vitale mosaics are without question the best of the best! The 6th-century late antique church, with seven other structures in Ravenna, is designated a UNESCO World Heritage site. Construction on the church was completed in the year 547 AD.\nWhile the presbytery and choir are covered in mosaics, there are three main mosaics that grab the bulk of your attention. In the apse above the altar is a gorgeous mosaic of Jesus, seated on a globe and flanked by an angel on either side. At the bottom of the side walls of the apse are two of the most famous Byzantine mosaics in the world, one depicting Emperor Justinian and his courtiers, and the other depicting Empress Theodora and her ladies in waiting. The colors and details of the mosaics are incredible and will leave you awestruck. Theodora's jewels, the colors in the robes of the ladies, and the beautiful backdrop make the Theodora panel one you'll want to examine at length. The Justinian panel shows the Emperor in the center, with members of the military and court on his one side and members of the clergy on his other, indicating that he is the head of both church and state.\nAllow plenty of time to view the magnificent mosaics of Basilica di San Vitale when you visit Ravenna!\nMadaba Map Mosaic in Madaba, Jordan (Byzantine)\nContributed by Cosette from www.karstravels.com\nThe Saint George church in Madaba, Jordan houses a mosaic treasure from early Christianity. It’s a map of Palestine and it’s the oldest in existence. The map dates from 560 AD and it is a floor mosaic. The map is located in the apse of the church and faces eastward towards the altar. The Saint George church is a Greek Orthodox church from the 19th century. The ancient mosaic map is among the remnants of an early Byzantine church that once stood on that spot. The map was discovered in 1884 when the Greek Orthodox church was being built. Sadly, much of the mosaic has been lost. However, there’s still enough of it remaining to get a sense and feel of its enormity.\nThe mosaic is about the Holy Land sites and depicts 157 captions in Greek of the major biblical sites from Palestine to Egypt. You can only see about a quarter of the original map, but still get a good feeling of the Holy Land. For example, you can see Mount Sinai, the Dead Sea, fish swimming in the Jordan River, and prominently in the center, Jerusalem.\nNext to the church is an exhibition of the map. Visit this exhibition first, for it explains a lot about the map and makes it easier to recognize the various sites when you see the actual map.\nConstantine and Justinian Mosaic in the Hagia Sofia in Istanbul, Turkey (Byzantine)\nContributed by Ildiko at www.indulgewithildi.com\nOnce Emperor Constantine granted the Christians freedom of religion in 325 AD, it didn't take long for Christianity to become the official religion of the Roman state, in 380 AD. With that, the Roman emperor invariably became the leader of both state and religion. Constantine re-located the governance of the Roman Empire to the east and established Constantinople. There he built a church.\nAt that same location in Constantinople a couple of centuries later, Emperor Justinian re-built a new and significantly larger church to reflect the immense wealth and power of his empire. His church came to be known as the Hagia Sophia, the Church of the Holy Wisdom. Justinian filled it lavishly with slabs of marble, granite, porphyry, and precious stones from across the empire. Interestingly, the original Byzantine mosaics that the craftsmen laid were simple Christian symbols, such as crosses and fish, as well as floral and geometric motifs. It wasn’t until the post-iconoclastic period in the mid 9th and early 10th c. that Byzantine artists decorated the Hagia Sophia with gleaming, radiant golden mosaics featuring Christian scenes and figures, as well as imperial ones. Both Emperor Justinian and his wife, Empress Theodora, are represented in several scenes.\nOne byzantine mosaic panel located at the southwestern entrance of the basicila, is particularly impressive. In gleaming golden and polychrome tesserae, the scene depicts four figures. On the right is pictured Emperor Constantine I presenting the center figures, the Virgin Mary with the Christ Child on her lap, with a model of the city of Constantinople. The figure on the left is Emperor Justinian I presenting the Virgin Mary with a model of the Hagia Sophia. Both emperors are offering the blessed Mary their unique gifts.\nPantocrator Mosaic in Monreale, Sicily (Byzantine)\nContributed by Ildiko at www.indulgewithildi.com\nThe Cathedral of Monreale on the outskirts of Palermo, Sicily is also known as ‘The City of the Golden Temple.' Germanic invaders, the Normans, gradually settled, occupied, and ultimately overtook the island of Sicily beginning in the 11th century under Roger I. With full papal endorsement and a desire to restore Christianity to the area previously taken over by Arabs, the Normans built massive fortress churches throughout Sicily. Under William II, ecclesiastical authority was transferred to Monreale, just outside of Palermo, where a monastic Abbey was established between 1172-1176 AD. The resultant cathedral, by way of the splendor and magnificence of its architecture, boldly displayed the regal power and wealth of the Norman sovereigns.\nUpon entering the cathedral you see gleaming golden and polychrome tesserae on every wall column and arch, reaching out into every conceivable space. Over an area of 8000 square meters, radiant mosaics pictorially tell the story of the Christian faith beginning with Creation, the Old Testament stories, the Life of Christ, the Apostles, and ending with the Evangelists announcing the word of Christ and His church to the world. The mosaics were not just decorative, but intent on conveying a message aimed at the very often illiterate populace… a message of faith in Christ the Savior.\nThe crown jewel of the mosaics is not surprisingly Jesus, the Pantocrator, looming large in the apse vault over the main alter. The entire image spans 13 meters. Jesus is wrapped in rich pleated robes and His arms are extended, welcoming all. His right hand is held in a gesture of a blessing and in His left hand he holds a bible opened to a page in the gospel. The gospel inscriptions are written in both Greek and Latin and read ‘I am the light of the world. He who follows shall not walk in the darkness.' He is surrounded by his Heavenly Court of angels, prophets, and saints.\nThe entire cathedral is spectacular aesthetically, architecturally, and artistically. If you travel to Sicily, be sure to include a visit to Monreale so you can experience this treasure firsthand.\nSo, there you have it ...\nTen stunning mosaics for you to seek out and admire as you travel the vast stretches of the former Roman and Byzantine Empires. On those journeys, you will undoubtedly see MANY, MANY more such mosaics. I hope you are on the lookout for these beautiful works of art and slow down long enough to appreciate the fine craftsmanship of the ancient Roman and Byzantine artists.","The structure and appearance of Byzantine churches evolved significantly during the thousand year history of that empire. Early churches were based closely on patterns drawn from Roman civic and religious architecture. Churches constructed during the middle years of the Byzantine Empire tended to follow a unique architectural plan featuring large and richly-decorated domes. Byzantine churches erected during the waning years of the empire were often less richly-decorated, and began to feature a wall of icons.\nThe first Byzantine churches were built on a Roman model, as the Byzantine Empire was the Eastern Roman Empire. These churches typically featured a basilica layout. This type of floor plan features twin rows of columns that partially separate aisles along the side of a rectangular structure, and also serve to support the roof. A curved apse is usually located at the end of the basilica. Wings were sometimes added to this structure, creating a cruciform shape, but were generally shorter than the main hall of the basilica.\nAs the culture of the Byzantine Empire became more thoroughly Greek, a new style of Byzantine church emerged. The Hagia Sophia, perhaps the most famous Byzantine building of all, showcases the key features of this style. In this church, there is a central dome, and four wings of equal length lead off from that dome. This is a substantial departure from a traditional basilica plan and was made possible by architectural advances that made the construction of larger domes possible.\nReligious art in Byzantine churches typically employed rich materials to decorate most visible surfaces. Churches in wealthy regions would be covered entirely in mosaics, an art at which the Byzantines excelled. Glass shards and gold leaf were used together to create vivid colors and to enhance the impact of the light that was allowed into Byzantine churches by improved dome construction. Marble and other expensive materials were used to make churches more beautiful, and although some churches featured religious frescos, mosaics were preferred.\nArtwork in Byzantine churches usually depicted stylized religious figures. These figures were meant to convey a symbolic and spiritual message, rather than to precisely depict the human form. Early churches, such as San Vitale in Ravenna, did sometimes depict recognizable human figures, but this became much less common in later years. The depiction of the human form, even for religious reasons, was controversial in the Byzantine church, and a period of iconoclasm began in the 700s, during which much church art was destroyed. Churches erected during this period were typically not ornamented with images of human beings, even stylized ones.\nIn the waning years of the Empire, icons were once more embraced. Byzantine churches built in the last centuries of the Byzantine Empire not only featured religious images on their walls but added a wall of icons at the front of the church. This wall came to be entirely covered in Byzantine icons, painted in the stylized manner that had developed centuries earlier. Church decoration during this period was generally less lavish, as the Empire’s fortunes were fading."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:39cbd4c2-b2b6-48f5-b82d-1afac595687c>","<urn:uuid:387d53e5-0114-40b7-8d7b-d6a0d2b1cd52>"],"error":null}
{"question":"What are the key security objectives of the Shanghai Cooperation Organization (SCO), and how significant is its economic cooperation between member states?","answer":"The SCO was established to moderate US unipolar influence and limit NATO's expansion into post-Soviet space, serving as a security-oriented institution rather than an economic one. However, economically, the SCO has shown significant growth with total trade volume reaching 255 billion US dollars in 2018, marking a 17.2% increase. By 2019, China's investment in SCO member states exceeded 87 billion dollars, and according to IMF projections, the SCO region's economic potential will surpass G7 economies by 2023. The organization has developed 30 different mechanisms and emphasizes collaboration in trade, investment, transport, infrastructure, agriculture, and customs services.","context":["India is riding high on multilateralism in 2023. From the Shanghai Cooperation Organization (SCO) to the G-20 meetings, the Indian establishment is busy playing host to leaders from the Global South and the North. Last week, India was busy with the SCO Foreign Ministers’ Meeting in Goa on the Indian west coast.\nHosting the meeting, Chief Minister of Goa Dr. Pamod Sawant tweeted, “Indian presidency is driven by a commitment to SECURE SCO. Its key focus areas are startups, traditional medicine, youth empowerment, heritage and science & technology.” Highlighting the importance of India’s engagement with the SCO, an official statement reportedly said that the engagements with SCO have “helped India promote its relations with the countries in the region with which India has shared civilizational linkages, and is considered India’s extended neighborhood.”\nThe SCO is one of the older minilateral regional groupings that India is a part of. The group consists of Russia, China, and the Central Asian states. Pakistan and India became full members of the SCO in 2017. There are also currently four observer states – Afghanistan, Mongolia, Iran, and Belarus – as well as six dialogue partners – Armenia, Azerbaijan, Cambodia, Nepal, Sri Lanka, and Turkey. Iran and Belarus are anticipated to become full members at this year’s SCO Summit in July. This, along with the fact that Russia and China dominate the SCO, indicate the anti-Western character and goals of the grouping.\nEstablished in 1996 as the Shanghai Five before expanding into the SCO in 2003, the group was meant to moderate the U.S. unipolar moment and limit Washington’s strategic pursuit in Russia’s backyard, or its “near abroad” as Russia calls it. From China’s viewpoint, it was also a platform to strengthen its economic outreach in Central Asia, although Russia has taken pains to ensure that the SCO remains a security-oriented institution rather than an economic one. For both Russia and China, it was also an instrument to limit the NATO’s expansion into the post-Soviet strategic space.\nAll of these ideals may have had some resonance in New Delhi until even a decade ago when India was less aligned with new security partners such as the United States, Japan, France, and Australia. It is not clear if these SCO goals are still consistent with India’s strategic goals, however. Today, under changed geopolitical circumstances, driven by the structural changes of Asian and global geopolitics, India has more in common with its new security partners than the SCO partners, especially Russia and China.\nThat being said, what were the substantial outcomes at the SCO Foreign Ministers’ Meeting? India’s External Affairs Minister Dr. S. Jaishankar in his opening remarks at the meeting highlighted what Prime Minister Narendra Modi had articulated as India’s goal for the SCO under its presidency – a “SECURE” SCO. “SECURE” is an acronym for “Security, Economic development, Connectivity, Unity, Respect for sovereignty and territorial integrity and Environmental protection.”\nJaishankar then went on to highlight major challenges including global supply chain disruptions, which have resulted in adverse effects on the supply of energy, food, and fertilizers, with most of the impacts being felt particularly by developing countries. Jaishankar said that these disruptions have led to “a credibility and trust deficit in the ability of global institutions to manage challenges in a timely and efficient manner.” In the minister’s view, the continuing failures and challenges of multilateral institutions can be a positive trigger for groupings like the SCO to frame a cooperative agenda. But this is going to be an uphill task for India.\nWith India’s ties with China and Pakistan going through one of their worst phases, it is difficult for India to pursue whatever limited agenda it may have with the SCO. Also, many of these disruptions were also a direct result of the Russian invasion of Ukraine in February 2022, which was not mentioned even once at the SCO meeting. With Russia and China continuing to form a united front against the United States and the liberal international world order, it is unclear what India can hope to achieve through mechanisms such as the SCO, or the RIC (Russia-India-China) or BRICS (Brazil-Russia-India-China-South Africa) groupings. But these efforts also are a strain on India’s limited diplomatic capacity, as former Indian Foreign Secretary Ambassador Shyam Saran wrote in a recent article.\nDespite growing concerns from outside the Indian government, New Delhi appears quite confident that it can straddle the intensifying global divisions and keep all of India’s various partners happy. Whether this confidence is well founded or not is something that will become clear in the next few years.","The Shanghai Cooperation Organization (SCO), widely regarded as Alliance of the East, is an intergovernmental organization established in Shanghai on 15 June 2001. The SCO at present comprises of eight Member States including China, India, Kazakhstan, Kyrgyzstan, Russia, Pakistan, Tajikistan and Uzbekistan, four Observer States including Afghanistan, Belarus, Iran, and Mongolia and six “Dialogue Partners” including Armenia, Azerbaijan, Cambodia, Nepal, Sri Lanka and Turkey. In the recent decades, Globalisation has appeared as the major trend in international relations and keeps on deepening the relations between countries as well as regions of the world. The speedy growth of contemporary technologies in areas of transport, communications, and information delivery and diffusion is contributing to the formation of a unified international community of states. Collectively the states can fight together to cope with the challenges. In the same way, Regionalisation, the stable procedure of structuring the new forms of dealings between the states by involving the world’s macro-regions, has also appeared to be another primary trend in the growth of global order.\nWith the passage of time while realizing the need of the time more countries are determined to structure a system of steady ties with their neighbours. This trend and formulation not only enhance their own potential but also facilitate the regional entities by searching out the solutions of the emerging challenges. All Such regionalisation has taken gradually more varied forms. Regional integration is one of the more evident purposes of all such forums. This regional integration includes institutionalization of supranational regulatory systems, more stretchy models of collaboration in a range of areas that advance at diverse speeds and at different levels as well as new varied type models of partnership. The aims and objectives of such partnerships are scrutinized first and foremost by the requirement to deal with the regional tribulations by paying attention to the frequent desire for dialogue. This way the regional entities can find out the ways and approaches to attain equally advantageous and sustainable development for the whole region.\nIt is imperative to highlight that partnership within the SCO vary from a conventional coalition. One of the most significant consequences of the SCO summit in Astana in June 2017 was the appointment of India and Pakistan as full-fledged SCO members. This way, both countries became unconditional signatories to all documents of SCO and are committed to contribute productively to strengthen the cooperation within the framework of the Organisation. Altogether with the inclusion of four nuclear powers China, Russia, Pakistan and India, the SCO has become a forum to support for maintaining international strategic stability. On the other hand the forum is also known as a foundation stone of the Eurasian continent.\nSCO is an influential transcontinental organization extending from the Arctic in the north to the Indian Ocean in the south, and from Lianyungang, China in the east to Kaliningrad, and Russia in the West. This also shows the impact which this organization has with regards to geographical boundaries. The SCO was initially envisaged as a multifaceted organization with three levels of collaboration which are the foundations of this forum. These include cooperation in politics and security, cooperation in trade and economic activity, as well as cooperation in the development of cultural and humanitarian ties.\nSCO’s ultimate goal is the growth and prosperity of the population living under the jurisdiction of this organization.\nAs the world is experiencing major reconfigurations at the financial fronts and the international economy has a direct link with the mounting instability and doubts, SCO need to come forward with a more productive role in order to produce tangible results. SCO member states jointly have a huge market. The member states have a major portion of the world’s mineral resources, as well as a huge industrial base. Both these fields are highly imperative and helpful while implementing successful and mutually beneficial trade and economic projects. The legal framework of the organization consisted of 122 documents associated to the economy. The major purpose of these documents is to facilitate communication between the states on trade, banking, financial and investment activities, manufacturing, agriculture, transport, telecommunications, customs, and the progress of tourism.\nTo further enhance the trade and financial activities substantial efforts are needed under the umbrella of this forum to guarantee the projects related to Multilateral Trade and Economic Cooperation of the Member States. The member states have already signed numerous international agreements regarding banking and financial activities and the improvement of business, agriculture, as well as tourism. The Organisation is also emphasizing on the formation of SCO Development Bank as well as a Special Account that would offer financial help for project activities. With the inclusion of Pakistan and India, the two important Asian states,\nSCO has become the world’s leading trans-regional alliance in terms of the total area, population, and monetary prospective of its member states. The entrance of the new and influential states unlocks the door extensively to new prospects.\nAll these prospects are basically additional support to further augment development of the infinite potential of the “Shanghai Eight” in all areas of life pertaining to mutual interest within the organization as well as in the international arena. The SCO has around 30 diverse mechanisms that have developed over the years with the help of the founding members and the support of those included later. It is also pertinent to mention that SCO put together all of its resolutions through consensus. Become accustomed to these beforehand recognized formats and operational methods of communication is not an effortless task, it also requires alteration and evaluation with the passage of time while adapting the contemporary trends.\nIn 2018, with the joint efforts of the member states the “Big Eurasian Eight” has revealed that SCO is competent enough to develop and progress in retort to the necessities of the time, altering realities, and objective needs. The SCO has evidently been growing the economic aspect of its cooperation in recent years. The 2018 summit in Qingdao confirmed this tendency. The Organisation is vigorously increasing support related to trade and investment, transport, infrastructure, agriculture, as well as custom services. Without any doubt the organization is amalgamation of manufacturing clusters, including high-tech and science-intensive centres. The member states hold the full range of mineral affluence. This further makes it easy for SCO to formulate any type of production cycle, from the removal of raw materials to their processing with a high degree of additional value.\nThe notion of collaboration of the SCO member states in the field of environmental protection was agreed by the Council of Heads of State, is an imperative step to enhance cooperation in the field of environmental administration.\nDue to their geographic proximity, the economies of the SCO member states affect one another as their regional economic agreements and shared programs within the Organisation’s framework are the basis of their progressive combined growth. This year marks the 20th anniversary of the founding of the Shanghai Cooperation Organization (SCO), and the intergovernmental organization is anticipated to see another expansion by giving way the status of dialogue partners to Saudi Arabia and Egypt, a verdict that was agreed at the recent meeting of the SCO Council of Foreign Ministers in Dushanbe, Tajikistan. Financial cooperation is one of most significant pillars of the framework, through which regional security can also be successfully endorsed. Amid hasty withdrawal of US and NATO forces from Afghanistan and the ongoing spread of the coronavirus as well as the West’s speedy hegemony, it is vital for SCO members to hasten the layout of collaboration for mutual development and progress the industrial chain association of the region. SCO has contributed immensely in the areas of economy and trade. In 2018, the total trade volume among the member states reached 255 billion US dollars, a year-on-year increase of 17.2 percent.\nFurthermore, in the 2019, China’s total investment in SCO member states exceeded 87 billion U.S. dollars. Numerous large-scale energy-related, mineral and industrial manufacturing projects have made advancement. According to the IMF report, in the next five years (up to the year 2023), the economic potential in the SCO region will exceed that of the G7 economies. During the first five months of 2021, trade between China and Central Asian countries remained $16.86 billion, up 24.5 percent year-on-year. A multimodal transportation center in Qingdao confirmed 297 trips by China-Europe freight trains, among which 243 connected China and Central Asian economies, up 57 percent. The freight trains also opened a portal for SCO members to enter the Asia-Pacific market.\nBeside the endorsement of Belt and Road Initiative (BRI), investment among the SCO member stated has also increased. For example, China has become the chief source of investment for Tajikistan, with a cumulative investment exceeding $2 billion. More prominently, the economies under the SCO have enormous potential to further discover collaboration in rising areas, such as new or green energy, digital trade as well as the health sector in contemporary scenario. During the pandemic, the e-commerce among the member countries extended quickly in the region. Profound transformative procedures are visibly unfolding in this remarkable Eurasian region. The development of Asia will further strengthen the trend towards the structuring of a regional system based on economic interrelations and mutual interests."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:85866948-0e19-436c-9cfd-33180d25970d>","<urn:uuid:033c49ec-7af1-4730-8133-7c112ed7a11a>"],"error":null}
{"question":"How does PECS help autistic children communicate, and what role does ABA therapy play in reinforcing these communication skills?","answer":"PECS (Picture Education Communication System) helps autistic children communicate through a handheld device with buttons featuring images corresponding to words and phrases. The system has six stages where children learn to use images to communicate, ask questions, and express feelings. Through a speaker attached to the device, children can produce words and phrases by pressing buttons. Meanwhile, ABA therapy complements this by focusing on increasing desired behaviors and communication skills through reinforcement. ABA therapy involves collecting data during sessions, with Behavior Technicians and Board Certified Behavior Analysts working together to make data-driven decisions. Both PECS and ABA are particularly beneficial for children with autism, with research showing that early and intensive ABA intervention can improve development and reduce the need for special services.","context":["Picture education communication system, also known as more simply PECS, is an education system designed to help children of various ages. Teachers, therapists and others working with children use the system as a way to get responses from those who have problems communicating and those who cannot communicate verbally. PECS uses six different stages that help children learn how to respond to direct questions and make comments to their teachers and counselors.\nStages of PECS\nPECS has six stages that parents and others working with children can use. The first stage teaches students how to communicate via images found on the device itself. During the second stage, they learn how to ask questions or make comments to others without the need for that individual to make any comments. Later stages teach children how to use different symbols and images to get what they want and how to press different buttons to put together sentences and phrases. A picture education communication system can also teach a child how to respond to questions asked by others and how to express their feelings.\nHow Does PECS Work?\nThese systems look like small devices that children can hold in their hands. The device features images on different buttons that correspond to different words. Children learn how to press those buttons to communicate with others. Most devices have a speaker attached to one side that produces words and phrases based on the buttons that the child pressed. Therapists often start out slowly and show children how they can receive rewards for simply pressing a specific button on the device before showing them how they can put words together to form sentences and how they can talk about their thoughts and feelings with the machine.\nWho Can Use the System?\nMost of the children using these systems have autism. Nonverbal autistic children cannot or do not speak and have a difficult time expressing how they feel or communicating with others. According to Pyramid Educational Consultants, these machines are also suitable for those who have physical impairments that prevent them from communicating with loved ones and for those who have cognitive disabilities too. The company found that some children who use these machines will eventually learn how to use their own voices as they develop strong speaking skills. Others can use the devices as a replacement for their own voices.\nUse in the Field\nThere are a few different ways in which educators can use a picture education communication system in the field. The first is in the special education classroom of a public or private school. Teachers working in those schools can use the devices to cut down on frustrations among students and to control the environment. Therapists and counselors who work with children with autism and other cognitive issues can also use these systems in private homes. Many parents also invest in a PECS system as a way to communicate with their own children and to help their children talk with others.\nRelated Resource: Top 10 Best Applied Behavior Analysis Online Programs\nChildren diagnosed with autism and similar conditions often have a difficult time speaking with others and even being around others. A picture education communication system, also known as PECS, allows those children to communicate with others via buttons on the device that correspond with objects, emotions and simple words.","What is ABA?\nBehavior Analysis is the scientific study of behavior and Applied Behavior Analysis (ABA) is the application of the principles of behavior. ABA focuses on the functions of behavior and how learning takes place. ABA therapy applies these principles to help increase desired behaviors and decrease problem behaviors. For instance, it is used to increase skills such as language and communication, attention, social skills, daily living skills, and academics. It is also used to decrease maladaptive and harmful behaviors. ABA therapy focuses on antecedents (what happens before a behavior occurs), behavior (what is the behavior), and consequences (what happens after the behavior). ABA suggests that children are more likely to repeat behaviors that are reinforced, and they are less likely to continue behaviors that are not reinforced. It is also important to note that collecting data is critical in ABA therapy. The Behavior Technicians, working with Board Certified Behavior Analysts (BCBA), collect data during the therapy sessions, and the BCBA makes data-driven decisions. ABA training is most beneficial when parents participate in training as well. That way, parents can reinforce positive behaviors effectively and help their child reach their highest potential.\nWhy ABA Therapy?\nAs every child with an is unique, therefore, each child should receive individualized treatment that meets their specific needs. Applied Behavior Analysis (ABA) is one of the most widely accepted therapies for children with autism spectrum disorder (Autism - Treatment Overview, 2015). ABA therapy is most effective as an early intervention when children are younger than age five. However, older children can also benefit from ABA. Recent studies confirm that ABA techniques are effective for building important life skills in adolescence and adults with autism, such as helping individuals transit successfully into independent living and employment.\nMany studies confirm that ABA is effective in increasing behaviors and teaching new skills (National Autism Center, 2015). In addition, research demonstrates that ABA is effective in reducing problem behavior (NAC, 2015). Several studies also indicate that early and intensive intervention of ABA (beginning before age 4 and more than 20 hours per week), can increase the chance of improvements in development and reduce the need for special services (Reichow, 2012). The United States Surgeon General (1999) concluded, “Thirty years of research demonstrated the efficacy of applied behavioral methods in reducing inappropriate behavior and in increasing communication, learning and appropriate social behavior.”\nA quality ABA program can improve children’s ability to express their own personality and preferences by teaching them the skills they need to communicate, play, and enjoy life. ABA is effective because it teaches children how to learn! Contact us at email@example.com to find out more about how ABA can help propel your child’s progress!\nABA RESOURCES: WHAT IS ABA? (n.d.). Retrieved from Center for Autism and Related Disorders : http://www.centerforautism.com/aba-therapy.aspx\nApplied Behavior Analysis (ABA). (2017). Retrieved from Autism Speaks : https://www.autismspeaks.org/what-autism/treatment/applied-behavior-analysis-aba\nAutism - Treatment Overview. (2015). Retrieved from WebMD: http://www.webmd.com/brain/autism/autism-treatment-overview\nAutism Teaching Methods. (2017). Retrieved from AutismWeb: http://www.autismweb.com/aba.htm\nNational Autism Center. (2015). Findings and conclusions: National standards project, phase 2. Randolph, MA: Author.\nReichow, B. (2012). Overview of meta-analyses on early intensive behavioral intervention for young children with autism spectrum disorders. Journal of Autism and Developmental Disorders, 42, 512-520.\nTherapies for Autism Spectrum Disorder. (2016). Retrieved from WebMD: http://www.webmd.com/brain/autism/autism-therapies-aba-rdi-and-sensory-therapies#1\nUnited States Surgeon General (1998). Mental health: A report of the Surgeon General. Washington, DC: Author."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:563ee58c-e941-4b58-85cc-fc24a9babca1>","<urn:uuid:786de9aa-cde8-4d4f-82fb-261e03eca659>"],"error":null}
{"question":"What evidence exists for the effectiveness of alternative education and farming methods compared to traditional approaches, considering both homeschooling's impact on creativity and vertical farming's resource efficiency?","answer":"Research demonstrates that homeschooling shows superior outcomes in creative thinking development, with a study of 549 participants aged 8-12 revealing that homeschooled students scored significantly higher on creative thinking tests compared to public school students. This finding remained consistent even after controlling for student background variables. Similarly, vertical farming has proven more effective than traditional farming in several aspects, particularly resource management. It requires 90% less water than conventional agriculture and significantly less space, while allowing year-round crop production in controlled environments. The system enables precise control of growth factors like light, temperature, humidity, and nutrients, offering a more efficient alternative to traditional farming methods that require extensive land use and resources. Both alternative approaches demonstrate measurable advantages over conventional methods in their respective fields.","context":["Homeschooling has grown phenomenally during the past 30 years around the world, and especially during the past two years. For example, “The number of homeschooling families approved by the Israel Ministry of Education increased by 700% from 2005 through 2019” (Madara & BenDavid-Hadar, 2021). Numerous studies have examined the demographics and academic achievement of home-educating families and the students (e.g., Ray, 2017). An increasing number of scholars have become focused on an increasingly wider variety of topics with respect to homeschooling. Recently, Michal Unger Madara and Iris BenDavid-Hadar probed the creative thinking and social competencies of home-educated children. This brief review will touch upon only the former topic in the study.\nThe researchers aimed to evaluate the creative thinking of homeschool children. “Creative thinking refers to the creative process of discovering new affiliations, conclusions, and connections …” (p. 7). To do this, the investigators gathered data from 549 participants who were between the ages of 8 and 12 years old. Of these, 280 were homeschooled students and 269 were public conventional (traditional) school students.\nCreative thinking was the explained variable and was measured by using The Test for Creative Thinking Drawing Production (TCT-DP). “The test includes six geometric shapes inside a large square frame that serve as the test pattern. Participants are asked to complete the picture as they see fit” (p. 12).\nThe explanatory variables were (a) child’s background variables (i.e., sex, age, and country of origin), (b) parents’ background variables (i.e., education, number of children; and (c) background variables of the community (i.e., residential area). The mediating or independent variable was homeschooling or public schooling. Six regression models examined the relationships between the variables.\nNone of the background variables was statistically significant in terms of explaining variance in creative thinking. “Model III reveals a significant statistical relationship between type of education and creative thinking (β = 0.43***). More specifically, students who are homeschooled have a higher level of creative thinking than students attending public schools” (p. 18). Furthermore, the differences in creative thinking were retained even after statistically controlling for student background variables.\nCiting others’ research, scholars Madara and BenDavid-Hadar point out that “… schools have several drawbacks, including traditional teaching methods that interfere with the creative process and even reduce creativity … (p. 19). They also note to a recent study in Jakarta, Indonesia that found homeschooled children ranging in age from 15 to 19 had higher levels of creative thinking than their peers in conventional education settings. Finally, the researchers conclude that their “… study shows that homeschooling may be more effective in terms of developing creative thinking … than traditional learning. Therefore, it can offer a high-quality alternative to public education or private schools for those who choose it” (p. 21).\nThis is a well-planned, -executed, and –reported study. The area of the creative thinking of the homeschooled has been explored in a very limited number of studies and this piece is a stellar addition to the research base.\nMadar, Michal Unger; & BenDavid-Hadar, Iris. (2021). Does home schooling improve creative thinking and social competencies among children? Home schooling in Israel. Journal of School Choice, DOI: https://doi.org/10.1080/15582159.2021.1977584","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c71550e9-ad0a-4443-9fd2-f3bb5e0f9c3d>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"What was Amy Adams' film debut and who were her co-stars in that movie?","answer":"Amy Adams made her film debut in Drop Dead Gorgeous (1999), starring alongside Kirsten Dunst, Ellen Barkin, Brittany Murphy, Allison Janney, Denise Richards, and Kirstie Alley.","context":["Amy Lou Adams (born August 20, 1974) is an American actress. Known for both comedic and dramatic roles, she has featured three times in annual rankings of the world’s highest-paid actresses. She made her film debut in Drop Dead Gorgeous (1999), with Kirsten Dunst, Ellen Barkin, Brittany Murphy, Allison Janney, Denise Richards, and Kirstie Alley.\nFilms in the early 2000s include Psycho Beach Party (2001), with Lauren Ambrose, Thomas Gibson, Nicholas Brendon, and Matt Keeslar; The Slaughter Rule (2002), with Ryan Gosling, David Morse, Clea DuVall, Kelly Lynch, and Eddie Spears; Pumpkin (2002), with Christina Ricci, Hank Harris, and Brenda Blethyn; Serving Sara (2002), with Matthew Perry, Elizabeth Hurley, Bruce Campbell, Vincent Pastore, and Cedric the Entertainer; and Steven Spielberg’s Catch Me If You Can (2002), with Leonardo DiCaprio, Tom Hanks, Christopher Walken, Martin Sheen, and Nathalie Baye.\nAdams received her first Academy Award nomination for Best Supporting Actress for Junebug (2005), with Embeth Davidtz, Ben McKenzie, Alessandro Nivola, Frank Hoyt Taylor, Celia Weston, and Scott Wilson. Other films in the mid 2000s include The Wedding Date (2005), with Debra Messing, Dermot Mulroney, Peter Egan, and Holland Taylor; Standing Still (2005), with Adam Garcia, Aaron Stanford, Melissa Sagemiller, Jon Abrahams, Mena Suvari, Colin Hanks, and James Van Der Beek; Adam McKay’s Talladega Nights: The Ballad of Ricky Bobby (2006), with Will Ferrell, John C. Reilly, Sacha Baron Cohen, Gary Cole, Michael Clarke Duncan, Leslie Bibb, and Jane Lynch; The Ex (2007), with Zach Braff,\nAmanda Peet, Jason Bateman, Charles Grodin, Mia Farrow, Donal Logue, Amy Poehler, and Fred Armisen; a voice role in Underdog (2007), with Jim Belushi, Peter Dinklage, John Slattery, and Patrick Warburton, with the voices of Jason Lee, and Brad Garrett; Enchanted (2007), with Patrick Dempsey, James Marsden, Timothy Spall, Idina Menzel, Rachel Covey, and Susan Sarandon; and Mike Nichols’ Charlie Wilson’s War (2007), with Hanks, Julia Roberts, Philip Seymour Hoffman, and Ned Beatty.\nAdams received her second Academy Award nomination for Best Supporting Actress for John Patrick Shanley’s Doubt (2008), with Meryl Streep, Hoffman, and Viola Davis. Other films in the late 2000s include Miss Pettigrew Lives for a Day (2008), with Frances McDormand, Lee Pace, Ciarán Hinds, Shirley Henderson, and Mark Strong; Sunshine Cleaning (2009), with Emily Blunt, Jason Spevack, Mary Lynn Rajskub, Clifton Collins Jr., Eric Christian Olsen, Kevin Chapman, Steve Zahn, and Alan Arkin; Night at the Museum: Battle of the Smithsonian (2009), with Ben Stiller, Owen Wilson, Hank Azaria, Christopher Guest, Alain Chabat, Steve Coogan, Ricky Gervais, Bill Hader, Jon Bernthal, and Robin Williams; Nora Ephron’s Julie & Julia (2009), with Streep, Stanley Tucci, Chris Messina, and Linda Emond; and Moonlight Serenade (2009), with Alec Newman, Harriet Sansom Harris, and Jeremy Glazer.\nAdams received her third Academy Award nomination for Best Supporting Actress for David O. Russell’s The Fighter (2010), with Mark Wahlberg, Christian Bale, and Melissa Leo. Other films around this time include Leap Year (2010), with Matthew Goode, Adam Scott, and John Lithgow; the anthology film Love & Distrust (2010), with Robert Pattinson, Sam Worthington, Robert Downey Jr., and James Franco; and The Muppets (2011), with Jason Segel, Chris Cooper, and Rashida Jones.\nAdams received her fourth Academy Award nomination for Best Supporting Actress for Paul Thomas Anderson’s The Master (2012), with Joaquin Phoenix, Hoffman, Laura Dern, and Rami Malek. Other films during this time include Trouble with the Curve (2012), with Clint Eastwood, Justin Timberlake, Matthew Lillard, and John Goodman; and On the Road (2012), with Garrett Hedlund, Sam Riley, Kristen Stewart, Alice Braga, Tom Sturridge, Danny Morgan, Elisabeth Moss, Kirsten Dunst, and Viggo Mortensen.\nAdams received her first Academy Award nomination for Best Actress for Russell’s American Hustle (2013), with Bale, Bradley Cooper, Jeremy Renner, Jennifer Lawrence, Louis C.K., Michael Peña, Alessandro Nivola, and Robert De Niro (uncredited). Other films around this time include Man of Steel (2013), with Henry Cavill, Michael Shannon, Kevin Costner, Diane Lane, Laurence Fishburne, Antje Traue, Ayelet Zurer, Christopher Meloni, and Russell Crowe; Spike Jonze’s Her (2013), with Phoenix, Rooney Mara, Olivia Wilde, Chris Pratt, and the voice of Scarlett Johansson; Lullaby (2013), with Hedlund, Richard Jenkins, Anne Archer, Jessica Brown Findlay, Jessica Barden, Terrence Howard, and Jennifer Hudson; and Tim Burton’s Big Eyes (2014), with Christoph Waltz, Danny Huston, Jon Polito, Krysten Ritter, Jason Schwartzman, and Terence Stamp.\nAdams received her fifth Academy Award nomination for Best Supporting Actress for Vice (2018), with Bale, Steve Carell, Sam Rockwell, Justin Kirk, Tyler Perry, Alison Pill, Lily Rabe, and Jesse Plemons. Other films in the late 2010s include Batman v Superman: Dawn of Justice (2016), with Ben Affleck, Cavill, Jesse Eisenberg, Lane, Fishburne, Jeremy Irons, Holly Hunter, and Gal Gadot; Denis Villeneuve’s Arrival (2016), with Renner, Forest Whitaker, Michael Stuhlbarg, and Tzi Ma; Tom Ford’s Nocturnal Animals (2016), with Jake Gyllenhaal, Shannon, Aaron Taylor-Johnson, Isla Fisher, Armie Hammer, Laura Linney, Andrea Riseborough, and Michael Sheen; and Justice League (2017), with Affleck, Cavill, Gadot, Ezra Miller, Jason Momoa, Ray Fisher, Irons, Lane, Connie Nielsen, J.K. Simmons, and Ciarán Hinds.\nFilms in the 2020s include Ron Howard’s Hillbilly Elegy (2020), with Glenn Close, Gabriel Basso, Haley Bennett, Freida Pinto, Bo Hopkins, and Owen Asztalos; Joe Wright’s The Woman in the Window (2021), with Gary Oldman, Anthony Mackie, Fred Hechinger, Wyatt Russell, Brian Tyree Henry, Jennifer Jason Leigh, and Julianne Moore; Dear Evan Hansen (2021), with Ben Platt, Kaitlyn Dever, Amandla Stenberg, Nik Dodani, Colton Ryan, Danny Pino, and Moore; Disenchanted (2022), with Dempsey, Maya Rudolph, Gabriella Baldacchino, Menzel, Marsden, Yvette Nicole Brown, Jayma Mays, and Oscar Nunez.\nAdams also starred in the HBO miniseries Sharp Objects (2018), with Patricia Clarkson, Chris Messina, Eliza Scanlen, Matt Craven, Henry Czerny, Taylor John Smith, Madison Davenport, Miguel Sandoval, Will Chase, Jackson Hurst, Sophia Lillis, Lulu Wilson, and Elizabeth Perkins. She received a nomination for the Golden Globe Award for Best Actress – Miniseries or Television Film.\nEach review will be linked to the title below.\n(*seen originally in theaters)\n(**seen rereleased in theaters)\n- Drop Dead Gorgeous (1999) – directed by Michael Patrick Jann\n- The Chromium Hook (2000) – directed by James Stanger – short\n- Cruel Intentions 2 (2001) – directed by Roger Kumble – straight to video\n- Psycho Beach Party (2001) – directed by Robert Lee King\n- The Slaughter Rule (2002) – directed by Alex Smith & Andrew J. Smith\n- Pumpkin (2002) – directed by Anthony Abrams & Adam Larson Broder\n- Serving Sara (2002)* – directed by Reginald Hudlin\n- Catch Me If You Can (2002) – directed by Steven Spielberg\n- The Last Run (2004) – directed by Jonathan Segal\n- The Wedding Date (2005) – directed by Clare Kilner\n- Standing Still (2005) – directed by Matthew Cole Weiss\n- Junebug (2005) – directed by Phil Morrison\n- Stephen Tobolowsky’s Birthday Party (2005) – directed by Robert Brinkmann – herself – documentary\n- Talladega Nights: The Ballad of Ricky Bobby (2006) – directed by Adam McKay\n- Pennies (2006) – Warner Loughlin & Diana Valentine – short\n- Tenacious D in The Pick of Destiny (2006) – directed by Liam Lynch – cameo\n- The Ex (2007) – directed by Jesse Peretz\n- Underdog (2007) – directed by Frederik Du Chau\n- Enchanted (2007)* – directed by Kevin Lima\n- Charlie Wilson’s War (2007) – directed by Mike Nichols\n- Miss Pettigrew Lives for a Day (2008) – directed by Bharat Nalluri\n- Doubt (2008) – directed by John Patrick Shanley\n- Sunshine Cleaning (2009) – directed by Christine Jeffs\n- Night at the Museum: Battle of the Smithsonian (2009)* – directed by Shawn Levy\n- Julie & Julia (2009) – directed by Nora Ephron\n- Moonlight Serenade (2009) – directed by Giancarlo Tallarico\n- Leap Year (2010) – directed by Anand Tucker\n- Love & Distrust (2010) – directed by Daisy Gili, Darcy Yuille, Eric Kmetz, Warner Loughlin and Diana Valentine, & Lorraine Bracco – anthology – straight to video\n- The Fighter (2010) – directed by David O. Russell\n- The Muppets (2011)* – directed by James Bobin\n- The Master (2012) – directed by Paul Thomas Anderson\n- Trouble with the Curve (2012) – directed by Robert Lorenz\n- One the Road (2012) – directed by Walter Salles\n- Man of Steel (2013) – directed by Zack Snyder\n- Her (2013) – directed by Spike Jonze\n- America Hustle (2013) – directed by David O. Russell\n- Lullaby (2014) – directed by Andrew Levitas\n- Big Eyes (2014) – directed by Tim Burton\n- Batman v Superman: Dawn of Justice (2016) – directed by Zack Snyder\n- Arrival (2016) – directed by Denis Villeneuve\n- Nocturnal Animals (2016) – directed by Tom Ford\n- Justice League (2017) – directed by Zack Snyder & Joss Whedon (uncredited)\n- Sharp Objects (2018) – directed by Jean-Marc Vallée – miniseries\n- Vice (2018) – directed by Adam McKay\n- Hillbilly Elegy (2020) – directed by Ron Howard\n- Zack Snyder’s Justice League (2021) – directed by Zack Snyder\n- The Woman in the Window (2021) – directed by Joe Wright\n- Dear Evan Hansen (2021) – directed by Stephen Chbosky\n- Disenchanted (2022) – directed by Adam Shankman"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:3d87f19d-9811-4e7c-9d17-2da3095f98c5>"],"error":null}
{"question":"I'm an art student studying sound representation in paintings. What techniques did the artwork use to represent audio elements visually?","answer":"The artwork used several techniques to represent sound visually, including vibrational lines (like visible wave forms or waves in water) to represent sound waves around telephone line wires, bird sounds, and insect sounds. They also used jagged wave forms, moving twice up and down, to represent the brief strident chirp of crickets. These visual elements represented both audible vibrations and inaudible vitalist emanations from living matter.","context":["Inspired by Burchfield in Artvoice\nThursday, July 30, 2015\nDual installations by multimedia artist Emil Schult at the Burchfield Penney gallery honor painter Charles Burchfield and musician Charles Ives in multimedia ways. Schult is a musician, painter, computer technologist, among assorted other categories. He has made art for and played in performance with the German new music group Kraftwerk.\nThe Burchfield homage segment-—in complementary conjunction with the gallery current exhibit on Charles Burchfield, on aural synesthesic effects in his watercolors—consists of visual works in a novel painterly technique Schult invented called reverse glass painting that attempt to “analyze the structural and rhythmic elements” of some of Burchfield’s paintings.\nThe reverse glass paintings are not by Schult himself—he has reverse glass paintings of his own in the other segment—but by some of his students at Alfred University. The reverse glass painting technique is not well explained in the exhibit, but each student was given a 16 x 32 inch print of a Charles Burchfield painting and a large piece of plexiglass, and the assignment was “to create a simplified version of the original painting...”\nThe student works are basically abstracts of Burchfield’s basically representational works with abstraction elements. Such as vibrational lines (like visible wave forms, like waves in water) to represent sound waves around telephone line wires (which noticeably vibrate in the wind and make an audible sound, much like an Aeolian harp), or represent bird or insect sounds (audible but invisible vibrations), or represent invisible and inaudible vitalist emanations (but discernible apparently by the artist) from living or once living matter. Trees and telephone poles alike. Or such as his abstract cricket insect forms—jagged wave forms, twice up and down—to represent the brief strident chirp of the cricket.\nIn her reverse glass version of a portion of Burchfield’s Insect Chorus, student Olivia Juarez translates the cricket form into a straw pile of hatch marks, a bamboo sticks jumble with a vaguely Chinese calligraphy look. Milo Harper interprets Orion in December in a way a little reminiscent of Van Gogh’s Starry Night. Isabel Bowser interprets another portion of Insect Chorus as red and silver flame form pyrotechnics. Aodi Lange’s version of Sunrise in the Forest is an epiphany jumble of Matissean large color blocks, darkish below, bright above.\nThe other installation, in the gallery Project Room, is entirely Schult’s work, including reverse glass paintings, some sculptural pieces, and the centerpiece of the installation, a movie with computer-generated imagery and a sound track of Schult’s recomposition of music by Charles Ives, possibly his piece in reference to New England Transcendentalist sages Emerson and Thoreau, The Unanswered Question. The movie is called Unanswered Questions, and amid futuristic imagery—robotic-looking human figures and symmetrical and a-symmetrical abstract changing patterns—asks in English and Chinese questions such as: Who am I? What am I? What was I in the past? Will I die? Shall I exist in the future? What is eternity?\nSchult said he made the work also in Chinese because the Chinese are “the largest [ethnic and language] group in the world,” and he wanted to “share my work with them.” Potential audience.\nSchult’s framed art includes interpretive portraits of some icon figures in areas of electronics and electronic arts, the likes of composers Karlheinz Stockhausen and John Cage, and inventors of electronic musical instruments Harald Bode (Melodium, Melochord, et al., 1930s, 1940s) and Robert Moog (Moog Synthesizer, 1960s). Also, some mandala-like computer chip paintings. One that looks like an early version chip—relatively simple and symmetrical—duplicated as a painting and shallow-relief sculpture. Another more elaborate, called Human Rights Charta. A kind of computer chip world sociogram.\nWhy did he do it all? “In my art,” he says, “I wanted to see things I have never seen before.”\nThe Emil Schult installations continue through September 27. The Charles Burchfield synesthesia aural effects exhibit continues through August 23.\nRead more at www.Artvoice.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:153dc78b-653b-4d57-9490-cc0237138175>"],"error":null}
{"question":"什么是山岳云 what is orographic cloud?","answer":"Orographic clouds are clouds that develop in response to the forced lifting of air by topographical features on the earth's surface, like mountains. When air passes over a mountain, it oscillates up and down as it moves downstream.","context":["- 1 Which of the following cloud types would commonly be found downwind of a mountain group of answer choices?\n- 2 Which cloud will have the highest base?\n- 3 What is the most common ice crystal shape?\n- 4 Why is snow challenging measured?\n- 5 What is an example of orographic clouds?\n- 6 Which set of conditions working together will make the atmosphere more stable?\n- 7 What is the 4 types of clouds?\n- 8 What are 5 major types of clouds?\n- 9 Is fog a cloud?\n- 10 What kind of crystal is formed by frozen water?\n- 11 Why do ice crystals have different shapes?\n- 12 Why are ice crystals symmetrical?\n- 13 Why is snow more challenging than rainfall?\n- 14 What is snow measured in?\n- 15 How does NOAA measure snowfall?\nWhich of the following cloud types would commonly be found downwind of a mountain group of answer choices?\nWave clouds that often form over or downwind of a mountain are called: lenticular clouds.\nWhich cloud will have the highest base?\nHigh-level clouds: The three main types of high clouds are cirrus, cirrostratus, and cirrocumulus. Cirrus clouds are wispy, feathery, and composed entirely of ice crystals.\nWhat is the most common ice crystal shape?\nThe most common ice crystal shape is: graupel.\nWhy is snow challenging measured?\nWhy is snow challenging to measure? Snowy areas are too inaccessible to measure. Snow accumulations vary greatly from one spot to the next. It is too difficult to block wind around gauges.\nWhat is an example of orographic clouds?\nOrographic Clouds: forced by the earth’s topography. Orographic clouds are clouds that develop in response to the forced lifting of air by topographical features on the earth’s surface, like mountains for example. Air passing over a mountain will oscillate up and down as it moves downstream (see diagram below).\nWhich set of conditions working together will make the atmosphere more stable?\nWhich set of conditions, working together, will make the atmosphere the most stable? Cool the surface and Warm the air aloft.\nWhat is the 4 types of clouds?\nThe Four Core Types of Clouds\n- Cirro-form. The Latin word ‘cirro’ means curl of hair.\n- Cumulo-form. Generally detached clouds, they look like white fluffy cotton balls.\n- Strato-form. From the Latin word for ‘layer’ these clouds are usually broad and fairly wide spread appearing like a blanket.\nWhat are 5 major types of clouds?\nTen Basic Clouds\n- Cirrus (Ci), Cirrocumulus (Cc), and Cirrostratus (Cs) are high level clouds.\n- Altocumulus (Ac), Altostratus (As), and Nimbostratus (Ns) are mid-level clouds They are composed primarily of water droplets.\n- Cumulus (Cu), Stratocumulus (Sc), Stratus (St), and Cumulonimbus (Cb ) are low clouds composed of water droplets.\nIs fog a cloud?\nFog is a kind of cloud that touches the ground. Fog forms when the air near the ground cools enough to turn its water vapor into liquid water or ice. There are many different types of fog, too. Ice fog forms when the air near the ground is cold enough to turn the water in fog into ice crystals.\nWhat kind of crystal is formed by frozen water?\nOn terrestrial objects the ice crystal is the elemental unit of hoarfrost in all of its various forms. Ice crystals that form in slightly supercooled water are termed frazil. Ice originating as frozen water (e.g., hail, graupel, and lake ice ) still has hexagonal symmetry but lacks any external hexagonal form.\nWhy do ice crystals have different shapes?\nThe shapes of snowflakes are influenced by the temperature and humidity of the atmosphere. Depending on the temperature and humidity of the air where the snowflakes form, the resulting ice crystals will grow into a myriad of different shapes.\nWhy are ice crystals symmetrical?\nThe more detailed explanation is this: The ice crystals that make up snowflakes are symmetrical (or patterned) because they reflect the internal order of the crystal’s water molecules as they arrange themselves in predetermined spaces (known as “crystallization”) to form a six-sided snowflake.\nWhy is snow more challenging than rainfall?\nIt is more difficult to measure snowfall amount than rainfall amount because of the variations in the density of the snow. Density of snow greatly varies from place to place, and therefore, similar volumes of snow collected from two different places do not necessarily give the same amount of water when it melts.\nWhat is snow measured in?\nWhile the depth of snow is normally measured in centimetres, the measurement of melted snow (water equivalent) is in millimetres. An estimate of the snow depth can be obtained by multiplying the water equivalent by ten.\nHow does NOAA measure snowfall?\nMEASURING SNOW Snowfall is measured to the nearest tenth of an inch. Measure the greatest amount of snowfall that has accumulated on your snowboard since the last observation. You can measure on a wooden deck or ground if a snowboard is not available. Snowfall should not be measured more than 4 times in 24 hours."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f2fa93ee-afac-4ec6-ae09-2f0df69bbaf4>"],"error":null}
{"question":"What are the parallels between liver progenitor cells and bone graft substitutes in tissue regeneration?","answer":"Liver progenitor cells and bone graft substitutes both contribute to tissue regeneration but through different mechanisms. Liver progenitor cells serve as a facultative source of regeneration when hepatocyte regeneration fails, acting as resident cells that can contribute to liver regeneration. Similarly, bone graft substitutes serve as scaffolds for tissue regeneration, but they work through various mechanisms depending on their type. Cell-based bone grafts use stem cells or osteoblasts to promote regeneration, while factor-based grafts utilize growth factors to stimulate bone formation. Both approaches aim to restore damaged tissue, with liver progenitors responding to factors like HMGB1 released from damaged hepatocytes, while bone graft substitutes provide a structural framework for new tissue growth and can incorporate growth factors or cells to enhance regeneration.","context":["Kennedy Trust Prize Studentships\nPromoting liver regeneration\n- Project No: KTPS-Clinical-3\n- Intake: 2021 KTPS-Clinical\nNon-alcoholic fatty liver disease (NAFLD) has emerged as the most common cause of chronic liver in the Western world, affecting 30% of the adult population in the USA and encompasses non-alcoholic fatty liver (NAFL) and non-alcoholic steatohepatitis (NASH). NAFL and NASH each carry a significant morbidity and mortality with 2-3% and 15-20% respectively progressing to cirrhosis, with some developing hepatocellular carcinoma. Indeed, NAFLD will soon represent the principal indication for liver transplantation in the USA, with the attendant problems of immunosuppression and shortages of suitable donors. Despite the huge disease burden, there is no approved therapy for patients with NASH. Immense effort is concentrated in developing potential pharmacological treatments and several agents are being evaluated in early phase clinical trials. Even once effective therapeutics have been developed, the focus will remain on preventing further damage and relying on the regenerative capacity of the liver aided by improvements in dietary habits, bariatric surgery and exercise.\nA treatment that promotes regeneration hepatocellular damage would revolutionise the management of patients. Liver has an inherent capacity to regenerate and we have previously identified a pool of resident progenitor cells in the liver that together with hepatocytes contribute to liver regeneration (Raven et al., 2017). These progenitor cells act as the facultative source of regeneration when hepatocyte regeneration fails. HMGB1 is released from damaged hepatocytes during liver injury, in particular, in the setting of cellular senescence. The release of HMGB1 from senescing hepatocytes may act on liver resident progenitor cells and alter cell fate decision and exogenous administration of HMGB1 is an attractive option for promoting endogenous regeneration during chronic liver disease. We showed that administration of fully-reduced HMGB1 promotes regeneration of multiple tissues, including bone, skeletal muscle and blood (Lee et al., 2018), and other have shown that it promotes hepatocyte proliferation following liver injury (Tirone et al., 2018).\nThis project will focus on mapping the landscape of liver regeneration and define how HMBG1 regulates progenitor cells during chronic hepatic injury. It will be powered by the integration of advanced next generation sequencing techniques, such as single cell RNA-seq and ChIP-seq, with established murine models of liver disease, fate mapping studies and liver organoids. Our expertise in computational biology (Croft et al., 2019; Layton et al., 2020) will support the construction of a single cell atlas of liver repair and prioritise potential disease areas and biomarkers.\nChronic liver disease, liver regeneration, progenitor cells, organoids, single cell RNA-sequencing\nThe successful candidate will benefit from supervision by a surgeon scientist with a focus on translational medicine, a clinician scientist with expertise in computational biology and translational research, as well as two renowned authorities on liver diseases, including a senior hepatologist. In addition, you will be supported by two junior supervisors with expertise in HMGB1 biology and computational biology.\nYou will be based in the modern building and laboratories of the Kennedy Institute of Rheumatology, a world-leading centre in the fields of cytokine biology and inflammation, with a strong emphasis on clinical translation. The project will use a combination of human samples, organoids and murine models of chronic liver diseases. There is support available from post-doctoral scientists and lab managers in our groups. In summary, you will be working within:\n- Cutting-edge liver biology and next generation sequencing techniques available in-house, including tissue culture, cell sorting, organoids, chronic liver diseases models and single cell RNA-sequencing analysis\n- Emphasis on translational work: findings from human and murine samples in conjunction with next generation sequencing to promote liver regeneration can have a high impact on future therapeutic development\n- Well-established DPhil programme with defined milestones, ample training opportunities within the University and Department, and access to university/department-wide seminars by world-leading scientists\n- Highly collaborative environment with expertise ranging from molecular and cell biology to in vivo models and computational biology / genomics analysis. You will also have the opportunity to participate in several other collaboration within the University of Oxford and worldwide.\n- Croft, A.P., J. Campos, K. Jansen, J.D. Turner, J. Marshall, M. Attar, L. Savary, C. Wehmeyer, A.J. Naylor, S. Kemble, J. Begum, K. Durholz, H. Perlman, F. Barone, H.M. McGettrick, D.T. Fearon, K. Wei, S. Raychaudhuri, I. Korsunsky, M.B. Brenner, M. Coles, S.N. Sansom, A. Filer, and C.D. Buckley. 2019. Distinct fibroblast subsets drive inflammation and damage in arthritis. Nature 570:246-251. doi: 10.1038/s41586-019-1263-7.\n- Layton, T.B., L. Williams, F. McCann, M. Zhang, M. Fritzsche, H. Colin-York, M. Cabrita, M.T.H. Ng, M. Feldmann, S.N. Sansom, D. Furniss, W. Xie, and J. Nanchahal. 2020. Cellular census of human fibrosis defines functionally distinct stromal cell types and states. Nat Commun 11:2768. doi: 10.1038/s41467-020-16264-y.\n- Lee, G., A.I. Espirito Santo, S. Zwingenberger, L. Cai, T. Vogl, M. Feldmann, N.J. Horwood, J.K. Chan, and J. Nanchahal. 2018. Fully reduced HMGB1 accelerates the regeneration of multiple tissues by transitioning stem cells to GAlert. Proc Natl Acad Sci U S A 115:E4463-E4472. doi: 10.1073/pnas.1802893115.\n- Raven, A., W.Y. Lu, T.Y. Man, S. Ferreira-Gonzalez, E. O'Duibhir, B.J. Dwyer, J.P. Thomson, R.R. Meehan, R. Bogorad, V. Koteliansky, Y. Kotelevtsev, C. Ffrench-Constant, L. Boulter, and S.J. Forbes. 2017. Cholangiocytes act as facultative liver stem cells during impaired hepatocyte regeneration. Nature 547:350-354. doi: 10.1038/nature23015.\n- Tirone, M., N.L. Tran, C. Ceriotti, A. Gorzanelli, M. Canepari, R. Bottinelli, A. Raucci, S. Di Maggio, C. Santiago, M. Mellado, M. Saclier, S. Francois, G. Careccia, M. He, F. De Marchis, V. Conti, S. Ben Larbi, S. Cuvellier, M. Casalgrandi, A. Preti, B. Chazaud, Y. Al-Abed, G. Messina, G. Sitia, S. Brunelli, M.E. Bianchi, and E. Venereau. 2018. High mobility group box 1 orchestrates tissue regeneration via CXCR4. J Exp Med 215:303-318. doi: 10.1084/jem.20160217.\nTranslational medicine, liver regeneration, systems biology and genomics\nCONTACT INFORMATION OF ALL SUPERVISORS:","Bone grafting is a surgical procedure in which bone material is transplanted from one location to another within the same individual or between different individuals. It is an important technique in surgical dentistry, allowing for the restoration of lost bone tissue in the jaw, which can be caused by trauma, infection, or tooth loss. The purpose of this article is to provide an overview of bone grafting techniques, materials, and applications.\nKeywords: Allograft, autograft, bone reconstruction, bone repair, calcium sulphate, ceramic, hydroxyapatite, implant, polymer\nWhat are Ridge Defects?\nRidge defects are depressions or concavities in the alveolar ridge of the jawbone that occur due to the resorption or loss of bone tissue. This can be caused by tooth extraction, periodontal disease, or injury. Ridge defects can lead to functional and aesthetic problems, making it difficult to place dental implants or perform other dental procedures.\nReasons for Bone Grafting\nThe primary reason for bone grafting is to restore lost bone tissue due to ridge defects or other causes. Bone grafting can also be used to augment existing bone tissue, improve implant stability, promote bone healing, and enhance soft tissue regeneration.\nBiologic Mechanisms of Bone Grafting\nBone grafting relies on the body’s natural regenerative capacity to form new bone tissue. The transplanted bone material serves as a scaffold for the migration and proliferation of osteoblasts, which are responsible for bone formation. Over time, the transplanted bone material is gradually replaced by new bone tissue, resulting in a functional and aesthetic restoration of the jaw.\nClassification of Bone Grafts Based on Material Groups\nBone grafts can be classified into several material groups, including allograft-based, factor-based, cell-based, ceramic-based, and polymer-based bone graft substitutes.\nA. Allograft-Based Bone Grafts\nAllograft-based bone grafts are derived from human donors and can be further classified into fresh or fresh-frozen bone, freeze-dried bone allograft (FDBA), and demineralized freeze-dried bone allograft (DFDBA).\nFresh or Fresh-Frozen Bone\nFresh or fresh-frozen bone is obtained from cadavers within 24-48 hours of death. This type of bone graft has the advantage of providing intact bone tissue with preserved cellular components, including growth factors and osteoblasts.\nFreeze-Dried Bone Allograft (FDBA)\nFDBA is obtained by freezing and drying bone tissue to remove water and preserve the organic components. This type of bone graft is available in various sizes and shapes and can be easily stored and transported.\nDemineralized Freeze-Dried Bone Allograft (DFDBA)\nDFDBA is prepared by demineralizing freeze-dried bone allograft to remove inorganic components such as calcium and phosphorus. This type of bone graft contains growth factors such as bone morphogenetic protein (BMP) and is more osteoinductive than FDBA.\nB. Factor-Based Bone Grafts\nFactor-based bone grafts are derived from natural or recombinant growth factors that stimulate bone formation. These include natural growth factors such as platelet-rich plasma (PRP) and recombinant growth factors such as BMP.\nNatural Growth Factors\nNatural growth factors are derived from the patient’s own blood or tissues and contain a high concentration of platelets, which release growth factors such as transforming growth factor-beta (TGF-beta) and insulin-like growth factor (IGF). PRP is commonly used in dental implant surgery to enhance bone healing and regeneration.\nRecombinant Growth Factors\nRecombinant growth factors are produced using genetic engineering techniques and can be more potent and specific than natural growth factors. BMP-2 is a recombinant growth factor that has been approved by the FDA for use in certain bone grafting procedures.\nC. Cell-Based Bone Grafts\nCell-based bone grafts involve the use of stem cells or osteoblasts to promote bone regeneration. These cells can be obtained from the patient’s own bone marrow or adipose tissue, or from other sources such as umbilical cord blood or placenta.\nD. Ceramic-Based Bone Graft Substitutes\nCeramic-based bone graft substitutes are synthetic materials that mimic the chemical and physical properties of natural bone. These include calcium phosphate, calcium sulfate, and bioglass.\nCalcium phosphate is a ceramic material that is similar in composition to natural bone mineral. It can be fabricated into various shapes and sizes and is resorbed over time by the body, allowing new bone tissue to form in its place.\nCalcium sulfate is a bioresorbable material that is commonly used as a bone graft substitute due to its ability to promote bone healing and regeneration. It can be easily molded into different shapes and is resorbed by the body over time, leaving behind new bone tissue.\nBioglass is a type of ceramic material that contains silica, calcium, and phosphorus, which are similar in composition to natural bone mineral. It has the ability to bond directly with bone tissue, forming a strong interface that promotes bone healing and regeneration.\nE. Polymer-Based Bone Graft Substitutes\nPolymer-based bone graft substitutes are synthetic materials that are used as alternatives to natural bone grafts. These include polylactic acid (PLA), polyglycolic acid (PGA) and their copolymers such as polylactic-co-glycolic acid (PLGA).\nIV. Bone Graft Types and Tissue Sources\nBone grafts can also be classified based on their tissue sources and include autografts, allografts, xenografts, synthetic variants, growth factors, cell-based bone graft substitutes, and ceramic-based bone graft substitutes.\nAutografts are bone grafts that are harvested from the patient’s own body and transplanted to another location within the same individual. These can be obtained from nonessential areas such as the iliac crest or tibia.\nHarvesting Bone from Nonessential Areas\nHarvesting bone from nonessential areas of the body minimizes donor site morbidity while providing a reliable source of bone tissue for transplantation.\nAdvantages and Disadvantages of Autografts\nThe advantages of autografts include a high success rate, minimal risk of infection, and the presence of live cells and growth factors. The disadvantages include longer surgical time, increased postoperative pain, and limited availability of donor tissue.\nAllografts are bone grafts that are obtained from human donors and transplanted to another individual. These can be obtained from cadavers or living donors.\nSources of Allografts\nThe sources of allografts include bone banks, tissue banks, and living donors.\nTypes of Allografts\nThe types of allografts include fresh or fresh-frozen bone, freeze-dried bone allograft (FDBA), and demineralized freeze-dried bone allograft (DFDBA).\nXenografts are bone grafts that are obtained from animal sources such as cows or pigs. These can be used as an alternative to allografts or autografts in certain situations.\nD. Synthetic Variants\nSynthetic variants of bone graft substitutes include hydroxyapatite, calcium carbonate, and tricalcium phosphate. These materials mimic the chemical and physical properties of natural bone and can be fabricated into various shapes and sizes for transplantation.\nE. Growth Factors\nGrowth factors such as BMP-2 can be used as a bone graft substitute to promote bone healing and regeneration.\nF. Cell-Based Bone Graft Substitutes\nCell-based bone graft substitutes involve the use of stem cells or osteoblasts to promote bone regeneration. These cells can be obtained from the patient’s own body or from other sources such as umbilical cord blood or placenta.\nG. Ceramic-Based Bone Graft Substitutes\nCeramic-based bone graft substitutes such as calcium phosphate, calcium sulfate, and bioglass can be used as an alternative to natural bone grafts.\nCalcium phosphate is a ceramic material that mimics the chemical composition of natural bone mineral. It can be fabricated into various shapes and sizes and promotes new bone tissue formation.\nCalcium sulfate is a bioresorbable material that can be easily molded into various shapes and promotes bone healing and regeneration.\nBioglass is a ceramic material that can bond directly with bone tissue, promoting bone healing and regeneration.\nV. Applications of Bone Grafting\nBone grafting has several applications in dental and orthopedic surgery.\nA. Dental Implants\nBone grafting is commonly used in dental implant surgery to restore lost bone tissue and promote the stability of the implant.\nB. Restoration of Skeletal Integrity to Long Bones\nBone grafting can be used to restore skeletal integrity following fractures or other injuries to long bones such as the femur or tibia.\nC. Fusion of Joints\nBone grafting can be used to promote fusion of joints such as the spine or wrist.\nD. Repair of Broken Bones\nBone grafting can be used to repair broken bones and promote bone healing and regeneration.\nIn conclusion, bone grafting is an important surgical technique in dentistry and orthopedics that allows for the restoration of lost or damaged bone tissue.\nThe different types of bone grafts and bone graft substitutes provide surgeons with a wide range of options to choose from depending on the specific needs of the patient. Understanding the biologic mechanisms of bone grafting, as well as the advantages and disadvantages of each type of graft material, is essential in making informed decisions in surgical practice.\nFuture research in bone grafting will focus on improving the efficacy and safety of existing techniques, developing new materials and technologies for bone regeneration, and discovering new growth factors and cell-based therapies that can enhance bone formation and healing. With continued advancements in this field, bone grafting will continue to play an important role in restoring function and aesthetics to patients who have lost bone tissue due to injury or disease.\nAuthors: Dr Mriganka Sekhar Ghose & Dr Nivedita Biswas\nDr Nivedita Biswas has completed her BDS in 2019 from Hitkarini Dental College and Research Center, Jabalpur and is currently a MDS 3rd year student in the department of Oral & Maxillofacial Surgery at the Peoples College Of Dental Science and Research Center, Bhopal.\nFrequently Asked Questions:\nWhat is the ideal material for bone grafting in dental implant surgery?\nThere is no one-size-fits-all answer to this question, as the ideal material for bone grafting will depend on several factors, including the size and location of the defect, the patient’s medical history, and the surgeon’s preference and experience. Autografts are considered the gold standard due to their high success rates and low risk of infection, but allografts, xenografts, and synthetic materials can also be effective alternatives depending on the circumstances.\nAre there any risks or complications associated with bone grafting?\nAs with any surgical procedure, there are potential risks and complications associated with bone grafting, including infection, bleeding, nerve damage, and rejection of the graft material. However, these risks can be minimized by careful patient selection, proper surgical technique, and postoperative monitoring.\nHow long does it take for a bone graft to heal?\nThe healing time for a bone graft will depend on several factors, including the type and size of the defect, the type of graft material used, and the patient’s overall health. In general, it can take anywhere from several weeks to several months for the graft material to fuse with the surrounding bone tissue and form new bone.\nCan bone grafting be performed in patients with systemic diseases such as diabetes or osteoporosis?\nYes, bone grafting can be performed in patients with systemic diseases such as diabetes or osteoporosis, but careful consideration must be given to the patient’s overall health and the potential risks and benefits of the procedure. Consultation with other healthcare providers may be necessary to optimize the patient’s medical management prior to and following the bone grafting procedure.\nWhat are the latest advancements in bone grafting technology?\nThere have been several recent advancements in bone grafting technology, including the development of new materials such as synthetic bone graft substitutes and growth factors that can enhance bone formation and healing. Stem cell therapy is also an emerging field in bone grafting, with promising results in preclinical studies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1835311e-cdd4-4291-81c6-08cbea98b693>","<urn:uuid:8f24da0b-b58f-435d-aaf3-0c5a1fc12122>"],"error":null}
{"question":"How do pregnancy complications affect both immediate delivery outcomes and long-term child development?","answer":"For immediate delivery outcomes, conditions like placenta previa require cesarean delivery to prevent severe bleeding, as vaginal deliveries could disturb the placenta. Long-term impacts are significant - perinatal mood and anxiety disorders are associated with adverse outcomes like pre-term labor and low birth-weight babies. These conditions can severely compromise a mother's ability to establish critical attachment with her infant. Studies show that children who had attachment problems in infancy become more aggressive, have difficulty managing emotions, and face higher risks of developing serious psychopathology through adolescence.","context":["During pregnancy, there might be some complications or difficulties that will be experienced by the mother. To be able to detect these in time, regular appointments to your doctor should be made and religiously followed. These would enable the doctor to foresee anything that might be of concern and perform the necessary actions to correct these situations. Awareness is key for doctors and this might be the primary factor in them being able to guide you through a successful high risk pregnancy.\nWhat Exactly Is Placenta Previa?\nPlacenta Previa is a complication during pregnancy that is characterized by the cervix of the woman being blocked by the placenta. The placenta on the other hand, is a structure that enables nutrients and oxygen to be transferred from the mother to the baby. This structure is extremely important in maintaining the baby’s overall health inside the mother’s womb.\nThe position and location of the attachment of the placenta to the uterus plays an important role in determining the chances of the woman having this complication or not. In normal cases of pregnancy, the attachment area of the placenta to the uterus is seen high up and away from the cervix. This position in the uterus avoids the blockage of the cervix. In Placenta Previa, the attachment of the placenta to the uterus is seen in a low position in the uterus. This then leads to the complete or partial blockage of the cervix and therefore, the acquisition of the complication.\nWhat Are The Symptoms of Placenta Previa To Look Out For?\nAlthough symptoms are not present in all women with placenta previa, warning signs may be observed. This includes bleeding of the vagina that happens suddenly. This might not be noticed right away by the mother since the vaginal bleeding happens in a painless manner. The color of the blood is characterized as bright red in color and the manner of the bleeding can be heavy or light. Symptoms of having placenta previa can also include those that are felt during early stages of labor. The mother might experience contractions that happen regularly, as well as pains or aches that are felt on the belly or lower back.\nWhen Should You Call Your Doctor?\nSince the vaginal bleeding may happen in different intensities, you can call your doctor when this happens excessively. When vaginal bleeding occurs in medium or heavy instances when the pregnant woman is on her first trimester, the doctor should definitely be called. Also, during the second or third trimester, no bleeding should happen already. So, if any vaginal bleeding occurs, be it light or medium, the attention of the doctor should be called right away. This is to make sure that no further difficulties will be experienced throughout the pregnancy.\nHow Is Placenta Previa Treated?\nTreatment of women with placenta previa is dependent on various factors. These include whether the woman is experiencing vaginal bleeding and how much she is bleeding as well. Also, the extent of how the pregnant woman’s health is affected and of course how her baby is affected is taken into consideration too. Additionally, the due date of the pregnant mother is kept in mind and whether or not the current time is close to the said due date or not.\nIf Bleeding Is Not Present\nIn cases where vaginal bleeding does not occur, confinement in the hospital is not required. However, the pregnant woman should be very cautious in doing any sort of activity. Anything that is strenuous such as running, jogging or lifting should be avoided. Also, sexual activity should not be made during the pregnancy. And if any form of vaginal bleeding happens, you should go to the emergency room immediately and call your doctor as well.\nIf Bleeding Is Present\nIf this is the case then your doctor might require you to stay in the hospital. When bleeding is present and the pregnant woman has a due date that is close to the current date, the baby will have very high chances of getting delivered right then. A cesarean delivery is always done when doctors see that placenta previa is present during the time of delivery. This is because for a high risk pregnancy, vaginal deliveries could cause the placenta to be disturbed and sever bleeding can ensue.","What Are Perinatal Mood and Anxiety Disorders?\nPerinatal mood and anxiety disorders (PMAD) are considered the leading complication of childbirth. The period of concern includes preconception health, pregnancy and the first 18 months postpartum. Pregnancy and new motherhood place a woman at greater risk for developing a perinatal mood disorder which can include prenatal depression and anxiety, postpartum depression, postpartum anxiety which includes obsessive-compulsive disorder (OCD), panic disorder and post-traumatic stress disorder (PTSD), and postpartum psychosis (PPP). It is estimated that 20% of childbearing women suffer from perinatal depression or anxiety, with much higher prevalence rates where multiple risk factors occur. A rare 0.1-0.2% of women will develop postpartum psychosis. In Pima County, where an estimated 14,000 live births occur annually, approximately 4000 women and their families may be affected by perinatal mood disorders.\nAwareness of PMAD is increasing as a result of growing concern about the effects of PMAD on women, infants and their families. PMADs are associated with adverse obstetrical outcomes such as pre-term labor, low birth-weight babies and transmission of psychopathology to the fetus. A woman’s ability to establish critical attachment to her infant is significantly compromised when suffering from a PMAD. Studies demonstrate that children (pre-school through adolescence) who had attachment problems in infancy are more aggressive, display difficulty managing emotions, and are at risk for developing serious psychopathology. PMADs have long-term consequences on women’s lives as well as their partners with increased risk for substance abuse, chronic depression and disturbed relationships.\nWhat Does It Feel Like?\nSluggishness, fatigue, sadness, hopelessness, in a fog\nAppetite and sleep change, confusion, poor concentration, memory loss\nLack of interest, uncontrollable crying, over-concern for the baby\nGuilt, inadequacy, shame, worthlessness, fear of being alone\nFear of harming self/baby, exaggerated high and lows, intrusive thoughts\nIntense fear, anxiety, anger, sense of doom, wish to leave situation\nFast heart rate, dizziness, tingling, chest pain, shaking\nHow Can I Tell If I Am At Risk?\nThe Edinburgh Postnatal Depression Scale helps you determine if you may need help with postpartum depression/anxiety. Complete this anonymous easy 10-item assessment online to find out now. También disponible en español.\nIs There Help in Tucson?\nYes, please see our Resource page.\nWhat Are The Usual Treatments For Perinatal Mood and Anxiety Disorders?\nMany women engage in counseling, marital therapy, medication, and support groups to help resolve and heal perinatal emotional illness. These options are available locally and each women may choose what avenue is most beneficial for her.\nSupport in the form of doula care, babysitters, play groups, group exercise programs, phone support from peers who have shared this experience, faith-based support, and family and friends is a very helpful addition to any treatment.\nWhy Is This An Important Issue?\nEach year about 4,000 women in Pima County suffer with anxiety and depression around childbirth. Many of these women are not identified or treated. Suffering in silence can lead to chronic depression for the mother, an inability to feel the joy of parenting and to engage in the lives of her children, and relationship difficulties. The worst outcomes can occur when a mother cannot care for her children, might even harm or neglect them, and can bring harm to herself. You can read about Hope’s story on Hope’s Page. You can also read Naomi’s story on Naomi’s page."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4a033e41-c954-49bc-9971-b4c8a0e4901f>","<urn:uuid:1a243e94-f272-437c-8bf1-cbc55a277de1>"],"error":null}