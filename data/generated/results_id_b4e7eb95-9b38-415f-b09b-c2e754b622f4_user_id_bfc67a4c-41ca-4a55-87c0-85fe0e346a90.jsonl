{"question":"What are the core strengthening exercises recommended for back health, and how can their benefits be compromised by overtraining symptoms?","answer":"Several core strengthening exercises are recommended for back health, including Decompression Breathing, Founder to Forward Fold, Adductor-Assisted Back Extension, and Eight-Point Plank. These exercises target important muscle groups like back extensors, deep core stabilizers, abdominals, and glutes, and should be done 2-3 times per week for 20-30 minutes. However, these exercises' benefits can be undermined by overtraining symptoms such as decreased performance, loss of muscle strength, prolonged recovery from training sessions including tenderness in muscles and joints, and emotional instability like fatigue and depression. Additionally, overuse injuries can occur from doing too much too soon or not taking enough time to recover.","context":["There are several simple moves that (when practiced often) can reduce and eliminate low back pain by helping prevent weakness from inactivity and improving the body’s ability to tolerate activity as you get older.\nThe most important muscle groups to target include the back extensors and deep core stabilizers, abdominals, and glutes. It’s also important to keep the upper leg muscles strong and flexible in order to avoid strain on the supporting structures of the back  . Movements that target these muscles should focus both on stabilization through the core and building endurance . Here, we outline five of the best moves you can do to keep your back healthy now and down the road.\n5 Strength Exercises to Beat Back Pain\nYou can do these moves virtually anywhere to strengthen your back and core and support your lower back. The American College of Sports Medicine recommends doing functional training like these exercises two to three times per week for 20 to 30 minutes per session.\nTry moving through the routine below, repeating each exercise 2 to 4 times. To be sure you reap all the back-boosting benefits, pay close attention to your form during every movement.\n1. Decompression Breathing\nStand with your toes touching and your heels slightly apart. Shift weight into your heels, unlock your knees, and gently pull your heels toward each other. Stand tall, reach your arms overhead, and press your fingertips together. With your inhale, lift the ribcage away from the hips. On the exhale, tighten the core to support the “lengthened” spine. Repeat this breathing process until you feel tall and supported.\n2. Founder to Forward Fold\nSticking out your booty might feel embarrassing, but this move is called a founder because it’s setting you up for success in building integrated back and core strength. If a full founder puts too much tension on your lower back, try doing a modified founder with your hands back behind you (pictured below). If you’ve got tight hamstrings, use a prop (such as a chair) to help bring the ground just a wee bit closer to you. Remember—the goal is to reinforce good movement patterns. Use props or modify the full move if it helps you keep a neutral and stable spine.\nFrom the modified founder position, inhale and reach your arms out in front of your heart, keeping your hips back and pressing your fingertips together, with the pinkies pressing in the hardest. Slowly lift the arms all the way up, and keep the core pulled in to maintain a neutral spine. Hold 15 to 20 seconds.\nFrom full founder, float the hands down to the ground as you drive your hips back. Unlock your knees and keep the weight in your heels. When your hands are down to the ground (or on a prop, for people with tight hamstrings) pull your hips back, up, and away, reaching your hands as far forward as possible to counterbalance. Hold 20 to 30 seconds.\nTo stand up, keep your weight in your heels, slide your hands up your shins, and bring your spine into neutral. Sweep the arms back into Modified Founder position. Press the heels into the ground and bring your hips forward to stand up.\n3. Adductor-Assisted Back Extension\nThis well-known exercise isolates some of the deeper muscles of the lower back. Add in a little extra support from your inner thighs and some increased activation of the hamstrings, and you’ve got a recipe for building back muscles strong like a superhero’s.\nStart on the ground, lying on your stomach. Flex your feet and zip your legs together, keeping just a slight bend at the knees. Press your hips and knees into the ground and lift your elbows up until the hands “float” above the ground. Pull your shoulders down towards your butt while lifting your chest off the ground. Keep your neck long and hold the pose for 20 to 30 seconds.\n4. Eight-Point Plank\nLie on your stomach with your feet flexed, knees touching, and elbows a few inches in front of your shoulders. Pull your shoulders away from your ears, and gently squeeze the knees and elbows toward the centerline of body. Press knees, toes, and elbows into the mat as you lift your hips up to the height of the shoulders. Tighten your core and maintain a long, neutral spine. Pull the elbows and knees toward each other (as though you’re trying to bring the top and bottom of your mat together), and hold the plank for 20 to 30 seconds. If you begin to tremble, you’re doing this right.\nFrom a lunge position, press through your front heel and stand tall. Lift the back heel up off the ground and reach the arms out in front of your heart. Drive your butt as far back as you can, without moving your knee, until you feel a stretch in your hamstrings. Your arms will naturally reach further forward to counterbalance. Tighten your core, taking care to maintain a neutral spine, and slowly reach the arms overhead. Hold for 20 to 30 seconds, then repeat on the other side.\nResearch shows that frequently bracing the core and strengthening your back muscles can be effective in promoting long-term relief from chronic low back pain  . Meanwhile, sedentary behavior has been shown to be a health risk in and of itself itself—so next time you catch yourself sitting for a long period of time, stand up and do some Decompression Breathing, a Founder, or any of the exercises outlined above. You’ll give your body a break from sitting and you’ll be working the muscles that support your spine. With a practice and minimal time commitment, a strong and supple back can be yours for the long haul!\nBeth Alexander is a California-based personal trainer with a passion for teaching proper posture and movement patterns. Certified through both the National Academy of Sports Medicine and the American Council On Exercise, her specialty is integrated movement—teaching people to use muscles to work together for healthy functional movements. To learn more about Beth, visit her website or follow her on Facebook or Twitter.\nAll referenced postures were created by Dr. Eric Goodman, creator of Foundation Training, a series of bodyweight exercises designed to strengthens the posterior chain of muscles. Foundation Training includes several simple moves that (when practiced often) can reduce and eliminate low back pain by helping prevent weakness and inactivity and improving the body's ability to tolerate activity over time. For more information about Foundation Training, visit the website.","How to Prevent Overtraining and Overreaching\nSerious athletes have no time to break from training, right? Wrong. Conditioning for your sport is not all about work, activity and movement. You also have to rest, relax, and recover (or restore, recuperate, and regenerate). Overtraining can lead to a number of serious consequences and set your training back weeks or even months.\nAre you overtraining (pushing your body too far over the course of a training program) or overreaching (going too hard in a single workout or series of workouts)? You've gone too far if you're experiencing any of these symptoms:\n- Sympathetic overtraining syndrome, where your resting heart rate, blood pressure, and metabolic rate are abnormally elevated\n- Parasympathetic overtraining syndrome, where your resting heart rate and blood pressure decrease abnormally\n- Emotional instability like fatigue, apathy, depression or irritability\n- Decreased desire for and enjoyment of training\n- Decline in performance\n- Loss of muscle strength\n- Weight loss and loss of appetite\n- Prolonged recovery from training sessions, which can include tenderness and soreness in muscles and joints\n- Sleep disturbances\n- Gastrointestinal disturbances\nOveruse injuries often result from repeated, abnormal stress applied to a muscle, tendon, ligament or bone—by doing too much, doing too much too soon, or not taking enough time to recover and recuperate (Baechle and Earle, 2008). They can also occur as a result of training errors, faulty technique, decreased flexibility or insufficient strength. Find out if you're overtraining or underrecovering.\nHow to Prevent Overtraining\nAre you guilty of overtraining or overreaching? Start incorporating some of these ideas into your training.\n- Design a good training program that incorporates sound exercise principles, including rest days\n- Design a program that is appropriate for your level of conditioning\n- Use the principles of cross training (variety of activity)\n- Use the principles of interval training (variety of intensities)\n- Learn to control your stress in daily life so your body can recover from exercise sessions\n- Get enough sleep to allow your mind and body to recover from workouts\n- Get a massage periodically\n- Use self-massage tools after a workout or on rest days—e.g., massage stick, foam roll, or small massage balls\n- Use a steam bath, sauna, or whirlpool, as needed\n- Eat a balanced healthy diet so that you replenish fluids and nutrients needed for recovery\n- Take a vacation several times a year to allow your mind and body to recharge; an often overlooked area in training is how your daily life impacts recovery from exercise, workouts, and sports competition\nYou can also prevent overtraining and overreaching by tweaking aspects of your personal life, such as:\n- Cutting back on smart phone time, including texting, web surfing, checking emails, and talking, to give your brain a break\n- Driving slower in your community so that your body is not on overdrive (not to mention that it's safer for you and other people on the road)\n- Keeping your personal finances simple and in order\n- Enjoying time with family and friends\n- Avoiding being a constant weight watcher. Don't micromanage every second of your life\n- Learn to complain less throughout your day\nAmerican College of Sports Medicine (ACSM). ACSM's Resource Manual for Guidelines for Exercise Testing and Prescription, 6th ed. Philadelphia; Wolters Kluwer Lippincott Williams & Wilkins, 2010.\nBaechle TR, Earle RW, eds. Essentials of Strength Training and Conditioning, 3rd ed. Champaign, Ill.: Human Kinetics, 2008.\nKreider RB, Fry AC, O'Toole ML, eds. Overtraining in Sport. Champaign, Ill.: Human Kinetics, 1998.\nRichardson SO, Andersen MB, Morris T. Overtraining Athletes: Personal Journeys in Sport. Champaign, Ill.: Human Kinetics, 2008.\nVerkhoshansky Y, Siff M. Supertraining, 6th ed. Rome, Italy: Verkhoshansky, 2009. www.verkhoshansky.com and www.melsiff.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d410d83e-bc8e-4f0f-a7c1-6c8e3655b68c>","<urn:uuid:eece625b-c80a-4ab3-9588-c1b3ad4c937a>"],"error":null}
{"question":"Compare the maximum sizes of bull sharks and bala sharks in captivity","answer":"In captivity, bull sharks maintain their large size and can reach over 7 feet in length, even in freshwater environments. In contrast, bala sharks, despite their shark-like appearance, are much smaller, reaching only 10-12 inches (25.4-30.48 cm) in captivity, though they can grow slightly larger (13-14 inches) in their natural habitat.","context":["While sharks are typically associated with saltwater environments, there are several species capable of surviving and thriving in freshwater habitats.\nOf these species, the infamous bull shark is most notorious for its ability to swim far up rivers and live in completely freshwater lakes and reservoirs.\nIn this in-depth article, we’ll examine how certain sharks have adapted to inhabit freshwater systems, which species are most likely to be encountered, where they can be found, their feeding habits, how to catch them, and why they venture so far upriver.\nWe’ll also dispel some myths and address important questions around freshwater sharks.\nHow Sharks Physiologically Adapt to Freshwater\nSharks are specifically adapted to survive in seawater, so how can some species thrive in freshwater rivers and lakes? The secret lies in their ability to regulate salt levels within their bodies.\n- Bull sharks, in particular, have specialized rectal glands, kidneys, and highly concentrated urea in their blood to maintain proper internal salt levels in freshwater.\n- Their kidneys limit the amount of salt excreted from the body when in freshwater environments.\n- The rectal gland can essentially be “turned off” to conserve salts in low-salinity water.\n- Special proteins called spermines reduce urea and ammonia in the tissues, protecting from osmotic shock.\nSo while saltwater is still optimal for them, sharks like bulls can adapt to very low salt levels through physiological adjustments and behave normally in freshwater systems.\nShark Species Most Commonly Found in Freshwater\nOnly certain shark species have developed adaptations to survive in freshwater for temporary periods:\n- Bull Shark – The most common freshwater shark. Can live permanently in freshwater lakes and rivers.\n- Great White Shark – Occasionally found in freshwater temporarily when traveling between saltwater habitats.\n- Tiger Shark – Juveniles are sometimes encountered in fresh and brackish coastal waters.\n- Sandbar Shark – Young sandbar sharks enter mangrove estuaries and river mouths.\n- Speartooth Shark – A rare species that spends its entire life cycle in freshwater rivers and lakes.\nWhile additional species may venture into estuaries, these sharks are most likely to travel deep into freshwater zones. The bull shark is by far the most frequent and successful long-term inhabitant of river and lake systems.\nWhy Sharks Swim So Far Up Rivers and Into Freshwater\nSharks have several motivations for entering freshwater zones:\n- Refuge for juveniles – Young sharks seek the safety of freshwater rivers where large marine predators cannot follow.\n- Birthing grounds – Coastal rivers serve as protected nurseries for giving birth.\n- Abundant prey – Freshwater habitats teem with vulnerable prey like fish, turtles, birds, and land animals.\n- Spawning migration – Some sharks use rivers as migration routes to and from inland spawning sites.\n- Wet season flooding – Tropical sharks naturally swim inland when monsoons flood coastal rivers.\nBy moving into freshwater ecosystems, sharks reduce competition for resources and threats from oceanic predators. The twisting bends and concealment of rivers likely feel reminiscent of natural reef habitats as well.\nHow Far up Rivers Can Sharks Swim?\nBull sharks, in particular, are capable of extraordinary freshwater migrations:\n- They have been recorded nearly 2,500 miles up the Amazon River in Peru, and as far as 1,000 miles up the Mississippi River.\n- In Lake Nicaragua, bull sharks travel over 90 miles upriver from the Caribbean Sea via the San Juan River.\n- Tagged sharks tallied 600 miles upriver movements in South Africa’s Breede River.\n- Many Florida rivers like the Caloosahatchee and St. Lucie Estuary have bull sharks 15+ miles from the coast.\nClearly, the Bull shark is perfectly adapted for lengthy journeys into purely freshwater zones far from the ocean. Their ability to regulate salt balance gives them this tremendous advantage over other predator species.\nWhere Are Freshwater Sharks Most Often Encountered?\nWhile a few major shark species have been recorded across continents, freshwater sharks in North America occur most frequently in these areas:\nBull sharks in Florida rivers, the Mississippi River delta, Alabama’s Mobile Bay, and the Intracoastal Waterway. Also Texas waterways like the Rio Grande.\nBull sharks and juveniles in the Colorado River delta and Salton Sea region; great whites occasionally in the San Francisco Bay.\nYoung sandbar sharks and great whites found during summer in estuaries from North Carolina to New England.\nGreat Lakes Region:\nRare stray bull shark and great white sightings within the Great Lakes interconnected system.\nIsolated populations of bull sharks and great whites have been observed in Nevada’s Lake Mead reservoir and Lake Nicaragua in Central America.\nFreshwater sharks are most reliably encountered along Southeastern coastal river drainage’s, where warm climates promote seasonal migrations.\nWhat Do Freshwater Sharks Eat?\nSharks swimming into rivers take advantage of abundant new prey:\n- Young bull sharks feed on bass, catfish, carp, gar, bream, and freshwater stingrays.\n- They also eat small mammals like nutria, rats, birds, turtles, and amphibians when available.\n- Larger bull sharks continue hunting familiar prey like tarpon, mullet, jacks, and catfish that also swim upriver.\n- Occasional great whites and tigers sample fish species they don’t encounter in their usual habitats.\n- Speartooth sharks subsist primarily on native freshwater fish found year-round in the rivers they inhabit.\nThe diverse food sources represent a veritable buffet for sharks expanding their menu in freshwater.\nHow to Catch a Shark in Freshwater\nThe best method for locating and catching freshwater sharks like bulls includes:\n- Fish upper tidal river zones, inland bays, and miles up major tributaries. Seek brackish to freshwater transition zones.\n- Target bull sharks in warm coastal rivers during late spring through early fall periods.\n- Slow troll or anchor cut baitfish to the bottom of channels, holes, and bends with strong current.\n- Use heavy tackle, leader, and large circle hooks to prevent bite-offs. Land sharks quickly once hooked.\n- Live bait like mullet, shad, bluegill, or catfish also work well.\n- Chum with crushed baitfish to draw sharks to your location. Consider using a wire stringer.\n- Focus efforts at dawn, dusk, and night when bull sharks actively feed.\nA unique challenge requires appropriate gear and strategy. But landing a freshwater shark is the catch of a lifetime!\nMyths and Facts About Freshwater Sharks\nFiction and misunderstandings surround freshwater sharks:\nMyth: Sharks like only saltwater.\nFact: Bull sharks thrive in freshwater and certain species tolerate it just fine. Their physiology adjusts.\nMyth: Lakes and rivers are safe from sharks.\nFact: Sharks, especially juveniles, can be found far inland, not just oceans.\nMyth: All sharks avoid freshwater.\nFact: Great whites, tigers, sandbars, and speartooth also frequent freshwater.\nMyth: Sharks enter freshwater by mistake.\nFact: Sharks intentionally swim up rivers and lakes for specific biological reasons.\nMyth: Sharks only stay briefly in freshwater.\nFact: Bull sharks can complete their entire lifecycle in rivers and lakes.\nMyth: Sharks hate freshwater.\nFact: For some sharks, freshwater migration serves an essential part of their life history.\nThe fact is, freshwater systems make perfectly valid habitat for several shark species that have adapted to take advantage of the abundant resources. Far from a mistake, migrating into lakes and rivers helps sharks survive and thrive.\nFrequently Asked Questions About Freshwater Sharks\nCan bull sharks live permanently in freshwater?\nYes, bull sharks are able to spend their entire lifecycle in freshwater rivers, lakes, and reservoirs. They do not require any saltwater once adapted to freshwater environments.\nWhy do freshwater sharks swim so far inland?\nBull sharks migrate into freshwater systems to find refuge as juveniles, give birth, follow seasonal floods, and locate abundant food. The resources allow them to survive and grow better than in the crowded ocean.\nWhat species of shark has been found farthest upriver?\nBull sharks have made the most remarkable freshwater migrations, swimming nearly 2,500 miles up the Amazon and over 1,000 miles up the Mississippi River from the ocean. Their specialized kidneys and glands allow this.\nDo freshwater sharks only stay in rivers temporarily?\nWhile great whites and tigers temporarily venture into estuaries and rivers, bull sharks are able to complete their entire lifecycle permanently in freshwater lakes and rivers if conditions are favorable.\nHow can I catch a bull shark in a lake?\nFish brackish tidal sections of rivers and estuaries during warmer months. Slow troll baitfish or cut bait near the bottom of channels and holes. Dawn, dusk, and nighttime are best. Use strong tackle to land sharks quickly.\nAre there shark species that only live in freshwater?\nYes, the extremely rare speartooth shark inhabits only freshwater rivers and lakes, mainly in New Guinea and Australia. It represents the most specialized freshwater shark found exclusively in inland systems.\nWhat kinds of fish do freshwater sharks eat?\nBull sharks feed on all varieties of local freshwater fish species including bass, catfish, gar, bream, carp, and stingrays. Plus they still pursue any saltwater fish like tarpon that migrate between fresh and marine waters.\nCan I swim safely where freshwater sharks live?\nExercise caution swimming in any body of water known to have sharks. Most freshwater shark bites result from provoking or accidentally snagging the animal. Unprovoked attacks are very rare, but still avoid swimming alone at night when sharks actively feed.\nAre freshwater sharks smaller than saltwater sharks?\nJuveniles in rivers tend to be smaller, but adult bull sharks and other species reach normal maximum sizes despite living in freshwater. Large bull sharks over 7 feet long are routinely caught in some freshwater fisheries.\nThe ability of bull sharks and certain other species to swim far up rivers and traverse between salt and freshwater is a remarkable adaptation. Their specialized physiology allows sharks to regulate their internal salt levels and take on the unique challenges of freshwater habitats.\nSeeking refuge and food sources unavailable in the open ocean, sharks have proven to be the ultimate habitat generalists, venturing into the deepest lakes and farthest rivers if conditions allow. While startling to some, freshwater sharks inhabit more areas than most people realize and play important ecological roles in these unique ecosystems. So next time you’re swimming in a lake or river, be glad that the vast majority of shark species stick to the seas. But for a few well-adapted sharks like the bull, the entire continental watershed is their domain.\n- Freshwater Sharks: Clarifying the Terrifying Fact From Fiction - September 19, 2023\n- Top 5 Killer Whale Documentaries You Need to Watch - September 19, 2023\n- Great White Sharks: 22 Wild Facts You Didn’t Know - September 19, 2023","Bala Shark – The Complete Care Guide of This Sparkling Beauty\nWhat about keeping a shark in your freshwater aquarium? Before you get a spine-chilling feel, you should know that it is a shark (precisely a mini-shark) by name only. Bala Shark or Silver Shark is not at all related to those ocean-dwelling Sharks you see in movies. Rather, it got the name just because of the similar structure and appearance. Impressive indeed!\nBut what about caring for the Shark? If you consider that it is going to be extremely difficult, it’s high time to come out of misconception. It is a large yet very friendly and peaceful fish that can transform your boring freshwater aquarium into a pleasing centerpiece.\nKey Specifications of Bala Shark\n|Scientific Name||Balantiocheilos melanopterus|\n|Origin||South-east Asian Countries|\n|Size||Maximum 12” (30.48 cm)|\n|Color||Black and grey with yellow gradients|\n|Temperament||Peaceful but semi-aggressive to small fish|\n|Compatibility||Medium (with peaceful and small fish)|\nThis fish is also known as Hangus, Tricolor Shark, Silver Bala, Silver Shark, Bala Shark Minnow, Malaysian Shark, and Tri-Color Minnows. Its colors give a metallic look that attracts the fish keepers at once. In 1850, Pieter Bleeker first found this species. At present, the number of this one is decreasing fast due to the massive change in their habitat.\nOrigin and Habitat of Bala Shark\nThis fish is originally from South-East Asia; to be more specific, it is available in Thailand, Indonesia, Malaysia, Cambodia, Kalimantan, Borneo, and Sumatra in large numbers. In Chao Phraya basin and Mekong Basin in Thailand, it used to be available in huge quantities. They are found in the lakes and medium to large rivers of these areas.\nRight now, in most of its natural habitats, it has become extinct. However, trade for the aquarium is not mentioned as a reason behind this, but pollution and deforestation are two definite reasons behind this. This fish is an inhabitant of clean water, and it is simply unable to cope up with an increasing level of water and air pollution. Fish farms are now trying to breed this fish in their natural habitat to meet the demand of aquarists.\nAppearance of Bala Shark\nIt looks like the sharks because of the torpedo-shaped body and slanting, triangular-shaped dorsal fin. It is a beautiful looking fish whose scales have a different sickle-like shape. The fluke of this fish is seen to be bifurcated. Overall, it has a slender structure. Females are larger than males, and they don’t have the slender structure like males, rather they are round.\nTheir eyes are larger in comparison to their small heads. These big eyes help them to see the prey clearly and have a better focus at the time of hunting. It has a flattened body from both sides and its body is covered with very large silver scales.\nSize of Bala Shark\nAt the juvenile stage, this fish is 2” (5.08 cm), but adult ones are 10-12” (25.4-30.48 cm). Precisely, in their natural habitat, they are generally 13-14” (33.02-35.56 cm). But in captivity, they can be found 10-12” (25.4-30.48 cm).\nColor of Bala Shark\nDifferent shades of black, grey, and golden can be seen on their bodies. All their fins (dorsal, pelvic caudal, and anal) have different shades of golden yellow, followed by a black edge. Ventral fins are sometimes found to be mono-colored as well. Among the adults, this golden-yellow stretch is seen to turn into white.\nSilver-steel scales of this fish have truly made it a sparkling fish. Towards the fins, the scales are light golden. The density of the scales of this fish makes its appearance even shinier. In some parts of its body, silver with tinges of teal can also be noticed among the juvenile ones. It looks splendid in ample lighting conditions.\nBehavior of Bala Shark\nIt is generally a peaceful fish, and they stay in the mid-level of the aquarium. Being the fish of mid-level, they do not tend to dig a hole in the substrate like the bottom dwellers. On the other side, they prefer to swim, and they need more and more space to swim. They only go towards the bottom to fetch the food in case the food particles sink before eating and at that time, they make sounds.\nThey are definitely unlike the ocean Sharks, and they do not mess with the aquarium decors or plants.\nThis fish is usually active, but sometimes they like to take rest in the caves and logs, and during their resting time, if you tap on the aquarium from outside, they get startled very easily. Also, they take some time to cope up with the environment of the aquarium.\nLifespan of Bala Shark\nIn a properly maintained aquarium, Bala Sharks can live up to ten years. However, their lifespan in the wild is longer than this.\nDiet of Bala Shark\nThis fish is omnivorous, and an aquarist will get several food options for them. It is seen to be stressed due to the same type of foods; therefore, variations are needed.\nWhen it comes to diet, it is a very greedy kind of fish, and right after eating, it starts looking for food at the bottom of the tank. In spite of being greedy, they will refuse the food because of the lack of variations and live on algae.\nAs they are core omnivores, they can eat the plant substance as well as the meat in their natural habitat. When it is in the aquarium, you have to combine the plant materials with the non-veg items in a balanced proportion so that the fish can get all the nutrients from the food. A list of food options is given below to assist you:\n- Dry Seaweed\n- Shelled peas\n- Small pieces of fruits\n- Freeze-dried vegetables\n- Tropical flakes and Algae Wafers\n- Brine shrimps\n- Insect Larvae\n- Small insects\n- Chopped Earthworms\nEnsure that your pet fish is consuming the food in five minutes. Determine the quantity of food according to that. It is also recommended to feed the fish twice or thrice. Make sure you do not overfeed the fish. When you are feeding the fish twice or thrice, make sure your fish is consuming that food within a couple of minutes.\nKeep it in mind that this fish grows quite large. Therefore, the protein is a must in their diet. If you are feeding it twice or thrice a day, one meal among them has to be composed of fleshy food items. In case, you feel that you have overfed the fish, you are free to feed it in less quantity in its next feeding.\nAlong with the store-bought foods, live and frozen foods are required to keep them active and playful. The quantity of herbivorous flakes and pellets should be lesser than non-veg foods as they will eat algae often.\nSkip Bloodworms which are preferred by most of the freshwater species, as Bala Shark may have some digestion problems for eating this.\nTank Requirements of Bala Shark\nIndeed, food is the secret of a healthy fish, but Bala Shark has typical tank requirements. It will fall sick, and they will be stressed due to improper tank conditions. You need to be very careful while making these adjustments. Know about the ideal tank for this fish:\nIn the pet shops, you will get the juvenile ones, which are just ¼ in size, which can mislead you that your small tank will do well for them. But an adult one is very large which means that a spacious tank will be required. In addition to that, this fish is among the shoaling fish which prefer the individuals from own community. It is generally advised to keep four Bala Sharks in a tank for which you will need a minimum 150-gallon tank.\nYou can keep 2-3 pairs of juvenile fish in a 70-80-gallon tank, but as soon as it becomes mature, you have no other options except upgrading into a bigger tank. In case you want to keep a pair or a single one in a community tank, the size should be minimum 150-gallon because it has to be kept with other large fish.\nThey are very fast swimmers, and as soon they will see that there is a shortage of space in the aquarium, they will be stressed. For a single adult Bala Shark, a minimum 45-gallon tank will be required.\nThis fish is among the jumpers, so you cannot keep the tank uncovered. It stays peaceful most of the time, but it gets startled easily and jumps right then. A tight lid is required to keep the fish secured because they can jump out of the aquarium and die.\nAs this fish is too shiny, it will be better to get a substrate of dark pebbles. It is not a bottom dweller or hole-digger, so, it is unnecessary to use sand. Also, it enjoys swimming most of the time, so hiding places like caves will not be required much. 1 cm thick substrate will be good for them.\nThe use of aquatic plants is completely optional, but if you want, you can use the floating plants with thick leaves and strong roots. There is a possibility that the fish will unintentionally damage the plants at the time of swimming. Water Wisteria can be a good choice here. Duckweed, Anubias, Hornwort, Dwarf Water Lettuce and Cabomba can be used in this matter.\nA filter is mandatory as this fish is intolerant with polluted water. In its natural habitat, they are used to the fast-flowing water, which you can replicate with the filter. A tank with Bala Shark should be clean and oxygenated. With the help of a canister filter, you can keep the tank clean and produce low to medium current. Alternatively, you can use the powerheads for current.\nIt prefers natural lighting, and if that is not possible, you can use any mild lighting. Simple aquarium lamps for eight hours will be sufficient for them.\nDriftwood will be a good décor here, along with small rocks. Actually, this fish is more concerned about the space of swimming it gets in the aquarium, and it is least bothered about the decors. If they get space to swim, it will not displace the decors, or they will not hide. Initially, they are stressed, and at this time they tend to hide in the logs. So, small logs and caves will be alright for the first few months. Avoid any décor with sharp edges or light color as it will make the fish pale.\nWater Type for Bala Shark\nFor Bala Shark, water parameters need to be strictly maintained, and it sometimes becomes difficult for the aquarists. Know these in detail:\nThe temperature has to be between 72-82°F (22.2-27.8°C), but 77°F (24°C) is considered as the ideal water temperature for them.\nThe range of pH levels of such tanks will be between 6.5 and 7.8. Higher or lower from this range will make the fish unhealthy, and they will become prone to various diseases.\nMaintain the water hardness within 10-13 dGH, even though Bala Sharks are not very sensitive to water hardness.\nJust like any other aquarium fish, nitrate is extremely harmful to Bala Sharks. Nitrate level should be the least possible, and to do so, food wastes, and fish excretes need to be removed regularly.\nUse a vacuum siphon to clean the tank. It is necessary because Bala Sharks feed on the sinking foods, which may be stuck to the aquarium base. Manual cleaning can also be done to make sure you do not use any chemicals or soap to clean the dirt from the substrates.\nThe natural habitat of Bala Sharks is the origin of the rivers and the bends where water is clearer than the rest of the river basin. Therefore, cleaning the aquarium regularly is mandatory. It will be good to change one-third of water each week.\nCompatibility of Bala Shark\nThis fish is likely to stay within its own community due to its shoaling tendency. You can add 5-6 Bala Sharks in your tank, but make sure your tank is not overcrowded. In a small tank, you can keep a pair as well, but keeping it alone is not a good idea. You can notice their initial stress and aggression are getting reduced if you keep a small group of four. Also, they will soon get used to the aquarium and engage in playful activities if they are kept in the group.\nSuitable Tank Mates of Bala Shark\nIt is a peaceful fish, and you will see them staying in harmony with various large fish. Also, their aggression will be suppressed with these fish. It is recommended to keep the same sized fish and slightly larger ones with this fish. You can keep them with peaceful to semi-aggressive fish too. Following are the suitable tank mates for this fish:\n- Any large Gourami\nBefore adding Bala Sharks to a community tank, put four or more Bala Sharks together because they are stressed for many days initially. Keeping them together will help them to settle in smoothly. They get easily scared, and adding them in the community tank right at the beginning will startle them, which will result in their attempts to jump out of the aquarium.\nUnsuitable Tank Mates of Bala Shark\nIn case you want to raise this fish in a community tank, beware of keeping them with small fish, which is generally a part of their diet in the natural habitat. Extremely large and carnivorous fish are also restricted as they will eat the Bala Sharks. The unsuitable tankmates of Bala Sharks are as follows:\nBreeding of Bala Shark\nCaptive breeding of this fish is not successful because of their large size. Also, their ideal breeding requirements are still unknown. They are egg-scatterers, which makes home aquaria breeding impossible. However, breeding in fish farms in Thailand and some other Asian countries have some successful reports. For this reason, commercial breeders often use hormones to spawn in order to meet the demand of the aquarists.\nCommon Diseases of Bala Shark\nIt is an extremely sensitive fish that generally does not catch diseases if aquarists follow strict guidelines and parameters for this fish. Due to negligence in water change or faulty filters or temperature or feeding issues, this fish can have Dropsy and Ich.\nIt is a bacterial infection which makes the fish fat. In most of the cases, excessive fluid is seen in the fish. Some antibiotics can control this disease. Since the size of this fish is quite big, get experts’ advice to know about the dosage.\nThis disease affects the scales most, and the fish rubs its body against the gravels to heal itself. Along with that, small white spots can be seen in its body. It is a parasite that goes into the body of fish from foods. You can adjust temperature and apply aquarium salt to heal the fish. If these two do not work, you can apply the copper or potassium-based solutions in the water to control the growth of the parasite.\nBala Shark is for experienced fish keepers, and you cannot neglect its requirements. It is an extremely sensitive fish that cannot tolerate a slight mismatch in the tank and water requirements. You have to keep this fixed in your mind before petting this fish.\nWell, difficulties are a part of fish keeping. But are you willing not to pet such a magnificent fish just because of these difficulties? Of course not! You have to put some efforts for this fish, and in return, it will fill you with the joy of keeping a shiny look-alike of shark in the aquarium. Now, that sounds really cool, right?\nInteresting facts about Bala Shark\n- In the natural habitats, Bala Sharks do not look after their young ones. The fries are self-dependent from the beginning, and they find a group and grow up with them.\n- The main reason behind the extinction of this fish is the adults eating almost all their eggs. So, it becomes difficult to collect their eggs from the wild. Presently, fish farms are playing a major role in breeding them.\nSome Other Mini-Sharks for Freshwater Aquarium\nIf you find Bala-Shark interesting, the following species will definitely interest you:\n- Iridescent Shark – Pet a shark that is always active and never sleeps! Also, it is a peaceful Shark that needs the least aquarium adjustments.\n- Red Tail Shark – Get the feeling of ocean-dwelling Sharks with this semi-aggressive species. Know how to handle them.\n- Rainbow Shark – Know about the multi-colored mini-Shark, which becomes aggressive at different times to attack their enemies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:2909dc52-ae33-48ab-91ac-2f0e570d37e8>","<urn:uuid:4124f429-e19d-4734-bbe4-cd1e91d6d563>"],"error":null}
{"question":"What is the physical process of offering a Korban sacrifice, and what spiritual significance does it have in terms of the soul's elevation?","answer":"The physical process involves several steps: the animal is killed, blood is sprinkled around the altar, and specific parts like fat, kidneys, and liver are burned upon the altar. The meat and skin are burned outside the camp, and the innards and legs are washed and burned on the altar. The spiritual significance lies in taking a physical, carnally oriented being (the animal) and elevating it to the spiritual realm. This transformation is symbolized by turning the animal into an aroma, as smell is considered a soulful experience that doesn't physically fortify the body but is reserved for the soul. The word Korban itself means 'to come close,' indicating that sacrifices are a method to draw closer to Hashem by transforming something purely materialistic into a means of transcendence.","context":["Leviticus 9:1 ←\nOn the eighth day, Moses called Aaron and his sons, and the elders of Israel;\nLeviticus 9:2 ←\nand he said to Aaron, “Take a calf from the herd for a sin offering, and a ram for a burnt offering, without defect, and offer them before Yahweh.\nLeviticus 9:3 ←\nYou shall speak to the children of Israel, saying, ‘Take a male goat for a sin offering; and a calf and a lamb, both a year old, without defect, for a burnt offering;\nLeviticus 9:4 ←\nand a bull and a ram for peace offerings, to sacrifice before Yahweh; and a meal offering mixed with oil: for today Yahweh appears to you.’ ”\nLeviticus 9:5 ←\nThey brought what Moses commanded before the Tent of Meeting. All the congregation came near and stood before Yahweh.\nLeviticus 9:6 ←\nMoses said, “This is the thing which Yahweh commanded that you should do; and Yahweh’s glory shall appear to you.”\nLeviticus 9:7 ←\nMoses said to Aaron, “Draw near to the altar, and offer your sin offering, and your burnt offering, and make atonement for yourself, and for the people; and offer the offering of the people, and make atonement for them, as Yahweh commanded.”\nLeviticus 9:8 ←\nSo Aaron came near to the altar, and killed the calf of the sin offering, which was for himself.\nLeviticus 9:9 ←\nThe sons of Aaron presented the blood to him; and he dipped his finger in the blood, and put it on the horns of the altar, and poured out the blood at the base of the altar;\nLeviticus 9:10 ←\nbut the fat, and the kidneys, and the cover from the liver of the sin offering, he burned upon the altar, as Yahweh commanded Moses.\nLeviticus 9:11 ←\nThe meat and the skin he burned with fire outside the camp.\nLeviticus 9:12 ←\nHe killed the burnt offering; and Aaron’s sons delivered the blood to him, and he sprinkled it around on the altar.\nLeviticus 9:13 ←\nThey delivered the burnt offering to him, piece by piece, and the head. He burned them upon the altar.\nLeviticus 9:14 ←\nHe washed the innards and the legs, and burned them on the burnt offering on the altar.\nLeviticus 9:15 ←\nHe presented the people’s offering, and took the goat of the sin offering which was for the people, and killed it, and offered it for sin, like the first.\nLeviticus 9:16 ←\nHe presented the burnt offering, and offered it according to the ordinance.\nLeviticus 9:17 ←\nHe presented the meal offering, and filled his hand from there, and burned it upon the altar, in addition to the burnt offering of the morning.\nLeviticus 9:18 ←\nHe also killed the bull and the ram, the sacrifice of peace offerings, which was for the people. Aaron’s sons delivered to him the blood, which he sprinkled around on the altar;\nLeviticus 9:19 ←\nand the fat of the bull and of the ram, the fat tail, and that which covers the innards, and the kidneys, and the cover of the liver;\nLeviticus 9:20 ←\nand they put the fat upon the breasts, and he burned the fat on the altar.\nLeviticus 9:21 ←\nAaron waved the breasts and the right thigh for a wave offering before Yahweh, as Moses commanded.\nLeviticus 9:22 ←\nAaron lifted up his hands toward the people, and blessed them; and he came down from offering the sin offering, and the burnt offering, and the peace offerings.\nLeviticus 9:23 ←\nMoses and Aaron went into the Tent of Meeting, and came out, and blessed the people; and Yahweh’s glory appeared to all the people.\nLeviticus 9:24 ←\nFire came out from before Yahweh, and consumed the burnt offering and the fat upon the altar. When all the people saw it, they shouted, and fell on their faces.","In concord with all other Mitzvah procedures, there are set manual instructions to follow when bringing a Korban. Every Mitzvah has it’s protocol of how it is to be performed. Besides for acting out the specific motions, there are particular thoughts that are expected to accompany the offering. The Mishna (Zevachim 4:6) delineates a total of six necessary intentions one should bear in mind when slaughtering a sacrifice:\n- Category of sacrifice (Chatos, Shlomim etc…)\n- Donor who is offering the Korban\n- for Hashem\n- Carcass should fuel the Altar fire\n- A pleasent aroma should be created\n- Foster appeasement\nWhy does it have to be “leshem reiach” for the purpose of making a smell? Isn’t the smell subsidiary to the more integral purpose in sacrificing, i.e. ensuring the animal contributes to the Mizbach fires. One would think that the aroma that comes from burning the meat is quite incidental, nothing more than a by-product of burning flesh.\nMan is a compound between the upper and lower worlds, a combination which is part beast and part angel. This results in opposing forces, on the one hand there are animalistic tendencies which attempt to drag one ever lower. These direct Man to the sensual, by convincing and enticing the need to satisfy basic drives. Simultaneously one’s heavenly soul pushes him in the opposite direction to emulate the Creator. A battle has now been fashioned, the carnal earthling trying to avoid the pull of his Neshamoh, vesus the sublime ethereal soul attempting to rein in the beast.\nPrior to gleaning benefit from this world we bless the Creator. The enjoyment from pleasing smells and fragrances is no exception. The Talmud (Berachos 43b) sources blessing on scents from the very last verse in Tehillim (150:6) “Every soul should bless Hashem”. The ‘soul’ benefits from pleasant fragrances, for smell is a delight that does not physically fortify the body, it is truly reserved for the soul, the spirit of Man.\nKorbanos are a far cry from the pagan idea of appeasing and placating Satan. The word Korban is etymologically derived from the word Korav, which in turn means ‘to come close’. This signifies that Sacrifices are a method to draw us closer to Hashem. The primary tool to achieve this closeness is by slaughtering and cremating a domestic creature. There is nothing more physical than an animal; from morning to night it does not cease in gratifying its urges. To offer a Korban, is to take a carnally oriented being, a purely materialistic item, an animal, and use it as an aid to transcend to new lofty and noble heights. By bringing the sacrifice we chart a progressive journey, from the realm of the physical to the spiritual.\nLet us return to the original question. Why are korbonos sacrificed leshem reaich? Why is creating a smell critical in bring a sacrifice? The answer is now apparent, generating a savory scent, accentuates the whole purpose of the Korban. Smell as we mentioned earlier is a soulful experience, this syncs with the donor’s efforts in attempting to take the physical and elevate it to the spiritual, moving from the sensual to the mental. As I once heard from Howard Witkin: In essence, one who brings a Korban is taking the animal and turning it into a aroma.\nEliyohu Hanavi, chastised the people for adhering to the idolatry of Baal. Worse still they were sitting on the fence “How long will you waver between the two sides? If Hashem is G-d, then follow Him and if Baal is G-d. than follow him”. He then proceeded to challenge the prophets of Baal to prove the veracity of their religion, by bringing down fire from Heaven.\nTwo twin bulls were brought, and two lots were prepared. On one was written “for Hashem” and on the other “for Baal”. They then drew the lots.\nThe bull that was selected for Hashem obediently followed Eliyohu without any coercion. However the other bull was a different story. The Baal prophets were having trouble because their bull would not budge. All 450 Prophets of Baal combined their energies to shift the bull, but to no avail.\nEliyohu prophetically understood the bull’s reservation. He and his twin were pastured on the same fields and lived similar lives; they were for all intents and purposes identical. The bull wondered “why should my brother have the opportunity to bring the people close to Hashem, while I will be sacrificed to an idol – Baal.”\nEliyohu replied, the same Kiddush Hashem that will happen by your brother being sacrificed to Hashem, will be brought about by you being offered to Baal. It is the contrast of fire descending for Hashem’s Korban and none coming down for Baal that is conclusive. The two of you together will jointly bring the people closer to Hashem."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:92a1b026-efdf-4485-b260-8d3b3386de7d>","<urn:uuid:14d3b846-ee03-45ac-a8c7-d354dd7018d4>"],"error":null}
{"question":"As someone interested in sustainable practices - how do Solstice Chocolate and Ghirardelli approach manufacturing efficiency?","answer":"Solstice Chocolate and Ghirardelli take different approaches to efficient manufacturing. Solstice operates a zero impact, zero waste facility using 100% solar power and fully recyclable, resealable packaging, focusing on sustainable small-batch production with organic ingredients. Ghirardelli, on the other hand, emphasizes lean manufacturing principles with a 'culture of zero losses,' using Total Productive Maintenance programs, 5S methodologies, and designated teams to reduce breakdown time and improve production efficiency. They focus on smooth production lines, proper tool placement, and continuous process improvements to maintain their large-scale operations.","context":["The Best Cities For Bean-to-Bar Chocolate\nLike wines grown from different kinds of grapes, chocolates made from cacao beans sourced from Ecuador, Bolivia, and other countries offer deliciously different tastes. Visit any of these craft chocolatiers to sample their melt-in-your-mouth treats.\nXocolatl Small Batch Chocolate Micro-factory, Atlanta, Georgia\nThis jewel of Atlanta's chocolate scene was born in the steamy jungles of Puerto Viejo, Costa Rica, after founders Elaine Read and Matt Weyandt moved there with their children and began meeting local cacao growers and artisanal chocolate makers. Today, the company’s micro-factory, located in the city's Krog Street Market, grinds whole beans from small farms in the Americas and East Africa and turns them into chocolate liquor. You won’t find cocoa butter used at Xocolatl; the makers craft their single-origin, dark chocolate with only cocoa and cane sugar, and add ingredients like dried apple, sea salt, and peppermint to make scrumptious flavored bars.\nBook a tour and tasting to learn about the bean-to-bar process; a wine pairing is included. And don’t miss Xocolatl's Frozen Drinking Chocolate, a “slushy” beverage made with Nicaraguan chocolate in coconut milk, topped with coconut whipped cream and cacao nibs (nibs are like the \"chocolate chips\" of chocolate). For the holidays, the makers will release a new 60 percent dark milk chocolate bar.\nDandelion Chocolate, San Francisco, California\nVisit San Francisco’s Mission District to experience Dandelion Chocolate, where beans are roasted, cracked, ground and otherwise prepared before being molded into bars by hand. Dandelion, which has won multiple awards for its chocolate, works directly with cacao producers to buy high-quality beans grown by sustainable methods. Each year, the company leads trips to visit some of its bean producers. They’ve taken groups to Hawaii; Maya Mountain Cacao, in Belize; Ecuador; and the Dominican Republic. Contact them for details, or just treat yourself to a Kokoa Kamili Bar from Tanzania, with flavors of ripe mango and caramelized red berries, or a bar made from Venezuelan beans that tastes of roasted almond, dulce de leche and chocolate fudge. Dandelion's Madagascar Chocolate Bar offers a punch of raspberry and Meyer lemonade flavors.\nChocoVivo, Los Angeles, California\nLike the ancient Mayans and Aztecs, ChocoVivo grinds its beans with a mano and metate, tools that are used like a mortar and pestle. The only ingredients in ChocoVivo's products are whole cacao nibs and spices, so its dark chocolate – the only kind of chocolate the company makes—has a pure, stone-ground, traditional taste. “Chocolate is truly food, not a candy bar,” says owner and chocolatier Patricia Tsai. Look for bars made with 100 percent, 85 percent, 75 percent or 65 percent cacao, or try flavored chocolates like Cherries + Almonds + Black Peppercorns (crumble the bars to make a yummy trail mix).\nYou can also snack on Shangri-La Bars or melt them for a drinking chocolate. They’re made with beans from Tabasco, Mexico; toasted black sesame; unrefined cane sugar; and bits of chewy, organic Goji berries. They make for great pairings with white or light red wines. ChocoVivo hosts a variety of events so you can meet their growers, sample café beverages, and more.\nSolstice Chocolate, Salt Lake City, Utah and other locations\nRich, delicious Solstice small-batch chocolate is made from exotic, slow-roasted cacao beans. All the ingredients are 100 percent organic—and even its zero impact, zero waste facility embraces “green” initiatives, using 100 percent solar power and fully recyclable, resealable packaging. Try the 70 percent Ugandan Bundibugyo Dark Chocolate; its mildly bitter taste comes from cocoa powder made from fine Bundibugyo cacao beans, combined with bits of berries and tree fruit. Palos Blancos Bars are produced from beans harvested in a rainy area in Bolivia; they have a deep, chocolatey taste with hints of fresh cream and nuts. Check the website for a list and map of store locations.\nMadre Chocolate, Honolulu and Kailua, Hawaii and other locations\nTalk about paradise: Madre’s organic, fair-trade chocolates and the tropical beauty of these Hawaiian destinations make an unbeatable combination. The makers lightly process their beans to retain healthy antioxidants, and flavor some bars with fruits and spices traditionally used by the Aztec, Maya and Olmec tribes that invented chocolate.\nMadre has won 23 Hawaiian, U.S. and international awards for its chocolates, which include such treats as the Lili’koi Passionfruit Hawaii Chocolate Bar, Chipotle Allspice Bar, and Hibiscus Bar. Sign up for events or classes to make your own chocolate, or to enjoy a whiskey and chocolate pairing. Madre also offers farm to factory tours, wine and chocolate pairings, and an experience with craft coffee and chocolate. Madre products are available in Hawaii and other locations around the world.\nAmano Chocolate, Orem, Utah\nAward-winning Amano chocolatiers say they’re passionate about their craft; in fact, “amano” is Italian for both “by hand” and “they love.” They've won international awards for products like their Raspberry Rose Bar, made with ground rose petals and raspberries mixed into smooth chocolate, and the Mango Chili Bar, which combines the creamy, fruity flavor of mangos with the warmth of chiles. One of the company’s original bars, the Madagascar 70 percent Dark Chocolate Bar, is packed with fruity citrus and berry tastes—and it earned a gold medal from London’s respected Academy of Chocolate. If you’re into making your own chocolate—and you have a generous budget—opt for ChocoVision's $2,250 Revolation Delta, sold by Amano, to melt and temper up to ten pounds of chocolate in an hour. (\"Temper\" refers to making smooth, glossy chocolate.) The Revolation Delta is suitable for use in your home or a lab setting.","Smooth Production a World of Pure Reality at Ghirardelli\nChocolate manufacturer’s secret recipe: staying lean\nBy Christine Torres\nGhirardelli Chocolate Company prides itself as a smooth operator. That’s heavily thanks to the company’s culture of zero losses and a steadfast commitment to lean, according to Helen Tan, lean manufacturing engineer at the Bay Area chocolatier’s manufacturing facility. Even with intense pressures to produce enough delicious confections in time for holidays such as Valentine’s Day and Christmas, production must follow a solid plan that’s been integral to the Ghirardelli brand’s foundation.\nSince 1852, Ghirardelli has set itself apart from other chocolate manufacturers by branding its quality and centralizing its capacity. Through modern improvements to its traditional chocolate-making process, the company (part of Lindt & Sprüngli AG) thrives among the top chocolatiers in the world.\nAn official San Francisco landmark, the famous Ghirardelli illuminated light bulb sign is visible for miles. It is a welcome sight to air travelers and ships passing through the Golden Gate Strait. A popular tourist destination, Ghirardelli Square delights visitors with intimate views of the chocolate-making process, which is set inside a large Victorian-style complex blocks from San Francisco Bay. Huge sacks of raw cocoa beans come in from throughout the world, which are processed into nibs before entering a liquefied chocolate format.\n“Chocolate is very fickle,” Tan said, whose favorite is Ghirardelli’s dark chocolate Sea Salt Soiree. “We need to be careful with how we temper it to produce the best crystalline structure and reduce any defects downstream.”\nAs an industrial performance engineer, Tan leads the facility’s Total Productive Maintenance program. “We try to instill a culture of zero losses, engaging 100% of the workforce, and empowering them to own their respective factory equipment and brainstorm solutions to problems on the shop floor,” she said. “By instilling a lean culture, we hope that Ghirardelli transforms to a state of pure Zen — happy machines and happy workers.”\nPart of Ghirardelli’s lean master plan to reduce losses and breakdown time includes designated teams, one of which that focuses on improving tools for 5S. Tan says besides having smooth production lines and tools in their proper place, operators are engaged and looking out for hazards and risks. Improvements in the process and re-training are common solutions for worker safety.\n“Lean and safety are linked heavily,” she said. “Visual management of your workplace is probably the easiest, quickest win for lean manufacturing. Floor marking and signs have been especially helpful for safety. For example, having a yellow marked tape underneath a fire extinguisher means no one can block it and that it is fully accessible during an emergency. When you walk out onto the Ghirardelli factory floor, you will see many tool shadow boards with engraved tools and 5S checklists for operators to make sure all equipment and tools are in place and all equipment is marked with colored tape.”\nTan says she believes 5S is the best investment of everyone’s time, not just at Ghirardelli Chocolate Company. Her advice to other manufacturers?\n“Change takes a while,” she says. “Your job as a lean proponent is to show everyone why this change is important for them: Why should they care about lean? Why should they care about reducing losses in their plant? Because it makes their job easier — less cleaning time, less breakdowns, and less risk for their safety.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c702f64e-0613-4a2c-8ec2-2beab38b5bd9>","<urn:uuid:ba19d56d-bf06-45cd-8717-5029d4be6daa>"],"error":null}
{"question":"What data types and processing methods were used to study the 2009 Padang earthquake?","answer":"The study used both seismic and GPS data. For seismic data, 7 three-component broadband stations within 750 km of the epicenter were used, with displacement waveform data bandpass filtered between 0.01 to 0.3 Hz. For GPS data, 18 three-component permanent GPS stations within 380 km of the epicenter from the SuGAr and ENS-INSU networks were included. The analysis employed finite fault inversions and moment tensor inversions, using Green's functions calculated with a 1D frequency-wavenumber integration method, based on the Lange et al. velocity structure, PREM densities, and typical crustal attenuation values.","context":["Figure 1. Comparison of geodetic data with the one-plane joint inversion models (shown in Figure 2) and the two-plane joint inversion models. The two-plane slip distributions are plotted.\nFigure 2. Left: Slip models for the east-west nodal plane using GPS, seismic, and joint data. Right: Slip models for the north-south nodal plane. The hypocenter is marked with a star and the arrows indicate the variable inverted rake direction.\nThe Mw 7.6 Padang earthquake occurred on 30 September 2009 offshore of central Sumatra (Figure 1). Seismicity in the Sumatra region is driven by the oblique subduction of the Indian and Australian plates beneath the Burma forearc block and Sunda plate at the Sunda trench. The Sunda megathrust has been extremely active ever since the 2004 Mw 9.2 Sumatra-Andaman earthquake, with additional megathrust earthquakes in 2005, 2007, and 2010. The last remaining section of the Sunda megathrust without a modern great earthquake is the Siberut segment, which lies offshore of Padang. Thus, it is especially important to understand the fault mechanism of the 2009 Padang earthquake in order to assess how it affects the stress levels on the Siberut segment of the megathrust.\nThe Padang earthquake is an unusual event because it likely ruptured the subducting Australian plate. The USGS locates the earthquake at 81 km depth, 250 km from the Sunda trench, where the (Hayes et al., 2009) Sunda plate interface model places the interface at 67 km depth. The Padang earthquake rupture mechanism is also consistent with an intraslab event. The strike is approximately perpendicular to the trench with significant left-lateral strike-slip motion. Possible candidates for the rupture plane could be subducted paleo transform faults, fractures, or ridges. We attempt to resolve the fault plane by using regional broadband seismic data, GPS data, and aftershock locations.\n|Data and Processing||\nThere are 7 three-component broadband stations located within 750 km of the epicenter, with useable data available on IRIS, operated by the GEOFON network, Malaysian National Seismic Network, and the Singapore National Network. For our finite fault inversions, both the displacement waveform data and the Green's functions are bandpass filtered between 0.01 to 0.3 Hz. The maximum waveform amplitude for each component varies from about 0.5 to 1.5 cm at this frequency range. We also include data from 18 three-component permanent GPS stations, located within 380 km of the epicenter, from the SuGAr and ENS-INSU regional networks (Figure 1).\n|Inversion Method and Results||\nFor both our finite fault inversions and moment tensor inversions, the Green's functions are calculated using a 1D frequency-wavenumber integration method (Saikia, 1994). Our starting earth model consists of the Lange et al. (2010) velocity structure, PREM densities, and typical crustal attenuation values. To invert for finite fault slip, we use a least-squares inversion method that employs simultaneous smoothing and dampening (e.g. it Kaverina et al., 2002). The fault geometry is based on the GCMT moment tensor solution, with variable rake. We allowed a 90 km x 90 km fault plane, with the hypocenter located in the central position along strike and 4 patches downdip, to prevent slip propagating to depths shallower than the plate interface.\nThe initial seismic finite fault inversion for both nodal plane (NP) geometries does a fairly good job of fitting the waveform amplitudes and polarities. However, all of the synthetics have significant phase shifts on the order of 10 seconds that cause the variance reductions to be negative. The highest variance reductions for each NP geometry are achieved using the Lange model with a uniform 10% increase in velocities across all depths. NP1 has a variance reduction (VR) of 54% and NP2 has a VR of 51%. For both inversions, there are two high slip regions, one near the hypocenter and the other 40 km downdip (Figure 2).\nThe GPS-only inversions have shallower slip and less moment than the seismic-only inversions. The NP1 inversion is rougher than the NP2 inversion, with three separate high-slip patches. The NP1 model has a VR of 84% and the NP2 model has a VR of 81%.\nThe joint inversions both have one 40 x 30 km high-slip patch, southwest of the hypocenter, with moment similar to the GPS-only models. The joint models still fit the GPS data well, at VR = 73% for NP1 and 75% for NP2. However, they do not fit the amplitudes of the seismic data and the VR for NP1 drops to 35% and to 31% for NP2. We also tested two-fault plane models that include slip on the megathrust structure, above the hypocenter, similar to the Mw 8.0 2000 Enggano earthquake (Figure 1). These joint models fit the GPS data equally well, have larger moments equal to Mw 7.7, and better fits to the seismic data with NP1 VR = 54% and NP2 VR = 53%.\nOur joint inversions of the Padang earthquake, using both regional seismic waveform and geodetic data, have shown that the east-west NP1 has a few percentage better fit to the data than the north-south NP2. In addition, the NEIC aftershock sequence better aligns with the strike and depth range of the east-west plane (Figure 1). Allowing slip on both the intraslab fault and the megathrust improves the fit to the seismic data, however the NEIC aftershock catalog does not include events at the depth range of the plate interface. Therefore are preferred fault model is slip on the east-west nodal plane, primarily southwest of the hypocenter.\n|Geographic Location||Central Sumatra|\n|Group Members Involved||\nKelly Wiseman <Email> <Personal Web Site>\n|More Information||< ***link description*** >"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:a8c49e33-7614-4ab5-9bdd-c73f1832d75e>"],"error":null}
{"question":"How do external forces affect cell stiffness compared to cell volume changes?","answer":"External forces and cell volume changes both impact cell stiffness, but in different ways. When external forces are applied through integrins, cell stiffness increases with the magnitude of the forces, and this response depends on an intact cytoskeleton. Meanwhile, when cells experience water efflux and reduced volume (either through osmotic pressure or growth on stiff substrates), they also become stiffer. This increase in stiffness from volume reduction is accompanied by molecular crowding due to the increased concentration of intracellular material.","context":["© TALLEY LAMBERT/SCIENCE SOURCE\nIt is well known that some human diseases are related to changes in mechanical properties of tissues. In patients suffering from arteriosclerosis, the arteries lose some of their elasticity and become thicker and stiffer. In liver or lung fibrosis, excessive fibrous connective tissue has a similar hardening effect on those organs. And patients with aneurysms have balloon-like bulges in their blood vessels that, if left untreated, can expand under pressure until they burst.\nOf course, mechanical properties and forces aren’t just important in disease, but in health as well. Almost all living cells and tissues exert and experience physical forces that influence biological function. The magnitudes of those forces vary among different cell and tissue types, as do cells’ sensitivities to changes in magnitudes, frequencies, and durations of the forces. Touch, hearing, proprioception, and certain other senses are well-known examples of specialized force sensors. But force detection and sensing are not limited to these special cases; rather, they are shared by all living cells in all tissues and organs. The underlying mechanisms of force generation and detection are not well understood, however, leaving many open questions about force dynamics; the distance over which a force exerts its impact; and how cells convert mechanical signals into biochemical signals and changes in gene expression.\nApplied forces concentrate at actin stress fibers and propagate over longer distances in the cytoplasm.\nIn recent years, biologists have begun to uncover the molecular players that mediate force sensation and propagation at the cellular level, and they’re collecting clues as to how mechanical stimuli influence biological function. Such work could pave the way for a deeper understanding of how physical forces influence biological functions in embryonic development, normal physiology, and complex diseases. Translating this research into the clinic may help create new ways of treating certain diseases using mechanics- and engineering-based tools.\nGenerating and sensing physical forces\nIn the early 1980s, Donald Ingber of Harvard University and Mina Bissell of Lawrence Berkeley National Laboratory independently proposed that the extracellular matrix (ECM) that surrounds and supports cells could affect cell/tissue organization and function as well as gene expression. But at the time, experimental evidence was scarce, and the mechanism was not clear.\nOver the next several years, researchers began to report that cells experience mechanical stimuli via cell surface receptors. In 1986, MIT’s Richard Hynes and colleagues cloned one such receptor, which they called integrin, that turned out to be the primary transmembrane molecule that mediates cell adhesion to the ECM. Most researchers at the time presumed that integrins (there are now 24 known subtypes) primarily functioned in chemical signaling. But in the early 1990s, while working as a postdoc in Ingber’s lab, I provided the first experimental evidence that integrins, and associated intracellular protein complexes known as focal adhesions, mediate mechanical force transmission to the cytoskeleton.1\nUsing Arg-Gly-Asp peptide–coated magnetic beads that clustered integrins and induced the formation of focal adhesions beneath the beads on the inner surface of the cell membrane, I applied measured amounts of stress to the surfaces of living cells and found that cell stiffness increased with the magnitude of the forces. By disrupting cytoskeletal filaments such as filamentous actin (F-actin), I could abolish the transmission of force into the cell. This study changed the scientific community’s view of integrins, which are now recognized as key molecular force sensors.\nSubsequently, using a laser tweezer, Mike Sheetz’s lab at Columbia University independently confirmed that focal adhesions transmit external forces into the cell.2 In addition, two other groups—those of Yu-Li Wang, now at Carnegie Mellon University, and Benny Geiger at the Weizmann Institute of Science—found that focal adhesions also transmit forces generated inside the cell by powerful molecular motors such as myosin II, which binds to F-actin in the cytoskeleton, out into the ECM. This research showed that focal adhesion–mediated transmission of mechanical signals is bidirectional.3,4\nAnother class of mechanosensors used by the cell are stretch-activated ion channels on the plasma membrane. Over the past decade, Martin Chalfie of Columbia University and other labs have worked on several candidate channels that, in response to stretch, open to allow ions to flow into the cytoplasm. The result is mechanoelectrical transduction—analogous to a neuronal action potential—that can activate enzymes or proteins in the cytoplasm to affect intracellular activities, or even influence gene expression. The detailed mechanisms of this are still unclear, however.\nMore recently, many labs have searched for intracellular mechanosensors downstream of integrins at focal adhesions. For example, Sheetz and his colleagues have found evidence of a mechanosensing role for the focal adhesion protein talin,5 while Martin Schwartz’s group at Yale has demonstrated a similar role for the focal adhesion protein vinculin.6 But a fundamental question remains: How does a living cell integrate forces sensed by different mechanosensors and respond in a coherent manner?\nIngber first proposed the model of cellular tensegrity (tensional integrity) in the early 1980s, emphasizing the importance of tension among cytoskeletal structures in the cell’s ability to configure a holistic sense of the forces at play. If there is tension in the cytoskeletal filaments, then a local change in one part of the filament is quickly transmitted to all connected parts.\nExperimental evidence now supports this model. In the early 2000s, collaborating with the Ingber lab, my group used chemicals to either contract or relax the cytoskeleton of cultured human smooth muscle cells, which systematically varied the cells’ inherent tension, or “prestress,” without changing their shape. We found that the cell stiffness changed accordingly. In other words, cell stiffness is determined by the cytoskeleton’s tension.7 Moreover, after disrupting the cell’s microtubules with specific drugs, we found that the force transmitted out of the cell increased.8 This suggests that microtubules, which are relatively stiff components of the cytoskeleton, are essentially balancing some of the cell’s endogenous prestress, and when they are disturbed, that tension is transferred to the ECM.\nMechanotransduction at a distance\n© THOM GRAVESFor many years, the prevailing view in the field of mechanotransduction was that forces transmit only a short distance in living cells, and thus a local force can only exert significant effects at the periphery of the cell. From a materials science point of view, this limited reach would be reasonable if the material was homogeneous and isotropic—in other words, there is no difference in its stiffness or other mechanical properties when the force direction is changed. In this case, a local stress would rapidly decay as the distance increases. However, the cytoplasm of a living cell is neither homogeneous nor isotropic; it is heterogeneous and anisotropic, meaning that the material’s mechanical properties do depend on the direction of force. Importantly, there are stiff, prestressed actin bundles (also called stress fibers) in the cell. Applied forces concentrate at these actin bundles and propagate over longer distances in the cytoplasm.\nSince the early 2000s, my group has demonstrated that forces do propagate across relatively vast cellular distances—on the order of tens of micrometers—in living cells, and that this long-distance signal is dependent on the inherent tension in the cytoskeleton.9,10 Just as a violin string can only ring with the correct resonance and sound the right note if it has proper tension, when the prestressed actin bundles are disrupted, force propagation becomes short-range (acting over only a few μms). The higher the tension, the farther the force will be propagated.\nMost recently, we have found that specific signaling molecules—in particular, the tyrosine kinase Src and the GTPase Rac1—can be activated at distances of more than 60 μm away from the site of the local force application via integrins at the cell membrane.11,12 Importantly, this activation is fast, taking less than 300 ms from force application to the activation of Src and Rac1, making mechanotransduction much faster than the 10 to 20 seconds it takes a soluble growth factor–induced signal to travel over the same distance.11\nMechanotransduction in the nucleus\nIn contrast to the emerging picture of force propagation in the cytoplasm, we know very little about nuclear mechanotransduction. The nuclear envelope is physically tethered to the actin cytoskeleton via the LINC (linker of nucleoskeleton and cytoskeleton) complex. In the late 1990s, Ingber and colleagues published the first evidence that force-carrying connections reach from the plasma membrane to the nucleus, perhaps playing a role in the regulation of gene expression. Using a micropipette coated with fibronectin to attach to the cell surface, the researchers pulled on the cell and found that the nuclear envelope distorted.13 Later, my group revealed a force-induced protein-protein dissociation inside the nucleus.14 This change was dependent on both a properly stressed cytoskeleton and an intact nuclear lamina, a layer of intermediate filament proteins called lamins that line the inside of the bilamellar nuclear envelope. Subsequent research has demonstrated that lamins are mechanosensors critical for extracellular matrix stiffness–directed differentiation,15 and for regulation of transcription factors.16\nMechanomedicine is poised to emerge as an exciting branch of medicine that uses mechanics- and engineering-based principles and technologies for precision diagnostics and effective therapeutics.\nTo more directly investigate whether a physiologically relevant force can directly deform chromatin structure in a living cell to regulate specific gene expression, my group recently teamed up with Andy Belmont’s lab at the University of Illinois at Urbana-Champaign. Belmont’s team used bacterial artificial chromosomes to insert multiple green fluorescent proteins and the gene for dihydrofolate reductase (DHFR), an essential enzyme for the synthesis of thymine, into the same chromatin domain in Chinese hamster ovary (CHO) cells. My lab applied a local force to those modified cells via integrins. Sure enough, we measured an uptick in DHFR transcription in response to the applied force. Conversely, disrupting cytoskeletal tension, or the force transmission pathways from the cell surface to lamins and to the nuclear structural proteins that connect to the chromatin, abolished force-induced DHFR expression.17\nThis work provides the first evidence that externally applied forces can stretch chromatin and promote gene expression. As expected with physical force–mediated processes, the response was rapid; we were able to quantify DHFR transcription upregulation within 15 seconds after force application. Interestingly, force-triggered transcription is sensitive to the angle and direction of force relative to the actin bundles: the higher the stress angle, the greater the transcription. Because endogenous forces are constantly generated inside a living cell, these findings suggest that gene expression might be incessantly regulated by physical forces via this direct structural pathway and the indirect pathways of matrix rigidity–dependent nuclear translocation of certain factors, such as yes-associated protein (YAP) and TWIST1. More research is needed to understand the relative contributions of each of these mechanisms in determining overall gene expression levels in any given cell.\nFrom mechanobiology to mechanomedicine\nSCI REP, 6:19304, 2016.Mechanobiology is becoming increasingly relevant to stem cell biology. For many years, researchers have cultured cells on top of rigid plastic or glass coverslips. However, it is well known that various types of living cells in soft tissues attach to matrices of varying stiffness. Tuning the substrate stiffness in a controlled manner, Yu-Li Wang and colleagues demonstrated that the size and dynamics of focal adhesion complexes as well as the migration of living cells are dramatically altered by substrates of different rigidity.3 Later, Adam Engler of the University of California, San Diego, and Dennis Discher of the University of Pennsylvania reported that mesenchymal stem cell differentiation can be directed by extracellular matrix stiffness.18 And my lab has demonstrated that applying local force can spur the differentiation of a single embryonic stem cell.19 Physical forces also appear critical in the patterning and organization of germ layers during early mammalian embryonic development.\nResearchers are also considering mechanical forces in cancer research. For example, despite decades of study, it is still unclear why only a few cancer cells out of thousands are able to metastasize. The answer may lie in the tumor’s physical environment. Scientists have shown that, in primary tumors, high mechanical tension and matrix stiffening are important in cancer progression, and high fluid/solid pressure in the primary tumor often accompanies tumor growth. However, secondary metastatic sites of tumors appear to be softer—suggesting that they have lower forces—than the surrounding normal tissues.\nSee “The Forces of Cancer,” The Scientist, April 2016.\nUsing a 3-D soft matrix made of fibrin gels, my group has managed to isolate and grow cells that are highly tumorigenic and malignant, called tumor-repopulating cells (TRCs), from several murine or human cancer cell lines.20 Interestingly, melanoma TRCs cultured in soft 3-D matrices are less differentiated—and thus more tumorigenic—than melanoma cells grown in stiff matrices or on rigid plastic, suggesting that low matrix stiffness drives TRC growth.21 These soft-cultured melanoma TRCs also move out of the blood vessels in zebrafish to secondary sites more efficiently than more-differentiated melanoma cells cultured on stiff substrates.22 These findings suggest a common thread in metastatic colonization of malignant tumors: a few tumorigenic cells are able to survive, metastasize, and grow at the secondary sites of soft matrices because these cells are undifferentiated.\nAnd the role of physical forces in biology is by no means limited to stem cells and cancer biology. Across the life sciences, researchers are continuing to draw on insights into mechanobiology to better understand and treat a wide variety of conditions. Human organs-on-a-chip for novel drug screening, shear force–activated cleaning of thrombosis, mechanically tuned hydrogels for bone formation, and tumor cell membrane–derived therapeutic microparticles for reversing cancer drug resistance are just a few recent examples of clinical applications of mechanobiology-based technologies. Mechanobiology-based medicine (mechanomedicine) is poised to emerge as an exciting branch of medicine that uses mechanics- and engineering-based principles and technologies for precision diagnostics and effective therapeutics of diseases that are beyond the reach of existing toolboxes.\nNing Wang is the Leonard C. and Mary Lou Hoeft Professor in the Department of Mechanical Science and Engineering at the University of Illinois at Urbana-Champaign and adjunct professor at Huazhong University of Science and Technology.\n- N. Wang et al., “Mechanotransduction across the cell surface and through the cytoskeleton,” Science, 260:1124-27, 1993.\n- D. Choquet et al., “Extracellular matrix rigidity causes strengthening of integrin-cytoskeleton linkages,” Cell, 88:39-48, 1997.\n- R.J. Pelham Jr., Y.L. Wang, “Cell locomotion and focal adhesions are regulated by substrate flexibility,” PNAS, 94:13661-65, 1997.\n- N.Q. Balaban et al., “Force and focal adhesion assembly: A close relationship studied using elastic micropatterned substrates,” Nat Cell Biol, 3:466-72, 2001.\n- A. del Rio et al., “Stretching single talin rod molecules activates vinculin binding,” Science, 323:638-41, 2009.\n- C. Grashoff et al., “Measuring mechanical tension across vinculin reveals regulation of focal adhesion dynamics,” Nature, 466:263-66, 2010.\n- N. Wang et al., “Cell prestress. I. Stiffness and prestress are closely associated in adherent contractile cells,” Am J Physiol Cell Physiol, 282:C606-C616, 2002.\n- N. Wang et al., “Mechanical behavior in living cells consistent with the tensegrity model,” PNAS, 98:7765-70, 2001.\n- S. Hu et al., “Intracellular stress tomography reveals stress focusing and structural anisotropy in the cytoskeleton of living cells,” Am J Physiol Cell Physiol, 285:C1082-C1090, 2003.\n- S. Hu et al., “Mechanical anisotropy of adherent cells probed by a three-dimensional magnetic twisting device,” Am J Physiol Cell Physiol, 287:C1184-C1191, 2004.\n- S. Na et al., “Rapid signal transduction in living cells is a unique feature of mechanotransduction,” PNAS, 105:6626-6631, 2008.\n- Y.C. Poh et al., “Rapid activation of Rac GTPase in living cells by force is independent of Src,” PLOS ONE, 4:e7886, 2009.\n- A.J. Maniotis et al., “Demonstration of mechanical connections between integrins, cytoskeletal filaments, and nucleoplasm that stabilize nuclear structure,” PNAS, 94:849-54, 1997.\n- Y.C. Poh et al., “Dynamic force-induced direct dissociation of protein complexes in a nuclear body in living cells,” Nat Commun, 3:866, 2012.\n- J. Swift et al., “Nuclear lamin-A scales with tissue stiffness and enhances matrix-directed differentiation,” Science, 341:1240104, 2013.\n- C.Y. Ho et al., “Lamin A/C and emerin regulate MKL1-SRF activity by modulating actin dynamics,” Nature, 497:507-11, 2013.\n- A. Tajik et al., “Transcription upregulation via force-induced direct stretching of chromatin,” Nat Mater, 15:1287-96, 2016.\n- A.J. Engler et al., “Matrix elasticity directs stem cell lineage specification,” Cell, 126:677-89, 2006.\n- F. Chowdhury et al., “Material properties of the cell dictate stress-induced spreading and differentiation in embryonic stem cells,” Nat Mater, 9:82-88, 2010.\n- J. Liu et al., “Soft fibrin gels promote selection and growth of tumorigenic cells,” Nat Mater, 11:734-41, 2012.\n- Y. Tan et al., “Matrix softness regulates plasticity of tumour-repopulating cells via H3K9 demethylation and Sox2 expression,” Nat Commun, 5:4619, 2014.\n- J. Chen, “Efficient extravasation of tumor-repopulating cells depends on cell deformability,” Sci Rep, 6:19304, 2016","You are hereNovember 6, 2017 | Mesenchymal Stem Cells\nCell Volume modulates Mesenchymal Stem Cell Fate\nReview of “Cell volume change through water efflux impacts cell stiffness and stem cell fate” from PNAS by Stuart P. Atkinson\nMultiple studies have established that external osmotic pressure prompts a reduction in cell volume via the efflux of water, leading to higher cell stiffness and altered cell mechanics/behavior [1-3]. With this knowledge in hand, researchers from the laboratory of Jennifer Lippincott-Schwartz (Howard Hughes Medical Institute, Virginia, USA) and David A. Weitz (Harvard University, Massachusetts, USA) asked whether cells alter their volume to modify cell behavior in response to additional stimuli. In their new study, Guo et al. describe their new findings, and report a striking link between cell volume and mesenchymal stem cell (MSC) fate .\nThe authors discovered that cell growth on a stiff growth substrate also leads to a reduction in cell volume and an increase in cell stiffness mediated by water efflux - a similar behavior to cells cultured under external osmotic pressure. Substrate stiffness, cell spread area, and external osmotic pressure all correlated to decreased cell volume and increased stiffness mediated by water efflux. This reduction in cell volume leads to an increased concentration of intracellular material and a phenomenon known as “molecular crowding” and an overall modification of cell mechanics and cell behavior.\nInterestingly, the study then established that cellular water efflux, accompanied by a decrease in cell volume and an increase in stiffness, prompted alterations to MSC differentiation propensity, specifically towards an osteogenic or adipogenic fate. This all suggests that the consequences of reduced cell volume influences stem cell fate, a hypothesis strengthened by the finding that MSCs undergoing multilineage differentiation exhibit alterations to their cell volume.\nThe reduction in cell volume and increase in cell stiffness can affect processes occurring both in the cytoplasm and in the nucleus that influence stem cell fate. These include protein folding and binding kinetics, cell structure, in-cell transport, protein expression, and, importantly, chromatin structure, and, therefore, transcription and gene expression patterns [3, 5]. The next quest is to understand how MSCs control this phenomenon and to discover whether this means of fate determination exists in other stem cell types.\nTo hear more about cell volume and stem cell fate, keep the Stem Cells Portal bookmarked!\n- Zhou EH, Trepat X, Park CY, et al., Universal behavior of the osmotically compressed cell and its analogy to the colloidal glass transition. Proc Natl Acad Sci U S A 2009;106:10632-7.\n- Oh D, Zidovska A, Xu Y, et al., Development of time-integrated multipoint moment analysis for spatially resolved fluctuation spectroscopy with high time resolution. Biophys J 2011;101:1546-54.\n- Irianto J, Swift J, Martins RP, et al., Osmotic challenge drives rapid and reversible chromatin condensation in chondrocytes. Biophys J 2013;104:759-69.\n- Guo M, Pegoraro AF, Mao A, et al., Cell volume change through water efflux impacts cell stiffness and stem cell fate. Proceedings of the National Academy of Sciences 2017.\n- Swift J, Ivanovska IL, Buxboim A, et al., Nuclear lamin-A scales with tissue stiffness and enhances matrix-directed differentiation. Science 2013;341:1240104."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2b2dc5f4-ed0b-4106-9749-9b7fab814236>","<urn:uuid:a8206997-7a01-4b8b-9297-fb8b6d606d4e>"],"error":null}
{"question":"How do the rules of chess enforce careful decision-making?","answer":"The rules of chess enforce careful decision-making through strict laws such as 'if you touch a piece, you must move it somewhere' and 'if you set it down, you must let it stand.' This teaches players not to make moves too hastily and helps them develop caution in their gameplay.","context":["Was andere dazu sagen - Rezension schreiben\nEs wurden keine Rezensionen gefunden.\nAndere Ausgaben - Alle anzeigen\nMr. Hoyle's Game of Chess: Including His Chess Lectures, With Selections ...\nKeine Leseprobe verfügbar - 2018\nadversary's king BACK GAME bishop gives check bishop's fourth square bishop's second square bishop's third square black king Chess covers the check defence king castles king takes king's bishop's fourth king's bishop's pawn king's bishop's third king's fifth king's knight's fourth king's knight's pawn king's pawn king's rook king's rook's pawn king's second square king's square king's third square knight's fourth square knight's second square knight's third square pawn one move pawn one step pawn two moves pawn two steps pieces queen gives check queen takes queen's bishop's fourth queen's bishop's pawn queen's bishop's third queen's fourth square Queen's Gambit queen's knight's pawn queen's pawn queen's rook's pawn queen's second square queen's third square retakes the knight retakes the pawn rook gives check rook's fourth square rook's third square stale-mate takes the bishop takes the knight takes the pawn takes the queen takes the rook white king's knight's wins the game\nSeite xi - The game of chess is not merely an idle amusement. Several very valuable qualities of the mind, useful in the course of human life, are to be acquired or strengthened by it, so as to become habits, ready on all occasions.\nSeite xi - For life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effects of prudence or the want of it.\nSeite xii - If you touch a piece, you must move' it somewhere ; if you set it down, you must let it stand...\nSeite xii - Caution, not to make our moves too hastily. This habit is best acquired by observing strictly the laws of the game ; such as,\nSeite 13 - ... the adversary a piece ; but one separated from the others is seldom of any value. And whenever you have gained a pawn, or other advantage, and are not in danger of losing the move thereby, make as frequent exchanges as you can.\nSeite xii - ... more the image of human life, and particularly of war ; in which, if you have incautiously put yourself into a bad and dangerous position, you cannot obtain your enemy's leave to withdraw your troops, and place them more securely, but you must abide all the consequences of your rashness. And, lastly, we learn by chess the habit of not being discouraged by present bad appearances in the state of our affairs, the habit of hoping for a favourable change, and that of persevering in the search of...\nSeite xi - If I move this piece, what will be the advantage of my new situation? What use can my adversary make of it to annoy me? What other moves can I make to support it, and to defend myself from his attacks?\nSeite 10 - MOVE your pawns before your pieces, and afterwards bring out the pieces to support them ; therefore the king's, queen's, and bishop's pawns should be the first played, in order to open the game well. 2. Do not, therefore, play out any of your pieces early in the game, because you thereby lose moves, in case your adversary can, by playing a pawn, make them retire, and also opens his game at the same time; especially avoid playing your queen out, till your game is tolerably well opened.\nSeite 12 - Let not your adversary's knight (particularly if duly guarded) come to check your king and queen, or your king and rook, or your queen and rook, or your two rooks at the same time : for in the..."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5b1a09e9-f0ab-40c6-a087-350dff0ae9cc>"],"error":null}
{"question":"What are the key features of the new coastal energy planning tool, and what adaptation strategies does it support for managing climate change impacts?","answer":"The new decision-making tool combines sea-level rise projections with economic flood risk modeling for coastal energy infrastructure. It uses a 'Real Options' methodology to compare flood impact costs with protection measure costs, running multiple flood model scenarios based on UK Climate Projections. For adaptation strategies, the tool supports low-regret measures such as building with nature solutions, adjusting infrastructure design criteria, making spatial reservations for future defense needs, and introducing set-back lines to gradually replace settlements in high-risk areas. The tool is particularly relevant for the UK, which has three times more coastal facilities than other European countries.","context":["A new decision-making tool to help the energy sector plan effectively for the future and mitigate the consequences of flooding and rising sea levels on their coastal facilities has been developed by a research team led by the University of Liverpool.\nThe NOC has been involved in the development of a new decision-making tool to help the energy sector plan effectively for the future and mitigate the consequences of flooding and rising sea levels on their coastal facilities.\nThe new tool is the result of research by the Adaptation and Resilience of Coastal Energy Supply (ARCoES) project led by the University of Liverpool, working with researchers from the NOC and the University of Exeter, as well as industrial partners, National Grid, Electricity North West and EDF.\nThe tool combines sea-level rise projections for each decade in the 21st Century with modelling to determine the economic costs of flooding to coastal energy infrastructure – such as electricity substations and nuclear power facilities.\nThe research team used their expertise in hydrodynamic modelling, flood risk assessment and economics to develop a `Real Options’ methodology for weighing up the costs and benefits for making coastal energy facilities more resilient to coastal storms and sea-level rise.\nThe impact costs are compared with the cost of putting measures in place to protect these assets, such as flood defences, retaining walls, and raising the height of switch gear.\nThe novelty of the tool comes from running the flood model many times over using probabilities assigned to UK Climate Projections (UKCP09) of sea-level rise.\nThis information is particularly important for the UK which has three times as many coastal facilities as any other European country.\nProfessor of Geography and Planning and principal investigator on the ARCoES project, Andy Plater, said: “Scientists often focus on the problems of climate change and identifying the underlying cause, perhaps overlooking the fact that the problem needs to be dealt with by someone in a cost-effective and timely manner.\n“Through the ARCoES project, we’ve provided partners across the energy and coastal sectors with information and decision-support tools that help to inform operations and strategy in relation to their climate change adaptation measures. The Real Options method applied here will be used to shape optimal resource allocation to build resilient infrastructure.”\nDr Jenny Brown, lead scientist at the NOC, said: “It is important that our collaborative research, delivered through the Liverpool Institute for Sustainable Coasts and Oceans (LISCO) benefits the economy, public policy or services, the environment and society. Through the ARCoES project and associated Impact Acceleration Account we also developed modelling tools for designing large-scale beach recharge schemes, known as Sand Engines, to mitigate problems of coastal erosion and flooding.”\nDr Tom Prime, from the NOC and developer of the Real Options methodology, alongside Karyn Morrissey at the University of Exeter, added: “The approach developed here for the NW region can be applied anywhere in the world to coastal infrastructure of any kind.\n“This versatility is critical in ensuring that our research can be used to achieve resilient coastal communities and economies, particularly in areas where storms are a threat to life and livelihoods. The impacts of hurricanes Harvey, Irma and Maria in the Caribbean and US last year evidenced the vulnerability of energy systems to severe tropical storms.”\nResearch on the ARCoES project is now continuing through BLUEcoast, a NERC-funded research consortium investigating how sediments and biota interact to provide natural coastal resilience. The focus of this project is on providing stakeholders with new information on coastal response to climate change that will inform decisions about nationally important energy infrastructure at Sizewell, Dungeness, Hinkley Point and Oldbury-on-Severn.\nThe adaptation and Resilience of Coastal Energy Supply (ARCoES) project is funded by the Engineering and Physical Sciences Research Council (EPSRC).\nThe research is published in the Journal of Ocean and Coastal Economics.","Climate adaptation policies for the coastal zone\nThis article addresses the question which policy strategies have the best ‘fit’ for the challenges of climate adaptation for the coastal zone. Parts of this article are based on a paper by Dronkers and Stojanovic (2016).\n- 1 Urgency of climate adaptation\n- 2 Uncertainty and awareness\n- 3 Risk-based adaptation\n- 4 Scenarios\n- 5 Adaptation pathways\n- 6 Low-regret adaptation strategy\n- 7 Mainstreaming climate adaptation\n- 8 Knowledge and monitoring\n- 9 Financing climate adaptation\n- 10 Related articles\n- 11 Further reading\n- 12 References\nUrgency of climate adaptation\nClimate change, and sea level rise in particular, is a major threat for many coastal zones (see the article Sea level rise). Many coastal zones around the world are already lying around or even below high-water sea levels, especially delta coastal plains and small islands. Soil subsidence, saline intrusion and water shortage add to the vulnerability, which is further exacerbated by fast population growth. Measures for dealing with the impacts of climate change in these coastal zones are already urgent today. Such measures interfere with other developments and interests in the coastal zone and should therefore be embedded in an ICZM strategy (see the article Integrated Coastal Zone Management (ICZM)).\nUncertainty and awareness\nCountries with low-lying coastal zones will have to face climate change and some impacts are already occurring. However, separating the impacts of climate change from change produced by other natural or human causes is still very difficult. Uncertainty about the impacts of climate change is a serious (perhaps the most serious) obstacle to raising public awareness and for getting climate adaptation high on the political agenda, compared to issues with a more immediate impact . Uncertainty about the possible impact of climate change is not the only reason. The fact that the greatest impacts are related to exceptional extreme events also plays a role. According to a survey among European policymakers, the occurrence of an extreme weather event is presently the most important trigger for progress in climate adaptation .\nThe largest climate change impacts in the coastal zone result from extreme events which have a low probability of occurrence within a given time interval. The assessment of risk, defined as the product of probability of occurrence and resulting damage, theoretically provides an objective measure for the need to adapt to these impacts. By evaluating which damage is avoided at which costs, informed choices can be made among different adaptation strategies. Uncertainty in the probability of occurrence and uncertainty in the estimated damage can be incorporated in a risk assessment, for instance, by using a Monte Carlo method (assumptions have to be made for the probability distributions of independent variables involved in the risk assessment). The application of the risk concept in adaptation strategies is limited, however, by the difficulty to quantify uncertainty in the probability of occurrence and by the more fundamental difficulty to assess possible damage caused by rare extreme events.\nA further complication arises when a choice has to be made among different possible adaptation measures: which time scales and spatial scales have to be considered ? The choice of these scales strongly influences the outcome of ranking methods (based, for example, on cost-benefit analysis, cost-effectiveness or multi-criteria analysis). This complication is enhanced by the uncertainty about the future in general. How are values of present assets affected by other future global or local changes, in addition to climate change, and how do societal interests evolve? The conjugation of these different sources of uncertainty is sometimes called “deep uncertainty”.\nIt is very likely that sea level rise will go on for a long time . The same holds for other developments, for example developments related to population growth. Planning for climate change adaptation therefore requires a long-term prospect, taking into account different scenarios for the future. Scenarios provide a way to deal with limitations related to quantifying uncertainty (the probability that a damaging event will occur) and to quantifying possible damage (loss of human lives, loss of assets and loss of other values). Scenarios describe the various futures that can be imagined. These scenarios should be internally consistent, but they are not necessarily expressed in terms of probability and money. Their main function is to open the views of those who are involved in climate adaptation to the broad spectrum of situations and adaptation options that should be considered. Scenarios help avoiding suboptimal sector approaches and a one-sided focus on certain adaptation options, which are in general major shortcomings of current coastal adaptation strategies. However, scenarios do not answer the question which adaptation strategy among different options should be preferred.\nThere is general agreement that adaptation to the impacts of climate change is inevitable and that preparatory actions should already be initiated. But once it becomes clear that a fundamental revision of present coastal policies is needed, the questions arises which actions are most appropriate for coping with the impacts of climate change in the long term. Revised policies have to deal not only with the uncertainty related to the future impacts of climate change, but also with uncertainties related to future social and economic developments. A static plan is inadequate, as the future can unfold differently from what is anticipated. Actions that are appropriate for the foreseeable future can reveal inadequate for the long term and even hinder actions that may become necessary later.\nOne way to deal with this problem of “robust decision making” is the strategy of adaptive pathways (Hallegatte, 2009) . According to this strategy, adaptation pathways are developed that consist of different sets of successive adaptation actions. Each step of such a pathway should ultimately lead to successful long term adaptation within a particular scenario of climate change and socio-economic development. The analysis of the different pathways enables the selection of short term actions that are suitable (no adverse lock-in effects) within different scenarios. The most promising actions are those with the best performance in terms of societal benefits and costs. The steps of pathway definition and analysis is repeated when new follow-up actions become needed; the lessons of the first actions (according to “learning-by-doing”) as well as the newest knowledge of climate change and socio-economic development serve as input. A refined version of this approach (“strategy of dynamic adaptive policy pathways”) has been used to support the Dutch Delta programme for adaptation to climate change (Haasnoot et al., 2013). A similar method has been developed by Sayers et al. (2013)  and applied to the Thames estuary (McGahey and Sayers, 2008) .\nLow-regret adaptation strategy\nA reasonable and generally preferred strategy for climate change adaptation is to start with so-called 'low-regret measures' (or, even better, 'no-regret measures'). These are adaptation measures that generate immediate benefits without the need for high additional investments. Examples of such measures are:\n- Choose building with nature solutions for renewing coastal infrastructural works when they have reached the end of their life span (e.g., wetland restoration, dune/beach/shoreface nourishments);\n- Adjust design criteria to extend the lifetime of infrastructural works by incorporating expected the sea level rise in the periodic maintenance/renovation scheme;\n- Make spatial reservations for nature development (or for other temporal benefits) which can eventually serve later for future reinforcement or realignment of coastal defences;\n- Introduce set-back lines to gradually replace settlements in future high-risk areas with nature development with a protective function (e.g., stimulating dune growth, mangrove development).\nOther low-regrets measures include early warning systems; risk communication between decision makers and local citizens; sustainable land management, including land use planning; ecosystem management and restoration; improvements to water supply, sanitation, irrigation and drainage systems; development and enforcement of building codes and better education and awareness . Such measures deliver additional benefits, such as opportunities for tourism, recreation, nature development and other ecosystem services.\nLow-regret measures are implemented step by step, allowing for adjustment when better knowledge of the impacts of climate change impacts becomes available. They respond to the insight that natural dynamics generally offer greater long-term resilience (self-regulating capacity) against climate change impacts than hard man-made structures. A few examples of low-regret adaptation measures are illustrated in figures 1-4.\nMainstreaming climate adaptation\nMainstreaming climate adaptation means that actors in all policy areas that affect the state of the coastal zone are permanently aware of the consequences of climate change and adjust their policies accordingly. Climate adaptation should become a natural component of relevant current policies, at national level, at regional level and at local level. Policy measures are tested for robustness in relation to climate change and adapted to better anticipate the consequences of climate change.\nClimate adaptation is an essential component of Integrated Coastal Zone Management and must be part of the policy cycle for the implementation of ICZM:\nClimate adaptation plan => Implementation => Monitoring => Evaluation => Plan revision => Implementation => Monitoring => Evaluation, etc.\nfollowing the same lines as discussed in Integrated Coastal Zone Management (ICZM). A broad range of measures in different sectors can contribute to climate adaptation. An overview of relevant measures is drawn up by USAID (2009) , see Table 1.\nKnowledge and monitoring\nAdaptation efforts benefit from iterative risk management strategies due to the complexity, uncertainties and long-term developments related to climate change . Such an iterative risk management strategy consists of an iterative process of monitoring, research, evaluation, learning and innovation. Addressing knowledge gaps through improved observation and research reduces uncertainty and helps to design effective adaptation and risk management strategies.\nMonitoring is essential for a better understanding of climate change impacts in the coastal and marine zone. A coordinated and consistent approach to marine and marine monitoring is essential for a proper analysis of change in the coastal and marine system. This analysis should focus on the establishment of cause-impact relationships, which make it possible to distinguish climate change impacts from natural variability and other impacts. Monitoring data are often not directly fit for policy evaluation; translating data into indicators pertinent to policy making is a further subject of special attention. Various examples are given in the literature, for instance by Breton (2006)  and Marti et al. (2007) ), see Integrated Coastal Zone Management (ICZM). Measurable indicators and quantitative targets are essential to assess progress in climate adaptation, to inform policy and the general public and to develop adaptive capacities of institutions and the wider society.\nFinancing climate adaptation\nIn many developing countries the impact of climate change is exacerbated by fast urban development in the coastal zone. However, financial claims for coastal zone climate adaptation have to compete with other urgent development priorities. Grants and loans from international donor programs are an important resource for many developing countries. Several donor programs provide opportunities for financing coastal zone climate adaptation. An overview of these programs is given in 'A Resource Guide to Climate Finance' (2018). A few important international funding programs are specifically mentioned below.\nThe Adaptation fund\nThe Adaptation Fund (https://www.adaptation-fund.org/) was established to finance concrete adaptation projects and programs in developing countries that are parties to the Kyoto Protocol and are particularly vulnerable to the adverse effects of climate change. Since 2010, the Adaptation Fund has committed US$ 532 million, including supporting 80 concrete adaptation projects with about 5.8 million direct beneficiaries.\nThe Global Environmental Facility (GEF)\nFunds of the Global Environmental Facility (https://www.thegef.org/about/funding) are available to developing countries and countries with economies in transition to meet the objectives of the international environmental conventions and agreements. GEF support is provided to government agencies, civil society organizations, private sector companies, research institutions, among the broad diversity of potential partners, to implement projects and programs in recipient countries. The Global Environment Facility (GEF) was established at the 1992 Rio Earth Summit to tackle environmental problems; the World Bank serves as the GEF trustee, administering the GEF Trust Fund. Since 1992, the GEF has provided US$ 17 billion in grants and has mobilized an additional US$ 88 billion in loans for 4000 projects in 170 countries.\nThe Green Climate Fund (GCF)\nThe Green Climate Fund (https://www.greenclimate.fund/home) is a financial mechanism under the UNFCCC, established at COP16 in 2010, adopted in 2011, and operational since 2015. The Fund is a global platform to respond to climate change by investing in low-emission and climate-resilient development. GCF was established to limit or reduce greenhouse gas (GHG) emissions in developing countries, and to help vulnerable societies adapt to the unavoidable impacts of climate change. In 2018 the committed funding was 4.6 billion US$.\n- Integrated Coastal Zone Management (ICZM)\n- Integrating Climate Change into the ICZM planning process - Introduction\n- Sea level rise\n- Ecomorphology and habitat restoration: introduction\n- Articles in the category Climate change, impacts and adaptation\nYoung, O. R., King, L. A. and Schroeder, H. Eds. 2008. Institutions and environmental change. Principal Findings, Applications, and Research Frontiers. Cambridge, MIT Press\n- Dronkers J., Stojanovic, T. 2016. Coastal Management and Governance. In: North Sea Climate Change Assessment (Editors F. Colijn, M. Quante), Springer Verlag: 475-488\n- Wong, P.P., I.J. Losada, J.-P. Gattuso, J. Hinkel, A. Khattabi, K.L. McInnes, Y. Saito, and A. Sallenger 2014. Coastal systems and low-lying areas. In: Climate Change 2014: Impacts, Adaptation, and Vulnerability. Part A: Global and Sectoral Aspects. Contribution of Working Group II to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Field, C.B., V.R. Barros, D.J. Dokken, K.J. Mach, M.D. Mastrandrea, T.E. Bilir, M. Chatterjee, K.L. Ebi, Y.O. Estrada, R.C. Genova, B. Girma, E.S. Kissel, A.N. Levy,S. MacCracken, P.R. Mastrandrea, and L.L. White (eds.)]. Cambridge University Press, pp. 361-40\n- EEA, 2014. National adaptation policy processes in European countries — 2014. EEA report 2014/4\n- IPCC, 2014. Climate Change 2014: Synthesis Report. Contribution of Working Groups I, II and III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Core Writing Team, R.K. Pachauri and L.A. Meyer (eds.)]. IPCC, Geneva, Switzerland, 151 pp.\n- Hallegatte, S. 2009. Strategies to adapt to an uncertain climate change. Global Environmental Change 19: 240–247\n- Haasnoot, M., Kwakkel, J.H., Walker, W.E. and Ter Maat, J. 2013. Dynamic adaptive policy pathways: A method for crafting robust decisions for a deeply uncertain world. Global Environmental Change 23: 485–498\n- Sayers, P., Li, Y., Galloway, G., Penning-Rowsell, E., Shen, F., Wen, K., Chen, Y. and Le Quesne, T. 2013. Flood Risk Management: A Strategic Approach. Paris, UNESCO\n- McGahey, C. and Sayers, P.B. 2008. Long term planning – robust strategic decision making in the face of gross uncertainty – tools and application to the Thames. In: Flood Risk Management: Research and Practice. Proceedings of FLOODrisk 2008, Taylor & Francis, London, UK, pp. 1543–1553\n- IPCC, 2011. Summary for Policymakers. In: Intergovernmental Panel on Climate Change Special Report on Managing the Risks of Extreme Events and Disasters to Advance Climate Change Adaptation. Authors: Field, C. B., Barros, V., Stocker, T.F., Qin, D., Dokken, D., Ebi, K.L., Mastrandrea, M. D., Mach, K. J., Plattner, G.-K., Allen, S. K., Tignor, M. and P. M. Midgley (eds.). Cambridge University Press\n- USAID 2009. Adapting to coastal climate change: A guidebook for development planners. The U.S. Agency for International Development Global Climate Change Team Washington, DC, USA. https://www.crc.uri.edu/download/CoastalAdaptationGuide.pdf\n- Breton, F. 2006. Report on the use of the ICZM indicators from the WG-ID. A contribution to the ICZM evaluation. EEA, European Topic Centre Terrestrial Environment, Universitat Antònoma de Barcelona\n- Martí, X., Lescrauwaet, A-K., Borg, M. and Valls, M. 2007. Indicators Guidelines To adopt an indicators-based approach to evaluate coastal sustainable development. Deduce project, Department of the Environment and Housing, Government of Catalonia. http://www.im.gda.pl/images/ksiazki/2007_indicators_guidelines.pdf\n- ACT Alliance 2018. A Resource Guide to Climate Finance. ACT Alliance Global Climate Change Project. https://actalliance.org/wp-content/uploads/2018/06/ENGLISH-quick-guide-climate-finance.pdf\nPlease note that others may also have edited the contents of this article."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:64e3cf70-9db4-4e7a-ae77-10ada05cf23a>","<urn:uuid:e63a50b8-32b5-40a3-993f-893e20ba8b02>"],"error":null}
{"question":"How do both Vietnamese cuisine and turmeric contribute to inflammation reduction?","answer":"Turmeric has explicit anti-inflammatory properties, with scientific evidence showing it acts like a vacuum cleaner for the system and controls internal inflammation through its curcumin compound. Vietnamese cuisine, while generally healthy with its emphasis on fresh herbs and vegetables, isn't specifically noted in the documents for anti-inflammatory properties. The documents show that turmeric appears in some Vietnamese dishes like cha ca la vong and Com ga Hoi An, but its anti-inflammatory benefits are best obtained by consuming half a teaspoon daily with a pinch of black pepper.","context":["One of the healthiest cuisines in the world, Vietnamese dishes feature plenty of fresh herbs and vegetables. Rice and noodles are also a big part of Vietnamese cuisine. Don’t leave the country without getting a taste of these Vietnamese dishes.\nThese steamed rice rolls are filled with minced pork, wood ear mushrooms and onions, topped with fried shallots and coriander, and served with nuoc cham (dipping sauce comprising fish sauce, lime juice, garlic, chilli, water and sugar), bean sprouts, basil and cha lua (Vietnamese pork sausage).\nThese mini savoury pancakes are made with coconut milk, rice flour and turmeric powder, topped with shrimp, mung beans, spring onions, fried garlic or dried shrimp flakes, served with herbs and vegetables, and a spicy sweet dipping sauce.\nOne of Vietnam’s most popular street foods, this sandwich is made from crusty baguette (introduced to Vietnam by French colonists in the late 19th century) that is stuffed with meats such as pork, pate, ham and sausage, eggs and vegetables such as cilantro, cucumber, pickled carrot and daikon, drizzled with chilli sauce or mayonnaise.\nThese crispy savoury crepes, which translate as “sizzling cakes”, are made with rice flour, coconut milk and turmeric powder, and filled with pork, shrimp, bean sprouts and spring onions. They are usually served with rice paper, herbs and vegetables such as lettuce, basil and mint, and nuoc cham (dipping sauce comprising fish sauce, lime juice, garlic, chilli, water and sugar). Place small portions of banh xeo together with some greens in the moistened rice paper and roll it up before dipping it into the nuoc cham.\nBo la lot\nThis popular southern Vietnamese dish is made of minced beef mixed with garlic, shallots and spices, wrapped in wild betel leaf (or piper lolot leaves), then grilled over a charcoal flame. It is topped with crushed roasted peanuts, and served with rice paper, vermicelli rice noodles, raw herbs and vegetables, and nuoc cham (dipping sauce comprising fish sauce, lime juice, garlic, chilli, water and sugar).\nBo luc lac\nThis dish, which translates as “beef shaking”, takes its name from the motion of the wok or pan when the beef cubes are being stir-fried over high heat. The beef is marinated in garlic, fish sauce, oyster sauce, soy sauce, sugar and sesame oil, then stir-fried with onions, and served on a bed of vegetables such as watercress, cucumbers and tomatoes.\nThis popular street food is the Vietnamese version of Singapore’s and Malaysia’s fried carrot cake. Instead of daikon or white radish, the rectangular flour cakes are made with rice flour and tapioca starch, fried with soy sauce, fish sauce, chilli sauce and eggs, and topped with green onions and shredded green papaya.\nBun bo Hue\nA Hue speciality, this spicy beef noodle soup features a rich and flavourful broth which is made by slowly simmering beef and pork parts such as ox tail, beef shank, pork neck bones, pork feet and pork knuckles or ham hocks, together with lemon grass, shrimp paste, garlic, shallots, red pepper flakes, paprika and ground annatto seeds to give it a bright red colour. The soup is eaten with round rice noodles and topped with beef slices, ham slices, cubes of congealed pig’s blood, green onions, white onions and coriander leaves. It is usually served with a plate of lime wedges, banana blossoms, bean sprouts, mint leaves and other herbs.\nOne of Hanoi’s most famous foods, this dish of rice vermicelli with grilled pork is served with dipping sauce, pickled carrots and papaya, and fresh herbs and vegetables.\nBun dau mam tom\nAs its name indicates, there are three main ingredients in this dish – bun is Vietnamese for vermicelli noodle, dau is tofu (usually fried) and mam tom is fermented shrimp paste sauce. Some places also serve this dish with boiled pork, young rice cake, fried spring rolls, cucumber and fresh herbs.\nA Hanoi speciality, this chicken-based soup is served with vermicelli noodle and topped with shredded chicken, Vietnamese ham slices, finely sliced omelette, green onions and fresh herbs.\nBun thit nuong\nThis popular dish comprises grilled pork, thin vermicelli rice noodles, chopped lettuce, sliced cucumbers, bean sprouts, pickled daikon and carrots, basil, mint and chopped peanuts, served with fried spring roll and eaten with nuoc cham (dipping sauce comprising fish sauce, lime juice, garlic, chilli, water and sugar).\nHoi An speciality – rice noodles with barbecued pork slices, crunchy pork crackling, bean sprouts and local greens\nCa phe trung\nThis Vietnamese egg coffee is made by beating egg yolks with sweet condensed milk until it becomes airy and creamy. The meringue-like mixture is then slowly poured on top of hot or iced coffee.\nCha ca la vong\nThis iconic Hanoi dish features fish that has been marinated with turmeric, shrimp paste and fish sauce before being grilled, tossed with dill and spring onions, and served with vermicelli noodles, roasted peanuts and dipping sauce.\nCom ga Hoi An\nA Hoi An speciality, this chicken rice features rice cooked in chicken broth and turmeric, served with shredded chicken, shredded papaya and herbs.\nThis dish of broken rice is served with grilled pork chops, shredded pig skin, egg meat loaf, cucumber, tomato, pickled vegetables, fried egg and dipping sauce.\nA Hanoi speciality and a health tonic, this stewed sweet herbal chicken soup is cooked with chicken and a mix of Chinese herbs such as red dates, honey dates, wolfberries, dried longan, pei ji and yuk chuk (Solomon’s Seal).\nOne of Vietnam’s most iconic dishes, these fresh spring rolls can be eaten as a snack, appetiser or main meal. The rolls are made with rehydrated rice paper that is filled with shrimp or pork, raw vegetables like basil, coriander and mint, and vermicelli noodles, and served with nuoc cham (dipping sauce comprising fish sauce, lime juice, garlic, chilli, water and sugar) or peanut sauce.\nThis popular dish of rice noodle soup is topped with pork, prawns and fish balls.\nThis Hoi An speciality features turmeric noodles topped with roasted pork, shrimp, fresh lettuce, mint, basil, garlic, spring onions, quail eggs and crushed peanuts.\nRegarded as the national dish of Vietnam, this flat rice noodle soup features two basic types – beef (pho bo) or chicken (pho ga) – and is usually served with a plate of fresh herbs and vegetables such as basil, cilantro and bean sprouts, along with lime wedges, chilli sauce and hoisin sauce (a sweet and spicy sauce made from soy beans, sugar, garlic and vinegar).\nOne of the country’s most popular foods, xoi is Vietnamese for sticky rice or glutinous rice. There are many sweet and savoury variations available, with the steamed sticky rice mixed with peanuts, beans, fruit, coconut, pandan juice and sesame seeds, and others eaten with meat and eggs.","Turmeric, whose Sanskrit name translates to “germicide, known for its glorious yellow color, has been used in Asian kitchens for thousands of years. The spice is more commonly known in the United States for the hue it imparts to prepared mustard and curries, but on its home turf of India it is revered for its medicinal properties: it works as a powerful anti-inflammatory for arthritic conditions; aids digestion; may improve cognitive function; and may even suppress tumors.\nNutritionist Rebecca Katz, a senior chef in residence at a leading cancer wellness center and the author of the upcoming book The Longevity Kitchen: Satisfying, Big-Flavor Recipes Featuring the Top 16 Age-Busting Power Foods, says turmeric acts like a vacuum cleaner for your system: “It vacuums up free-radical debris that can cause disease. Turmeric is the aspirin or ibuprofen of the spice set. It controls internal or systemic inflammation, which is implicated in so many chronic diseases, from arthritis and Alzheimer’s to cancer.”\nTurmeric, whose Sanskrit name translates to “germicide,” is a rhizome (an underground root), like ginger. While it is available fresh in some Asian markets, it is most commonly sold in its dried, ground form. One of the foundations of a good curry, it pairs well with cumin, coriander, and cinnamon. When used sparsely, the spice doesn’t have much flavor on its own. About half a teaspoon is enough to flavor a dish that serves four people; too much will make your dish bitter. “If you do find there’s a bitter taste in the back of your palate, balance it with a little sweetness by adding a few drops of honey or maple syrup,” Katz advises.\nTurmeric contains the powerful phytochemical curcumin. According to a research paper released this past June by the National Institutes of Health, scientists studying curcumin have found evidence of its anti-inflammatory and antioxidant properties and of its potential use in the treatment of neurodegenerative diseases like Alzheimer’s, but indicate that more studies will have to be done before it can be adapted for that use.\nSo how much of this magical spice do you need to gain its benefits? Just half a teaspoon a day! But there is also one secret: Katz says that to derive turmeric’s full anti-inflammatory benefits, you should always use it with a pinch of black pepper.\nEasy Monday-Night Vegetable Pilaf\nYou can substitute your choice of long-grain or brown rice; just be sure to adjust the cooking time as needed.\n1 cup brown basmati rice\n2 tablespoons vegetable oil\n1 teaspoon cumin seeds\n2 shallots, peeled and sliced\n1 cup frozen peas\n1/2 teaspoon ground turmeric\n1/4 teaspoon ground black pepper\n1 teaspoon fresh lemon juice\n2 cups vegetable broth\n- Rinse the rice at least 3 or 4 times with water. Drain well and set aside.\n- In a deep pan, heat the vegetable oil. Add the cumin seeds, and when they begin to sizzle, add the shallots. Sauté for 2 to 3 minutes, until the shallots are soft.\n- Add the peas and sauté for about 2 minutes.\n- Add salt to taste, if desired, then the turmeric, pepper, lemon juice, and rice. Mix well.\n- Add the broth and bring to a rolling boil. Reduce the heat to low. Cover and cook for 12 to 15 minutes, until all of the liquid has been absorbed.\n- Remove from heat. Let sit, covered, for about 5 minutes. Fluff with a fork and serve.\nMakes 4 servings\nEasy ways to benefit from nature’s golden treasure.\nBy: Monica Bhide – Issue: 2012 September-October"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:091db215-1f4d-4019-b17a-42042e2bb43e>","<urn:uuid:60ef362b-3ad7-4b03-88bf-3efc0fd1d730>"],"error":null}
{"question":"How do information systems support product success in the market, and what role do they play in environmental impact reporting?","answer":"Information systems are crucial for product success by helping companies identify potential markets for new products, track marketing campaign effectiveness, and monitor product status for decision-making. Companies with effective marketing information systems better understand product lifecycle curves and can make decisions to extend product lifespan. In terms of environmental reporting, information systems have become essential for managing and disclosing complex data requirements. Under modern standards like LEED v4, manufacturers must track and report detailed information about environmental impacts, raw material sources, and ingredient compositions. This includes managing data for Environmental Product Declarations (EPDs) and documentation of supplier commitments to responsible practices for raw materials.","context":["Product Lifecycle Management (PLM) is a strategic process of allocating resources – how to reduce cost and introduce the product in the market that is according to customer needs and wants. Furthermore, the sound PLM also enables the new product to have a healthy lifespan and profitability.\nSince every product has a limited life and goes through five stages of products life cycle i.e. product introduction, growth, maturity and decline.\nProduct Lifecycle Management\nEffective Product Lifecycle Management needs the company to focus on different business areas that can affect the PLM.\nResearch and Development\nWhen a product introduces in the market it is backed with rigorous research and development process. R&D department works hard to convert product concept into a successful prototype that satisfies market needs and wants. New product manufacturing takes even years and huge financial resources to make a success in the eye of consumers.\nWhen a company manufactures a product, the manufacturing cost. During the product lifecycle, the manufacturing cost varies at each stage. In the initial stage, product manufacturing cost is very high due to low customer base and sale volume.\nWhen the product life goes on, through economies of scale and learning curve the manufacturers can reduce cost and produce more affordable products to customers.\nThe company can also cut product cost by attainting technological advancement in the manufacturing process that can produce more units in less time.\nAs I earlier said, manufacturing needs both time and huge funds to introduce a new product in the market. The finances of growth and maturity stages can be covered out of the revenue generated. The decline stage may need extra finances, depending on the strategies adopted, for example rejuvenating by adding more features that are acceptable to the new customers. The company must adopt those financial strategies that maximize profitability throughout the product life.\nInformation is Crucial for Product Success\nEffective information system plays an important role at each stage of product lifecycle. Information can help the company to know about the potential market when introducing a new product. Data and statistics can identify which marketing campaigns achieve targeted goals. The marketing information let management know the product status to take decisions accordingly.\nThose companies having effective marketing information system can better understand product life cycle curve and take marketing decision that can increase the lifespan of a product.\nMarketing and Promotional Activities\nThere are several marketing strategies available when companies launch a new product in the market. These strategies can help to set the intensity of each marketing mix element i.e. price, place (distribution) and promotion.\nFor example, A company may launch a new product and set high prices to recover development cost and low prices to build market share. Distribution is very selective until consumers accept the product. These strategies will change when the product reaches growth, maturity and even decline stage. When management designing campaigns, distribution and pricing strategies, it tries to ensure market share and prosperous product life.\nEvery product has a limited life. But those companies focus on business areas of product lifecycle management can make sure that the product has a successful and healthy life during its life cycle stages.","What is the life cycle environmental impact of your product? Where and how do you get your raw materials? What is in your product, and is any of it hazardous?\nThese may seem like reasonable questions to the average sustainability professional, but few manufacturers like to consider a future in which answering these questions publicly is a basic requirement of market entry.\nBut this dream or nightmare scenario will become a reality in the upcoming era of LEED v4.\nThe membership of the U.S Green Building Council recently approved major changes to the LEED rating system, now known as LEED v4, including a significant overhaul of the materials and resources credits. As a building products manufacturer, it is our business to keep track of changes in the green building certification world, but why should anyone outside the building industry care about LEED v4?\nIn short, the changes to LEED, the world’s dominant green building standard, with more than 2 billion square feet of certified commercial space, are so dramatic that they will send ripples into other industries and shift expectations on sustainability reporting and performance far beyond the building industry.\nLEED v4’s materials and resources credits are likely to accelerate four existing sustainable business trends by creating a global incentive for firms to make their products meet the complex and demanding new transparency and performance requirements. As ever in LEED, these credits are optional, but products that contribute to more of the six new LEED points possible are more likely to gain favor with building designers and owners.\n1. Harnessing the power of transparency\nFor the first time, a building product can contribute to a LEED point just by disclosing information related to environmental and health impacts.\nYes, this means that even if your product contains carcinogens and has a King Kong-sized carbon footprint, it theoretically could contribute to two LEED points just by being transparent about these unfortunate facts.\nThis has created controversy in some quarters, which led to the addition of matching “optimization” or performance-related credits. We have and will continue to argue that transparency drives improvement for business in a way that static performance requirements such as “minimum 30 percent recycled content” or “No Red List Chemicals” do not. So even if a building designer never reads the detailed product disclosures submitted for a LEED v4 project, manufacturers know what they disclosed. This knowledge starts the internal inquiry into how questionable processes or materials could be improved before the next disclosure.\nWe can only imagine a similar phenomenon occurred when the FDA first started requiring trans-fats to be disclosed on the Nutrition Facts Label. How many food companies reformulated products knowing this transparency mandate was coming? You could argue that it created more change faster, while creating less resistance from a powerful business lobby than banning trans-fats outright.\n2. Using LCA as a product differentiator\nEarlier versions of LEED have relied on single-attribute proxies, such as recycled, reused or bio-based content for identifying building materials with reduced environment impacts. LEED v4 is pioneering the use of verified life cycle assessment (LCA) data in an attempt to more holistically assess environmental impacts across the entire life cycle of a product. A new credit asks manufacturers to provide Environmental Product Declarations (EPDs) or third-party verified life cycle assessments.\nJust having an EPD contributes to one point, while showing that your product’s impacts are below industry averages contributes to a second point. Recycled content still will contribute, but only to the credit for responsibly sourced raw materials.\nSince LEED v4 was approved this summer, firms that verify LCA data report a flood of new customers that want to produce EPDs. For the first time, businesses see a real possibility of LCA becoming a market differentiator or a baseline requirement to compete in a high-profile market.\n3. Responsible sourcing of (all) raw materials\nFor those familiar with chain-of-custody requirements for certified wood or conflict minerals reporting, the new credit for raw materials sourcing will seem familiar.\nThe credit requires manufacturers to report extraction locations and supplier commitments to responsible practices for 90 percent of a product’s raw materials. It will be interesting to see how manufacturers handle this one, given that supply chain information, when available, can be seen as strategic and highly confidential. It is probably safe to say that this credit will drive many interesting conversations, including with raw materials suppliers traditionally spared the scrutiny afforded to the most controversial materials.\nHigh-profile problems in the food, electronics and apparel industries have made companies scramble to influence their complex supply chains. LEED v4 aspires to reward companies who have these conversations with suppliers before there is a problem.\n4. Hazard-based ingredients reporting\nThe void created by the absence of meaningful federal chemical regulatory policy once again is being filled by NGOs and leading businesses. LEED v4 marks USGBC’s first major move into addressing the potential toxicity of building product ingredients.\nProducts can contribute to one point by declaring all ingredients more than 0.1 percent by weight, and another point if companies can prove that they are avoiding some of the most hazardous chemicals as determined by several governmental lists.\nIndustry associations are up in arms about this approach, which makes judgments based on the inherent toxicity of ingredients rather than any proven risk of chemical exposure based on how they are used in products. Unfortunately, the current reality is that government and industry experts on risk assessment have not adequately addressed rising public demand for credible data on the human health impacts of common chemicals and ingredients, especially outside of food and cosmetics.\nIn this void, NGOs, corporations such as Google, and dozens of architecture firms have banded together to create the Health Product Declaration, a hazard-based standard format for reporting ingredients and health warnings, which is recognized by LEED v4. In what may be a case of too little, too late, one industry association has just responded by putting out its own format, the Product Transparency Declaration, which curtails the reporting of chemical hazard warnings in favor of providing basic information on exposure risks to users.\nIt is too early to say, but the new LEED credit, combined with Wal-Mart’s recent announcement on chemical disclosure and avoidance, could mark a turning point in making hazard-based chemical assessment, rather than the more complex risk-based assessment, the de facto method for weighing the “healthiness” of product ingredients.\nThe new normal\nDo you remember when it was acceptable to only report greenhouse gas emissions you produced directly through your fuel or electricity use? Or when you could claim you weren’t responsible for the actions of your second- and third-tier suppliers? Or when it was possible to distinguish a product as “healthy” simply by declaring what substances weren’t in it?\nThe days of hiding behind cop-outs such as “Scope 3 emissions,” “independent suppliers” and “BPA-free” are ending. The new credits in LEED v4 are just the latest sign that yesterday’s brand of corporate responsibility is no longer sufficient in the age of transparency and global supply chain accountability.\nFiguring out how to best implement more holistic sustainability reporting is a work in progress, but we see life cycle assessment, chemical hazard assessment and supply chain transparency as part of the new normal for manufacturers in every sector.\nThe final question for companies everywhere remains: Will you contribute proactively to making new tools such as LEED v4 work for business, or will you simply react as these trends move into your sector?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e7b7c726-c8e0-432f-8c6f-463e391925b9>","<urn:uuid:5cbb918b-fa8a-4eb8-808b-7acf37a46d71>"],"error":null}
{"question":"How do logarithms help with calculations historically vs their modern applications?","answer":"Historically, logarithms were primarily used to simplify multiplication and division by converting them into addition and subtraction operations. This was especially important before calculators when people only had slide rules. In modern times, logarithms have diverse applications across multiple fields. They are used in economics and banking to measure deposit growth, in biology for statistical analysis of genetic inheritance, in geology to calculate earthquake intensity, and in chemistry for pH calculations. Currently, less obvious applications include logarithmic scales and being the inverse of exponential functions.","context":["This is the first in my series of posts that describes lesser known tricks that I use to teach specific mathematical topics. The content about which I’ll be blogging will range from elementary school level through calculus; hopefully, there is a little something for everybody. When the muse strikes, I might even embellish the more elementary topics by giving a little extra insight from the advanced point of view. So, without further ado…\nTricks of the Trade\n(with Professor Glesser)\nIt must be strange to teach logarithms to students now if you were teaching in 1960’s or 1970’s. For then, there was an application you could immediately point to: logarithms make multiplication and division easier by turning them into addition. As students didn’t own calculators, only slide rules, as long as they bought into the usefulness of multiplication, you were solid. However, today, we have to rely on less obvious applications (e.g., logarithmic scales, being the inverse of the exponential function, being an antiderivative for ). One of the drawbacks is that the students still need to learn the basic rules of logarithms but without the immediate motivation. [Update: A very nice worksheet to help this initial process in given by Kate Nowak here.]\n(When I use the symbol it refers to any base.)\nOf course, these rules are why logarithms were so important for computational purposes. But, without getting a chance to use these rules a few hundred times, I’ve noticed students have difficulty remembering them. One technique I’ve used in the past with success is to tie it to something they do know.\nNearly all students are familiar with this acronym and many know mnemonics for remembering it (e.g., Please Excuse My Dear Aunt Sally). In case you don’t, the letters stand for Parentheses, Exponents, Multiplication, Division, Addition, Subtraction and the order refers to the order in which you should evaluate an expression. However, it is also a keen way of recalling the logarithm rules. First, I always encourage my students to write PEMDAS as follows:\nas it helps reinforce the idea that multiplication and division as well as addition and subtraction are related operations. Next, to get the rules for logarithms, simply draw down arrows as follows:\nThe arrows tell you that exponentiation turns into multiplication (rule 1 above), multiplication turns into addition (rule 2) and division turns into subtraction (rule 3). Invariably, a student will say, “but my problem is remembering which way to write it.” They mean, for instance, which of the following is the correct rule 2?\nThis is where the P comes in (note that we didn’t use it above). The P, which still stands for parentheses, tells you that you what you are changing goes in the parentheses. So, if you are turning multiplication into addition, the multiplication is what goes in the parentheses. Occasionally, students still have problems remembering and I tell them to look at what is closer to the P. In the diagram, since M is closer than A to P, the multiplication goes inside the parentheses.\nIn the next edition of Tricks of the Trade, I’ll discuss adding and subtracting fractions…in the quotient field of an integral domain.","Math is all around us in our daily lives, from grocery shopping, for gas, use a GPS or place a rack on the wall. In 2013, the renowned mathematician and writer of popular books, Ian Stewart he published a book entitled “17 equations that changed the world”. In this article, we explain some of them and why they have been so important for the history of mankind.\n1. The Pythagorean Theorem\nThis famous equation is the basis of geometry as we know it. Dating from 530 BC and describes the relationship between the sides of a right triangle on a flat surface, stating that the sum of the square of the length of the short sides b ( “Hicks”) is equal to the square of the length of (“hypotenuse”) longest side c.\nThis theorem is particularly useful in the field of construction, but also in navigation to triangulate positions in police investigations to calculate the trajectory of a bullet or determine how close was the shooter or to find the location of a mobile phone triangulation. The GPS we use today could not function without this theorem.\nLogarithms, presented by John Napier in 1610, they are the inverse (opposite) of the exponential functions. A logarithm of a given base is the power that we have to raise the base to get a number.The equation log (ab) = log (a) + log (b) is one of the most useful applications of logarithms, converting multiplication sum. Until the development of digital computers and computing, this was the method most common to quickly multiply numbers together large and considerably accelerated the calculations in fields such as physics, astronomy and engineering.\nIt is currently used in numerous fields such as the economy and banking, where it is used, for example, to measure the growth of deposits in time; on the advertising, to compile statistics of the advertising campaign; in biology, for statistical analysis of how genetic inherit a child of his parents; in geology, to calculate the intensity of earthquakes; in chemistry, for calculating the pH, etc.\n3. The law of gravity\nThis law of classical physics, also called law of universal gravitation, was enunciated by Isaac Newton in 1687 and describes the mutual attractive force F experienced two bodies according to their respective masses m 1 and m 2, the distance separates r and gravitational constant G. Under this law, the greater the mass of bodies and the closer they are to each other, more strongly they attract. This law explains almost perfectly the movement of the planets and is completely universal, ie not only it works on Earth but throughout the universe. In our daily life, explains many things, including why the earth revolves around the sun, why a pendulum swinging movement can keep your forever, because if we launch any object always ends up falling down and why its speed drop increases as it approaches the Earth, etc. the law of gravity Newton remained in force until it was replaced by the theory of general relativity Albert Einstein in the early twentieth century.\nYou may also like to read another article on WeiWeiCS: Methods to Improve Memory and Recall\n4. The wave equation d’Alembert\nThe wave equation or d’Alembert equation is a differential equation developed by the French mathematician and philosopher Jean le Rond d’Alembert in 1746. This equation is used to describe the behavior or change shape over time a wide wave range, including water waves, sound and light. For example, a guitar string vibrates, the ripples in a pond after throwing a stone or the light emitted by an incandescent bulb. Hence it is so important in fields such as acoustics, electromagnetism or fluid dynamics, among others. In addition, techniques developed to solve this equation helped others understand differential equations.\n5. Theorem of polyhedrons of Euler\nThe mathematician Leonhard Euler in 1751 published his “Theorem of polyhedrons,” which includes a formula for amazing results. But what is a polyhedron? Because it is a three-dimensional version of a polygon. For example, a cube or a polyhedron would dimensional version of the square polygon. The corners of a polyhedron are called “vertices”, the lines connecting the vertices together are called “edges” and the remaining surfaces between them are the “faces” of the polyhedron. Euler’s genius is to have found a formula valid for all polyhedra and allows us to know if the polyhedron is well constructed. According to Euler, the sum of the vertices (V) and the faces (C) of a polyhedron least its edges (A) must always be equal to two. For example, a cube has eight corners, 6 faces and 12 edges. If we apply Euler’s formula, V – E + F = 2, we see that 8 to 12 + 6 = 2.\nChoose any other polyhedron and try to apply the formula. The result will always be 2!\nThis observation Euler was one of the first examples of what we now call “topological invariant”: a number or property sharing a class of similar shapes to each other; and paved the way for the development of topology, a branch of the essential to modern physics and mathematics 3D mapping.\n6. The normal distribution\nThe normal distribution, also called Gaussian distribution or bell curve describes the behavior of certain properties or independent processes in large groups of people or things. The importance of this distribution is that constantly appears in nature and attitude of the people, allowing modeling many natural, social and psychological phenomena. Its use is very common in fields such as physics, biology, psychology or social sciences, among others.\nFor example, if we look up to everyone in a sample group will see that most people will be around average height and that as we move away from the average height above or below the number of people decreases, leading to the graphic typical bell-shaped developed by Carl Friedrich Gauss in 1810. the same will happen if we analyze your IQ. This distribution is also a fundamental tool in the laboratory, to see the percentage of effectiveness that has a drug in clinical trials.\n7. Maxwell’s equations\nAre four differential equations describing the behavior of the electric (E) and magnetic (H) and how they relate to each other. These equations, published by James Clerk Maxwell in 1865 are the basis for explaining how electromagnetism works on a daily basis. In everyday life, we explain to them how information from television, Internet and transmitted mobile phones, how long it takes to reach Earth the starlight or how neurons work. However, today it is known that these equations provide only an approximation to electromagnetism works well in human scale but that is not accurate, hence modern physics has replaced Maxwell’s equations by a quantum mechanical explanation.\n8. The 2nd law of thermodynamics\nUnder this law, formulated by Boltzmann in 1874, in a closed system, entropy (S) is always constant or increasing. But what is the thermodynamic entropy? Generally speaking, entropy determines the amount of disorder in a system there. For example, a system of irregular and orderly like a hot region next to a region cold- always tend to equalize, the heat flow from the hot zone to the cold zone, reaching a uniform distribution state. Unlike most of which are usually reversible physical processes the second law of thermodynamics is irreversible, it only works in this direction and time dependent. Thus, if we prepare an iced coffee, always ice cubes will melt and never coffee freeze.\nYou may also like to read another article on WeiWeiCS: Learning to Study Better Results\n9. The theory of relativity\nAlbert Einstein changed the course of physics in stating his theories of special and general relativity in the early twentieth century. The famous equation E = mc2 implies that mass and energy are equivalent. Special relativity, published in 1905, taught us that the speed of light is a universal speed limit and that the passage of time is different for people who They are moving at different speeds. While the general theory of relativity or general relativity, published in 1915, is a new theory of gravitation replaced Newton’s Law and which describes gravity as a curvature and folding space and time for themselves. General relativity is essential to understanding the origins, structure and ultimate fate of the universe.\n10. The Schrödinger equation\nThis equation, published by the Austrian physicist Erwin Schrödinger in 1927, governs the behavior of atoms and subatomic particles and is the basis of quantum mechanics. It is one of the most successful scientific theories of history, along with the general theory of relativity, since all experimental observations to date agrees well with predictions. In everyday life, quantum mechanics is present in the most modern technologies, such as nuclear power, semiconductor-based computers or Laser.\n11. Information Theory\nThis is the equation of the information entropy, Shannon published in 1949. As thermodynamic entropy, this is a measure of disorder, but applied in this case, the information contained in a message, either a book, a picture or anything else that can be represented symbolically. data compression is applied in different fields of information, including. Compressing a message may be the case that there is any loss of content and greater compression, the more likely that losses. In this sense, Shannon entropy indicates the limit compression of a message, that is, to what extent can compress without losing some of its contents. This theory marked the beginning of the mathematical study of the information and results are fundamental today for communication over networks.\n12. Chaos theory\nSurely you’ve ever heard that a butterfly flapping its wings in one continent can cause a hurricane in another continent. Well, that is basically chaos theory, the idea that a small event or change in initial conditions can cause a chain reaction that results in a completely different result. The Australian mathematician and biologist Robert May developed his equation in 1976 while he is studying the evolution of animal populations. The equation describes a process that evolves over time, where xt would be the amount of studied factor x (in the case of May, the number of children of each individual in the population) at the present time t, K is a constant chosen and xt + 1 would be the evolution of the value x with the passage of time. For certain values of K , the logistics map of May shows a chaotic behavior. According to this theory, if we start at a specific initial value of x, the process will evolve in a way, but if we start from a different initial value, even if it is extremely close to the first value, the process will evolve in a completely different way.\nThis highly sensitive to initial conditions chaotic behavior is present in many aspects of our daily lives. A very clear example is the weather, where a small change in atmospheric conditions a day can lead to completely different weather systems within days. Today is especially common use in the field of economics, medicine and meteorology."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:f22abe03-860c-46cc-bfc1-27034710d7de>","<urn:uuid:58c08db5-7bb7-46bd-a4c9-bdf727b4f919>"],"error":null}
{"question":"What is the duration temporelle des essais cliniques pour le ribociclib versus l'étude CONTRADYS?","answer":"The ribociclib trial had an estimated study completion time of January 2020, with an expected average participation length of 1 year per patient. The CONTRADYS study had a total duration of 2 years and 9 months, including 2 years and 6 months for enrollment, with a maximum patient participation duration of 2 months.","context":["Early-Phase Study to Assess Inhibitor Ribociclib in Patients With Recurrent Glioblastoma or Anaplastic Glioma\n|The safety and scientific validity of this study is the responsibility of the study sponsor and investigators. Listing a study does not mean it has been evaluated by the U.S. Federal Government. Read our disclaimer for details.|\n|ClinicalTrials.gov Identifier: NCT02345824|\nRecruitment Status : Unknown\nVerified June 2018 by Camilo E. Fadul, MD, University of Virginia.\nRecruitment status was: Active, not recruiting\nFirst Posted : January 26, 2015\nLast Update Posted : June 7, 2018\n|Condition or disease||Intervention/treatment||Phase|\n|Glioblastoma Glioma||Drug: Ribociclib||Phase 1|\nPreliminary evaluation of efficacy and determination of the ribociclib safety profile in patients with brain tumors will be studied. All patients will be treated with ribociclib (600 mg/day) for 8-21 days before surgical resection of the recurrent tumor. Rb status of the recurrent tumor will be determined by immunohistochemistry within 2 weeks after surgery. Patients with Rb-positive tumors will continue treatment with ribociclib (21 days on, 7 days off) after surgery. Patients will be treated until unacceptable toxicity is observed, or until disease progression as assessed by radiographic or clinical metrics. We will determine whether molecular markers associated with changes induced by ribociclib treatment in tumors (matching initial vs. recurrent tumors) correlate with progression-free survival.\nApproximately 20% of patients with recurrent high-grade glioma will undergo surgical resection of their tumor typically at the time of first recurrence. These patients will be eligible for this study. An extra 10 mL of blood will be collected during a routine clinical procedure prior to initiation of ribociclib treatment (i.e., baseline blood sample), separated into plasma and buffy coat fractions, and frozen for pharmacokinetic (PK) analysis. Patients will be treated with ribociclib for 8-21 days prior to surgery to identify drug effects on tumor cells, and to determine drug PK. The last presurgical dose of ribociclib will be administered on the day of surgery at 4-8 hours prior to surgery to allow sufficient time for the drug to enter tumor cells, inhibit CDK4/6, and modulate downstream effectors. Blood will be acquired within 1 hour before surgery (as close to the time of surgery as possible). Blood will be separated into plasma and buffy coat fractions, and frozen. During surgery, samples of brain tumor core and infiltrating brain tumor will be acquired. Tissue samples will be frozen or fixed in paraffin embedded blocks and histology for PK and molecular analyses.\n|Study Type :||Interventional (Clinical Trial)|\n|Actual Enrollment :||3 participants|\n|Intervention Model:||Single Group Assignment|\n|Masking:||None (Open Label)|\n|Official Title:||Early-Phase Study to Assess Tumor Pharmacokinetics and Efficacy of the CDK4/6 Inhibitor Ribociclib (LEE011) in Patients With Recurrent Glioblastoma or Anaplastic Glioma|\n|Study Start Date :||March 2016|\n|Estimated Primary Completion Date :||September 2019|\n|Estimated Study Completion Date :||January 2020|\nExperimental: Ribociclib (LEE011) Treatment\nPatients will be treated with ribociclib (LEE011) (recommended phase 2 dose of 600 mg/day) for 8-21 days prior to surgery. For preliminary evaluation of efficacy and toxicity, patients with Rb-positive tumors will resume treatment with ribociclib at 14-28 days post-surgery on a schedule of 21 days on, 7 days off in a 28-day cycle. Patients will be treated until unacceptable toxicity is observed, or until disease progression as assessed by radiographic or clinical metrics.\nOther Name: LEE011\n- Inhibition of CDK4/CDK6 Signaling Pathway in Cell Proliferation [ Time Frame: an expected average of 1 year to allow laboratory quality assessment of tumor proliferation ]Levels of phospho-Rb in tumor cells, the fraction of Ki67-positive tumor cells, and the fraction of TUNEL-positive tumor cells in primary (baseline) vs. recurrent (post-LEE011) tumor samples will be assessed via laboratory practices as a means of studying Inhibition of CDK4/CDK6 Signaling Pathway in Cell Proliferation.\n- Ribociclib Concentration in Tissues [ Time Frame: 1 Day Collection as the time of initial surgery ]\nConcentrations of ribociclib will be gathered from tumor core and infiltrating tumor tissue which is gathered at the time of initial surgery for the removal of the patients tumor.\nThis will be completed to assess whether ribociclib treatment induces changes in brain tumor cell fate (proliferation, apoptosis, senescence) and E2F activation. Biomarkers will be compared in ribociclib-treated recurrent tumors vs. matching baseline tumors vs. archived recurrent tumors from other patients.\nLaboratory will assess the frequencies of Rb mutations and loss in primary and recurrent brain tumors.\n- Ribociclib Concentration in Plasma [ Time Frame: time of Pharmacokinetic draws ]\nConcentrations of ribociclib in blood plasma will be gathered through pharmacokinetic draws at specified time points throughout study participation and will be used to assist in the assessment of whether ribociclib treatment induces changes in brain tumor cell fate (proliferation, apoptosis, senescence) and E2F activation. Biomarkers will be compared in ribociclib-treated recurrent tumors vs. matching baseline tumors vs. archived recurrent tumors from other patients.\nTo determine the frequencies of Rb mutations and loss in primary and recurrent brain tumors.\n- Tumor Progression [ Time Frame: Over 1 year, which is expected average length of participation per patient ]Preliminary rates of progression-free survival in patients with high-grade gliomas treated with ribociclib will be measured through radiographic and clinical response metrics, specifically Response Assessment in Neuro-Oncology (RANO) criteria and investigator discretion.\n- Survival Outcomes [ Time Frame: Over 1 year, which is expected average length of participation per patient ]Overall survival in patients with high-grade gliomas treated with ribociclib will be assessed by medical record review and survival follow up.\n- Ribociclib Safety Profile [ Time Frame: 30 days post last dose of LEE011 per patient ]Common Toxicity Criteria Adverse Event (CTCAE 4.0) will be utilized to review ribociclib treatment effects in patients with brain tumors.\n- Identifying Treatment Induced Changes in Oncogenic Pathways [ Time Frame: Over 1 year, which is expected average length of participation per patient ]To identify treatment-induced changes in compensatory oncogenic pathways that may promote drug resistance, through the use of reverse-phase protein array analysis of tumor samples.\n- Ribociclib Concentrations in Cerebrospinal Fluid (CSF) [ Time Frame: 1 Day Collection as the time of initial surgery ]Cerebrospinal Fluid (CSF) obtained at the time of surgery and processed to assess the amount of ribociclib concentrations\nTo learn more about this study, you or your doctor may contact the study research staff using the contact information provided by the sponsor.\nPlease refer to this study by its ClinicalTrials.gov identifier (NCT number): NCT02345824\n|United States, Virginia|\n|University of Virginia Health System|\n|Charlottesville, Virginia, United States, 22908|\n|Principal Investigator:||Camilo Fadul, MD||University of Virginia|","New Prophylactic Maneuver: the \"Pushing\" Maneuver, Aiming to Reduce the Risk for Shoulder Dystocia (CONTRADYS)\n|Shoulder Dystocia, Brachial Plexus Injury Asphyxia, Hematoma, Clavicle Fracture, Humerus Fracture.||Procedure: suctioning of fetal nose and mouth during delivery Procedure: Pushing maneuver|\n|Study Design:||Allocation: Randomized\nIntervention Model: Parallel Assignment\nMasking: Single (Participant)\nPrimary Purpose: Prevention\n|Official Title:||CONTRADYS : A Randomized Controlled Trial of a New Prophylactic Maneuver, the \"Pushing\" Maneuver, Aiming to Reduce the Risk for Shoulder Dystocia|\n- occurrence of shoulder dystocia [ Time Frame: during delivery ]shoulder dystocia is defined as a necessity of requiring a specific obstetrical maneuver (McRoberts' maneuver).\n- complications [ Time Frame: 5 days after delivery ]neonatal complications including neurological damages (brachial plexus injury), generalized asphyxia, hematoma, clavicle and humerus fractures.\n|Study Start Date:||March 2011|\n|Study Completion Date:||March 2014|\n|Primary Completion Date:||November 2013 (Final data collection date for primary outcome measure)|\nNormal delivery without \"pushing\" maneuver suctioning of fetal nose and mouth during delivery\nProcedure: suctioning of fetal nose and mouth during delivery\nNormal delivery without \"pushing\" maneuver: either an expectative attitude or a suctioning of fetal nose and mouth during delivery, since the crowning of the head (appearance of the fetal scalp at the introitus between pushes).\nOther Name: M:Mouchage meaning suctioning of fetal nose and mouth\nExperimental: group C\n\"Pushing\" maneuver on the fetal head\nProcedure: Pushing maneuver\nThe \"pushing\" maneuver is performed gently on the fetal head since the crowning of the head (appearance of the fetal scalp at the introitus between pushes), during one uterine contraction, aiming to facilitate the anterior shoulder to slip off behind the symphysis pubis, reducing thus the risk of shoulder dystocia.\nOther Name: C ;\"contre pulsion\" in French, meaning \"Pushing maneuver\"\nHypothesis: the \"pushing\" maneuver reduces of 50% the risk of shoulder dystocia in comparison with either an expectative attitude or a suctioning of fetal nose and mouth.\nMain objective: to assess whether prophylactic use of the \"pushing on the fetal head\" maneuver decreases the prevalence of shoulder dystocia, in comparison with an expectative attitude or a suctioning of fetal nose and mouth.\nSecondary objective: to compare the occurrence of neonatal complications including brachial plexus injury, clavicle and humerus fracture, hematoma and generalized asphyxia.\nMain criterion: occurrence of shoulder dystocia, defined by a necessity of requiring a specific obstetrical maneuver (McRoberts' maneuver).\nSecondary criterion: neonatal complications including neurological damages (brachial plexus injury), generalized asphyxia, hematoma, clavicle and humerus fractures.\nMethods: prospective, randomized, multicenter blind study with a modified intention-to-treat analysis. Patients are included during the last obstetrical consultation and randomized in the delivery room.\nNumber of patients (α error, β error): a sample size of 1126 patients was calculated to allow detection of a 50% reduction of shoulder dystocia, with a prevalence of dystocia reaching 4.3% in usual deliveries (expectative attitude or suctioning of fetal nose and mouth), with a 65% dystocia risk decrease in the group C (α error of 0.05, β error of 0.20).\nInclusion and exclusion criteria. Inclusion: women having completed 37 or more gestational weeks with singleton vertex fetus, delivering vaginally. Exclusion: patients with caesarean section are excluded.\nPlace of the study: department of gynecology and obstetrics, BEAUJON hospital, Clichy, France and department of gynecology and obstetrics, BICHAT hospital, Paris, France.\nDuration of inclusion: two years and 6 months Duration of patients' participation: two months maximum Duration of the study: two years and 9 months. Mean number of inclusion each month: 30 Number of investigation centre: 2 (BEAUJON hospital, BICHAT hospital).\nPlease refer to this study by its ClinicalTrials.gov identifier: NCT01297439\n|Clichy, France, 92110|\n|Principal Investigator:||Olivier Poujade, MD||Assistance Publique - Hôpitaux de Paris|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:88177a9e-426a-4e99-acac-a2b13c8285be>","<urn:uuid:78a948fb-5b1c-4a3b-8e81-d130fa595d67>"],"error":null}
{"question":"What are the characteristics and timing of the Anthelion meteors in March versus the Southern Taurids in October?","answer":"The Anthelion meteors in March are visible from western Virgo, with their radiant best placed near 0200 local time, producing slow velocity meteors at 30 km/sec with rates of 2 per hour. The Southern Taurids in October are visible from southern Aries, also best viewed around 0100 local time, with similar slow velocity at 29 km/sec but higher rates of 3 per hour regardless of location.","context":["During this period the moon will wane from nearly one-half illuminated to almost new (invisible). This weekend the nearly half illuminated moon will rise near 02:00 local standard time and will interfere with meteor viewing if it lies within your field of view. With each passing night the moon will become a bit thinner and will rise later in the morning providing the meteor observer with a better view of the heavens. The estimated total hourly meteor rates for evening observers this week is 3 as seen from mid-northern latitude (45N) and 5 from the southern tropics (25S). For morning observers the estimated total hourly rates should be near 6 as seen from mid-northern latitudes (45N) and 9 from the southern tropics (25S). The actual rates will also depend on factors such as personal light and motion perception, local weather conditions, alertness and experience in watching meteor activity. Evening rates are reduced by moonlight during this period. Note that the hourly rates listed below are estimates as viewed from dark sky sites away from urban light sources. Observers viewing from urban areas will see less activity as only the brighter meteors will be visible from such locations.\nThe radiant (the area of the sky where meteors appear to shoot from) positions and rates listed below are exact for Saturday night/Sunday morning March 10/11. These positions do not change greatly day to day so the listed coordinates may be used during this entire period. Most star atlases (available at science stores and planetariums) will provide maps with grid lines of the celestial coordinates so that you may find out exactly where these positions are located in the sky. A planisphere or computer planetarium program is also useful in showing the sky at any time of night on any date of the year. Activity from each radiant is best seen when it is positioned highest in the sky, either due north or south along the meridian, depending on your latitude. It must be remembered that meteor activity is rarely seen at the radiant position. Rather they shoot outwards from the radiant so it is best to center your field of view so that the radiant lies near the edge and not the center. Viewing there will allow you to easily trace the path of each meteor back to the radiant (if it is a shower member) or in another direction if it is a sporadic. Meteor activity is not seen from radiants that are located far below the horizon. The positions below are listed in a west to east manner in order of right ascension (celestial longitude). The positions listed first are located further west therefore are accessible earlier in the night while those listed further down the list rise later in the night.\nThese sources of meteoric activity are expected to be active this week.\nThe center of the large Anthelion (ANT) radiant is currently located at 12:12 (183) -01. This position lies in western Virgo, 2 degrees west of the 4th magnitude star known as Zaniah (eta Virginis). Due to the large size of this radiant, Anthelion activity may also appear from eastern Leo, and Crater as well as Virgo. This radiant is best placed near 0200 local daylight saving time (DST), when it lies on the meridian and is located highest in the sky. Rates at this time should be near 2 per hour no matter your location. With an entry velocity of 30 km/sec., the average Anthelion meteor would be of slow velocity.\nThe xi Herculids (XHE) were discovered by Sirko Molau and Javor Kac of the International Meteor Organization using video data from the IMO network. These meteors are active from March 8-12 with maximum activity occurring on the 10th. The current radiant position lies near 17:12 (258) +48, which lies in northern Hercules, 5 degrees southwest of the 3rd magnitude star known as Rastaban (beta Draconis). Rates are expected to be less than 1 per hour no matter your location. These meteors are best seen during the last dark hour of the morning when the radiant lies highest above the horizon in a dark sky. At 35 km/sec. this source would produce meteors of average velocities.\nAs seen from the mid-northern hemisphere (45N) one would expect to see approximately 4 sporadic meteors per hour during the last hour before dawn as seen from rural observing sites. Evening rates would be near 2 per hour. As seen from the tropical southern latitudes (25S), morning rates would be near 7 per hour as seen from rural observing sites and 4 per hour during the evening hours. Morning rates are reduced due to moonlight. Locations between these two extremes would see activity between the listed figures.\n|SHOWER||DATE OF MAXIMUM ACTIVITY||CELESTIAL POSITION||ENTRY VELOCITY||CULMINATION||HOURLY RATE||CLASS|\n|RA (RA in Deg.) DEC||Km/Sec||Local Daylight Saving Time||North-South|\n|Anthelion (ANT)||–||11:16 (169) +04||30||02:00||2 – 2||II|\n|xi Herculids (XHE)||Mar 10||17:12 (258) +48||35||07:00||<1 – <1||IV|","During this period the moon reaches its first quarter phase on Tuesday October 20th. On that date the moon will be located 90 degrees east of the sun and will set near 2300 (11pm) local daylight saving time (LDT) as seen from mid-northern latitudes. This weekend the waxing crescent moon will set during the mid-evening hours allowing perfect viewing the more active morning hours. The estimated total hourly meteor rates for evening observers this week is near 4 as seen from mid-northern latitudes (45N) and 3 as seen from tropical southern locations (25S). For morning observers the estimated total hourly rates should be near 30 as seen from mid-northern latitudes (45N) and 20 as seen from tropical southern locations (25S). The actual rates will also depend on factors such as personal light and motion perception, local weather conditions, alertness and experience in watching meteor activity. Rates during the evening hours are reduced during this period due to moonlight. Note that the hourly rates listed below are estimates as viewed from dark sky sites away from urban light sources. Observers viewing from urban areas will see less activity as only the brightest meteors will be visible from such locations.\nThe radiant (the area of the sky where meteors appear to shoot from) positions and rates listed below are exact for Saturday night/Sunday morning October 17/18. These positions do not change greatly day to day so the listed coordinates may be used during this entire period. Most star atlases (available at science stores and planetariums) will provide maps with grid lines of the celestial coordinates so that you may find out exactly where these positions are located in the sky. A planisphere or computer planetarium program is also useful in showing the sky at any time of night on any date of the year. Activity from each radiant is best seen when it is positioned highest in the sky, either due north or south along the meridian, depending on your latitude. It must be remembered that meteor activity is rarely seen at the radiant position. Rather they shoot outwards from the radiant so it is best to center your field of view so that the radiant lies at the edge and not the center. Viewing there will allow you to easily trace the path of each meteor back to the radiant (if it is a shower member) or in another direction if it is a sporadic. Meteor activity is not seen from radiants that are located below the horizon. The positions below are listed in a west to east manner in order of right ascension (celestial longitude). The positions listed first are located further west therefore are accessible earlier in the night while those listed further down the list rise later in the night.\nThese sources of meteoric activity are expected to be active this week.\nThe Gamma Piscids (GPS) were first discovered by A. K. Terentjeva in a study of fireball sources. This is a weak shower active from October 14 through the 21st with maximum activity occurring on the 17th. At maximum the radiant is located at 01:10 (017) +17. This position lies in central Pisces, 5 degrees northwest of the faint star known as Eta Piscium. Rates of less than 1 per hour are expected, even at maximum. With an entry velocity of 21 km/sec., the average Gamma Piscid meteor would be of slow velocity.\nThe Southern Taurids (STA) are currently active from a radiant located at 02:36 (039) +12. This position lies in southern Aries, 3 degrees northwest of the 4th magnitude star known as Mu Ceti. These meteors may be seen all night long but the radiant is best placed near 0100 local daylight time (LDT) when it lies on the meridian and is located highest in the sky. Rates at this time should be near 3 per hour regardless of your location. With an entry velocity of 29 km/sec., the average Southern Taurid meteor would be of slow velocity.\nThe Eta Taurids (ETT) were discovered by Sirko Molau and Juergen Rendtel using data from the IMO video network. These meteors are active from Oct 15-29 with maximum occurring on the 24th. The radiant is currently located at 03:20 (050) +22. This area of the sky is located in eastern Aries, 5 degrees southwest of the bright open cluster known as the Pleiades. Current rates should be near 1 per hour as seen from the northern hemisphere and less than 1 as seen from south of the equator. With an entry velocity of 45 km/sec., most activity from this radiant would be of medium-swift speed.\nThe Orionids (ORI) reach maximum activity on Wednesday night/Thursday morning October 21/22. Unlike most major showers the Orionids offer a broad maximum with similar rates on the nights prior and after maximum. This weekend the radiant lies at 06:12 (093) +16, which places it in northeastern Orion, 5 degrees west of the 2nd magnitude star known as Alhena (Gamma Geminorum). This area of the sky is best placed near 0500 LDT, when it lies highest above the horizon. Rates this weekend would be near 10 per hour no matter your location. Rates should increase to 15-20 per hour at maximum With an entry velocity of 67 km/sec., most activity from this radiant would be of swift speed.\nThe Epsilon Geminids (EGE) are active from a radiant located at 06:48 (102) +27, which places it in northwestern Gemini, 3 degrees northeast of the 3rd magnitude star known as Mebsuta (Epsilon Geminorum). This is fairly close to the Orionid radiant so care should be taken to separate these two sources. The best way to do this is to include both radiants within your field of view so that meteors can be easily traced back to their source. This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be near 2 per hour as seen from the northern hemisphere and 1 when view south of the equator. This should increase to near 3 per hour at maximum (Oct 22). With an entry velocity of 70 km/sec., most activity from this radiant would be of swift speed.\nThe October Lyncids (OLY) were discovered by Peter Jenniskens and verified by IMO video data. This weak shower is active from October 16-24 with no distinct night of maximum activity. The date listed in the shower table is the midpoint of the activity curve. The radiant is currently located near 07:16 (109) +53. This position lies in a remote area of central Lynx, halfway between the stars Omicron Ursae Majoris and Menkalinan (Beta Aurigae). This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be less than 1 per hour no matter your location. With an entry velocity of 68 km/sec., most activity from this radiant would be of swift speed.\nThe Tau Cancrids (TCA) are active from October 9-25, with maximum activity occurring near the 17th. This shower was discovered by Peter Jenniskens and verified by IMO video data. The radiant currently located at 09:00 (135) +30. This position lies in northern Cancer, close to the 4th magnitude star known as Iota Cancri. This area of the sky is best placed in the sky during the last hour before dawn, when it lies highest above the horizon in a dark sky. Current rates should be near 1 per hour as seen from the northern hemisphere and less than 1 as seen from south of the equator. With an entry velocity of 69 km/sec., most activity from this radiant would be of swift speed.\nThe Leonis Minorids (LMI) are active from October 17-27 with maximum activity occurring on October 22nd. This radiant is currently located at 10:24 (156) +38, which places it in northeastern Leo Minor, close to the position occupied by the fourth magnitude star Beta Leonis Minoris . The radiant is best placed just before dawn when it lies highest in a dark sky. This shower is better situated for observers situated in the northern hemisphere where the radiant rises far higher into the sky before the start of morning twilight. Rates at maximum should be near 4 per hour for those in the northern hemisphere and 2 per hour as seen south of the equator. At 60km/sec., the average Leonis Minorid is swift.\nAs seen from the mid-northern hemisphere (45N) one would expect to see approximately 10 sporadic meteors per hour during the last hour before dawn as seen from rural observing sites. Evening rates would be near 2 per hour. As seen from the tropical southern latitudes (25S) morning rates would be near 7 per hour as seen from rural observing sites and 2 per hour during the evening hours. Locations between these two extremes would see activity between the listed figures.\nThe list below offers the information from above in tabular form. Rates and positions are exact for Saturday night/Sunday morning except where noted in the shower descriptions.\n|SHOWER||DATE OF MAXIMUM ACTIVITY||CELESTIAL POSITION||ENTRY VELOCITY||CULMINATION||HOURLY RATE||CLASS|\n|RA (RA in Deg.) DEC||Km/Sec||Local Daylight Saving Time||North-South|\n|Gamma Piscids (GPS)||Oct 17||01:10 (017) +17||21||01:00||<1 – <1||IV|\n|Southern Taurids (STA)||Oct 10||02:36 (039) +12||29||02:00||3 – 3||II|\n|Eta Taurids (ETT)||Oct 24||03:20 (050) +22||45||02:00||1 – <1||IV|\n|Orionids (ORI)||Oct 22||06:12 (093) +16||67||06:00||10 – 10||I|\n|Epsilon Geminids (EGE)||Oct 22||06:48 (102) +27||70||06:00||2 – 1||II|\n|Oct. Lyncids (OLY)||Oct 20?||07:16 (109) +53||68||08:00||<1 – <1||IV|\n|Tau Cancrids (TCA)||Oct 17||09:00 (135) +30||69||09:00||1 – <1||IV|\n|Leonis Minorids (LMI)||Oct 22||10:24 (156) +38||60||10:00||2 – 1||II|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:fe5ca226-4856-4832-81ad-f81817d00e55>","<urn:uuid:22c57e10-5532-45ee-8a94-d186257d5358>"],"error":null}
{"question":"¿Cuántos ciervos fueron etiquetados en la ciudad de Manitowoc en 2021? 🦌","answer":"In 2021, a total of 23 deer were tagged in Manitowoc, consisting of 6 bucks and 17 doe. There were also 37 crashes involving deer that year.","context":["Neshotah Beach in Two Rivers brings catamaran race and live music this weekend, plus more news in your weekly dose.\nHartman's Bakery fire fundraiser: Click here to donate to a GoFundMe campaign to raise $15,000 to help Hartman's Bakery and its staff recover after a July 27 fire at its Manitowoc shop. The bakery's three locations in Manitowoc, Two Rivers and Plymouth are closed until the Manitowoc shop can reopen and begin making products again.\nMANITOWOC - Neshotah Beach in Two Rivers will be the place to be this weekend.\nSaturday and Sunday brings the Catamaran Racing Association of Wisconsin's Trivers Trap Out Regatta and catamaran race.\nPeople will be able to view the sailboats from shore between 10:30 a.m. and 5 p.m. Saturday and 10 a.m. and 3 p.m. Sunday.\nAlso, Neshotah's Bands on the Beach concert series will bring Nettle Hill from 5 to 9 p.m. Saturday. Remaining dates in the concert series include Aug. 14, The Listening Party, 2-6 p.m.; Aug. 21, Breakwater Rocks, 2-6 p.m.; and Aug. 27, Running Blind, 5-9 p.m.\nNeshotah Beach is at 500 Zlatnik Drive, Two Rivers.\nTwo Rivers is also continuing its Central Park Concert Series this summer. The concerts are free and are in the evening at Central Park, 1700 Washington St. Remaining dates in the series include Aug. 11, Eddie Larsen Classic Memories; Aug. 18, Clipper City Chordsmen; and Aug. 25, Out of the Blue.\nThe city's Parks & Recreation Department is also coordinating Gone Fishing Free from 8 to 10:30 a.m. Aug. 13 at Shoto Conservation Club, 609 County Road V. The program is open to ages 3-8 and 9-14. Registration is required. More info or to register: 920-793-5592 or click here.\nWelcome to your weekly dose.\nStart your week here in weekly dose, a roundup of local community news. Here are some more stories to catch up on:\n► Two Rivers endowment fund has big local impact: With additional support from miscellaneous donors this year, the Helen Gajdys Reis & Lester Reis Endowment Fund has been able to contribute $2,500 each to the following Manitowoc County organizations: Hope House, InCourage, Lighthouse Recovery Community Center, Soles for Souls, The Haven and Wisconsin Maritime Museum.\nAlso, two bikes were donated to the Two Rivers Optimist Club for its annual bike giveaway in July.\nThe fund’s board of directors has also been instrumental in donating more than 1,000 new coats over the past six years to The Salvation Army’s Coats for Kids campaign.\n“In keeping with the philosophy of the Reis Endowment Fund to helping the people of our community, we thank all who have helped to make this possible,” said board member Howard Zimmerman in a news release.\nThose interested in donating to help the fund achieve its mission can send contributions to Reis Endowment Fund, P.O. Box 388, Two Rivers, WI 54241.\nThe Helen Gajdys Reis & Lester Reis Endowment Fund is a 501(c)(3) organization. Contributions are tax deductible.\nThe fund’s board members are Kathy Hanke, George Reis, Mary Reis, Andy Steimle, Gloria Theis and Zimmerman. Honorary members are Jerilyn Laurino and Abigail Spaeth.\n► Manitowoc Marine Band’s annual Cruiser Night and corn roast is Friday: Manitowoc Marine Band will present its annual Cruiser Night and corn roast starting at 6:30 p.m. Friday at Washington Park, 1115 Washington St., Manitowoc.\nAll are invited to park their four-wheeled cruiser and enjoy a night of upbeat music as the band performs popular rock hits from multiple decades and other toe-tapping specialties.\nHP Enterprises will be serving the corn roast, along with burgers, brats, popcorn and more.\nA&W will also be on hand with deep-fried food and root-beer floats.\nLocal artists and vendors will also be at the park selling their wares.\n► Open house at Lakeshore Technical College Aug. 10: Lakeshore Technical College is hosting a “Level Up with Lakeshore” open house at its Cleveland campus Aug. 10. The public is welcome to attend and tour the campus any time between 11 a.m. and 6 p.m.\nCollege representatives will be available to discuss career options, financial aid, student resources, credit for prior learning and application steps. Deans and faculty members will also be available to answer questions about new and continuing programs and courses. LTC’s esports arena will be available for open play.\nRegistration is requested, but not required. For details about the open house or to register, visit gotoltc.edu/open-house.\n► Buffalo Bill Wild West Show returns to Pinecrest Village Aug. 13-14: The Buffalo Bill Wild West Show will return to Manitowoc County Historical Society’s Pinecrest Historical Village from 11 a.m. until 3 p.m. Aug. 13-14.\nThose attending will spend the day amid Wild West legends like Buffalo Bill and Annie Oakley.\nThe main arena show will take place at 1 p.m. both days, with the thrill and excitement of fast horses, trick riding and fancy shooting.\nActivities will be held throughout the day, including a flea circus, horse parade through the grounds and a bank robbery.\nA full schedule can be found at https://www.manitowoccountyhistory.org/programs/buffalo-bill.\nOther activities throughout the museum grounds will include blacksmithing demonstrations, gold panning, the Annie Oakley Shooting Gallery and a chuck wagon cooking demonstration.\nGuests can tour the canvas boomtown camp and the entire village grounds, shop for period fashions, leatherwork and their own treasures from the Wild West.\nFood and beverages will be available for purchase from Brian’s Smokehouse.\nPinecrest is at 924 Pinecrest Road, Manitowoc, and admission is $10 for adults, $7 for ages 4-17, and free for members and those younger than 4.\n► Manitowoc bow hunting meetings Aug. 18: Manitowoc Police Department will hold two meetings on bow hunting in the city.\nThe meetings will be at noon and at 5:30 p.m. Aug. 18 in the lower-level meeting room at the department, 910 Jay St.\nBoth meetings will cover the same topics, including the purpose of the hunt, type of permissions needed to bow hunt in the city and rules set by the city to conduct a safe hunt.\nBow hunting and landowner applications can be obtained and submitted on the city’s website under the Police Department page, or at the front counter of the police department between 7:30 a.m. and 4:30 p.m. Mondays-Fridays.\nAll bow hunting and landowner applications need to be returned to the police department by Sept. 9. Applications received after that date may get a delay in approval.\nAll bow hunting applicants requesting to hunt private land must have a landowners’ application attached to their bow hunting application when turned in, as they will be denied until such paperwork is provided.\nFor more info, contact Lt. Mark Schroeder at 920-686-6551.\nThe deer population in Manitowoc causes damage and problems for property owners and drivers. Between 2007 and 2021, 448 deer have been tagged in the city. Here are the numbers by year, including the number of crashes involving deer:\n2021: 6 bucks, 17 doe, 37 crashes\n2020: 5 bucks, 21 doe, 37 crashes\n2019: 8 bucks, 24 doe, 25 crashes\n2018: 10 bucks, 22 doe, 55 crashes\n2017: 15 bucks, 30 doe, 53 crashes\n2016: 16 bucks, 27 doe, 58 crashes\n2015: 8 bucks, 19 doe, 50 crashes\n2014: 10 bucks, 12 doe, 59 crashes\n2013: 10 bucks, 27 doe, 53 crashes\n2012: 14 bucks, 17 doe, 45 crashes\n2011: 8 bucks, 31 doe, 36 crashes\n2010: 13 bucks, 28 doe, 46 crashes\n2009: 7 bucks, 24 doe, 45 crashes\n2008: 1 buck, 13 doe, 54 crashes\n2007: 1 buck, 4 doe, 37 crashes\nLast week's top headlines\nUpcoming movie filmed in Manitowoc and Sheboygan a twist-filled tale about a missing Oostburg girl\nAfter 'significant' fire damage, Hartman's Bakery in Manitowoc sees outpouring of support from donors, local businesses\nManitowoc visitor bureau faces future with low funding, hopes lawsuit will bring organization back on track\nCivil lawsuit alleges Manitowoc County Jail doctor showed 'deliberate indifference' in treating inmate in 2019\nBank First, of Manitowoc, has announced plans to acquire Fond du Lac-based Hometown Bank\nYour weather forecast\nMonday: High 83, low 61, a shower and thunderstorm\nTuesday: High 72, low 63, partly sunny and not as warm\nWednesday: High 86, low 69, warmer; humid in the p.m.\nThursday: High 81, low 65, humid with a t-storm in spots\nFriday: High 77, low 63, an afternoon thunderstorm\nSaturday: High 78, low 64, mostly sunny and humid\nSunday: High 80, low 66, mostly sunny and humid\nCourtesy of accuweather.com.\nThe Herald Times Reporter — part of USA TODAY NETWORK-Wisconsin — strives to make a difference in our community. Read our 2021 Community Impact Report.\nGet your dose — stay connected\nStart each week with your dose of local community news here. For updates during the week, come back to htrnews.com or follow us on Twitter, Facebook and Instagram.\nSend tips to email@example.com. See our contact page.\nThanks for reading!\nWe appreciate your readership! Support our work by subscribing.\nContact Brandon Reid at 920-686-2984 or firstname.lastname@example.org. Follow him on Twitter at @breidHTRNews.\nThis article originally appeared on Manitowoc Herald Times Reporter: Neshotah Beach in Two Rivers hosts catamaran race, regatta, live music"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7986c1d1-3798-475b-ad44-f2a2fe1fe1c2>"],"error":null}
{"question":"What are the benefits of automated license verification systems, and what security measures can protect the verified data?","answer":"Automated license verification systems offer several benefits including time and cost savings, improved accuracy, reduced regulatory risk, and enhanced visibility into workforce compliance. Organizations can instantly verify credentials, reduce manual errors, and track licenses in real-time. Regarding data security, the verified information can be protected through multiple measures including encryption, Identity and Access Management (IAM) systems requiring passwords, de-identification of personal data, and differential privacy which limits access to only the minimum required information needed for operations.","context":["License Verification Tool | NNP-BC – Neonatal Nurse Practitioner\nCompliance is an integral part of any workplace ? especially in the United States, where stringent regulations govern the process of verifying and maintaining the professional licenses and certifications of employees. Without verifying and monitoring staff credentials, organizations can be exposed to potential compliance risks, causing them significant liability.Fortunately, technology and automation has allowed the license verification process to be simplified. Primary source verification (PSV) is now used globally by large organizations with thousands of employees to ensure license compliance is met, and to reduce the time and money spent on manually verifying credentials. In this article, we will explore the benefits of automating the professional license verification process, how it works, and why it’s such an important part of staying in compliance. The Benefits of Automating License Verification with Primary Source VerificationVerifying credentials with PSV offers organizations a variety of important benefits. Here are a few of the advantages of automated license verification: Time and Cost SavingsOne of the main advantages of using automated PSV is the time it can save organizations. Gathering and verifying credentials can be both time-consuming and expensive. In contrast, an automated system can instantly verify credentials and help to reduce manual effort. This helps organizations to streamline processes and ensure HR teams can focus on other important activities. Improved AccuracyManually verifying credentials and other documents can lead to data entry errors and other inaccuracies. Human error can be time-consuming to fix and can result in organizations not meeting compliance standards. An automated PSV system, however, will reduce the potential for errors and can act as a safeguard against faulty data. Reduced Regulatory RiskOrganizations need to make sure their workforce has the necessary qualifications to meet their specific license requirements. Without an automated system, adhering to these regulations can be a challenge. In addition, failing to renew expired licenses and certificates, or incorrectly verifying them, can create legal ramifications. Automating license verification, however, can help to reduce the potential for these kinds of situations from happening.Enhanced VisibilityAccurately tracking employee certifications and other credentials can also be difficult without a centralized system. Automated license verification offers organizations full visibility into their workforce compliance program. This is especially useful for larger organizations with multiple teams and sub-teams, as it allows the HR team to get a clear picture of their overall compliance status. How Does Automated License Verification with Primary Source Verification Work?Using an automated license verification system with PSV provides organizations with the ability to validate and monitor the status of their occupational licenses and certifications. The system will track licenses in real-time and use pre-built workflows to ensure accuracy. It also leverages primary source verification for added security and reliability. The system begins by collecting the credentials of each employee, including their licenses, certifications, and qualifications. It verifies vital information, such as expiration dates, renewals, and any potential sanctions or disciplinary actions. It will also track and manage required document submissions, such as license applications and renewal forms. Each employee’s credentials are then stored in a system of record for immediate access. Automated alerts and notifications can also be sent when these documents approach their expiration dates. In addition, organizations can apply for renewal applications and other documents through the system, allowing them to easily stay ahead of their workforce compliance requirements. In summaryAutomating license verification with PSV is an effective way for organizations to streamline their processes and ensure compliance is met. It offers a number of advantages, from time and cost savings to visibility and improved accuracy. Automated license verification also reduces the potential for potential regulatory risks and helps organizations to create a reliable system of record for their employees.","The terms “privacy” and “security” are sometimes used interchangeably. However, there are some important distinctions between the two concepts, and understanding each is vital if you want your organisation to be as protected as possible against threats.\nAn effective organisation has both security and privacy, and takes measures to reinforce both. In this guide, we will delve into each concept, the relation between the two, and whose responsibility it is to protect them.\nPrivacy vs Security\nThe term “security” is defined as the state of being protected from dangers and threats. Within cyber security, this means all organisational assets (e.g., data) being safe, and the processes, procedures, assets, and tools used to protect them.\nIn the age of Big Data, there is more data (much of it personal and sensitive) than ever before stored by various organisations, and it is impossible to exist as an organisation without interacting with it to at least some extent (this includes simple day-to-day uses like GPS or Google Maps). This means that responsibility for data security is shared by every individual and organisation that interacts with it.\nPrivacy is also a responsibility for individuals and organisations. While data often needs to be accessible in order to carry out operations, accessibility must be restricted and limited to only the necessary parties. These necessary parties can change on a regular basis, so accessibility must be enforced accordingly using the relevant controls.\nSecurity controls are used to enforce privacy, and each is important, so they often intertwine.\nDifferences between data privacy and security\nAccording to the European Data Protection Supervisor, the term “privacy” is defined as “the ability of an individual to be left alone, out of public view, and in control of information about oneself.” What constitutes privacy in general can be subjective.\nWhen it comes to data privacy in the age of Big Data, the responsible use of data and information is essential. Organisations have a responsibility to keep individuals’ personal information safe, and access to it restricted as much as possible.\nSecurity refers to the specific controls put in place to protect personal data from unauthorised access, as well as to protect an organisation’s other assets. This can include operational security controls, such as Clean Desk policies, Mobile Device policies, and Identity and Access Management (IAM) solutions. Technical security controls, such as encryption, antivirus software, firewalls, multi-factor authentication (MFA), are also essential.\nAll of these factors contribute towards a secure layer and “defence-in-depth”—a cyber security term which refers to using multiple layers of security within an organisation to ensure that there are no weaknesses. Your organisation is only as strong as its weakest asset!\nRelationship between data privacy and security\nFor example, an individual may share personal information with their bank in order to open an account. If the bank sold information to an advertising or marketing company, this would be a breach of privacy (although would be legal if agreed to when the account holder signed up), although the account would still be secure.\nIf the bank’s database was hacked, information would be accessible for the purposes of identity theft or fraud, and both privacy and security would have been compromised. To maintain both privacy and security, the bank would need robust defences against hacks as well as a policy that forbids sharing of information with third parties.\nSecurity measures to protect privacy\nAlongside common cyber security defences like antivirus protection and firewalls, security measures used to protect privacy include:\n- Identity and Access Management (IAM): a framework of policies and technologies that authenticate (verify the identity of) users attempting to access information. For example, requiring the input of a password in order to access information is a form of IAM.\n- Encryption: the conversion of data into an unrecognisable form or “code”, which can only be decrypted (or “unscrambled”) with a specific password or “key”.\n- De-identification: the removal (or transformation) of personal identifiers (e.g., names, addresses, numbers, etc) from information. This can be carried out in a number of ways, including encryption.\n- Differential Privacy: the use of only the minimum required information needed to carry out operations. For example, data analysis can be carried out on only the relevant fields of databases.\nBeing mindful of both security and privacy will benefit your own protection, along with that of your organisation and the data you handle.\nHow important is data privacy and security?\nData security and privacy are crucial within an organisation. Without both, an organisation is more susceptible to attacks than one that has these controls in place. There are various factors that can impact data privacy and security.\nCyber crime is rising sharply and constantly in England and Wales. Action Fraud, the national reporting centre for fraud and cyber crime, reported a 28% rise in fraud offences since March 2020. With a 57% increase in online shopping and auctions (according to the same report), more and more people are at risk from cyber crime and fraud.\nIn 2019, a huge 90% of data breaches in the UK were due to human error, according to the UK Information Commissioner's Office. This statistic reinforces the urgent need for all members of an organisation to learn and abide by privacy and security procedures.\nAccording to a study by US IT services firm Cognizant, over half of consumers would stop using a company that uses personal information in a way that they consider irresponsible. Another Cognizant study shows that half of consumers are even willing to pay more for products and services that sufficiently protect their information. When it comes to the handling of personal data, there is serious money to be made—or lost!\nWho is responsible for breached or leaked data?\nA data breach can happen to any organisation, and the frequency of breaches is increasing across every industry. An organisation must adhere to certain data protection legislation based on their location, industry, and the type of data that they are handling.\nDifferent industries may have more specific legislation to follow. For example, companies may need to comply with UK GDPR or EU GDPR, or even PCI-DSS, depending on their nature.\nFor example, GDPR requires organisations to prove that they have taken sufficient measures to protect customer data, as well as to notify customers in the event of a data breach. Failing to meet these requirements can result in heavy fines, legal action from customers, or even criminal charges, as well as notifying customers and the relevant data protection agency (e.g. the Information Commissioner's Office in the UK) in the event of a data breach.\nUnder GDPR, data controllers (individuals or organisations who “own” data, and determine the reasons and methods of collecting it) are primarily responsible for compliance with data protection requirements. However, data processors (individuals or organisations instructed by the data controller to process data, e.g., cloud storage systems and those who own and maintain them) do have some responsibilities, such as implementing sufficient cyber security measures.\nWhile the owners of cloud storage systems must notify data owners (the organisations that use that cloud system to store their customer’s data), the data owners are considered the data controllers, and are otherwise legally responsible for protecting information. Failure to sufficiently restrict access, encrypt data, or other safeguards can increase a data owner’s liability for the results of a data breach.\nUltimately, if an organisation suffers a data breach, all staff can be impacted. All staff members have an obligation to ensure that they are following organisational processes and procedures when managing data. More importantly, the organisation would most likely experience reputational damage, loss of income, and possible legal consequences.\nBoth privacy and security are crucial for yourself and for your organisation. Handling data is a responsibility which must be taken seriously, with all necessary procedures and policies implemented and relevant legislation followed.\nFollowing security and privacy measures is not just good practice to follow as an employer—it is also critical for employees to follow, and for everybody to follow in daily life outside of the workplace! In the modern world, data is usually encountered daily, and both security and privacy play critical roles in protecting and regulating this data."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:78e596d8-142b-4bd4-a61f-5513fd2bc57d>","<urn:uuid:779faaec-b159-48c3-8c6e-405eda2519c5>"],"error":null}
{"question":"How did the Maya and pre-Columbian Amazonian societies differ in their response to environmental challenges?","answer":"The Maya and pre-Columbian Amazonian societies showed different patterns of adaptation to environmental challenges. The Maya developed sophisticated water management systems, building large underground reservoirs to store rainwater due to groundwater scarcity. In contrast, pre-Columbian Amazonian societies showed diverse responses to climate change, with some cultures persisting while others declined during drought periods. For example, in Eastern Amazonia, the Marajoara culture disappeared entirely, while in the Bolivian Llanos de Moxos, changes in land use patterns occurred about a century before European arrival.","context":["Different ways of coping with climate change in pre-Columbian Amazonia\nOur paper \"Climate change and cultural resilience in late pre-Columbian Amazonia\" summarises archaeological and palaeoclimatological data from six regions of the Amazon. These areas experienced social change, collapse and migrations during periods of climate change. In many cases, this happened only a few centuries before the arrival of Europeans. Although our initial expectation was to find a single, clear trend, the final version of the paper focuses on the diversity of responses from different kinds of pre-Columbian societies to climatic events.\nMost of the general public knows about the collapse of the Maya. They are aware that Maya cities in the tropical forest were abandoned and the ruling elite disappeared centuries before the Spanish conquest. We felt, however, that the overall perception was different in the case of Amazonia, especially with the number of recent publications reappraising the size and complexity of pre-Columbian populations. Many people imagined that Amazonia was becoming gradually more populated, with growing impacts on the environment, and societies becoming increasingly more complex until the arrival of Europeans caused their demise.\nA challenge for that scenario was the fact that the archaeological data from various parts of Amazonia showed abrupt changes and abandonment of settlements before 1492. This was clear, for example, in the disappearance of the Eastern Amazonian Marajoara culture - famous for its beautiful painted pottery. In the Bolivian Llanos de Moxos, the pollen record showed changes in land use about a century prior to the Columbian encounter, coinciding with the abandonment of the large habitation mounds in the region.\nBy 2015, the palaeoecological data collected by the Leverhulme project \"Pre-Columbian human land-use in Amazonia\" were showing ubiquitous transformations in the Amazon long before the arrival of Europeans. The trend was too evident to be ignored, and author José Iriarte decided to present the results in the XIX INQUA Congress in Nagoya, Japan. Palaeoecologist Henry Hooghiemstra was in the audience that day. As José and Henry talked over a cup of coffee after the presentation, the idea for this project was born.\nWhen we started joining the pieces of the puzzle, we realised that making sense of the big picture was too much for a single brain. In 2016, José Iriarte, leader of the PAST (Pre-Columbian Amazon-Scale Transformations) project funded by the European Research Council (Grant agreement ID: 616179), organised the workshop \"Land use and climate changes at the eve of the conquest in the Neotropics: an interdisciplinary approach\" and invited archaeologists, palaeoecologists and palaeoclimatologists working in the Amazon. Sometimes it takes a fresh view from an outsider to solve a long-standing problem. In our case, this perspective came from the palaeoclimatologists, who immediately started noticing patterns in the way the archaeological record seemingly responded to changes in precipitation.\nAfter the workshop, it took us a long time to organise all the data. We were all busy with other projects, there was too much information to be compiled, and, unfortunately, the clear patterns that we believed to have found during the workshop did not resist deeper scrutiny. An ideal scenario for a high-impact paper, I thought, would be to show massive abandonment in the archaeological record coinciding perfectly with climatic events in the past, like prolonged droughts. Even better if this happened throughout the Amazon. Some people in the workshop were also hoping to find this compelling story. Others were more interested in the possibility of different responses to climate change. In either case, we were stuck for a couple of years as no clear pattern emerged, and we could not find the central theme around which to write the paper.\nIt was only in 2018, when I was determined to solve the question once and for all, that I had a new insight. I was disappointed because, even though the archaeological record of some parts of the Amazon showed clear declines during phases of drought, others did not. Sometimes, even in the same region, some cultures were doing well in spite of climate change, while others were declining. This is when I started to ask myself: what if there is a pattern in the types of societies and economies that perished during events of climate change? A new discussion began among all authors, who contributed with several brilliant conceptual approaches. In the end, we were all happy with the new paper that resulted, which has a more complex and nuanced discussion than we could have hoped for in 2016.","- The Ancient Mayan lived in the Yucatán around 2600 B.C. Today, this area is southern Mexico, Guatemala, northern Belize and western Honduras. By 250 A.D., the Ancient Maya were at their peak of power.\n- The Maya had no central king ruling their huge empire. Instead, there were as many as 20 separate areas, similar to ancient Greece city-states. Each major city had its own ruler and noble class supported by smaller cities and the surrounding farms and villages.\n- The Ancient Maya developed the science of astronomy, calendar systems and hieroglyphic writing. They were also known for creating elaborate ceremonial architecture, such as pyramids, temples, palaces and observatories. These structures were all built without metal tools.\n- The Mayan people were also skilled farmers. In order to farm, they had to clear huge sections of tropical rain forest. Groundwater was scarce in these areas, so they had to build large underground reservoirs to store the rainwater.\n- The Maya were skilled weavers and potters. They also cleared routes through jungles and swamps to create trade routes. This allowed them to sell and trade the goods they had made for goods they needed.\n- The Maya writing system was made up of 800 glyphs. Some of the glyphs were pictures and others represented sounds. They chiseled the glyphs into stone and inside codices. Codices were books that were folded like an accordion. The pages were fig bark covered in white lime and bound in jaguar skins. The Maya wrote hundreds of these books. They contained information on history, medicine, astronomy and their religion. The Spanish missionaries burned all but four of these books.\n- The Ancient Mayans were a very religious people. Mayan actions were based on rituals and ceremonies. The Maya had many different gods. They also had rituals. One of those rituals was human sacrifice. The Mayan Kings were considered to be direct descendants of the Mayan Gods. Mayan religion was divided into three parts with earth as one part, the level above the earth as another part, and the level below the earth as the third part. The level above the earth was like the Christian heaven and the level below the earth was like the Christian’s hell.\n- The Ancient Maya had a class society. At the top were the nobles and priests. Their middle class was made up of warriors, craftsmen and traders. The farmers, workers and slaves were at the bottom.\n- The Mayas wove beautiful fabrics and designed musical instruments like horns, drums and castanets. They also carved huge statues. Archaeologists can tell a great deal about the ancient Maya from their wonderful pottery and clay figures. The art they created honored their leaders, gods, and their daily life.\n- About AD 300 to 900 the major centers of the Mayan civilization were Palenque, Tikal, and Copán. Something happened and these places were mysteriously abandoned. Many theories have been considered such as disease, invasion by another culture, natural disaster or collapse of their trade routes which would have destroyed their economy. No one knows for sure what happened.\nDownload the Ancient Mayan Study Pack\nIncluded in This Study Pack (PDF Format)\n- Study Guide & Fact File\n- Fill in the Blank Quiz Sheet\n- Word Search\n- Crossword Puzzle\nThis is a free study pack for all KidsKonnect members to download, print, and use.\nYou need to be logged in to download this worksheet. Login now or signup for a free KidsKonnect membership to get instant access!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:24a17221-a0fe-4db5-9f2e-25eaff2c6226>","<urn:uuid:4c495c37-a5cb-4415-a1f4-4daba465dcd9>"],"error":null}
{"question":"What are the control methods for preventing invasive species in ballast water, and what technological innovations are emerging in vertical farming systems?","answer":"Ballast water control methods include implementing management plans, maintaining record books, and following specific procedures like exchanging at least 95% of ballast water while at sea (minimum 200 nautical miles from shore and 200 meters depth). Treatment can also involve chemical or mechanical methods such as UV-radiation, filtration, deoxygenation, cavitation, and ozone treatment. In vertical farming, technological innovations include automated sensors and imaging techniques combined with crop simulation models and artificial intelligence for environmental control. The systems allow precise management of growth factors like light, temperature, humidity, CO2 concentration, and nutrients. Additionally, advances in gene editing and plant genome modifications are enabling the development of faster-growing, larger, and healthier crops in vertical farming systems.","context":["Ballast water discharge and the environment\nBallast water discharges by ships can have a negative impact on the marine environment. The discharge of ballast water and sediments by ships is governed globally under the Ballast Water Management Convention, since its entry into force in September 2017. It is also controlled through national regulations, which may be separate from the Convention, such as in the United States.\nCruise ships, large tankers, and bulk cargo carriers use a huge amount of ballast water, which is often taken on in the coastal waters in one region after ships discharge wastewater or unload cargo, and discharged at the next port of call, wherever more cargo is loaded. Ballast water discharge typically contains a variety of biological materials, including plants, animals, viruses, and bacteria. These materials often include non-native, nuisance, exotic species that can cause extensive ecological and economic damage to aquatic ecosystems, along with serious human health issues including death.\nThere are hundreds of organisms carried in ballast water that cause problematic ecological effects outside of their natural range. The International Maritime Organization (IMO) lists the ten most unwanted species as:\n- Cholera Vibrio cholerae (various strains)\n- Cladoceran Water Flea Cercopagis pengoi\n- Mitten Crab Eriocheir sinensis\n- Toxic algae (red/brown/green tides) (various species)\n- Round Goby Neogobius melanostomus\n- North American Comb Jelly Mnemiopsis leidyi\n- North Pacific Seastar Asterias amurensis\n- Zebra Mussel Dreissena polymorpha\n- Asian Kelp Undaria pinnatifida\n- European Green Crab Carcinus maenas\nBallast water issues by country\nThe ballast tanks in New Zealand carry animals and plants that kill ecosystems. Ballast tanks are only used in cargo ships there. Ballast water is controlled under the Biosecurity Act 1993.\nThe zebra mussel, which is native to the Caspian and Black Seas, arrived in Lake St. Clair in the ballast water of a transatlantic freighter in 1988. Within 10 years it had spread to all of the five neighbouring Great Lakes. The economic cost of this introduction has been estimated by the U.S. Fish and Wildlife Service at about $5 billion.\nBallast water discharges are believed to be the leading source of invasive species in U.S. marine waters, thus posing public health and environmental risks, as well as significant economic cost to industries such as water and power utilities, commercial and recreational fisheries, agriculture, and tourism. Studies suggest that the economic cost just from introduction of pest mollusks (zebra mussels, the Asian clam, and others) to U.S. aquatic ecosystems is more than $6 billion per year.\nCongress passed the National Invasive Species Act in 1996 in order to regulate ballast water discharges. The Coast Guard issued ballast water regulations in 2012. Under the authority of the Clean Water Act, the Environmental Protection Agency (EPA) published its latest Vessel General Permit in 2013. The permit sets numeric ballast water discharge limits for commercial vessels 79 feet (24 m) in length or greater. EPA issued a separate permit for smaller commercial vessels in 2014.\nAmong 818 ports in the Pacific region, Singapore alone accounts for an estimated of 26 percent of cross-region (long range) species exchange. Via targeted ballast management on Singapore and a few other \"influential\" ports, cross-region species exchange to/from the Pacific region can be combinatorially reduced.\nTo react to the growing concerns about environmental impact of ballast water discharge, the International Maritime Organization (IMO) adopted in 2004 the \"International Convention for the Control and Management of Ships' Ballast Water and Sediments\" to control the environmental damage from ballast water. The Convention will require all ships to implement a \"Ballast water management plan\" including a ballast water record book and carrying out ballast water management procedures to a given standard. Guidelines are given for additional measures then the guidelines.\nThe goals of the convention are to minimise damage to the environment by:\n- Minimise the uptake of organisms during ballasting.\n- Minimising the uptake of sediments during ballasting.\n- Ballast water exchange while at sea (the ship should be minimum 200 nautical miles from shore with a depth of minimum 200 metres and can use the flow through or sequential method). At least 95 percent of the total ballast water should be exchanged.\n- Treatment of the ballast water by chemical or mechanical influences (UV-radiation, filter, deoxygenation, cavitation, ozone…)\nControl measures include:\n- International Ballast Water Management Certificate\n- Ballast water management plan\n- Ballast water record book\nThe IMO convention was ratified by enough countries and entered into force on September 8, 2017.\n- \"International Convention for the Control and Management of Ships' Ballast Water and Sediments\". International Maritime Organization.\n- Living Beyond Our Means: Millennium Ecosystem Assessment, 2005. Statement from the Board.[full citation needed]\n- Statement of Catherine Hazlewood, The Ocean Conservancy, “Ballast Water Management: New International Standards and NISA Reauthorization,” Hearing, House Transportation and Infrastructure Subcommittee on Water Resources and Environment, 108th Cong., 2nd sess., March 25, 2004.\n- David Pimentel, Lori Lach, Rodolfo Zuniga, and Doug Morrison, “Environmental and Economic Costs Associated with Non-indigenous Species in the United States,” presented at AAAS Conference, Anaheim, CA, January 24, 1999.\n- United States. Pub. L. 104-332. October 26, 1996.\n- U.S. Coast Guard, Washington, D.C. \"Standards for Living Organisms in Ships’ Ballast Water Discharged in U.S. Waters.\" Federal Register, 77 FR 17254, 2012-03-23.\n- \"Vessels Incidental Discharge Permitting\". National Pollutant Discharge Elimination System (NPDES). Washington, D.C.: U.S. Environmental Protection Agency (EPA). 2015-12-09.\n- EPA (2014-09-10). \"Final National Pollutant Discharge Elimination System (NPDES) Small Vessel General Permit for Discharges Incidental to the Normal Operation of Vessels Less Than 79 Feet.\" Federal Register. 79 FR 53702.\n- Xu, Jian; Wickramarathne, Thanuka L.; Chawla, Nitesh V.; Grey, Erin K.; Steinhaeuser, Karsten; Keller, Reuben P.; Drake, John M.; Lodge, David M. (2014). \"Improving management of aquatic invasions by integrating shipping network, ecological, and environmental data\": 1699–1708. doi:10.1145/2623330.2623364.\n- \"Ballast Water Convention to Enter into Force in 2017\". Maritime Executive. 8 September 2016. Retrieved 14 September 2016.\n- Buck, Eugene H.(2012). \"Ballast Water Management to Combat Invasive Species.\" U.S. Congressional Research Service. Report No. RL32344.","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e9749ea3-2ea0-436c-95fd-f59b7fb5adc2>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"Should older adults (65+) and professional baseball players approach shoulder mobility training differently? Curious about the key differences! 💪","answer":"Yes, there are different approaches for these populations. For older adults, modifications are needed and activities can be performed using a matt table. For professional baseball players, the focus is on balancing physical workload of games, practice, and team travel while maintaining consistent training. Baseball players need 1-2 strength sessions per week in-season, with workouts divided into lower and upper body days as needed, while being cautious not to add new exercise movements during the season.","context":["Lee Burton, Ph.D., ATC |\nFunctional Movement Systems (FMS) and ACSM recently hosted an industry-presented webinar with Lee Burton, PhD, ATC, entitled Secrets to Enhancing Shoulder Strength and Function. This is Part II of the Q&A.\nRead Part I of the Q&A here.\nSeveral questions were asked by attendees during the webinar and the answers pertaining to the rotator cuff, thoracic spine mobility, and more are below.\nQ: What kind of exercises do you recommend for somebody with Scheuermann's kyphosis?\nUtilize the quadruped rotations that were shown in the presentation, focus on the tighter side. The key would be to also focus on other patterns like squatting, lunging and ensure they are not getting worse. So not just focusing on the mid-back, but also making sure other movements are at least acceptable and can work those patterns, like split squats and forward single arm presses.\nQ: What is the best exercise to strengthen an injured rotator cuff pre-surgery? Post?\nKeep it simple, push-ups, pull-ups, deadlifts (single-arm) and farmer’s walks.\nQ: How do you apply this to the older adult population (65+) who are not able to perform a lot of these movements/screenings?\nThere are a variety of ways to incorporate some of these activities with older adults. For example, a simple idea would be to try using a matt table.\nQ: What are your thoughts on overhead exercises where you're sitting on the floor with your feet out front like with a Z-press for correcting shoulder dysfunctions?\nI believe you should avoid loaded presses in a seated position, these are better performed standing or half-kneeling.\nQ: If we detect an upper crossed syndrome and thoracic stiffness, do we have to eliminate all overhead movements until the problem is fixed?\nNot necessarily, just focus on more one arm activities or half-kneeling/standing split stance push/pull activities with a cable system.\nQ: What do you do to work on kyphosis?\nStart on the ground doing crawling, quadruped rotations, etc. focus on the tighter side.\nQ: What do you do for functional movement with someone with double knee replacement that can’t bend down?\nThere are quite a few things that can be done. For example, try working on single-leg balance toe-touching, deadlifting, etc.\nQ: Are there benefits to static stretching for shoulder health, if so, what are they?\nSure, static stretching for shoulder health is more based on the individual and what the underlying issues are.\nQ: Are there any benefits to doing rotator cuff exercises such as internal/external movements and which do you recommend for healthier shoulders?\nI don’t believe it is necessary unless there is a medical problem with the shoulder/rotator cuff. Focus more on stability activities such as deadlifts and farmer’s walks, these are better activities for stability.\nQ: What is the best method to improve shoulder external rotation?\nFirst focus on mid-back/t-spine mobility, which most often is the problem. Most people have limitations in shoulder internal rotation and not external rotation.\nQ: I'm a PT and know how to treat shoulder impingement in the clinic, but do you have any tricks to combat impingement in the gym (without the manual therapy for ex. Mobilizations)?\nHave people focus on getting and maintaining thoracic spine mobility through some of the techniques I reviewed and ensure good postural control when performing lifts. Poor technique is the primary cause of shoulder problems when lifting.\nQ: How do you implement some of these movements with morbidly obese patients? What special considerations need to be taken?\nIn my opinion, the focus should be on general mobility and body weight activities and not worrying about isolating any one thing. Working on increasing their overall activity level, keep it simple and slowly progress.\nQ: Are these breathing focused stretches OK for someone who has asthma, kyphosis and poor cervical and thoracic mobility?\nYes, I definitely recommend breathing focused stretches for these individuals.\nQ: What is the best way to train the rotator cuff muscles?\nEnsure the client has good mid-back mobility, postural control, and focus more on stability activities such as deadlifts and, farmer’s walks as these are better activities for stability.\nQ: With a client that has very poor movements but is also very overweight, what type of movements do you recommend, or do you have a strategy to correct both?\nIn my opinion, the focus should be on general mobility and body weight activities and not on worrying about isolating any one thing. Working on increasing the client’s overall activity level is key. Keep it simple and slowly progress.\nQ: You mentioned that the overall role for the rotator cuffs was to stabilize the joint. Should you focus on strengthening the muscle specifically in order to protect the shoulder?\nI don’t believe it is necessary unless there is a medical problem with the shoulder/rotator cuff. Focus more on stability activities such as deadlifts and farmer’s walks as these are better activities for stability.\nDr. Lee Burton has an extensive background in sports medicine and strength and conditioning. He currently consults with a wide variety of professional sports organizations as well as leading health and fitness facilities on injury prevention and performance enhancement. He lectures both nationally and internationally on various topics in the fields of sports medicine and strength and conditioning. Dr. Burton is a Certified Athletic Trainer, Certified Strength and Conditioning Specialist and one of the founding owners of Functional Movement Systems.\nView More Popular FMS Content\nCEC Course | FMS - Move Well then Move Often\nSecrets to Enhancing Shoulder Strength and Function Q&A Part I","High School Baseball Strength & Performance\nHigh Baseball Players enrolled in the WBP High School to College Baseball Recruiting Program, receive a personal custom built baseball strength and performance training program specifically designed for them by Professional Baseball Strength Coaches Brian Niswender, MA,CSCS and/or Dave Yeager, ATC, CSCS.\nBrian Niswender MA, CSCS\nWBP Strength and Performance\nMentor and Coach\nPartial list of Players that have improved their game with Brian include,\nAlex Cintron White Sox, Chad Tracy Diamondbacks, Brandon Webb Diamondbacks, Edger Gonzales Diamondbacks, Brandon Meeders Diamondbacks, Jose Valverde Diamondbacks, Robbie Hamock Diamondbacks, Casey Diagle Diamondbacks, Andy Green Diamondbacks, Brian Bruney Yankees, Scott Hairston Padres, Lance Cormier Braves, Oscar Villarreal Braves, Mike Gosling Reds, Jerry Gil Reds, Chris Capuano Brewers, Matt Kata Pirates, Enrique Gonzales Nationals, John Paterson Nationals, Mike Koplove Indians, Jason Bulger Angels, Lyle Overbay Blue Jays ........\nDave Yeager, ATC, CSCS\nWBP Strength and Conditioning Mentor and Coach\nAssistant Trainer, Milwaukee Brewers\nDave olds national certifications as an Athletic Trainer from the National Athletic Trainer’s Association (NATA) and as a Strength and Conditioning Specialist from the National Strength and Conditioning Association (NSCA). Throughout his career, David has specialized in prescribing exercise program designs for general conditioning, performance enhancement, rehabilitation, and reconditioning of athletes and individuals in all levels of competition and physical activity. David has served as an Athletic Trainer and Performance Enhancement Coach for the Milwaukee Brewers, the 2001 World Series Champion Arizona Diamondbacks and the Boston Red Sox organizations. He has also been involved with other high school, collegiate, Olympic, and professional athletes from organizations such as: The Cleveland Indians, The Cincinnati Reds, The Pittsburgh Pirates, The Detroit Tigers, The Tampa Bay Devil Rays, The Los Angeles Dodgers, The Oakland A’s, The Chicago White Sox, The San Diego Padres, The Detroit Lions, Auburn University, The University of Georgia, The University of South Carolina, Jacksonville State University, The University of Alabama-Huntsville, U.S.A. Bobsled, and Team Nicaragua\n\"Worldwide Baseball Prospects is an organization dedicated to providing young athletes with the tools that they need to be successful in life and sports. As a Coach / Mentor with Worldwide Baseball Prospects, I am looking forward to sharing my knowledge and experiences with the players of WBP to help guide them through their quest for athletic development. Not only can I teach athletes, but I believe that each athlete that I work with teaches me something new about how I can better serve them. With assistance from all of the WBP mentors, the player can become a well-rounded person, student, and athlete. Success comes from hard work and opportunity.\"\n“It’s good to be green.” - Kermit the Frog\nby Minor League Athletic Trainer, Milwaukee Brewers\nDave Yeager, ATC, CSCS\nAs a student-athlete, it helps to have mentors that can provide you with guidance along your path through developmental milestones and athletic achievements. For me, one of those mentors was Dr. Jack Hughston. Considered by most as a pioneer in the field of sports medicine, he was among the first to provide medical coverage to collegiate athletic programs.\nWhen I arrived for my freshman year of college on the campus of Auburn University, I was fortunate enough to receive a scholarship as a student athletic trainer and Dr. Hughston was the university’s Team Physician. As a student, it didn’t take me long to be introduced to his favorite saying:\n“As long as you’re green, you’re still growing. Once you’re ripe, you’re next to rotten.”\nThis saying has stayed with me throughout my entire career as an athletic trainer and strength and conditioning coach. It has reminded me that once, I think that I know everything that there is to know about my profession, then I’ve missed out on a lot of new information. The sports medicine and sports performance fields are always changing and evolving and it is important to continue to grow a base of knowledge, develop new concepts, and fine tune my training philosophies and programs. It is also important to pass on that knowledge to others.\nIn sports, Dr. Hughston’s saying is still true. In order to be successful on the field, you have to continue to keep your body strong, improve your skill techniques, hone and adjust your mental approach, and fine tune your perceptual abilities. If you don’t, then your game becomes stagnant and reaches a plateau. Only when you resume the training process will you continue to improve.\nIf Sliding Head First Were Faster, World-Class Sprinters Would Dive Across the Finish Line!\nBy David Yeager, ATC, CSCS\nWBP Strength Coach/Mentor\nMinor League Athletic Trainer, Milwaukee Brewers\nThe 2011 baseball season started with tragedy at Arizona State University. While sliding head-first into second base during an attempted steal, freshman player Cory Hahn collided with the knee of the fielder and suffered a fractured neck and is reportedly paralyzed.\nThough there are injury risks with feet-first sliding, it is commonly believed that the more devastating injuries are associated with head-first slides (i.e. cervical spine injuries, shoulder dislocations, and other elbow, wrist, and hand trauma). Yet, coaches continue to teach, and players continue to attempt head-first sliding because they believe it is a faster baserunning technique.\nThe truth…IT’S NOT. A 2002 study proved once and for all that at all levels, there is no difference in speed between head-first and feet-first sliding. The authors concluded that in fact, feet-first sliding may even be slightly faster.\nKane SM, House HO, Overgaard KA. Head-first versus feet-first sliding: A comparison of speed from base to base. The American Journal of Sports Medicine. 2002; 30(6): 834-836.\nAs I mentioned, there is injury potential to the lower body with the feet-first technique (i.e. ankles, knees, hips, and hands), but these are not considered to be in the same class of severity as those associated with the head-first method. It can be argued that when the feet-first slide is taught correctly and practiced, the potential for injury is low – particularly now with breakaway bases, etc.\nNEVER SHOULD A PLAYER SLIDE HEAD-FIRST INTO HOME PLATE!\nAvoiding Muscle Soreness: Micro Tears vs Muscle Tears\nBy Eric McMahon, MEd, CSCS\nMinor League Strength and Conditioning Coach Texas Rangers\nFor professional baseball players, the majority of training occurs in-season while balancing the physical workload of games, practice, and team travel. Spring Training begins early in the year (February-March) and is followed by a 140-162 game regular season lasting through September or later (The 2010 World Series ended on November 1st). It is also not uncommon for players to participate in Winter Leagues in the Dominican Republic, Venezuela, or Arizona during the off-season months. So, given the limited time for physical preparation during the off-season and the high baseball workload throughout the year, how should coaches go about avoiding muscle soreness in players from in-season strength and conditioning work? A simple answer is that coaches must use caution towards high-volume types of training while promoting a variety of fatigue-reducing recovery strategies (i.e. SMFR, contrast bathing, massage, nutrition, and rest). Possessing a better understanding of muscle soreness is helpful in providing guidance to players who do not wish to perform resistance training in fear of becoming sore.\nThe muscle soreness occurring 24-72 hours following unaccustomed training is referred to by exercise professionals as DOMS (Delayed Onset Muscle Soreness). Symptoms of DOMS may include compromised running mechanics, decreased functional range of motion or stiffness, and reductions in strength and power. Of importance to coaches, baseball performance may be negatively affected by excessive muscle pain, leading to compromised movement patterns in throwing, running, and hitting. A risk of muscle injury in tissues surrounding the prime movers (major muscle groups) is of increased concern from altered baseball mechanics.\nIt is important to realize that the negative effects of DOMS are temporary, and that the damage to the muscle from training (i.e. overstretching) occurs within the muscle cells (at the level of the myofibril) and not in the muscle tissue as a whole. From a medical perspective, a muscle tearing injury (occurring in an entire muscle) would result in a less pliable, scarring over of the affected tissue (i.e. scar tissue). This is NOT the same tearing of muscle tissue which results in DOMS. The term “micro-tears” is often used to describe the displacement, or tearing, of the myofibril proteins (actin, myosin, troponin, and tropomyosin) which serve as cogs to contract our muscles. Micro-tears are believed to promote muscle growth and gains in strength, leading to the notion that DOMS may be a natural part of the early strength building process.\nThe most effective method of preventing DOMS is to avoid unaccustomed training through a consistent routine. The complete replacement of myofilament proteins naturally occurs every 7-15 days within muscle cells, allowing for relatively quick recovery and adaptation from exercise induced stress. A consistent training approach which considers the overall workload of professional baseball players (i.e. games, practices, travel, nutrition, and sleep) will allow for the players to gradually improve muscular fitness to achieve a higher training level. A workload of 1-2 strength or power sessions per week in-season is a good starting point to avoid overtraining. Workouts can be divided into lower and upper body days as needed.\nKey Points For In-Season Training:\nDOMS is most common with strenuous initial training sessions after prolonged inactivity – Perform a consistent routine and do not add new exercise movements in-season.\nDOMS is caused by the eccentric, or lowering, portion of exercise movements – Avoid performing “negatives” or exercises with strenuous deceleration components.\nDOMS symptoms temporarily lessen after a warm-up which promotes blood flow – Recommend that players perform a general cardiovascular warm-up before all activity.\nDOMS can negatively impact players physical and psychological performance – Be cautious in overloading players too soon to avoid poor performance on the field.\nGo to College Baseball Recruiting Program"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:1c9619ab-72e6-44e6-bf99-a492888300ae>","<urn:uuid:7ba29ad9-3c47-4079-8c94-be4bde45f1f9>"],"error":null}
{"question":"Hey! I'm curious about both educational psychology and special needs classrooms 🤔 How do they work together?","answer":"Educational psychology and special needs education are closely connected. Educational psychology is a scientific field that applies psychological theories to improve teaching and learning, including studying how different groups learn (like gifted children and those with learning disabilities). This knowledge is practically applied in special needs inclusive classrooms, where children with special needs learn alongside typically developing peers. The approach requires careful identification of each student's abilities and needs, development of Individual Educational Plans (IEPs), and implementation of adaptive teaching methods like small group instruction and specialized equipment to ensure effective learning for all students.","context":["Regular Facebook use could contribute to depressive symptoms, according to the results of one recent study. Continue reading\nIn a landmark experiment in 1935, John Ridley Stroop demonstrated a cognitive effect which has fascinated psychologists for centuries. In the first of a series of experiments reported in his dissertation, Stroop asked participants to read the names of a list of colour words (e.g. blue, red, etc) under two conditions. In the first condition, participants were asked to read words that were printed in black ink whereas in the other condition they were expected to read words which were printed in ink colours that did not match the color names. For example, the word blue may have been printed in red ink (i.e. blue – in this case, the correct answer would have been blue). In this experiment, Stroop found that there was no significant difference in performance between the two conditions. Continue reading\nWhile depression and aggression affect both males and females, gender differences in each of these conditions have frequently been noted in the literature. As it relates to depression in particular, Piccinelli & Wilkinson (2000) mentioned that there is a female preponderance in the prevalence, incidence and morbidity risk of this disorder. Continue reading\nOh what changes the seasons bring!\nFrom falling leaves to an icy sting\nTo a sudden shift in the songs we sing\nWe all know the common saying: “For everything there is a season.” A season to laugh, a season to cry… and apparently a season to listen to certain types of music. At least that’s what the findings of two studies conducted by Pettijohn, Williams and Carter (2010) seem to suggest. Both studies, conducted in the United States, were designed to examine how seasonal conditions influence music preferences in a sample of male and female college students. Continue reading\nEducational psychology is the scientific field concerned with applying psychological theories and concepts to the understanding and improvement of teaching and learning in formal educational settings. In simpler terms, it is concerned with the study of how students learn and how teachers can help them to learn effectively. Educational psychology draws on and combines various psychological theories and principles – such as those related to human development, motivation, learning, behavior management and assessment, among others – in order to improve the conditions of teaching and learning. Educational psychologists study the process of learning not only among the general population but also among sub-groups such as gifted children and those with various learning disabilities. Continue reading\nAll of us, at one time or another, have experienced what it means to be afraid. Fear is a normal feature of human existence and serves an adaptive function in that it triggers reactions which allow us to respond to danger or threat. At times, however, fear can become excessive, disturbing and out of proportion with reality. Persons who experience such abnormal fear are described as having a form of anxiety disorder known as a phobia.\nThe term phobia refers to an intense, irrational fear of a particular object or situation, whether real or imagined. The fear is so severe that it interferes with the individual’s daily functioning, restricting their activities and causing much distress. In many cases, individuals experiencing phobias recognize that their fears are irrational but feel helpless to control them. Continue reading","How to Teach Special Needs Students in an Inclusive Classroom\nAn inclusive classroom allows children with special needs to learn with typically developing peers of their own age in the same classroom. This model is becoming more common as schools try to mainstream their classrooms. Although an inclusive classroom requires many adaptations to accommodate the needs of its children as well as a skilled staff, with the right foundation and support, children of all abilities can learn together.\n1 Identify the abilities and needs\nIdentify the abilities and needs of each special-needs student. Include all individuals who are familiar with the student in the assessment. This can include the therapist, former teacher and parents. By getting a picture of the child, you can implement a teaching approach that is appropriate to the child's needs and learning style.\nAssist in developing a well-designed Individual Educational Plan (IEP) for the special-needs student. As a teacher, you have the ability to share the child's strengths and weakness not only with the school board but also with the other therapist. An IEP details the needs of the child, determines the goals for the child, identifies the resources that will assist in implementing those goals and specifies adaptations for the special-needs child to function in the classroom.\n3 Build a dream\nBuild a dream team of educators in the classroom. Teaching special-needs children in an inclusive classroom is most effective when you have a general education teacher, special education teacher and teacher assistants in the classroom at all times, working together to achieve a common goal.\n4 Establish a relationship with the family\nEstablish a relationship with the family. Parents know their children best and can provide valuable information about the child as well as be the educational support outside of the classroom. Create a notebook for each special-needs child as a form of communication for teachers and parents to share thoughts and ideas.\n5 Implement an adaptive environment\nImplement an adaptive environment. Teaching special-needs children in an inclusion classroom requires the necessary equipment as well as adaptive forms of communication so that everyone can participate in classroom activities. An adaptive environment can include special chairs, sensory equipment, voice boxes, special visuals for teaching and communicating and specialized scissors, just to name a few.\n6 Create a sense of community\nCreate a sense of community. Assigning jobs to each student and letting each child have a voice during circle time are ways to make special-needs children feel that they are part of the community. This will boost self-esteem as they are able participate in the same activities as their typical-developing classmates.\n7 Allow therapies\nAllow therapies to take place in the classroom. For those special-needs children who require therapy, having some of their therapy sessions in the classroom reinforces the inclusion concept since they do not have to be taken out of the classroom. This is also an excellent way for the therapists and teachers to coordinate efforts in teaching the child, allowing for carryover from therapy to the classroom.\n8 Teach in small groups\nTeach in small groups. Creating centers with different activities and lessons allows for the special-needs child to interact with his peers as well as get the attention that is needed to complete the task.\n- Children taught in an inclusive classroom learn to respect each other, no matter their differences."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9ff77397-47a2-4639-9dd5-2981840cc07a>","<urn:uuid:88a63cbc-c335-4534-a3d8-7e4c5e57caa1>"],"error":null}
{"question":"I'm a non-profit director interested in maximizing our impact. What are the key differences between traditional charity approaches and effective altruism when it comes to selecting causes to support?","answer":"Traditional charity approaches often focus on local causes, emotional connections, and addressing complex problems in developed nations. In contrast, effective altruism emphasizes using evidence and reason to identify causes that can do the most good per dollar spent. While traditional approaches may support local medical clinics or arts programs, effective altruists typically prioritize interventions in the developing world that can save lives at a much lower cost (around $2,500 per life saved through top-rated international charities versus $10,000+ per child served in U.S. programs). Effective altruism also specifically rejects making decisions based on personal passion or emotional appeal, instead focusing on objective impact measures and cost-benefit analysis to identify the most effective interventions globally.","context":["Toward the end of the summer, bioethicist Peter Singer raised the hackles of art lovers everywhere with a New York Times op-ed that considered a hypothetical dilemma: should you donate to a charity that combats blindness in the developing world or should you spend that money instead on an art museum? After running through a cost-benefit analysis of each option, he determined that the charity addressing blindness “offers [donors] at least 10 times the value” of the museum.\nTo no one’s surprise, the arts community didn’t exactly roll out the welcome mat for the piece, calling Singer’s argument “a shocker,” “absurd,” and “tyrannical.” Another round of alarm ensued recently when none other than megaphilanthropist Bill Gates threw his support behind Singer’s thesis. The responses from our field to date have generally coalesced around two broad counter-arguments:\n- Why does it have to be “either/or”? Why can’t we support both? Singer forces a false choice in “assuming charitable giving is a zero sum game.” Weighing the value of saving a life against the value of donating to an art museum is comparing apples to oranges when “both are essential, and if either disappeared you’d be in bad shape.” We need a holistic approach to ensure we don’t “solv[e] Third World crises at the expense of fostering crises right here at home.” Just as we have “multiple passions in [our] lives,” donors can and should target multiple causes and direct their charitable dollars in a “proportionally prioritized” manner. Anyway, we can’t really be sure than curing blindness is more important than inspiring the next Jackson Pollock, and even if we were, concentrating all our resources with one or two tried and true nonprofits runs counter to the “messiness and power of America’s [decentralized] approach to charity.”\n- Saving lives is all fine and good – but only if those lives have meaning. If we’re so concerned with making sure that people can see, shouldn’t we also try to make sure they have beautiful things to look at? Singer’s logic is dangerous because he fails to acknowledge the “creative outlet[s] and emotional oas[e]s that only art museum[s] can provide.” If all philanthropic dollars were channeled toward alleviating disease and poverty, arts and culture would languish, society would become monochromatic and dull, and life would cease to be worth living.\nAs satisfying as these rebuttals may feel to arts advocates, they unfortunately miss the point. The crucial assumptions behind Singer’s argument are that\n- “there are objective reasons for thinking we may be able to do more good in one [sector] than in another,” and\n- we have a moral obligation to make choices that do as much good as possible.\nIt’s important to understand this perspective in the context of “effective altruism,” a relatively nascent but growing area of applied ethics that has been featured more than once on this blog, not to mention a recent edition of This American Life. Besides Gates, fellow philanthropic heavyweight and past Hewlett Foundation President Paul Brest has declared himself a fan. “Effective altruists,” or EAs, are on a quest to “do good” by way of hard-nosed rationality. “Doing good” doesn’t mean recycling a little more, or occasionally doling out spare change to a beggar on the street. It doesn’t mean foregoing a high-powered corporate career to work for a nonprofit. It means taking the time to analyze how to do the most amount of good possible with the resources available – or, to use a more nerdy turn of phrase, to “[use] science and rational decision-making to help as many sentient beings” as they can.\nMost funders are already in search of a big “bang for your buck,” but in trying to identify the objectively best causes to support, effective altruists stray from the conventional wisdom of mainstream philanthropy. EAs cast a global net when determining where to focus, and often settle on supporting causes in faraway parts of the world, the results of which they may never see in person. They also believe that while human lives are created equal, philanthropic causes are not. Those causes that can save or improve the most lives must take first priority.\nHow does this play out in practice? Let’s say you donate to the free medical clinic in your area. You do this for good reasons: you care about inequities in the American healthcare system, and want to give back to your community. You like the feeling you get when you walk by that clinic every day. Maybe you even know people who benefit from the services the clinic provides. The clinic gets its donation, and you get warm fuzzies. Everybody wins. Right?\nNot so, an EA would counter. Despite your good intentions, your donation amounts to a near-waste of resources:\nWe understand the sentiment that ‘charity starts at home,’ and we used to agree with it, until we learned just how different U.S. charity is from charity aimed at the poorest people in the world. Helping people in the U.S. usually involves tackling extremely complex, poorly understood problems… In the poorest parts of the world, people suffer from very different problems…\nWe estimate that it costs [Givewell’s] top-rated international charity less than $2,500 to save a human life… Compare that with even the best U.S. programs… over $10,000 per child served, and their impact is encouraging but not overwhelming.\nEAs advocate making evidence-based decisions even if they don’t resonate on an emotional or intuitive level:\nEffective altruism is consistent with believing that giving benefits the giver, but it’s not consistent with making this the driving goal of giving. Effective altruists often take pride in their willingness to give (either time or money) based on arguments that others might find too intellectual or abstract, and their refusal to give suboptimally even when a pitch is emotionally compelling. The primary/driving goal is to help others, not to feel good about oneself.\nIf this approach leaves you with an empty feeling in the back of your throat, it is by design. “Opportunity costs” – the costs of choosing not to behave in a certain way – weigh heavily on EAs. Every time you make a donation, considering where your money could have gone is as important as considering where it will ultimately go (emphasis mine):\nIn the “Buy A Brushstroke” campaign, eleven thousand British donors gave a total of £550,000 to keep the famous painting “Blue Rigi” in a UK museum. If they had given that £550,000 to buy better sanitation systems in African villages instead, the latest statistics suggest it would have saved the lives of about one thousand two hundred people from disease… Most of those 11,000 donors genuinely wanted to help people … But these people didn’t have the proper mental habits to realize that was the choice before them, and so a beautiful painting remains in a British museum and somewhere in the Third World a thousand people are dead.\nWeighing choices isn’t limited to how we spend our money – it also applies to how we spend our time. Just as EAs dispute the notion that people should support whichever charities they feel “passionate” about, they question whether channeling those passions into a nonprofit or medical career is the best way to make a difference. Many suggest instead that people “earn to give,” saying they “might be better off…in a high-earning job and making a deliberate commitment to give a large portion of what [they] earn away.“ The organization 80,000 Hours, founded to “become the world’s number one source for advice on pursuing a career that truly makes a difference in an effective way,” elaborates,\nWorking at a non-profit can be a great way to make a difference. But it’s no guarantee. Amazingly, lots of non-profits probably have no impact. And do workers at [a] non-profit have more impact than the people who fund them? The researchers who push forward progress? The entrepreneurs who transform the economy? Policy makers? Maybe. No one stops to ask.\nPutting ideas like these on the table is a great way to make those of us in the arts squirm. While there are echoes of the effective altruism movement in some recent trends within our field, like the “universal call” for better data on the impact of the arts and the pointed questions about who ultimately benefits from arts funding, the arts are chock-full of people – artists and arts administrators alike – who were drawn to their work by that same passion that EAs claim clouds our judgment. The idea of allowing cold rationality to dictate and limit our quest to “do good” flies in the face of our artistic sensibilities, and challenges the assumptions many of us made when we entered the nonprofit sector in the first place – even those of us who have a sincere desire to address social inequities.\nTempting as it may be, it would be short-sighted to dismiss the EA movement as the pet project of a bunch of aesthetically stunted curmudgeons. It’s hard to dispute the notion that we could improve the human condition if only we could get our act together and commit our resources to a data-driven approach. After all, the nonprofit darling of the moment, collective impact, is based on the same premise. What effective altruism does is counter our cause-specific argument for the arts with a dizzying moral appeal for cause agnosticism. And to be honest, it’s hard to see how the arts win if they play the game by the EAs’ rules. The “both/and” argument mentioned previously is unlikely to sway an effective altruist who weighs each decision as a choice between two different futures, one in which a museum gets funded and some lives get saved and one in which the museum struggles and more lives get saved. Even if the museum shut down completely, its patrons could probably find or create an alternative “creative outlet and emotional oasis,” while the people dying of malaria can’t very well make the mosquito nets themselves. The “we give lives meaning” argument likewise rings hollow when we’re talking about lending privileged lives (anyone living on more than $2 a day is privileged in a global context) a dose of incremental “meaning” at the expense of giving others a shot at basic survival. It also comes across as incredibly condescending to those others considering that they would likely never get the opportunity to visit or benefit from Singer’s hypothetical museum. In any case, art is hardly the only possible delivery mechanism for meaning. In the words of one effective altruist,\nTrying to maximize the good I accomplish with both my hours and my dollars is an intellectually engaging challenge. It makes my life feel more meaningful and more important. It’s a way of trying to have an impact and significance beyond my daily experience. In other words, it meets the sort of non-material needs that many people have.\nWhether the EA movement sputters or gathers steam, taking the time to engage with its principles, even critically, is a healthy exercise. The bottom line is that EAs may actually be onto something when they argue it’s possible to make a bigger dent in one sector than another. Rather than insisting otherwise or dodging the argument altogether, we could heed the call to examine how altruism really manifests in our work, particularly when examined through the lens of what benefits the people we engage, rather than what benefits our organizations or our donors. Might we, too, have objective reasons for thinking we may be able to do more “good” in one program, or with one population, than in another? Do we, too, have a moral obligation to maximize that good? How would that change how we operate and who we serve? Do we want to change how we operate?\nIf the effective altruism debate makes anything clear, it’s that to be able to make art, not to mention argue about it, is to be fortunate. Taking a hard look at our assumptions about what draws and keeps us to this work may not be easy, but if we squirm a little, so be it. In the grand scheme of things, a little squirming is a luxury too.","Effective altruism is an area of practical moral philosophy that extends the ideas of altruism, i.e. selflessly doing good things for other people, to determine, through evidence and reason, the most good you can do for other people. It combines both philosophy and the scientific method to determine the most effective way for an individual to improve the world and gives compelling reasoning to incorporate this into their lifestyle.\nCharity ain’t giving people what you wants to give, It’s giving people what they need to get.\nYou are wealthier than you think\nAs of 2016, there are 767 million people living less than US$1.90 (in purchasing power) per day. That is, one in nine people on Earth do not have enough money to satisfy their or their family's hunger. Furthermore, the wealthiest 8 people now have as much wealth as the poorest 3.5 billion. The vast majority of the suffering endured by the very poorest in the world is due to starvation and illnesses for which there exists a cheap fast cure or prevention.\nClearly the wealthiest people in the world could donate a large proportion of their wealth without impacting their lifestyle significantly.\nGiven this context, how much do you think the wealthiest 1% should donate, as a percentage of their annual earnings, to help curb the suffering of the poorest people in the world?\nHow wealthy are you? The chances are that since you are wealthy enough to afford the means to be reading this right now, then you are one of the 5% wealthiest people on the planet, and probably much higher than that. If you would like to calculate more accurately the percentile that your earnings lie given the purchasing power in your country, you can use the online calculator here. At the time of writing, I am in the top 3.7% wealthiest people on Earth. Clearly, I, and most likely you, could live comfortably on significantly less money than we earn.\nYOU have the power to significantly reduce the suffering of many human beings (and non-human animals) who endure every day with a suffering you have likely never encountered. But do you have the moral responsibility to act on this?\nHow can I be morally responsible for the reduction of suffering in people I have never met?\nConsider the following scenarios:\nIf we can prevent something bad, without sacrificing anything of comparable significance, we ought to do it.\nYou are walking along the bank of a waist-deep lake and you are the only person for several miles. In the lake you see a young orphaned* child, who is drowning. Clearly it is very easy to wade into the water to attempt to save the life of the child. By most people's standards, to continue walking past without attampting to save the child's life would be morally reprehensible.\nNext, consider the same scenario, but this time in your pocket you have your smart-phone. If you were to wade in to save the child's life it would be at the cost of your mobile phone, which could be several hundred pounds depending on which model you own. Again, by most people's standards, to continue walking past without attampting to save the child's life, i.e. putting such a low price on a child's life, would be morally reprehensible.\nNow consider a similar scenario where the child is drowning, and you can save the child's life, at the expense of a new mobile phone, but you wouldnt be doing this directly. In other words, you can save a child's life indirectly, e.g. they are in a different country. Of course, whether the child is drowning in front of you or in a different location has no moral significance. If a child's life can be saved at the price of a new phone, regardless of where that child lives, or whether you can directly or indirectly save their life, it would be morally reprehensible not to do it.\nThis is the position we find ourselves in every single day. Every day about 17,000 children die from avoidable poverty related causes. On top of this, many millions more experience a huge amount of daily suffering through hunger or curable illness. Surely there is no morally significant difference between scenario 3 and the position we are in as extremely affluent people today. The implication of this is that if we are morally responsible to act to save the life of the child in scenarios 1, 2, and 3, then we are morally responsible to act to save the lives (or reduce the suffering) of the children, and adults, dying from preventable causes today.\nAren't I already a good enough person?\nwhat I would like is to change the culture of giving, so that it becomes unacceptable to be comfortably off and do nothing for the world's poor.\nPerhaps you are someone who doesn't steal things, doesn't hurt people, doesn't get angry when people make mistakes, and certainly doesn't judge people by the colour of their skin or their sexuality. These are certainly all necessary characteristics that a 'good person' should have; but are they sufficient? Is it sufficient just to not do wrong things?\nConsider the world 100 years ago, before the telephone, the television, the internet: a world vastly different from ours. What did it mean to be a 'good person' in 1917? It probably meant you were loving to your family and friends and you weren't unpleasant to anyone, stranger or not. These characteristics sound remarkably similar to the characteristics of a good person today. How can it be that almost all areas of life have progressed in the last 100 years, yet the characteristics that make someone a good person have remained broadly the same, i.e. don't do wrong things and you are a good person? I agree with the proposal that the characteristics of a 'good person' today require significant refinement.\nDonation of one's time, money, or energy to those less fortunate than oneself is generally considered as a deed that is good to do but not morally wrong not to do. A deed of this type is known as a supererogatory act. Examples of supererogatory acts include, for example, blood donation or a soldier jumping on an enemy grenade. A shift from supererogation to obligation of acts of charity would bring our characterisation of a good person up to the modern age. My view on this issue can be summed up nicely by the following quote:\nIn particular, donating one's time or money to those in more need than themselves is perhaps the most strikingly archaic supererogation**. That is, perhaps it is no longer sufficient to not do wrong things***, the donation of what we have surplus to our requirements should, to a degree, be part of what makes a person good.\n... our standards of distributive justice are far too minimalist and ... much of what is considered supererogatory in the transference of wealth from the rich to the poor should really be considered obligatory.\nHow can I do the most good?\nTo give away money is an easy matter and in any man's power. But to decide to whom to give it, and how large and when, and for what purpose and how, is neither in every man's power nor an easy matter.\nWe have briefly covered an argument for one to practice altruism in their life. Where does effective altruism go from here? Altruism becomes effective altruism when we use evidence and reason to determine the most altruistic thing to do in a given situation.\nFirstly, the use of the superlative \"most\" is not strictly appropriate here. Whilst effective altruism is a philosophy that aims to make the most altruistic decisions in life, it is practicably impossible to determine the most effective charity or the most altruistic career path to take. However, we can often improve the impact of our altruism, often by several orders of magnitude, by stepping back and making a better choice with the help of evidence and reason.\nOften we give (if we give at all) to causes based on emotion or relation to someone effected by a certain ailment. Either we know someone who has been effected by an affliction or we feel emotionally drawn to a particular cause due to empathy with an afflicted group. While giving money to charity for these reasons will usually be a good thing for the world, can we do better? Effective altruism says that we should put the impact of a donation to a particular cause before relation or emotion. To understand the reasons behind doing this, let's consider the spread of possible causes one could donate to and the impact this donation would have.\nWe might be inclined to think that the the majority of causes have high impact. For a given sum of donated money, we might expect that the distribution of impact it has across a range of problems to which it might be given, to look something like this: Here, most problems have tangible impact. There are many organisations (such as GiveWell, Giving What We Can, 80,000 hours, and Centre for Effective Altruism) that use scientific procedures to determine the impact of many charitable causes. These organisations find that the spread of impacts is actually more like this: Donations to the majority of causes have negligible impact compared to a few high-impact causes. The high-impact causes tend to be the treatment or prevention of easily curable diseases that cause a large amount of suffering to a large number of humans in third world countries. For these causes, a small amount of money donated goes a very long way. For example, one of GiveWell's top charities is the Against Malaria Foundation (AMF) that distributes long-lasting insecticide-treated mosquito nets to people in developing countries. It is estimated that a life is saved by this charity for every US$500-7000 donated, as well as the huge amount of suffering reduced in those who would have otherwise caught malaria but not died. Comparing this to the hundreds of thousands of dollars it would cost to pay a person in a first world country to go through cancer treatment to extend their life by a few months. Whilst it is undoubtably a good thing to give to the second cause, there is no reason that the life and suffering of a first-world human has any more importance than that of a third-world human. Therefore, it is several orders of magnitude more impactful to give to AMF****.\nThe most impactful cause is also an individual choice. It is impossible to fully detatch ourselves from the emotional reasons to support a cause. Different people think that different causes are most pressing; for example, one might argue that overpopulation is the most pressing global issue so our donations are better spent on education and distributing contraception rather than trying to save as many lives as possible. Alternatively, one might feel that activism to encourage others to eat plant-based foods rather than contributing to the slaughter of the 70 billion land animals that are killed for food each year is the most effective way to reduce suffering in the world. The key point is that you must donate with your head as well as your heart.\nWhere can I learn more...\nAbout effective altruism philosophy?\nFirstly, I would suggest either listening to this if you like podcasts or this if you prefer videos. The podcast is an episode from the series Philosophize This which I would also recommend as a good starting point to learn about other areas of philosophy. These resources provide good short introductions to effective altruism and how to start thinking about leading a life that is in line with this philosophy. Further to this, I would recommend signing up to the Centre for Effective Altruism's newsletter here, they send out some very interesting resources.\nAbout effective charities?\nIf you would like to learn more about which charities are the most effective, i.e. which charities give you the most bang for your buck, or if you are interested in learning about how we can use a scientific approach to quantify the impact a charity has and how it is much more than just a percentage of overhead costs then GiveWell is a good place to start. Also, there is a flowchart available here that gives a good step-by-step walkthrough to help determine your individual leanings to decide which effective causes are the most deserving of your donation.\nAbout my journey with effective altruism?If you would like to know more about how effective altruism has affected my life and how I try to align my decisions with this philosophy then sit and wait patiently for a future blog post that I plan to write about just this. Alternatively, you can contact me here.\n* To remove the complication of arguments based on the premise that one might be more inclined to save the child if they had a family who would suffer greatly at their loss.\n** While very interesting, a discussion about the utilitarian consequences of this statement (i.e. am I still obligated to donate if I am only marginally more fortunate than the people I would be giving to?) is largely irrelevant here because if you are in a position where you have the means to read this blog, you can likely give a large proportion of your income without it affecting your lifestyle significantly. A discussion about utilitarianism and effective altruism can be found by Singer, 1972.\n*** That being said, we are far from an age where people don't do wrong things. Even in developed countries, we still have a significant number of murders, rapes, and other acts that cause immense suffering to other humans. Also, the vast majority of people still contribute to the slaughter of the 1 million land animals for meat, dairy, and egg consuption and the 20 million fish for food and fish oil products every ten minutes which must cause an indescribable amount of suffering.\n**** One could argue further that a donation to an organisation trying to make more people donate large sums to effective charities would be even more effective because it has the broad effect of multiplying any donation by the number of people that donation influences to donate to effective charities. This is, however, very difficult to quantify.\n- WORLD BANK GROUP, 2016: Poverty and Shared Prosperity 2016: Taking on Inequality, World Bank Publications, p.184.\n- Hardoon, D., 2017: An Economy for the 99%: It’s time to build a human economy that benefits everyone, not just the privileged few. Oxfam Briefing Papers.\n- WORLD HEALTH ORGANISATION, 2011: Child mortality: Millenium Development Goal (MDG) 4. Viewed 19th Feb 2017.\n- Heyd, D., 2016: Supererogation, The Stanford Encyclopedia of Philosophy (Spring 2016 Edition), Edward N. Zalta (ed.).\n- GIVEWELL, 2016: Against Malaria Foundation: Cost per life saved.\n- COMPASSION IN WORLD FARMING, 2017: Strategic Plan 2013-2017: For Kinder, Fairer Farming Worldwide.\n- Singer, P., 1972: Famine, Affluence, and Morality, Philosophy and Public Affairs 1 (3):229-243. Online here\n- FISH COUNT, 2014: Fish Count Estimates. Viewed 11th Feb 2017."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:83bc055e-337a-457c-a230-bf02b8b50555>","<urn:uuid:b156cc12-0155-4234-9cb6-41a7d3e7761a>"],"error":null}
{"question":"Hey there! Could you explain how intrusive thoughts affect our brain's processing vs. how they manifest in kids specifically with OCD?","answer":"Intrusive thoughts affect brain processing by activating the amygdala (the fear center), causing the brain to treat them as real threats. This triggers a fight-or-flight response, leading to heightened perception of danger and making the thoughts feel incredibly real. The brain's processing can also blend these thoughts with personal memories, intensifying their emotional impact. In children with OCD specifically, these intrusive thoughts manifest as various obsessions, such as exaggerated fears of contamination, excessive doubts about completing tasks, marked overconcern with appearance of homework, or fears of accidentally harming others. Children may respond to these thoughts with compulsive behaviors like repeated washing, ritualized movements, specific bedtime rituals, or seeking constant reassurance. This can lead to secondary problems including academic difficulties, behavioral issues, peer conflicts, and family tensions.","context":["Intrusive thoughts can be incredibly unsettling. They creep into our minds, unwelcome and disturbing. But what’s even more perplexing is how real they feel. These vivid and unwanted thoughts often trigger intense emotions, causing us to question our sanity. So, why do intrusive thoughts feel so real?\nThe answer lies in the way our brains process information. When faced with intrusive thoughts, our brains treat them as real threats, activating the amygdala, the fear center of the brain. This heightened brain activity leads to a heightened perception of danger, making the intrusive thoughts feel incredibly lifelike.\nFurthermore, intrusive thoughts can blend with personal memories, tapping into deep-seated emotions and experiences. This connection intensifies their impact, reinforcing their sense of reality. Additionally, intrusive thoughts often reflect our deepest fears and anxieties, resonating with our most primal emotions. They may even tap into hidden desires or forbidden thoughts, further amplifying their impact.\nUnderstanding the psychology behind why intrusive thoughts feel so real is essential for managing their impact on our mental well-being. By recognizing the nature of intrusive thoughts, understanding the brain processing involved, and addressing the connections to personal memories, fears, and desires, we can regain control over our thoughts and lead healthier lives.\n- Intrusive thoughts feel real because our brains treat them as real threats, activating the fear center of the brain.\n- These thoughts can blend with personal memories, intensifying their impact and reinforcing their sense of reality.\n- Intrusive thoughts often reflect our deepest fears and anxieties, resonating with our most primal emotions.\n- Understanding the psychology behind intrusive thoughts is crucial for managing their impact on our mental well-being.\n- By recognizing their nature, addressing brain processing, and exploring personal connections, we can regain control over our thoughts.\nThe Nature of Intrusive Thoughts\nIntrusive thoughts are unwelcome and often disturbing thoughts that can occur repeatedly and intrude into a person’s consciousness. These thoughts can involve various themes, such as harm, contamination, or uncertainty. They can feel extremely real and are often characterized by a sense of urgency and danger.\nFor individuals experiencing intrusive thoughts, it can be challenging to differentiate between these intrusive thoughts and actual reality. The vividness and intensity of these thoughts make them difficult to dismiss or ignore. They can consume a person’s attention and impact their emotions, leading to significant distress.\nUnderstanding the nature of intrusive thoughts is crucial for addressing their impact on mental health and well-being. By gaining insight into why these thoughts occur and the factors that contribute to their intensity, individuals can develop effective strategies for managing them.\nThe Themes of Intrusive Thoughts\nIntrusive thoughts can manifest in a variety of themes, although their specific content may vary from person to person. The following are common themes observed in intrusive thoughts:\n- Harm-focused thoughts: Thoughts of causing harm to oneself or others\n- Contamination-related thoughts: Fears of contamination or becoming infected, leading to excessive cleanliness rituals or avoidance behaviors.\n- Uncertainty and doubt: Persistent doubts and uncertainties about everyday decisions, causing excessive rumination and anxiety.\n- Sexual or taboo thoughts: Intrusive thoughts related to inappropriate or taboo subjects\nThese themes are not exhaustive, and intrusive thoughts can take many other forms. It is essential to recognize that even though these thoughts may feel real, they do not necessarily reflect the true desires, intentions, or character of the individual experiencing them.\nUnderstanding the Impact\nThe intense realism of intrusive thoughts can have a profound impact on mental health and overall well-being. They can lead to heightened anxiety, depression, and impaired daily functioning. Individuals may develop avoidance behaviors to manage the distress caused by these thoughts, further exacerbating the impact on their quality of life.\nMoreover, the persistent nature of intrusive thoughts can be emotionally exhausting, leading to increased stress and feelings of helplessness.\nThe Role of Brain Processing\nResearch suggests that the brain processes intrusive thoughts in a similar way to real threats. The amygdala, the brain’s threat detection center, is activated, leading to the perception of danger and triggering a fight-or-flight response. This response is a survival mechanism designed to protect us from harm. However, in the case of intrusive thoughts, the brain is interpreting them as genuine threats, even if they are not based on real events or situations.\nThis heightened brain activity contributes to the vividness and realism of intrusive thoughts. As a result, intrusive thoughts can elicit strong emotional responses and contribute to the perception that they are immediate and imminent dangers.\nUnderstanding the Fight-or-Flight Response\nThe fight-or-flight response, triggered by the amygdala, is a physiological reaction that prepares the body to either confront a threat or flee from it. This response causes various bodily changes, such as increased heart rate, heightened senses, and the release of stress hormones like adrenaline and cortisol.\nWhen the amygdala perceives a threat, it activates this response, even if the threat is only a thought. As a result, the brain’s processing of intrusive thoughts mirrors its response to real threats, leading to a heightened sense of their realism.\nDr. Samantha Martinez, a psychiatrist specializing in anxiety disorders, explains, “During the fight-or-flight response, the brain directs resources towards dealing with the perceived threat, which further reinforces the intensity and reality of intrusive thoughts.”\nUnderstanding the role of brain processing in the experience of intrusive thoughts can provide valuable insights into their nature and why they feel so real. By recognizing the physiological processes involved, individuals can develop strategies to better manage and cope with intrusive thoughts.\nThe Connection to Personal Memories\nIntrusive thoughts have a unique ability to intertwine with personal memories. This connection occurs because these thoughts often tap into deeply ingrained emotions and experiences, leveraging the brain’s tendency to associate them with personal moments.\nThe brain treats these thoughts as significant events, triggering intense emotions and a strong belief in their authenticity. This blending of intrusive thoughts and personal memories can be particularly challenging to manage, as it creates a powerful cognitive and emotional fusion.\nThe Role of Emotional Associations\nOne of the reasons why intrusive thoughts and personal memories become intertwined is the emotional associations they share. Personal memories have the ability to evoke strong emotions, whether positive or negative, due to the experiences and feelings attached to them.\nIntrusive thoughts, with their often disturbing and unwanted nature, tap into these emotional associations. They exploit the brain’s tendency to link thoughts with significant emotional experiences, which intensifies their impact and reinforces their perceived reality.\nRecognizing the Connection\nRecognizing the connection between intrusive thoughts and personal memories is crucial for effectively managing their impact. By understanding that intrusive thoughts can hijack emotionally charged memories, individuals can develop strategies to untangle the fusion and regain control over their thoughts.\nTherapy techniques such as cognitive restructuring and exposure therapy can help individuals separate intrusive thoughts from personal memories.\nBy challenging distorted beliefs and engaging in controlled exposure to triggering stimuli, individuals can gradually replace the emotional associations between intrusive thoughts and personal memories with healthier and more accurate perceptions.\nStrategies for Managing Intrusive Thoughts\nWhen it comes to managing intrusive thoughts, there are several strategies that individuals can employ to regain control over their thinking patterns. These strategies can help alleviate the intensity and frequency of intrusive thoughts, promoting improved mental well-being.\nHere are some effective approaches:\n1. Cognitive-Behavioral Therapy (CBT) Techniques\nCognitive restructuring and exposure and response prevention are commonly used CBT techniques for managing intrusive thoughts. Cognitive restructuring involves challenging and changing negative thought patterns, while exposure and response prevention focuses on gradually confronting and resisting the urge to engage in compulsive behaviors triggered by intrusive thoughts.\n2. Mindfulness and Grounding Exercises\nMindfulness exercises, such as meditation and deep breathing, can help individuals develop a present-moment awareness that reduces the power of intrusive thoughts. Grounding exercises, such as focusing on the senses or engaging in physical activities like walking or stretching, can also help distract from and diminish the impact of intrusive thoughts.\n3. Seeking Professional Help\nFor individuals struggling with intrusive thoughts, seeking professional help from a therapist or counselor can provide invaluable guidance and support. A trained mental health professional can assist in developing personalized strategies tailored to the individual’s specific needs and circumstances.\nClick the banner below to get access to a professional and start your online therapy journey!\nBy incorporating these strategies into their daily lives, individuals can gradually gain control over intrusive thoughts and improve their overall quality of life. It’s important to remember that managing intrusive thoughts is an ongoing process, and it may take time to find the strategies that work best for each individual.\nConclusion: Taking Control of Intrusive Thoughts\nIntrusive thoughts can feel incredibly real and significantly impact an individual’s mental well-being. Understanding the psychology behind the realism of these thoughts is key to effectively managing them. By recognizing their nature, understanding brain processing, and addressing connections to personal memories, fears, and desires, individuals can regain control over their thoughts.\nIt is important to remember that taking control of intrusive thoughts is a process. Through therapy and the implementation of effective strategies, individuals can gradually reduce the impact of intrusive thoughts and improve their overall mental well-being.\nWhat are Intrusive Thoughts?\nIntrusive thoughts are unwelcome and often disturbing thoughts that can occur repeatedly and intrude into a person’s consciousness.\nWhy Do Intrusive Thoughts Feel So Real?\nIntrusive thoughts can feel incredibly real because the brain treats them as real threats, leading to a heightened feeling of their reality.\nHow Does the Brain Process Intrusive Thoughts?\nResearch suggests that the brain processes intrusive thoughts in a similar way to real threats, activating the amygdala, the brain’s threat detection center, and triggering a fight-or-flight response.\nHow Do Intrusive Thoughts Blend with Personal Memories?\nIntrusive thoughts can blend with personal memories because they often tap into deeply ingrained emotions and experiences, intensifying their emotional impact and reinforcing their sense of reality.\nAre Intrusive Thoughts Linked to Individual Fears and Desires?\nYes, intrusive thoughts are often linked to individual fears and desires, reflecting a person’s deepest anxieties and tapping into hidden desires or forbidden thoughts.\nWhat Strategies Can Help Manage Intrusive Thoughts?\nCognitive-behavioral therapy (CBT) techniques, mindfulness exercises, and seeking professional help from a therapist or counselor are effective strategies for managing intrusive thoughts.\nHow Does Therapy Help in Managing Intrusive Thoughts?\nTherapy plays an important role in managing intrusive thoughts by providing individuals with tools, techniques, and coping mechanisms to challenge and change negative thought patterns, as well as exploring the underlying causes of intrusive thoughts.\nHow Can I Take Control of Intrusive Thoughts?\nBy understanding the nature of intrusive thoughts, the brain processing involved, addressing the connection to personal memories, fears, and desires, seeking therapy, and adopting strategies for managing intrusive thoughts, individuals can take steps to regain control over their thoughts and improve their mental well-being.","OCD in Children and Adolescents – Symptoms and Treatment\nUnfortunately, many people, including many psychotherapists, mistakenly think that OCD in children and adolescents is rare. As a result, children and adolescents with OCD are frequently misdiagnosed with depression, ADHD, conduct disorder, or other conditions. In actuality, childhood-onset OCD is fairly common, occurring in approximately 1% of all children. Furthermore, recent research indicates that approximately half of all adults with OCD experience clinical symptoms of the disorder during their childhood.\nSymptoms of Childhood and Adolescent OCD\nSymptoms of childhood-onset OCD vary widely from child to child. Some common obsessions experienced by children and adolescents with OCD include:\n- exaggerated fears of contamination from contact with certain people, or everyday items such as clothing, shoes, or schoolbooks\n- excessive doubts that he/she has not locked the door, shut the window, turned off the lights, or turned off the stove or other household appliance\n- marked over-concern with the appearance of homework assignments\n- excessive worry about symettrical arrangement of everyday objects such as shoelaces, school books, clothes, or food\n- fears of accidentally harming a parent, sibling or friend\n- superstitious fears that something bad will happen if a seemingly unconnected behavior is done (or not done)\nSome common compulsions experienced by children and adolescents with OCD include:\n- Compulsive washing, bathing, or showering\n- Ritualized behaviors in which the child needs to touch body parts or perform bodily movements in a specific order or symmetrical fashion\n- Specific, repeated bedtime rituals that interfere with normal sleep\n- Compulsive repeating of certain words or prayers to ensure that bad things don’t occur\n- Compulsive reassurance-seeking from parents or teachers about not having caused harm\n- Avoidance of situations in which they think “something bad” might occur\nIn addition to the above symptoms, children and adolescents with OCD may exhibit secondary problems that arise due to the impact of their obsessions and/or compulsions on daily functioning. Academic difficulties, behavioral problems, peer conflicts, sleep disturbances, and family conflict are just some of secondary symptoms that frequently appear in children and adolescents with OCD. Unfortunately, these difficulties often become the focus of therapeutic attention, while the OCD that actually leads to these problems remains untreated.\nGenetics and OCD in Children and Adolescents\nAccording to a recent report from the International Obsessive-Compulsive Foundation (IOCDF), “there is evidence that OCD which begins in childhood may be different than OCD that begins in adulthood. Individuals with childhood-onset OCD appear much more likely to have blood relatives that are affected with the disorder than are those whose OCD first appears when they are adults”. In fact, one recent study found that children with OCD are much more likely to have a close relative with OCD when compared to the general population. This finding suggests that, while the cause of OCD is not fully understood, genetics plays a significant role in the development of OCD symptoms, and that the condition appears to be heritable. In fact, recent research has uncovered six specific genes which appear to play a role in the development of OCD. Unfortunately, researchers do not yet understand the exact mechanism that connects these genes to the onset of OCD symptoms.\nHowever, it is crucial to note that there is a learning component to OCD as well. Like adults, children and adolescents with OCD perform compulsions and/or avoidant behaviors in an effort to reduce the anxiety caused by unwanted obsessions. While this may at first reduce the anxiety and obsessions, it actually reinforces and worsens both in the long-term. This in turn leads to more compulsions, which leads to even more obsessions and more anxiety. In the course of going through this cyclical process, the child develops a “learned response” in which he/she automatically become anxious in reaction to specific thoughts, objects or situations. While the specific thoughts and behaviors of OCD may vary with each individual, this process, called the Obsessive-Compulsive Cycle, is identical. For children and adolescents with OCD, this can become a near-constant cycle of obsessive thoughts and compulsive behaviors that significantly interferes with daily functioning at home and school. To learn more about this Obsessive-Compulsive Cycle, click here.\nChildhood OCD and PANDAS\nOver the past 15 years, researchers have published a number of studies suggesting a possible link between common strep infections and sudden-onset OCD and tics in children. This relatively unknown syndrome has been called pediatric autoimmune neuropsychiatric disorders associated with streptococcal infections (PANDAS).\nResearch in this area was initiated when reports surfaced that some children with strep throats were experiencing the sudden onset of severe OCD, as well as tics similar to those seen in Tourette’s Disorder. Clinicians and researchers discovered that when these children received a course of antibiotics for their strep throats, their OCD and tic symptoms also decreased significantly.\nResearchers then discovered that children with strep and rapid-onset OCD had increased levels of the antibodies associated with strep infection. Later research found that children with this syndrome exhibited brain swelling similar to that seen in situations in which the brain has been compromised by the body’s immune system in diseases such as Sydenham’s Chorea. More recent research has supported the thesis that for some children, exposure to the strep antibodies (not the strep virus itself) leads to neurological changes and development of OCD and tics in these cases.\nor call us at 310 824-5200\nWhile these findings are controversial, it appears that, in some cases at least, the rapid-onset of OCD and tics in children may be caused by the immune system doing what it is designed to do – creating antibodies to fight infection. This research contributes to a growing body of evidence that indicates that OCD may in some cases have its origins in neurological and biological pathology. To learn more about this research, click here.\nFor many children with PANDAS-based OCD, the treatment is the same Cognitive Behavioral Therapy (CBT) used to treat other types of OCD. The only difference would be during an active strep infection, in which case the strep is treated with antibiotics. In those situations, one could expect a reduction in OCD symptoms as the strep infection is reduced as a result of the antibiotic. So the only time the treatment would vary from standard OCD treatment would be when a child has a strep throat.\nUnfortunately, antibiotics and CBT are not always successful in reducing the severe OCD symptoms that are experienced by children with PANDAS. This has led to the development of other more controversial treatments. One such approach called plasmapheresis involves intravenous injections of immunoglobin (blood plasma) from healthy donors. Unfortunately, research studies on plasmapheresis completed thus far have had contradictory results, with some children experiencing little or no benefit, along with significant side effects. Investigators continue to research this and other alternative approaches as potential treatment options for Childhood OCD related to strep infections.\nTreatment for Children and Adolescents with OCD\nFortunately, for most children and adolescents with OCD, the basic principles of treatment are the same as for adults with OCD. A specific type of Cognitive-Behavioral Therapy (CBT) known as “Exposure and Response Prevention” (ERP) has been found by researchers to be the most effective treatment for OCD in children, adolescents, and adults. Another CBT technique that is extremely valuable in the treatment of children and adolescents with OCD is called “Cognitive Restructuring”. And a variant of ERP has been developed that has also been found to be extremely effective with the “pure obsessional” symptoms often seen in children and adolescents with OCD. This method, called imaginal exposure, involves writing short stories based on the child’s obsessions. When combined with standard ERP and Cognitive Restructuring, imaginal exposure can greatly reduce the frequency and magnitude of the child’s symptoms. To learn more about Cognitive-Behavioral Therapy for the treatment of OCD in children and adolescents, click here.\nIt is important to note that, while the basic treatment for children and adolescents with OCD and related conditions is the same as for adults, childhood-onset OCD presents specific age-related issues that complicate treatment. Many children and adolescents with OCD do not yet have the emotional and cognitive skills to fully address their irrational fears and compulsive behaviors. As such, they may have difficulty identifying and articulating their fears and/or why they feel compelled to do certain behaviors. They also may not recognize that their fears are exaggerated or unrealistic. Furthermore, children with OCD may be resistant to discussing these problems with anyone, even their parents. Likewise, children and adolescents with OCD may anticipate being very uncomfortable or frightened by the prospect of discussing these issues with a psychotherapist. It is not unusual for children and adolescents with OCD to exhibit “magical thinking” in which they believe that their fears will come true if they talk about them with a therapist (or anyone). Others may deny symptoms, or want to avoid dealing with them in the hope that their OCD will just go away by itself. And some children just want to avoid dealing with their OCD due to embarrassment or shame.\nAs a result of these issues, it is critical that children and adolescents with OCD be treated by a therapist who not only specializes in the treatment of OCD, but also specializes in the treatment of children and adolescents. If your child or adolescent is experiencing any of the above OCD symptoms, and you would like to discuss treatment at the OCD Center of Los Angeles, you can call us at (310) 824-5200 (ext. 4), or click here to email us. If you live outside Southern California, we recommend that you contact a licensed Cognitive-Behavioral therapist in your local area.\nIntensive Treatment for Children with OCD and Anxiety\nWe also offer an intensive treatment program for children and adolescents suffering with OCD and other anxiety issues. This program is designed to meet the needs of kids for whom standard outpatient treatment is either unavailable or insufficient. Our intensive outpatient program is ideal for children and adolescents from other states or countries who cannot find effective treatment near to their homes, and for kids whose symptoms require a more rigorous treatment protocol. Please note that some younger children may not be developmentally appropriate for intensive treatment. To learn more about our intensive outpatient treatment program, click here.\nAbout the OCD Center of Los Angeles\nThe OCD Center of Los Angeles is a private outpatient treatment center specializing in the treatment of OCD and related anxiety based conditions. We treat adults, adolescents, and children, and offer services six days a week, including evenings and Saturdays. We have offices at four locations in Southern California:\n- Woodland Hills\n- Orange County\n- Santa Barbara\nFor more information on treatment for your child or adolescent with OCD or a related anxiety based condition, please call one of our client coordinators at (310) 824-5200 (ext. 4), or click here to email us."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b4948096-6aec-49be-a1ef-9c52fd9f1525>","<urn:uuid:405893f6-0ed9-4705-ad93-ae4df85dea4c>"],"error":null}
{"question":"As a policy researcher, I'm looking into modern security innovations. How do surveillance and monitoring systems compare between agricultural and national security applications?","answer":"In both agricultural and national security contexts, surveillance and monitoring systems play crucial roles but serve different purposes. In national security, Canada employs comprehensive monitoring through agencies like RCMP and CSIS, using tools such as physical surveillance, terrorism peace bonds, and coordinated information sharing through the National Security Joint Operations Centre. In vertical farming, monitoring is done through automated control systems that use sensors and imaging techniques combined with crop simulation models and artificial intelligence. These systems monitor environmental conditions like temperature, humidity, and nutrient levels. Both sectors use technology for preventive measures - national security focuses on preventing terrorist activities, while agricultural monitoring prevents crop diseases and optimizes growth conditions.","context":["Mitigating the Threat Posed By Canadian Extremist Travellers\nCanadian Extremist Travellers (CETs), are individuals suspected of travelling abroad to engage in extremist activity. Extremist activity is defined as any activity undertaken on behalf of, or in support of, a terrorist entity. It can include, but is not limited to: participation in armed combat, financing, radicalizing, recruiting, media production, and other activity.\nOffences specifically related to leaving or attempting to leave Canada for the purposes of committing certain terrorism offences are enacted in the Criminal Code.\nThe threat of extremist travellers is significant and presents difficult challenges to both Canada and our allies. However, the Government of Canada has taken many steps to ensure we are ready to address and mitigate this threat and protect Canadians.\nHow are Canadians protected from CETs who return to Canada?\nCriminal prosecution is a top priority and the preferred course of action in Canada's approach to dealing with returning extremist travelers.\nThe first objective in dealing with returning extremist travellers is arrest and prosecution to the full extent of the law. If there is sufficient evidence, the Government of Canada will pursue charges. If there is insufficient evidence for a charge, the Royal Canadian Mounted Police (RCMP) and its law enforcement, security and intelligence partners will continue their investigation, while other tools are leveraged to manage and mitigate the threat.\nThese tools include:\n- using a terrorism peace bond to have the court place conditions on the individual (including electronic monitoring); active physical surveillance;\n- using the Secure Air Travel Act to prevent travel to commit terrorism offences and threats to transportation security; and/or cancelling, refusing or revoking passports.\n- In certain circumstances, the Canadian Security Intelligence Service (CSIS) may also employ threat reduction measures to reduce the threat posed by an individual.\nCanada's law enforcement, security and intelligence, and defence departments and agencies continue to monitor and respond to the threat of Canadian extremist travellers through a coordinated, whole-of-government approach.\nWhen the Government learns that a CET is seeking to return, federal departments and agencies come together to tailor an approach to address the threat he/she may pose. Key departments and agencies work together to assess risks, develop options and manage the return of CETs.\nThese agencies are:\n- Public Safety Canada,\n- Global Affairs Canada (GAC),\n- the Royal Canadian Mounted Police (RCMP),\n- the Canadian Security Intelligence Service (CSIS),\n- the Integrated Terrorism Assessment Centre (ITAC),\n- the Department of National Defence and the Canadian Armed Forces (DND/CAF),\n- Canada Border Services Agency (CBSA),\n- Immigration, Refugee and Citizenship Canada (IRCC),\n- Transport Canada (TC),\n- the Privy Council Office (PCO).\nThe whole-of-government approach enables the collective identification of measures needed to manage the threat posed by these individuals.\nThe Government's Operational Response\nThe RCMP leads a National Security Joint Operations Centre (NSJOC), which includes several federal departments and agencies to enhance the Government's operational response to individuals seeking to travel abroad for terrorism purposes.\nThe NSJOC facilitates timely information sharing and provides direct support to national security criminal investigations by the RCMP-led Integrated National Security Enforcement Teams (INSETs).\nIn addition to the contribution to the NSJOC, CSIS investigates threat activities of CETs and their potential for return to Canada. CSIS uses all available resources under its mandate, working with partner agencies, including disclosing information to law enforcement organizations in their investigations or prosecutions of CETs and using measures to reduce the threat, when reasonable and proportionate to the nature of the threat.\nAnother mechanism to address and mitigate the threat of CETs is to prevent individuals from radicalization to violence in the first place. To that end, Public Safety Canada has established the Canada Centre for Community Engagement and the Prevention of Violence (the Canada Centre), which is mandated to lead, support, and coordinate Canada's prevention efforts.\nThese efforts include local-level programming aimed at countering radicalization to violence. The Canada Centre also funds innovative research, such as the project led by the University of Waterloo to better understand why Canadians travelled to Syria and Iraq to join terrorist groups.\nThe RCMP has long standing efforts to counter radicalization to violence which encompass all forms of violent ideologies. For instance, the RCMP's First Responder Terrorism Awareness Program provides training for first responders and key partners to promote awareness about a range of criminal threats. The training helps partners identify the early signs of radicalization to violence and outlines possible responses.\nSome returning CETs may also be suitable for programs which are designed to help them to disengage from violent extremism. These programs do not replace or prevent our security and law enforcement agencies from doing their work. Rather, these programs complement the work of these agencies by helping to reduce the threat posed by returning extremists travellers, while also addressing the health and social problems of associated travellers, including family members and children, returning from a conflict zone.\nThese programs are not limited to returning extremist travellers but may also be used forindividuals in Canada who are radicalizing to violence, including right-wing extremists.\nCanada's security intelligence agencies work in close cooperation with international partners, including the Five Eyes, the G7, the European Union, Interpol and the United Nations to enhance information sharing and share best practices, while ensuring that information is not disclosed to, or requested from, foreign entities if there is a substantial risk that it could result in mistreatment.\nHow is the Government made aware when a Canadian extremist traveller is planning a return?\nThe Government can be made aware of an extremist traveller attempting to return to Canada in many ways, including through Canada's intelligence, security and law enforcement agencies, as well as key international partners such as the Five Eyes and INTERPOL.\nDo Canadian extremist travellers have a right to return?\nUnder section 6 of the Charter, every citizen has the right to \"enter\" Canada. This is a right of entry if a citizen of Canada presents themselves at a Canadian port of entry. The section 6 right to enter Canada does not obligate the government to intervene to assist Canadians abroad in their efforts to leave a foreign country and return to Canada, except where a citizen needs an emergency passport or other travel document to allow them to return.\nSome CETs have admitted, on social media, leaving Canada to join a terrorist organization. Why can’t we prosecute them under the Anti-terrorism Act?\nCriminal prosecution is a top priority and the preferred course of action.\nNational security investigations pose unique and highly complex challenges to law enforcement agencies. These challenges include the need to gather information on an individual's alleged activities in a hostile environment to a standard that can be expected to withstand the rigours of the Canadian justice system.\nIn instances where charges cannot be laid, all other options to address extremist travellers may be pursued, including: using the Secure Air Travel Act; the cancellation, refusal and revocation of passports; threat reduction measures; and terrorism peace bonds.\n- Date modified:","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d9c11876-0562-4a73-8839-53580418a3d0>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"Does Asian text flow vertically, and when must timestamp data be stored in GMT format? ⌚️","answer":"Asian text can flow vertically in three ways: rotated 90 degrees right (bottom-to-top), rotated 90 degrees left (top-to-bottom), or in Asian layout mode (top-to-bottom, right-to-left). Regarding timestamps, they should always be stored in GMT/UTC/Zulu format or with full ISO 8601 encoding including timezone information. This practice ensures proper localization across different regions and protects against issues that could arise from server configurations, migrations, and scaling operations.","context":["Here is something I’ve been working on for the past few weeks. Since I just checked in the first version of this feature into ooo-build master, it’s probably a good time for me to talk about this.\nThis feature introduces a new justification option for cell text known as the “distributed justification”, where the left and right edges of the text are aligned with the left and right edges of the bounding box by adjusting space between characters (inter-character spacing), rather than space between words (inter-word spacing), across the entire width of the bounding box. This type of distributed text justification makes little sense for Latin-based languages such as English, French and German, but makes a big difference for Asian languages such as Japanese. The reason the normal justification doesn’t work for Asian languages is because, in those languages, you don’t put spaces between individual word boundaries, and the normal justification relies on presence of spaces at word boundaries. This is where the distributed justification comes into play.\nThis distributed justification method is commonly known as ?????? in Japanese, and is said to be one of the blockers when attempting to migrate users away from Excel to Calc.\nFirst and foremost, I’d like to cover the horizontal justification. The following screenshot shows the difference between the three horizontal alignment modes:\nAs you can see, in the normal left-aligned text, the right edges of the lines are not aligned. When the text is justified, the right edges of the lines are now aligned by adjusting the inter-character spacing, except for the last line, which remains left-aligned. When the text is distributed, even the right edge of the last line becomes aligned with the right edge of the bounding box by equally distributing the characters on that line.\nTo allow this new justification type, I added a new justification type Distributed to the existing Cell Formatting dialog.\nFor the vertical alignment setting, I’ve added two new options Justified and Distributed, to support justification in the vertical direction.\nJustifying Asian text mixed with Latin text\nWhile working on this feature, I have decided to also tweak the normal justification algorithm to make it work slightly better for Asian text mixed with Latin text such as English. As I mentioned earlier, distributed justification is not really ideal for Latin text. But with the society becoming more and more global, we are seeing more and more Asian text intermixed with Latin text, and vise versa. And correctly justifying a text having mixed script types requires using different justification methods for their respective script types. After a bit of trial and error, I think I got it right. You can see the result in the following screenshot:\nThe English portion of the text is justified by inter-word justification, whereas the Japanese portion is justified by inter-character justification. The spaces between the English and Japanese text portions are also slightly adjusted in this scheme.\nNow, let’s move on to the vertical justification. When you justify a text in the vertical direction, that is, in the direction perpendicular to the direction of text flow, the spacing between the lines gets adjusted so that the top and bottom lines get aligned with their respective edges of the bounding box, like so:\nThe top cell shows text with default justification, while the bottom cell shows text with vertical justification.\nThe Cell Format dialog itself provides both Justified and Distributed options for the vertical justification setting, but they do exactly the same thing for horizontally-flowing text. For vertically-flowing text, on the other hand, they do different things, but more on that in the next section.\nJustifying vertically flowing text\nNow, you can also justify text even when the text is flowing vertically. There are three ways you can make the text flow vertically. You can either\n- rotate 90 degrees to the right (bottom-to-top),\n- rotate 90 degrees to the left (top-to-bottom), or\n- switch to Asian layout mode, which flows text in the top-to-bottom, right-to-left direction.\nIn these modes, the Justified and Distributed vertical justification options do have different effects. The following screenshot demonstrates different vertical alignment settings in three different vertical flow modes.\nAs an added bonus…\nThe code responsible for the text layout, the code where I made my modification to support this feature, is actually shared between Calc, Draw and Impress. Calc uses it to render complex cell text, while Draw and Impress use it for their text box objects. This means that, any improvement I make in this area will automatically be made available for all three applications. All that needs to be done is to simply adjust the UI in each app and add hooks in their respective import/export filters. Whether or not I’ll work on that during this cycle is another question. Having said that, I’d like to eventually get that done, and I’d like to do it sooner rather than later. But we’ll see how that goes.\nBut even without making the extra code change in the Draw/Impress code, my change so far was enough to fix this bug which I didn’t even know existed. :-)\nAs of this writing, I’m not entirely done with this feature yet. I still have to cover some corner cases, and I still need to fix some bugs which I unfortunately discovered while taking screenshots for this post. So, stay tuned for further fine-tuning!","With the world becoming more and more connected, with better support for global users and global businesses, globalization (g11n), internationalization (i18n) and localization (l10n) are more commonly being viewed as functional requirements vs nonfunctional ones. Everyone is focused on supporting RTL layouts, date-time formats, and translations. But here are some commonly-overlooked internationalization challenges that nearly every site build misses on the first try.\nBut first, some definitions…\nEveryone has heard internationalization and localization bantered around, but if you ask what they really mean you often get some blank stares. So far the best definitions I’ve found are from the World Wide Web Consortium (W3C):\nLocalization refers to the adaptation of a product, application or document content to meet the language, cultural and other requirements of a specific target market (a locale).\nInternationalization is the design and development of a product, application or document content that enables easy localization for target audiences that vary in culture, region, or language.\n— Localization vs. Internationalization (https://www.w3.org/International/questions/qa-i18n)\nMost tools and frameworks out there do a great job of localizing content once it’s been added to a system. They also tend to offer good support for internationalization out of the box with language contexts, timezone support, and layout changes based on language. However, like any tool, just because a solution can do something well doesn’t always mean it is being used correctly. These common mistakes have plagued nearly every internationalized project I’ve ever worked on.\nDates and times\nYep, believe it or not these can be a real problem. I’m not talking about simply localizing the output but actually collecting the data. By allowing users to input raw date and time values (is 9/10/2015 September 10 or October 9?) you can introduce inconsistency in the values stored in the system. Same goes for time where people could be reporting time in 24-hr format vs AM/PM and you have to account for both formats. Oh, and don’t forget many cultures don’t use the Gregorian calendar common in the Western hemisphere.\nThe simple solution? Use some kind of date and time picker widgets that provide the software with a consistent date and time format that it can then act on.\nClosely associated with collecting date-time values, another pain point is often timezones. Now that you’ve collected a consistently-formed date and time, what timezone is it for? What timezone is that integer timestamp being stored in your database using? More often than not they are based on the server timezone, which isn’t necessarily the timezone your users are looking for – and is something that can change based on server migrations, scaling clusters, etc.\nThe fix? Store all values in GMT/UTC/Zulu (pick your flavor) which is basically no time offset, or store the full ISO 8601 encoding which includes the timezone. This makes localization a straightforward matter in pretty much any programming language and removes any ambiguity. It also insulates your data and software from side effects introduced by server configurations, migrations, and scaling operations.\nThis might seem to be obvious, but most countries in the world do not conform to the same address structure as the United States Postal Service’s Publication 28. I know, crazy talk, right? That means providing free form textboxes or hardcoding city/state/zip as address collection mechanisms are not only antipatterns for localization, they also make any kind of geocoding nearly impossible which is a requirement for many systems today that provide automated localization.\nWhat are your options? Base your address-collection solutions on the UPU S42 Standard which should cover most of the world’s addressable locations (UPU claims 192 countries as members). This will provide a comprehensive set of address components that can also be provided to geocoding and shipping automation systems, and a storage schema that can accommodate anything thrown at it.\nAge? For real? How could that be messed up, right? Surprisingly to those of us in the Western hemisphere, not every culture counts the day you were born as Time Zero. Some count the time in the womb, some consider you a year old the day you are born, etc ad nauseum. Generally speaking this isn’t an issue unless you’re dealing with regulatory age restrictions or a marketing and/or service strategy based on actual age, but it does come up. It can also wreak havoc in analytics as you need to consistently group users in the proper age demographic.\nEasy fix? Ask for their birthdate (or birth year if there’s a privacy concern), not their age. Not only can you now easily calculate their age going forward, but you remove any ambiguity regarding their time on Earth and can provide properly localized ages in user profiles.\nLocalization is hard. With all the different cultures, languages, and locales out there it’s tough even when you have everything you need. Make sure you back up the localization of your system with proper internationalization support, and you’ll find it makes things much easier and provides better data your organization can act on. And your users will appreciate it as well!\nFinally, if you’ve run into other common misses in i18n efforts you’ve worked on, chime in below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:89e19c5e-17f3-4539-bd46-0de6341d6182>","<urn:uuid:e8b33723-f843-484c-8d24-57560fc33215>"],"error":null}
{"question":"What are the modern approaches to flood management in urban areas, and what are their associated public health challenges?","answer":"Modern flood management approaches have shifted from building massive dams to creating adaptable floodscapes that work with natural processes rather than against them. These projects include interventions in various European regions that combine ancient strategies with modern design, emphasizing fluctuation rather than trying to neutralize flood effects. They incorporate disaster management strategies, evacuation exercises, and multi-step protection schemes. However, these approaches face significant public health challenges, particularly during extreme weather events. These include risks of waterborne illnesses, especially after heavy rains, strain on water treatment infrastructure in older cities, and contamination of water supplies with multiple pollutants. Studies have shown increases in gastrointestinal cases following heavy rains, and long-term health impacts can persist even after flood waters recede, including mental health problems and respiratory issues from mold exposure.","context":["January 11, 2021\nWhy Landscapes Designed to Flood Are Environmentally Sound\nThe author of a new book explains why a landscape may be more resilient when it is designed for and defined by its floods.\nA “floodscape” could be seen as a contradiction in terms: Flood spreads wherever gravity leads it, covering the familiar topography with a dark, gray, and uniform blanket. In that regard, flood is amorphous, as it can distort and temporarily erase forms and features from the visible landscape—nothing that could be described as a “scape” in the sense of articulated and meaningful scenery. But when the boundaries of a flood are not just defined by the quantity or the velocity of water but also by landforms and structures carefully designed and placed to influence and shape the “disaster,” the result can be considered as a landscape, physically and culturally defined by flood.\nIn my forthcoming book, Floodscapes, I explore humanity’s past relationship to flood processes, from ancient and mythical floods to the rise of environmental policies, as well as a selection of contemporary projects. The latter group includes interventions currently implemented upstream in the Swiss canton of Valais, the French Isère Département, the German state of Bavaria, and downstream in the Dutch Groningen province, Lower Rivers region, and Meuse-Rhine Estuary. These six adaptation and mitigation projects show that ancient strategies are a source of inspiration, bridging centuries of technical and territorial transformations that have left our rivers and coastlines heavily dammed and channelized. From the construction of the first artificial mounds in Frisia, the Netherlands, to the “depoldering” of the Overdiepse Polder in Dutch Brabant, more than 2,500 years have passed, while 500 years separate the first déversoir overspill in Blois from the champs d’inondation contrôlée currently built in the Isère Valley.\nThe revival of these past practices enlarges the range of possible landscape interventions and practices capable of restoring a balanced and dynamic relationship between human habitat and natural fluctuations. Furthermore, motivated by the need to exploit and produce, these practices show that mitigation does not necessarily entail the creation of enclosed and unproductive green buffer zones: on the contrary, millenary experience has shown that flexible and resilient landscapes could also offer productive living environments, provided that they are designed and inhabited in a way that allows natural fluctuations to take place.\nMore than just an answer to flood risks, these new floodscapes participate in the repositioning of natural processes within the living environment, and there-fore within the field of spatial design. Creating a space that emphasizes fluctuation rather than neutralizing all its effects means a radical breakthrough with regard to coercive flood defense that has turned most European rivers into drainage channels, hiding risks and flood processes behind ever-higher levees. Alternatively, these new “elastic landscapes” reintroduce physicality and positivism into environments that have been literally drained by intensive exploitation and petrified by urbanization. They form instead a permanent display of natural variation, offering all citizens the experience of change, stimulating observation, concern, and anticipation. Witnesses to a flood episode along the Isar, Meuse, Rhône, Rhine, or Isère Rivers can now observe the rivers’ seasonal changes, both affecting and protecting their daily environment and giving meaning and presence to the term “floodplain.” Floodscapes serve as reality checks more efficiently than communication campaigns, dramatized news, or apocalyptic fiction. In this regard, floodscapes not only mitigate the direct effects of the flood but, by making it visible, tangible, and acceptable, they also help foster stronger resilience on the part of individuals, communities, and territories.\nSignificantly, evacuation exercises are no longer taboo. In the Swiss Rhône Valley, the canton’s capital informs its inhabitants about the closest dry shelter reachable in case of a dam-breach alert. Some Dutch municipalities are developing disaster management strategies, and since 2015, the local Red Cross has become involved in exercises for flood rescue using a network of civilian volunteers. In case of high water, the municipality of Munich has set up a seven-step scheme of action in order to protect the population living along the Isar. Numerous examples show that the adaptation of spaces and minds go together and follow a path closer to citizens’ reactivity and self-organization capacity, but also closer to their desires regarding the accessibility and public use of watercourses.\nLike yesterday’s massive dams, today’s floodscapes are the expression of a new cosmogony. This new model sees a logic of negotiated cohabitation with natural processes replace the old narratives of heroic fight, hypothetical conquest, and absolute safety. Navigating between mobilization and a reassuring discourse, official communication and policies today must create space for anticipation, adaptation, and mitigation, rather than celebrate ambition or a pseudo-permanent safety. Rather than combative ardor, cautious care and measured adjustments are required to adapt our coasts and rivers. Technology, in particular hydro-engineering, is no longer perceived as the final remedy against natural threats but rather as one ingredient in the delicate balance that needs to be found, in each specific situation, between natural fluctuations and human needs. From this perspective, floodscapes are a collective achievement: They require a consensus in terms of risk redistribution, and a joint and lasting effort in terms of investment, design, and care. They are also the shared success of supranational, national, and local institutions, which are able to articulate the various scales of reflection and intervention.\nThe transformation of riverine landscapes is a highly specific enterprise, historically, geographically, and culturally, and it remains dependent on economic and political ups and downs. Yet the pioneering work done in the past 20 years can be studied and compared to reflect on the modus operandi that has led to successful transformations. They allow us to build combinatory and consensual adaptation strategies, and they encourage stakeholders and architects of regional planning to play an active role in this historical enterprise. Judging by the first landscapes transformed since the turn of the millennium, which are now a source of curiosity and pride, all relevant actors in the territories concerned are ready and willing to play an active role in this historical turn. There is every reason to believe that many more floodscapes will emerge in the coming decades, as necessary adaptation measures and as positive and flexible answers to an uncertain future.\nYou may also enjoy “When It Comes to Resiliency, Boston Can Think Bigger”\nWould you like to comment on this article? Send your thoughts to: [email protected]\nRegister here for Metropolis’s Think Tank Thursdays and hear what leading firms across North America are thinking and working on today.\nThe Pandemic’s Work-from-home Lessons","The best of EcoWatch, right in your inbox. Sign up for our email newsletter!\nReport Looks at Health Threats from Flooding in a Warming World\nExtreme precipitation and flooding, likely on the rise in a warming world, carry significant and often hidden health risks, according to a report, After the Storm: The Hidden Health Risks of Flooding in a Warming World, released today by the Union of Concerned Scientists (UCS).\n“Damage from floods is typically measured in terms of lives lost and the cost of damage to buildings and infrastructure,” said Liz Perera, UCS public health analyst and one of the report’s co-authors. “But what are often overlooked are the potentially costly public health impacts.”\nThe report calls attention to these impacts by listing the top five health risks of extreme precipitation and flooding—drowning while attempting to drive through rising water; drinking contaminated tap water; being exposed to fouled water bodies; coming into contact with backed-up sewage in your home, and being exposed to mold.\n“Over half of all outbreaks of waterborne diseases in the U.S. occur in the aftermath of heavy rains,” said Perera. “Health risks will likely increase as extreme rainfall events are projected to become more common in a warming world.”\nHeavy rains can contaminate drinking and recreational water with sewage, petroleum products, pesticides, herbicides, and waste from farm animals, wildlife and pets. Floodwaters may contain more than 100 types of disease-causing bacteria, viruses and parasites.\n“We think of waterborne illnesses as a problem in developing countries, but it’s a very real public health issue here in the U.S.,” said Dr. Marc Gorelick, division chief of pediatric emergency room medicine at Children’s Hospital of Wisconsin, in Milwaukee. “Climate change, because it likely causes heavier storms, could threaten our already vulnerable water supply and lead to more cases of gastrointestinal illness.”\nGorelick conducted a study which found that between 2002 and 2007, there was an 11 percent increase in gastrointestinal cases at Children’s Hospital within four days of heavy rains.\nThe UCS report also points out that flooded homes and buildings create a breeding ground for mold, which can cause debilitating respiratory and neurological problems. Mental health problems also tend to increase in the wake of extreme weather disasters. These health problems can persist long after flood waters have receded.\nExtreme weather events, such as heavy rains, are also creating challenges for waste water and water treatment plants.\n“Like many older cities, heavy precipitation can put a strain on New York City’s infrastructure,” said Angela Licata, deputy commissioner for sustainability at the New York City Department of Environmental Protection. “We are continually innovating to meet that challenge, while maintaining and improving essential services.”\nOther factors that affect flooding risks include where people live, how land is developed and the investments made in building resilience.\nFor more information, click here.\nEcoWatch Daily Newsletter\nAn area in Louisiana whose predominantly black and brown residents are hard-hit by health problems from industry overdevelopment is experiencing one of the highest death rates from coronavirus of any county in the United States.\nA central player in the fight against the novel coronavirus is our immune system. It protects us against the invader and can even be helpful for its therapy. But sometimes it can turn against us.\nCalling someone a delicate flower may not sting like it used to, according to new research. Scientists have found that many delicate flowers are actually remarkably hearty and able to bounce back from severe injury.\nWith global air travel at a near standstill, the airline industry is looking to rewrite the rules it agreed to tackle global emissions. The Guardian reports that the airline is billing it as a matter of survival, while environmental activists are accusing the industry of trying to dodge their obligations.\nThe outbreak of COVID-19 across the U.S. has touched every facet of our society, and our democracy has been no exception."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:681dac31-57e5-4e36-a43b-48ba8e94147f>","<urn:uuid:fa37c513-4371-450f-8673-0e4e9f4fc2e5>"],"error":null}
{"question":"Could you explain how different levels of performance standards can be structured in an organization? I'm interested in understanding the various tiers that can be implemented.","answer":"A multi-tiered standard structure can be implemented with four levels: 1) A minimum standard, below which the employee faces demotion or termination, 2) A training warning standard, below which remedial action is required, 3) An acceptable standard that requires no action, and 4) An excellence standard that comes with positive rewards.","context":["BOBKO, P. & COLLELA, A. (1994). Employee reactions to performance standards: A review and research propositions. Personnel Psychology, vol. 47, p. 1-29.\nPerformance standards should not replace other management tools but used in addition to foster job satisfaction and motivation, or at least, to help avoid dissatisfaction and low motivation.\nAn individual’s acceptance of external standards can happen through:\n- A congruence of individual and organizational values\n- An exchange of work for reward\n- Perceived affiliation with the organization\n- They have an evaluative component\n- Criteria established externally and imposed on an individual’s work task\n- As established entities, they remain somewhat stable over time and individuals.\nLocke and Latham (1990) suggest:\nThe aim or end of an action\nA rule to measure or evaluate things\nGoals are determined on an individual basis and standards are usually considered as constant across individuals and determined by organizational criteria.\nReview of related litteratures\nRelevant as performance standards have motivating potential. An individual’s personal goals are an immediate regulator of his or her actions (Locke and Latham, 1990).\nLocke and Latham (1990) suggest:\n- Those who work toward difficult, specific goals perform better than those who work toward easy, vague, or no goals\n- Performance tends to increase monotonically with goal difficulty\nGoal setting has a positive effect on performance through:\n- Behavioral direction\n- Task-specific strategies\nExternally defined standards must be translated by individuals into personal goals that are specific and difficult.\nAssigned goals are either translated into personal goals or rejected.\nIf an organization uses standards as a way to increase performance, then they must be sufficiently difficult and specific and conditions must exist to lead to commitment.\nFactors increasing commitment to assigned goals\n- Goals are assigned by authority figures who:\n- are seen as legitimate\n- are physically present\n- are supportive\n- are trustworthy\n- exert reasonable pressure\n- Assigned goals which:\n- imply rewards and punishment\n- convey positive self-efficacy information\n- foster a sense of achievement\n- are challenging\n- have high instrumentality and valence\n- have a hig expectancy for success\n- do not conflict with other goals\n- are participatively set\n- are made for ego involving tasks\n- are intense\nParticipation in standard setting procedures by top management and employees should increase motivation (based on MBO).\nTo understand employee’s reactions, one should consider:\nProviding people with feedback about their performance will have a positive effect on their future performance.\nThree factors to consider when setting standards\n- Clarity of the standard\n- How adequately information about the standard is disseminated\n- How important the standard is likely to be to employees\nStandards that are clear, descriptive and specific facilitate feedback and should produce more desirable responses.\nStandards should be perceived as both difficult and rewarding.\nIf external standards are too easily attained and no reward is provided for exceeding them, it may lower individual’s internal standards and motivation (Janz, 1982; Taylor et al., 1984).\nIf standards are too difficult, subject may end up giving up (Carver and Scheier, 1981) or lower their own standards (Campion and Lord, 1982)\nManagers hold performance expectations about their employees, communicate and behave according to those expectations and thus create SFP (self-fulfilling prophecies) whereby employees conform to their managers’ performance expectactions.\nEden (1990) distinguishes:\n- Pygmalion effect (holding and acting on positive expectations increases performance)\n- Galatea effect (employee’s own positive expectations lead to high performance)\n- Golem effect (employer’s and employee’s negative expectations lead to poor performance)\nEden (1990) argues that difficult performance expectations lead to greater efforts than easy expectations. Expectations should be realistically difficult, where the realistic level is determined based on abilities.\nThe pygmalion effect allows employees to achieve success (Eden, 1990) and those “small wins” (Weick, 1984) on the path to big challenges provide the needed sense of accomplishment.\nEmployee participation in standard definition may signal to employee that their manager has confidence in their abilities (Locke and Schweiger, 1979).\nMeeting or exceeding difficult and challenging standards will lead to a sense of achievement not found when meeting easy standards (Locke and Latham, 1990).\nIndividuals consider their past performance when judging their current performance (Simon, 1988; Vance and Collela, 1990).\nSatisfaction may be enhanced when feedback is presented in terms of chance in performance.\nUtility analysis and performance effectiveness\nNegative and positive effectiveness ratings imply the existence of a zero-point in the utility curve, itself an inverted U function with the minimum being used in evaluative standards.\nAcceptance of external standards\nGlobal: Individual reactions to standards depends on the degree to which external performance standards are accepted by those to whom the standards apply\nDepends on the degree to which the employee indentifies with and is commited to the goals of the organization\nStandards that are communicated and constructed using the factors are more likely to be accepted\nOrganizational newcomers are more likely to accept externally set standards\nThe effect of participation on the acceptance of standards will be greater for incumbents than newcomers\nStandards defined in specific, behavioral terms lead to greater motivation and performance.\nStandards stated in terms of outcomes may increase motivation. From the organization’s POV, standards defined in terms of outcomes seem more useful, from the employee’s POV, it may facilitate performance but has reduced potential for diagnostic feedback.\nPositively framed standards are preferred over negatively framed standards\nIncreased motivation and job satisfaction will result if performance standards\n- relate to outcomes valued by employees\n- result in rewards which are perceived as fair given the necessary inputs\nCommunicating performance standards\nPerformance standards reflecting high performance expectations are most likely to facilitate the performance of organizational newcomers.\nCommunicating performance standards along with a rationale will result in greater satisfaction than use of equally appropriate, but unjustified, performance standards.\nSetting the difficulty of standards\nIndividuals working under standards that they consider to be achievable and difficult will be more motivated and more satisfied than will individuals working under easy standards.\nTo the extent that only minimum performance standards are emphasized by organizations:\n- Intrinsic motivation to perform at a greater level will be reduced\n- These minimum standards will be accepted\n- They will serve as relatively easy individual goals\nEmployees with low self-efficacy will be most likely to accept and work towards easy performance standards. Employees with greater self-efficacy will be more likely to accept and work towards difficult performance standards.\nOrganizations can increase employee’s acceptance of difficult standards, and consequently motivation, by increasing employee’s self-efficacy on tasks addressed by performance standards. A multi-tiered standard could be appropriate, such as Bobko and Wise (1987):\n- Minimum standard – below which the employee is demoted or fired\n- Training warning standard – below which action for remediation is taken\n- Acceptable standard – needing no action\n- Excellence standard – with associated positive reward\nOrganizations that adopt a continuum of performance standards rather than a single common standard across all individuals in a particular job, will increase individual motivation.\nStating standards in terms of individual changes in performance will enhance individual motivation and increase subsequent individual performance.\nChange scores are usually much less reliable than the original component score.\nA focus on the evaluation of changes in performance will enhance individual satisfaction with the job\nIndividuals with an “incremental skill” orientation will be more likely to accept, and react positively to, performance standards stated in terms of change."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6bf1932d-92d7-4ca5-ba52-943f6dd228f8>"],"error":null}
{"question":"How can one diagnose and treat plague infection based on clinical symptoms?","answer":"Plague is diagnosed when a person shows symptoms including a swollen and painful lymph gland (called a bubo), fever, chills, headache, and extreme exhaustion, particularly with a history of exposure to rodents or fleas. Suspected cases are hospitalized and isolated. Laboratory tests are conducted, including blood cultures and microscopic examination of lymph gland, blood, and sputum samples. Treatment begins immediately after taking laboratory specimens. The first-line antibiotic is streptomycin, with gentamicin as an alternative. Tetracyclines and chloramphenicol are also effective but are not the preferred choice. All suspected cases must be reported to local and national health departments, who then report to the World Health Organization.","context":["Plague is an infectious disease caused by the bacillus Yersinia\nWhere is plague usually found?\noccurs mostly in temperate and sub-tropical areas of Asia, Africa and the Americas;\nin epidemic form, it has spread much wider, including most of Europe.\nin people occur in areas where housing and sanitation conditions are poor, in\nrural communities or in cities.\nHow common is plague?\nWorld Health Organization reports 1,000 to 3,000 cases of plague annually, while\nin the US, an average of 10 to 20 cases are diagnosed each year).\nis plague transmitted?\nFleas become infected by feeding mammals that have\nbeen infected with the bacteria Yersinia pestis. Fleas transfer the bacteria\nto humans and other mammals during their normal feeding. The bacteria are maintained\nin the blood of the rodents.\nOutbreaks are usually associated with infected\nrats and rat fleas that live in the home, though other ground-living rodents have\nbeen implicated, including prairie dogs, wood rats, chipmunks, and ground squirrels.\nplague be spread from person-to-person?\nWhen a person has plague pneumonia\nand coughs, droplets containing the plague bacteria are disperesed into the air,\nand may then reach a non-infected person.\nthe symptoms of plague?\nThe typical sign of human plague is a swollen and\nvery painful and tender lymph gland. The swollen gland is called a \"bubo\",\nhence bubonic plague.\nWhen plague bacteria reach the bloodstream,\nthey spread rapidly throughout the body causing a severe and often fatal condition.\nInfection of the lungs causes the pneumonic form of plague, a severe respiratory\nillness, with the potential to infect others.\nThe infected person may experience\nhigh fever, chills, cough, and breathing difficulty, with bloody sputum. If specific\nantibiotic therapy is not given, the disease can progress rapidly to death.\nis plague diagnosed?\nBubonic plague ise suspected when a person develops\na swollen gland, fever, chills, headache, and extreme exhaustion, when there is\na history of exposure to rodents or fleas.\nis the incubation period?\nUsually 2 to 6 days after being infected, an\nuntreated person becomes ill with bubonic plague, when the bacteria invade the\nIs there a treatment for plague?\nwith suspected plague are hospitalized and isolated. Laboratory tests, including\nblood cultures for plague bacteria and microscopic examination of lymph gland,\nblood, and sputum samples are arranged with all speed.\nbegins immediately after laboratory specimens have been taken; some of the results\nwill not be available for 72 hours, and treatment may be reviewed in the light\nof late results.\nStreptomycin is first line antibiotic; gentamicin may\nbe used when streptomycin is not available. Tetracyclines and chloramphenicol\nare also effective, but are not the drugs of choice. People who have been in close\ncontact with the index patient are routinely identified and assessed, particularly\nin cases of with plague pneumonia.\nAll cases of suspected plague must be\nreported to local and national health departments, who will report report to the\nWorld Health Organization. This is a legal requirement in most nations.\nis the mortality rate for plague?\nAbout 14% of plague cases in the United\nStates prove fatal. the death rate for untreated plague is much higher.\nis the plague vaccine?\nPlague vaccine is an active immunizing agent, which\nworks by causing the body to produce antibodies against the disease.\nshould get vaccinated against plague?\nPeople traveling to plague-infected\nareas of Africa, Asia, Latin America, or the western third of the United States\nshould consider plague vaccine to help prevent infection.\nthere adverse reactions to the plague vaccine?\nThe vaccine may cause flu-like\nsymptoms, or local inflammation and redness; more serious effects are uncommon.\nCan plague be controlled environmentally?\nand vermin control.\nWhat can be learned from history?\nwas the cause of Justinian's Plague (6th century) and the Black Death (14th century),\nboth outbreaks killed millions. A pandemic began in China and spread around the\nworld causing nearly 30 million cases with over 12 million deaths between 1896-1930.\nBibliography and Further Information Sources\nIf this article hasn't answered your question, email me at the address below, and I'll try to get the information you seek. I regret I cannot assist with individual cases or essays and school projects, but if it's something I've missed, I'll be happy to try and help.\nArticle written by Andrew Heenan BA (Hons), RGN, RMN"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9f9a7fd3-ee06-4fc2-b79b-e1707558876a>"],"error":null}
{"question":"Could you compare the reliability features of the Pratt & Whitney R-985 Wasp Junior engine and the Piper Archer's Lycoming O-360 engine? Both seem to be well-regarded powerplants.","answer":"Both engines have proven to be highly reliable powerplants. The Pratt & Whitney R-985 Wasp Junior was one of the most successful reciprocating engines ever built, with over 39,000 units manufactured, and many are still in service today. The Lycoming O-360 used in the Piper Archer is described as one of the industry's legends for reliability, durability, and economy. The Wasp Junior was rated between 300-450 hp depending on the model, while the Lycoming O-360 produces 180 hp and has a TBO (Time Between Overhaul) of 2000 hours.","context":["The Pratt & Whitney R-985 Wasp Junior is a series of nine-cylinder, air-cooled, radial aircraft engines built by the Pratt & Whitney Aircraft Company from the 1930s to the 1950s. These engines have a displacement of 985 in; initial versions produced 300 hp (220 kW), while the most widely used versions produce 450 hp (340 kW).\nWasp Juniors have powered numerous smaller civil and military aircraft, including small transports, utility aircraft, trainers, agricultural aircraft, and helicopters. Over 39,000 of these engines were built, and many are still in service today.\nBegun in 1925 by former Wright Aeronautical employees as a spinoff from a machine tool company, Pratt & Whitney became one of the world’s largest manufacturers of aircraft engines, and the Wasp Jr. is one of the most successful reciprocating engines ever built. Pratt & Whitney introduced it as a complement to the highly successful Wasp and Hornet families of engines in 1930. The Wasp Jr. was essentially a Wasp of reduced dimensions. Pratt & Whitney and its licensees manufactured over 39,000 versions of the R-985 until 1953 for a wide variety of military and commercial aircraft, including light transports, trainers, sport aircraft, and helicopters.\nThe R-985-AN-14B powered the McDonnell XHJH-1 and XHJD-1 Whirlaway helicopters and the Avro Anson V trainer. This Wasp Jr.-aptly nicknamed “The Dancing Engine”- has been sectionalized and motorized to demonstrate the movements of its internal components.\nThe Wasp Jr. (pictured here from the Smithsonian Museum) was essentially a Wasp of reduced dimensions. Pratt & Whitney and its licensees manufactured over 39,000 versions of the #R985 until 1953 for a wide variety of military and commercial aircraft, including light transports, trainers, sport aircraft, and helicopters.\nThe Pratt & Whitney R-985 Wasp Junior was a 9 cylinder, single-row, air-cooled radial engine with horsepower ranging from 300 hp to 450 hp, depending on the model and configuration. It was used in a range of aircraft that included the Grumman Goose, Lockheed Model 10A and Beechcraft Model 18. Jacqueline Cochran used the Wasp Junior to set speed and altitude records in a specially built D17W Beechcraft Staggerwing.\nIn the mid-1930s, Pratt & Whitney produced five basic engines:\n- The single-row Wasp Junior.\n- The single-row Wasp.\n- The single-row Hornet.\n- The double-row Twin Wasp.\n- The double-row Twin Wasp Junior.\nThe Wasp Junior was a smaller version of the R-1340 Wasp designed to compete in the market for medium-sized aircraft engines. Development was completed in 1929 and the engine went into production in early 1930 at the new 400,000 sq./ft Pratt & Whitney plant that opened on January 1, 1930, in East Hartford.1 The original Wasp Junior was rated at 300 hp at 2,000 rpm and was similar in power and displacement to the J-6-9 Whirwind. A supercharged version developed 400 hp at 2,300 RPM at 4,000 ft. and by 1932 power was up to 420 hp with racing versions greater than that.2\nIn 1931, Pratt & Whitney developed a “hot spot” or heat exchanger on the oil regulator to improve engine performance. It was installed between the carburetor and the rear section of the engine. The regulator used the temperature drop of the gasoline evaporation to cool the oil and the same unit used the engine exhaust to heat the fuel/air mixture in cold weather.\nMore than 39,000 Wasp Juniors were produced from 1929 until the end of production in 1953.5It was considered the best engine in its class and many Wasp Junior engines are still flying today.\nSpecifications of the Pratt & Whitney R-985 Radial Engine\n|Pratt & Whitney R-985 Wasp Junior|\n|Configuration:||Single-row, air-cooled radial|\n|Horsepower:||400 hp (298 kW)|\n|Bore and Stroke:||5-3/16 in. (132 mm) x 5-3/16 in. (132 mm)|\n|Displacement:||985 cu. in. (16.14 liters)|\n|Weight:||640 lbs. (290 kg)|\nAircraft Powered by the Pratt & Whitney R-985\n- Airspeed Oxford (AS.46 Oxford V)\n- Air Tractor AT-300\n- Avro Anson (Mk V)\n- Barkley-Grow T8P-1\n- Beechcraft Model 18 and military derivatives\n- Beechcraft Staggerwing D17S, D17W, G17S\n- Bell XV-3\n- Bellanca 300-W\n- Berliner-Joyce OJ\n- Boeing-Stearman Model 75 (in aftermarket conversions)\n- Bratukhin G-3\n- CAC Winjeel\n- de Havilland Canada DHC-2 Beaver and L-20/U-6 military versions\n- Douglas C-26 Dolphin\n- Fleetwings BT-12\n- Gee Bee Model Z\n- Grumman G-164 Ag Cat (some models)\n- Grumman G-21 Goose\n- Howard DGA-11\n- Howard DGA-15P\n- Junkers F13 (Rimowa F13 replica)\n- Koolhoven F.K.51 (some models)\n- Lockheed Model 10-A Electra\n- Lockheed Model 12-A Electra Junior\n- Max Holste MH.1521 Broussard\n- McDonnell XHJH Whirlaway\n- Naval Aircraft Factory N3N (in aftermarket conversions)\n- North American BT-14\n- Seversky BT-8\n- Sikorsky H-5 helicopter (and S-51 civil version)\n- Sikorsky S-39 amphibian\n- Snow S-2B and S-2C\n- Spartan Executive 7W\n- Stinson Reliant SR-9F and SR-10F\n- Vought OS2U Kingfisher\n- Vultee BT-13 Valiant\n- Waco S3HD\n- Waco SRE Aristocrat\n- Weatherly 201 series\n- Weatherly 620\nWhat’s your favorite part of a the Pratt & Whitney R-985? Our is the sound, so here is a video of that round sound:\nThis #DHC2 comes to life with the #RoundSound of the #R985 we call music to our ears. This is @kenmoreair’s #aircraft with photo from @hangarnc #aviationphotography #aviationphoto #aviation #aviationlovers #aviationgeek #aviationdaily #AvNerd #AvGeek #AviationNerd #AviationGeek #PlaneSpotting #PlaneSpotter #aircraftmaintenance #aviationlove #aircraftenginemaintenance #instagramaviation #IGAviation #IGAircraft #avlove #pilot #privatepilot #radialengines #aircraft","Archer: In the Fourth Generation\nReliability and Easy Handling have made the Archer a Mainstay in the Piper Lineup\nMany people regard the Piper Archer as the quintessential Cherokee. While it’s true today’s four-seat, fixed-gear Pipers have long since ceased being plain-Jane Cherokees, the airplanes need make no apologies for their lineage.\nDesigners Fred Weick and John Thorpe were obsessed with safety and simplicity when they conceived the Cherokee, and Weick’s previous design success, the Ercoupe, was an essentially unstallable, unspinnable machine. In slight contrast, the Cherokee would stall, but only reluctantly, and spins were highly unlikely. Weick and Thorpe concentrated on gentle handling and efficient, production-simple construction. In fact, the original Cherokees were built with 400 fewer parts than the PA22 Tri-Pacer.\nTechnically, the Cherokees were all straight-wing airplanes, and Piper probably cringes every time some smart-aleck aviation writer calls the newer, semi-tapered designs “Cherokees.” Still, a Cherokee by any other name would still be as friendly, stable, and forgiving.\nCall it an Archer and you’ve just named what many pilots consider the best compromise of power, performance, useful load, and economy in the PA28 line, not to mention one of the longest-lived Pipers still in production—now well-past a half-century.\nThe PA28-181 Archer is built around the carbureted, 180-hp, O-360 Lycoming, certainly one of the industry’s legends for reliability, durability, and economy. The more powerful Dakota may be the better high-performance load lifter and the Warrior a superior economy champ, but the Archer is the in-between machine that sells.\nToday’s PA-28-181 Archer can trace its roots to the original Cherokee 180 of 1963, a design that has gradually outlived virtually all its competitors. Beech’s Sport and Sundowner, Cessna’s Cutlass, Hawk XP, and Cardinal, and the Grumman American Tiger are gone, and the only other airplane remaining in the class is the Skyhawk S.\nNone of this is to suggest that commercial success and superior aeronautical design are synonymous. Look back through the history of aviation, and you’ll find examples of airplanes that were cleverly marketed and sold well – for a while – but weren’t exactly winners in the sky. It’s nevertheless significant that various versions of the PA-28-180 received a strong financial endorsement from the people who really matter—buyers. In the arena that counts the most, the Archer has endured for more than 50 years.\nToday’s Archer is very much the airplane it has always been, only different and better. If you’re expecting to hear that there have been significant performance improvements in the last 30 years, forget it. It’s probably true that a design genius such as the late Roy Lopresti could have gleaned a half-dozen knots and some extra climb with an aerodynamic clean-up program, but there’s only so much you can do, given the limitations of fixed-gear, 180 hp, and a fixed-pitch prop.\nIt’s true there are several other stiff-legged home-builts that will easily outrun the Archer on the same horsepower, but remember that those airplanes aren’t required to jump through the same certification hoops for crashworthiness, pre-stall buffet, spin recovery, etc. If they were, their performance wouldn’t be so stellar.\nThe Archer’s talents have always been more associated with its Jack-of-all-trades can-do attitude. It’s one of those designs that seems to do everything well, if nothing spectacularly so. Piper learned a long time ago that performance alone isn’t a guarantee of sales and, as a consequence, the Archer has been the beneficiary of a long series of evolutionary changes, many of them related to creature comforts, systems integrity, and operating convenience rather than better climb or cruise, longer range, or more payload.\nPark a newer Archer alongside an original 1963 Cherokee 180 and the differences would be dramatic, though both airplanes use essentially the same power and fuselage configuration. Indeed, the development from Cherokee 180 through Challenger to Archer has been a long one, and I won’t try to summarize all the steps along the way.\nThe most significant aerodynamic change, however, came in 1978 when Piper adapted the Warrior’s high dihedral, semi-tapered, NACA wing, with a five-degree sweepback at mid-span, just outboard of the flaps. The Warrior wing was a significant departure from the original, constant chord, Cherokee wing, in that it incorporated a longer airfoil with bigger, more effective Frise ailerons, 100 inches in span.\nThe improved wing also utilized a higher aspect ratio from the same area, resulting in slightly less drag. Finally, the angle of incidence decreased three degrees from root to tip, pushing the stall outboard and increasing aileron control when the roots were already stalled.\nThe airplane has been progressively upgraded since that time, but most of the changes have emphasized comfort rather than performance. In 1995, Piper made a series of improvements that resulted in raising the Roman numeral from II to III. Piper stepped up to the Archer III by incorporating a new cowling utilizing NASA inspired, University of Mississippi-developed, axisymmetric engine inlets, a new windshield line, an improved panel, and a revamped interior.\nThe idea behind the new cowling was to glean an extra knot or two of speed, modernize the airplane’s looks, and increase cooling efficiency. To that end, Piper incorporated round inlets in place of outboard-facing, D-shaped inlets of old. Piper also moved the landing light out of the high-vibration cowling onto the wingtip.\nThe lowered windshield line was incorporated to allow moving electrical switches (master, mags, alternator, primer, starter, landing/taxi lights, nav lights, and strobes) onto a small, overhead panel on the cabin ceiling. This is the traditional position for electrical controls on corporate airplanes, and Piper concluded there was no reason the Archer shouldn’t benefit from the same professional layout.\nPiper also borrowed upgrades from two other company models, the Malibu Mirage and Saratoga HP (the latter now discontinued). In short, the Archer was a comfortable place to spend an hour or a day lofting above the Earth. Interior dimensions are 42 inches across (coincidentally, exactly the same as a Bonanza) by 45 inches tall, adequate for most pilots. Anything that’s not covered in tweeds or leathers is covered in plush pile or rich wool fabric. The Archer III’s panel reflects recent improvements on the Mirage, scaled-down to accommodate the Archer’s smaller instrument housing and reduced complexity.\nPayload with a full stack of avionics and a full service of fuel works out to about 575 pounds. That’s three full-sized passengers with a modicum of luggage. The heaviest option is air conditioning (60 pounds), but if you operate an Archer regularly in the Deep South or Southwestern U.S., the extra comfort might be well worth the loss of payload.\nNone of the PA-28 line was intended for high-altitude operation, as evidenced by the rate of climb (667 fpm) and service ceiling (13,240 feet). This makes climb a fairly leisurely affair to the optimum cruise height of 7000 feet. Back in the early ‘80s, I flew an Archer out of Grand Junction, Colorado, and employed ridge lift off the Rockies to soar across the top of the tallest mountains at 15,500 feet. This was far above the airplane’s unassisted best efforts, and as soon as I reached Colorado Springs on the east side of the hills, the airplane involuntarily descended to about 11,500 feet.\nLevel at a more reasonable 6500 feet with the left knob against the panel, you’ll see true airspeed wind around to about 125 knots. Piper lists max cruise at 128 knots on just over 9.0 gph, and while that’s probably attainable on a totally stripped machine devoid of antennas, few production machines with a typical stack of radios and accompanying antennas will come close to that ideal. Throttle back to 65 percent cruise and the Archer will lose about five knots. Fifty-five percent settings don’t make much sense, as the loss of airspeed is disproportionate to the minor amount of fuel saved. In other words, you’ll score equal or better mpg at 65 percent power.\nA speed of 120-125 knots isn’t exactly burning up the sky, but at least you’re flying in a cabin well-insulated against the noise and vibration so common in other airplanes. The Archer is one of the quietest fixed-gear singles I’ve flown, with noise levels in the 80 db range. The airframe isolates the cabin from much of the vibration, so it’s a friendly place to fly in that regard, as well. For better or worse, I’ve flown a wide variety of singles and twins across many of the world’s oceans, and I can personally attest that cockpit noise level can be a major concern when the legs are long and weather isn’t clement.\nFlying above Florida’s Indian River with a Piper demo pilot in the right seat, I couldn’t resist trying to get the airplane mad at me. Stalls are a total non-event in the 181, a fact I reconfirmed in the test airplane. Frustrated to 60 degrees of bank, the Archer steadfastly refused to bite.\nDespite, or perhaps because of its other talents, the Archer shines brightest in the pattern. Pilots who artificially judge their overall competence by greaser landings are bound to love the Archer. It’s certainly one of the easiest airplanes around for returning from sky to Earth. The flare is almost totally predictable and the payoff so gentle, it’s easy to hold the airplane a few inches or a few feet above the runway and squeak onto the asphalt in classic, nose-high attitude. With flaps fully extended, the 181 hangs in there to 52 knots, so you can easily fly approaches at speeds of 65 knots or less and not feel as if you’re pushing the envelope.\nRunway requirements are less than 1500 feet for both takeoff and landing, and that’s at gross. If you operate with only two people up front most of the time, the way many owners of four-place airplanes fly, you may do even better. The point is, you can fly an Archer into and back out of any paved runway with clear approaches and at least 1550 horizontal feet, assuming the runway isn’t located in the high Sierras or Rockies.\nIn truth, the Archer is about as close as any manufacturer has come to an everyman’s single. Though the old Warrior (now discontinued) was less expensive with almost the same cruise performance, the Archer has the extra punch to carry four people into places where a Warrior or Skyhawk would be marginal at best. Combine that talent with the easiest landing and gentlest handling characteristics in the class and you begin to understand why the Archer continues to survive as one of Piper’s evergreens.\nSpecifications & Performance – Piper Archer III\nAll specs and performance numbers are drawn from official sources, often the aircraft flight manual or the manufacturer’s web site. On used aircraft, common sources of information are Jane’s All-The-World’s Aircraft or RVI’s Aircraft Bluebook Price Digest.\nEngine make/model: Lyc O-360A4M\nHorsepower@altitude: 180 @ SL\nHorsepower on takeoff: 180\nTBO – hours: 2000\nFuel type: 100LL\nPropeller: Sensenich FP\nLanding gear type: Tri/Fxd\nGross weight (lbs): 2558\nStd empty weight (lbs): 2550\nUseful load – std (lbs): 1658\nUsable fuel – std (gal/lbs): 48\nPayload – full std fuel (lbs): 612\nOverall length: 24’\nHeight: 7’ 3”\nWing area (sq ft): 170\nWing loading (lbs/sq ft): 15.0\nPower loading (lbs/hp): 14.2\nWheel size: 6.00 x 6\nSeating capacity: 4\nCabin doors: 1\nCabin width (in): 2\nCabin height (in): 45\nCruise speed (kts): 75% 128\nFuel Consumption (gph): 75% 9.4\nMax Range (nm): 75% 443\nBest rate of climb, SL (fpm): 667\nMaximum Operating Altitude (ft): 13,240\nStall – Vso (kts): 46\nTO ground roll (ft): 1150\nLdg ground roll (ft): 920"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:7f143984-95c8-4928-bcb8-60f94c09c84e>","<urn:uuid:8e8b94ec-fe84-4735-b777-5e89351b8ed5>"],"error":null}
{"question":"Can you explain how the nose-controlled smart glasses work and what metrics would be best to evaluate its gesture recognition accuracy?","answer":"The smart glasses use 3 motion sensors positioned on the nose bridge and under nose pads to detect electrostatic changes when users perform different nose movements. These sensors, typically used for eye movement tracking in medicine and CGI, record specific directional changes in static electricity based on different nose gestures like stroking, flicking, and rubbing. For evaluating such a gesture recognition system, probability calibration methods would be important, similar to how other prediction models are evaluated. The system's accuracy could be measured using metrics like the Brier score, which evaluates probabilistic predictions on a 0-1 scale (0 being perfect predictions). Additionally, reliability diagrams could help visualize how well the system distinguishes between intentional and unintentional gestures, with well-calibrated predictions matching their actual occurrence rates.","context":["Join us to learn the concept of smart glasses concept that will allow you to control the phone just by swiping the nose in the article below!\nUsing a phone or smartwatch while talking to others is considered a “quite rude” act, causing discomfort to the opposite person in the 21st century. Therefore, a research team has discovered a cure solution with a smart glass model, which allows users to control their technological equipment just by “scratching their nose”. You can reject a call, stop a video, or skip to a song by merely swiping your nose.\nUnfortunately, this smart product is not yet available in the market. According to public information, researchers from KAIST University of Korea, St. Andrew’s University of Scotland and Georgia Institute of Technology of the United States have researched and developed this smart glass with the purpose to create a way of “controlling the handset without getting the attention of others in public.”\nThanks to a set of 3 motion sensors attached to the nose of the glass and under the nose pads, this device allows electrostatic recording movements around the user’s nose. Typically, these sensors are used in medicine to record a patient’s eye movements. Besides, they are also used quite a lot in movies, usually films using CG technology, as a method to simulate the eye movement of the character in the most realistic way.\nOf course, for this concept of smart glasses concept, the sensor does not record eye movement but will focus on the nose of the user. When a user touches their nose in different ways, the amount of static electricity on the device varies in specific directions, which include stroking or flicking their nose from both sides and rubbing the lower part of the nose.\nJuyoung Lee, the first author of the study, said the smart lens system, called ItchyNose, was used to minimize social attention when using the device. Juyoung Lee thought about wearable devices directly in front of his eyes. In practice, Google Glass (wearable technology device with an optical head-mounted display – OHMD) has a similar control system that displays information like a smartphone in a hands-free mode.\nJuyoung Lee told The Verge, “If an important message from the spouse is sent during the meeting time, the user can check it and quickly glance without causing excessive attention when interacting. Similarly, if the user has a list of names and faces to remind about the people present at the meeting, you can scroll the list until you find the name of the person you forgot. These quick interactions can be beneficial without getting too much attention.”\nHowever, this design still exists quite a lot of different limitations. Specifically, the above effects can only be associated with a given statement.\nResearcher Hui-Shyong Yeo said: “The current challenge facing the team is how the system can distinguish between intentional and unintentional stroking.”\nHui-Shyong Yeo added: “Some of the strokes are very different and rarely trigger false. Other gestures, such as pushing the nose, have more fake activation. There is a balance between user consistency with the gesture of control versus the degree of system failure.”\nYeo said: “The answer is training smart glasses concept system to adapt to each individual’s style. As soon as it meets the requirements, there is no problem.”","Do you ever encounter a storm when the probability of rain in your weather app is below 10%? Well, this shows perfectly how your plans can be destroyed with a not well-calibrated model (also known as an ill-calibrated model, or a model with a very high Brier score).\nWhen building a prediction model, you take into account its predictive power by calculating different evaluation metrics. Some of them are common, like accuracy and precision. But others, like the Brier score in the weather forecasting model above, are often neglected.\nIn this tutorial you’ll get a simple, introductory explanation of Brier Score and calibration – one of the most important concepts used to evaluate prediction performance in statistics.\nWhat is the Brier Score?\nBrier Score evaluates the accuracy of probabilistic predictions.\nSay we have two models that correctly predicted the sunny weather. One with the probability of 0.51 and the other with 0.93. They are both correct and have the same accuracy (assuming 0.5 threshold) but the second model feels better right? That is where Brier score comes in.\nIt is particularly useful when we are working with variables that can only take a finite number of values (we can call them categories or labels too).\nFor example, level of emergency (which takes four values: green, yellow, orange, and red), or whether tomorrow will be a rainy, cloudy or sunny day, or whether a threshold will be exceeded.\nThe Brier Score is more like a cost function. A lower value implies accurate predictions and vice versa. The primary goal of dealing with this concept is to decrease it.\nThe mathematical formulation of the Brier Score depends on the type of predicted variable. If we are developing a binary prediction, the score is given by:\nWhere p is the prediction probability of occurrence of the event, and the term oi is equal to 1 if the event occurred and 0 if not.\nLet’s take a very quick example to assimilate this concept. Let’s consider the event A=”Tomorrow is a sunny day”.\nIf you predict that the event A will occur with a probability of 100%, and the event occurs (the next is sunny which means o=1), the Brier score is equal to:\nThis is the lowest value possible. In other words: the best case we can achieve.\nIf we predicted the same event with the same probability, but the event doesn’t occur, the Brier score in this case is:\nSay you predicted that the event A will occur with another probability, let’s say 60%. In case the event doesn’t occur in reality, the Brier Score will be:\nAs you may have noticed, the Brier score is a distance in the probability domain. Which means: the lower the value of this score, the better the prediction.\nA perfect prediction will get a score of 0. The worst score is 1. It’s a synthetic criterion that provides combined information on the accuracy, robustness, and interpretability of the prediction model.\nDotted lines represent the worst cases (if the event occurs, the circle is equal to 1).\nWhat is probability calibration?\nProbability calibration is the post-processing of a model to improve its probability estimate. It helps us compare two models that have the same accuracy or other standard evaluation metrics.\nWe say that a model is well calibrated when a prediction of a class with confidence p is correct 100p % of the time. To illustrate this calibration effect, let’s consider that you have a model that predicts cancer with a score of 70% for each patient out of 100. If your model is well calibrated, we would have 70 patients with cancer, if it is ill-calibrated, we will have more (or less). Therefore, the difference between these two models:\n- A model has an accuracy of 70% with 0.7 confidence in each prediction = well calibrated.\n- A model who has an accuracy of 70% with 0.9 confidence in each prediction = ill-calibrated.\nFor a perfect calibration, the relationship between the predicted probability and the fraction of positives follows the given:\nThe expression of this relationship is given by:\nThe figure above represents the reliability diagram of a model. We can plot it using scikit-learn as below:\nimport sklearn from sklearn.calibration import calibration_curve import matplotlib.lines as line import matplotlib.pyplot as plt x, y=calibration_curve(y_true, y_prob) plt.plot(x,y) ref = line.Line2D([0, 1], [0, 1], color='black') transform = ax.transAxes line.set_transform(transform) ax.add_line(line) fig.suptitle('Calibration – Neptune.ai') ax.set_xlabel('Predicted probability') ax.set_ylabel('Fraction of positive') plt.legend() plt.show()\nPlotting the reliability curve for multiple models allows us to choose the best model not only based on its accuracy, but on its calibration too.\nIn the figure below, we can eliminate the SVC (0.163) model because it is far from being well calibrated.\nIf we want a numeric value to check the calibration of our models, we can use the calibration error given theoretically by:\nWhen should you use the Brier score?\nEvaluating the performance of a machine learning model is important, but it’s not enough to evaluate the real-world application predictions.\nWe often worry about:\n- the model’s confidence in its predictions,\n- its error distribution,\n- and how probability estimates are made.\nIn such cases, we need to use other performance factors. Brier score is an example.\nThis type of performance score is specifically used in high-risk applications. This score allows us to not treat the model results as real probabilities, but instead go beyond the raw results and check the model calibration, which is important for avoiding bad decision making or false interpretation.\nExample of needing well-calibrated probabilities/model calibration\nLet’s consider that you want to build a model that shows news pages to users by the chance of clicking on them. If the chance of the user clicking on a suggested item is high, the item is shown on the main page. Else, we show another item with a higher chance.\nIn this kind of problem, we don’t really care about how much the exact chance of clicking is, but only which item has the highest chance between all of the existing items. The model calibration is not really crucial here. What matters is which one of them has the highest probability (chance) of being clicked.\nOn the other hand, consider a problem in which we build a model that predicts the probability of contracting a specific disease based on the output of some analysis. The exact value of the probability is crucial here because it affects the decision of the doctor and the health of the patient.\nGenerally, calibration is used to improve a model when the results show that it has mistakes with high probabilities (or prediction score when the model doesn’t output the probability estimate e.g. random forest).\nOf course, you can also look at other metrics that take prediction scores as input, like ROC AUC score, but they usually don’t focus on correctly calibrated probabilities. For example, ROC AUC focuses on ranking predictions.\nOk, now we can tell when the model is not well calibrated, but what can we do about it? How can we calibrate it?\nProbability calibration methods\nPeople use a lot of calibration methods. We will focus on the two most popular approaches:\nPlatt Scaling is often used to calibrate a model that we have already built. The principle of this method is based on the transformation of the outputs of our classification model into probability distribution.\nOur model will not only give a categorical result (label or class), but also a degree of certainty about the result itself.\nInstead of returning the class 1 as a result, it will return the probability of the correctness of this class prediction. Unfortunately, some classification models (like SVM) do not return probability values, or give poor probability estimates.\nThat’s why we use specific transformations to calibrate our model and convert the results into probability.\nTo use the Platt scaling method, we train our model normally, and then train the parameters of an additional sigmoid function to map the model outputs into probabilities.\nYou can do it using Logistic Regression fitted on the output of the model.\nfrom sklearn.linear_model import LinearRegression model=LinearRegression() model.fit(p_out, y_out) calib_p=model.predict(p_test)[:,1]\nIsotonic Regression does the same thing as Platt Scaling – they both transform model output into probability, and therefore calibrate it.\nWhat’s the difference?\nPlatt Scaling uses a sigmoid shape to calibrate the model, which implies a sigmoid-shaped distortion in our probability distribution.\nIsotonic Regression is a more powerful calibration method that can correct any monotonic distortion. It projects a non parametric function into a set of increasing functions (monotonic).\nIt is not recommended to use the isotonic regression if you have a small dataset, because it can easily overfit.\nTo implement this approach, we will again use sklearn (we assume you have already built and trained your “uncalibrated model”):\nfrom sklearn.linear_model import IsotonicRegression model=IsotonicRegression() model.fit(p_out, y_out) calib_p=model.transform(p_test)\nUsing model calibration on a real example\nLet’s get some practice!\nIn order to keep it unique, we will generate two classes using\nmake_classification from sklearn.\nAfter that, we will train and fit an SVM classifier on the dataset.\nfrom sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC X, y = make_classification(n_samples=2500, n_classes=2) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nWe have just created our unique random data, and split it into a training set and test set.\nNow we build the SVC Classifier, and fit it to the training set:\nfrom sklearn.svm import SVC svc_model=SVC() svc_model.fit(X_train, y_train)\nLet’s predict the results:\nNext, let’s plot the calibration curve that we talked about earlier:\nfrom sklearn.calibration import calibration_curve x_p, y_p=calibration_curve(y_test, prob, n_bins=10, normalize=’True’) plt.plot([0, 1], [0, 1]) plt.plot(x_p, y_p) plt.show()\nThe result of the code above is the calibration curve or reliability curve:\nThis shows that our classifier is ill-calibrated (the calibration reference is the blue line).\nNow let’s calculate the Brier score of this ill-calibrated model:\nfrom sklearn.metrics import brier_score_loss, roc_auc_score y_pred = svc_model.predict(X_test) brier_score_loss(y_test, y_pred, pos_label=2) roc_auc_score(y_test, y_pred)\nFor this, we get a ROC AUC score equal to 0,89 (means a good classification) and a Brief score equal to 0,49. Pretty high!\nThat explains the fact that the model is ill-calibrated. What do we do?\nLet’s calibrate our model using sklearn again. We will apply Platt scaling (calibrate using sigmoid distribution). Sklearn offers a predefined function that does the job:\nfrom sklearn.calibration import CalibratedClassifierCV calib_model = CalibratedClassifierCV(svc_model, method='sigmoid', cv=5) calib_model.fit(X_train, y_train) prob = calib_model.predict_proba(X_test)[:, 1] #plot the calibration curve (see above to know what is it) x_p, y_p = calibration_curve(y_test, prob, n_bins=10, normalize='True') plt.plot([0, 1], [0, 1]) plt.plot(x_p, y_p) plt.show()\nThe calibration or reliability curve of the model after calibration is shown below:\nThe reliability curve shows a tendency towards the calibration reference (the perfect case). For more verification, we can use the same numerical metrics as before.\nIn the following code, we calculate the Brier score and ROC AUC score of the calibrated model:\nfrom sklearn.metrics import brier_score_loss, roc_auc_score y_pred = calib_model.predict(X_test) brier_score_loss(y_test, y_pred, pos_label=2) roc_auc_score(y_test, y_pred)\nThe Brier score gets decreased after calibration (passed from 0,495 to 0,35), and we gain in terms of the ROC AUC score, which gets increased from 0,89 to 0,91.\nWe note that you may want to calibrate your model on a held-out set. In this case, we split the dataset to three parts:\n- We fit the model on the training set (first part).\n- We calibrate the model on the calibration set (second part).\n- We test the model on the testing set (third part).\nCalibrating your model is a crucial step to increase its prediction performance, especially if you care about “good” probability predictions that have a low Brier score.\nHowever, you should keep in mind that it’s not obvious that improved calibrated probabilities will contribute to better predictions based on class or probability.\nThis potentially depends on the particular evaluation metric you use to test predictions. According to some papers, SVM, decision trees, and random forest are more likely to be improved after calibration (in our example we used a support vector classifier).\nSo as always proceed with caution.\nI hope that, after reading this article you have a good understanding of what Brier score and model calibration are and you’ll be able to use that in your ML projects.\nThanks for reading!\n Brier GW. “Verification of forecasts expressed in terms of probability”. Mon Weather Rev 1950.\n Gneiting T, Raftery AE. “Strictly proper scoring rules, prediction, and estimation”. J Am Stat Assoc 2007.\n Is your model ready for the real world? – Inbar Naor – PyCon Israel Conference 2018\n A guide to calibration plots in Python – Chang Hsin Lee, February 2018.\nThe Ultimate Guide to Evaluation and Selection of Models in Machine Learning\n10 mins read | Author Samadrita Ghosh | Updated July 16th, 2021\nOn a high level, Machine Learning is the union of statistics and computation. The crux of machine learning revolves around the concept of algorithms or models which are in fact statistical estimations on steroids.\nHowever, any given model has several limitations depending on the data distribution. None of them can be entirely accurate since they are just estimations (even if on steroids). These limitations are popularly known by the name of bias and variance.\nA model with high bias will oversimplify by not paying much attention to the training points (e.g.: in Linear Regression, irrespective of data distribution, the model will always assume a linear relationship).\nA model with high variance will restrict itself to the training data by not generalizing for test points that it hasn’t seen before (e.g.: Random Forest with max_depth = None).\nThe issue arises when the limitations are subtle, like when we have to choose between a random forest algorithm and a gradient boosting algorithm or between two variations of the same decision tree algorithm. Both will tend to have high variance and low bias.\nThis is where model selection and model evaluation come into play!\nIn this article we’ll talk about:\n- What are model selection and model evaluation?\n- Effective model selection methods (resampling and probabilistic approaches)\n- Popular model evaluation methods\n- Important Machine Learning model trade-offs"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:aedba243-c046-41b3-beb9-d9c44f7264d3>","<urn:uuid:5691cbff-0812-45de-9e97-af1c27b2a42b>"],"error":null}
{"question":"How far back does Variety magazine's coverage of the entertainment industry go?","answer":"Variety magazine's coverage of the entertainment industry dates back to 1905, with records available through 2000.","context":["Includes core US and UK trade magazines covering film, music, broadcasting and theater, together with film fan magazines and music press titles from the era of vaudeville and silent movies through to the 21st century.\nProQuest's Entertainment Industry Magazine Archive is an archival research resource containing the essential primary sources for studying the history of the film and entertainment industries, from the era of vaudeville and silent movies through to 2000. The core US and UK trade magazines covering film, music, broadcasting and theater are all included, together with film fan magazines and music press titles. The magazines have been scanned cover-to-cover in high-resolution color, with granular indexing of all articles, covers, ads and reviews. Trade papers have long been recognized as potentially the single most valuable research material for scholars of the film and media industries. This database includes several trade magazines which have effectively provided the main historical record for their subject areas throughout the 20th century, such as Variety (1905-2000), Billboard (1894-2000), Broadcasting (1931-2000) and The Stage (1880-2000).\nArticles from scholarly and trade theatre and dance journals covering professional practice and research.\nUse the International Bibliography of Theatre & Dance to search for journal articles, books, book articles and dissertation abstracts on all aspects of theatre and performance. IBTD was initiated by the American Society for Theatre Research and continued by the Theatre Research Data Center (TRDC) at Brooklyn College.\nCitations to articles and books about Shakespeare’s plays as well as a record of productions.\nA database of scholarly and popular materials related to Shakespeare and published or produced between 1972 and the present. Includes books, articles, book reviews, dissertations, theatrical productions, reviews of productions,audiovisual materials, electronic media.\nSearchable text and images of articles, reviews, and advertising from each issue of the newspaper from 1851 to 2013.\nIncludes a digitized image of every backfile issue of The New York Times from cover to cover, including news stories, editorials, photos, graphics, and advertisements. Searchers can use basic keyword, advanced, guided, and relevancy search techniques to locate information. Or, they can browse through issues page by page, as one would browse a printed edition. Search results lists provide bibliographic information, including date, issue, article headline, page number, and byline (where given). Users may choose to display the full page image of any page in any issue.\nThe complete searchable text and images for two hundred years of the newspaper.\nResearchers can search through the complete digital edition of The Times (London), using keyword searching and hit-term highlighting to retrieve full facsimile images of either a specific article or a complete page. The entire newspaper is captured, with all articles, advertisements and illustrations/photos divided into categories to facilitate searching.\nText and image from the entire newspaper. The Sunday Times is distinct from the daily Times of London.\nIndependent from the Times of London, the Sunday Times was known for its investigative journalism, commentary and in-depth analysis of the week's news. To search with the Times of London and other historical British news sources use the Gale News Vault database\n\"The Journal of Dress History is the academic publication of The Association of Dress Historians through which scholars can articulate original research in a constructive, interdisciplinary, and peer reviewed environment.\" An open access journal."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:12c3051b-0488-4086-8157-c9fefdcd1063>"],"error":null}
{"question":"Why did the Museum of Modern Art remove frames from their Abstract Expressionist paintings in 2009?","answer":"The Museum of Modern Art removed frames from their Abstract Expressionist paintings because, according to chief curator Ann Temkin, the paintings were meant by their creators to be unframed, and the frames were creating separation between the paintings' material immediacy and the viewer's reception of them.","context":["In 2009 the Museum of Modern Art made a major announcement concerning its displays that was dutifully reported by the New York Times: the chief curator of painting and sculpture, Ann Temkin, had decided to remove the frames from the museum’s collection of Abstract Expressionist paintings, thus “freeing” the paintings from the “domestication” of the gallery space.1 The paintings had been meant by the creators to go unframed all along, Temkin noted, and the frames contributed to a separation between the paintings’ material immediacy and the viewer’s reception of them. The paper anticipated that viewers would feel astonishment at the paintings “hang[ing] naked, their rough, paint-splattered edges and rusting staples on view to the world.”\nAs this example makes clear, the ways that viewers have been trained to look at paintings depend, at least partly, on physical frames and their virtual counterparts, which anchor us frontally, flatten paintings visually, and which clothe the works, making them presentable as art. Absent the frame, the edges of paintings present chaos, revealing painterly process, traces of questioning and decision making, and the world of the indeterminate. Given their potential power, we might assume the edges of paintings to have been subjected to rigorous art historical analysis, but it seems instead that we regulate the authority of the edge largely by ignoring it, folding it into a larger discussion of art in general, or by subsuming framing edges to a function within an image’s composition, thereby affording them a position in the centripetal movement of the composition but diminishing the part they play merely to defining the outermost limits of the picture plane. (By contrast, frames and framing devices have received sustained attention, including Immanuel Kant’s commentary on the ergon and parergon, as well as Jacques Derrida’s elaboration of those concepts.) In these strategies, the edge is normalized, its position as a site of potential infection from and to the outside reduced and managed. But what if it is the case that an unframed painting is “naked,” that somehow revealing the edges of a canvas or a panel uncovers something unseemly and inappropriate about painting? How do we speak about painting, and especially those paintings whose edges are exposed, without considering the implications of its edges?\nThe essays included in this issue’s Critics’ Page attempt a reckoning with edges. For Laura Lisbon, Maija Miettinen, and Suzanne Silver, grappling with the edge is part of their painterly practice and something that causes them to consider, respectively, the possibility (or perhaps inevitability) of painting’s objecthood, the historical convention of the tableau and the moment when painting seemed to try to free itself to become self-aware as art, and the precarity of art and the artist’s position in contemporary political society. To Squeak Carnwath, the edge possesses use-value, even as it also negotiates between the two-dimensionality of an image and the three-dimensionality of material. A curator, Aimee Ng regularly sees paintings’ edges without frames and reflects here on a 16th-century portrait that was seemingly painted to hang “naked” and unprotected. Looking at a print made one century later, Amy Knight Powell explores 17th-century understandings of artistic convention and their ramifications for plein air painting, as well as its masculine proponents. George Rush asks, “Why do my students paint the sides of their canvases?” and ruminates on the possibility that they could be resisting—by covering up—the commercial nature of the art world to which they are inextricably bound. Likewise seeing the edges of paintings as sites of resistance, César Paternosto proposes painting the framing edges rather than the face of canvases as a potent inversion of the Western evolution of the art form, a move that acknowledges Latin American civilizations’ pre-contact fascinations with edge relationships.\nAs for me, I approach edges as a lover of painting, though that tactic is not without its frustrations. When the sculptor Pygmalion fell in love with his creation, Ovid tells us, he kissed it and felt her lips become warm.2 In Jean-Léon Gérôme’s 1890 painting of the story, the living statue does more than grow warm: Galatea encircles Pygmalion with newly fleshy arms, even as her legs remain riveted to her sculpted pedestal. Unlike sculpture, painting has no such Pygmalion myth: Neither Pygmalion nor Galatea gaze back admiringly at us.\nThere are stories from art history, of course, of viewers believing themselves seduced by paintings. Why otherwise would their eyes follow us around the room even though they never reach for us? More common, though, is the lack of reaction from Leonardo’s most famous image, the Mona Lisa (1503). We think her smile mysterious precisely because it mollifies the embarrassment of suspecting that she might be laughing at us. And how much more difficult, too, to be a lover of abstract painting, in which we cannot even see reflections of our likeness. When we love abstract painting, we find ourselves loving painting itself: thus, the drive to determine its “peculiar” features. But viewers are never the equal of paintings, even when we approach them face-to-face. We are always the pursuers, the lovers. We are never the beloved.\nAnd that is the thing about the lover/beloved relationship: it is always asymmetrical. The persistence of boundaries spurs longing, so edges are useful to propagating desire, even as we viewers are spurned: “So although I couldn’t possibly feel cross with [Socrates, his beloved] and keep away from him, I couldn’t find a way to make him mine either,” Alcibiades tells us in the Symposium.3 Desire seems to thrive on asking, pursuing, looking; once possessed, understood, paintings run the risk of inertness. Their edges are built to confront the moment when yearning for them ends, knowing it is inevitable, preparing them for the moment when we leave them. Art is ideally constant; it is we who are licentious with our gazes and desires and who leave painting. The edge is not only what we see as we approach painting, it is what remains as we walk away. “He never looks back at you from the place from which you see him. Something moves in the space between. That is the most erotic thing about Eros.”4 Painting as well.\n- Ted Loos, “At MoMA, ‘Permanent’ Learns to Be Flexible,” The New York Times, 22 October 2009, http://www.nytimes.com/2009/10/25/arts/design/25loos.html.\nOvid, Metamorphoses, Book X, trans. Anthony S. Kline, 243-97, available online at http://ovid.lib.virginia.edu/trans/Metamorph10.htm#484521423.\nPlato, Symposium, 66, §219d-e.\n- Anne Carson, Eros. The Bittersweet (Princeton: Princeton University Press, 2014), 167."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:db465539-70f3-4847-872b-4bb8cfcdc624>"],"error":null}
{"question":"What are the key differences in how ash dieback and Dutch elm disease spread? How does each fungus reach healthy trees?","answer":"Ash dieback spreads when fungal spores blow from infected trees to uninfected ones. In contrast, Dutch elm disease primarily spreads through elm bark beetles that carry fungal spores on their bodies as they move between trees. Dutch elm disease has an additional transmission method - it can spread through root systems when infected tree roots graft together with healthy tree roots. Both diseases are caused by fungi that ultimately disrupt the trees' vital functions.","context":["Genes to resist ash dieback\nIn 2012, the first cases of a fungal disease - dubbed “ash dieback” - that was wiping out ash trees across Europe were detected in the UK, probably after the fungus hitched a ride here on infected imported plants. The disease spreads when fungal spores blow from an infected tree onto an uninfected specimen, which they invade. Luckily, the disease is not universally fatal, and about 5% of ash trees carry genes that mean they’re naturally resistant to the infection. To find out what those genes are, UK scientists have compared the DNA sequences of susceptible and more resilient ash trees. And this has enabled them to come up with a way to spot the trees not to fell and that are best suited for breeding programmes to repopulate forests. Amalia Thomas reports...\nAmalia - Ash trees are one of the most common types of trees in the UK and they are under serious threat of being wiped out by a fungal disease called ash dieback, which was introduced from Europe in 2012. There is to this day, no cure to this disease once the tree is affected, but curiously a small portion of our Ash trees are immune to dieback and in a recent study, scientists from Queen Mary University London and from the Royal Botanic Gardens in Kew found out why. I spoke with one of the authors of the study, Richard Nichols, who explained their results.\nRichard - We found that the trees that survived were genetically distinct from the trees that died from ash dieback. We found evidence, not that there's a single gene involved in resistance, but genes spread all across the chromosomes. Hundreds, perhaps thousands of genes, each with a small effect contributing to the resilience of some trees.\nAmalia - To put things in perspective, Richard explains the devastating effects of this disease on the UK's ash tree population.\nRichard - When the disease arrives, it's catastrophic. Around about 70% of the ash trees in a woodland will die. That includes the well-established mature trees, so across the country that amounts to about 70 million trees. A major component of our woodland.\nAmalia - Their study into how to stop ash dieback was based on samples collected by forest research.\nRichard - There was a mass screening trial, that means 150,000 trees have been planted out in various locations around the South of Britain, and we went and found the worst effected trees and the relatively unaffected trees. All in all, we took a thousand samples.\nAmalia - And from these samples they were able to extract genetic information from their DNA.\nRichard - We had a whole series of millions of reads from trees which had been affected and another series of reads, which we knew had come from trees which were unaffected, and we compared the two to look for differences. So a lot of the differences that we've found may be spurious differences because there are so many variations in the genetic code. But if we look across all of the chromosomes in an ash tree, even if nothing was going on genetically, we would be bound to find some differences just by chance between the diseased trees and those which are relatively unaffected. But when we looked at those genes which had the largest, most impressive differences, they did seem to make sense. They did seem to be genes which in other species have been found to be associated with disease resistance.\nAmalia - So from a small piece of an ashtree, Richard and his team are able to predict whether or not it will be susceptible to ash dieback.\nRichard - What we do is we create a score. We look at the trees and say, well, of our suspect genes, the genes that we think might be involved, how many is this tree got? And then we look to see how well the tree performed. And when we did that, we found we got remarkably accurate predictions. So when a tree had a really high score, then with 80% probability it was one of the more resilient trees.\nAmalia - Identifying which trees are resilient to ash dieback is a huge step forward in the battle of conservation of the ash tree population and the ecosystems they sustain, which are so significant in the UK.\nRichard - What we want to be able to do is to breed trees which will be useful to to repopulate woodlands which will be resilient to the disease and we can use the genetic information to do that, and the second thing we would like to be able to do is go to a woodland which has been infected and identify the trees which are going to do relatively well because we don't want to cut down all the trees. If some trees are going to do well and would be good to re-seed the woodland. We want to be able to identify those and not to cut them down.\nRichard - This is a really important example of why our conservation efforts should maintain large, diverse populations, because if our ash trees had been just one cultivar which was planted all over the country, we wouldn't have this natural reservoir of resilience.","Dutch Elm Disease\nDutch elm disease is caused by a fungus (Ophiostoma novo-ulmi; O. ulmi) that affects elm trees, primarily the American elm (Ulmus americana). The fungus grows in the xylem layer of the tree, blocking the movement of water and nutrients, which eventually girdles and kills the tree. The fungus is transported to host trees on elm bark beetles which feed on the bark, creating wounds through which the fungus spores can enter the tree. The fungus can also move through the root system of diseased trees into the roots of adjacent trees if the roots have grafted together.\nsigns and symptoms\nSymptoms develop quickly, usually within 4-5 weeks after infection. The first visual symptom is called “flagging”, which is when the leaves on one or more branches begin wilting and yellowing. Flagging may begin in a discrete part of the crown but can rapidly spread to all branches. Eventually, the leaves may die and fall. Brown streaking can be seen under the bark in the xylem. This brown discoloration looks like a circle of brown dots when observed in a cross section.\nSee image slideshow above for signs and symptoms.\nDutch elm disease was first described in Holland in 1921, although it is thought to have originated in Asia. It was introduced into the United States in the 1930’s on diseased timber from Europe. It rapidly spread across the country, reaching the west coast in 1973. Since its arrival, it has killed over 40 million American elm trees and has proven to be one of the most destructive shade tree diseases in North America.\nThe fungus reproduces through spores that are stored in the xylem of the host tree. Bark beetles burrow through the bark into the wood to lay their eggs, and in that process spores become attached to their bodies. When the beetles travel to a new elm tree, they transport the fungal spores and thus spread the pathogen. The pathogen can also spread between root systems of trees if they are planted close enough together to allow root grafting. From the point of inoculation, the fungus moves up and down the trunk through the xylem. Once the fungus reaches the root system, it may ascend the trunk in a wave of infection that kills the whole tree or a major section of it.\nThe American elm tree was an important forest and urban tree in North America, up until the early 1900’s. Its form and tolerance of the urban environment made it a favored street tree across the United States. It was also a prominent species in bottomland forests, greatly contributing to the health of riparian ecosystems. Since the disease’s arrival in the 1930’s, over 40 million American elm trees have been killed, and today it continues to suppress the population of elm trees in forested ecosystems. American elms can still be found in forests since the tree is a prolific seed producer and grows quickly, allowing many trees to bear seed before succumbing to the disease.\nCheryl Kaiser, University of Kentucky, Bugwood.org\nGeorge Hudler, Cornell University, Bugwood.org\nUK Forestry Commission, Bugwood.org\nWilliam M. Brown Jr., Bugwood.org\nPenn State Department of Plant Pathology & Environmental Microbiology Archives, Penn State University, Bugwood.org\nJoseph Obrien, USDA Forest Service, Bugwood.org\nNorth Carolina Forest Service, Bugwood.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:837a114e-b959-41f1-b3ff-17068226788c>","<urn:uuid:d3e615fd-2b7b-45b5-a8fe-9ed46b1969a7>"],"error":null}
{"question":"I run a small company with £500k turnover and 8 employees. Can I submit micro-entity accounts to both Companies House AND HMRC, or do I need different formats for each?","answer":"As your company meets the micro-entity criteria (turnover under £632,000 and fewer than 10 employees), you can indeed submit micro-entity accounts to both Companies House and HMRC. However, there's an important difference in what must be included for each. While you can submit 'filleted' accounts to Companies House (excluding the profit and loss account), when submitting to HMRC you must include the profit and loss account even if you removed it for Companies House filing. The accounts must be prepared using the provisions of the micro-entities regime and following FRS 105 accounting guidelines.","context":["They say two things in life are guaranteed; death and taxes. Well, if you’re a limited company in the UK then you can add a third element to that - accounts. If you do business in the UK, whatever the size of your operation and whatever your turnover, filing company accounts each year is mandatory.\nSo, if you’re a small business without an in-house finance team or accountant, how can you go about navigating the complex world of balance sheets and directors' declarations that are part of preparing company accounts?\nRead on to enjoy our guide to the whole process - demystifying this often hard-to-follow world.\nWhat is a micro entity?\nA micro entity is, put simply, a small business. To qualify as a micro entity, your business must meet at least two of the following criteria:\n- An annual turnover of £632,000 or less\n- A balance sheet total of £316,000 or less\n- An average of 10 employees or fewer throughout the year\nIf you are the owner of a small business, then the chances are your company is a micro entity, given that there are 5.4 million of them in the UK, making up 96% of all businesses.\nWhat is Companies House and who has to file with them?\nCompanies House is a government executive agency that falls under the remit of the Department for Business, Energy & Industrial Strategy and has the function of registering and dissolving all limited companies in the UK.\nIn essence, Companies House holds the details of all limited companies in the UK, along with public records of their accounts and directors. Every year, each company in the UK (including dormant businesses) must file their accounts with Companies House or risk being fined.\nIt is worth noting that Companies House is not the same as, or part of, HM Revenue and Customs (HMRC) who oversee taxation. The two bodies are separate and do not overlap in their functions. As a business, you will have to submit a tax return to HMRC separately.\nWhat are simplified accounts?\nIf your business qualifies as a micro entity, then you may be eligible to submit simplified accounts that are quicker and easier to create than the full accounts that larger businesses must file.\nCrucially, while most micro entities can file simplified accounts, there are some exceptions. You cannot file abridged accounts if your business is any of the following:\n- A limited liability partnership (LLP)\n- A public limited company (PLC)\n- A financial institution such as a bank or a mortgage lender\n- A not-for-profit organisation\nWhat are the benefits?\nFor small businesses, the benefits of simplified micro entity accounts are primarily that the process is significantly simpler and shorter than with preparing full company accounts. Micro entities do not have to:\n- Prepare a directors’ report\n- Prepare profit and loss accounts for the public record (though you should always keep your own profit and loss accounts)\n- Provide any detailed notes about the company’s creditors and debtors\nSounds ideal, right? There are some reasons you might not want to file simplified accounts, even if you do qualify as a micro entity. If you are at the larger end of the micro entity scale and are growing rapidly, then you might reasonably assume that by the following year, you will no longer qualify as a micro entity. At this point, you will have to file full accounts and this process becomes harder when you do not have the full accounts for the previous year.\nFor most very small businesses, however, micro entity accounts are a welcome way in which to cut down on the time and expense of preparing complex documents.\nThe step-by-step guide\nSo, how do you actually go about filing your company’s micro entity abridged accounts? While the process is simpler than submitting full accounts, it can still be relatively complex.\nIf your business sits at the larger end of the spectrum of micro entities, then you may want to consider hiring an accountant to help you prepare your documents. However, for many small businesses filing yourself is perfectly achievable.\nStep 1 - Prepare your documents\nAs mentioned above, as a micro entity, it is likely you qualify to file significantly simplified accounts. The principle part of your submission is a simplified balance sheet which will include the following for both the end of the accounting period in question and the end of the previous accounting period:\n- Fixed assets - these are assets that will continue to benefit your business in the long term, beyond the accounting period in questions.\n- Current assets - this includes actual money held by the business or assets that can quickly be converted into money such as stock, liquid assets and stock inventory.\n- Prepayments and accrued income - this covers advance payments and sales that your business has not yet recorded in its books. For micro-entity accounts this might include the cost of raw materials and consumables, value adjustments, staff costs and tax.\n- Creditors: Amounts falling due within one year - this is simply anything owed in the short term, up to 12 months from the end of the accounting year.\n- Net current assets - this is the value of your current assets (no. 2 in this list) minus prepayments and accrued income (no. 3) and creditors: amounts due within one year (no. 4).\n- Total assets less current liabilities - quite simply this value is your business’ total assets less your total liabilities.\n- Creditors: Amounts falling due after more than one year - refers to any amounts that are either payable or repayable by the business in the longer-term, after more than a year from the end of the accounting period.\n- Net assets - your business/ total assets minus your total liabilities.\nAs a micro entity, you will not need to include any information about creditors or debtors, but you will need to include footnotes in your filing and the signature of one or more of the company’s directors. Finally, it must include a statement to confirm that the balance sheet has been prepared in line with micro entity provisions, such as:\nThese accounts have been prepared in accordance with the micro entity provisions in the Companies Act 2006, Pt. 15 and FRS 105 the Financial Reporting Standard applicable to the micro-entities regime.\nThese accounts have been delivered in accordance with the provisions applicable to companies subject to the small companies’ regime.\nYou have the option to submit a profit and loss account, but this is not a requirement. You can also choose to submit an auditor’s report, but almost all micro entities are actually exempt from being audited under the Companies Act 2006 so this isn’t required by law. Similarly, for micro entities, a directors report is optional and you are free to save time and effort by not submitting one.\nFor micro entities choosing to file only a balance sheet and the footnotes sections, Accountstemplate.co.uk have prepared a useful example submission so you can see how it should look.\nStep 2 - Log into Companies House\nNext, you’ll need to sign into the Companies House web filing service. To do this you’ll need both your Government Gateway ID and your six-character Companies House authentication code.\nIf this is your first time filing company accounts then all you need to do to get your authentication code is register your email address and password with Companies House. Then, simply:\n- Select ‘Request an authentication code’\n- Enter your company registration number\n- Select ‘Request code’\nUnfortunately, for security reasons, you cannot be told your authentication code by email or over the phone. A letter with your code will be sent to the address where your business is registered.\nStep 3 - File your accounts before the deadline\nThe deadline for filing company accounts could be different for each business and is calculated based on your company’s Accounting Reference Date (ARD).\nFor a new business, your first ARD will be the anniversary of the last day in the month the company was incorporated. The filing deadline for your first accounts will then be 9 months on from your ARD. For example:\n- Your business was incorporated on 16th April 2020\n- The last day of the month is 30th April 2020\n- Your Accounting Reference Date is 30th April 2021\n- Your accounts deadline will be 30th January 2022\nFor existing businesses, your ARD will be the anniversary of the day after the previous financial year ended. It may sound complicated but in simple terms, you will have 9 months after the end of each accounting period in which to file your micro entity accounts with Companies House.\nYou can choose to register for an email reminder service from Companies House who will message you 4 weeks before your deadline to remind you to file your accounts.\nYou may also like:\nThis content has been created for general information purposes and should not be taken as formal advice. Read our full disclaimer.\nWe've made buying insurance simple. Get started.\n- 06 January 20222 minute read\nFind the right person for your vacancies with our tips for writing an effective job description, including a detailed, free, downloadable template.","A micro-entity is a very small company or LLP. We will define the criteria for micro-entity status below. Micro-entities may be able to produce a much simpler set of year end accounts for their members and provide less information for the public record.\nWhy did the Government create the new micro-entity classification?\nWhen the UK Government approved The Small Companies (Micro-Entities’ Accounts) Regulations 2013, the Department for Business, Innovation & Skills released the following statement:\nMicro-entities are, in many instances, effectively owner-managed. Statutory financial statements of micro-entities, therefore, may not need to facilitate communication between shareholders and management in relation to the company’s performance. For the smallest of companies, the burdens associated with comprehensive financial reporting requirements may be disproportionate when compared with other small companies.\nWhat is the aim of the micro-entity regime?\nThe aim of the micro-entity regime is to save the very smallest of businesses both time and costs by:\n- Offering a highly simplified format of statutory accounts containing fewer elements even when compared to small company abridged accounts.\n- Simplifying the accounting standards that should be applied such that they will be more widely understood and with less specialised knowledge required.\nReady to file micro-entity accounts?\nInform Direct provides a simple and efficient approach to the task of producing fully compliant micro-entity accounts.\nNotes regarding calculation of key company size thresholds:\nThis threshold should be proportionately adjusted if the accounting period is shorter or longer than 12 months. For example, if the accounting period were only for a nine-month period then the reduced turnover total would be £474,000 (being 9/12 of £632,000).\n2 Balance sheet total\nThe total of the amounts shown as assets on the balance sheet.\n3 Average number of employees\nThe ‘two years in, two years out’ rule\nIf this is the company’s first financial year and the company is within at least two of the three size thresholds, then it may qualify as a micro-entity. For subsequent years, however, the rules are more complex and are explained in our guide entitled How to calculate company size when preparing year end accounts, including a flowchart showing how to navigate the potential complexities of the ‘two years in, two years out rule’.\nAre some types of company excluded from being treated as micro-entities?\nHelping you manage your companies, whatever their size\nAn important part of managing a company is keeping statutory books and Companies House filings up to date.\nInform Direct is the perfect tool to make this task a whole lot easier, meaning you can focus more on running your business.\nEven if your company qualifies on the basis of size, the nature of the company’s business may exclude it from being treated as a micro-entity.\nSection 384B excludes the following from being treated as micro-entities:\n- Any company excluded from being treated as a small company under section 380 of the Small companies’ regime\n- Investment undertakings, financial holdings, credit institutions and insurance undertakings, and\nWhat about group companies?\nThere are further exclusions if the company is part of a group. Micro-entity provisions cannot be applied if:\n- The company is a parent company which prepares group accounts, then it cannot produce its individual accounts using micro-entity provisions.\n- The company is part of a group and its accounts are included in consolidated group accounts.\nCan LLPs prepare and file micro-entity accounts?\nYes. Regulation 6 of The Limited Liability Partnerships, Partnerships and Groups (Accounts and Audit) Regulations 2016 states that:\nSections 384A and 384B(1) [of the Companies Act] apply to LLPs.\nQualification is by meeting the same company size criteria already detailed in this article and by ensuring that the LLP does not undertake any ineligible business or form part of a group.\nIs a company that qualifies as a micro-entity required to adopt the micro-entity regime?\nNo. A company’s managers may always choose to prepare and file year end accounts that provide more than the minimum required disclosure (by adopting a financial reporting regime that would apply to larger companies). Indeed, there are both advantages and disadvantages to opting for the reduced disclosure requirements that apply when preparing micro-entity accounts, and these should be carefully weighed.\nWhat are the advantages of preparing micro-entity accounts?\nThe main advantages of preparing micro-entity accounts are:\n- Highly simplified presentation of the balance sheet and profit and loss. The company may choose from two different formats for the balance sheet and only one format for the profit and loss account.\n- No requirement to prepare a directors’ report. This requirement was removed for micro-entities by section 415 (1A) of the Companies Act 2006.\n- No detailed notes to the accounts required. Instead, details are only required of certain ‘minimum accounting items’ – and these should be disclosed at the foot of the balance sheet.\n- Accounts are presumed to give a true and fair view. This contrasts with accounts prepared under the small companies’ regime which require a degree of appraisal to ensure that they present a true and fair view despite limited disclosure requirements.\n- Accounts can be ‘filleted’. There is no requirement to file the profit and loss account for the public record.\n- Significant simplifications to recognition and measurement requirements. For example, the accounting for financial instruments has been simplified whilst the need to account for deferred tax is not required (or indeed allowed).\nThese advantages mean that a decision to prepare micro-entity accounts is likely to save the company both time and money. Moreover, with less disclosure required for the public record, less company information will be available to competitors.\nWhy might a company choose not to prepare micro-entity accounts?\nIt might not always be advantageous for an entity to opt for the reduced accounting disclosure allowed by the micro-entity regime. For example, you should consider the following:\n- Will micro-entity accounts provide enough information? Far less detail is provided in a set of micro-entity accounts as compared to accounts produced under the small companies’ regime. This reduced disclosure may not be sufficient for current or future lenders and creditors, for credit rating agencies or for shareholders. Additional costs may therefore arise if any of these agencies request additional information. It may be difficult to secure new investment or shareholder funding without providing more financial details than those given in micro-entity accounts.\n- Will the company be able to prepare micro-entity accounts next year? It may not be appropriate to prepare micro-entity accounts for a company that is growing very quickly and that is unlikely to qualify to prepare accounts under the same regime in the near future. A better option might be to prepare accounts under the small companies’ regime but taking advantage of the reduced disclosure allowed when preparing abridged or ‘filleted’ small company accounts.\n- Will the available accounting formats be flexible enough? Only one format of profit and loss account is allowed (Format 2), and whilst two formats of balance sheet are available, asset and liability classes are highly summarised and cannot be analysed out into further detail. For example, there is only one line item for fixed assets and this cannot be analysed into intangible assets, fixed assets and investment property.\n- Will the accounts give an accurate presentation of the company’s position? The highly summarised format, particularly when accompanied by a move away from the use of accounting expertise, may increase the risk of the accounts providing an inaccurate or misleading picture of the company’s true position.\n- What impression of the company might micro-entity accounts give? In some industries and for some companies, choosing to file more detailed accounts may give a sense of prestige or status to the company.\n- Will the removal of accounting policy options present a problem? In order both to simplify their preparation and to deal with the lack of explanatory notes in a set of micro-entity accounts – many accounting policy options have been removed. For example, FRS 105 (the financial reporting standard that must be applied by micro-entities) does not allow the recognition of deferred tax, and the following commonly adopted accounting treatments are also disallowed:\nAssets carried at fair value or revaluation\nA micro-entity that has an investment property currently valued at fair value or open market value would have to restate the value of the property to cost. In addition, if the company has not been reflecting depreciation because it has been revaluing the property, FRS 105 would require that all depreciation accumulated since the date of acquisition of the property now be reflected in the micro-entity accounts. The inclusion of a revaluation reserve on the balance sheet would not be allowed.\nIn these circumstances a decision to prepare micro-entity accounts could result in a significant change to the company’s balance sheet position.\nCapitalisation of development costs and borrowing costs\nA company that has in the past capitalised development and/or borrowing costs is not allowed to do so when preparing micro-entity accounts. Instead the previously capitalised costs must be expensed to the profit and loss account.\nWhat financial reporting standard should be used when preparing micro-entity accounts?\nCompanies choosing to apply the micro-entities regime must apply FRS 105, The Financial Reporting Standard applicable to the Micro-entities Regime when preparing their statutory accounts. FRS 105 is effective for accounting periods beginning on or after 1 January 2016 but earlier adoption of FRS 105 for accounting periods beginning before this date is allowed.\nIf the company chooses to include information in addition to the ‘minimum accounting items’ required in their micro-entity accounts, then for these additional disclosures the company must refer to section 1A of FRS 102.\nWhat elements must be included in a set of micro-entity accounts?\nA full set of micro-entity accounts includes the following elements:\n- Simple balance sheet and footnotes.\n- Signature of a director and their name printed on the balance sheet.\n- Statement on the balance sheet above the director’s signature that the accounts have been prepared in accordance with the micro-entity provisions.\n- Simple profit and loss account.\n- Auditors’ report. However, in reality most micro-entities will be able to claim exemption from audit.\nHowever, if the micro-entity prepares and files ‘filleted’ accounts in accordance with the provisions of section 444 of the Companies Act 2006 then they can choose not to file the profit and loss account and any supporting notes.\nWhat are ‘minimum accounting items’?\nNo disclosure notes are required to support micro-entity accounts. However, details of charges, contingent liabilities, capital commitments, advances, guarantees and other financial commitments must be disclosed at the foot of the balance sheet. These items are referred to as the ‘minimum accounting items’. In December 2017 the FRC also released Triennial Review amendments to FRS 105, and these require two additional notes to the accounts: the average number of employees and (if applicable) details of any off-balance sheet arrangements. These must be reported for accounting periods beginning on or after 1 January 2019, but earlier application of these amendments is allowed.\nCan micro-entity accounts be submitted to HMRC?\nYes. Micro-entity accounts that have been prepared using the provisions of the micro-entities regime and following the accounting guidelines set out in FRS 105 can be submitted to HMRC as part of your company’s annual tax return. Note however that you must include the profit and loss account, even if you prepared ‘filleted’ accounts and removed the profit and loss account for filing with Companies House.\nIf you use Inform Direct to file micro-entity accounts, then we will also provide an IXBRL-tagged version of the accounts that can be included with the corporation tax return (CT600) that you file separately with HMRC outside of Inform Direct.\nWhat are the alternatives for a company choosing not to apply the micro-entities regime?\nIf an eligible company chooses not to apply the micro-entities regime (and FRS 105) then in practice they will usually choose to apply the small companies’ regime and the accounting guidelines set out in amended FRS 102 (including the new Section 1A for small entities).\nWhat statements should be included on the balance sheet of a micro-entity?\nMost very small companies using the micro-entity regime will be exempt from audit and will be filing unaudited accounts. These companies should include the following statements on their balance sheet, in a prominent position above the director’s signature:\nStatement that the accounts have been prepared under micro-entity provisions:\n“These accounts have been prepared in accordance with micro-entity provisions.”\nStatements when the company is entitled to audit exemption:\n“The directors consider that the company is entitled to audit exemption under Section 477 of the Companies Act 2006 for the year ended [insert year end].”\n“The members have not required the company to obtain an audit of its accounts for the year ended [insert year end] in accordance with Section 476 of the Companies Act 2006.”\n“The directors acknowledge their responsibilities for complying with the requirements of the Companies Act 2006 with respect to accounting records and the preparation of accounts.”\nStatement that the profit and loss account has not been delivered:\n“These accounts have been delivered in accordance with the provisions applicable to companies subject to the small companies’ regime. The profit and loss account has not been delivered to the Registrar of Companies in accordance with special provisions applicable to companies subject to the small companies’ regime.”\nOur streamlined wizard for efficient micro-entity accounts production makes it easy to get it right first time.\nA previous version of this article was originally published on 2 October 2018. It was completely revised and updated in June 2022 for accuracy and comprehensiveness."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ecdb8f6b-ec84-4737-969f-4f9fb3919280>","<urn:uuid:fa679d52-73fc-4918-9a96-7fc1e70a3583>"],"error":null}
{"question":"What were the main differences between early US and Soviet diplomatic presence in Central Africa during the Cold War?","answer":"The US maintained a minimal diplomatic presence in Central Africa, exemplified by their small embassy in the Central African Republic with just a handful of staff, a few thousand dollars in aid, and 7 Peace Corps Volunteers. In contrast, the Soviet Union maintained a more substantial presence, with each of their embassy delegations including both KGB and GRU (military intelligence) agents. The Soviets actively expanded their presence by opening multiple embassies across African countries from 1960 onwards, viewing intelligence and covert operations as the best means of waging a 'low-cost' Cold War, while the US appeared to follow a more passive 'don't just do something—stand there' approach.","context":["by Ambassador James Rosenthal\nDiplomacy is more than high-level foreign policy-making and execution, formal negotiation of treaties and other agreements, carefully-crafted representations to foreign governments, and fancy parties and ceremonies. The mundane day-to-day issues a diplomat at a foreign post deals with can also be compelling. And sometimes a little goes a long way….\nThe Central African Republic, with its capital at Bangui, did not weigh heavily in U.S. foreign policy in the 1970’s. Our instructions were essentially to maintain a modest official presence at minimum cost and worry to Washington, perhaps most aptly summed up as “Don’t just do something—stand there”. We had little at our disposal—a few thousand dollars in aid, 7 doughty Peace Corps Volunteers, and a small embassy staffed by a handful of Americans and local personnel.\nSo when the ruling dictator, the mercurial President for Life and Field Marshal Jean Bedel Bokassa, announced the “official” joint wedding of his two adopted daughters, we did not expect to figure in it prominently. The rest of the diplomatic corps, however, was in a tizzy over just what an official wedding meant and what their official participation and—most importantly—what their official wedding gifts (if any) might be.\nWe had not planned on any such gifts. However, our remarkable desk officer back in Washington somehow unearthed a couple of them at the White House—two large, hand-made porcelain plates left over from President Nixon’s historic trip to China a few months before—that she proposed as suitable offerings. We readily accepted, and soon the gifts arrived, impressively wrapped in gold ribbon prominently affixed with the presidential seal.\nThe Central African Foreign Office had directed that all gifts should be left off at their protocol office, from whence they would be forwarded to His Excellency the President for Life and the two brides-to-be. But I argued to the Director of Protocol that these were personal gifts from my sovereign leader to his and that I would like to present them directly to the President myself. The Director agreed, and a few days later I received a call saying that I should come to the presidential palace with my gifts the next afternoon.\nAt the appointed time my administrative officer and I, each bearing one of the precious packages, entered the palace reception room, where we found the Soviet and the Romanian ambassadors already waiting. They asked what we were there for, and they were clearly uncomfortable when I responded “To present President Nixon’s wedding gifts to President Bokassa personally, of course. Why are you here?” The Soviet ambassador replied “To present Comrade Brezhnev’s best wishes to the President”. I inquired whether he had any gifts to present, and he rather glumly replied that he had dropped them off at the Protocol Office a few days earlier. The Romanian was there to offer Comrade Ceaucescu’s congratulations and did not mention any gifts at all. I quietly savored this small Cold War victory, which was further enhanced when I was called in ahead of either of them to see the President.\nBokassa greeted me effusively and marveled that President Nixon had found time from his busy schedule to think of him and his family on this joyous occasion. He was clearly pleased with the gifts we presented, and he insisted on a quick toast to the amicable, longstanding, and everlasting ties between our two countries. Responding to him in kind, I privately wondered just how everlasting those ties really were. However, for the moment we were in his good graces and U.S.-Central African relations were riding high.\n(How high I found out later at the wedding banquet when I was seated at the table much closer to Bokassa and the wedding party than was the French Ambassador, who was Dean of the Bangui diplomatic corps and much senior to me, a lowly Charge d’Affaires. His government had inexplicably refused to recognize the wedding as “official” and had sent no gifts, and I was personally rather embarrassed at being used by Bokassa in this obvious retaliatory snub.)\nI had no illusions that things would last, and in fact we went slowly downhill from there in the ensuing months. But with very modest means—it probably cost the U.S. Government no more than a few hundred dollars—we were able to keep relations on an even keel and “…just… stand there” a while longer. And considering that the Soviet and other Eastern European presence in Bangui had always far exceeded ours, that tiny Cold War triumph in the waiting room was an added bonus!\nAmbassador Rosenthal graduated from Stanford with a degree in International Relations. He taught International Relations and Political Science at the U.S. Military Academy at West Point, where he served as a State Department faculty member. He is also a graduate of the National War College. After retirement from the Foreign Service, he became Executive Director of the Commonwealth Club of California and is the Past President of the San Francisco International Diplomacy Council. He travels extensively and speaks and writes on foreign affairs.","Vladimir Putin is not developing his networks of influence in Africa by chance. It draws on the rich history of relations that the Soviet Union has forged with African countries since the 1960s and the efforts made by Russian spies against the background of the Cold War.\nThe summer of 1960 was very hot in the future Democratic Republic of the Congo. The country wrests independence from Belgium in June, the first democratically elected government is installed, then power struggles culminate in Joseph-Désiré Mobutu’s first coup d’état in September and a few months later with the assassination of Prime Minister Patrice Lumumba. A rapid series of events that will mark the spirits of this crucial year of the struggles for liberation.\nAnd not just in Africa. About 11,000 kilometers from Kinshasa in Russia, the Kremlin’s foreign policy is taking a new turn in light of the crisis in the Belgian Congo. Alexandre Chélépine, then head of the KGB, realizes that he has almost no spies south of the Sahara. The secret agents had a strong presence in Egypt, also somewhat in the Maghreb and had strong friendships with the Communist Party in South Africa.\nA handful of spies to save Lumumba\nAn inadequate net for the chief of Soviet spies. Especially since for Nikita Khrushchev, in power in Moscow, opening up to Third World countries, especially in Africa, is a priority to mark a break with his predecessor, Joseph Stalin. The “little father of the people” actually did not care much for his “children” on the African continent.\nThis is how the Congo crisis became “the first proven case of KGB intervention in the affairs of a sub-Saharan African country”, notes Natalia Telepneva, a specialist in the history of Soviet intelligence services in Africa at the University of Strathclyde in Glasgow.\nThis interference marks the beginning of a race for Russian influence in black Africa. And despite a lack of interest in the region between the early 1990s and late 2000s, the Kremlin made its mark. Thus, “to bring Russia back to Africa, Vladimir Putin was able to take advantage of the relatively good image of the USSR on the continent and of a network of old contacts”, summarizes Marcel Plichta, specialist in Russian influence in Africa at the university of St. Andrews.\nHowever, at the time of the Congo crisis, this legacy does not yet exist. “The chief Africanist in the USSR at the time, Ivan Potekhine, had only visited Africa for the first time during the 1950s”, underlines Natalia Telepneva.\nThe operation to rescue Prime Minister Patrice Lumumba, who appeared to be an ideal fellow traveler for the USSR, had at the time benefited from few funds. “Moscow was only able to send a handful of agents to the site,” explains Natalia Telepneva. Joseph-Désiré Mobutu’s 1960 coup, actively supported by the CIA, was therefore an all the more painful failure for the KGB.\nCold War “low cost” in Africa\nThe Soviets therefore had some catching up to do in the area of influence strategy. They could count on the enthusiasm from the beginning to try to achieve this. “For the first agents who joined the Africa division (of the KGB, ed. note), the continent offered interesting perspectives in terms of espionage, and the goals they pursued – to support liberation movements while dissecting the activity on the ground in the United States – appeared as noble”, writes Natalia Telepneva in her book “Cold War Liberation” (ed. The University of North Carolina Press, 2022) based on the memoirs of Vadim Kirpitchenko, who was the first director of the Africa section of the KGB.\nFrom 1960, Russia multiplied the openings of embassies in African countries. Each of its delegations “included one KGB agent and another from the GRU (military intelligence, ed. note),” Natalia Telepneva specifies.\nThe crisis in the Congo served as a lesson. “Moscow understood that the USSR did not have the same resources as the Western powers present in Africa. Intelligence and covert operations appeared to be the best means of waging a ‘low-cost’ Cold War (the investment is essentially human, Ed)”, sums up Natalia Telepneva.\nAfter all, the Russian failure will have had a beneficial effect for Moscow. Russia appeared there as an ally for a man – Patrice Lumumba – who would become a myth for the liberation movements on the continent. The Americans were perceived as partners of the colonial countries. This image of a Soviet Union on the “good side” of history in Africa was reinforced by its support – sometimes exaggerated by Russian propaganda – for Nelson Mandela’s ANC in the face of the racist apartheid regime.\nRussian spies will go to great lengths to cultivate this impression. It was the start of a major campaign of “active measures”—comprising what today would be called disinformation and propaganda operations—to portray the USSR as a selfless supporter of a decolonized Africa, while Washington would represent the puppet master scheming in the shadows . to protect his interests.\nThe KGB will deploy its entire arsenal: manipulation of the local media, production of false documents to make the CIA the enemy to be defeated. Moscow will especially feed the paranoia of Kwame Nkrumah, the first leader of independent Ghana, who saw himself as “the African Lenin”. Eventually, he would see American spies everywhere: “In 1964, a forged letter produced by Section A describing a CIA plot so enraged him that he wrote a letter directly to US President Lyndon Johnson, accusing the CIA of to mobilize all its resources for the one purpose. to overthrow it,” reads the Mitrokhin archive, named after Vasily Mitrokhin, the KGB’s chief archivist who defected in 1992, taking 30 years of notes with him.\nFrom Soviet dream to disappointment\nIt is hard not to see in these “active measures” the progenitor of the online disinformation activities of the “troll factories” of Yevgeny Prigojine, the head of the Wagner mercenary group. Putin’s Russia uses a version 2.0 of the Soviet narrative in Africa: at the time, the USSR presented itself as the champion of decolonization, while “Russia claims to be the ally of Pan-Africanism against the old colonial powers”, explains Marcel Plichta. . The Russian campaign to promote anti-French sentiment in the Central African Republic (and Mali) is just one example.\nBut all these efforts of the KGB, which so inspired today’s Russia, were not crowned with success at that time. At least not up to Moscow’s hopes. The USSR “believed that these countries would naturally approach communist ideology and therefore the Soviet bloc. But it was more complicated than expected,” says Natalia Telepneva.\nThe first “friend” of the USSR in sub-Saharan Africa, Kwame Nkrumah, at the head of Ghana for six years, was overthrown in 1966 after his authoritarianism. The two other countries that have most openly sided with Moscow – Mali under Modibo Keïta and Guinea under Ahmed Sékou Touré – have not abandoned the memory of communist paradises. The painter was ousted from power in 1968, after eight years in power, while the Guinean remained for more than 25 years, until 1984, at the head of a very brutal regime.\nIt was not until the second wave of decolonization and the dismantling of the former Portuguese empire in Africa – Mozambique, Guinea-Bissau, Angola – in the 1970s that Soviet influence operations resumed. But this time, leader Leonid Brezhnev is urging the intelligence services “to redeploy their efforts to strengthen military and security cooperation with the armies of ‘friendly’ countries,” says Natalia Telepneva. The Kremlin is becoming aware of having, until now, underestimated the role of the military in power struggles in Africa.\nUSSR and “soft power”\nThe USSR then became one of the main arms suppliers to African countries. During the winter of 1977, Ethiopia, backed by the USSR against Somalia, saw “a Soviet plane loaded with military equipment and instructors land every 20 minutes,” read the Mitrokhine archives. .\nHere again, it is an approach reminiscent of that of Vladimir Putin and the Wagner group. “Moscow’s main strategy to expand its influence in Africa, besides sending Wagner’s mercenaries, is the multiplication of military cooperation agreements (21 signed between 2014 and 2019, editor’s note)”, emphasizes Marcel Plichta.\nDuring the Cold War, military aid was not limited to the supply of weapons. The USSR also trained thousands of “freedom fighters” at home. Training Center-165 at Perevalnoe in Crimea, now the Ukrainian peninsula annexed by Russia, became the most famous example.\nThe handling of weapons was one lesson among others: “There was also political training, made of excursions to tourist sites, visits to collective farms or film screenings. The courses also included an introduction to Leninism-Marxism and discussions on the history of colonization.” , specifies Natalia Telepneva.\nIn addition, Moscow very early on measured the role of education in deepening ties with Africa. This was the aim of the Patrice-Lumumba University, which was inaugurated in Moscow by Khrushchev in 1961. It has trained more than 7,000 students from 48 African countries in fifty years in fields as diverse as physics, economics or public administration. But African students were also admitted to other institutions in the USSR.\nFor Russian spies, it was a good breeding ground for finding potential recruits. The deputy director of Lumumba University was also a member of the KGB. But “it was not the most important thing for Moscow”, judge Konstantinos Katsakioris, a specialist in education issues in Africa and the former USSR at the University of Bayreuth. It was a matter of improving the USSR’s brand image among Africans. All these students were to preach the good Soviet word when they returned to their country.\nIt is also an asset for Vladimir Putin. After the fall of the USSR, Moscow, too busy with its internal problems, gradually withdrew from Africa. But all these former students who were educated in the former USSR stayed put. When Vladimir Putin decided in 2014 to reinvest in the African continent in search of new allies to overcome the diplomatic isolation caused by his annexation of Crimea, he knew his agents could find friends there. “The warriors and students were young when they left for the USSR. Today, some of them have become influential members in their country of origin,” emphasizes Marcel Plichta. So many potentially receptive ears that Putin’s and Prigozhin’s men can whisper into."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cd676ba2-e480-4295-97fe-8cecc6cdf61f>","<urn:uuid:bde87d48-dc28-47b1-8300-ef33d722c515>"],"error":null}
{"question":"What is the relationship between sunlight and natural ecosystems, and how does UV exposure affect human health across different skin types?","answer":"In natural ecosystems, sunlight plays vital roles - plants require it for photosynthesis, grazing animals orient themselves to optimize sunlight exposure, and marine life like corals utilize filtered sunlight in shallow-water reefs. Looking up from a rainforest floor shows how vegetation creates openings for sunlight penetration. Regarding human health impacts, UV exposure affects people differently based on skin type. Caucasians, especially those with blonde or red hair and blue or green eyes, have a greater risk of developing skin cancer than non-whites. People with fair skin that burns or freckles easily are particularly vulnerable. The risk increases with age due to accumulated UV exposure, and having a family history or previous diagnosis of skin cancer also increases susceptibility.","context":["sunlight in a sentence\nExample sentences for sunlight\nAdded to that, universities don't want the ideological make-up of their professors to undergo too much sunlight.\nThrongs of students were out that day, lounging in the kind of late-summer sunlight that keeps brochure photographers in business.\nDo make sure to set up your schedule so you get out in the sunlight.\nWard is more inclined toward orbiting sunshades that would cut down on the amount of sunlight hitting the planet.\nOn the one hand, it is kind of pretty, shiny things that catch the sunlight.\nIt had long been dogged by a stereotype as a place for nerds and social misfits who shun sunlight and conversation.\nSunlight streamed through the winter-bare canopies of the tupelos and cypresses.\nCollect sunlight and extract useful forms of energy from it, rather than expending energy on air conditioning.\nImagine a sculpture that could help make the air cleaner simply by sparkling in the sunlight.\nSunlight bores in through a few knotholes in the siding.\nThe sunlight pours in through the window, translating color to vision.\nOzone forms when sunlight reacts with various pollutants, so concentrations can soar on hot, sunny days.\nWe keep walking, and soon the sunlight stabbing through the trees signals a clearing.\nThe graffiti-covered archways overhead let sunlight into the station.\nIts skin was dark gray, glinting in the sunlight, with mottled white dots.\nEight columns form the basis of a roof that lets in shards of sunlight.\nFurther inspection revealed the source of the light: an underwater cavity that allows sunlight to filter in.\nBookshelves line three of its walls from floor to ceiling, and two windows let in a cheery amount of sunlight.\nIt was interrupted in mid-construction by a lawsuit filed by local residents who objected to their loss of sunlight and views.\nTimrod described the dawn of the eventful day as the city in the broad sunlight of heroic deeds waited for the foe.\nThe blinding sunlight did not temper the cold, which cut the face and made the lungs ache.\nIt took long years of weary labor to make good the claim of the sunlight to such corners of the dens as it could reach at all.\nDappled sunlight filters through the branches of ancient trees and a fountain creates cool music.\nClimate was cold and both of them are warming each other sleeping under the sunlight.\nPhoto sensors dim lamps when sunlight is bright enough to read by.\nSunlight is free energy, but the equipment that captures it and turns it into usable electricity can be costly.\nFor example, big windows placed on the sunny side of a building allow sunlight to heat-absorbent materials on the floor and walls.\nAt left a bird and a mountain symbolize the sunlight that emerges in the east every morning.\nUse guided inquiry to have students investigate direct and indirect sunlight.\nGrazing animals are known to orient themselves to minimize heat loss by wind or maximize their exposure to sunlight.\nSet up the solar stove on a flat surface with unobstructed sunlight.\nCorals and fish shine in sunlight trickling through to the shallow-water reef.\nLooking up from a rain forest floor shows openings where sunlight gets through.\nParticles thrown into the atmosphere by volcanic eruptions are known to cause a global cooling effect by reflecting back sunlight.\nIn the winter feeble sunlight breaks out for only an hour or two, if at all.\nReturn unused product to original package and store in a dark place away from high temperatures, humidity, or direct sunlight.\nThe vine is invasive, killing anything it can overtake by blocking sunlight.\nAtmospheric oxygen comes from the splitting of water molecules, a combination of hydrogen and oxygen, by sunlight.\nBut in the future some robots will need to operate beyond the reach of power grids, sunlight, or helping human hands.\nSulfur dioxide combines with water vapor to form sulfuric acid particles that scatter, reflect, and absorb sunlight.\nWhat interests scientists is how these wings shimmer in sunlight.\nRooftop gardens require particularly unfussy plants since they get direct sunlight and sporadic water.\nSelecting a warm toned hue adds instant sunlight to any space.\nNeeds protection from drying winds and strong, reflected sunlight.\nSuffused with sunlight and open to a deck, the room allows easy indoor-outdoor living.\nFilmy white shades diffuse sunlight through the sitting area-helped by pale fabrics and rugs.\nTo make more plants, break off a one- to three-segment piece and set it in a warm place out of direct sunlight to callus.\nThe unobtrusive panels protect the bamboo from rain but allow diffused sunlight to filter through.\nKeep the containers out of direct sunlight for a few days to let the transplants recover from the move.\nGarden microclimates are influenced by hills and hollows, sunlight, and structures.\nBt breaks down quickly in sunlight, and several applications may be required to control an infestation.\nAlways go darker and browner than what you think you want, because full sunlight blows out color.\nThe bending of sunlight by the raindrops is the key to forming colorful rainbows.\nSpend time outdoors in the sunlight in your destination.\nMove both indoor and outdoor plants out of direct sunlight.\nDuring the day, the monument appears gold in the sunlight and glows pink at night.\nLightweight light-colored clothing reflects the sunlight and helps in keeping cool.\nThe temperature of the sun is such that it supports a nuclear-fusion reaction that generates bright sunlight.\nSunlight coming through the windows heats up the internal air which has nowhere to go as the air-conditioning struggles to cope.\nThe location of these craters means that parts of their floors never see direct sunlight.\nFast-growing crops would be planted in the tropics, where sunlight is abundant.\nThe temperature of the sun is such that it supports nuclear fusion that generates bright sunlight.\nBut above it is a thick smog, made of organic compounds broken up by the sunlight.\nMost of it will be supplied, belatedly, by the market itself-especially if it is bathed in the cleansing sunlight of transparency.\nAnd it can help with the efficient capture of sunlight to make electricity and non-fossil fuels.\nConventional photovoltaic cells, made of silicon, are designed to collect energy from the wavelengths of sunlight.\nConventional photovoltaic cells, made of silicon, are designed to collect energy from sunlight.\nOne is to reduce the amount of incoming sunlight that the planet absorbs.\nFruit, vegetable and fiber plants flourish with sunlight and water but so do plant pests and diseases.\nOne is albedo-the posh scientific name for how much sunlight is absorbed by a planet's surface, and how much is reflected.\nIce is pure white and reflects sunlight back into space.\nAirports can pose a far bigger threat to local air than previously recognized, thanks to the transformative power of sunlight.\nWhen sunlight hits a raindrop, some photons glance off the surface.\nSpots where the atmosphere is denser block more sunlight than spots where it is thinner.\nDepending on the intensity of your sunlight, it should take an average of four hours to reach a full charge.\nAny amateur gardener knows plants need two things to survive: sunlight and water.\nSunlight, which appears white to the human eye, is a mixture of all the colors of the rainbow.\nThe answer: store sunlight as heat energy for such a rainy day.\nThe light emanates from molecules excited by sunlight or cosmic rays in the upper atmosphere.\nMeanwhile, sunlight also strikes the polymer, where it's converted to electricity.\nPlankton in the open ocean need a precise mixture of sunlight in shallower waters and nutrients from the deep.\nThere is the gradual stirring in response to the warming pink glow of sunlight on the eyelids.\nPhotovoltaic cells remain woefully inefficient at converting sunlight into electricity.\nIt can be obtained from food or manufactured by human skin exposed to sunlight.\nPlants turn sunlight into energy using photosynthesis, but they rely on two slightly different forms of the process.\nWhen the volcanoes erupted, they emit particles in the atmosphere that reflect sunlight and the planet cools.\nImagine a world where sunlight can be captured to produce electricity anywhere, on any surface.\nFor example, cooler weather causes fewer clouds, which reflect less sunlight.\nIt is said in politics that sunlight is the best disinfectant.\nNature has favored genes for tall stalks, because in nature plants must compete for access to sunlight.\nSo few atoms are present that the total available energy is much less than that from ordinary sunlight.\nThe dry sunlight and the green summer landscapes have been invested with a sinister quality which must be new in literature.\nThe rain begins again, and water gleams in sunlight.\nSome even come through exposure to sunlight that can harm the eyes and the skin.\nSunlight was shining on the aircraft, and some surfaces were more reflective and some less reflective.\nWhen he looked away from the sun, his eyes were drawn to the cool shadow of a deer standing against the yellow ribbon of sunlight.\nAs sunlight drew away from the orchard, it came to him: the thread that bound their lives together.\nPerhaps it's because flu viruses flourish in cool temperatures and are killed by exposure to sunlight.\nBasically, baboons have about a half dozen solid hours of sunlight a day to devote to being rotten to each other.\nMoreover, because it had been buried, sunlight would not break it down.\nHowever, our biological clocks seem to need sunlight to stay set on a regular cycle.\nIt is especially desirable in southern climates where both harsh sunlight and frequent downpours occur.\nThe need for tablets that can be read in direct sunlight becomes more pressing.\nSunlight all over his face breaking the surface of something.\nThe heart of the world lies open, leached and ticking with sunlight.\nHot sunlight can't help but glint from a bumper and produce a faint reflection of the windshield on a garage door.\nHe can feel a tremor of fresh sunlight, warm and warmer.\nSunlight streamed through a two-story bank of windows.\nSunlight filled the kitchen from the big window behind the sink.\nThe fog was thinning, and there were rays of sunlight here and there.\nThree crystalline liquor bottles glint in the sunlight refracted by a chrome lamp.\nIf you looked away from the site, at the buildings gleaming in the sunlight, it was almost as if nothing had happened.\nIn my memory, beyond the shadows of the hemlock the spring was always in a ray of sunlight.\nIn the background, sunlight dances over a hedge of yellow flowers.\nIf a flavor can be verdant, here it is: the heady soul of a plant with leaves that marinate in sunlight.\nThey use no herbicides and have developed their own trellising system to admit maximum sunlight into the foliage.\nLand and sunlight cannot be concentrated in cities but are spread more or less evenly over the planet.\nPhotos disappear into the sunlight, books get eaten by all sorts of insects and stuff.\nE-book readers can be used outdoors and in direct sunlight indoors.\nBiotechnology could be a great equalizer, spreading wealth over the world wherever there is land and air and water and sunlight.\nSunshades in the stratosphere to reflect sunlight away from our planet.\nGreen algae may rely on quantum computing to turn sunlight into food.\nEverything goes blurry, and sunlight seems to reach his eyes bent every which way, as though he has suddenly slipped underwater.\nTiny chemical particles thrown into the atmosphere reflect sunlight back into space.\nThe zodiacal light-the faint glow of sunlight reflected off interplanetary dust-is easiest to see about now.\nAt the same time, the extra sunlight causes more evaporation off the ocean, which adds to downpours in the western tropics.\nBut our whole cultural evolution has been to remove us from sunlight.\nIt's the cloud of expanding gas that reflects sunlight and makes a comet bright, so there you go.\nConsider that cep patients learn early in life that sunlight is harmful and so leave their homes only at night.\nIt will start giving us our first real pictures from sunlight reflected off the planet's atmosphere.\nThose deserts have tremendous energy resources in the form of sunlight.\nThat's called the zodiacal light, and is caused by the reflection of sunlight by dust in the plane of our solar system.\nThe ash would have darkened the atmosphere, letting slightly less sunlight down.\nSome clones turn a brilliant, shining yellow that almost seems to generate sunlight.\nWhen sunlight streams from above, they choose the see-through option.\nHe explained that his color was due to lack of sunlight.\nFocus enough sunlight on a sheet of paper and you can light a fire.\nThe startup isn't the first company to attempt to reduce costs by concentrating sunlight onto smaller solar cells.\nFocusing sunlight into a tiny space can increase the amount of available energy and drive down the price per watt.\nSilicon, in the form of photovoltaic cells, is good at generating electricity from sunlight.\nOne way to squeeze more power out of sunlight is to ensure that it always hits a solar panel at the ideal angle.\nSolar thermal systems use mirrors to focus sunlight, generating temperatures high enough to produce steam to drive a turbine.\nFocus the same sunlight on a solar cell and you can generate plenty of electricity.\nSilicon wafers are the core component in conventional solar cells--they're what absorbs sunlight and generates electrons.\nSunlight, wind, and waves aren't the only sources of renewable energy.\nIt shows video in color, and under full sunlight, but without draining the battery.\nE-paper reflects light instead of emitting it, which makes it less power hungry and easier to read in bright sunlight.\nDirect sunlight may injure the bracts, so use a shade or sheer curtain.\nResearchers say they have moved a step closer to a cost-effective way to power automobiles with only sunlight and water.\nOne is through photovoltaic panels, which transform sunlight directly to electricity.\nThe flashlight absorbs sunlight during the day through its handle and converts it into electric light at night.\nThe virus can be triggered by outside stress, such as exposure to sunlight, a fever or emotional distress.\nThese spores are hardy little things, resistant to sunlight, heat and disinfectant.\nIt all comes down to the precise amount of sunlight that hits your roof.\nClouds cool the planet by reflecting sunlight back into space.\nSunlight interacts with the skin to produce endorphins, natural opiates that send the brain a message of well-being.\nOpen water also reflects a lot less sunlight than ice does, which lets the sun warm things up more as more water shows.\nOther methods include spraying seawater mist from ships toward low-lying clouds, which would then reflect more sunlight.\nFamous quotes containing the word sunlight\nThe sunlight on the garden Hardens and grows cold, We cannot cage the minute Within its nets of gold,... more\nJust across the Green from the post office is the county jail, seldom occupied except by some backwoodsman who has been ... more\nWild honey smells of freedom The dust—of sunlight The mouth of a young girl, like a violet But gold—sme... more","Summer Sun Increases Skin Cancer Risk\nAuthor: Daniel Kellman, ND\nFor most people, summer weather means enjoying the outdoors, doing yard work or vacationing at the beach. But the summer sun can damage your skin. In fact, exposure to the sun’s ultraviolet (UV) rays is a leading risk factor for skin cancer – and more than 3.5 million Americans are diagnosed with it every year.\nSkin cancer is a broad term that refers to any cancer that begins in skin cells. Basal cell carcinoma, which tends to occur in areas that receive the most sunlight (head, neck, hands, etc.), is the most common form of skin cancer and accounts for about 80 percent of cases. Squamous cell carcinoma, which accounts for about 20 percent of skin cancers, is also common in areas with high sun exposure. Melanoma is the most dangerous type of skin cancer. Although it accounts for only about five percent of skin cancers cases, it’s the cause of more than 75 percent of the 12,000 annual skin cancer deaths in the U.S.\nMost skin cancers are slow to spread and are treatable, if not curable, when caught early. But because skin cancer also can be deadly, it is important to understand the risk factors and how to reduce your risk.\nUV exposure: The primary risk factor for skin cancer is exposure to ultraviolet (UV) light, including sunlight, sunlamps and tanning beds. The greater exposure, the greater the risk. Skin cancer is more common where the sun is strong, such as in the South. People who have had at least one severe (blistering) sunburn, frequent sunburns as a child, or used sunlamps or tanning beds before age 30, are also at increased risk.\nFair Skin: Caucasians have a greater risk of developing skin cancer than non-whites. The risk is also higher in individuals with blonde or red hair, blue or green eyes, or skin that burns or freckles easily.\nOlder Age: Skin cancer risks increase as you age, likely due to accumulated exposure to UV radiation.\nFamily or Personal History: Individuals with a first-degree relative (parent or sibling) or who have previously been diagnosed with skin cancer are at increased risk.\nAlthough skin cancer is usually highly treatable, prevention is best. No matter your age or previous sun exposure, decreasing your exposure to UV light (direct sunlight and tanning beds) is the most important thing you can do to reduce your risk of developing skin cancer. When you do go out in the sun, wear protective clothing, hats, sunglasses and sunscreen.\nWhen choosing a sunscreen, the higher the spf (sun protection factor), the stronger the protection. But don’t let a high spf lull you into thinking you’re safe. It is important to reapply sunscreen frequently. This is especially true for children, since childhood sun exposure can be a significant risk factor for developing skin cancer later in life.\nAlso remember that spf refers only to protection against UVB radiation, which burns the skin, and not to UVA radiation that penetrates deep into the skin, accelerates skin aging and may cause skin cancer. Some sunscreens protect against both, so be sure to check labels. In addition, sunscreens from KINeSYS, Soleo, Green Beaver and Badger offer organic formulations with no added chemicals that can also sometimes damage skin.\nFinally, regular, thorough skin examinations are important, especially if you have a large number of moles or other risk factors. While this will not prevent skin cancer from developing, exams can help catch it early. Always tell your doctor if you see any new, unusual or changing moles or growths on your skin.\nIt’s virtually impossible to go through life with no sun exposure, so we all have some level of risk for skin cancer. But by being aware and taking steps to protect yourself from the sun, you can help keep your skin healthy and reduce your risk.\nFor more information about skin cancer risks, signs, symptoms and treatments, visit the Cancer Treatment Centers of America website at: http://www.cancercenter.com/skin-cancer.cfm.\nDaniel Kellman, ND, FABNO, is clinical director of naturopathic medicine with\nCancer Treatment Centers of America at Southeastern Regional Medical Center in"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:5bf571ce-e8d8-4c34-b6bf-3d9b3968e204>","<urn:uuid:f46e0d0c-be6f-4432-b255-804bde7b5bbb>"],"error":null}
{"question":"Can you list the cost-saving measures implemented at Portsmouth Naval Base? 💰","answer":"Portsmouth Naval Base has implemented several cost-effective, energy efficient measures including: 1) Installation of solar panels 2) LED lighting in offices 3) LED street lighting 4) Air-source heat pumps 5) Intelligent control systems. Additionally, the new fleet agreement, which includes 48 electric vans and 59 diesel vehicles, will save the Ministry of Defence an estimated £360,000 in fuel and other costs over the contract duration.","context":["Nissan electric fleet for Royal Navy base\n- 48 all-electric Nissan e-NV200 Combi vans for Portsmouth Naval Base\n- Two-year deal with option to extend\n- Deal with 59 diesel vans renews fleet for BAE Systems on behalf of Royal Navy\n- Fleet deal by Lex Autolease with Nissan\n- 26 charging points and 28 designated parking bays for electric vehicles\n- Cuts CO2 by 40%\n- Contract will save £360,000 in fuel and other costs\nA 48-VAN Nissan electric fleet docks at Portsmouth Naval Base in a deal with BAE Systems who manage the historic site on behalf of the Royal Navy.\nIn a deal with fleet management specialists Lex Autolease, it is the first batch of electric Nissan e-NV200 vans to support operations at the base, home of the Royal Navy.\nThe Nissan electric fleet has been ordered by BAE Systems on behalf of the Naval Base Commander as part of his commitment to investing in green energy on the naval base. The vehicles will be used by BAE Systems’ personnel to transfer stores, engineering and load-lifting equipment from warehouses within the Naval Base.\nThe all-electric Nissan vans will reduce carbon dioxide emissions from the naval base’s vehicle fleet by around 40%.\nThe agreement, which also includes 59 standard diesel vehicles, will save the Ministry of Defence (MOD) an estimated £360,000 in fuel and other costs over the duration of the contract, which runs for two years with an option to extend into a third.\nTo support the new electric fleet and as part of BAE Systems’ commitment to reduce emissions and increase energy efficiency across the naval base, 26 charging points and 28 designated parking bays for electric vehicles are being installed.\nBAE Systems’ agreement with Nissan and fleet management specialists Lex Autolease reiterates the company’s commitment to delivering value for money. It also supports the MOD’s drive to create a more modern and energy-efficient naval base, equipped with innovative technology, fit for the 21st century.\nPortsmouth Naval Base Commander, Commodore Jeremy Rigby, said: “I am very pleased to welcome the addition of these 48 electrical vans to our fleet in the naval base. It’s great for the Royal Navy, great for Portsmouth and great for the environment.”\nIan Anderton, Integrated Delivery Director at BAE Systems, added: “We have worked closely with Lex Autolease and Nissan to introduce this new fleet that makes a significant reduction to carbon dioxide emissions and respects our customer’s need for value for money. Replacing nearly half of the transport fleet at the naval base reiterates the company’s commitment to creating more energy efficient ways of working and adopting the latest technologies.”\nTim Porter, managing director of Lex Autolease, said: “While adoption of ultra-low emitting commercial vehicles in fleets is on the rise, in part due to a continued reduction in running costs, it’s when we work with forward-thinking organisations such as BAE Systems that we can drive significant change together. At Lex Autolease we are extremely pleased to deliver a solution that suited the needs of both BAE Systems and the naval base.”\nBAE Systems has already introduced a range of cost-effective, energy efficient measures at Portsmouth Naval Base such as the installation of solar panels, LED lighting in offices, LED street lighting, air-source heat pumps and intelligent control systems."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e2ac23c5-8b42-4c52-8faa-973b6d0bdfbd>"],"error":null}
{"question":"If I'm staying near Gion district, what vegetarian-friendly accommodations and dining options are available in that area for someone who wants an authentic Kyoto experience?","answer":"In the Gion district, you can stay at Kyokoyado Yasaka Yutone, a beautifully-renovated townhouse that offers sumptuous kaiseki multi-course dinners prepared by their chef from an exclusive ryotei restaurant. For dining options, Gion is conveniently located near several vegetarian establishments. The area has strong Buddhist traditions that support vegetarian dining, though it's important to note that many traditional restaurants use dashi (bonito-based broth) in their cooking. However, high-end establishments can usually prepare kombu (kelp) based broth instead if requested in advance through a hotel concierge.","context":["Where to go in Kyoto and Shiga this season\nUkishima Garden Kyoto: Shojin-inspired, vegan cuisine in Kyoto\nEveryone knows how difficult it is to find vegetarian and vegan food in Japan: that tofu soup might contain dashi made from bonito stock, or sweet potato tempura cooked in the same oil as for the meat and fish! But when in Kyoto, look no further than the lovely machiya restaurant Ukishima Garden, where everything they serve is fully vegan and organic!\nThe delectable flavours and gorgeous presentation of their shojin ryori-inspired cuisine will not disappoint! Shojin ryori refers to the type of meals found in Buddhist temples. Several of the dishes are gluten-free too.\nYou can also read more about the owner Naoko Nakasone in upcoming issue KJ91: Sustainability! Find our subscriptions here.\nUkishima Garden (MAP)\n543 Asakura-cho, Nakagyo-ku, Kyoto 604-8074\nCamellia Garden: Exquisite Japanese Tea Ceremony Experience\nA short walk from Ryoanji Zen Temple, Camellia Garden is a 100-year-old property surrounded by peaceful gardens. Gain fascinating glimpses into the Way of Tea in an exclusive private session with one of their expert instructors. Book your visit here.\nIf you would prefer to join a group session, head to Camellia Flower on Ninenzaka Street. You may also need to book in advance depending on the time of year.\nKyokoyado Yasaka Yutone & Kyokoyado Muromachi Yutone: Luxurious boutique accommodation in Kyoto\nYasaka Yutone is a cozy, beautifully-renovated Kyoto townhouse. It is located in the backstreets of Gion district and a mere few minutes away from Yasaka Shrine, one of Kyoto’s iconic landmarks.\nThe hotel is designed to combine Japanese aesthetics with Western-style comforts, with plush bedding instead of futon. All ensuite bathrooms are fitted with aromatic cypress wooden baths for a thoroughly relaxing experience.\nSumptuous kaiseki multi-course dinners are on offer here, courtesy of new chef Aida-san, who was recently plucked from one of Gion’s exclusive ryotei restaurants. His creations are to die for.\nHotel Kyokoyado Yasaka Yutone\n413 Minamimachi, Higashiyama Ward, Kyoto, Kyoto Prefecture 605-0824, Japan\nMuromachi Street is historically where many kimono and Nishijin-ori fabric wholesalers were located, and this lovely guesthouse was formerly one such property.\nThere are only 7 rooms here, each tastefully decorated to preserve the character of the original building and and with their own unique touches. The furnishings are finely crafted and we especially covet their lounge chairs!\nMuromachi Yutone prides itself on its cuisine: Meals here use only the freshest seasonal ingredients (including kyo-yasai, Kyoto’s famous veggie varieties), and always paired with an excellent saké.\nKanou Shoujuan — Sunai no Sato: Experience wagashi-making in rural Shiga\nWagashi traditional Japanese confections are delightful parcels of typically sweet red bean filling and enjoyed with matcha tea. They are delicate, artful creations that are often an expression of the seasons: like the budding of new green leaves in spring, or the emergence of fireflies in early summer.\nNow, one of Japan’s most recognisable confectioners, Kanou Shoujuan, is offering wagashi-making experiences at their Sunai no Sato complex in Shiga: the neighbouring prefecture to Kyoto, and where the brand was born. Sunai no Sato itself is immediately adjacent to a sprawling orchard of plum trees which bloom in late February-early March. Gorgeous!\nThere are two class options are on offer here. You can take up the morning class, crafting three different varieties of sweet under the guidance of the knowledgeable staff, followed by a warming bowl of matcha, and this runs between 10:45~12:30. For a more leisurely session to include a beautiful, seasonal bento box lunch, this runs between 10:30~14:30.\nThere is also an opportunity to browse the shop for souvenirs before hopping back on the bus.\nBe sure to reserve your place well in advance for this wonderful opportunity! A free shuttle bus departs from Ishiyama Station, just under 15 minutes away from Kyoto Station.\nKanou Shoujuan —Sunai no Sato (MAP)\n4 Chome-2-1 Oishiryumon, Otsu, Shiga Prefecture 520-2266\nGroup classes held Tues, Thurs and Sat(*subject to change) 10:45 – 12:15\nReservations: 077-546-3131 (Japanese) or email@example.com (English OK)\nwww.sunainosato.com (Japanese only)\nPapa Jon’s: The best NY cheesecake in Kyoto\nPapa Jon’s is a firm KJ favourite, with many an editorial meeting held over scrummy cakes and milkshakes. Their gorgeous renovated machiya café on Rokkaku Street downtown was also featured in our Small Buildings of Kyoto book! (Now sold out and a second edition coming soon!)\nShokokuji Monzencho 642-4, Kamigyo-ku, Kyoto 602-0898\n115 Horinouecho, Nakagyo-ku, Kyoto 604-8117\n8-57 Yamabana Itchodacho, Sakyo-ku, Kyoto 606-8007\nwww.papajons.net/ (Japanese only)\nKYOTO Butoh-kan: The world’s first theatre dedicated to Butoh avant-garde dance\nJust off Sanjo Street in downtown Kyoto, Art Complex 1928 has renovated an Edo-period kura storehouse to create the Butoh-kan, an intimate performance venue of 8 seats. Butoh is a form of avant garde dance that emerged in Japan in the 1950s.\nAmong the performers here is a veteran of Butoh: Ima Tenko, who was a member of the renowned group Byakkosha which toured internationally. Each 45~50 minute performance is an immersive experience. Described as “a theatre of protest, an assault on the senses, bewildering, visually raw, emotionally overwhelming”, Ima Tenko dances accompanied by a live shamisen duo. Two other performers are also featured at the Butoh-kan, each with their own signature visuals and style.\nPerformances are scheduled regularly in the evenings on Tuesdays, Thursdays, and Saturdays at 6pm and 8pm.\nNorth on Koromonotana St. + Sanjo St. intersection, Nakagyo-ku Kyoto 604-8202\nPerformance schedule and tickets (9 seats only): www.butohkan.jp/","With strong Buddhist traditions and local specialties like tofu and Kyo-yasai (Kyoto vegetables), Kyoto has some excellent vegetarian restaurants. Here is essential information on vegetarian dining in Kyoto and a list of great vegetarian restaurants in Kyoto.\ntemple dinner © _foam\nKyoto is the home of most of Japan’s main Buddhist sects (which are, in theory at least, vegetarian). Kyoto is also famed for its tofu and its distinctive locally grown vegetables (known as Kyo-yasai). Thus, Kyoto is a pretty good place for vegetarians.\nHowever, there is one important thing to note: If you do not eat fish or fish products, your choices will be limited. That is because a lot of Japanese cooking is based on dashi (broth made with bonito flakes). Fortunately, many high-end places can usually prepare a broth made only from kombu (kelp) if you request in advance via a hotel concierge or private tour operator.\nIn addition, vegans and pescatarians can eat shojin ryori (traditional Buddhist temple cuisine) without any problem and these meals are likely to be among the most elaborate and interesting vegetarian meals you’ve ever had. See my separate page on Vegan Kyoto for more info and a list of recommended Kyoto vegan restaurants.\nThe following is a list of some of my favorite vegetarian restaurants in Kyoto. Where gluten-free and vegan dishes are available, I note that as well.\nThis cramped little restaurant in the middle of downtown Kyoto is the go-to choice for many of Kyoto’s vegetarians. They do occasionally serve meat in some of their dishes, but there’s always a full vegetarian choice available for the day’s lunch or dinner set. Vegan dishes are also available. The food is generally locally sourced and organic. It’s highly recommended.\nTosuiro will open your mind to the full potential of tofu. Here, they elevate tofu cuisine to an art form. Even non-vegetarians will find the food here to be delicious. The set meals involve a flight of varied and filling tofu dishes. Highly recommended! Note that the broth used here contains bonito flakes (ie, fish). If you want to avoid all animal products, you will have to skip the dishes that contain broth.\nThis downtown tofu and yuba specialist serves delectable set meals at a very reasonable price. The fare is filling and nutritious.\n- Falafel Garden\nA great vegetarian restaurant in northern Kyoto, Falafel Garden specializes in the eponymous falafel, but there’s plenty more on the menu to keep things interesting.\n- Shokudo Terasu\nVegetarian ramen?! It sounds like a contradiction in terms, since ramen almost automatically implies a meat-based soup. But, this fantastic eatery in Southern Higashiyama (within walking distance of downtown), serves a delicious ramen with a soy milk-based soup. Note that the broth used here contains bonito flakes (ie, fish). If you want to avoid all animal products, you will have to skip the dishes that contain broth.\n- Mumokuteki Café\nThis popular vegetarian eatery in the middle of downtown serves a wide variety of vegan and vegetarian dishes. The bilingual menu lists all the information you need to know to choose a dish. The lunch sets are filling and are a good value.\nJust down the hill from Ginkaku-ji Temple, Omen is one of our favorite restaurants in Kyoto. They serve some of Kyoto’s best udon noodles. The noodle broth has bonito flakes in it, but several of their a la carte dishes are pure veg.\nTosca is a friendly restaurant that serves healthy and delicious Japanese-style vegetarian and vegan food. It’s within walking distance of Ginkaku-ji and other sights in Northern Higashiyama.\nKerala is one of the best Indian restaurants in Kyoto, and its downtown location make it the perfect place to fuel up while doing some shopping.\n- Yudofu Sagano\nYodofu Sagano is a great place to try that great Arashiyama Buddhist specialty: yudo (chunks of tofu simmered in broth).\nInside the grounds of Tenryu-ji Temple, Shigetsu is a great place to try traditional Japanese Buddhist cuisines, also known as shojin-ryori.\nYoshuji is the nicest place to eat in Kurama, full stop. You can choose from sets of Buddhist vegetarian cuisine or simpler bowls of noodles.\nA short walk east of downtown, Choice is a haven for vegan and gluten-intolerant diners. They serve a wide and varied menu of tasty dishes, including excellent lunch sets. There’s a full English menu.\nFor information on gluten-free dining in Kyoto, see our Gluten-Free Kyoto page.\nKyoto Vacation Checklist\n- Check Kyoto accommodation availability on Booking.com - usually you can reserve a room with no upfront payment. Pay when you check out. Free cancellations too.\n- See my comprehensive Packing List For Japan\n- Buy a data-only SIM card online for collection when you arrive at Kansai International Airport (for Osaka and Kyoto) or Tokyo's Narita Airport\n- Compare Japan flight prices and timings to find the best deals\n- If you're visiting more than one city, get your Japan Rail Pass\n- Get travel insurance for Japan - we recommend World Nomads (and here's why)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2507e630-dc4d-4aac-b357-3220b76ea35f>","<urn:uuid:806f8e49-79bd-4196-ad47-768802ab5bff>"],"error":null}
{"question":"What are the precise transmission methods of West Nile Virus between humans?","answer":"West Nile Virus can only be transmitted between humans in three documented ways: through blood transfusion, organ donation, and from mother to unborn baby.","context":["Mosquito larva infected with West Nile Virus has been identified in several Northern Nevada counties.\nWest Nile Virus Alert\nINFECTION CONTROL FYI\nI can’t help it. It’s just the way my mind works. When I think about mosquito bites, I think about disease transmission. We don’t have to worry about most mosquito transmitted diseases here in our little corner of the world, but we do have to worry about West Nile Virus. There hasn’t been much in the media so far this season, a couple of cases in Washoe County and a few mosquitoes testing positive, but that doesn’t mean it is no longer a problem. We can expect a few cases every season from now on. Not near as many as the 2004 and 2005 seasons when the epidemic was raging, (I hope) but a few cases none the less.\nThe truth is most of us have been bitten by infected mosquitoes already. Since most people don’t experience any symptoms at all the infection goes undetected. Our bodies however, recognize the threat and develop antibodies to protect us from getting the infection in the future. But some unprotected people still come down with the illness which can be very serious. Since we don’t have any way of knowing which group we are in, we must protect ourselves and prevent West Nile Virus. Let’s review the facts:\nFact #1. West Nile is transmitted by a bite from an infected mosquito. The only human to human transmission documented is by blood transfusion, organ donation, and from a mom to her unborn baby.\nFact#2. Only 20% of those infected will develop the disease, and only 1 out of 150 patients with disease symptoms will develop full blown encephalitis. Most of the cases present as minor flu like symptoms that last 3-6 days. The incubation period is 3-14 days after being bitten. Those most susceptible to the infection are people over the age of 50, pregnant women, and anyone who is immune-suppressed.\nFact#3. The primary means of protection from West Nile infection is to avoid mosquito bites. This is especially important for pregnant women. The CDC recommends the use of a mosquito repellant containing DEET, ( consult with your physician before using DEET if you are pregnant) wearing long sleeved shirts, long pants, and avoiding infested areas at dawn and at dusk when mosquitoes are most active.\nFact #4. There is currently no vaccine for West Nile, but vaccination against other arboviral infections such as yellow fever may provide some protection.\nFact #5. The symptoms for mild disease are malaise, anorexia, nausea, vomiting, headache, eye pain, myalgias, rash, and swollen lymph nodes. In severe disease, fever, weakness, gastrointestinal symptoms, flaccid paralysis and changes in mental status are seen which may progress to the neurological symptoms of encephalitis with or without seizures.\nFact #6. The treatment for West Nile infection is supportive, often involving hospitalization, intravenous fluids, respiratory support, and prevention of secondary infections. Some antiviral drugs such as ribavirin in high doses have been shown to help.\nThe best thing that can be done at this time is to get rid of any sources of standing water, or treat water sources to kill the mosquito larvae. Mosquito fish, work very well, and are environmentally friendly, so you might want to consider that as an alternative. For more information including the CDC’s “Fight the Bite” program, go on line to cdc.gov. As always if you have any questions, feel free to email me at [email protected] Remember that infection control is about making it difficult for the little buggers to get in. Understanding the epidemiology of how germs work is the best way to stay healthy. Don’t make it easy for them. Don’t let them bite."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d032df50-295e-4d41-8bb3-46290fae7604>"],"error":null}
{"question":"As a sailor looking to maximize time in tropical waters, what are the key advantages of choosing a catamaran over a traditional monohull sailboat?","answer":"For tropical sailing, catamarans offer several key advantages over monohulls: they provide less draft for exploring shallow bays, more living area, better visibility, and improved light-air performance. They offer superior stability at anchor, eliminating the rail-to-rail rolling common in monohulls. The large protected cockpit provides excellent visibility and living space, making it ideal for tropical conditions. Catamarans also provide a spacious and stable platform for activities like scuba diving. However, these benefits come with higher costs, both in terms of initial purchase and maintenance, as catamarans require lighter, stronger, and more expensive materials.","context":["There is nothing new under the sun; it’s just that we forget. By general consensus, the beginning of the great debate over monohulls versus multihulls dates to the cabal of multihull designers in the 1960s, such as Arthur Piver, Dick Newick and Jim Brown. But in fact this debate has raged for centuries, if not millennia. From the day the first log was dug out and set afloat, the question always has been how best to stabilize it, for the very shape that allowed it to glide through the water was what made it so perilously tippy. The physics have not changed, only the vocabulary; then, as now, seaworthiness can only be accomplished by one of two methods: geometric stabilization or placing weight where it’s needed to keep a vessel upright.\nFor the Polynesians, the first and easiest step was to add an outrigger to the dugout. As it only functioned to leeward, the vessel had to be able to sail in either direction without coming about (precluding a fixed rudder), and thus was born perhaps the first multihull, the proa. Then it occurred to someone: Why not add an outrigger to the other side and allow the vessel to present either side to the wind? Behold our first trimarans. No one could fail to notice that while the outriggers added to the craft’s stability, they did not enhance its cubic capacity. Why not take two hulls and separate them with crossbeams? Eureka, our first catamaran.\nDating back to the time of the Vikings and before, the West opted for stability through weight. If heavy stones could be placed beneath the metacenter of a single hull, they had a notable stiffening effect. It became apparent that the deeper and heavier the ballast, the better, and thus the deep-draft, heavily ballasted vessel evolved. Especially as the manufacturing and trading economies of Europe developed, cargo capacity trumped speed. Thus the sailing ships grew in beam, freeboard and displacement.\nFast-forward those many centuries to the aforementioned 1960s. It was a tumultuous and radical time, when everything “established” was questioned if not outright challenged. A new generation of sailors eschewed tradition and approached every aspect of design, materials and construction from a utilitarian point of view. There was no varnished teak or polished brass for this rebellious crowd; it was all about speed, speed and more speed.\nThe yachting community didn’t know what to do with these wild men and their multiple-hull flying machines. Outraged race committees banned them from regattas, and the sailing press seemed to pay more attention to their occasional failures than their overall successes, further widening the great divide.\nBut that was a half century ago, and multihulls are certainly fully in the mainstream today. One might have thought that by now the hardened opinions on either side of the mono/multi rift would have softened, but apparently the issue remains as polarized as red-state/blue-state politics, as witnessed by this posting on a popular sailing forum:\n“One [the monohull] represents the zenith of thousands of years of maritime tradition and expertise, offering its skipper and crew a safe, sturdy platform on which to sail forth and experience all that is truly beautiful in this world of ours. The other [the multihull], well the other cannot honestly be termed anything more than an abomination. Crimes against nature, and each and every one of them a futile floating attempt to defy the laws of physics and common sense. True sailors avoid them.”\nI don’t believe the Polynesians, who explored the unknown expanses of the Pacific Ocean, can be so easily dismissed as sailors. So at the risk of being hanged from the pilings as a heretic by my fellow monohullists, I would like to offer a few observations on the subject.\nMatters of Choice\nI have owned a series of monohulls, and not because I am a traditionalist per se. Rather it is because I have sought many of my adventures in the higher latitudes, which served up some horrifying conditions: mountainous graybeards of the Southern Ocean, crushing ice of the High Arctic, and gale upon gale in the North Pacific. When sailors linger in these zones, they can reasonably expect eventually to be turned upside down if not inside out. It is here that you might want a vessel that is compact in design, robust in construction, and at least somewhat likely to come back up should it capsize.\nBut I have also spent years wandering through tropical paradises, exploring balmy bays and lost lagoons. And always in these areas I wished I had less draft, more living area, better visibility and improved light-air performance — all traits that multihulls serve up in spades. So it is only through an honest assessment of personal priorities that one can come to a fair and logical conclusion as to which design suits best.\nI met retired airline pilots Mark McClellen and Anne MacDonald, who sailed the Pacific for six years on their Deerfoot 50. This is an exceptionally spirited performer, and its interior was fitted out by Hinckley Yachts, so they wanted for nothing in terms of speed, weatherliness or elegant accommodations. Nevertheless, they recently changed their cruising vessel to a Dolphin catamaran.\nMark explained, “We loved our boat and most things about it, but one day in a very rolly anchorage in Vanuatu, we watched our neighbors on a catamaran relaxing over drinks in the aft cockpit while we were rolling rail-to-rail like a metronome.” Anne added, “The places we stopped were so beautiful. I wanted to be up above water level looking out, not trapped deep below. That’s why we named our new boat Three Sixty Blue. On rainy days we felt cooped up [on the Deerfoot]. On our cat the largest living area is the protected cockpit.”\nPerhaps more important, they are both enthusiastic scuba divers, and the catamaran provides them with a spacious and stable diving platform. But these attributes come at a cost. Multihulls are generally more expensive, not just because of the extra materials required for multiple hulls, but because they are weight-sensitive enough to demand the use of lighter, stronger and therefore more costly materials.\nBut lining up a 40-foot mono with a 40-foot catamaran is not an apples-to-apples comparison. A more accurate measure may be with accommodations. A typical 40-foot catamaran offers at least 25 percent more room than its single-hulled cousin; thus the proper comparison would be a 40-foot multi to a 50-foot mono.\nNow that monetary gap begins to close.\nDebunking the Myths\nWhen deciding how many hulls you want to sail on, there are also the issues of capsize, motion at sea, performance to weather, maneuvering, maintenance and dockage to consider.\nI discussed at great length the mono/multi comparison with well-known multihull designer Ron Givens. Givens has had more than 40 of his multihull designs built, and he has sold over 6,000 plans for his Paper Tiger design alone. By any measure this is a runaway success.\n“I don’t like to frame this discussion as mono versus multi,” he says. “I have sailed both extensively, and each has its merits. But I have averaged three trips a year up to the islands from New Zealand for the last 40 years, mostly on multihulls, and I am still fascinated with their potential.”\nAs for safety concerns, he says, “I have lost several friends on monos, and not a single one on a multi. And when did you last hear of a multihull flipping?”\nThis is a function of enhanced form stability via hull volume, shape, and width-to-length ratios. There is also a growing awareness of balancing performance with safe loading parameters. But even in the rare event of a capsize, we must remember the story of Rose Noelle, a trimaran that capsized off the coast of New Zealand and floated upside down for three months until its crew of three finally drifted back to shore. They were in such good condition that initially authorities thought their story to be a hoax. Offsetting the disadvantage of not being self-righting, as most keel boats are, is the fact that an overturned multihull makes the best life raft in the world — visible from the air, unsinkable, and full of essential equipment and supplies.\nAs for performance, one hesitates to make blanket statements, for, using a highway analogy, there are dump trucks and sports cars in both genres. Defying stereotypes, there are ultralight monohulls that plane off at astonishing speeds, and lumbering, overloaded cats with performance-punishing freeboard, substantial wetted surface and deep-dragging keels.\nIn general, one can expect the average cat to outsail the average mono off the wind, yet lose substantial ground to windward. However, if each design is sailed to its natural tactical advantage, the resulting VMG in variable wind strengths and directions, which ultimately is what matters, is surprisingly close.\nThat said, ocean passages are usually by design planned and conducted off the wind, and in simple terms of noon-to-noon runs, the multis will enjoy up to a 20 percent advantage. Further, this inherent speed potential can become a significant safety factor. With accurate, modern weather forecasting, a fast boat can position itself to avoid the worst of approaching storms and take advantage of wind shifts.\nHere is where Givens gets excitable: “Design, design, design! A proper cat utilizes daggerboards, not keels,” he declares. “And many new designs are simply too heavy. There is no need to overbuild. Good engineering puts the strength where it is needed and lightness where it is possible.”\nSo there is a schism even within the catamaran camp. If opting for a multi, and especially for a performance design, one must adhere to strict weight limitations or increase the overall length to accommodate a given amount of personal gear. And size, or rather carrying volume, comes at substantial extra cost.\nLife at Sea\nAs for the feel of sailing, traditionalists take pleasure in finding the sweet spot or “groove” of their vessel, usually at around 15 degrees of heel. The boat then responds to gusts and lulls with changing inclination, in a way that speaks to the helmsman. To be overcanvassed is obvious, and remedial reefing takes place in a timely fashion. This is an interactive, richly rewarding process. Also, because the boat heels in strong gusts, thus spilling the pressure, a mono should never even approach the breaking loads of its rigging wires.\nBy design, multihulls do not heel and can only be protected from rig overload by reefing early or accelerating out from beneath increased wind pressure. This demands that they be kept light and responsive, but, even so, this characteristic has its limits. Thus the multihull skipper must always be aware that the pressure of the wind increases at the square of its velocity. In squally conditions where the wind may suddenly increase from 20 to 40 knots, the actual pressure on the sails increases four times; at 60 knots, it’s nine times. This can place enormous loads upon the rigging. For all the talk of capsizes, dismasting is the most likely danger with catamarans. Put simply, a multihull can punish inattentiveness, and therefore requires a much higher degree of vigilance than a monohull.\nMotion at sea is one of the most hotly contested talking points. The traditionalists see the multihulls as “jerky. Out of sync with the waves. And how about that slapping noise due to low bridgedeck clearance!” The multi enthusiast retorts with, “How do you enjoy being down in that deep, dark galley cooking on your ear?” While both arguments have their merits, they are too often overstated. Countless voyages have been safely and enjoyably executed in both designs.\nAs for maneuvering in tight quarters, until recently the twin engines of the catamarans had it hands-down over the single screws of the monos. Without touching the helm, the average cat can spin in its own length by using forward and reverse on the dual engine controls. However, the relatively high windage and lack of lateral resistance combine to create significant sideways slippage in windy conditions with most multihulls. On larger monohulls (and many smaller ones these days), bow thrusters have become nearly standard, and with the new integrated thruster and articulating saildrive systems gaining acceptance, the maneuverability gap has essentially been closed. Under power, it’s worth noting that catamarans have the advantage of redundancy with twin engines, especially if the systems share no common components. But, of course, that means twice the cost and time of maintenance.\nComforts and Cost\nOn the topics of stability, space and privacy, no one can rationally deny that catamarans hold the clear advantage. Even the most avid voyagers spend approximately 90 percent of their time at anchor. The stability of multihulls tames even the rolliest of open roadsteads. They offer enormous deck space for lounging and sport activities. Rear davits make shipping heavy RIBs and large outboards a breeze — something modern cruisers tend to do nightly. Wide aft arches and brackets hold solar panels, wind generators, antennas and grills without a sense of clutter. The average cockpit can entertain eight; comforts include built-in coolers, wet bars, visual contact with the galley, and more amenities than there is space here to list. The interior is bright, elevated and spacious. But perhaps the defining difference is in the privacy the two hulls offer. Sharing the cruising experience with family and friends can be a joy, but it is ever so much more enjoyable if one can retreat to an owner’s private hull (or even a guest cabin) for some quiet time and solitude.\nThe question of available dockage and marina fees is often raised. Yes, it is generally more difficult to find available space and more expensive to dock a beamy multihull. But marinas are retail businesses that respond to market forces. As cats become more prolific, more facilities to accommodate them are appearing.\nFor regular underwater inspection and light maintenance, a multihull can be safely and easily careened on any flat bottom with a minimum of tidal rise and fall. This advantage may be offset, though, because the typical Travelift cannot span a multihull, and specialty trailers and cradles, often hard to locate, are required to haul out.\nFactors regarding resale value are too fluid to speak of with certainty. On the one hand, the greater investment in a multihull is somewhat protected by traditionally high resale values. However, as more multihulls are released from the charter fleets into the secondhand market, the average price of a used catamaran may moderate, thanks to supply and demand.\nI acknowledge that trying to present a balanced argument with a litany of “on the other hands” might not make purchase decisions that much easier. I would suggest that potential boat buyers invest a good deal of time and thought into how they will most likely be using the vessel in terms of time on board, number of crew, a draft suitable to the area to be cruised, etc. Then charter a monohull and go back to the same waters aboard a cat. The cost of two charters is insignificant compared to the price of a new or used vessel. Sail both hard for a week in the areas and conditions that you think you’re most likely to encounter. Invite people on board. Get a feel for the draft, windward performance, speed, motion, accommodations and overall sense of utility. Even once the big decision is made as to which side of the great divide you’ll sail on, this process will help you refine your specific criteria and settle on a size and model that’s most suitable for you.\nIn the end, our approach to sailboats is not strictly a practical matter. There is a deeply emotional, even mysterious connection with our vessels. They reflect our sense of style and aesthetics and represent our longings for freedom. Whatever the size, shape or number of hulls, the right boat is the one that puts a smile on your face at the mere mention of it. That’s called love.\nAlvah Simon is a CW contributing editor and has served as a Boat of the Year judge on several occasions, sailing aboard catamarans and monohulls."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:79ff631a-d6e9-451b-972d-1c7e9a95407e>"],"error":null}
{"question":"What is the comparison between air pollution deaths and deaths from wars/violence worldwide?","answer":"According to global health data, air pollution causes 7.4 million deaths annually, which is 15 times greater than deaths from all wars and other forms of violence combined.","context":["According to a ground-breaking new report in the leading medical journal The Lancet, pollution in the air, water, soil, and the workplace is linked to an estimated nine million deaths each year worldwide equivalent to one in six (16%) of all deaths. This death toll is three times more than the number of AIDS, tuberculosis, and malaria-related deaths combined, and 15 times greater than fatalities of wars and all other forms of violence. [Source: NPR]\nIn the EU alone, pollution causes more than 400,000 deaths - representing 7.8% of all deaths. Most of these deaths are due to non-communicable diseases caused by pollution such as heart disease, stroke, lung cancer, and chronic obstructive pulmonary disease (COPD). In the European Union (as in other industrialised regions), air pollution is the most harmful source of pollution, responsible for more than 280,000 deaths.1\nThe Lancet report on Pollution and Health is the first to analyse all forms of pollution (air, water, soil, occupational) and its impact on human health, the economy, and society.\nThe European Union is at the forefront of tackling pollution both within its borders and globally. In addition to specific laws such as the REACH chemicals law or air quality standards, the EU’s Environmental Action Programme (EAP) has been an important driver for pollution control action. With its emphasis on tackling environmental threats to health, the 7th EAP includes important pollution control goals.\nThe Lancet Commission on Pollution and Health is a two-year project that has involved more than 40 international health and environmental authors, and provides new estimates on the effect of pollution on health and its economic costs. Using data from the Global Burden of Disease study, it examines outdoor and indoor air pollution, water, soil, and workplace pollution.\nThis Lancet report was conceived to draw attention to pollution’s enormous impact, and to highlight that this fatal issue does not receive sufficient attention as well as showing that the issue of pollution is solvable. The report’s relevance to the European Union will be discussed during a panel discussion with authors and partners on the 26th October in Brussels. See the invitation here.\nThe report highlights the need to tackle sources of pollution associated with the healthcare sector, such as greenhouse gas emissions from the use of fossil fuels, pharmaceutical pollution, as well as patients’ and healthcare worker’s exposure to chemicals used in health facilities.\nReducing healthcare's climate footprint: Opportunities for European hospitals and health systems (2016)\nThis HCWH Europe report presents a number of case studies of European healthcare systems implementing strategies to reduce healthcare's climate footprint, as well as an overview of the EU Climate and Energy policy landscape.\nSafer Pharma Campaign\nPharmaceuticals in the environment are a global pollution problem – more than 600 pharmaceuticals and their metabolites have been found in the environment worldwide. They enter the environment (through water, soil, sludge, and organisms) at all stages of their life cycle and can end up in drinking water, and accumulate in fish, vegetables, and livestock. The aim of the Safer Pharma campaign is to protect the environment from pharmaceutical pollution at all stage of their life cycle – this campaign is coordinated by HCWH Europe, visit: www.saferpharma.org\nThe WIDES Database for Choosing Disinfectants\nThis leaflet introduces the online database for professional users that evaluates the health and ecological effects of disinfectant products in order to make substitution easier for hospitals and healthcare settings.\nHazardous Chemicals in Health Care A Snapshot of Chemicals in Doctors and Nurses (2009)\nThis study reports on the first biomonitoring investigation of healthcare professionals conducted by Physicians for Social Responsibility (PSR). The results were worrying; it was discovered that each participant had at least 24 individual chemicals in their body, and two participants had a high of 39 chemicals detected, and eighteen chemicals were detected in every single participant - all participants had Bisphenol A, and some form of phthalates.\n1. Recent figures from the European Environment Agency (EEA) even estimated air pollution deaths in the EU at more than 399,000 in 2014. The EEA based its numbers on measurements of fine particulate matter (PM 2.5), nitrogen dioxide (NO), and smog (O3). The Lancet report bases its numbers of 2015 figures from household air pollution, ambient fine particulate pollution (PM2.5), and tropospheric ozone pollution, which explains the difference in findings.\nPreview image: Skaja Lee via Flickr cc","pollution might just be humanity’s most pressing problem.\nMore than 9 million deaths were linked to “polluted air, water and soil,” in 2015, according to an explosive new report published in The Lancet, a UK medical journal. To provide some context, this means that pollution accounted for 16 percent of all deaths worldwide, “three times more deaths than from AIDS, tuberculosis, and malaria combined and 15 times more than from all wars and other forms of violence,” according to the report’s executive summary. This figure, as the study’s authors point out, may in fact be conservative, and could be millions higher, “because the impact of many pollutants are poorly understood.”\nThe study drew from the work done in previous studies that “show how pollution is tied to a wider range of diseases than previously thought,” including asthma, cancer, heart and lung disease, and neurological disorders. The vast majority of deaths – a full 92 percent, according to the report – took place in undeveloped or developing countries, a shocking statistic that nevertheless still provides some hope: the study’s authors say that “the big improvements that have been made in developed nations in recent decades show that beating pollution is a winnable battle if there is the political will.”\nOther highlights from the study:\n- Deaths from air pollution “are on track to double by 2050” in southeast Asia;\n- The costs of pollution are $4.6 trillion each year, according to the study’s estimate – “equivalent to more than 6% of global GDP”;\n- Of the various forms of pollution, toxic air contributed the greatest number of deaths, at 7.4 million, water pollution (this means sewage, most often) resulted in 1.8 million deaths, and workplace pollution, “including exposure to toxins, carcinogens and secondhand tobacco smoke,” resulted in 800,000 deaths.\n- Although the highest rates of pollution death are found among small developing countries, it’s India, with 2.5 million deaths, and China, with 1.8 million, that have the greatest number of deaths linked to pollution. Both Russia and the US are in the top ten as well.\nAccording to lead author Dr. Philip Landrigan, a pediatrician and professor of environmental medicine and global health at the Icahn School of Medicine at Mount Sinai, stricter regulation of toxic chemicals can often benefit economies, which “puts the lie to what we hear that controlling pollution is going to kill jobs.” As an example, Landrigan cites the passage in the US of the Clean Air Act, which has brought air pollution levels down 70 percent in the intervening years, even as GDP has increased 250 percent.\n“Pollution is one of the great existential challenges of the Anthropocene era. Pollution endangers the stability of the Earth’s support systems and threatens the continuing survival of human societies.”\nThe Lancet Commission on pollution and health\nThe Lancet Commission on pollution and health\nThe study’s conclusions, along with a just-released report by the nonpartisan Government Accountability Office (GAO) which “found that costs [linked to climate change] may rise to as much as $35 billion per year by 2050,” are surfacing at a particularly transformative moment for the EPA – established the same year, 1970, that the Clean Air Act was amended to give the federal government much greater power to regulate chemical use. Administrator Scott Pruitt, who has not accepted scientific consensus around climate change, and who pressed President Donald Trump to pull out of the Paris Climate Accord, has “embarked on record-setting rollbacks,” which include “plans to repeal pollution in the nation’s waterways [and] delaying rules requiring fossil fuel companies to rein in leaks of methane and greenhouse gases.” Pruitt also refused to ban a farm pesticide, chlorpyrifos, that poses a health risk to children and farm workers, and that has already, since 2000, been banned from most household use. The list goes on: the EPA under Pruitt has moved to suppress research into – and even mention of – climate change, including removing references on its website, canceling public talks, and removing from advisory roles scientists who’ve received federal grants for studies, and politicizing the issue by creating “red team-blue team” exercises between scientists who doubt climate change and those who don’t. Such exercises create false equivalences on a topic that’s already got near-unanimous scientific consensus. Pruitt has also moved to repeal the Obama administration’s signature climate regulation, the Clean Power Plan, in an effort to bolster the coal industry, long in decline due to the rise of natural gas and renewables, and has tasked his agency to respond to permit requests in six months or fewer, which means big, complicated projects with difficult-to-predict outcomes could get the greenlight without receiving thorough vetting.\nWhat this amounts to, in the eyes of many folks on the left, is a full-on assault on the environment, but Pruitt and co. see themselves as reformers, trimming away at bureaucratic fat and demanding the science around disputed chemicals be utterly airtight before acting to regulate, rather than allowing what Dr. Nancy Beck, one of Pruitt’s top deputies, calls “phantom risks” to bottleneck industry. It’s a typically Republican approach, maybe more destructively so in this administration than in previous ones, although this may just be because there’s more to destroy: the Obama administration significantly expanded the role of the EPA in its efforts to address climate change, and also took a rather-safe-than-sorry approach to chemical regulation which, in light of The Lancet report, may prove to be the right one. As Wendy Cleland-Hamnett, who until her retirement last month was “the agency’s top official overseeing pesticides and toxic chemicals,” asserts,\nYou are never going to have 100 percent certainty on anything, but when you have a chemical that evidence points to is causing fatalities, you err more on the side of taking some action, as opposed to ‘Let’s wait and spend some more time and try to get the science entirely certain,’ which it hardly ever gets to be.\nThe New York Times’s The Daily podcast posted a fantastic episode on Tuesday about the perceptions each side of the EPA debate has of itself, which you can find here.\nA new study summarizing observations “across 63 German nature reserves” reveals a disturbing 76 percent decline in flying insect biomass.\n- The study’s authors aren’t sure why this happened, but speculate that pesticide use in adjacent areas, coupled with climate change, has played a significant role.\n- The authors also believe their findings to “be representative of much of Europe and other areas of the globe.”\n- A key quote: “Lack of insects is very likely to be detrimental to the entire ecosystem…insects play a crucial role in ecosystems, being responsible for plant pollination and nutrient recycling as well as acting as a food source for animals such as birds, amphibians, reptiles, bats and small animals.”\nAn 11-year-old motivated by Flint’s still-polluted (honestly, how?) water invented a lead-detecting device.\n- From NPR’s description of the device: “there is a disposable cartridge containing chemically treated carbon nanotube arrays, an Arduino-based signal processor with a Bluetooth attachment, and a smartphone app that can display the results.”\n- What we’d like to know: how does an 11-year-old get her hands on carbon nanotubes?\nChina’s new ban on 24 categories of recyclables and solid waste could mean that much of what we recycle ends up in the trash.\n- Why? “China is the dominant market for recycled plastic,” according to Christian Cole in The Conversation. Here in the US, we export around 1.4 million tons of recycled plastic to China.\n- Double bonus: a tiny Montana company with links to Secretary of Interior Ryan Zinke just landed a $300 million contract to rebuild Puerto Rico’s electrical infrastructure. Whitefish Energy Holdings, which had just two employees when Hurricane Maria made landfall last month in Puerto Rico, is based in Zinke’s hometown and owned by a friend of his.\nThe first-ever floating wind turbines—these five turbines off the coast of Scotland have the potential to power, at peak, up to 20,000 homes.\nAccording to a brand new study in Nature Communications, “water rose rapidly, in punctuated bursts, rather than gradually over time” during the last global warming period more than 10,000 years ago. The researchers “suggest[ed] these past events could be viewed as a kind of ‘analog’ for the future.”\n- Double bonus: a new story in The Atlantic, summarizing the results of a different study, would seem to confirm this: “Climate Change Will Bring Major Flooding to New York Every 5 Years”.\nLet’s end this on a positive note: here’s a livestream from the inside of a little penguin burrow.\nMakayla Esposito contributed to this report.\nheader image: \"pollution,\" possan / flickr"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:5665ee37-0069-4d88-b629-7faf76fe4ccd>","<urn:uuid:5467784a-63f2-4851-b930-f9c8cfc042fd>"],"error":null}
{"question":"Please tell me how often should athletic fields with cool-season grasses be fertilized with nitrogen throughout the year?","answer":"For athletic fields with cool-season grasses, nitrogen should be applied each month from March through November, especially on highly leachable, sandy soils. However, it's important not to exceed one pound of soluble nitrogen per 1,000 square feet per month. This frequent fertilization is particularly necessary in high traffic areas to force growth and recovery of turf.","context":["Bermudagrass for Athletic Fields\nBrad S. Fresenburg\nDepartment of Horticulture\nThis publication has been prepared for managers of football and soccer fields who are frustrated by trying to grow cool-season grasses where irrigation is limited or where fields receive heavy use during optimal seeding times of the year. Another situation many groundskeepers face is a field that receives heavy use through the summer with no opportunity to recover before the the fall season begins. If you have found yourself in these situations, you may want to consider bermudagrass as a means of establishing a reasonable stand of grass to maintain good playability and safety.\nCool-season grasses are preferred for athletic field turf in Missouri. However, their success is sometimes compromised because of harsh summers, excessive traffic and restricted cultural management practices. The following information is provided as a possible solution to heavily worn athletic fields where cool-season grasses such as Kentucky bluegrass, perennial ryegrass or tall fescue have been tried without success.\nConsider the six factors listed below to evaluate efforts to manage cool-season grasses. Drainage, irrigation and traffic control are the most crucial factors in grass survival from year to year, especially on a limited budget. Once the options for maintaining cool-season grasses have been exhausted, bermudagrass may be the choice.\nBefore changing to bermudagrass, consider the following six factors necessary for growing cool-season grasses on athletic fields in Missouri:\n1. Rapid drainage\nPlaying surfaces should be graded or crowned (1 to 1.5 percent) to provide rapid removal of surface water from heavy rains. Fields with reverse crowns (concave) in the center should have soil replaced to reestablish proper grade.\nSubsurface drainage also is necessary to prevent anaerobic conditions (lack of air) in the root zone. Even when drainage is adequate for plant growth, turf on soil with a low water infiltration rate may be severely damaged when athletic activities are held during or shortly after heavy rain. Sand or modified soil fields that have infiltration rates between 2 and 4 inches per hour offer the best means of rapidly removing water from the surface and reducing the chance of severe field damage.\nThere is no substitute for adequate irrigation. It should be a high priority in upgrading athletic fields. Irrigation will do more for the potential to establish and keep turf in Missouri than any amount of seed, sod, fertilizer, pesticide, or cultivation.\nThe best solution is a permanent, and preferably automatic, irrigation system that evenly supplies a minimum of 1/4 inch of water every day to each field. Commercial traveling gun sprinklers have been used successfully to irrigate athletic fields. They are portable for multifield use but usually require 8 to 10 hours to irrigate a single field. Home lawn sprinklers, even traveling models, are not adequate for large athletic fields.\n3. Traffic control\nAssess nontraffic areas of the field, such as beyond the end zone. If the current level of management is providing acceptable turf in the nontraffic areas, change the pattern and amount of traffic, rather than changing the grass species or management practices.\nManagement of athletic fields requires coordination among administrators, coaches and groundskeepers. Administrators should keep in mind that proper traffic control costs nothing and at the same time offers the most effective means of reducing dangerously worn areas on game and practice fields. The coach must take an active interest in scheduling activities and preventing excessive turf wear. The coach and the grounds superintendent can work together to develop improved grass areas specifically for drills that are conducted off of the game and practice fields.\nAs much as possible, reserve athletic fields for games only. Hold scrimmages on practice fields. Practice drills should be held in areas off the game and practice fields, and they should be rotated among those areas to allow for turf recovery.\nBand practice on the game field should be limited to once per week and only when the soil is dry enough to resist compaction in marching paths. Yard lines can be painted on parking lots or other turf areas for daily band practice.\n4. Spring, summer and fall fertility\nSoils should be tested yearly and proper adjustment should be made for pH, phosphorus and potassium. Table 1 represents a good program for scheduling nitrogen application to cool-season grasses. Additional nitrogen may be required on soils amended with sand.\nIn addition to a general nitrogen schedule, it may be necessary to force growth and recovery of turf in high traffic areas of athletic fields. In these special areas, apply nitrogen each month from March through November, especially on highly leachable, sandy soils. Do not exceed one pound of soluble nitrogen per 1,000 square feet per month. Forcing growth in this manner will require additional irrigation and mowing.\nIf these basic fertility requirements have not been met, cool-season grasses have not been given a fair chance to grow.\nCore cultivation needs to be a regular practice for any athletic field program. In areas receiving little traffic core cultivation may only be needed once or twice during the year to prevent thatch and increase water infiltration. In heavily worn and compacted areas, however, cultivation may be needed on a more regular basis to break up hard ground, allowing for spread of established grasses and establishment of seedling grasses. Few athletic fields exist that would not benefit from core cultivation during certain times of the year.\nOverseeding to reestablish turf may be necessary where it has been worn to the point that recovery from underground stems and crowns is not possible. Bluegrass/ryegrass fields should be overseeded with a bluegrass/ryegrass mixture or straight perennial ryegrass. Tall fescue/bluegrass fields should be overseeded with a tall fescue/bluegrass mixture or straight tall fescue.\nTall fescue and perennial ryegrass should not be combined in a seed mixture, nor should one be interseeded into an established stand of the other. Fall or early winter dormant seedings of cool-season grasses are preferred to spring seedings.\nSod should also be considered when cool-season grasses are desired but there is not enough time to establish cover from seed during the off season.\nBermudagrass, a warm-season grass, has advantages and disadvantages. You should know what to expect once you are committed to using it.\nAdvantages of bermudagrass\n- Can be successfully established or renovated during the summer — May through August — when fields are seldom in use. Late spring and summer seeding of cool-season grass is almost never successful in Missouri. Heavy traffic in the spring and fall may not leave time for cool-season grass establishment from seed.\n- Fast-growing grass with aggressive creeping rhizomes and stolons that can spread as much as 3 feet during the summer to cover worn areas and bare soil. Produces a tough, wiry surface that results in good traction.\n- Compared to cool-season grasses, it has a much better chance of surviving the summers on low-budget fields with limited irrigation.\n- Performance is good to excellent in the southern part of the state (south of Interstate 70) in Missouri, with a greater potential for winterkill in the Kansas City and Lake Ozark regions. Severe winterkill seldom occurs in the southern third of Missouri.\n- Excellent choice for practice football fields that generally have limited irrigation and receive most activity early in the season — August through September — when bermudagrass is still growing.\n- Can be overseeded with cool-season grasses to improve fall and spring performance. Dormant bermudagrass has good traction and cushion as long as the dormant vegetation is not entirely worn away.\n- Can be mixed with perennial ryegrass to improve the success of late spring and summer seedings for repair of cool-season grass athletic fields. Addition of bermudagrass in summer seedings usually ensures green cover and improved field performance during the early part of the fall playing season. This is especially popular where good turf cover during the playing season takes precedence over a uniform green color throughout the growing season.\nDisadvantages of bermudagrass\n- Turns brown and goes dormant after the first frost, leaving the field with poor color and poor recovery potential during the last half of the fall season (October and November) and during the entire spring season (March to mid-May).\n- Even though bermudagrass tolerates drier conditions and requires less irrigation than cool-season grasses, it still requires some irrigation or timely rains during establishment and for recovery of worn areas.\nBermudagrass is not the answer to all athletic field problems. It should not be viewed as a substitute for irrigation, proper fertility and a regular cultivation program. Winter survivability of bermudagrass in the lower two-thirds of Missouri is greater than the summer survivability of cool-season turfgrasses grown without irrigation and subjected to the rigors of athletic field use.\nUsing bermudagrass to repair worn out areas of the field will result in better vegetative cover during more of the playing season. However, the appearance of the field may not be uniform since bermudagrass has a slightly different texture and shade of green. It is brown during the fall and spring and is easily distinguished from weeds and other cool-season grasses.\nFields that are entirely bermudagrass will have a more uniform appearance, but will still have the problem of winter dormancy. Bermudagrass overseeded with Kentucky bluegrass or perennial ryegrass will have the best vegetative cover and appearance during the entire year.\nSeed or sprig bermudagrass between early May and mid-July. August seeding or sprigging is not recommended because of insufficient time for proper establishment before cold weather. Bermudagrass will not germinate or grow until soil temperatures have adequately warmed. Seeding or sprigging before May will result in heavy competition from spring weeds.\nIf an early spring covering is needed, bermudagrass sod can be cut and laid from late March through July. Dormant sod should not be cut and laid on fields where spring games are scheduled. Improved varieties of bermudagrass are now available as seed, sprigs, plugs and sod. Seeding and sprigging are the most common means of establishing bermudagrass (Table 2).\nMany improved varieties of bermudagrass are now available as seed. Seeding rates should range from 1 to 2 pounds of seed per 1,000 square feet.\nSprigs are pieces of torn turf usually containing a stolon with roots and up to four nodes. Sprigs can be planted by broadcasting them over loose soil followed by a light disking to partially cover them with soil. A portion of each sprig should remain exposed after planting. Mechanical spriggers are available that slit the soil open, plant the sprig and cover the sprig with a small amount of soil. In either case, the sprig should promote roots and creeping stems from the nodes. Sprigs can be purchased as sod and then shredded, or often can be purchased by the bushel. One square yard of sod makes approximately one bushel of sprigs.\nPlugs are usually 1 to 2 inches in diameter with 1 to 2 inches of soil attached. They should be fitted tightly into prepared holes and tamped firmly into place. Plugs are generally placed 6 to 12 inches apart. Closer spacing may be used to hasten cover of the turf area.\nField preparation and establishment\nControl broadleaved weeds, particularly knotweed and creeping speedwell in April so that broadleaf herbicide applications will not interfere with summer establishment of bermudagrass.\nImmediately after spring field activities, regrade the surface if necessary, fill depressions with soil and smooth the surface.\nIrrigate in late April and early May to germinate as much crabgrass as possible.\nWhere no desirable grasses exist, kill all vegetation with glyphosphate and seed or sprig seven to 14 days later. If desirable grasses exist, kill newly emerged crabgrass with MSMA or DSMA and seed or sprig 14 days after application. Killing the first flush of crabgrass before seeding or sprigging bermudagrass reduces crabgrass competition, and the need to control it, the remainder of the year. Bermudagrass establishes much better with less weed competition and if there is no need to apply repeat applications of post-emergence herbicides.\nLoosen hard soils and prepare a seed bed by intensively coring, spiking, or slicing. After seeding lightly, rake or drag the surface to ensure good seed-to-soil contact. If a drill or slit seeder is used, make at least two passes in opposite directions using half the total amount of seed. The remainder of the seed should be broadcast to provide faster and more uniform coverage of bermudagrass.\nAdd lime, if needed, and fertilizer. Add one pound of nitrogen, phosphorus (K2O) and potassium (P2O5) per 1,000 square feet per month during the first three months of establishment. Make the lime and first fertilizer applications immediately before preparing the seed bed to improve incorporation into the soil.\nWater frequently to encourage germination and rapid turf coverage.\nBegin mowing at 1.5 inches as soon as the grass is tall enough to be clipped.\nApply MSMA or DSMA according to label recommendations to control crabgrass that competes with the establishing bermudagrass.\nOne of the reasons bermudagrass has been recommended is because of its ability to maintain active growth and recovery during typically dry summers in Missouri. However, establishment of bermudagrass is greatly enhanced when some irrigation is provided. Sprigs should not be installed unless they can be kept moist for at least the first three to six weeks after planting. Seeded bermudagrass will not germinate and establish unless rainfall or irrigation keeps the top 1 inch of soil moist for a minimum of two to four weeks. Ideally, temporary or permanent irrigation should be available to supply water daily for the first month after planting and at least twice a week thereafter. During the first three weeks, water shallow and frequently to keep only the area of actively growing roots moist. As plants begin to tiller and produce deeper roots, irrigate deeper and with less frequency.\nEstablished bermudagrass will usually survive the driest summers in Missouri, however if it has been severely worn and is expected to rejuvenate from rhizomes and spread into bare areas, it will require weekly water from either rain or irrigation.\nWhether bermudagrass is seeded, sprigged or plugged, crabgrass and nut sedge rapidly establish in bare spots during summer. Preemergent herbicides should not be used on weakened bermudagrass in the spring or summer, or when attempting to establish newly seeded bermudagrass in the summer. Ronstar preemergent herbicide is labeled for the establishment of bermudagrass sprigs. Read the label for proper rates.\nControl annual grass weeds and nut sedge with postemergence applications of MSMA or DSMA. Repeat applications, seven to 14 days apart, may be necessary for adequate control. Delay application of MSMA or DSMA until turfgrass rooting has occurred and spreading stolons are visible. Try to time your MSMA application after three or four mowings when warm season grasses are actively growing and crabgrass begins to dominate the turfgrass canopy.\nCool-season grasses are preferred for athletic fields in Missouri where proper attention is given to turf management and traffic control. Bermudagrass may be the best choice, however, where irrigation is limited and fields receive use through the spring, summer and fall."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:eb1b2bba-7d75-4ef3-9e7d-a60039acff66>"],"error":null}
{"question":"How did Willie Houston first learn to play the guitar?","answer":"Willie Houston learned to play guitar in Joplin, Missouri from a man known only as 'Bruce.' He would give Bruce a bottle of beer in exchange for lessons. Bruce taught him that if he had blues in him, he'd understand what it was all about, and warned him that sometimes the guitar would 'play you' if you didn't know how to handle it properly.","context":["Unsung champions of the Delta blues have often boasted colorful nicknames. And while a memorable moniker never hurts a shot at notoriety -- Jaybird, Bo Weavil, Scrapper and Peg Leg, for instance -- few bluesmen go on to become household names, let alone footnotes to Robert Johnson. For Willie Houston, a 76-year-old guitarist, singer and junkman who's called Denver home since the end of World War II, an unadorned handle and complete obscurity come with the territory.\n\"I thought about callin' myself 'Junkyard Willie,'\" Houston says. \"But I'd rather use my own name.\"\nHouston prefers to keep things simple. With a passion for the blues, the seasoned scrap collector understands the origins of American black music in a way that predates television by nearly three decades. He started out with nothing (and still has most of it) in Grand Cane, Louisiana, a northwestern speck twenty miles from the Texas border. The oldest of ten children raised in a two-room country shack mostly by his mother, Beatrice Acy, he can relate stories about Chickasaw Indians and the Civil War told by his 112-year-old grandfather, C.D. Houston. (\"He could tell you about slavery, too,\" the junkman notes.)\nA yarn-spinner in his own right, Houston summons memories of his childhood as if the events happened yesterday. (Like the time he caught on fire when he was five years old: \"I took off runnin' and the more I run, the more it burn. I don't know why.\") What he remembers most vividly from those days, though, is years of backbreaking labor in the hot sun. He split time working a neighboring plantation with fifteen other families and living with his kinfolk in the sticks.\n\"We was a thousand miles from nowhere, man, in the swamp part -- where the alligators and bears and stuff is at. So far back, a car couldn't get down there,\" Houston recalls. \"Wagons and teams -- that's all you could use. I think about the South when I'm playin' blues -- goin' to the cotton fields, goin' to eat this watermelon for dinner, watermelon for supper, watermelon for breakfast, piece of bread for dinner or whatever. I tells this because that's my blues.\n\"I been through the hacker; I been through the cuts. My life, it hasn't been easy,\" Houston continues. \"The most of it was in the farm life. Had to work instead of go to school, like I wanted. That was the worst time there. You worked on the farm from six to six, you don't even get fifty cents. All the kids had to do that.\"\nBut growing up dirt-poor in the swamp had its share of happy moments, too.\n\"I used to take a tin bucket and make music as good as anything,\" Houston recalls, smiling. \"My mom told me -- I'll never forget. The moon was shinin' bright -- whew, that moon was shinin' pretty! Daddy was gone. I was sittin' on our porch, and I started playin' that bucket. I beat it so much I could bend the tin and change the tone. And she says, 'Boy, you keep that up, and one day you're gonna be a 'fessional.\"\nHis mother's prophecy was realized shortly after he had taught himself to play the Jew's harp (or \"juice harp,\" as he calls it), earning pennies, nickels and dimes from the folks in town. Through the worsening days of the Great Depression, Houston, then seventeen, grew restless and moved to Joplin, Missouri. Armed with a seventh-grade education, he found work in a lumber mill. More important, he met a man with a guitar whom he remembers only as \"Bruce.\"\n\"I used to give him a bottle of beer to try to teach me how to play. I told him I could learn it in a week. He said, ŒYou can try; I'm gonna give you one note. If you got any blues about yourself, you'll understand what it's all about. You need to know that box,'\" Houston says, echoing his mentor. \"'If you gotta feel your way around, that guitar gonna play you. That's what it's gonna do. I don't care how well you think you know it; sometimes those jokers'll play you. Sometimes it gets mean, boy. And if you do hit 'em bad, you gotta know how to cover 'em up right quick.'\"\nAfter Houston improved as a six-stringer, his wanderlust -- along with a draft notice during the Korean War, in 1951 -- took him to Fort Knox, in Kentucky, where he pulled special duty stateside as a colonel's chauffeur, shined a lot of shoes and even saluted Eisenhower in person. Following the war, with visions of becoming an entertainer, he caught a Greyhound west and ended up staying at the Rossonian Hotel in Five Points for a spell. In 1953, he was injured in a car crash and spent 72 days flat on his back in Denver's VA hospital. He eventually found steady employment at a plant that processed telephone poles, all the while honing his skills as a guitarist and vocalist.\n\"I can play solo; I did that a lot,\" Houston says. \"But my sound and style is better with a group.\"\nIn 1959, the newcomer's first placard -- announcing \"Guitar Willie and His Rhythm Rockers\" in silver, glitter-flecked letters -- went up at the Saddle Club. More gigs followed at various Denver venues, including the Casino Ballroom (\"In them days, admission was only fifty cents,\" Houston notes), and the group soon became an American Legion Hall staple.\n\"I tried to sneak in a little blues stuff, but they wouldn't let me do that,\" Houston says. \"They said, 'You got to get in here with the swing.' But I hate to give up what I been raised with.\"\nEven so, in 1962 Houston played backup on \"Snowflakes,\" a vinyl 45 in the spirit of Chuck Berry, on Denver's now-defunct Band Box imprint. It was a far cry from his traditional Southern roots.\n\"Everybody started from the cotton fields; ain't no city boys here,\" Houston says emphatically. \"We call it the bigfoot country. We all from down there -- Blind Lemon, Muddy Waters, Lightnin' Hopkins, John Lee Hooker. B.B. King pulled cotton right there in Arkansas.\"\nIn less grandiose company at his monotonous day job, Houston found himself gigging less frequently. Then, in 1968, after learning to use a cutting torch, he became his own boss -- a full-time junkman hauling scrap metal from old cars and the like -- and hasn't looked back since. The cluttered yard that currently surrounds his old frame house in Denver's industrial Cole district (and home to his faithful Chow, Jinx) is evidence of a long and junk-filled career: Old washers and stoves rub shoulders with ancient automobiles in a vast sea of rusted box springs, broken vacuum cleaners, discarded aluminum and claw-foot tubs.\n\"To be a scrap man, you got to put everything to good use, because you don't know what tomorrow is gonna bring,\" Houston declares. \"Being a bluesman is much easier, but it works on about the same order: It's a talent that you got. I love to do either or both.\"\nAnd he shows no sign of slowing down at either or both -- especially his music. Filled with gut-driven tunes that grab the listener with honesty and conviction, his 2001 full-length debut Blues Man Willie Houston and His Guitar, released on Northglenn-based FastTrack Records, arrived fully developed, a lifetime in the making. Playing a replica of B.B.'s Lucille-edition Epiphone, the bluesman celebrates the longstanding twelve-measure form with a vibrato-rich field shout, covering the bases of heartache (\"I Need Your Love So Bad\"), isolation (\"1000 Miles\") and unrequited lust (\"My Apple Tree\"). Houston is more concerned with Greyhounds leaving the station than hellhounds on his trail. He may never have made a pact with the devil at the crossroads or had his guitar tuned at the stroke of midnight, but with a loose, fluid, economic style and a down-home approach, the junkman adheres to traditional six-string rundowns and turnbacks with raw, personal expression. It's an overall familiar sound that he affectionately qualifies as coming from \"the alley.\"\n\"People don't come to the street to hear blues,\" Houston insists. \"They come to the alley. They say, ŒPut me in the alley.' You can't go no deeper -- old moonshine sittin' up there 'Gad-dawgit, boy! Gals out there doin' that Betty-rubbin' stuff, you know -- that's dance. Everything goes on in the alley -- everything. You don't do no uptown stuff. You goin' down. You down and out.\"\nFor all his implied troubles, Houston, a bachelor, rejoices the females who've done him right over those who might have dogged him for a fool. A case in point is the standout opening track, \"Sally Mae.\"\n\"I met that girl, I was about thirteen years old,\" Houston recalls. \"We'd go swimmin' in the country lakes together, go up to the hedgerow. Sally Mae was a runaway; she was a rambler. She didn't let no grass grow under her foot, not at all. Sally Mae would go one plantation to another one, 'cause her mother passed on. I could write a lot of things about Sally Mae. She was a sweet little girl; Sally Mae was the one. She did me right, but she got away.\n\"I stopped smokin' and drinkin' in the '70s,\" he adds. \"I might take a social drink with some water or something. Guys come by -- we get through playin', I might give 'em a sneak. But I'm not livin' that kind of life.\"\nDeceptively youthful and hernia-free, with all of his original teeth, Houston prefers hard candy to hooch these days. He's a refreshing contrast to seniors who clip coupons or comb the beach with metal detectors in Sears activewear and cuts a rugged figure with his bib overalls, callused mitts and workboots. And with the Night Cats -- his latest backing band, which features bassist Henry Lopez, drummer Donald Williams and reed expert Don Albright -- he's been hosting weekly practices at home for several months in order to take another shot at the stage. \"I always did like to entertain,\" he says. \"I don't care how old I am.\"\nThough such a desire spits in the eye of a strict Baptist upbringing -- fit only for backsliding types on the mourner's bench -- Houston cuts through the once-popular distortion of blues music as infernal noise.\n\"They call that workin' for the devil. It's not!\" he fumes. \"You want to talk about your history, your life. Is it wrong to tell your life? I'll play the blues in the church, 'cause I'm talkin' 'bout what we're goin' through -- 'specially the black people. And if they don't understand it, it's just too bad. It's me and the good Lord, that's who it is. He's involved in it, too. He gives me the talent and he tells me to use it.\"\n\"I still goes to church now,\" Houston adds. \"I turn on my TV. I get all the parish I want on that.\"\nMore at home in a juke joint than Amen Corner, Houston -- equal parts working stiff and junkyard griot -- sums everything up with characteristic simplicity: \"Blues ain't just said; blues is lived. People can tell on a bandstand if you're fakin' or you're telling the truth. You don't need a smile. The things I had to go without, and the things I had to go through -- that's not a good feeling. And it's not something to laugh at. But I want to be a gentleman up on the bandstand, like B.B.,\" He got a good name of bein' a nice, beautiful person. Warmhearted -- that's what I want to be.\n\"You got to have it here,\" Houston concludes, pointing to his chest. \"You got to make your heart talk.\"\nGet the Music Newsletter\nKeep your thumb on the local music scene each week with music news, trends, artist interviews and concert listings. We'll also send you special ticket offers and music deals."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:df795a4c-27f0-4e41-82ee-ea18f9d26a5a>"],"error":null}
{"question":"How do the automation challenges faced by modern finance departments compare to those encountered in the development of the Transit satellite navigation system?","answer":"Modern finance departments and the Transit system faced distinct automation challenges. Finance departments struggle with spending too much time on mechanical tasks, lack of immediate information access, and difficulties with data availability and analytic tools for contingency planning. The Treasury function particularly needs automation to handle complex cash management across multiple accounts and jurisdictions. The Transit system tackled automation challenges through progressive technological development, implementing advanced features like cesium oscillators for timekeeping (improving from 5 parts in 1010 to 2 parts in 1015 stability), automated orbital ephemeris broadcasts, and computerized systems for continuous data transmission. The Transit system eventually achieved automation that allowed satellites to broadcast ephemerides for five days autonomously without ground input, while many finance departments still heavily rely on manual processes and spreadsheets.","context":["The treasury function in finance departments doesn’t get a lot of attention, but it’s a fundamentally important one: to ensure that all funds are accounted for and that there is sufficient cash on hand each day to meet operating requirements. Keeping track of and managing cash, especially in larger organizations, can be complicated because of multiple bank accounts, complex financing requirements and various methods of receiving and making payments; the complexity deepens when more than one currency is used across multiple jurisdictions, which also can pose regulatory issues.\nMany finance executives want to improve their department’s effectiveness in order to play a more strategic role in their company. However, frequently they find at least three serious challenges to achieving this sort of finance transformation. One is that too much time and resources are devoted to purely mechanical tasks. Another is that the information executives need is not always available immediately. A third is that they lack the data (which is unavailable or too difficult to access), the analytic tools or both to do rapid contingency planning. One area in the Office of Finance that needs particular attention is treasury, as I commented recently. Treasury management is a challenge because it’s highly detailed and demands complete accuracy. These requirements make it an area that can benefit from more automation.\nTopics: Predictive Analytics, Office of Finance, Controller, credit, debt, Kyriba Financial Performance Management, Analytics, Business Performance Management (BPM), CFO, Financial Performance Management (FPM), cash management\nAlong with other aspects of the finance organization, there’s increasing emphasis on having the treasury function play more of a strategic role in the organization. Typically, Treasury is charged with keeping track of and managing cash. Especially in larger organizations, this can be complicated because of multiple bank accounts, complex financing requirements and many methods of receiving and making payments; the complexity deepens when more than one currency is used across multiple jurisdictions, which also can pose regulatory issues. Treasury’s primary directive is to ensure that all funds are accounted for and that there is sufficient cash on hand each day to meet operating requirements. To accomplish this, finance professionals must perform key analytic tasks accurately to produce a clear picture of cash inflows and cash requirements. Analysis often is challenging because these numbers are constantly changing and because the process of collecting, analyzing and reporting all the data can be excessively time-consuming if done manually. This is a situation perfectly suited for dedicated applications that automatically manage the data needed to orchestrate treasury processes and provide analysis to inform decisions. Yet our benchmark research finds that more than half (56%) of companies with more than 1,000 employees either use spreadsheets exclusively or employ them heavily in conjunction with a treasury application.\nTopics: Predictive Analytics, Office of Finance, Controller, credit, debt, Analytics, Business Performance Management (BPM), CFO, Financial Performance Management (FPM), cash management, Financial Performance Management\nAt this year’s Global Pricing Forum, host Nomis Solutions announced the availability of its Discretion Manager software, which supports dynamic price negotiations. The annual event brings together thought leaders and practitioners interested in pricing. Nomis currently has 17 of the largest 100 banks as customers. With more customers, this year’s event had larger attendance than last year’s.\nTopics: Big Data, Sales, Office of Finance, Operational Performance Management (OPM), credit, financial analytics, Nomis Solutions, PRO, Analytics, Business Analytics, Business Performance Management (BPM), Financial Performance Management (FPM), Sales Performance Management (SPM), banking, financial services\nIncreasingly, global financial markets compete on speed, so much so that high-speed trading capabilities have become a performance differentiator for the largest financial services firms and some investment funds. Transmitting messages with quotes, prices and trade data is a core capability for currency dealers. Informatica recently introduced Ultra Messaging, which is designed to offer global currency traders an efficient, high-throughput, lower-latency (that is, faster) and more secure method of linking their worldwide operations.\nI recently attended the 2012 Global Pricing Forum hosted by Nomis Solutions, a provider of software and services to banking and finance companies. This annual event brings together thought leaders and practitioners in the area of pricing and revenue optimization (PRO). This technique uses analytics to sift through large data sets to tease out customer behavior characteristics, identify customer segments and quantify their price sensitivities. These complex calculations require software designed for the purpose, but most in the financial services industry rely on older methods that produce less-than-optimal results. Analytics can help organizations more carefully manage the process of defining offers to customers (especially the levels of discretion offered to account managers and sales people) and the terms and conditions.\nTopics: Sales, Office of Finance, Operational Performance Management (OPM), credit, financial analytics, Nomis Solutions, PRO, Analytics, Business Analytics, Financial Performance Management (FPM), Sales Performance Management (SPM), Supply Chain Performance Management (SCPM), banking, financial services\nAt first thought, it seems as if having a mountain of cash to manage is a problem most companies would like to have, but it’s a real problem nevertheless. To be sure, the large majority of companies are able to deal with their cash and short-term and longer-term monetary investments because the amounts are small enough to be manageable. Indeed, many companies, especially smaller ones, face the opposite problem and spend more time focused on their uncertain funding requirements. Still, over the past decade highly profitable companies have been generating more cash than they need to fund expanding operations and capital spending requirements (Apple and Oracle are two examples), and now they have to manage it. Larger companies may have portfolios in the tens of millions to billions of dollars in multiple currencies in multiple jurisdictions, so there’s a lot at stake.","Transit - US Navy Navigation Satellite System (NNSS)\nTransit is a US Navy first generation satellite radio navigation system (the world's first), designed and built by JHU/APL - the entire development program (S/C hardware, ground support network, user equipment, etc.) was conducted within APL at Laurel, MD, USA; RCA (later named: GE Astro Space, now Lockheed Martin) was the satellite integrator starting with Oscar-18 (Transit-O-18). The program was initiated by Frank T. McClure in the spring of 1958, based on JHU/APL's Doppler tracking discoveries by William H. Guier and George C. Weiffenbach (the Sputnik orbit could be determined from RF Doppler data) and the realization of the “navigation problem” (it states: if the position of the satellite were accurately known, then Doppler data could tell an observer on the ground his unknown position). The Transit demonstration program was sponsored by DARPA (Defense Advanced Research Projects Agency) and officially begun in 1959. 1) 2) 3) 4) 5) 6)\nFigure 1: William H. Guier, Frank T. McClure, and George C. Weiffenbach (l to r) discuss the principles of the Transit Navigation System (image credit: JHU/APL)\nThe initial navigational accuracy requirement was set at 0.5 nmi (926 m) with an ultimate goal of 0.1 nmi (185 m). It was recognized from the beginning that significant improvement in geodesy would be needed to achieve these accuracies and that the Doppler shift measurements from orbiting near-Earth spacecraft were an excellent source of data to make this improvement. Thus, a worldwide network of tracking stations was established, and a constellation of geodetic satellites was proposed to obtain the necessary Doppler data.\nThe Space Segment:\nThe first satellites (up to and including Transit-3B) were spherical in shape. Transit-1A was completed in seven months after initial program funding. Transit-4A and -4B were drumshaped (almost a cylindrical body with four solar panels) to provide more space for solar cells. In addition, operational frequencies of 150 and 400 MHz were used for the first time. The dual frequency corrected for ionospheric errors. On-board time keeping was done with cesium oscillators. Throughout the Transit system service period (1959 - 1996) four generations of cesium frequency standards and three generations of hydrogen masers were experienced, starting with a stability of 5 parts in 1010 to about 2 parts in 1015 for the best tuned hydrogen maser.\nFigure 2: Illustration of the Transit-1B experimental satellite (image credit: JHU/APL)\nTransit-1A to -4B were regarded experimental satellites. The mass of each experimental satellite was about 136 kg. The TRAAC (Transit Research And Attitude Control) satellite, also built by APL and launched together with Transit-4B on an Ablestar vehicle (launch Nov. 15, 1961), flew the first gravity-gradient boom in history. However, stabilization was not demonstrated since only about 1 m of the 18 m boom (total length) deployed. The TRAAC S/C lost signal on Aug. 12, 1963 due to the degradation of solar cells. 7) 8)\nThe next series of Transit satellites (starting with -5A-1) were regarded as early operational prototype spacecraft. They were launched on a solid-fuel Scout vehicle from VAFB, CA, into polar orbits. The spacecraft mass of the first satellite, Transit-5A-1, was 55 kg. Three-5B series S/C were launched by Thor Ablestar vehicles, each with a piggyback 5E series S/C. The 5E S/C series had a solar power supply, while the 5B series S/C had nuclear power supplies. The Transit-5A series evolved into the Transit-5C-1 satellite with solar power supply.\nOrbits: Transit satellite series orbits were generally at altitudes (apogees) of about 1100 km. The initial satellite launches (up to Transit-4B) were from Cape Canaveral, FL. All further launches took place from VAFB, CA, with polar orbit inclinations (Table 1).\nFigure 3: Artist's view of the deployed TRANSIT Oscar spacecraft stacked on Scout (image credit: JHU/APL)\nThe Transit Oscar series:\nThe operational series of Transit satellites, more commonly referred to as “Oscar” series [or SOOS (Stacked Oscars on Scout)], closely followed the design of Transit-5C-1 with an important change: Hysteresis rods were installed on the solar panels to dampen the residual motion after the S/C despin maneuver in early orbit. The method of magnetic core storage (30 kbit) was used. Oscar-11 (Transit-O-11) was not used, it was later modified and launched as Transat on Oct. 27, 1977 from VAFB (Scout-D1 vehicle). The design life of the Oscar series was 3 years. Early Oscar series satellites had only short lifetimes. However, beginning with Transit-O-12 (Oscar-12), the satellites demonstrated an average orbital lifetime of more than 14 years. In fact, two S/C, Transit-O-13 and Transit-O-20, operated more than 21 years.\nNote: the Transit Oscar series should not be confused with the AMSAT Oscar series.\nTIP (Transit Improvement Program):\nTIP was initiated in 1969 with the objective to provide a radiation-hardened satellite. Each TIP satellite was equipped with a minicomputer providing a memory of 64 kbyte. In addition the 150 and 400 MHz transmitters were redesigned, providing an output power of 3 W (150 MHz) and 5 W (400 MHz). A hydrazine thruster system was used to correct for orbital precession. - Each TIP satellite had the requirement to broadcast ephemerides for five days autonomously without input from the ground. This in turn required the development of a drag-free satellite series, referred to as “Triad” (Transit-improved DISCOS). 9) 10)\nFigure 4: Artist's rendition of the TRIAD spacecraft in orbit (image credit: JHU/APL)\nThe DISCOS (Disturbance Compensating System) technology was spearheaded by Stanford University (the “ball-in-the-box” concept). Triad uses a three-body system [the actual satellite at bottom facing nadir, DISCOS at the center position, and a nuclear power supply (radioisotope thermal generator) at the top] connected by deployable booms. A major improvement in the TIP-satellite design was the introduction of the quadrifilar helix antenna. All major electronics subsystems were made redundant, radiation-hardened integrated circuits were interconnected on a ceramic substrate to obtain high-density packaging.\nFigure 5: Functional diagram of DISCOS control system (image credit: JHU/APL)\nTriad-1 (launched Sept. 2, 1972) was the first satellite to fly a completely gravitational orbit, free from all surface forces such as drag and radiation pressure. The orbit could in fact be predicted for up to 60 days.\nTriad-2 (TIP-2) and Triad-3 (TIP-3) satellites were each equipped with a redundant pulsed-plasma electric propulsion system (Isp = 225 kg s) used for drag compensation. The 1 kg Teflon, used for both thrusters, provided a fuel supply for 10 years.\nNova satellites. Three more spacecraft, nearly identical to the TIP series design, were built by RCA on Navy request. Improvements included: the addition of magnetic damping to the DISCOS and a stiffening to the boom assembly, reference clock, computer capacity, 6 m gravity boom, S/C mass was 166 kg. The NOVA satellites used the electric propulsion system (PPT system, Isp = 543 seconds, total impulse = 2450 Ns) of the TIP series satellites to compensate for drag with thruster firings. The NOVA satellites had operational lifetimes of 8 to 9 years. 11) 12)\nFigure 6: Artist's view of the Nova spacecraft (image credit: JHU/APL)\nThe control segment:\nTransit system operations and control are conducted at NAVSOC (Naval Satellite Operations Center, since 1962), Point Mugu, CA - working through tracking stations at Laguna Peak (near Point Mugu, CA), Prospect Harbor, ME, Rosemont, MN, Wahiawa, HI, Finegayan, Guam (since 1993) and at Falcon AFB, Colorado Springs, CO (since 1988). The tracking stations collected Doppler data and transmitted in the uplink the predicted ephemeris to each satellite. This data was stored on-board for continuous rebroadcasts to the user community. 13) 14)\nWhen Transit navigation services were terminated/decommissioned on Dec. 31, 1996, NAVSOC retained other mission assignments, among them operations of GFO-1 and NIMS (Navy Ionospheric Monitoring System). On Jan. 1, 1997, the remaining Transit system constellation (of six Oscars in three orbital planes) became NIMS with a new application, namely to utilize the Transit system resources for computerized ionospheric tomography (CIT). In this setup, the NIMS satellites are being used as dual-frequency beacons by ground collection sites to determine the free electron profile of the ionosphere.\nTable 1: Overview of Transit program satellites\nThe User Segment:\nThe Transit navigation operates on the principle that a receiver's position can be determined on a single satellite pass by measuring the Doppler shift for a period of 10 to 15 minutes. A user's receiver on the ground measured the time history of the refraction-corrected Doppler data and recorded the orbital ephemeris as the satellite passed overhead. The user was able to calculate position initially within a few hundred meters, later within 15 to 25 meters. Some limitations: The user had to know his altitude and the satellite ephemeris; an error was introduced for an unknown vessel speed. A position fix with the Transit constellation was only available every 35 to 100 minutes. Navigation information could only be obtained by instruments of slow-moving users (like ships). For stationary users, such as surveying and oil platform location, integration of measurements from several passes yielded rms accuracies in the order of 5 m. Toward the end of Transit system operations in 1995, commercial receiver prices were as low as $1000. The 2-D system did not permit velocity determination.\nIn 1964, the Transit system became operational. In 1967 Transit navigation services were made available to the general public (commercial and private users). Commercial companies were permitted to manufacture and to sell low-cost receiving equipment. This action resulted in the use of more than 80,000 privately-owned Transit receivers (in particular in the field of commercial shipping). Also, oil-drilling platforms were among the first to use Transit to determine the boundaries of oil deposits. 15)\nTo reduce the size of instrumentation that had to be carried to remote places for site survey operations, a special-purpose receiver called the Geoceiver was developed by the Magnavox Corporation. This self-contained instrument was slightly larger than a briefcase and combined single-sideband receivers for the Transit operating frequencies, a standard frequency oscillator, and a data reduction system to provide a data record for each satellite pass. APL used this data record along with tracked orbital data to determine a site survey.\nFigure 7: Illustration of the Georeceiver AN/PRR-14 (image credit: JHU/APL)\nDuring its 32 years of operation, the Transit Navigations System (or NNSS) provided accurate and reliable global navigation for the US Navy and for the civilian community. The system was continuously improved, in addition it contributed to many technology advances (see Table 2).\nLegacy of Transit: It all began with the Transit all-weather global navigation system. That system helped propel America into the space age, sparking a large number of firsts in space engineering, technology, and science, and led to the eventual development of today’s Global Positioning System. Transit formed the foundation of what we are and what we do in the Space Department today. The remarkably short time from initial concept to operational availability was achieved by a group of professionals who were able to design, build, launch, and operate some 20 spacecraft (7 of which did not reach orbit) in approximately 4 years. 18)\nFigure 8: Schematic architecture of the Transit system measurement concept (image credit: JHU/APL)\n1) Robert J. Danchik, “An Overview of Transit Development,” Johns Hopkins APL Technical Digest, Vol. 19, No. 1, 1998, pp. 18-26, URL: http://www.jhuapl.edu/techdigest/TD/td1901/danchik.pdf\n2) William H. Guier, George C. Weiffenbach, “Genesis of Satellite Navigation,” Johns Hopkins APL Technical Digest, Vol. 18, No. 1, 1998, pp. 14-17, URL: http://www.jhuapl.edu/techdigest/TD/td1901/guier.pdf\n3) H. D. Black, R. E. Jenkins, L. L. Pryor, “The TRANSIT System, 1975,” APL/JHU Technical Memorandum, Dec. 1976, URL: http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA037398&Location=U2&doc=GetTRDoc.pdf\n4) Steve M. Yionoulis, “The Transit Satellite Geodesy Program,” Johns Hopkins APL Technical Digest, Vol. 19, No. 1, 1998, pp. 36-42, URL: http://www.jhuapl.edu/techdigest/TD/td1901/yionoulis.pdf\n5) Carl O. Bostrom, Donald J. Williams, “The Space Environment,” Johns Hopkins APL Technical Digest, Vol. 18, No. 1, 1998, pp. 43-52, URL: http://www.jhuapl.edu/techdigest/TD/td1901/williams.pdf\n6) “The Legacy of Transit,” URL: http://www.jhuapl.edu/techdigest/td/td1901/index.htm\n7) Note: The very first Transit satellites transmitted signals at four frequencies: 54, 162, 216, and 324 MHz. The signals provided experimental data to evaluate ionospheric effects as a function of frequency. The final design is based on a two-frequency method for correcting ionospheric error.\n9) J. Dassoulas, “The TRIAD Spacecraft,” Johns Hopkins APL Technical Digest, Vol. 12, No. 2, pp. 2-13, June 1973\n10) Daniel DeBra, “A Satellite Freed of all but Gravitational Forces: TRIAD I\" AIAA 12th Aerospace Sciences Meeting, Washington, D.C., January 30-February 1, 1974, AIAA Journal Vol. 11, No 9, Sept. 1974, pp. 637-644, URL: http://einstein.stanford.edu/content/sci_papers/papers/DeBra_AIAA-TRIAD-I_1974.pdf\n11) W. L. Ebert, S. J. Kowal, R. F. Sloan, “Operational NOVA Spacecraft Teflon Pulsed Plasma Thruster System,” AIAA-89-2497, AIAA/ASME/SAE/ASEE 25th Joint Propulsion Conference, Monterey, CA, July 10-12, 1989\n12) Y. Brill, et al., “The Flight Application of a Pulsed Plasma Microthruster: the NOVA Satellite,” AIAA-82-1956, 16th International Electric Propulsion Conference, Nov. 1982\n13) Gary C. Kennedy, Michael J. Crawford, “Innovations Derived from the Transit Program,” Johns Hopkins APL Technical Digest, Volume 19, No. 1, 1998, pp. 27-35, URL: http://www.jhuapl.edu/techdigest/TD/td1901/kennedy.pdf\n14) A. J. Tucker, “Computerized Ionospheric Tomography,” John Hopkins APL Technical Digest, Vol. 19, No. 1, 1998, pp. 66-71, URL: http://www.jhuapl.edu/techdigest/TD/td1901/tucker.pdf\n15) Lauren J. Rueger, “Development of Receivers to Characterize Transit Time and Frequency Signals, John Hopkins APL Technical Digest, Vol. 19, No. 1, 1998, pp. 53-59, URL: http://www.jhuapl.edu/techdigest/TD/td1901/rueger.pdf\n16) E. J. Hoffman, “Spacecraft Design Innovations in the APL Space Department,” Johns Hopkins APL Technical Digest, Vol. 13, No. 1, 1992, pp. 167-181\n17) R. B. Kershner, “Technical Innovations in the APL Space Department,” Johns Hopkins APL Technical Digest, Vol. 1, No. 4, 1980, pp. 264-278\n18) Stamatios M. Krimigis, “APL’s Space Department After 40 Years: An Overview,” Johns Hopkins APL Technical Digest, Vol. 20, No 4, 1999, pp. 467-476, URL: http://www.jhuapl.edu/techdigest/td/td2004/krimigis.pdf\nThe information compiled and edited in this article was provided by Herbert J. Kramer from his documentation of: ”Observation of the Earth and Its Environment: Survey of Missions and Sensors” (Springer Verlag) as well as many other sources after the publication of the 4th edition in 2002. - Comments and corrections to this article are always welcome for further updates."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:691d555b-a116-4966-bbe5-f4489eaa8bfe>","<urn:uuid:3d0d162c-7f68-4393-9c23-47e8a9aed55f>"],"error":null}
{"question":"How do climate disasters impact businesses vs real estate in California?","answer":"Climate disasters impact both sectors significantly but differently. For businesses, the 2017 California wildfires caused $18 billion in damages with ongoing economic consequences, while business interruptions from natural disasters can disrupt local economies and reduce tax revenues - for example, the 2017 wine country fires caused $6-8 billion in economic losses from property damage and business interruptions. For real estate, climate change is already causing coastal properties exposed to sea-level rise to sell at a 7% discount, and by century's end, rising seas and storms will put $150 billion of California coastal real estate at risk of flooding. Additionally, wildfire risk is affecting real estate markets - the Camp Fire was 2018's largest insurance event, and if risks become too high, insurers may exit certain California regions, impacting moderate-priced homes.","context":["The year 2017 will stay on the record as one of the most expensive years to date for climate and weather disaster events. The U.S. experienced 16 weather and climate disasters that caused over $1 billion in damages, tying the record year of 2011 for the most billion-dollar disasters. From summer through the fall, wildfires in various parts of California led to fatalities, destruction of entire communities, and damage costs of $18 billion, with economic consequences that will continue to impact the region. These events have highlighted that climate change has already begun to and will continue to impact local communities and businesses, and that local economies will benefit from more coordinated resilience planning.\nCommunities across the U.S. are taking steps to identify their climate change risks and enhance their resilience to changing climate conditions. Many local governments have assessed their vulnerabilities and are developing resilience plans with support from local stakeholders. However, a key set of stakeholders are often not at the table: businesses. Collaboration between local governments and the business community on climate change resilience remains limited. As local and regional climate change planning continues, it becomes increasingly important for local governments to engage with businesses, both large and small, on these issues.\nThe success of businesses and communities is intertwined\nMany larger companies recognize the impacts of climate change on their operations, including risks to physical assets, disruptions to supply chains, and impacts on their workforce. In fact, some businesses, like Google, are examining how to develop company resilience strategies that address changing climate conditions. Businesses are also dependent on public infrastructure and local government services, and climate risks on these “outside the fence” components are much harder for businesses to evaluate. In fact, a number of companies have highlighted these uncertainties as a major barrier in addressing adaptation.\nLocal governments are dependent on the private sector in many ways. Businesses are essential to the economic health and growth of communities. Business interruptions can affect the quality of life for residents, disrupt the local economy, and reduce tax revenues. The costs of Hurricane Harvey are still being evaluated, but preliminary estimates suggest that lost economic output from this storm was in the range of $9 billion to $11 billion, including $540 million for goods-producing industries and $141 million for oil and gas industries. The October 2017 wildfires in California’s wine country are estimated to have caused economic losses between $6 and $8 billion dollars due to property damage and business interruptions alone, with $789 million in commercial property claims. These costs do not include the potential losses to the wine industry for many years to come.\nLocal governments have a strong interest in ensuring that businesses are resilient and remain operational as the climate continues to change. Companies will also benefit from engaging with the public sector on community resilience to enhance their business continuity plans and support their employees. In addition to better protecting their employees and operations, this type of collaboration will help businesses better understand community needs.\nBusinesses can assist local governments with expertise and solutions\nLarger businesses often already understand local risks because of internal risk management processes. Risk management and emergency management plans, along with drills and training exercises with employees, help businesses prepare for extreme events. Local governments can coordinate with businesses on risk management, including participating in drills and trainings, to build and maintain community resilience.\nLocal governments can also use larger companies’ expertise and data on risk. Businesses may be monitoring information that could be relevant to local resilience planning. For example, utilities often track potential risks to their assets, such as those related to storms (e.g., wind, precipitation, flooding), wildfire, and temperature impacts on energy demand. This information can be helpful to local decision-makers in both emergency management and long-term resilience planning.\nThe private sector also offers opportunities in services and solutions. Businesses are often interested in developing and improving technologies, engineering approaches, technical assistance, and opportunities to connect with their communities. For example, Airbnb offered disaster relief to people impacted by the California wildfires, connecting displaced residents to available housing. The company also worked with the City of San Francisco’s Department of Emergency Management to share their lessons learned from Superstorm Sandy. Airbnb is also partnering with various local governments to help communities prepare for and recover from disasters. Local governments’ suggestions for climate change solutions and services can help businesses tailor their products to best serve the community.\nIn addition, financing for implementing community resilience can often be a challenge for local governments. The private sector can offer financing solutions to help fund climate change resilience. For example, Pacific Gas and Electric Company (PG&E) is investing $1 million over five years through their Better Together Resilient Communities grant program to support local climate resilience initiatives in California.\nLocal governments can share data and information with businesses\nSome local governments have undertaken vulnerability assessments and climate change scenario planning for their regions. The data and results from these studies can be shared with businesses to help them understand what assumptions are being used by local governments, and whether their scenarios align, which will be increasingly important to ensure regional coordination as conditions change.\nWhile larger companies may undertake scenario planning and vulnerability assessments, most small businesses do not. However, small businesses can also benefit from data and information sharing. Small companies do not often have the expertise or resources to adequately assess climate change risks and undertake resilience planning. Local governments can share information with small businesses to help them better understand their potential risks and prepare for extreme events. In California, Valley Vision has developed the Capital Region Business Resiliency Initiative to help engage the small business community in resilience planning. This effort helps small businesses engage with local stakeholders to understand potential risks and provides resources to help these businesses plan for disaster resilience.\nLocal governments can engage with businesses through existing networks or by creating new processes to assist with engagement\nLocal governments can engage with both small and large businesses through networks and organizations for the private sector, like local chambers of commerce, trade associations, and other business networking groups. For example, the City of Annapolis has engaged the Anne Arundel County Chamber of Commerce and the Downtown Annapolis Partnership in its Weather It Together initiative, which is focused on adapting the historic community to minimize the risks associated with flooding. Through this effort, local businesses are part of the planning process to help the community become more resilient. The City of Cambridge, Massachusetts has also engaged businesses in long-term planning efforts like the Cambridge Compact and the city’s Climate Change Preparedness & Resilience Plan. Establishing public-private partnerships focused on climate resilience will also help to facilitate conversations and collaboration between these two sectors.\nLocal governments may already engage with businesses individually, but it can be helpful to set up an ongoing process for involving the private sector in resilience planning. For example, business representatives can participate in local planning and advisory committees, contributing their perspectives and identifying any key issues for the business community. Effectively engaging the business community will often require targeted outreach and potentially different strategies, as businesses may not be aware of ongoing stakeholder processes or may not realize their relevance to company needs. Some communities have incorporated businesses into resilience planning through regional climate collaboratives. Several regional climate collaboratives in California focus on engaging different stakeholder groups, including businesses, to further climate change planning. For example, the Sierra Climate Adaptation and Mitigation Partnership was founded by the Sierra Business Council and has various business members, including ski resorts and forestry companies.\nEffectively preparing for climate change’s impacts requires that cities coordinate with many different stakeholders. Businesses, public agencies, community groups, and citizens are all important to the discussion on community resilience, as they will all be impacted by climate change and have important ideas to contribute. Engaging the private sector is an important way for local governments to improve community resilience, and will benefit both the public and private sector through information sharing, aligning needs and goals, and developing multi-sector networks.","Risk & Reward: 5 California Industries Being Reshaped by Climate Change\nCalifornians are feeling the effects of climate change: wildfires, record heat waves, and even atmospheric rivers have begun to wreaked havoc on the Golden State.\nIt’s increasingly clear that certain sectors of the world’s fifth largest economy are emerging as ground zero for the costs of climate change – and the potential costs are staggering.\nThis year alone, nearly half a million homes in California face high or very high wildfire risk, at a potential cost of $268 billion.\nBut at the same time, there are industries — new and existing — that may flourish in this changing landscape. With that contrast in mind, here’s a look at five sectors being reshaped by climate change:AT RISK Food & Wine | Agriculture Faces Water Challenges\nThe changing climate will likely increase the need for fresh water, but reduce the supply. California is the top food producer in the U.S., so this is a serious concern for its agriculture industry.\nWith nearly 90% of California’s crops grown on farms that are entirely irrigated, climate change may force farmers to reduce cultivated acreage or shift from water-intensive crops. For example, over the next 75 years, the land capable of consistently growing high-quality wine grapes is likely to shrink by more than 50%.\nMy colleague Adam Beak has experience on both sides of the wine business. Based in Napa, he’s helped lead wineries as a CFO and owner himself and brings this unique perspective to his work as managing director of our wine & beverage group.\n“Water access and water supply is a top concern for California’s wine industry and the impact of global warming on rainfall and water availability is a serious concern for the long term.”\nAdapting to climate change by growing different varietals is an option for California’s farmers, including those in the wine industry. However, there may be fewer ways to adapt crops such as almonds, which was valued at $5.6 billion in 2017, because a gallon of water is required to grow a single nut.Tourism: Snowfall is an Achilles’ Heel for Winter Sports\nSnow sports are one of the state’s biggest draws. An average snow year in California represents $1.6 billion in economic value from millions of skier visits.\nBut higher temperatures result in more rain and less snow. As the climate warms, California’s snowpack will diminish, shortening the ski season, reducing both in-state and out-of-state visitors and severely affecting the range of businesses that service the snow sports industry and provide jobs.Real Estate: Fire and Water are a Risky Combination\nSome studies say climate change is already impacting real estate markets, with U.S. properties exposed to sea-level rise selling at a 7% discount compared with those with less exposure.\nBy the end of the century, rising sea levels, combined with more serious storms and erosion, will put an estimated $150 billion worth of California’s iconic coastal real estate at risk of flooding. Wildfire is another risk. California’s Camp Fire was the single largest insurance event of 2018.\nIf the risk becomes high enough, insurers may be pushed out of certain California regions, affecting the market for moderately priced homes.\nCalifornia is the top state in the nation for generating electricity from renewable resources.\nThese industries are well-positioned for growth in part because the state’s renewable portfolio standard requires 33% of retail electricity to come from renewables by 2020, 60% by 2030, and 100% by 2045.\nIn addition, in 2018, traditional energy jobs in the U.S. – those related to non-renewables – declined while those related to energy efficiency grew. Energy efficiency-related jobs involve work on buildings, appliances, and transportation to reduce energy use or to improve “smart grid” technology. They remain the highest source of employment within the energy sector and show the highest growth rate nationally. The state with the most energy efficiency jobs? California.Cleantech: State’s Zero-Emissions Goals Shoot for the Moon\nA low-carbon economy will be fueled by the clean energy, bio-engineering, and electric vehicle industries among others.\nCalifornia’s largest source of GHG emissions comes from the transportation sector. With the policy support of a plan to put 1.5 million zero-emission vehicles on the road by 2025, car manufacturers are incentivized to build better and cheaper electric vehicles that appeal to more buyers.\nPossibly the biggest opportunity for would-be climate disruptors are so called “moonshot” technologies. Many of these focus on removing CO2 from the atmosphere and either transforming it into a useful raw material or simply storing it.\nCalifornia companies – whether in Silicon Valley, Los Angeles, or other regional innovation hubs – that can leverage technology innovation to address climate change, will be able to capitalize on the carbon-free economy of the future.\nThe potential annual cost of climate change to the U.S. economy has been placed at $224 billion by the end of this century. As with almost all upheavals, there will be winners and there will be losers.\nWill California’s climate economics impact your industry in the future?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:abac6898-51b0-42fd-a5b1-b5052c4a2b07>","<urn:uuid:192b8f0b-ab82-4a8f-b0fa-68ed9734517e>"],"error":null}
{"question":"How do BREEAM and LEED handle building assessment post-construction?","answer":"BREEAM offers a specific scheme called BREEAM In-Use, designed to help building managers and owners reduce running costs and improve environmental performance of existing buildings, requiring assessment by independent auditors. LEED similarly covers existing building projects and operations, monitoring aspects like energy efficiency and indoor environmental quality throughout the building's lifecycle.","context":["Co to jest BREEAM?\nSystem BREEAM definiuje standard najlepszych praktyk w zakresie zrównoważonego projektowania, budowania i użytkowania budynków. System akredytacji BREEAM wykorzystuje uznane miary właściwości, które porównywane są do wartości wzorcowych w celu oceny procesów specyfikacji, projektowania, budowy i użytkowania budynku. Wykorzystane miary reprezentują szeroką gamę kategorii i kryteriów, od energii aż po ekologię. Obejmują one między innymi: jakość środowiska wewnętrznego, efektywność energetyczną, dostępność transportową, materiały i konstrukcję, zarządzanie eksploatacją i realizacją, gospodarkę wodą i odpadami. Z punktu widzenia użytkownika najcenniejszymi kategoriami są te obejmujące komfort użytkownika (dostęp do światła dziennego i widoku z okna, możliwość sterowania oświetleniem, ogrzewaniem i chłodzeniem) oraz transport (dostęp do komunikacji publicznej, parkingi rowerowe czy miejsca do ładowania samochodów elektrycznych).\nCo umożliwiają szkolenia PLGBC?\nBREEAM International New Construction Assessor\nDelivery of this course is achieved by using a blended learning approach of on-line pre-course materials in the form of digital modules, followed by interactive classroom workshops and examinations.\nOn successful completion of the course and examinations, you will emerge as a fully qualified BREEAM International assessor.\nPlease note: if you are an assessor who previously qualified and became licenced under the 2013 scheme you will still be able to assess under the 2016 version.\nThis scheme uses calculation tools which run in MS Excel with macros. Macintosh computers are unable to use this function so you will need access to a PC to complete these assessments.\nBREEAM International Assessor training commences with approximately 30 hours of interactive online modules delivered through our dedicated online learning portal. You will be led on a journey into BREEAM by way of interactive content and technical presentations, which will gradually build your knowledge and confidence within the BREEAM International 2016 scheme. You can complete these modules at your own pace, and there is on-line support to help you. Each module concludes with an automated quiz to allow you to track your progress and attainment throughout the course.\n- Module 1 – An introduction to BREEAM\n- Module 2 – The BREEAM International and Bespoke processes, reporting and evidence requirements\n- Module 3 – Technical modules\n- Management, Health & Wellbeing, Energy, Transport\n- Module 4 – Technical modules\n- Water, Materials, Waste, Land Use & Ecology, Pollution\nFollowing the online learning phase, you will attend 2.5 days of classroom based learning which will consolidate your understanding and newly acquired knowledge. Through workshops and group learning activities, we will refine your understanding gained through the online training, before moving on to the more complex and technical aspects of the scheme. You will be fully supported by a dedicated tutor for each module. Training is delivered by leading BRE expert technical trainers.\n2.5 day classroom based learning\nDay 1 – Brief initial test to ensure you have completed the online self-study modules\n- Review of the BREEAM International processes\n- Focus on a couple of the most complex issues within the scheme\nDay 2 – Tailoring BREEAM International for different building types\n- Selling BREEAM\n- Relationship between different BREEAM Schemes\n- BREEAM International exam\nDay 3 (half day)\n- Question and Answer Session\n- BREEAM General Understanding exam\nYou will undertake two multiple choice examinations. Both exams are open book, allowing you to refer to any notes or documentation that may assist you. Assessors will need to pass both examinations to become qualified within the scheme. Please note that you must present acceptable photo identification (ID) for admission to the examination. This may be either your passport, driving licence or EU ID card. Please note that without photo ID you will not be permitted to undertake the examinations.\nPlease note that once qualified, to offer and carry out BREEAM assessments through to certification you or your organisation must hold the relevant BREEAM licence. To become licensed, all assessors/assessor organisations are required to hold a minimum level of professional indemnity insurance. Furthermore, depending on the scheme chosen, there may be an annual fee payable to BRE Global to hold the required BREEAM licence. For more information please see our FAQ page or contact us at firstname.lastname@example.org\nBREEAM Membership opportunity:\nAfter successful completion of this course you can register as a BREEAM Member with the BRE Academy. The BRE Academy BREEAM Membership is a badge of recognition that highlights your level of knowledge and shows your commitment to realising the benefits of BREEAM.\nAs a member, you will join a group of growing and experienced individuals across a variety of areas within the built environment.\nSpecific benefits you will be able to take advantage of include discounts on BRE Academy training courses, FREE monthly webinars and exclusive discounted BRE services. Find our more information about BRE Academy membership to see if you qualify!\nBREEAM Accredited Professional\nAfter successfully completing this training course and examination, delegates will become qualified BREEAM Accredited Professional (AP). As a BREEAM AP delegates will be able to support project teams to deliver their sustainability goals and influence how a development will meet BREEAM targets.\nLearning outcomes and benefits:\nThis qualification enables you to work across the relevant BREEAM schemes as a sustainability professional.\nDelegates completing all aspects of this course will learn the role of the AP and what a development needs to do in order to meet BREEAM targets and sustainability goals. Delegates will learn the need to facilitate projects teams to schedule activities, set priorities and negotiate the trade-offs required to achieve the target BREEAM rating when the development is formally assessed.\nOnce qualified, a BREEAM AP can help developments achieve up to six BREEAM credits dependent on the schemes that they are working on. They are recognised throughout the industry as a sustainability champion who can:\n- Encourage an integrated design and construction process\n- Use BREEAM as a framework to establish, agree and achieve the desired level of sustainability performance for a development\nThe qualification and annual membership recognises an individual’s ability to provide expert advice on how projects should achieve their sustainability goals and performance.\nThis course has been designed to allow suitably experienced construction professionals who have a good knowledge of environmental design and the design process to deliver new construction and refurbishment buildings to meet BREEAM.\nThere are three aspects to this course:\n- Interactive online modules\n- A classroom based workshop\n- An examination\n1. Online training\nThe online pre-workshop modules must be completed prior to attending the workshop, to enable delegates to have the best chance to pass the examination. The online modules cover:\n- The role of a BREEAM AP\n- Background and scope of BREEAM\n- BREEAM Assessment methodology\n- BREEAM schemes technical content\n- Decision timeline\n- Decision priorities\nAfter completing the online modules, delegates will attend a workshop followed by an examination. The workshop will allow delegates to apply and share their expertise in a series of individual and team exercises. The workshop will cover:\n- Making a case for Sustainability and BREEAM\n- Decision Making\n- Comparison on costs per percentage point basis\nIn order to become a BREEAM AP, delegates will need to pass an examination. This takes place at the end of the workshop and includes one 60 Minute multiple choice examination\nBREEAM In-Use is designed to help building managers and owners reduce the running costs and improve the environmental performance of existing buildings. The scheme uses an assessment tool that can be completed by building managers and owners to self-assess their buildings. These self-assessments have to be audited by independent BREEAM In-Use assessors before certification can be granted.\nTo become a BREEAM In-Use assessor you need to complete the online learning, attend a two day course, pass the examination and take out a BREEAM In-Use licence. To become licensed, assessors are required to hold professional indemnity insurance. For more information please see our FAQ page.\nBREEAM in-Use can be used to assess assets worldwide and has already been used in over 25 countries.\nThere are two possible entry routes for this course\nCurrently qualified BRE Global assessors of one of the following:\n- Code for Sustainable Homes,\n- Fire Risk\n- Energy Assessor schemes.\nThose with at least two years full time experience (within the last five years) in one or more of the following professions:\n- Building surveying\n- Building regulatory (control) services\n- Facilities management\n- Product or manufacturing auditing\n- Certification experience\n- Environmental/Energy background","Your Guide to LEED Certification and the LEED Rating System\nThe Leadership in Energy and Environmental Design, or LEED, is a based on a Green Building LEED rating system created by the USGBC, or United States Green Building Council. The LEED system is a certain set of guidelines that dictate the environmental viability of construction projects. Since the founding year of this council in 1988, it has had a hand in approximately 14,000 construction projects in 30 countries including United States, equaling to a development area of 1.062 billion square feet. This council provides third-party verification for various building and community projects.\nThe Purpose of LEED\nThe entire purpose of LEED is to keep watch over aspects regarding water efficiency, energy saving capability, indoor environmental quality, carbon dioxide emissions, and resource preservation. The LEED rating system is geared toward residential and commercial construction and is put into place during the entire building phase from design, construction, and operations to tenant fit, maintenance, and significant retrofit. Aside from these aspects, LEED certification also promotes whole-building and integrated design practices, environmental building industry leadership, green building consumer awareness, construction market modification, and green competition.\nBecoming LEED Certified\nObtaining a LEED Certification is possible only when a building is stated to be eco-friendly because it fits certain criteria. Not only does this certification promote the construction process and the profits that stem from it, but also the reduction of negative environmental factors in order to enhance the health of the building’s occupant. This certification will guarantee that society, builders, and their clients and competition will know that a building possesses certain environmental objectives and performs according to its intentional design. There are several benefits to this certification. It causes society to be more conscientious of safe environments, the press to express interest in building projects, and builders to reap government incentives on the local and state level.\nThere are various levels to the LEED rating system–a system guided by points. 26-32 points provides a “Certified” rating, 33-38 points is a “Silver” rating, 39-51 equals “Gold,” and 52-69 means “Platinum.” These points are related to the five different green design classifications–indoor environmental quality, water efficiency, sustainable site, materials and resources, and energy and atmosphere. A few projects that are LEED certification covered include major renovations, new commercial construction, existing building projects, and interior projects. Additionally, there is a LEED affiliate for Neighborhood Development. This affiliate provides certain benefits to building projects that are advantageous to an entire neighborhood.\nBefore obtaining your LEED certification, you should first consider what LEED rating system level you intend to target as well as your total budget for the particular project you are planning. Keep in mind that the higher rating levels, such as “gold” and “platinum,” require a large budget. You must also take the appropriate steps to ensure that the building’s economic aspects and environmental aspects mesh harmoniously with one another. Above all else, be sure that your project’s team is focused and well-versed on meeting your particular LEED level, all within the confines of your budget.\nLearn how to turn your corporate office into a LEED Certified Green Building. Browse through our extensive library of hand picked recommended books on LEED certification, the LEED AP Exam, Green construction, sustainable living, and environmental issues.\nVolunteer to become a LEED certified professional at your current company. LEED certification training is often provided for in return for your ability to make your company comply with environmental and LEED standards."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:34d363f3-8d88-4826-9496-af72038440c2>","<urn:uuid:30493d77-b08f-4c25-a3a7-7c502bdf8b8d>"],"error":null}
{"question":"What was unique about the domestication process of common beans compared to other food crops?","answer":"Unlike most other food crops, the common bean was domesticated twice by humans about 8,000 years ago - once in Mexico and once in South America. These two domestication events involved largely non-overlapping, unique subsets of genes with very little mixing between the two domesticated populations.","context":["Huntsville, Ala. String bean, snap bean, haricot bean, and pinto and navy bean. These are just a few members of the common bean family scientifically called Phaseolus vulgaris. These beans are critically important to the global food supply. They provide up to 15 percent of calories and 36 percent of daily protein for parts of Africa and the Americas and serve as a daily staple for hundreds of millions of people.\nNow, an international collaboration of researchers, led by Jeremy Schmutz of the HudsonAlpha Institute for Biotechnology and Phillip McClean, of North Dakota State University (NDSU) have sequenced and analyzed the genome of the common bean to begin to identify genes involved in critical traits such as size, flavor, disease resistance and drought tolerance. The study was funded by the US Department of Agriculture, National Institute of Food and Agriculture and the US Department of Energy Office of Science.\nThe researchers learned that, unlike most other food crops, the common bean was domesticated twice by humans about 8,000 years ago once in Mexico and once in South America through the selection of largely non-overlapping, unique subsets of genes.\n\"We found very little overlap, and very little mixing, among the two domesticated populations,\" said Jeremy Schmutz, who co-directs the HudsonAlpha Institute's Genome Sequencing Center and serves as the Plant Program Leader for the Department of Energy Joint Genome Institute. \"Evolutionarily, this makes the common bean very unique and interesting.\"\nSchmutz shares lead authorship of the current study, which was published on June 8 in Nature Genetics, with Phillip McClean, director of the genomics and bioinformatics program at NDSU. Scott Jackson, from the University of Georgia, is the senior author.\nThe HudsonAlpha Genome Sequencing Center specializes in the production of reference plant genomes and genomic resources with a focus on improving agriculture and developing plant-based energy sources. In 2010, Schmutz led a team of researchers that used the Center's unique facilities to be the first to sequence the genome of the soybean another vital global crop.\nIdentifying genes involved in the domestication of the common bean, and comparing locally adapted domesticated bean groups (called landraces) to their wild counterparts throughout Mexico and South America will help researchers understand how beans evolved, and how modern breeding programs might be improved to yield tastier, more-easily harvested, and, yes, even more-nutrient-packed beans. It may also help scientists to develop bean varieties resistant to pests, or better able to grow in challenging environments.\nThe common bean originated from a wild bean population in Mexico, and shares a common ancestor with the soybean. In addition to its role as a critical food crop, it serves as a partner in a symbiotic relationship with nitrogen-fixing bacteria to improve the soil in which it is planted.\n\"We're trying to understand what the common bean looked like before human intervention, to identify what occurred during early domestication and to apply that to modern bean breeding,\" said Schmutz. \"Modern beans have been bred to fill specific expectations with regard to color, size and shape, and as a consequence have very little diversity. Studies such as this are necessary to identify genes that could be used to improve traits such as ease of harvest, flavor, yield and disease resistance.\"\nOnce genes are identified, they could be reintroduced into the population by selective breeding with wild populations, or careful breeding of existing landraces or even commercial beans. The Common Bean Coordinated Agricultural Project, or BeanCAP, launched in 2009 under the direction of study co-author McClean, is dedicated to the identification of gene markers that can be used in such breeding programs.\n\"The genome sequence has important implications for world-wide efforts to improve beans,\" said McClean. \"The sequence will help breeders release varieties that are competitive with other crops and more climate resilient.\" The sequence revealed that disease resistance genes are highly clustered in the genome, knowledge that will lead to better breeding strategies to combat the many diseases that challenge the bean crop. Data from the study is being actively used by the many international bean breeders and geneticists to develop the next generation of molecular markers to aid bean breeding efforts.\nFrom a global perspective, this information could be beneficial to farmers in developing countries that practice the intercropping system known as \"milpa\", where beans, corn, and occasionally squash, are planted together. The historical practice ensures that their land can continue to produce high-yield crops without resorting to adding fertilizers or other chemical methods of providing nutrients to the soil. McClean noted that \"Breeders and genomic scientists in these countries are already working with the international bean community to utilize this important new genetic resource to address the production constraints unique to the \"milpa\" system.\"\n|Contact: Beth Pugh|\nHudsonAlpha Institute for Biotechnology"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cc5bb605-ff97-41d2-9a4a-4c42852162ee>"],"error":null}
{"question":"What are the primary purposes of a REST API, and what security measures are essential for protecting it?","answer":"A REST API provides an interface for developers to communicate with applications through their software, enabling extended functionality and accessibility of existing applications. For example, it allows creating applications that deliver statistics or automate posting across social media platforms. Regarding security, REST APIs must implement several essential measures: they must only provide HTTPS endpoints to protect authentication credentials, perform access control at each endpoint, use JSON Web Tokens (JWT) for access control decisions, validate JWT integrity through signatures or MACs, and verify standard claims like issuer, audience, and expiration time. Additionally, API keys can be used to mitigate excessive usage and denial-of-service attacks.","context":["Imagine wanting to develop an application which has the main purpose to store, process and deliver data. Your data model is mature enough, you have a decent interface in place. Basically, the whole application is functioning well as a whole and is technically fulfilling its primary requirements. But what is the next step? The users of the platform might want to run statistics or analysis? The interface is not to the liking of all users? Or maybe it would be nice if the platform could be used from other applications through integration with add-ons or such? How would we go about that?\nThe simple answer to this question is to provide an API. An API is an interface, with which developers can communicate with your application through other software they’ve written. This answer by itself is likely to invoke other questions. Questions regarding security for instance: Is this technology safe? Is it keeping all the bad people out? Can I restrict this functionality to certain users only? Questions regarding accessibility: How should I be presenting my data, and in what format should I accept it? And many other.\nThis series of posts gives an introduction about what an API does, who it is for, and how it is used. It highlights a few considerations or problems that will occur during the development process as well as the preferred solutions. (In most cases, these would be the industry standards)\n(We’re going to have to make the assumption that we’re indeed going to be developing an API for a system that stores, processes and delivers information. Systems that execute instructions (Like a robot, for instance) require a different form of API.)\nWhat does the API do?\nThe API will do exactly what was described in the introduction. It provides an interface for developers to communicate with your system/application. If we take a few well known applications, we know that for instance Twitter, Facebook have API’s, and Google even has a whole bunch of them.\nWhat these API’s provide is the opportunity to provide functionality that the original products don’t have by enthusiastic developers that are not employees of these companies. In the case of Facebook, imagine writing an application that delivers statistics on your friends list. Which of your friends comments the most on your posts and who just presses the like button a lot? Or maybe you want to automate posting a news flash on each of your social media, every time you publish a new article on your blog without having to do this manually? This is what API’s are ideal for. Extending the functionality and accessibility of your existing applications/products.\nWho will the API be for?\nThe people that will be working with your API will likely be software developers. These developers will likely have worked with other API’s too. In the ideal situation, you’d like for them to have little to no learning curve regarding working with your API. Therefore it is important to focus on standards and making the API as intuitive as possible.\nThere are multiple standards for creating an API. The more well known technologies are REST, SOAP and WSDL. Our focus will be on REST, as this is the most well known and most adopted variation.\nHow is the API used?\nHere it gets a little bit more technical. REST stands for Representational State Transfer. This means that every resource request, as well as the responses, represent the full state of the entity. These resources are likely to be objects from your application’s data model, or composites of it. In simple terms, a resource should be able to be represented as a noun. These could for instance be ‘cats’, ‘dogs’, or if you want to be more generic ‘animals’.\nThe resources are accessed or manipulated by sending HTTP requests to the server. These requests can be made using any application that has a form of HTTP client, for instance a browser like Chrome, Firefox or Edge. The request ‘methods’ define the ‘verb’ being applied to the resource and provide your basic CRUD functionality. It maps as follows:\nGET /cats– Reads/Provides the list of cats\nGET /cats/1– Reads/Provides the cat with ID 1\nPOST /cats– Creates a new cat\nPUT /cats/1– Updates the cat with ID 1\nDELETE /cats/1– Deletes the cat with ID 1\nThere is one other verb which is not always implemented, as it doesn’t represent a full state of an entity. This is the ‘PATCH’ method and is often ignored during implementation as it kind of doesn’t support the full REST principles.\nPATCH /cats/1– Partially updates the cat with ID 1\nIt is common practice that the names of the resources are pluralized, this avoids getting inconsistent names like\nGET /mice and\nWhat about security?\nYour system might contain personal user data, confidential data or otherwise sensitive data that you do not want to have accessible to everyone. The classic solution is to have a user account for each user that would restrict what he/she is eligible to see. This user information would be cached on the server in form of a ‘session’. By maintaining this session between the user and the server, the server knows what it is allowed to deliver back to the user and what not.\nFor the Web API, this scenario would not be applicable, but a very similar solution is available.\nInstead of these sessions, the Web API could work with tokens (For instance JWT tokens) .These tokens can be seen as a temporary claims of trust, that allow for an external application to communicate with the API of your system. These tokens typically contain the information of the application that is using the token, information regarding the user of the system, and some information regarding the validity of the token. (Example: The application is the previously mentioned Automated-Social-Media-Updater, the user is either your company’s Twitter or Facebook account and the validity would be 5 minutes since creation of the token.) In order to prevent the token from being tampered with, these tokens would also contain the claims in an encrypted format. The server can then compare the encrypted and the non-encrypted information, in order to determine what the user has access to.\nIs that all there is to it?\nThe short answer is ‘no’. There are many more aspects to developing a RESTful Web API, but these are the primary topics for an API from a product and/or business perspective. Future posts will contain a more in-depth approach on how to tackle the various challenges that will come up during the development of a Web API. The focus will then also shift to actual implementation, rather than a high level implementation.\nThis post will be updated with references to the follow-up posts once they are posted.\nDein KommentarAn Diskussion beteiligen?\nHinterlasse uns Deinen Kommentar!","REST Security Cheat Sheet¶\nREST (or REpresentational State Transfer) is an architectural style first described in Roy Fielding's Ph.D. dissertation on Architectural Styles and the Design of Network-based Software Architectures.\nIt evolved as Fielding wrote the HTTP/1.1 and URI specs and has been proven to be well-suited for developing distributed hypermedia applications. While REST is more widely applicable, it is most commonly used within the context of communicating with services via HTTP.\nThe key abstraction of information in REST is a resource. A REST API resource is identified by a URI, usually a HTTP URL. REST components use connectors to perform actions on a resource by using a representation to capture the current or intended state of the resource and transferring that representation.\nThe primary connector types are client and server, secondary connectors include cache, resolver and tunnel.\nREST APIs are stateless. Stateful APIs do not adhere to the REST architectural style. State in the REST acronym refers to the state of the resource which the API accesses, not the state of a session within which the API is called. While there may be good reasons for building a stateful API, it is important to realize that managing sessions is complex and difficult to do securely.\nStateful services are out of scope of this Cheat Sheet: Passing state from client to backend, while making the service technically stateless, is an anti-pattern that should also be avoided as it is prone to replay and impersonation attacks.\nIn order to implement flows with REST APIs, resources are typically created, read, updated and deleted. For example, an ecommerce site may offer methods to create an empty shopping cart, to add items to the cart and to check out the cart. Each of these REST calls is stateless and the endpoint should check whether the caller is authorized to perform the requested operation.\nAnother key feature of REST applications is the use of standard HTTP verbs and error codes in the pursuit or removing unnecessary variation among different services.\nAnother key feature of REST applications is the use of HATEOAS or Hypermedia As The Engine of Application State. This provides REST applications a self-documenting nature making it easier for developers to interact with a REST service without prior knowledge.\nSecure REST services must only provide HTTPS endpoints. This protects authentication credentials in transit, for example passwords, API keys or JSON Web Tokens. It also allows clients to authenticate the service and guarantees integrity of the transmitted data.\nSee the Transport Layer Protection Cheat Sheet for additional information.\nConsider the use of mutually authenticated client-side certificates to provide additional protection for highly privileged web services.\nNon-public REST services must perform access control at each API endpoint. Web services in monolithic applications implement this by means of user authentication, authorisation logic and session management. This has several drawbacks for modern architectures which compose multiple microservices following the RESTful style.\n- in order to minimize latency and reduce coupling between services, the access control decision should be taken locally by REST endpoints\n- user authentication should be centralised in a Identity Provider (IdP), which issues access tokens\nThere seems to be a convergence towards using JSON Web Tokens (JWT) as the format for security tokens. JWTs are JSON data structures containing a set of claims that can be used for access control decisions. A cryptographic signature or message authentication code (MAC) can be used to protect the integrity of the JWT.\n- Ensure JWTs are integrity protected by either a signature or a MAC. Do not allow the unsecured JWTs:\n- See here\n- In general, signatures should be preferred over MACs for integrity protection of JWTs.\nIf MACs are used for integrity protection, every service that is able to validate JWTs can also create new JWTs using the same key. This means that all services using the same key have to mutually trust each other. Another consequence of this is that a compromise of any service also compromises all other services sharing the same key. See here for additional information.\nThe relying party or token consumer validates a JWT by verifying its integrity and claims contained.\n- A relying party must verify the integrity of the JWT based on its own configuration or hard-coded logic. It must not rely on the information of the JWT header to select the verification algorithm. See here and here\nSome claims have been standardised and should be present in JWT used for access controls. At least the following of the standard claims should be verified:\nissor issuer - is this a trusted issuer? Is it the expected owner of the signing key?\naudor audience - is the relying party in the target audience for this JWT?\nexpor expiration time - is the current time before the end of the validity period of this token?\nnbfor not before time - is the current time after the start of the validity period of this token?\nAs JWTs contain details of the authenticated entity (user etc.) a disconnect can occur between the JWT and the current state of the users session, for example, if the session is terminated earlier than the expiration time due to an explicit logout or an idle timeout. When an explicit session termination event occurs, a digest or hash of any associated JWTs should be submitted to a blacklist on the API which will invalidate that JWT for any requests until the expiration of the token. See the JSON_Web_Token_for_Java_Cheat_Sheet for further details.\nPublic REST services without access control run the risk of being farmed leading to excessive bills for bandwidth or compute cycles. API keys can be used to mitigate this risk. They are also often used by organisation to monetize APIs; instead of blocking high-frequency calls, clients are given access in accordance to a purchased access plan.\nAPI keys can reduce the impact of denial-of-service attacks. However, when they are issued to third-party clients, they are relatively easy to compromise.\n- Require API keys for every request to the protected endpoint.\n429 Too Many RequestsHTTP response code if requests are coming in too quickly.\n- Revoke the API key if the client violates the usage agreement.\n- Do not rely exclusively on API keys to protect sensitive, critical or high-value resources.\nRestrict HTTP methods¶\n- Apply a whitelist of permitted HTTP Methods e.g.\n- Reject all requests not matching the whitelist with HTTP response code\n405 Method not allowed.\n- Make sure the caller is authorised to use the incoming HTTP method on the resource collection, action, and record\nIn Java EE in particular, this can be difficult to implement properly. See Bypassing Web Authentication and Authorization with HTTP Verb Tampering for an explanation of this common misconfiguration.\n- Do not trust input parameters/objects.\n- Validate input: length / range / format and type.\n- Achieve an implicit input validation by using strong types like numbers, booleans, dates, times or fixed data ranges in API parameters.\n- Constrain string inputs with regexps.\n- Reject unexpected/illegal content.\n- Make use of validation/sanitation libraries or frameworks in your specific language.\n- Define an appropriate request size limit and reject requests exceeding the limit with HTTP response status 413 Request Entity Too Large.\n- Consider logging input validation failures. Assume that someone who is performing hundreds of failed input validations per second is up to no good.\n- Have a look at input validation cheat sheet for comprehensive explanation.\n- Use a secure parser for parsing the incoming messages. If you are using XML, make sure to use a parser that is not vulnerable to XXE and similar attacks.\nValidate content types¶\nA REST request or response body should match the intended content type in the header. Otherwise this could cause misinterpretation at the consumer/producer side and lead to code injection/execution.\n- Document all supported content types in your API.\nValidate request content types¶\n- Reject requests containing unexpected or missing content type headers with HTTP response status\n415 Unsupported Media Type.\n- For XML content types ensure appropriate XML parser hardening, see the XXE cheat sheet.\n- Avoid accidentally exposing unintended content types by explicitly defining content types e.g. Jersey (Java)\n@consumes(\"application/json\"); @produces(\"application/json\"). This avoids XXE-attack vectors for example.\nSend safe response content types¶\nIt is common for REST services to allow multiple response types (e.g.\napplication/json, and the client specifies the preferred order of response types by the Accept header in the request.\n- Do NOT simply copy the\nAcceptheader to the\nContent-typeheader of the response.\n- Reject the request (ideally with a\n406 Not Acceptableresponse) if the\nAcceptheader does not specifically contain one of the allowable types.\n- Ensure sending intended content type headers in your response matching your body content e.g.\n- Avoid exposing management endpoints via Internet.\n- If management endpoints must be accessible via the Internet, make sure that users must use a strong authentication mechanism, e.g. multi-factor.\n- Expose management endpoints via different HTTP ports or hosts preferably on a different NIC and restricted subnet.\n- Restrict access to these endpoints by firewall rules or use of access control lists.\n- Respond with generic error messages - avoid revealing details of the failure unnecessarily.\n- Do not pass technical details (e.g. call stacks or other internal hints) to the client.\n- Write audit logs before and after security related events.\n- Consider logging token validation errors in order to detect attacks.\n- Take care of log injection attacks by sanitising log data beforehand.\nThere are a number of security related headers that can be returned in the HTTP responses to instruct browsers to act in specific ways. However, some of these headers are intended to be used with HTML responses, and as such may provide little or no security benefits on an API that does not return HTML.\nThe following headers should be included in all API responses:\n||Prevent sensitive information from being cached.|\n||To protect against drag-and-drop style clickjacking attacks.|\n||To specify the content type of the response. This should be\n||To require connections over HTTPS and to protect against spoofed certificates.|\n||To prevent browsers from performing MIME sniffing, and inappropriately interpreting responses as HTML.|\n||To protect against drag-and-drop style clickjacking attacks.|\nThe headers below are only intended to provide additional security when responses are rendered as HTML. As such, if the API will never return HTML in responses, then these headers may not be necessary. However, if there is any uncertainty about the function of the headers, or the types of information that the API returns (or may return in future), then it is recommended to include them as part of a defence-in-depth approach.\n||The majority of CSP functionality only affects pages rendered as HTML.|\n||Feature policies only affect pages rendered as HTML.|\n||Non-HTML responses should not trigger additional requests.|\n- Disable CORS headers if cross-domain calls are not supported/expected.\n- Be as specific as possible and as general as necessary when setting the origins of cross-domain calls.\nSensitive information in HTTP requests¶\nRESTful web services should be careful to prevent leaking credentials. Passwords, security tokens, and API keys should not appear in the URL, as this can be captured in web server logs, which makes them intrinsically valuable.\nPUTrequests sensitive data should be transferred in the request body or request headers.\nGETrequests sensitive data should be transferred in an HTTP Header.\nhttps://example.com/controller/123/action?apiKey=a53f435643de32 because API Key is into the URL.\nHTTP Return Code¶\nHTTP defines status code. When designing REST API, don't just use\n200 for success or\n404 for error. Always use the semantically appropriate status code for the response.\nHere is a non-exhaustive selection of security related REST API status codes. Use it to ensure you return the correct code.\n|200||OK||Response to a successful REST API action. The HTTP method can be GET, POST, PUT, PATCH or DELETE.|\n|201||Created||The request has been fulfilled and resource created. A URI for the created resource is returned in the Location header.|\n|202||Accepted||The request has been accepted for processing, but processing is not yet complete.|\n|301||Moved Permanently||Permanent redirection.|\n|304||Not Modified||Caching related response that returned when the client has the same copy of the resource as the server.|\n|307||Temporary Redirect||Temporary redirection of resource.|\n|400||Bad Request||The request is malformed, such as message body format error.|\n|401||Unauthorized||Wrong or no authentication ID/password provided.|\n|403||Forbidden||It's used when the authentication succeeded but authenticated user doesn't have permission to the request resource.|\n|404||Not Found||When a non-existent resource is requested.|\n|405||Method Not Acceptable||The error for an unexpected HTTP method. For example, the REST API is expecting HTTP GET, but HTTP PUT is used.|\n|406||Unacceptable||The client presented a content type in the Accept header which is not supported by the server API.|\n|413||Payload too large||Use it to signal that the request size exceeded the given limit e.g. regarding file uploads.|\n|415||Unsupported Media Type||The requested content type is not supported by the REST service.|\n|429||Too Many Requests||The error is used when there may be DOS attack detected or the request is rejected due to rate limiting.|\n|500||Internal Server Error||An unexpected condition prevented the server from fulfilling the request. Be aware that the response should not reveal internal information that helps an attacker, e.g. detailed error messages or stack traces.|\n|501||Not Implemented||The REST service does not implement the requested operation yet.|\n|503||Service Unavailable||The REST service is temporarily unable to process the request. Used to inform the client it should retry at a later time.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:99ee2723-bc09-43c5-a66f-9e9277102878>","<urn:uuid:813a04e3-15be-44bd-8a3d-77bf21230878>"],"error":null}
{"question":"How does the Louisiana museum's architectural expansion history connect to artwork preservation requirements?","answer":"The Louisiana museum expanded from its 1958 opening through several wings (North, West, South, East, and Children's), with each addition maintaining the original architectural vision while addressing specific artwork needs. The South Wing provided higher ceilings and optimal daylight, while the underground East Wing (Graphics Wing) was specifically designed to protect light-sensitive works like drawings and photographs. This specialized design aligns with conservation requirements, as different materials require specific climate conditions - works on paper, wood, and canvas are particularly sensitive to humidity fluctuations, which can cause warping and damage. The museum underwent modernization in 2003-2006 to ensure proper climate control while maintaining its distinctive architectural character.","context":["The Louisiana Architecture\nLouisiana is considered a major work of Danish modernist architecture. In the well-balanced style of the late 1950s’discreet modernism, the museum presents itslef as a horizontal and understated building complex that fits gracefully and intimately into the landscape.\nIt is precisely the unpretentious aspect of Louisiana’s architecture that strikes the eye on the first visit. In the mid-50s, when the museum’s founder, Knud W. Jensen, asked the architects Jørgen Bo and Wilhlem Wohlert to build a museum based on the old villa, their basic conception was to link the architecture with the natural surroundings.\nLouisiana has expanded considerably since opening in 1958. All seven extensions and alterations have been carried out by the architects Bo and Wohlert – supplemented by architect Claus Wohlert on the projects from the East Wing onwards. It has been possible to maintain the original plan and fit the gradually larger Louisiana perfectly into the terrain, the trees, the lawns and the rest. The museum thus still appears an integrated whole in which the interplay among architecture, the Park and nature create a special resonance for Louisiana visitors.\n1958 NORTH WING\nWhen Louisiana first opened to the public in 1958, it consisted of several glass corridors and the three pavilions that connected the old villa to the cafeteria with a view of Sweden. This section, which includes the Giacometti Hall and the Jorn Hall, is designated as the North Wing.\n1966-76 WEST WING AND CONCERT HALL\nLouisiana quickly outgrew its framework. As early as in 1966 and 1971, the museum was expanded with the two floors of the West Wing. In 1976 came the Concert Hall, whose acoustics and atmosphere make it particularly suitable for chamber music. The Concert Hall has also always been the place where Louisiana has invited the public to debates, lectures and other events. The seats in the Concert Hall were designed by Poul Kjaerholm, and the end walls are decorated with paintings created for the space by Sam Francis.\n1982 SOUTH WING\nWith the South Wing, Louisiana added an exhibition room with a higher ceiling and more space than in the previously existing buildings. The high space highlights the qualities of the artwork, giving it plenty of surrounding space and optimal daylight. The South Wing is built into the terrain in order to maintain Louisiana’s low profile.\n1991 EAST WING (GRAPHICS WING)\nWith the construction of the East Wing, which was completed in 1991, the museum buildings became connected in a roughly circular form. The floor plan was improved significantly, and now you could walk all the way around on a route where you sometimes concentrate on the artwork and sometimes shift over to a view of the Park or the Sound. The underground East Wing is also referred to as the Graphics Wing because it gives the opportunity to exhibit drawings and graphics that must not be exposed to daylight. For the same reason, the wing is often used for exhibitions of photographic, video and light artwork. The East Wing leads to the Great Hall, which is located beneath the Calder Terrace.\n1994-98 CHILDREN’S WING AND SHOP\nWith the Children’s Wing, from 1994, Louisiana acquired a unique setting for the museum’s activities for children and young people. In 1998 came an extension to improve the museum’s visitor facilities. At the same time, better space was allocated to the Louisiana Shop.\nLouisiana’s architecture is known for its discrete pavilions and semi-transparent glass corridors. The windows that separate the museum’s interior from the outdoors, however, place enormous demands on security and climate control – to an extent that could not have been foreseen at the museum’s founding. In order to continue to introduce the public to the very best contemporary artwork, it is essential that the museum meets the most stringent applicable standards and requirements. The museum therefore carried out, from 2003 to 2006, a comprehensive modernization. Without disturbing the buildings’ aesthetic integrity and ease, this project provided Louisiana with all the relevant technology and ensured that it would thrive as a meeting place for people and art well into the 21st century. The modernization was completed with a ten-figure amount provided by both private funds and an extraordinary grant from the Danish Ministry of Culture.\nJØRGEN BO & VILHELM WOHLERT\nWhen Knud W. Jensen took over the property in 1955, he imagined at first that the new museum would consist of the Old Villa and a separate exhibition pavilion at the edge of the cliff facing the Sound. But in his collaboration with the two architects Jørgen Bo and Vilhelm Wohlert, they conceived of a plan in which the buildings would be connected in such a way that visitors would feel as though they were on a \"covered stroll\" through the park.\n“It was an ideal location for a museum,” wrote Knud W. Jensen upon the museum’s fortieth anniversary,” but the lot made its own demands and became in a sense our employer, making the final decisions about where the buildings should stand and where the sculptures should be placed.”\nThe construction began in mid-1956 and lasted until the museum's opening in 1958. Bo and Wohlert’s original design idiom featured long whitewashed walls, exposed structures, laminated wooden ceilings and deep-red tiled floors. And then of course the large glass panels that open into the surroundings and contribute to a unique architectural lightness.\nThe two architects took their inspiration from both sides of the Pacific. Wohlert had studied at University of California at Berkeley, where he had the opportunity to become acquainted with the so-called Bay Area architecture of the wooden houses surrounding the San Francisco Bay. But Louisiana also has clear references to the traditional simplicity of Japanese building style, which the architects succeeded in transplanting elegantly in a Danish setting. It has been said that from the beginning two factors were critical to Louisiana's architecture: coherence and gentleness.\nJØRGEN BO (1919-99). Graduated from the School of Architecture at the Royal Danish Academy of Fine Arts in 1941, where he was appointed professor in 1960. In addition to the long-standing collaboration on the construction and expansion of Louisiana, he also designed with Vilhelm Wohlert the Danish Embassy in Brazil (1972-74) and the Art Museum in Bochum, Germany (1978-83), among many other buildings. With Karen and Ebbe Clemmensen, he built the Blågård Seminary in Gladsaxe, Denmark (1962-66), and with Anders Hegelund as associate architect, IBM's Danish headquarters in Lundtofte (1970-72). Jørgen Bo received many honors, including the Eckersberg Medal in 1959 and the C.F. Hansen Medal in 1983.\nVilhelm Wohlert (1920-2007). Studied at the School of Architecture at the Royal Danish Academy of Fine Arts under Kaare Klint. Graduated in 1944; professor of architecture from 1968 to 1986. As an independent architect, he completed a wide variety of assignments, including the renovation of the Ny Carlsberg Glyptotek museum (with Viggo Sten Moller, 1956-58). Afterwards, he became the permanent architect for Ny Carlsberg Foundation. In addition to their collaboration on Louisiana with Jørgen Bo, the two partners also built Kirstine Park in Hørsholm and Piniehøj in Rungsted. Wohlert designed several churches and – with the Exners – also Frederik IX’s burial site at Roskilde Cathedral. He played a key role in efforts to preserve traditional Danish building culture, and he managed numerous restorations, including Christian VII's palace at Amalienborg and the Copenhagen Cathedral. Wohlert was awarded the Eckersberg Medal and Træprisen (for Louisiana), both in 1958, and the C.F. Hansen Medal in 1979, among other honors.\nJEAN NOUVEL ON LOUISIANA\nIn 2005 french starchitect Jean Nouvel and the Louisiana Museum came together to collaborate on the exhibition 'Jean Nouvel - Louisiana Manifesto'.\nTen years on, Nouvel has revisited the museum and shares his thoughts and passion about a place where architecture, nature and art belong together. \"At Louisiana,\" says Nouvel, \"each thing is directly felt, and everything is at home\".","Museums around the world include a broad range of objects with widely differing ages: dinosaur bones, stone-age flutes made of mammoth tusks, sensitive photographs, paintings with thick layers of paint, contemporary sculptures. In all cases, the building has to maintain and protect the objects displayed and stored inside.\nExcessively dry air\nDry air absorbs humidity from objects, their weight is reduced and they contract. In the case of humid air, it is the other way round. Climatic fluctuations thus keep the objects in permanent movement and sooner or later a crack appears on the canvas or the color gilding chips off the baroque sculpture. Stabilization of the relative air humidity helps avoid tension in the material texture of the exhibits, the Building Climate Institute emphasizes.\nThe preservation of enshrined cultural artifacts generally requires a constant indoor climate which is defined within relatively tight limits . This climate has to be technically created. The air requires humidification — at least periodically. The values reached are measured using measuring systems. Nowadays, due mainly to lease agreements, international indoor climate values of between 50 and 55 ±5% RH and 20°C are required . The American ASHRAE standard formulates corridors for the indoor climate in even greater detail — from the narrowest AA to D. The narrowest climate corridor specifies RH = ±5% and T = ±5 K as long-term tolerance with seasonal adjustment. Positive from a conservational viewpoint is that there is a slow, seasonal adjustment of the indoor climate to the outdoor climate which lies within these limits.\nThe external climate and the relative air humidity show significant seasonal fluctuations. In winter, the RH is sometimes extremely low. In summer during rainfall, 100%. The external space and inner areas are more or less closely related at all times. This means that a change of the external climate is also noticeable indoors and can be even more pronounced there. Especially short-lived fluctuations of the indoor climate are harmful in the long term. Therefore, a change of the RH during one day may not be allowed to exceed 5%. During one hour, the fluctuations have to be below 2.5%. Basically, a change should be as minor as possible, while the frequency of fluctuations should be kept as low as possible . For particularly sensitive exhibits, there are special display cases. They may be damp-proof only, equipped with humidity regulation, or even fully air conditioned.\nSensitive wood products\nEach material has specific demands on its ambient climate. Metal, stone, canvas, oil, wood, leather, paper or ivory react differently to humidity and temperature fluctuations. Works on paper, wood, canvas or parchment are among the most sensitive objects. The main raw materials of our papers are plant fibers, textile fibers and wood pulp. These are strongly hygroscopic materials. By absorbing indoor humidity and releasing material humidity, they follow all humidity fluctuations in the environment. These exchange processes require the expansion or contraction of the material through a change of dimensions of the wood cells.\nThis is expressed by warping of parchment or paper, or by tears or bubbles on panel and canvas paintings or on color-gilded sculptures. On papers, humidity fluctuations lead to a displacement of soluble components such as the ink. Specialist terms here are ink corrosion and copper corrosion.\nFabrics, photographs, metal and stone\nIn textile objects, excessively low air humidity advances the fragility of the tissue. In photographic objects, substrates and binding agents become fragile and brittle in environments with low RH. Comfortable in a climate of between 20 and 60% RH, stone and ceramic can tolerate a low air humidity.\n- Anderson Art Gallery\n- Canadian Museum of HIstory\n- Guggenheim Museum\n- Harvard FOGG Art Museum\n- Milwaukee Public Museum\n- Royal Ontario Museum\n- San Francisco Museum of Modern Art\n- Smithsonian Museum"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7908ae4b-5999-458d-8496-a06e3b8d9f58>","<urn:uuid:1ae26b49-f1f7-4bd6-aa60-84ecab652c5e>"],"error":null}
{"question":"What's the connection between home renovation techniques and indoor air pollutants, particularly regarding wallpaper removal decisions?","answer":"When considering wallpaper removal, it's important to note that while stripping wallpaper is extremely difficult and time-consuming (taking days or weeks for even a small room), painting over it can be a viable alternative requiring repair of damaged areas and proper sealing. However, this renovation decision must be balanced against indoor air quality concerns. The paints and materials used in such projects can release Volatile Organic Compounds (VOCs), which are common indoor pollutants. VOCs from household products like paints can cause various health issues ranging from mild irritation to serious conditions affecting the kidneys and central nervous system. To minimize these risks, proper ventilation is crucial, and using source control methods such as correct product usage and storage is essential.","context":["Tips For Painting Over Wallpaper\nAfter years of living in your home, you might want to\nchange the way it looks. Over time, people get bored with\ntheir environment and because of this you may want to make\na few changes in your surroundings. To do this, you can\nconsider starting by getting rid of your old wallpaper.\nHowever, if you removed wallpaper before, you know how\ndifficult it is to remove it. Even with the chemicals\nadvertised on TV doesn’t work as well as it is supposed to\nbe. It is a fact that stripping down wallpaper is a very\nhard task to do. Even a small room can take you days or\neven weeks just to remove the wallpaper. So, how can you\nchange the look of your room without going through the\nlaborious process of stripping down your old wallpaper\nfirst and painting new color for your room?\nTo do this, you can simply paint over your old wallpaper.\nAlthough painting over your wallpaper can still require\nyou to work a little harder, it is not as hard as peeling\noff your old wallpaper first before painting. Old\nwallpapers tend to peel and will have some damaged areas,\nsuch as peeled off parts, ripped parts and parts where it\ncame loose and have air bubbles. You first need to deal\nwith these damages by repairing it. By not repairing it,\nyou will notice that there will be uneven parts that can\nbe very ugly to look at. What you want is a great looking\nwall where there are no signs of wallpaper under the\nSo, the first thing you need to do is repair the damaged\nareas. You can glue any of the loose section. However, if\na particular section of your wallpaper has a lot of damage\nand one that is considered to be irreparable, you have to\nstrip down this wallpaper section. But if the wallpaper\nsection only contains minimal damages, you may want to\nrepair it rather than stripping it down or peel it off.\nThe next step is by mixing some joint compound and\napplying it on the seams. This will provide a great fix\nfor peeled off wallpapers and will prevent the wallpapers\nfrom getting peeled off again.\nThere are also some textured wallpapers. If you have\ntextured wallpaper installed, you should apply the joint\ncompound mixture on the whole wall. Let the joint compound\ndry and the next step is to get rid of the rough surface\nby sanding it.\nIt is also very important to consider what color your old\nwallpaper has. If it is light in color, you can paint on\nyour wallpaper immediately. However, if you have dark\ncolored wallpaper, you first need to apply a sealer. This\nwill keep the wallpaper color from getting visible through\nThe next step is painting. Choose high quality house\npaints, prepare the room for painting by taping windows,\nputting some old newspapers on the floor to prevent\ngetting any paint on the floor and on your windows.\nAs you can see, it is quite simple to paint over your\nexisting wallpapers. You don’t need to peel off your\nwallpaper first, you just need to remember the few easy\nsteps, which are:\nRepairing damaged areas\nApplying joint compound\nApplying sealant (For dark colored wallpapers)\nThese are the tips you should remember if you want to\npaint over your old wallpaper. Home improvement is easier\nby following these few easy tips.\nPainting tips & tricks of\nWhen painting a room, dip a small card into the paint so that you\nhave the exact colour with you and can match accessories in store.\nWhen painting inside corners, trim the paint brush bristles to a V\nto save strokes and spread paint more easily.\nWhen you poke a paint brush into corners or allow it to rest on the\nbottom of the paint can, the bristles curl and stray. To straighten\nnatural bristles (not synthetics), try wrapping the brush in a\ncouple of thicknesses of damp cloth and press gently with an iron.\nThe steam and cloth binding do the job. Only light pressure is\nneeded. Let the bristles cool before you unwrap the brush.","The quality of our indoor air becomes more important as people spend increasing amounts of time indoors. Adverse health effects from poor IAQ can be mild and irritating (runny nose, headache), or they can be much more serious (asthma attack). If you suspect that the IAQ in your home is poor, the first step is to try and identify the possible source of the pollutant.\nCommon Indoor Air Quality Pollutants\nVolatile Organic Compounds (VOC's)\nEnvironmental Tobacco Smoke (ETS)\nMolds (fungi), bacteria, and dust mites are some of the main biological pollutants indoors. These are generally airborne particulates that can be inhaled and cause an allergic-type reaction, such as nasal congestion, sneezing, watery eyes, and a runny nose. They can also trigger asthma attacks. Molds and bacteria are often found in wet areas of the home, where there is excess humidity or a water leakage has occurred. For information on cleaning up a mold problem, refer to this Fact Sheet.\nThe Breathe Easies Videos\nVolatile Organic Compounds (VOC's)Carbon Monoxide\nThe term Volatile Organic Compounds (VOC's) encompasses many different chemicals which can come from a variety of different sources indoors. These sources can include household cleaners, paints, solvents, or other chemicals found in the home or garage. For these types of VOC’s, proper usage and storage should be taken to minimize your exposure. Typically, these compounds have an odor associated with them, making them easier to detect.\nFormaldehyde is a very common VOC because of its widespread use as a preservative in a variety of products such as paneling, particle board, furniture, and carpeting. Formaldehyde can take up to two years to completely off-gas in some cases, so residents of a newly built or remodeled home may notice some symptoms. Exposure can cause symptoms such as eye, nose and throat irritation, skin rashes, couching, headache, nausea, and sever allergic reactions. Some types of VOC's can cause damage to the kidneys and central nervous system, and others have been linked to cancer.\n^ Top of page\nCarbon Monoxide is the most dangerous of the potential IAQ pollutants, as it can be fatal in high levels. At lower levels it can cause headaches, dizziness, nausea, confusion, and disorientation. Other combustion by-products (gases or particles that come from burning fuels) can damage the respiratory tract and cause eye, nose, and throat irritation. Any appliances that burn fuels can introduce combustion by-products. These appliances should be properly installed (particularly vented to the outside) and maintained. Such appliances can include furnaces, boilers, fireplaces, space heaters, and automobiles.\nALERT- Put generators outside: Never use a generator inside homes, garages, crawlspaces, sheds, or similar areas. Deadly levels of Carbon Monoxide can quickly build up in these areas and can linger for hours, even after the generator has shut off.\nThese inhalable particles can range in size, and be organic/biological (mold, pollen, pet dander, skin flakes) or inorganic (wood, carpet fibers, byproducts from fuel combustion or tobacco smoke). Typically, a whole house filter can help keep levels down if it is properly maintained. For sensitive individuals (e.g., asthmatics or those with severe allergies), additional steps may be needed to control exposure. Health symptoms are typically an allergic reaction, such as nasal congestion, sneezing, watery eyes and runny nose. They can also trigger an asthma attack.\nStand-alone air purifiers can help with additional filtration if they use a filter, similar to a furnace. They will only work for a small area, and the filter needs to be changed on a regularly. The 'ionizing' or 'electrostatic' purifiers can create ozone, itself a powerful lung irritant, and are not recommended.\nIn most cases, it isn’t necessary to test. Knowing the sources of common pollutants, as well as the symptoms they cause, can often help a resident diagnose a suspected problem. Often, a thorough visual inspection can help as well. While there are companies in Nebraska that will offer a variety of tests, it can be very expensive, and at times, inconclusive.\nFor example, knowing a home has a high level of VOC’s may not help to determine the source for removal. Also, having a mold spore count for your home doesn’t tell you if those numbers are elevated or not, as there is not currently a state or federal standard.\nThere are some instances where testing can be beneficial, but they are limited. Radon or Carbon Monoxide testing is necessary to know if those toxins are at dangerous levels or not. If a legal action has been initiated, or if negotiating a claim with an insurance company, testing may be necessary.\nImproving Your Home's Indoor Air Quality (IAQ)\nThere are ways to improve your IAQ. They typically fall into one of three categories: Source Control, Ventilation, or Preventative Maintenance.\nPerhaps the best way to keep good IAQ is to control the sources of pollutants. Ensure that chemicals are correctly stored and used to minimize potential health problems. Use products in a well-ventilated area, and only for their intended use. Also, practice preventative maintenance (see below) to help reduce the need for such products.\nThere's a reason why 'Dilution is the Solution to Pollution'. One way to keep toxin levels low is to ventilate your home, either by 'general ventilation', which is removing toxins from a larger area, or by 'spot ventilation', which is removing toxins from a localized area.\nBringing in fresh air into a home for general ventilation is difficult to maintain year-round in Nebraska. With energy costs high and on the rise, homes are being constructed air-tight. During heating or cooling seasons when a house is closed up, the air can quickly become stale, and toxins can build-up. Most homes do not have an active way to bring in fresh air, meaning that little or no fresh air comes in, unless the windows are open.\nMany homes have 'spot ventilation', meaning fans that exhaust air from specific places, such as bathrooms, oven hoods, or clothes dryers. This type of ventilation has fans turned on only when the pollutant is being produced.\nWhen radon gas is a problem in the home, a permanently installed radon mitigation system is the answer, something a licensed professional can help you with. An added benefit of a radon mitigation system can be reduced moisture levels in the home. Visit the Nebraska Radon Program's homepage or call us for more information.\nPracticing preventative maintenance can keep your indoor air clean and healthy. Examples of preventative maintenance are to keep combustion equipment inspected on a yearly basis, and install a carbon monoxide detector. Quickly attend to spills, leaks, and stains to prevent mold. Keep your home clean and clutter free. Remove food waste to control dust, dander, and pets. Change your furnace filter on a regular basis (at least quarterly, if not more often) to control airborne particulates. Conduct a radon test and install a radon mitigation system, if necessary. Do not allow smoking in your home or vehicle. Keep chemicals stored properly. For example, knowing a home has a high level of VOC’s may not help to determine the source for removal. Also, having a mold spore count for your home doesn’t tell you if those numbers are elevated or not, as there is not currently a state or federal standard.\nDHHS- Indoor Air Quality Program\nPO Box 95026, Lincoln, NE 68509-5026\nDocuments in PDF format require the use of Adobe Acrobat Reader\nwhich can be downloaded for free from Adobe Systems, Inc.\nEnvironmental Health Page\nPublic Health Page"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ca3ed931-600c-43f6-abf9-df796ef194c7>","<urn:uuid:938eafea-9f7b-4ea8-b5f7-62fe027020d5>"],"error":null}
{"question":"My dog has chronic pain. What are the Five Animals exercise techniques from ancient China, and how can modern veterinary acupuncture help with pain management?","answer":"The Five Animals exercise techniques originate from ancient China and include specific movements based on the tiger (associated with liver/wood element), deer (kidneys/water element), bear (spleen/earth element), monkey (heart/fire element), and crane (lungs/metal element). Each animal movement corresponds to specific organs and elements in Traditional Chinese Medicine. For modern pain management, veterinary acupuncture offers effective treatment by stimulating points along meridians (energy channels) in the body. Acupuncture activates sensory nerves that signal the brain to release chemicals producing multiple physiologic effects. It can be administered through various techniques including dry needle, moxibustion, aquapuncture, electroacupuncture, laser acupuncture, and gold wire implantation. When combined with herbal medicine, the treatment effects are stronger and results can be obtained more quickly.","context":["|This article needs additional citations for verification. (July 2014)|\n|Part of a series on|\n|Chinese martial arts (Wushu)|\n|List of Chinese martial arts|\n|Wushu in the world|\nIn the Chinese martial arts, imagery of the Five Animals (Chinese: 五形; pinyin: wǔ xíng; literally: \"Five Forms\")—Tiger, Crane, Leopard, Snake, and Dragon—appears predominantly in Southern styles, especially those associated with Guangdong and Fujian Provinces. An alternate selection which is also widely used is the crane, the tiger, the monkey, the snake, and the mantis.1\nThe Five Animal martial arts supposedly originated from the Henan Shaolin Temple, which is north of the Yangtze River, even though imagery of these particular five animals as a distinct set (i.e. in the absence of other animals such as the horse or the monkey as in T'ai chi ch'uan or Xíngyìquán) is either rare in Northern Shaolin martial arts—and Northern Chinese martial arts in general—or recent (cf. wǔxíngbāfǎquán; 五形八法拳; \"Five Form Eight Method Fist\").2\nIn Mandarin, \"wǔxíng\" is the pronunciation not only of \"Five Animals,\" but also of \"Five Elements,\" the core techniques of Xíngyìquán, which also features animal mimicry (but of 10 or 12 animals rather than 5) and, with its high narrow Sāntǐshì (三體勢) stance, looks nothing so much like a Fujianese Southern style stranded in the North.\nAlthough the technique is mainly associated with the tiger, dragon, snake, crane and leopard, many other animal styles have been developed, including panther, praying mantis (northern and southern styles), horse, cobra, bull, wolf, deer, bear, boar, eagle, python, scorpion, elephant, lion, frog, duck, dog, crow, tiger cub, chicken, hawk, turtle, swallow, lizard and a host of others.\nAccording to legend,citation needed Jueyuan, a 13th-century Shaolin martial artist, used the original 18 Luohan Hands as a foundation, expanding its 18 techniques into 72. In Gansu Province in the west of China, in the city of Lanzhou, he met Li Sou, a master of \"Red Fist\" Hóngquán (紅拳). Li Sou accompanied Jueyuan back to Henan, to Luoyang to introduce Jueyuan to Bai Yufeng, master of an internal method.\nThey returned to Shaolin with Bai Yufeng and expanded Jueyuan's 72 techniques to approximately 170. Using their combined knowledge, they restored internal aspects to Shaolin boxing. They organized these techniques into Five Animals: the Tiger, the Crane, the Leopard, the Snake and the Dragon.3\nJueyuan is also credited with the Northern style \"Flood Fist\" Hóngquán (洪拳), which does not feature the Five Animals but is written with the same characters as the Southern style Hung Kuen, perhaps the quintessential Five Animals style.\nMoreover, as in the Southern Hung Kuen, the \"Hóng\" character (洪) in Hóngquán actually refers to a family name rather than its literal meaning of \"flood.\" However, the two styles have nothing in common beyond their shared name.\nThe \"Five Animal play\" (五禽戲, Wu Qin Xi) are a set of qigong exercises developed during the glorious and peaceful Han Dynasty (202 B.C.E - 220 C.E.). Some claim the author of this Qi Gong sequence to be Huatuo, however Yang Jwing-Ming suggests it was the Taoist Master Jiun Chiam and Huatuo merely perfected its application and passed it onto gifted disciples including Wu Pu, Fan E, and Li Dangzhi.4\nThe five animals in the exercises are the tiger, deer, bear, monkey and crane. According to TCM theory of Wu Xing (Five Elements), each animal has two exercises corresponding to the five yin (Zang) and five yang (Fu) internal organs. Regular practise of this Qi Gong is said to improve functioning of the Liver/Gall Bladder (Wood Element - tiger), Kidneys/Bladder (Water Element - deer), Spleen/Stomach (Earth Element - bear), Heart/Small Intestine (Fire Element - monkey) and Lung/Large Intestine (Metal Element - crane) respectively.5\nThe first animal is Tiger. It relates to the wood element, the season of spring and therefore the liver and gall bladder. The liver's emotion is anger and has many important functions including storing blood (xue), ensuring the smooth movement of qi in the body and housing the ethereal soul (hun). Liver xue nourishes the sinews therefore allowing physical exercise. The liver is often compared to an army general because it is 'responsible' for the smooth flow of qi, essential to all physiological processes of every organ and part of the body. The hun provides the mind (shen) with inspiration, creativity and a sense of direction in life.6:117 The emphasis of this exercise is grasping and stretching. By reaching up to bring down Heaven and reaching down to draw up Earth grasping is encouraged; which relates to sinews and therefore the liver. Rolling through the spine stimulates both yin and yang of ren mai and du mai channels activating the microcosmic orbit. This is then completed with a slow 'stalking' forward bend and sudden shout (release of anger) as the Tiger catches its prey with vigor while standing on one leg, to stretch the sinews while activating the jing well points at the tips of the fingers, opening PC-8 and incorporating another important Wood trait - balance.5\nThe second animal is Deer. It relates to the water element, the season of winter and therefore the kidneys and bladder. The kidneys emotion is fear and is often referred to as the 'root of life' as they store essence (jing). Jing determines basic constitution, is derived by our parents and established at conception. The kidneys are the foundation of yin and yang in the body and therefore every other organ and governs birth, growth, reproduction and development. They also produce marrow, control bones and the Gate of Life (Minister Fire), while housing willpower (zhi).6:153 By twisting the torso, the energy of one kidney is opened while the other is closed creating a pump to regulate chong mai and therefore yuan qi. Fire (heart) and water (kidney) must connect energetically to maintain health. The hand gesture replicating horns calms Shen and connects with the heart by keeping the middle fingers in touch with the palms via the pericardium and san jiao channels. The eyes are the 'window' to shen and are smiling and joyous as we turn to look at the back heel and medial malleolus (kidney channel), also connecting fire with water. In the second part, hunching the back activates the microcosmic orbit again and the gentle skip exemplifies the Deer's playful and fluid nature.5\nThe third animal is Bear. It relates to the earth element, the season of late summer and therefore the spleen and stomach. The spleens emotion is worry and is the central organ in the production of gu qi; from the food and drink we ingest. The spleen's transformation and transportation of gu qi is paramount in the process of digestion which is the basis for the formation of qi and xue. The spleen is where the intellect (yi) is said to reside and is responsible for applied thinking and the generating of ideas, memorising and concentration.6:143 This animal is cumbersome and its awkward traits are expressed in each movement. It starts off with circular abdominal massage to aid digestion by warming and supporting spleen yang, using the entire upper torso to move the hands. The arms are then poised to open and stretch the armpit activating the spleen's close relationship with Heart and Liver (Heart is the 'mother' of spleen and liver stores xue) by stretching the flanks. The palms are empty to open PC-8 as the hip is raised to shift the leg forward while keeping the knee straight. The swinging torso and heavy step activates kidney yang to supports spleen yang in heating and 'cooking' food.5\nThe fourth animal is Monkey. It relates to the fire element, the season of summer and therefore the heart and small intestine. The heart is considered the most important and therefore the 'emperor' of the internal organs. It relates to the emotion joy and its main function is to govern and circulate xue in the vessels to nourish tissues and house the mind (Shen). Shen is used to indicate the entire sphere of mental and spiritual aspects of a human being and therefore encompasses hun, zhi, yi and corporeal soul (po).6:107–109 Similar to the heart, the monkey is forever moving like the flickering of a flame. With the first exercise, suddenly lifting the hands with hook palms up towards the chest, the shoulders towards the ears and balancing on the toes with the monkey looking to the side, squeezes the heart and pumps xue as you release down again. The second part calms Shen by clearing the mind (moving the branch) to grasp the peach (fruit of heaven) with the thumb inside the palm to hold the Hun within. Grasping in this exercise relates to the liver's ability to hold and store xue, while the lifting of the back heel activates the Kidneys also supporting the Heart. The peach is then brought into view but is too heavy and must be supported as the monkey enjoys his find and soon to be 'treat'.5\nThe fifth animal is Crane. It relates to the metal element and the season of autumn and therefore the lungs and large intestine. The Lungs emotion is sadness and governs qi and respiration, while being in charge of inhalation and the regulation of water passages. They are the intermediary organ between man and his environment, likened to a prime minister in charge of qi regulation particularly in the blood vessels to assist the heart in controlling blood circulation. The lungs house po the most physical and material part of the human soul; sensations and feelings.6:129 The activation of the microcosmic orbit is again featured by firstly working the spine in a concave fashion. The shoulders are raised and squeezed into the neck to squeeze the heart and pump xue while the arms are brought up to mimic a beak and the tailbone is thrust out. The arms are brought back along with one leg to mimic gliding. The second part of the exercise regulates the ascending (liver - xue) and descending (lungs - qi) function of qi in the Lungs. The ultimate yin and yang expressed by breathing in (kidneys) and breathing out (lungs) connects these two organs to regulate xue and assist the heart. The rhythm created by the up and down movement of the body, the opening and closing of the arms (lung and large intestine channels) and the in and out breath helps us adapt to the rhythmical changes of the seasons. The final stretch upwards on one leg stretches the flanks and therefore the liver and gall bladder channels to balance with the Lungs. The lungs are said to be 'spoilt' being the last organ to start working just after birth and are therefore fragile and sensitive to change, explaining why gentle exercise is preferred.5\n- \"KUNG FU PANDA: Big Bear Cat was \"PO-fect\"\". Kung Fu Magazine. Retrieved 2009-12-27.\n- \"5 Animals Fundamental Training\". Inside Kung Fu. Retrieved 2010-03-10.\n- \"Power of the Animals\". Inside Kung Fu. Retrieved 2009-12-29.\n- Jwing-Ming, Y 1989, p. 16, The Root of Chinese Chi Kung: The Secrets of Chi Kung Training, YMAA Publication Centre, Massachusetts\n- Dr. Shulan Yang, Endeavour College of Natural Health, 2012\n- Maciocia, G 2005, The Foundations of Chinese Medicine, 2nd edition, Churchill Livingston, Edinburgh","Our goal at the Integrative Veterinary Center is to provide clients with all available options for pet care. This is achieved by using conventional medicine, diagnostics, and all other reasonable therapies or systems of medicine to obtain the best therapeutic results. Every system of medicine has its strong and weak points and each is utilized according to what tool is best suited for an individual pet’s problem. The goal is to try to resolve or cure disease and not to suppress symptoms alone. In cases where the animal is at the end of its lifespan or has a terminal condition, the goal is to provide our patients with as high a quality of life as possible.\nAcupuncture is a branch of Traditional Chinese Medical (TCM) that has been practiced for over 5000 years. Traditional Chinese Medicine theory holds that Qi (vital energy) flows throughout the body along energetic channels called meridians. Each meridian is also associated with a specific internal organ that reflects the physiological and pathological conditions of that organ. The meridians connect the exterior of the body with the interior.\nAcupuncture points are located along these meridians, and when stimulated can relieve pain and restore normal body functions. Acupuncture points have specific locations and effects and have been documented over millennia. From a Western perspective, acupuncture has been shown to stimulate a variety of sensory nerves in the body that transmit the signal through the nervous system to the brain, which then releases various chemicals that produce multiple physiologic effects that activate the body’s homeostatic regulatory mechanisms.\nAt IVC, we always say that anyone, human or pet, can benefit from acupuncture. Acupuncture can be used to treat numerous ailments from diarrhea to kidney failure. There are multiple acupuncture techniques: dry needle acupuncture, moxibustion, aquapuncture, alectroacupuncture, laser acupuncture, and gold wire implantation into acupoints. These various methods allow for flexibility in animal acupuncture because different pets respond to the various techniques in different ways.\nAcupuncture and herbal medicine are typically used together because clinical results can be obtained more quickly that way. The combined effects are also stronger, this is especially important in the treatment of difficult cases.\nFor more information on the different types of acupuncture practiced at IVC please go to the Pain Management page of our website.\nBotanical (Herbal) Medicine\nTraditional Chinese Medicine (TCM) has been used in China for over 2500 years to maintain health in humans and animals. TCM includes herbs, acupuncture and massage. Herbal therapy and acupuncture are typically used together, as the combined effect is greater than using one of them alone. Chinese herbal medicine is the primary type of botanical medicine practiced at IVC. However, Western, Aryuvedic, Native American, South American and Hawaiian herbs may also be used depending on the case.\nThe Chinese herbal prescriptions used at IVC are not available over the counter; they are prescribed the same as conventional pharmaceutical drugs. The herbs used in these prescriptions are from the same companies as those used to treat humans. They have been tested for purity and quality. TCM herbs are safe and effective and can be used for long periods of time without side effects when prescribed by a qualified herbalist and used appropriately.\nChinese herbs can be used to treat most conditions recognized by conventional medicine. They can be used alone or combined with other therapies for an enhanced or synergistic effect. Often, a Chinese herbal and a conventional prescription will be used together.\nChinese herbs are especially helpful in the treatment of internal medicine disorders, failing organs, chronic diseases and diseases of the geriatric animal. They can be used to relieve pain, help improve and restore organ function, strengthen and support the immune system. TCM herbal prescriptions are specific for the individual patient and are directed at the root cause of an illness to correct it, and are not given to disease present and are not used to control symptoms alone.\nIn general, TCM herbal prescriptions must be given for longer periods of time than pharmaceutical drugs, but the benefit lies in their natural ability to gradually return the body to a state of balance and health, without adverse effects. Herbs must be given regularly two to three times a day just like conventional medications to see a therapeutic effect. Modern botanical medicine research is currently working to identify and document the active chemical constituents of many of the herbal formulas used today. Clinical results from TCM herbs can be seen in as little as 3 days but some take up to 2 months depending on the formulation for full effect.\nHerbal medications come in a variety of formulations: pills, tablets, capsules, liquids, topical tinctures, washes or pastes, and concentrated herbal extracts in powder or granule form. The formulation used depends on the type of condition treated and the species of animal.\nFor best results, our veterinarians will periodically monitor your pet’s condition, utilizing both conventional diagnostics and Chinese veterinary medical examination (tongue and pulse diagnosis) to determine if the formula they are being given is effective and still appropriate. In general, for acute conditions an animal may need to be rechecked once every 3-7 days; for chronic cases, once a month or longer may be adequate.\nAnimal chiropractic is a medical therapy that is used to maintain the health and normal functioning of the nervous and musculoskeletal systems. It follows the same principles and practice as chiropractic medicine in human medicine.\nA chiropractic adjustment is a very specific treatment that corrects subluxations. Subluxations are defined as misaligned vertebrae that are stuck or unable to move correctly causing pain, stiffness and/or neurological deficits. When the movement of the vertebrae or bones of the spine are restricted, the animal will not have normal function or flexibility of the spine and/or limbs.\nAnimal chiropractic is a medical therapy that is used to maintain the health and normal functioning of the nervous and musculoskeletal systems. It follows the same principles and practice as chiropractic medicine in human medicine.In general, chiropractic treatments or adjustments correct subluxations.\nA subluxation is defined as a partial dislocation of the joints where the articular surfaces are still in contact with each other, but are misaligned. Subluxations commonly occur in the spine secondary to acute injury, over exercise and chronic orthopedic disease, but can affect other joints as well. Subluxations can cause poor performance, stiffness, pain, decreased flexibility and function of the muscles and nerves.\nWhat is a Chiropractic Adjustment?\nA chiropractic adjustment or spinal manipulation is a specific high velocity controlled thrust on the specific joint that is being manipulated to correct subluxations or misalignment of the spinal column to relieve pain and restore normal range of motion and neurologic function to the area being treated. Chiropractic medicine is designed to stimulate a natural healing response and return the body to homeostasis as quickly as possible without harmful side effect.\nChiropractic adjustments can treat neck pain, back pain, sacroiliac pain, and tail pain that occur secondary to hip and elbow dysplasia, arthritis, disc prolapse, cruciate ligament tears, cancer, and others. Early chiropractic treatment can prevent the development of more serious musculoskeletal conditions in the future.\nProlotherapy, also known as nonsurgical ligament reconstruction, is a medical treatment for the repair of torn or weak tendons and ligaments and for chronic pain.\n“Prolo” is short for proliferation, because the treatment causes the proliferation (growth, formation) of new connective tissue in areas where it has become weak. Clinical results using prolotherapy in dogs and cats appear to indicate the same response. Many elite human athletes use prolotherapy to strengthen their weak ligamentous tissues to prevent against future tears and injury.\nProlotherapy is helpful for many different types of chronic musculoskeletal pain including: chronic osteoarthritis, intervertebral disk disease, chronic back and neck pain, degenerative joint disease, strengthen weak tendon and ligaments and repair torn joint ligaments, particularly the cranial cruciate ligament. Not all animals are candidates for prolotherapy. Each pet is evaluated on an individual case by case basis, and an examination is required to determine if prolotherapy is an appropriate therapy for your pet.\nThe type of laser used at IVC is the Companion Therapy Class IV Laser. Laser therapy has been shown to accelerate the body’s natural healing process through photo-bio-modulation. Laser clinical studies over several decades have shown that laser therapy relieves pain and inflammation, decreases swelling, stimulates nerve regeneration and promotes tissue repair. Therapy Lasers have been scientifically proven and successful in treating post-surgical pain and inflammation and many acute and chronic pain conditions.\nConditions that can be treated with Laser Therapy\n→ Acute Conditions\n→ Cuts/ Bites\n→ Tooth Extraction Pain Relief\n→ Sprains, Strains & Fractures\n→ Post-Surgical Healing / Pain Relief\n→ Chronic Conditions\n→ Degenerative Joint Disease\n→ Inflammatory Bowel Disease\n→ Periodontal Disease\n→ Lick Granulomas\n→ Geriatric Care\n→ Hip Dysplasia\n→ Feline Acne\n→ Anal Sacculitis\n→ Perianal Fistulas\n“Let food be your medicine, and your medicine be your food.”\nHippocrates – The Father of Medicine\nAt the Integrative Veterinary Center we believe that many of our pet’s health problems can be traced to feeding species inappropriate diets. We do not follow or advocate a single dietary program for animals with a “one size fits all” approach. Instead, our specially trained veterinarians can prescribe a nutritional therapy plan that is tailored to your animal’s specific needs.\nOur dietary recommendations are formulated taking into account a variety of factors such as the species, age, breed, weight, work the animals is expected to perform and disease condition. Individual food therapy programs suitable for your lifestyle that range from complete and balanced homemade diets to commercially available pet foods can be formulated for your pet.\nFor more information please visit bensbarketplace.com\nChinese Food Therapy\nChinese food therapy is one of the five branches of Traditional Chinese Medicine (TCM) and is the study of food as therapeutic agents for the preservation of health and the treatment and prevention of disease. For over 2000 years, doctors in China evaluated and recorded the properties of foods and their effects on the body. Our doctors can formulate Chinese food therapy programs to treat your pet’s specific disease condition.\nChinese food therapy follows the same diagnostic and treatment principles as acupuncture and Chinese herbal medicine. Chinese food therapy can be thought of as food used as medicine. Diets are designed to bring the body back into balance and work synergistically with the other TCM modalities.\nFood is considered to have the same energetic actions as herbs (cooling, warming etc) for treating disease and its application is based on similar principles but with a much broader range of applications. Food energetics refers to the effects food has on the digestion, physiological processes and metabolism of the body.\nFood is grouped into 4 basic energetic classifications:\n- Thermal nature (hot, warm, neutral, cool, cold)\n- Flavor (sour, bitter, sweet, pungent, salty)\n- Organ association (a food can affect specific internal organs)\n- Channel affiliation (a food has a definite effect on a particular acupuncture channel more than any others)\nNutritional consultations are 30-45 minutes in duration and can be scheduled at the center or via the telephone by calling 916-454-1825\nConventional (Western) Medicine\nAt IVC all of our doctors are trained in conventional medicine and we provide the same veterinary services as those in a strictly conventional veterinary medical practice.\nThe goal of an integrative approach to veterinary medicine is to provide clients with all the available options for pet care. This is achieved by using conventional medicine, diagnostics and procedures together with all other reasonable systems of medicine (Chinese medicine, chiropractic) or therapies (prolotherapy, gold bead implants) to obtain the best possible results.\nEvery system of medicine has its strong and weak points and each is utilized according to what tool is best needed for a particular animal’s problem. The goal is to try and cure disease and not to suppress symptoms alone. In cases where the animal is at the end of its lifespan or has a terminal condition, the goal is to provide as high a quality of life as possible for that animal.\nWe carry a full line of conventional pharmaceutical drugs and offer the following services:\nIVC offers non-anesthetic dental cleaning through the Animal Dental Care. Non-anesthetic dental cleaning is performed without general anesthesia or sedation using the same cleaning techniques and instruments as dental cleanings performed under general anesthesia. This type of dental is indicated for animals that require routine dental cleaning more often than once per year, those that are sensitive to general anesthesia and for pets whose age and condition will not allow for general anesthesia.\nYour pet will have their mouth examined by one of our veterinarians and then the dental technician with assess the mouth by examining and probing each tooth. Any visible abnormalities of the teeth and or mouth will be recorded on a customized patient dental chart. Most dental cleanings take 20-30 minutes to complete. Due to limited kennel space, we kindly ask that you wait for your pet during this procedure. If your pet has a large amount of tartar or inflamed gums, our doctors may prescribe antibiotics to be given 48 hours before the dental cleaning and continued for at least 5 days after the procedure. They are not required in all patients.\nYour pet’s teeth will be scaled and polished and the oral cavity rinsed. Minor tooth extractions can be performed by our doctors under light sedation and or injectable anesthesia with appropriate monitoring. Major tooth extractions or other dental work such as root canals, fillings, etc, can’t be performed with this dental technique. If major dental work is required you will be referred to have your animal evaluated and treated for dental disease under general anesthesia.\nPost-dental, the doctor and dental technician will make recommendations as to how to keep your pets teeth clean and healthy and how often they will need to be cleaned in the future.\nFor more information about anesthesia-free dental services including an instructional video, please visit: animaldentalcare.info.\nIntravenous Vitamin C\nIntravenous (IV) Vitamin C Therapy involves the administration of Vitamin C directly into the bloodstream. This delivery system is very powerful because it allows the plasma concentration of Vitamin C to reach levels high enough to kill cancer cells.\nIV Vitamin C Therapy is a frequent treatment option for cancer, but it can treat much more than that. Vitamin C Therapy can also treat atopic dermatitis, gingivitis, chronic nasal discharge, chronic upper respiratory infections, chronic infections, autoimmune disease and immune deficiency conditions.\nFor more information on IV Vitamin C go to our Cancer Treatment page.\nWounds, both surgical and non-surgical can be difficult. At IVC we have a number of unique techniques to promote wound healing in addition to antibiotics and surgery.\nWe understand that many people live far away, and the ability to talk to a veterinarian regarding integrative treatment options for your pet is often requested. A phone consultation can be the first step in deciding what direction to pursue for your pets condition. For this reason, we offer phone consultations with Dr. Signe Beebe. This option provides flexibility for individuals that live long distances from our center, those out of state and internationally who are interested in obtaining information on an Integrative and or Traditional Chinese Medicine approach to healthcare for their pets.\nWe can provide dietary recommendations, herbal therapy and other integrative therapies after a review of your pet’s medical records. We may also be able to refer you to a veterinarian closer to your area that practices Integrative and or Chinese veterinary medicine.\nAs no physical examination of the pet was performed during a phone consult, we can’t prescribe any medication. Instead, you will need to follow up with a primary care veterinarian or schedule an examination of your pet with our office so that we may legally prescribe any recommended medications."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:57b672de-469f-4507-b8f5-599c48691630>","<urn:uuid:3cbff71c-1a55-478f-a542-273f3594a981>"],"error":null}
{"question":"How does contemporary literature approach individual identity in diverse societies, and what challenges does it present for political institutions? 📚","answer":"Contemporary literature approaches individual identity by exploring the intersectionality of belongings and the mobility of individuals' minds between different references of identification. This is evident in works like 'Blood Book,' which tackles fluid gender identity and family dynamics, and 'Dschinns,' which examines migrant experiences. The challenges for political institutions arise from increasingly complex relations with groups and individuals who have multidimensional and shifting identities. These identities are structured along various intersections including gender, ethnicity, religion, and migrancy, making it difficult for political institutions to effectively address and accommodate such diversity.","context":["- 1 hr ago Daily Horoscope, 06 October 2022: Today's Horoscope Predictions For All Zodiac Signs\n- 18 hrs ago Tribute: American Country Music Icon Loretta Lynn No More At 90\n- 18 hrs ago Navratri 2022: Goddess Chamunda Shakti Peeth Of Himachal Pradesh\n- 20 hrs ago Balaji (Hanuman) of Mehandipur In Rajasthan: Timings, How To Reach, Puja Rituals And Legends\n- Movies The Ghost Day 1 Box Office Collection Worldwide: Nagarjuna's Thriller Opens To Decent Numbers!\n- News Hurricane Ian: Biden and DeSantis stress unity amid cleanup\n- Sports Legends League Cricket: Full List of Award Winners, Player of the Match, Player of the Tournament\n- Finance Prabhudas Lilladher Picks 5 Stocks To Buy Amid Festive Cheer For Gains Up To 20%\n- Education JIO ANNOUNCES THE LAUNCH OF JIO TRUE 5G\n- Automobiles Mahindra XUV300 Sportz Edition Spotted - Comes With More Power & A Few Visual Tweaks\n- Travel Most Interesting Facts About Kullu Dussehra\n- Technology Google Pixel 7 vs Pixel 7 Pro: What Differences Can We Expect?\nOften compared to the British Booker Prize, the French Prix Goncourt or the American Pulitzer Prize, the German Book Prize is the most high-profile literary prize in the German-speaking world, where winners can hope for substantial sales of their works.\nThe shortlist, announced on September 20, represents the entire panorama of contemporary German-speaking literature: From the anecdotal to the art novel to the feminist working-class novel and post-migration literature.\nWhether set in the Hunsrück uplands in southwestern Germany, Istanbul or in Switzerland, among others, their stories tell of human folly, gender fluidity and the history of the German republic from the perspective of those who have built it, namely the guest workers.\nHere is an overview of the finalists of the German Booker Prize:\n1. Fatma Aydemir: 'Dschinns' (German for 'Djinns')\nGerman literary culture has shown its more diverse side in the last few years. Authors who have not been born in Germany, or whose parents were not born in Germany, reveal their lives pendulating between different worlds. The keyword \"post-migrant literature\" is actually used for novels about very normal lives in Germany, where around 25% of the population has a migrant background.\nIn \"Dschinns,\" Hüseyin, who has worked in Germany for 30 years, finally fulfills his dream of owning a house in Istanbul. However, on the day he has to move into the new place, he dies of a heart attack. His funeral brings the family together. Fatma Aydemir narrates the story of human longings, unachieved dreams, big and small secrets and German post-war history.\n2. Kristine Bilkau: 'Nebenan' ('Next Door')\nKnown for her withdrawn prose, Kristine Bilkau talks about themes that have occupied all her other novels: the lives, the problems and the neuroses of the upper middle class.\nIn an unnamed town at the Northern Baltic Sea channel, a 30-year-old, shy woman, who has moved in from the big city, meets an established, 60-year-old general practitioner, who has been running a practice in this place for several decades. A neighboring family disappears without a trace, the doctor receives threatening letters, and the small town changes insidiously.\nWith subtle tension, Bilkau narrates the diffuse fears of the well-off middle class in Germany.\n3. Daniela Dröscher: 'Lügen über meine Mutter' ('Lies about my mother')\nDaniela Dröscher narrates a story of growing up in a family, where one theme dominates everything: her mother's obesity. Is this beautiful, impulsive and unreliable woman too fat? Does she need to lose weight urgently? Yes, she must, her husband decides. And the mother faces this, day after day.\nOn the one hand, the book is a story about a childhood in the Hunsrück uplands; on the other, it looks at misogyny of the past and how it continues to this day. A daughter narrates the story of her mother, who migrated to Germany from Poland and perceives her as a complex personality as well as an ideal.\n\"Lies About My Mother\" is also a class novel. Daniela Dröscher has tackled the subject in the past: Her memoir about growing up as a working-class child in Germany was published in 2018 under the title \"Show Your Class. The story of my social origins\".\n4. Jan Faktor: 'Trottel' ('Idiot')\n\"What is the reason for my good mood? Simply, everything.\"\nThese are the opening lines of Jan Faktor's novel and amid the slowing COVID pandemic, rising energy costs and the Russian invasion, a good mood can make a huge difference.\nThis anecdotal novel offers exactly that: readers follow the story of a man, who calls himself an 'idiot,' living and re-living his stupidity to the unending enjoyment of the readers. As is characteristic of the anecdotal novel, the \"Idiot\" is naturally there to reveal his surroundings: the red wine-sipping intellectuals, the mother-in-law or the even more idiotic son. Anyone who doesn't see himself or herself in one of the stories is above all, a little bit idiotic.\n5. Eckhart Nickel: 'Spitzweg'\nNickel's novel takes its name from the 19th-century German painter, Carl Spitzweg. In the story, three students, including two boys and a girl, discover the attraction of the arts. The narrator admires Carl, a boy who joins the school shortly before his school-leaving exams.\nIn order to impress Carl, the narrator does not defend his classmate Kirsten when the art teacher calls her self-portrait a \"courage to be ugly.\"\nThis is a book about art, a love triangle, a highly talented girl and cunning plans for revenge. Nickel appeared in the 2019 Longlist for the German book prize for his novel, \"Hysteria.\"\n6. Kim de l'Horizon: 'Blutbuch' ('Blood Book')\nThe non-binary narrator of \"Blutbuch\" lives in Zurich, after they flee from the small, conservative village in Switzerland, where they were born.\nTheir grandmother's illness however, puts the narrator into a thoughtful mood: they talk to the older woman and list all themes about which both have never spoken, including the protagonist's fluid gender identity or the grandmother's racism.\nSecrets and silences in families are popular and timeless themes in literary history. De l'Horizon, through their non-binary protagonist, approaches the subject from a new perspective, adding to the many accusations made about families.Coming on the heels of the German government's proposed law in June 2022 to simply change gender entries, \"Blood Book\" comes at the right time politically — and rounds up a shortlist that depicts the modern society that the Federal Republic has become in the 21st century.\nThis article was originally written in German.\n- womenGeetanjali Shree's 'Tomb Of Sand,' The 1st Hindi Novel To Win International Booker Prize\n- womenWomen Dominate International Booker Prize 2022 Shortlist, Including Indian Author Geetanjali Shree\n- lifeAravind Adiga's - The White Tiger\n- art cultureRenowned Spanish Author Javier Marias Passes Away Due To Lung Infection Post COVID-19\n- art cultureExclusive: Author Navigates Hope, Debunks Superficial Differences Across 85 Countries And 7 Continents\n- lifeGajendra Singh Shekhawat Launches DLA's First Publication '8 Years 80 Miracles'\n- decorEasy Guide To Create The Perfect Reading Nook\n- art cultureAnnihilate: All About Author Michel Houellebecq's Book Portraying Dying White Patriarchy\n- home n garden221B Baker Street: Home Address Of Sherlock Holmes, Harry Potter, Mary Poppins, Hercule Poirot And Tintin\n- art cultureUS Author Joan Didion Passes Away Due To Complications From Parkinson's Disease\n- womenArundhati Roy: Writing Is Activism\n- menTribute:Tribute: Bestselling Author Wilbur Smith Passes Away At 88","What Happens When a Society is Diverse?\nExploring Multidimensional Identities\nSicakkan, Hakan G.\nLithman, Yngve G.\nWith its interdisciplinary and multi-paradigmatic approach, this book aims to bring our thinking about diversity one step further towards making coexistence and politics in diverse societies possible. Diversification in our societies takes place on at least three levels. On the societal level, one can speak of a multitude of cultural/social groups. On the group level, the multitude and intersectionality of individual belongings comes to the fore. On the individual level, the mobility of individuals’ minds between different references of identification becomes a crucial element in theorizing the diverse society. What happens in society, politics, and communicative public spaces when the society is diverse in these terms?\nMuch of the recent intellectual and policy work has not been able to comply with societies that are increasingly diverse and groups/individuals whose relations with political institutions are becoming more complex than ever. By focusing on social groups and individuals with multidimensional and shifting identities, which are structured along the intersections of gender, sexuality, ethnicity, religion, ideology, physical disability, generation, mobility and migrancy, this volume aims to increase the understanding of the complex relations in diverse societies between humans, social groups, and political institutions. The different chapters of the book bring into focus an array of experiences with diversity and, taken together, they contribute to an understanding of the complex realities of living in diversity.\nTo provide a solid interdisciplinary basis for theorizing diversity, the book brings together the conceptual and methodological tools of political theory, social theory, history, political science, sociology and social anthropology. In this book, scholars with unique competencies share their knowledge on the topic and provide novel angles for thinking about coexistence and politics in diverse societies.\n“At first glance, one might think that the interdisciplinarity of the scholars in this volume would be too diverse a mix to yield benefits, despite their scholarly interest in topics related with identity and diversity. However, when finishing reading this volume, the productive results are obvious, as researchers from diverse sets of orientation are brought together in an organized way towards a common goal. While each chapter is a ‘stand-alone’ contribution to the literature, the breadth and depth of the topics covered demonstrates both the opportunities and challenges that lie ahead as we attempt to gain a better understanding of diversity in contemporary society, and how multidisciplinary approaches can yield synergistic effects. The result in this book is a capturing robustness of the debate surrounding diversity and state practices.” – (from the Commendatory Preface) Professor James S. Frideres, University of Calgary, Canada\n“The resurgence of immigration in the late 20th century and the political emergence of gender groups, gays, indigenous groups and people with disabilities have posed serious questions about who belongs where. For example, is it necessary that a migrant or gay in the 21st century conform to a prior collective identity to belong? Or can political action by a disenfranchised minority help shape the nation states identity? ... Academics, students and policy makers now can inform themselves about the roles of identity politics, the politics of identity and modes of belonging by reading a series of cogently written essays in this volume which reach across the 19th to the 21st century in both European and North American contexts.” – Professor Don J. DeVoretz, Simon Fraser University, Canada\n“This is a well-edited and timely collection, addressing political, social and cultural problems in today’s world that are of primary significance both to individuals, groups and political regimes: problems pertaining to how and why people identify, construct their belongings (or have them constructed for them), take advantage of their citizenship status, are marginalized or recognized, and, not least, try to deal with the constraints politically imposed on their frequently multiple and variable senses of belonging by shaping new, composite and boundary-transgressing identities for themselves ... This book helps both political actors, individuals concerned with perceived disjunctures between citizenship and belonging, and scholars in a variety of disciplinary fields to think differently, more imaginatively about these core issues – and hopefully to develop a more sophisticated semantics for coping with them, practically and theoretically.”– Professor Ulf Hedetoft, Aalborg University, Denmark\nTable of Contents\nList of Figures and Tables\n1. Introduction: Diversity and Multidimensional Identities – Yngve G. Lithman and Hakan G. Sicakkan\n2. Identity and Sociology – Hildur Ve\n3. Ethnic Entrepreneurs: Identity Politics Among Pakistani Students in Norway – Mette Andersson\n4. Beyond “Man”: In Defense of Multidimensional Identities – Randi Gressgård and Christine Jacobsen\n5. Deaf Identities: Visible Culture, Hidden Dilemmas and Scattered Belonging – Jan Kåre Breivik\n6. Glocal Spaces as Prototypes of a Future Diverse Society: An Exploratory Study in Six European Countries – Hakan G. Sicakkan\n7. McJihad: Globalization, Radical Transnationalism, and Terrorism of the Diaspora – Yngve Georg Lithman\n8. The Struggle for Recognition – A Conceptual Framework – John Erik Fossum\nISBN10: 0-7734-5877-8 ISBN13: 978-0-7734-5877-2\nhors série Number: 0\nSubject Areas: Cultural Studies,\nEthnic and Immigrant Studies,\nSociology & Social Sciences,\nImprint: Edwin Mellen Press\nUSA List Price: $139.95 UK List Price: £ 94.95\nDiscounts: Discounts are available. Please\nRegister, or if you have already registered, Sign In, to view your personalized prices."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:9a1f5493-606c-4198-a00c-147089c17e62>","<urn:uuid:6296f5c4-7d77-4db7-8f50-8a5b05f73479>"],"error":null}