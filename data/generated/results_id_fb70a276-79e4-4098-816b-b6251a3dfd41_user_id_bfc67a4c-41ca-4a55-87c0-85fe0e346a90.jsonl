{"question":"Is a property registration consultant necessary in Georgia, and what liability concerns should be considered?","answer":"While not strictly required, a property registration consultant can provide valuable assistance in navigating the complex registration process. They offer expertise in preparing documentation, verifying ownership, submitting applications, liaising with authorities, and tracking registration progress. Their services can save time, reduce stress, and ensure compliance with regulations. Regarding liability concerns, property registration laws in Georgia require all transfers of ownership and encumbrances (like mortgages and liens) to be registered to be legally recognized. Additionally, under the Right to Repair Act, both property owners and contractors face strict timelines and requirements - homeowners must grant access for repairs within 30 days of accepting a remedy offer, while contractors must respond to claims within 30 days or risk legal action.","context":["Property Registration in Georgia\nIn Georgia, property registration is overseen by the National Agency of Public Registry (NAPR). The registration process is governed by the Georgian Law on State Registry of Real Property.\nThe property registration process in Georgia involves several steps and it is important to note that property registration is mandatory in Georgia, and failure to register a property can result in legal issues in the future.\nWhy Property Registration is Important?\nProperty registration is a crucial process that all property owners in Georgia must undertake.\n• The importance of property registration cannot be overstated, as it provides legal recognition of ownership and protects the property owner's interests. It establishes a legal record that can be used to settle disputes, resolve legal issues, and protect the owner's rights.\n• The benefits of property registration include providing proof of ownership, facilitating property transactions, protecting against fraud and disputes, and ensuring compliance with property laws and regulations.\n• Property registration is a legal requirement in Georgia, and failure to register a property can result in legal issues in the future. Therefore, it is essential to ensure that all properties are registered with the State Registry of Real Property.\nHow to Register a Property in Georgia?\nRegistering a property in Georgia requires a thorough understanding of the property registration process, including the steps involved, the requirements, and the necessary documentation.\n• The property registration process includes conducting a title search to verify ownership, preparing the required documents, and submitting the application to the State Registry of Real Property.\n• The process can be complex and time-consuming, requiring the property owner to adhere to all regulations and requirements.\n• Some of the requirements for property registration in Georgia include proof of ownership, a valid ID, and payment of fees. The documents required for registration may vary depending on the type of property and the location. Generally, the following documents are required:\n• Property deed or title\n• ID or passport of the owner\n• Power of attorney (if the owner is not registering the property in person)\n• Other documents related to the property, such as building permits, surveys, and tax declarations.\n• It is advisable to seek professional assistance from a property registration consultant to ensure a smooth and efficient registration process.\nUnderstanding Property Registration Laws in Georgia\nProperty registration laws in Georgia define the legal aspects of property registration and set forth the requirements and regulations for property owners. These laws govern the process of registering property ownership in the State Registry of Real Property and cover aspects such as:\n1. Property ownership verification.\n• Failure to register a property can result in legal issues in the future, including disputes over ownership and boundary issues.\n2. Property transfer\n• Any transfer of ownership must be registered in the State Registry of Real Property to be legally recognized.\n3. Property taxes\n• Property owners are required to pay property taxes annually, and failure to do so can result in penalties and legal issues.\n• The laws also address encumbrances on a property, such as mortgages, liens, and easements. These encumbrances must be registered in the State Registry of Real Property to be legally recognized.\n5. Dispute resolution\n• The property registration laws provide a framework for resolving disputes over property ownership. In the event of a dispute, the parties can seek legal remedies through the courts or alternative dispute resolution mechanisms.\nProperty registration laws in Georgia provide a legal framework for protecting the rights and interests of property owners and ensuring compliance with all regulations and requirements. Property owners should be familiar with these laws and regulations to ensure that they comply with all requirements for property ownership and registration.\nThe Role of a Property Registration Consultant\nA property registration consultant is a professional who provides expert assistance and expertise in the process of registering a property.\nThe role of a property registration consultant includes:\n1. providing guidance and advice on the property registration process,\n2. preparing the necessary documentation,\n3. verifying ownership,\n4. submitting the application,\n5. acting as a liaison with authorities,\n6. tracking the progress of the registration process.\nTheir services can help property owners navigate the complex process of property registration and ensure that all requirements are met in a timely and efficient manner. Professional property registration assistance from a property registration expert can save time, reduce stress, and ensure compliance with all regulations and requirements.","Georgia’s Right to Repair Act adds a number of additional steps that did not previously exist when filing a lawsuit. However, the aim of the program is to offer resolution options outside of the courtroom when a disagreement arises between a contractor and a homeowner.\nWhether you are considering how the Right to Repair Act might affect your current or recently completed contract on your home, or if you are a contractor seeking to understand your responsibilities in the light of the law, it is important that you review what the Georgia Right to Repair Act accomplishes and how the process works.\nWhat The Georgia Right To Repair Act Accomplishes\nThe intention of the act was to prevent lawsuits, forcing homeowners to proceed through a “claims” process outlined by the law that would prevent the dispute from being heard in a courtroom when possible.\nThe first step in the process requires contractors to inform the homeowner receiving the work of the content of the Act and explain what process is available to them by the Act in the event of a dispute of the contract. Contractors who fail to make this information available before any contract begins are in violation of the Right to Repair Act. It is mandatory that this information be presented conspicuously, and may be included within the contract.\nOnce the homeowners have been made aware of these rights, they may send a notice if the work was deficient, caused damage to their home or was not up to adequate standards. This “notice of claim,” differs from a lawsuit. Instead, the Georgia Right to Repair Act enacted this process in order to reduce the amount of litigation related to construction. It aims to reduce the burden to the court of construction cases, encouraging settlement outside of court.\nThe process delineated in the Right to Repair Act requires very stringent, specific pre-litigation actions by the homeowner in order to open a claim; it is important that all parties respond to their portion of the notice of claim in a timely and comprehensive manner.\nWhat The Right To Repair Process Looks Like\nThe very first step in the Right to Repair process, as mentioned above, is the contractor’s duty to inform the homeowners of their rights. Contractors are responsible for sharing information about the Act with homeowners before work begins.\nSimply telling the homeowners about the existence of the Act is not sufficient; it must be prominently written and must substantially explain that the individuals must notify their construction company a minimum of 90 days before filing a lawsuit.\nIf a homeowner elects to bring a complaint against a contractor for construction problems, the next step in the process is to send a Notice of Claim. Remember, a claim is not a lawsuit; instead, Georgia’s Right to Repair Act calls this process the “Notice of Claim” that is attempting to resolve the situation outside of a lawsuit context. Homeowners should act quickly after a construction defect has been noticed to send a Notice of Claim.\nThis “notice of claim” should include a detailed description of the issues with the construction or the damage incurred, as well as any relevant photographs. If pictures both before and after the work are available, these are ideal for inclusion. The notice of claim needs to be submitted at least 90 days before any attempt at a lawsuit is initiated.\nAfter the notice has been submitted, the next step is to wait for the construction company to react to the information. Contractors have 30 days to respond to a notice of claim. Their response may vary, falling into three distinct possibilities; the first is that they may offer a settlement to resolve the claim by the homeowner. This may include refunding the total or partial cost of the work done on the home in a manner that is agreeable to the homeowner, combining some repayment with a proposal for repairs or another resolution. If the homeowner chooses to accept this resolution, the situation is resolved.\nIf not, the contractor may seek an outside evaluation of the defect by a third party. The function of this step is to gather information from an unbiased source about whether a defect or issue has actually occurred or whether the homeowner is bringing unreasonable expectations to bear on the situation. Presentation of the third party’s decision to the homeowner may serve to resolve the dispute.\nAlternatively, after the construction company receives the notice of claim, they may choose simply to do nothing for the 30 days allotted to them.\nNo matter what type of response the company presented, the next step in the process is for the homeowners to make their own decision. They may accept the response provided by the contractor, whether that be a settlement or proof that the claim is unreasonable. If they do not accept the resolution proposed by the other party, or if the other party did not respond within the 30 days allotted to them, then the case may move to legal action.\nBe aware that the homeowner must still provide in writing their reasons for rejecting the contractor’s resolution in cases in which the contractor did offer to settle. Before the case moves to litigation, the homeowner must explain the reasons why the contractor’s proposed resolution was not sufficient to remedy the damages incurred.\nOnce the right to repair act provisions has been satisfied, the case may then move to litigation. The conflict has now progressed out of the Right to Repair notice of claim system and may become a lawsuit.\nThe Georgia Right to Repair Act gives both contractors and homeowners multiple means of resolving a construction dispute before it ever reaches court. However, it is important to be aware of the stringent time limitations and detailed requirements guiding every step of the notice of claim process.\nFailure to properly abide by the Right to Repair Act could impede or even prevent the ability to file a lawsuit later, which is why it is critical to correctly and carefully review the process as soon as an issue arises. This is why many individuals elect to seek advice from a legal professional who can ensure that the process goes as intended.\nChallenges may arise on the homeowners’ side in terms of abiding with the Right to Repair regulations. For example, if the homeowner agrees to the construction company’s proposed remediation of the problem, the homeowner must grant prompt access to their home for repairs within 30 days of the offer being made.\nAfter accounting for how long it may take for the offer to reach the homeowner, people being out of town and a number of other extenuating circumstances that could delay this process, some homeowners may find themselves on very tight deadlines throughout this process. Contractors must also respond timely to their portion of the process, which may be difficult with many workers out throughout the day on projects. The best resolution is for both parties to work together.\nRight To Repair Protects Homeowners In Georgia\nWhether you are a homeowner who is considering how to utilize the Right to Repair Act in Georgia to seek compensation for poor or damaging workmanship in your home or you are a contractor trying to understand your responsibilities to your clients in light of the Right to Repair Act, be sure to consult with an attorney who is experienced with this segment of the law. Both parties must be sure to be adequately informed.\nIf you would like to learn more about the Right to Repair Act and how it may affect you, the attorneys at KPPB LAW would be happy to help. Reach out to schedule a consultation."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:33c1ab18-672d-42bf-b9c7-daf7ea175c7e>","<urn:uuid:c43644b7-9950-45e3-8532-e5c0ab25d63d>"],"error":null}
{"question":"I'm interested in historical firearms - what makes the Winchester Model 1887 significant in shotgun development compared to the modern Benelli Nova?","answer":"The Winchester Model 1887 was historically significant as the first commercially successful repeating shotgun, designed by John M. Browning despite his initial reservations about the lever-action format. It offered a 5+1 shell capacity and faster shooting compared to the side-by-side shotguns of its era, though it had drawbacks like awkward loading. In contrast, the Benelli Nova represents modern shotgun evolution with its innovative one-piece receiver and buttstock made of steel-reinforced polymer, advanced features like the chamber empty button for tactical reloading, and modular design allowing for various barrel and sight configurations. While the 1887 was eventually superseded by pump-actions, it helped pave the way for repeating shotgun development.","context":["|Type||Pump action shotgun|\n|Place of origin||Italy|\n|Weight||8 lbs. (3.63kg)|\n|Length||45.5-49.5 in. (1,155.7-1,257.3mm)|\n|Barrel length||18½-28 in. (609.6-711.2mm)|\n|Cartridge||12-gauge shotshell, 20-Gauge Shotshell|\n|Feed system||4+1 internal magazine|\n|Sights||Mid: Metal bead\nFront: Red bar\nThe Benelli Nova is a pump action shotgun, popular for hunting and self-defense. Its most innovative and distinguishing feature is a one-piece receiver and buttstock, made of steel-reinforced polymer.\nTwo main models are available as well as one variant.\nThis model is available with a variety of barrel and sight configurations, most intended for hunting and/or trap/skeet shooting. It is made in both Matte and camouflage finishes. Due to the polymer coated receiver and stock, along with proprietary coatings on the action and barrel, it is considered impervious to the elements. Barrels may be rifled or smoothbore, and are usually 24\", 26\", or 28\" long. This model is available in 12 gauge or 20 gauge. Five types of chokes are available. Typically sold with improved modified and full, internal chokes. extended aftermarket chokes available.\nThis model is intended for defensive purposes. With an 18½\" barrel, and rifle or ghost- ring (diopter) sights, it is easier to wield and quicker to sight than hunting models. This barrel is smoothbore, and not tapped for chokes, reducing its versatility and rendering it less accurate at longer ranges. A slightly different model labeled the H2O Nova is similar, with the exception of an electroless nickel finish replacing the standard black coating, presumably with corrosion resistance in mind. This model is 12 gauge only.\nThis model incorporates a number of recoil reducing features, and has a removable stock that can be replaced by a pistol-grip stock. This model has a bigger trigger set.\n- Synthetic Stock and Receiver Cover: for moisture resistance.\n- Cross Bolt Safety\n- Extendable Magazine: may optionally hold up to 7 rounds in magazine. (2.75in. 12 gauge)\n- 3.5in. chamber: will fire and reliably cycle 2.75in., 3in., and 3.5in. shells. (in 12 gauge)\n- Recoil Reducer: this optional mercury recoil unit is installed via a bracket that is attached to the interior of the stock. The baffled tube holds 14 oz. of mercury, which raises the length of time that the shot's impulse is spread across, thus lowering felt recoil.\n- Optional Tritium Sights: for tactical units, to provide constantly illuminated sights.\n- Chamber Empty Button: on the forend, allows unloading of unfired shell without releasing additional shells from the magazine. This is very useful for so-called \"slug select\" drills, where the user needs to quickly select a different type of ammunition (for example, a slug in order to engage a target at longer range.) The operator simply depresses the button as he cycles the action back; the chambered shell will be ejected, but the magazine will not feed another shell. The user then places the desired shell into the chamber and closes the action. On another shotgun without this feature, if the magazine were full, the user would have to cycle the shotgun to eject the chambered shell and make room in the magazine, then insert the desired shell into the magazine, then cycle the action again. This means that two shells are needlessly ejected.\nRecoil without the internal reducer can be harsh using the 3.5 inch shells. The shotgun is very light due to its composite/steel construction. This light weight shotgun lends itself to heavy recoil using heavy loads with high velocities. The recoil reducer helps reduce the felt recoil and allows the shooter to obtain a faster second shot. The recoil reducer is an option and must be purchased separately from a Benelli dealer or any of the more common shotgun web sites, or incorporated in the original sale/order (the recoil reducer is standard on all Benelli Supernova.)\nThe recoil reducer consists of two elements; the apparatus that connects to the stock via the buttstock compartment, and the mercury element that is inserted into the recoil apparatus.\n- McNab, Chris (13 November 2007). Sporting Guns: A Guide to the World's Rifles and Shotguns. St. Martin's Press. p. 54. ISBN 978-0-312-36823-4.","From innovative black-powder repeater to modern throwback arm and dandy field gun, the All-American lever-action shotgun continues to soldier on.\nWhat You Need To Know About The Lever-Action Shotgun:\n- The first commercially successful repeating shotgun (Winchester 1887) was a lever-action.\n- While the style of shotgun disappeared for decades, it reemerged with the rise of Cowboy Action Shooting.\n- It makes an excellent option for game such as turkey, due to its ability to cycle quickly while holding on the target.\nFrom the start, John M. Browning didn’t think it was a good idea. After all, a lever-action shotgun would be unwieldy, comparably slow and God awful on the knuckles. Try as he might, the legendary gun designer couldn’t veer Winchester Repeating Arms to a pump-action design. Management wouldn’t budge. Dagnabit, they were a lever-gun company, anything less was an affront.\nSo was born the first successful repeating shotgun—the Winchester Model 1887, later to become the 1901 with the advent of smokeless powder.\nCertainly, Browning had it right, pump-action and later semiautomatic shotguns were the wave of the future for fast shooting, easy-to-operate smoothbores. For most applications, everything about them enhanced the efficient use of the generally hard kicking class of gun.\nBut it’s tough to argue the gun genius’s capitulation to the gun manufacturer’s whims has made for one of the most unique, timeless and downright nasty classes of shotguns. The lever-action shotgun will never top the heap, but there is no doubt it isn’t going anywhere soon.\nGenesis Of The Lever-Action Shotgun\nTo modern eyes, the Winchester 1887 has a lumbering appearance. Dromedary-shaped action, short and thick buttstock and the typically long barrel of a black powder shotgun, it’s not exactly the first iron you’d grab come grouse season. Yet, at the time—in an age that would have viewed the lever-action shotgun as equally as awkward—it had a great advantage.\nWhether you realized it or not, the shotgun was perhaps the most used—if not deadly—firearm of the Old West. James “Kill’n Jim” Miller was blistering hell with one in his hands (a skill that eventually led him to a dance at the end of a rope). But more so, it was the perfect tool for Manifest Destiny. Aside from good and bad men, every pioneer, farmer, rancher and likely last remaining mountain men had a smoothbore at beck and call. As pragmatic as those times, you could as easily fend off a grizzly bear or a claim jumper, as you could knock a duck off the wing with one. If you could do it in two shots.\nTherein lies the edge of the 1887—it wasn’t a side-by-side. Before the Winchester lever-action shotgun hit the scene, that’s all that was on hand. Aside from their limited capacity and glacial reload time (no ejectors), these side-by-sides were hammer guns that had to be manually cocked to get into action.\nOn the other hand, the Model 1887 held 5+1 shells and, comparably, shot at a respectable clip. Sure, it wasn’t optimized for an accelerated firing rate or as intuitive as other potential designs, but it sure beat the alternative. Succinctly put, in the land of the blind, the one-eyed man was king.\nCertainly, the 1887 had its drawbacks. As anyone who’s had the pleasure of being at the business end of one of the 12- or 10-gauges knows, it’s not the easiest firearm to load. Given there’s no loading gate and the action opens from the top, you have to reach through the receiver to insert a shell into the tubular magazine. And the lever-action shotgun and its progeny weren’t exactly nimble. Initially, the 1887 came with a 30-inch barrel standard—a 20-inch variation offered much later. And the 1901 came outfitted with a behemoth 32-inch barrel. Hyperbolically speaking, that’s teetering on punt gun territory.\nDespite these drawback, the Winchester 1887 is as elegant as any of the other guns scratched together by Browning. Utilizing a rolling block action, the lever-action shotgun was not only rock-solid, but also extraordinary simple. Early on, Winchester touted the latter aspect, boasting the action only had 16 parts.\nFurther, Browning made the 1887 as easy to operate as the design would allow. Primarily, this was achieved by dedicating the opening stroke to the lighter duty of extracting the spent shell and the closing stroke the more heavy work of compressing the hammer spring. You’d rather do the latter with your palm than your knuckles. Plus the trigger lays back on the trigger guard until the lever is almost closed, so you avoid the unpleasant experience of stabbing yourself with it.\nThese and other aspects made a good system, but not an enduring one. Less than a decade after its introduction, the 1887’s sales petered out. Though, it had one last gasp of life as the more stoutly built smokeless powder 10-gauge—the Model 1901. Another Browning design supplanted the lever-action shotgun, the iconic Winchester Model 1897—to a lesser extent its forerunner the 1893. Pump-actions both, it seemed Browning had it right from the start.\nRebirth Of The Lever-Action Shotgun\nFunny thing about the closing of the American West, it didn’t quite shut the lid on the lever-action shotgun.\nSure enough, the design disappeared for decades, smoothbore fanatics had other things to keep them busy—pump-actions, semi-automatics and over/unders. Then, like a prairie twister, the lever-action touched down again. Winchester dabbled with the .410 Winchester 9410 in the 1990s, nearly a decade later Marlin introduced a lever .410 of their own, Henry Repeating Arms followed suit a little over 10 years later.\nNifty (and useful, we’ll get to that in a moment) as those small-bores proved, it wasn’t really what breathed new life into the lever-action shotgun. Wistfulness for the old west did … that and a cyborg from the future.\nRaise Your Lever-Action IQ:\n- 5 Of The Best Lever-Action Rifle Options Available Today\n- 7 Best Lever-Action Rifles To Ever Sling Lead\n- 5 Must-Have Henry Lever-Action Rifles\n- 7 Marlin Lever-Action Rifles Worth Adding To Your Collection\n- .357 Magnum Lever-Action: The Best Pistol Caliber Carbine?\n- 9 Greatest Winchester Lever-Action Rifles, Shotguns and Bolt-Actions\nCowboy Action Shooting—basically 3-Gun with period-correct firearms (replica and original) and duds—saw an incredible surge in the last quarter of the 20th Century. Those who couldn’t root out or afford a vintage Model 1887 and wanted to shoot the lever-action in a match created a market.\nSo did the 1991 film Terminator II. Spin cocking a sawed-off riff on an 1887 on the back of a Harley-Davidson, Arnold Schwarzenegger brought more attention to the level-action shotgun than it had had since Grover Cleveland was in the White House. Go figure.\nby in large, the rebirth of the 1887 has had a foreign accent—Chinese and Italian. The Chinese Norinco was an early purveyor of the design, particularly a model fashioned after the Terminator’s gun, and won mixed review. Today there’s a non-descript “Chinese Made” 1887 is available at Century Arms for the princely sum of $380. A get-what-you-pay-for affair.\nItalian gunmaker Chiappa, on the other hand, has earned more accolades, producing a more finely made option. In addition to selling under their name, they’re also imported by several replica specialists, manufactured to those company’s specifications.\nGiven Browning never worked in polymer, there are several variations of the lever-action shotgun that are a drastic break from the original design. So what?\nAlmost every replica 1887 is in one major way—they’re capable of shooting shells loaded with smokeless powder. But they aren’t technically 1901 knockoffs—for marketing purposes, mainly—but also since nearly every dang one of them is a 12-gauge.\nA Modern Place Of The Lever-Action Shotgun\nThere are plenty of makes/models that owe their present existence to the field of competition. Yet, the lever-action shotgun has more depth to it than throwback shooting matches. Despite flying in the face of convention, it still has a role as a field gun and as a defensive arm.\nOutside the 1887, the lever-action shotgun is primarily a .410 affair now, which might sound a little light by modern standards. Certainly, no one will argue against a small-bore scattergun for first-timers and youths, looking to master arm before having to deal with recoil. More so, with ammunition advancements, the .410 has found a new lease on life in turkey season. Improvements in wad design and heavier than lead shot, hunters have gravitated to the smaller—arguably faster shooting—diminutive bore.\nCombine with a lever-action, the .410 becomes a formidable tool for knocking down a strutting gobbler—perhaps a better one than a pump. How so? You can shoot it more like a rifle. Take a kneeling or cross-legged position, for example. You can build a solid base with your support elbow on your knee to aim and never have to break it down to apply a follow-up shot. The same cannot be said of the pump-action.\nHenry’s 19.5-inch barreled lever-action .410 and Marlin’s 22-inch barrel 1895 .410 both fill this role extremely well, even if—in both cases—you have to drill and tap them if you hunt with a scope. Henry perhaps has the edge, given its shotgun uses Invector removable chokes—Marlin has choke options, but all are fixed. Either way, someone searching for a fast, accurate and light-recoiling turkey gun couldn’t do much better.\nOn the home front, the lever-action is also viable, particularly with the meteoric rise of the bird-head grip. Given these firearms are generally shot at the hip, the lever-action proves as efficient as the pump to cycle the shotgun. More so, for certain individuals more comfortable with the former over the latter. Black Aces Tactical certainly sees it this way.\nLong demanded, the Florida company came out with a convertible lever-action smoothbore line—Pro Series L— which comes with both a shoulder stock and bird’s head grip. The 12-gauge—capable of chambering 3-inch shells—is considerably larger than many grip-option firearms, boasting an 18.5-inch barrel. Still, outfitted with the grip it should prove plenty nimble and no less quick and potent.\nDespite its long history and somewhat recent resurgence, the lever-action shotgun isn’t going to become America’s preferred smoothbore anytime soon. That’s alright. It perfectly fills its present niche. As time goes on, clever gun designers will most definitely find new roles for the shotgun to play. Who knows what the future might hold for the All-American design? Whatever it is, one thing is certain—the lever-action shotgun will continue to endure.\nDraw A Bead On Shotguns:\n- 9 Affordable Double-Barrel Shotgun Options\n- 12 Affordable Pump-Action Shotgun Options\n- Understanding The Semi-Auto Shotgun\n- 5 Affordable Over/Under Shotguns Worth A Shot\n- 5 Best Bullpup Shotgun Options For Compact Defense"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f36c0ed5-bd59-481e-a497-cb4df949962b>","<urn:uuid:4eb6cdfe-6969-47c0-8922-1a7edf214aa5>"],"error":null}
{"question":"I'm studying art history - what similarities exist between Jean Tinguely's mechanical artworks Chaos I and the Méta-Matics referenced in ADA's description?","answer":"Both Tinguely's works share mechanical qualities but differ in their purpose. The Méta-Matics are described as unwearily struggling mechanical beings focused on industrial self-destruction. Meanwhile, Chaos I, installed at The Commons, is a more sophisticated 30-foot high kinetic sculpture that purposefully moves through a choreographed series of motions simulating a day in life. Both demonstrate Tinguely's interest in mechanical motion, but Chaos I creates a deliberate artistic sequence rather than focusing on self-destruction.","context":["ADA – analog interactive installation/kinetic sculpture/post-digital drawing machine\na project by Karina Smigla-Bobinski\nResidency period: 18. 11.–1. 12. 2013\nSimiliar to Tinguely’s “Méta-Matics”, “ADA” is an artwork with a soul. It acts itself. At Tinguely’s, it is sufficient to be an unwearily struggling mechanical being. He took it wryly: the machine produces nothing but its industrial self-destruction. Whereas “ADA” by Karina Smigla-Bobinski is a post-industrial “creature”, visitor-animated, creatively acting artist-sculpture, self-forming artwork, resembling a molecular hybrid such as the one from nanobiotechnology. It develops the same rotating silicon-carbon-hybrids, midget tools, miniature machines able to generate simple structures.\n“ADA” is much larger, esthetically much more complex, an interactive art-making machine. Filled up with helium, floating freely in the room, a transparent, membrane-like globe, spiked with charcoals that leave marks on the walls, ceilings and floors. Marks which “ADA” produces quite autonomously, although moved by a visitor. The globe obtains an aura of liveliness and its black coal traces produce the appearance of a drawing. The globe, when put in action, fabricates a composition of lines and points that remain incalculable in their intensity, expression or form, however hard the visitor tries to control “ADA”, to drive her, to domesticate her. Whatever they try out, they notice very soon that “ADA” is an independent performer, studding the originally white walls with drawings and signs. More and more complicated fabric structures arise. This is a movement experienced visually, which, like a computer, makes an unforeseeable output after entering a command. It is not by chance that “ADA” reminds of Ada Lovelace, who in the 19th century, together with Charles Babbage, developed the very first prototype of a computer. Babbage provided the preliminary computing machine, while Lovelace provided the first software. A symbiosis of mathematics with the romantic legacy of her father Lord Byron emerged there. Ada Lovelace intended to create a machine that would be able to create works of art, such as poetry, music or pictures, like an artist does. “ADA” by Karina Smigla-Bobinski follows this very tradition, as well as the one of Vannevar Bush, who built a Memex Machine (Memory Index) in 1930 (“We wanted the memex to behave like the intricate web of trails carried by the cells of the brain”), or the Jacquard’s loom that needed a punch card in order to weave flowers and leaves; or the “analytic machine” of Babbage which extracted algorithmic patterns.\n“ADA” uprose in a contemporary spirit of biotechnology. She is a vital performance-machine, and her patterns of lines and points get more and more complex as the number of the audience playing increases. Leaving traces that cannot be deciphered by neither the artist nor the visitors, let alone by “ADA” herself. And still, “ADA”’s work is unmistakably potentially humane because the only available decoding method for these signs and drawings is the association to which our brain corresponds especially when it sleeps: the truculent jazziness of our dreams.\n© ADA – analog interactive installation by Karina Smigla-Bobinski written by Arnd Wesemann\nAbout the artist:\nKarina Smigla-Bobinski lives and works as a freelance artist in Munich and in Berlin in Germany. She studied painting and visual communication at the Academy of Fine Arts in Krakow, Poland and Munich, Germany. She works as an intermedia artist with analogue and digital media. She produces and collaborates on projects ranging from interactive and mixed reality art in form of installations, objects, in-situ and online-art projects, art interventions and multimedia physical theatre performances, to digital and traditional painting, analogue interactive installations or kinetic sculptures. Since 2013, she is a member of The Dream Team by DiBari Innovation Design, a unique Associated Studio bringing together architects, artists and designers from all over the world to design the city of the future, today. She is also a lecturer in the Department of Art and Design at the University of Applied Sciences in Augsburg (Germany).\nHer works have been shown in 36 countries on 5 continents at festivals, galleries and museums internationally, including GARAGE Center for Contemporary Culture in Moscow (Russia), ZERO1 Biennial in Silicon Valley (US), FILE Electronic Language International Festival in São Paulo and Rio de Janeiro (Brazil), FACT in Liverpool (UK), Busan Biennale (South Korea), GAK – Gesellschaft für Aktuelle Kunst in Bremen (Germany), and the Bangkok University Gallery (Thailand).\nHer collaborative performances have been shown at the Festival Montpellier (France), Festival in Ramallah (Palestine), Grand Théâtre (Luxembourg), Fundação Calouste Gulbenkian in Lisbon (Portugal), Festival in Kabul (Afghanistan), GoDown Art Center in Nairobi (Kenya), National School of Drama in Delhi (India), Festival Caracas (Venezuela), Fadjr-Festival in Tehran (Iran), Art Festival (South Korea), Haus der Kunst in Munich (Germany), Teatro Sesc in São Paulo (Brazil), Biennale de la danse in Paris (France), Berliner Festspiele (Germany) and Biennale di Venezia – Arsenale, Venice (Italy).","Inside/Outside the Lines\nThe Commons Coloring Pages\nYouth Arts Activity\nA free Youth Arts Activity, brought to you by the Columbus Area Arts Council, and designed by Jenni Kiesler. This project supports CAAC’s strategic objectives, “to serve the community” and “to foster and develop young talent”.\nWe can’t wait to see what you imagine!\nAbout the Columbus Area Arts Council\nThe Columbus Area Arts Council’s mission is “to strengthen the community through arts and culture”, with programs, events and activities focused around three pillars: education, engagement, and enrichment, and a priority to make the arts accessible to everyone. CAAC has been a leading arts agency in Bartholomew County since 1972, partnering with state, regional and local arts and culture organizations to be a central resource for the arts by inspiring a creative life for all.\nLearn more at artsincolumbus.org.\nAbout Jenni Kiesler\nJenni Kiesler is a designer and illustrator based in Columbus. Jenni founded her business, Keywords Co., in late 2016, beginning with small milestone chalkboards and Christmas ornaments. As demand for personalized art expanded to businesses and events, Jenni broadened her offerings to include customized typography, chalk art, digital lettering, illustration and painted murals.\nLearn more at keywordsco.com.\nAbout The Commons\nThe Commons is an indoor community and event space in Columbus featuring modern event spaces and stage area, an indoor playground, and public art. Designed by the Boston-based firm, Koetter Kim, and referencing the former design by Cesar Pelli, The Commons is not only a community center, but also a community resource.\nLearn more at thecommonscolumbus.com.\nPublic Art in Columbus\nColumbus has a remarkable collection of public art! The city includes works by a variety of creative talent, from internationally-known names to local and up-and-coming artists.\nLearn more about Columbus public art at columbus.in.us/public-art.\nIncluded in this packet:\n2 Arcs de 212.5° by Bernar Venet\nBernar Venet’s 2 Arcs de 212.5° – also known as “the Red C” – is typical of this French artist's minimalist work in curved, mathematically precise metal. Seemingly precariously balanced, this work, like his others, reflects the artist’s love of mathematics and his process of adapting material, form, balance, and spatial perception.\nChaos I by Jean Tinguely\nChaos I is a seven-ton, kinetic (moving) sculpture by Swiss artist Jean Tinguely. The 30-foot high piece is the largest work by Tinguely in the United States. Driven by twelve motors, Chaos I cycles through a series of motions to simulate a day in a life, beginning slowly at first, adding movements and then winding down again. At the peak of its chaotic movements, steel balls roll and crash through a caged track, making quite the ruckus! The work was commissioned by J. Irwin and Xenia Irwin Miller and Clementine Tangeman in 1971.\nFlamenco by Ruth Aizuss Migdal\nCreated by Chicago artist Ruth Aizuss Migdal, this abstract, painted steel sculpture was installed as part of the Columbus Sculpture Biennial in 2014. A fundraising campaign by community members raised the money to make it a part of the Columbus permanent art collection in 2016."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:79a39df9-c6ad-467e-9194-f4ded8037a6c>","<urn:uuid:755a9586-c748-46fc-a29a-25fc518a3a31>"],"error":null}
{"question":"What were the different components included in the Mr. Batavia high school competition?","answer":"The Mr. Batavia competition included lip syncing, a talent contest, a Q&A session, a tuxedo walk, and a swimsuit competition. Five judges from the community awarded points to the contestants, and a committee tallied the points to determine the winner.","context":["Spencer Hubbard wins Mr. Batavia competition at BHS\nIn middle school, Spencer Hubbard was bullied. Today, you can call him Mr. Batavia.\nThe Batavia High School senior won the title Friday night during its second annual Mr. Batavia competition.\nWhen Hubbard's name was announced as the winner, a packed auditorium of high school students, parents and faculty let out a robust cheer and round of applause.\n\"It shows how far we've come as people that a gay student can become Mr. Batavia,\" Hubbard said during his acceptance speech.\nHubbard was tops in a field of 10 in a competition that included lip syncing, a talent contest, a Q&A and a tuxedo walk. Five judges from the community awarded points to the contestants and a committee tallied up the points to pick the winner.\nMore than $2,200 was raised for the winner's charity. In this case, Hubbard picked Habitat for Humanity.\n\"I feel like it's a basic need, that people need homes,\" Hubbard said. \"A lot of people are homeless who don't deserve to be. They really need that help.\"\nHubbard is planning to attend the University of Tampa on a $25,000 scholarship. He will major in journalism.\nBHS Principle Scott Williams praised Hubbard as a bright student and talented actor.\nAs part of the competition, Hubbard performed a self-written satire of a Target clerk that had more funny lines than a Saturday Night Live skit.\nHubbard said he thinks his acting talent and his self confidence, especially during the swimsuit competition and lip syncing, is what helped sway the judges in his favor.\nLast year, Lee Johnson won the contest, in its first year, and Johnson returned Friday night to entertain the crowd while the judges' ballots were tallied. It was a regular stand-up routine, but as he began to run out of material, Williams stepped in to help entertain the crown, including setting up an Ellen-like, Academy Awards selfie.\nCompeting this year were Adam Weaver, Charlie Williams, Blake Carter, Jake Paine, Tim Martin, Mathew Gabriele, Kenny McMaster, Casey Grice and Michael DiBaccco.\nDuring his acceptance speech, Hubbard praised his fellow competitors and thanked them for being so supportive of each other throughout preparations for the show.\nThe fact that Johnson is so different from Hubbard, Hubbard said, shows the competition is valuable to the community.\n\"It shows different people can win,\" Hubbard said. \"Last year Lee won, and he's really different from me. He runs in a different crowd. He was straight and I'm gay, and that was probably a really big thing, too. I said that in my speech as well. It shows how far we've come as people.\"\nA lot has changed for Hubbard, he said, from middle school to his senior year.\n\"The award means a lot because in middle school I was bullied and now, look how far I can come,\" Hubbard said. \"I think the big thing was I was just myself when I got to high school. I didn't really care what people thought. I didn't try hard to impress people.\"\nHubbard being congradulated by his fellow contestants.\nMike DiBacco sings Frank Sinatra's, \"I've Got the World on a String.\"\nHubbard performing his sketch, a satire of a clerk from Target.\nSydney Loria, Ashlee Yasses and Haley Case were hostesses for the competition.\nMatt Gabriele at the front of the stage during the tux walk.\nLee Johnson, last year's winner.\nPrincipal Scott Williams during an Ellen/Oscar-inspired selfie with the cast and crew of the Mr. Batavia competition.\nTo purchase prints of these photos and the photos in the slide show, click here.\nWOW ---- a gay kid wins the Mr. Batavia contest --- BHS HAS come a long way. Congratulations, Mr Hubbard !!!!!! I don't know anyone at that school, but I am impressed and proud of you all.\nAgreed Irene. As a mother to 4, I realized a long time ago that our children are much more tolerant and accepting of others than many adults. I remember several years ago when it was announced that a transgender teacher that had formally been a \"Mr.\" to the students would be teaching as a \"Miss\" when school started in the fall. Many parents were in an outrage and a few were even shortsighted enough (many other adjectives also come to mind) to transfer their kids to private schools. For the most part, the kids just shrugged it off. She was a great teacher as a man and would continue to be a great teacher as a female. Isn't that what we want for our kids - great teachers?\nAnyway, congratulations Spencer and BHS. He is a nice and very talented young man.\nhow about a competition for BOTH boys and girls featuring practical career skills (like those taught at BOCES). Such as body work, welding, etc. the girls may very well win. more power to them :)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f9fecbd0-ed9c-4c77-a0e7-a5261eaee7d0>"],"error":null}
{"question":"In studying ancient goddess worship across Europe, what were the key differences between how Greeks honored Enyo versus how Gauls venerated Belisama in their sacred spaces?","answer":"The worship of these goddesses differed significantly in their sacred spaces and practices. For Enyo, we know she was honored in formal temple settings, with a notable statue of her made by Praxiteles' sons standing in the temple of Ares in Athens. Meanwhile, Belisama was worshipped in sacred enclosures called 'nemetons' - these were specially designated sacred groves or sanctuaries where only initiates were allowed to enter. This is evidenced by the Vaison-la-Romaine inscription where Segomaros dedicated a nemeton to her. Additionally, while Enyo's worship was centered in Greek cities like Athens, Belisama's cult was particularly prominent throughout Gaul and Britain, with evidence of her worship found near water sources, such as the Belisama estuary in Britain's River Ribble.","context":["ENYO was the goddess or personified spirit (daimona) of war. She was the female counterpart and close companion of the god Ares Enyalios.\nEnyo was closely identified with Eris, the goddess of strife. Indeed Homer does not appear to distinguish between the two goddesses. She was also connected with the Anatolian goddess Ma and the Roman Bellona.\nFAMILY OF ENYO\nE′NYO (Enuô), the goddess of war, who delights in bloodshed and the destruction of towns, and accompanies Mars in battles. (Hom. Il. v. 333, 592; Eustath. p. 140.) At Thebes and Orchomenos, a festival called Homolôïa was celebrated in honour of Zeus, Demeter, Athena and Enyo, and Zeus was said to have received the surname of Homoloïus from Homoloïs, a priestess of Enyo. (Suid. s. v.; comp. Müller, Orchom. p. 229, 2nd edit.) A statue of Enyo, made by the sons of Praxiteles, stood in the temple of Ares at Athens. (Paus. i. 8. § 5.) Among the Graeae in Hesiod (Theog. 273) there is one called Enyo.\nSource: Dictionary of Greek and Roman Biography and Mythology.\nCLASSICAL LITERATURE QUOTES\nENYO GODDESS OF WAR\nHomer, Iliad 5. 333 ff (trans. Lattimore) (Greek epic C8th B.C.) :\n\"[The] goddesses, who range in order the ranks of men in fighting, [are] Athene (Athena) and Enyo, sacker of cities.\"\nHomer, Iliad 5. 590 ff :\n\"And with him followed the Trojan battalions in their strength; and Ares led them with the goddess Enyo, she carrying with her the turmoil of shameless hatred.\"\nAeschylus, Seven Against Thebes 41 ff (trans. Weir Smyth) (Greek tragedy C5th B.C.) :\n\"[The leaders of the army of the Seven Against Thebes :] Seven warriors, fierce regiment-commanders, slaughtered a bull over a black shield [before the commencement of battle], and then touching the bull's gore with their hands they swore an oath by Ares, by Enyo, and by Phobos (Rout) who delights in blood, that either they will level the city and sack the Kadmeans' (Cadmeans') town by force, or will in death smear this soil with their blood.\"\nCallimachus, Hymn 2 to Apollo 85 ff (trans. Mair) (Greek poet C3rd B.C.) :\n\"The belted warriors of Enyo.\"\nCallimachus, Hymn 4 to Delos 275 ff :\n\"On thee [the island of Delos] treads not Enyo nor Haides (Death) nor the horses of Ares (War) [i.e. the island was sacrosanct and so free of war].\"\nPausanias, Description of Greece 4. 30. 5 (trans. Jones) (Greek travelogue C2nd A.D.) :\n\"In the Iliad he [Homer] represented Athena and Enyo as supreme in war.\"\nQuintus Smyrnaeus, Fall of Troy 1. 365 ff (trans. Way) (Greek epic C4th A.D.) :\n\"Who is so aweless--daring, who is clad in splendour-flashing arms : nay, surely she shall be Athene, or the mighty-souled Enyo--haply Eris (Strife).\"\nQuintus Smyrnaeus, Fall of Troy 2. 525 ff :\n\"'[Akhilleus (Achilles) and Memnon engage each other in battle :] Twixt these Enyo lengthened out the even-balanced strife, while ever they in that grim wrestle strained their uttermost . . . Those glorious sons of gods, nor ever ceased from wrath of fight. But Eris (Strife) now inclined the fatal scales of battle, which no more were equal-poised.\"\nQuintus Smyrnaeus, Fall of Troy 5. 25 ff :\n\"[Among the images decorating the shield of Akhilleus (Achilles) :] And there were man-devouring wars, and all horrors of fight . . . Phobos (Panic) was there, and Deimos (Dread), and ghastly Enyo with limbs all gore-bespattered hideously, and deadly Eris (Strife).\"\nQuintus Smyrnaeus, Fall of Troy 8. 186 ff :\n\"[The Greek heroes Neoptolemos and Eurypylos are engaged in combat :] Hard by them stood Enyo, spurred them on ceaselessly : never paused they from the strife . . . Eris (Strife) incarnate watched and gloated o'er them.\"\nQuintus Smyrnaeus, Fall of Troy 8. 286 ff :\n\"Stalked through the midst [of the battle] deadly Enyo, her shoulders and her hands blood-splashed, while fearful sweat streamed from her limbs. Revelling in equal fight, she aided none, lest Thetis' or Ares' (the War-God's) wrath be stirred.\"\nQuintus Smyrnaeus, Fall of Troy 8. 424 ff :\n\"Many of them [soldiers battling at Troy] dyed the earth red: aye waxed the havoc of death as friends and foes were stricken. O'er the strife shouted for glee Enyo, sister of Ares (War).\"\nQuintus Smyrnaeus, Fall of Troy 11. 7 ff :\n\"The Akhaians (Achaeans) pressed hard on the Trojans even unto Troy. Yet these charged forth--they could not choose but so, for Eris (Strife) and deadly Enyo in their midst stalked, like the fell Erinnyes (Furies) to behold, breathing destruction from their lips like flame. Beside them raged the ruthless-hearted Keres (Fates) fiercely: here Phobos (Panic-fear) and Ares there stirred up the hosts: hard after followed Deimos (Dread).\"\nQuintus Smyrnaeus, Fall of Troy 11. 151 ff :\n\"The black Keres (Fates) joyed to see their conflict [the Greeks and the Trojans], Ares laughed, Enyo yelled horribly. With corpses earth was heaped, with torrent blood was streaming: Eris (Strife incarnate) o'er the slain gloated.\"\nQuintus Smyrnaeus, Fall of Troy 11. 237 ff :\n\"[Aeneas and Neoptolemos engage in combat :] Enyo level held the battle's scales.\"\nQuintus Smyrnaeus, Fall of Troy 12. 436 ff :\n\"[The Trojans drag the Wooden Horse into the town Troy :] Grimly Enyo laughed, seeing the end of that dire war.\"\nQuintus Smyrnaeus, Fall of Troy 13. 85 ff :\n\"[The Greek army enters Troy :] In deadly mood then charged they on the foe. Ares and fell Enyo maddened there : blood ran in torrents.\"\nPhilostratus the Elder, Imagines 2. 29 (trans. Fairbanks) (Greek rhetorician C3rd A.D.) :\n\"We see in the plain corpses upon corpses, and horses lying as they fell, and the arms of the warriors as they slipped from their hands, and this mire of gore in which they say Enyo delights.\"\nAnonymous, Persian War of Diocletian and Galerius Fragment (trans. Page, Vol. Select Papyri III, No. 135) (Greek poetry C4th A.D.) :\n\"They, maddened by Enyo's lash, all girded on their quivers full of arrows, each armed his hand with bow and spear.\"\nTryphiodorus, Sack of Ilium 560 ff (trans. Mair) (Greek poetry C5th A.D.) :\n\"[The slaughter at the sack of Troy :] Enyo, revelling in the drunkenness of unmixed blood, danced all night throughout the city, like a hurricane, turbulent with the waves of the surging war. And therewithal Eris (Strife) lifted her head high as heaven and stirred up the Argives; since even bloody Ares, late but even so, came and brought to the Danaans the changeful victory in war.\"\nOppian, Halieutica 2. 24 ff (trans. Mair) (Greek poet C3rd A.D.) :\n\"The gifts of Ares are swords and brazen tunics to array the limbs and helmets and spears and whatsoever things Enyo delights in.\"\nOvid, Heroides 15. 135 ff (trans. Showerman) (Roman poetry C1st B.C. to C1st A.D.) :\n\"Thither in frenzied mood I course, like one whom the maddening Enyo has touched, with hair flying loose about my neck. My eyes behold the grots, hanging with rugged rock--grots that to me were like Mygdonian marble.\"\nValerius Flaccus, Argonautica 4. 604 ff (trans. Mozley) (Roman epic C1st A.D.) :\n\"The Amazones (Amazons) . . . such a sort and of such might as Enyo triumphant over men.\"\nStatius, Thebaid 8. 655 ff (trans. Mozley) (Roman epic C1st A.D.) :\n\"[In the war of the Seven Against Thebes :] Enyo, afire with torch fresh-charged and other serpents, was restoring the fight. They yearn for battle, as though they had but lately borne the opening shock of combat hand to hand, and every sword still shone bright and clear.\"\nNonnus, Dionysiaca 5. 40 ff (trans. Rouse) (Greek epic C5th A.D.) :\n\"[In the battle between Kadmos (Cadmus) and the Aionians (Aeonians) :] To both armies alike Eris (Strife) joined Enyo and brought forth tumult.\"\nNonnus, Dionysiaca 2. 358 & 2. 475 ff :\n\"[When the monster Typhoeus engaged Zeus in battle :] Eris (Strife) was Typhon's escort in the mellay, Nike (Victory) led Zeus into battle . . . impartial Enyo held equal balance between the two sides, between Zeus and Typhon, while the thunderbolts with booming shots revel like dancers in the sky.\"\nNonnus, Dionysiaca 7. 7 ff :\n\"[Aion (Father Time) addresses Zeus :] ‘Lord Zeus! behold yourself the sorrows of a despairing world! Do you not see that Enyo [goddess of war] has made the whole earth mad, mowing season by season her harvest of quick-perishing youth?’\"\nNonnus, Dionysiaca 17. 316 ff :\n\"[During Dionysos' war against the Indians :] The cruel mellay was not ended yet : the struggle was only half done, the conflict unfinished. Indian Ares appeared on high and shouted loud; Bakkhos' (Bacchus') [Dionysos'] mad Enyo marshalled them for another bout, belching a loud of frenzied Lydian threats in the renewed battle, hurling on the foe volleys of deadly garlands, furious for war.\"\nNonnus, Dionysiaca 17. 376 ff :\n\"[At the close of a battle between the army of Dionysos and the Indians :] By this time then the barbarian goddess Enyo had quieted her voice among the fighters.\"\nNonnus, Dionysiaca 20. 35 ff :\n\"[The Indian War of Dionysos :] Godborn Dionysos! Deriades [the Indian King] summons you to battle, and you make merry here! Stepmother Hera mocks you, when she sees your Enyo (Warrior Spirit) on the run.\"\nNonnus, Dionysiaca 33. 55 ff :\n\"[The Indian War of Dionysos :] Battlestirring Ares in mortal shape, with Enyo by his side . . . has armed himself against Dionysos at Hera's bidding and supports the Indian king [Deriades].\"\nNonnus, Dionysiaca 39. 361 ff :\n\"Their assault woke a new conflict: Enyo went before their sails.\"\nSuidas s.v. Enyo (trans. Suda On Line) (Byzantine Greek lexicon C10th A.D.) :\n\"Enyo : A goddess of war.\"\nBELLONA ROMAN GODDESS OF WAR\nBellona was the Roman goddess of war whom the Latin poets identified with the Greek Enyo.\nOvid, Metamorphoses 5. 155 ff (trans. Melville) (Roman epic C1st B.C. to C1st A.D.) :\n\"[In the battle between Perseus and the supporters of Phineus in the halls of King Kepheus (Cepheus) of Aithiopia (Ethiopia) :] Bellona fouled the gods of hearth and home with flooding gore and stirred fresh scenes of violence.\"\nSeneca, Hercules Furens 686 ff (trans. Miller) (Roman tragedy C1st A.D.) :\n\"[At the entrance to the Underworld :] The foul pool of Cocytus' sluggish stream lies here; here the vulture, there the dole-bringing owl utters its cry, and the sad omen of the gruesome screech-owl sounds. The leaves shudder, black with gloomy foliage where sluggish Sopor (Sleep) [Hypnos] clings to the overhanging yew, where sad Fames (Hunger) [Limos] lies with wasted jaws, and Pudor (Shame) [Aidos], too late, hides her guilt-burdened face. Metus (Dread) [Deimos] stalks there, gloomy Pavor (Fear) [Phobos] and gnashing Dolor (Pain) [Algos], sable Luctus (Grief) [Penthos], tottering Morbus (Disease) [Nosos] and iron-girt Bella (War) [Enyo]; and last of all slow Senectus (Old Age) [Geras] supports his steps upon a staff.\"\nValerius Flaccus, Argonautica 2. 228 ff (trans. Mozley) (Roman epic C1st A.D.) :\n\"Bellona flashed her sword o'er their heads.\"\nValerius Flaccus, Argonautica 3. 60 ff :\n\"[The battle between the Argonauts and the Kolkhians (Colchians) :] Lo! Above the open portals appeared Bellona with bare flank, her brazen weapons clanging as she moved, and as with triple plume she smote the housetop she cried thence to the king [Aeetes]. He distraught follows the goddess along the city walls, onward to the fight that was to be his last.\"\nStatius, Thebaid 2. 718 ff (trans. Mozley) (Roman epic C1st A.D.) :\n\"Nor did ever Mavors [Ares] or Bellona with her battle-spear inspire more furious trumpet-blasts [than Athena].\"\nStatius, Thebaid 3. 424 ff :\n\"[Heralding the war of the Seven Against Thebes :] Amid the night-wandering shades the god of battle [Mars-Ares] from on high made to resound with the thunder of arms the Nemean fields and Arcadia from end to end, and the height of Taenarum and Therapnae . . . filled excited hearts with passion for himself [war]. Furor (Fury) and Ira (Wrath) make trim his crest, and Pavor (Panic), his own squire, handles his horses' reins. But Fama (Rumour), awake to every sound and girt with empty tidings of tumult, flies before the chariot, sped onward by the winged steeds' panting breath, and with loud whirring shakes out her fluttering plumes; for the charioteer [Bellona-Enyo] with blood-stained goad urges her to speak, be it truth or falsehood, while threatening from the lofty car the sire [Mars-Ares] with Scythian lance assails the back and tresses of the goddess.\"\nStatius, Thebaid 4. 5 ff :\n\"[Heralding the start of the first battle of the War of the Seven Against Thebes :] First from the Larissaean height Bellona [Enyo] displayed her ruddy torch, and with right arm drove the spear-shaft whirling; hissing, it flew through the clear heaven, and stood fixed on the high rampart of Aonian Dirce [the city of Thebes bracing for war]. Then to the camp she goes and, mingling with the heroes that glittered in gold and steel, shouts like a squadron; she gives swords to hurrying warriors, claps their steeds and beckons gateward; the brave anticipate her promptings and even the timid are inspired to short-lived valour.\"\nStatius, Thebaid 5. 155 ff :\n\"[The women of Lemnos plan to slay their husbands :] They pledged their solemn word, and thou wast witness, Martian [i.e. of Mars] Enyo, and thou, Ceres of the Underworld [Persephone], and the Stygian goddesses [the Erinyes] came in answer to their prayers.\"\nStatius, Thebaid 7. 64 ff :\n\"[During the war of the Seven Against Thebes :] With bloody hand dark Bellona guides the team [of horses] and plies them hard with her long spear.\"\nStatius, Thebaid 10. 855 ff :\n\"[During the war of the Seven Against Thebes :] Bellona with blood-stained brand drew nigh to graze their [the Thebans] towers to the ground.\"\nStatius, Achilleid 1. 25 ff :\n\"Bellona brings from the vessel [of Paris] amid uplifted torches a new daughter-in-law [Helene] to Priam [i.e. ‘War’ heralds the arrival of Helen in Troy].\"\nCULTS OF ENYO & MA\nI. ATHENS Chief City of Attica (Southern Greece)\nPausanias, Description of Greece 1. 7. 4 (trans. Jones) (Greek travelogue C2nd A.D.) :\n\"[In the sanctuary of Ares at Athens] is also an image of Enyo, made by the sons of Praxiteles.\"\nII. ANITAUROS City in Cappadocia) Kappadokia) (Anatolia)\nStrabo, Geography 12. 2. 3 (trans. Jones) (Greek geographer C1st B.C. to C1st A.D.) :\n\"In this Antitauros [in Kappadokia (Cappadocia), Asia Minor] are deep and narrow valleys, in which are situated Komana (Comana) and the temple of Enyo, whom the people there call Ma.\"\n[N.B. the Greeks identified Enyo with the Anatolian goddess Ma.]\nIII. PHRYGIAN MOUNTAINS Phrygia (Anatolia)\nValerius Flaccus, Argonautica 7. 634 ff (trans. Mozley) (Roman epic C1st A.D.) :\n\"The anger of the mournful Mother [Rhea-Kybele (Cybele)] rends every year the frenzied Phrygians, as Bellona [i.e. the Anatolian goddess Ma] lacerates the long-haired eunuchs.\"\n- Homer, The Iliad - Greek Epic C8th B.C.\n- Aeschylus, Seven Against Thebes - Greek Tragedy C5th B.C.\n- Callimachus, Hymns - Greek Poetry C3rd B.C.\n- Strabo, Geography - Greek Geography C1st B.C. - C1st A.D.\n- Pausanias, Description of Greece - Greek Travelogue C2nd A.D.\n- Philostratus the Elder, Imagines - Greek Rhetoric C3rd A.D.\n- Oppian, Halieutica - Greek Poetry C3rd A.D.\n- Quintus Smyrnaeus, Fall of Troy - Greek Epic C4th A.D.\n- Tryphiodorus, The Taking of Ilias - Greek Epic C5th A.D.\n- Nonnus, Dionysiaca - Greek Epic C5th A.D.\n- Greek Papyri III Anonymous, Fragments - Greek Poetry C4th A.D.\n- Ovid, Metamorphoses - Latin Epic C1st B.C. - C1st A.D.\n- Ovid, Heroides - Latin Poetry C1st B.C. - C1st A.D.\n- Seneca, Hercules Furens - Latin Tragedy C1st A.D.\n- Valerius Flaccus, The Argonautica - Latin Epic C1st A.D.\n- Statius, Thebaid - Latin Epic C1st A.D.\n- Statius, Achilleid - Latin Epic C1st A.D.\n- Suidas, The Suda - Byzantine Greek Lexicon C10th A.D.\nOther references not currently quoted here: Suidas s.v. Homolois, Eustathius on Homer's Iliad 140.","Belisama: Who Was this Beautiful, Powerful, and Popular Gaulish Goddess?\nWhen the Roman conquerors encountered the world of Gaulish deities and traditions, they tried to understand it in their own religious terms. Seeing a cult of gods and goddesses, they sought to find similarities between Roman and Gaulish deities. They saw in the beauty of the Gaulish goddess Belisama another face – that of their own wise deity, Minerva. However, the significance of Belisama ran far deeper for Gauls than the Roman interpretation.\nBelisama was a Gaulish goddess known as Minervae Belissimae, translating to Belisama Minerva. In mythology, she was a consort of the god Belenus. Her name most probably meant ''the brightest one'' or ''the most powerful''. Researchers believe that her position in the Gaulish pantheon of the deities was continuous, however, most of the information about her has been lost over time. There is a theory about her being a companion of Belenus, the Celtic Sun God (sometimes identified with Apollo), but this comes from uncertain speculation. She was a goddess related to fire, who connects the power of the light of the sun and moon. It is also possible that she was a goddess of the moon itself, but this idea has not been confirmed.\nA Gaulish Deity that the Romans Conquered\nEach time Romans conquered a new land, they tried to understand the culture of the people they found there. Although the primary idea wasn't converting the local inhabitants to their beliefs, it seems that applying some aspects of their own religion was seen as potentially beneficial. Cultural and religious icons were used to forge links between the different civilizations. Thus, it was a common practice to draw connections between the native deities and mythology taken from Rome to the new lands.\nHelmeted Minerva holding a tiny owl. Belisama has been compared to the Roman goddess Minerva. ( Public Domain )\nBelisama was a deity whose attributes were also related to war, bravery, force, and valor. It seems that - apart from the feminine aspects of her as a goddess, she was also worshiped on the battlefields. Her cult was well-known in the territory of modern France, but also in Britain. In France, she is known not only as Belisama but sometimes as Beleymas or Belleme. The number of supposed or confirmed sects of her cult there is convincing enough evidence to conclude that she was one of the more popular deities.\nWhen it comes to finding evidence of her cult in Britain, the situation is a little more complicated. According to the text by the ancient writer Ptolemy, the “Belisama estuary” was located in River Ribble, Lancashire, England. The closeness of her cult to the water begs the question of a possible relationship between the cults of Minerva and another Celtic-Roman deity, Sulis-Minerva (well-known in Britain), and the worshiping of Belisama. However, until now, this relationship has not been well-established.\n- The translation of the Gallic faith into the Roman pantheon\n- Excavations Reveal that the Oldest Roman Tavern Nourished and Served Ancient Life\n- Aquae Sulis: The Epitome of Roman Syncretization with the Celts\nIdentifying the Real Belisama\nBelisama only rarely appears in inscriptions. However, there are at least two examples that mention her name. Both were discovered in old Gaulish lands and the value of the short texts carved in the ancient stones is tremendous. Moreover, the two inscriptions have entirely separate origins.\nAccording to a report made by researchers from the University of Lyon in France:\n''The first dedication is engraved on an altar in white marble with blue-grey veins, discovered in re-employment* in the bridge of Saint-Lizier (Ariège), which was the main oppidum* of the Consoranni tribe. The inscription associates her with the Roman goddess Minerva: Minervae Belisamae sacrum Q(uintus) Valeriu[s] Montan[us e]x v(oto) [s(uscepto)], ‘Sacred to Minerva Belisama, Quintus Valerius Montanus (offered this monument) in accomplishment of his vow.’'\nPhotograph of the Saint-Lizier inscription mentioning Belisama. ( CC BY-SA 3.0 )\nIt is thought that the temple dedicated to her cult was in the northern part of the city. It seems that the settlement was frequently disturbed by battle, so Belisama’s help was vital. She may have been responsible for sending inspiration to warriors and aiding them in creating new strategies.\nThe second inscription gives more of an explanation of Belisama, but still doesn't answer all of the questions about this enigmatic goddess. It was discovered in 1840 in Vaison-la-Romaine (Vaucluse), the chief city of the Vocontii tribe. Similar to other Gaulish cities, they needed the support of a strong deity like Belisama. In this case, the Greek alphabet was used.\n''The inscription is of great interest, for it dates from around the 2nd or 1st c. BC and is in the Gaulish language and Greek lettering: σεγομαρος / ουιλλονεος / τοουτιους / ναμαυσατις / ειωρου βηλη- / σαμι σοσιν / νεμητον, ‘Segomaros son of Villū, citizen of Nîmes, offered this sacred enclosure to Belesama’. The name of the dedicator, Segomaros (‘Great Strength’ or ‘Great by his Victories’), and the name of his father, Villoneos, the meaning of which is unknown, are Celtic. Segomaros offers the goddess a nemeton, that is a ‘sacred enclosure’, ‘sacred grove’ or ‘sanctuary’. The nemeton was a sacred place of cult and veneration reserved to a deity, where human beings, apart from the initiates, were not allowed.''\nPhotograph of the \"Segomaros\" inscription in honor of Belisama. ( CC BY-SA 3.0 )\nThe inscription at Vaison-la-Romaine is carved on a hollow altar, located in the center of quadrangle lined with trees. Its edges were once marked by a fence, now part of the underground world of the remnants of the ancient site. Researchers who analyzed this inscription claim that there is no doubt that the name Belisama comes from the Celtic culture.\n- Ancient Roman Curse Tablets Invoke Goddess Sulis Minerva to Kill and Maim\n- Significance of Roman Curse Tablets recognised in Memory of the World Register\n- Christina, The Minerva of the North Who Abdicated Her Throne to Live Life by Her Own Rules\nThe Puzzle of Gaulish Beliefs\nDiscovering the real meaning behind the icons of the Gaulish religion and the true roots of the names and stories attached to them by folklore is not easy. A big problem for researchers is due to the language that was used. After many decades of work, they still cannot read some parts of several inscriptions. Thus, only with the advancement of research methods can we expect any new and more revealing information about the mysterious Belisama.\nTop image: Minerva arming herself. The Gaulish goddess Belisama has been linked to this Roman goddess. ( Public Domain )\nJohn T. Koch, Celtic Culture: A Historical Encyclopedia. Vol. 1-, Volume 2, 2005.\nBelisamam, A Gaulish and Brythonic Goddess: Summer Bright, available at:\nMinerve - Brigite – Belisama, available at:\nBelisama (‘the Most Powerful’?), available at:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f104e206-0a01-4bee-abaf-17f60a7f9fba>","<urn:uuid:866fe097-7dc8-4f97-b03e-ed660656f048>"],"error":null}
{"question":"As a DIY enthusiast, I've noticed similar sanding processes in different home projects. What's different between the surface preparation steps for stair tread refinishing versus bathroom grout repainting? I need to understand the key variations. 家里的楼梯和浴室都需要装修，想了解一下打磨过程有什么不同。","answer":"The sanding processes differ significantly in intensity and purpose. For stair treads, it's a thorough three-step process starting with rough 60-grit sandpaper, progressing to 80-grit, and finishing with 100-grit sandpaper to completely remove the old finish. In contrast, for bathroom grout repainting, the surface preparation only requires light cleaning and surface preparation to remove mold and mildew residue - you don't need to sand deeply, just prepare the surface for paint adhesion.","context":["Your home stairs can take a beating. Over time tread finish may wear away and vertical riser boards can get dented and scuffed. Tiling stair risers and refinishing the treads is something you could do on your own over a single weekend and will give your stairs a fresh new look. Here’s what you’ll need to take on this project:\nTOOLS & MATERIALS\n- Plastic Sheeting\n- Painter’s Tape\n- Belt Sander & 60-Grit Sanding Belt\n- Quarter Sheet Sander\n- Mouse® Detail Sander\n- Random Orbit Sander\n- 60-, 80-, & 100-Grit Sandpaper\n- Tile Nippers\n- Synthetic Pad Applicator\n- Cotton Cloth\n- Wood Stain/Polyurethane Floor Finish\n- Notched Tiling Trowel\n- Grout Float\n- Grout Sealer\n- Knife (If Using Mesh-Backed Tile)\n- Scoring Wheel & Nipper (If Using Mesh0Backed Tile)\n- Spacers (If Using Individual Tiles)\n- Score-And-Snap Tile Cutter (If Using Individual Tiles)\n- Stone Sealer (If Using Individual Tiles)\nWhen taking on this DIY task, be sure to equip yourself with the proper safety gear.\n- Eye Protection\n- Ear Protection\nRough sand treads. Refinishing your treads starts with a rough sanding. Using a belt sander start with 60 grit sandpaper. Sand with the grain along the length of each tread being careful not to bump the sander into the walls. Wrap a piece of 60 grit sandpaper around a sanding block to smooth the round surface of the tread and remove the finish. Work your way around the outside of each tread with a random Orbit sander, Quarter Sheet sander, and Mouse® Detail sander, all equipped with 60 grit sandpaper then vacuum the treads.\nFinish sand treads. Continue sanding the treads. This time with 80 grit sandpaper on a quarter sheet sander. Then also using 80 grit sandpaper smooth any areas you can’t reach with a sanding block or a Mouse® detail sander. Vacuum the treads again then carefully inspect the surface to ensure you’ve removed any ridges from the belt sander. Sand one more time with 100 grit sandpaper. Vacuum and wipe clean with a microfiber cloth.\nStain steps. If you plan on staining your steps you’ll do it now. But feel free to leave the treads natural if you’d like. Just apply a clear finish. If you decide to stain your steps start by taping off the areas where the treads meet the wall. Use a disposable foam brush to apply your stain, wiping off excess as you go with a cotton cloth. Let dry according to manufacturer’s instructions before moving on to the next step.\nScuff sand risers. Once your treads are dry you’ll start working on the risers. Scrape off any lose paint then lightly scuff with 80 grit sandpaper wrapped around a sanding block. Keep in mind that you’re not trying to remove paint or finish. You’re just dulling the surface. Use a vacuum and microfiber cloth to remove dust. Then tape sheets of paper to the treads to protect them while you tile.\nCut mesh-backed tiles. Measure the width and height of the riser you’re working on. If you’re using small tiles attached to a mesh backing you may only need to use a utility knife to cut sheets to fit. There’s no need to center the sheets. Just cut them to the correct height and butt the first sheet to the wall. Butt the last sheet into the opposing wall and mark where it needs to be cut. If you need to cut your tiles use tile nippers to trim them.\nApply mastic. Apply mastic with a notched trowel or plastic applicator to the riser in an even 1/4 inch layer. Holding the applicator at a 45 degree angle drag the notched edge across the riser to make horizontal ridges.\nInstall tiles. Place the first end tile or sheet of tiles on the mastic. Butt it against the wall. Wiggle the tile or sheet up and down while applying light pressure. You want to flatten the mastic ridges, not squeeze the mastic out. Lightly tap each tile with a rubber mallet to help secure them in place. If necessary, cut spare pieces to fit. Then move down the staircase tiling each riser the same way.\nGrout tiles. Before applying grout let the mastic dry according to manufacturer’s instructions. When you’re ready, mix grout according to instructions using a drill with a paddle attachment. Mix to a creamy consistency. Use your trowel to transfer grout to a rubber-faced float. Then smear on to the tiles working it into the joints and scraping if off the tile surface. Let the grout dry while you work on the next riser. Then after checking manufacturer’s instructions for dry time use a large damp sponge to clean tile surfaces. Repeat this process for all your tiles. As you work you may notice a hazy film as group residue dries on the tiles. To remove simply wipe with your grout sponge again. When the tiles are completely dry give them a final polish with a cotton cloth. Wait a week and apply grout sealer to protect against stains.\nStep back and admire your work. Your old stairs are new again.","Most of us want to be clean when we leave our bathrooms, so it only makes sense that we want our bathroom to be clean as well.\nIt’s inevitable that, in time, your bathroom will start to accumulate some dirt — which is why deep cleans are not only necessary but vital. A major culprit of bathroom dirt is grout.\nGrout is a type of filler for the joint between tiles and is usually composed of cement, lime, and colour pigment. Unfortunately, grout changes colour over time due to mold, mildew and dirt acclimated in the area, even if you use grout sealer in the bathroom. You can scrub away until your hands hurt, but it won’t make a difference if you don’t use the correct procedure.\nFor your personal hygiene, health and peace of mind, consider how you can keep your bathroom clean or, when it inevitably becomes dirty, how you can restore it to its glory.\nHow To Clean Bathroom Grout Mold\nCleaning out bathroom mold might not be the ideal way to spend a weekend, but once you walk into a fresh and sparkling bathroom, it will be worth it. Consider how you can clean and repair your bathroom grout.\nMultiple areas in your bathroom may need bathroom tile grout repair since different sectors can consist of tiles, such as walls and floors. As you may expect, different strategies will be needed for different surfaces. Follow these tips on creating a bathroom spa once you clean out the grout and make the place good as new.\nHow to Repair Bathroom Tile Grout\nBefore you begin any bathroom grout repair or cleaning procedure, ensure safety by wearing protective eyewear, a dust mask, and be careful when you use any products. Mold stuck in between the tiles can be toxic if you breathe them in. Also, use gloves when you use a bathroom floor grout cleaner to be on the safe side.\nBathroom grout tile repair doesn’t have to be as scary as it sounds. Despite the number of steps needed, you can repair bathroom tiles within one to three days, depending on how long the tiles take to dry. Here are the steps you can take to repair bathroom tile grout and get it ready for new grout.\nStep 1: Clean the broken grout\nMix one part white vinegar and one part water and dip a brush in the solution. Scrub the damaged grout until the dirt comes off.\nStep 2: Take out damaged grout\nRemove the top ⅛ inch of the damaged grout with a grout saw. Find out the correct way to use the saw and discard the top layer of the damaged grout to break them into pieces.\nStep 3: Clear out the grout\nUse a vacuum to clear the gap between the tiles to remove the old grout.\nStep 4: Clean up the tiles\nBefore you continue your bathroom grout repair process, clear out the surface by wiping it down with a wet sponge and then dry it off.\nHow to Repair Bathroom Shower Grout\nNow that you know how to repair bathroom tile grout, you can follow the same procedure for the shower. The only thing to note is that your shower might be more sensitive to grout since it contacts water more frequently, and water can create moisture and dampen the grout.\nFortunately, if you know how to repair bathroom shower grout, you can reset your bathroom’s healthy environment in no time.\nHow to Apply Bathroom Grout\nApplying new bathroom grout and a bathroom grout sealer can benefit your home in many ways. It can reduce the discoloration in your bathroom while giving your bathroom a new-home feeling.\nBefore you apply new grout, just be sure to clean out the old grout and mold. After the surface is clean, you can begin your new grout application process.\nStep 1: Measure the area\nTo fill in the tiles, you need to know exactly how much space there is between them.\nStep 2: Buy new grout and sealer\nLook for a new grout that best matches your existing grout. You can take off a chipped piece of your existing grout to the store and find the right match. In addition, ensure that you use a good-quality bathroom tile grout sealer in the process.\nStep 3: Apply new grout\nIt’s finally time to get started on your bathroom tile grout repair! First, use a grout float, which is a thick rubber pad with a handle. Scoop out the grout mixture with the float, press it into the gap and spread it out. If it spreads on the surrounding tiles, you can clean it up later.\nStep 4: Remove extra grout\nHold the float at a 45-degree angle with the floor, and drag the edge of the float on the gap to scrape off excess grout.\nStep 5: Let it set\nLet the grout mixture dry off for approximately 30 minutes, and use a damp sponge to clean out any excess grout. Don’t use a soaking wet sponge, or else your hard work might go down the drain if the grout comes off.\nYou may need to leave the mixture on for longer, depending on the manufacturer’s instructions. Since sponges play a significant role here, check out our list of the best silicone sponge reviews to pick a sponge that will ensure maximum efficiency.\nStep 6: Apply bathroom grout sealer\nApply bathroom tile grout sealer to finish off the process. Using a grout sealer in the bathroom will make the foundation safer and water-resistant.\nIs Bathroom Grout Waterproof?\nIt may seem like grouts in the bathroom are ideally waterproof. However, that is not the case. They may be water-resistant, but that won’t stop the damage. Despite your attempts to protect the grout from water, the areas can still develop bathroom grout due to mold. Luckily, you can get rid of mold in your shower using a bathroom grout cleaner to scrub out tough stains.\nIf you want to waterproof your shower, you have to look into the procedure before placing your tiles and grout. If you use grout and sealant to protect your tiles, water will still find its way around them. The best way to waterproof your bathroom is to install the tiles correctly. It is also beneficial to do a flood test before installing the tile to maximize its protection level.\nAfter following the procedures to ensure that your tiles are safe, the grout will still change colour over time. You can fix that by using a bathroom floor grout cleaner. You can apply the same strategy to clean out the mold in the shower as a bathroom tile grout cleaner hack.\nCan Bathroom Grout Be Painted?\nPainting bathroom grout can quickly refresh your bathroom and save you the cost of replacing all the grout. Although replacing grout might be the best way to replenish your tiles, painting them can be a quicker alternative. Here are the steps you can take to revamp your tiles and give them a new look.\nStep 1: Prep the surface\nLook for areas with broken tiles or grout and replace whichever is possible. Use a tile cleaner to wipe off the surface and remove any mold and mildew residue. Make sure you give the spray enough time to rest on the tiles. You could also use a bathroom tile grout cleaner to prepare the surface.\nStep 2: Tape tiles\nUse painter tape to cover the tiles to prevent paint from touching their surface. This step will enable you to achieve a cleaner look and save time cleaning up excess paint.\nStep 3: Start painting\nIt might be helpful to use paint that is a shade darker than your grout to cover any stains. Paint the grout slowly and use a brush with the best drill brush reviews to ensure that the work is clean. Apply a second or third layer of paint if necessary.\nStep 4: Clean grout\nRemove the tape and use a toothbrush or sponge to clean the area.\nStep 5: Seal grout\nIf you don’t know how to seal bathroom grout, you might want to take it slow. Apply a clear liquid sealer in an even line. Make sure there is no dirt on the grout surface as it can get stuck under the seal.\nIf you can figure out how to seal bathroom grout and paint it, it can last a long time. However, if the tiles are in awful condition, you may need to re-grout them.\nCan You Power Wash Bathroom Grout?\nIf you’re tired of scrubbing every corner of your tile and leaving the bathroom exhausted, power washing your bathroom grout might be a suitable solution. Although it is not the ideal way to get into every section and dig out the dirt, it can be an effective way to deep clean your apartment and the bathroom.\nPower washing bathroom tiles is a safe way to perform bathroom grout cleaning within a shorter time. If you want to figure out how to clean grout from tiles, you might need to know what steps to take and what to stay away from. Here are some things to avoid:\nIt can be challenging to operate a gas-powered washer during your bathroom grout cleaning procedure. Close contact with water can be harmful to the device and the person operating it, it is best to use an electric-powered washer for bathroom grout cleaning.\nIf you aren’t sure about how to clean the grout in shower without damaging the paint, you may want to skip using a power wash. A power washer will remove all kinds of surfaces, including any new layer of paint. However, you can pull the nozzle far away when you clean to reduce direct contact with the surface.\nHow to Clean Grout on the Floor & Other Areas\nWhether you’re cleaning the floor or figuring out how to clean grout from tiles, empty the entire bathroom. Also, make sure to turn on the extractor fans in the bathroom as power washers can cause humidity, leading to dampness and mold. Once your bathroom is ready, turn the machine on and get cleaning.\nWhat Really Cleans Grout?\nIf you’ve tried every method to clean the mold in bathroom grout and nothing worked, you might be wondering what you can do next. The thought of bathroom grout mold shouldn’t be stressing you out, so, here are some tips to follow to ensure that your bathroom becomes squeaky clean.\nHow to Clean Grout in the Shower\n- Wash and scrub the grout.\n- Use vinegar, baking soda, and hydrogen peroxide when needed.\n- Try more homemade recipes or buy a commercial product.\n- Use good-quality brushes to reach every corner.\n- Don’t rush the process.\nHow to Clean Grout on the Floor\n- Vacuum the dirt – Before you proceed with the mold and bathroom grout cleaning procedure, consider our tips to vacuuming your floors to remove dirt.\n- Scrub grout with water and brush.\n- Use vinegar, baking soda, and hydrogen peroxide when needed.\n- Try commercial products for cleaning.\n- Take your time with it.\nHow to Clean Mold in Bathroom Grout\nMold and bathroom grout are like two peas in a pod. If your bathroom surface grows mold, it can damage your bathroom grout.\nIf you notice black mold on your bathroom surface, for one, it could be dangerous for the home’s members. Black mold can be toxic, and it is typically visible in humid or water-damaged environments. Once you identify the type of bathroom grout mold, it can be simpler to get rid of it.\nWhat You Need to Do:\n- Wipe off mold and bathroom grout with an antifungal surface cleaner and cloth or sponge, then brush off any stubborn areas.\n- Mix equal parts bleach and water, fill in a spray bottle and spray the mixture on the surface.\n- Let it sit on the area, respray, and scrub with a toothbrush.\n- Spray vinegar in a bottle, apply on affected areas, keep for about an hour, and use a cloth to clean the area.\n- Wash the surface and wipe it down.\nPreventing Mold Growth:\n- Reduce humidity in the bathroom by using an exhaust fan or keeping windows open.\n- Wipe moisture from windows and mirrors frequently.\n- Frequently clean your bathroom with antifungal cleaners.\n- Keep vinegar and other cleaning products handy.\nBathroom Grout and Tile Cleaner\nA tile cleaner can enable you to remove mold from bathroom grout. If you know how to clean hardwood floors, it may be simpler for you to clean bathroom grout. Once mold gets built on your tiles, you need to get started on removing mold from bathroom grout.\nFollow the steps mentioned above to remove mold from bathroom grout within a short time. If you’re still unsure how to clean bathroom grout mold, you can look at YouTube videos or hire a professional to perform the service for you.\nTo sum up how to clean bathroom grout mold, here are tips that might come in handy:\n- Use both DIY and professional cleaning products to remove mold bathroom tile grout.\n- If you need to, then go ahead and google how to remove mold from bathroom grout ten times before rushing a process.\n- Removing mold from bathroom grout is not an instant task, so take your time with it and ask your family to help if needed.\n- As soon as you notice tile discoloration, remove mold bathroom tile grout to prevent further damage.\n- Make sure to use the right equipment in the process.\nIf you’re trying to figure out how to remove mold from bathroom grout or trying to find the best bathroom grout cleaner product, proper research is key to all problems. The sooner you solve the issue, the more minor problems you have to tackle for a bathroom revamp."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:ab9f0956-4628-4ae2-b907-184bdb8e68fc>","<urn:uuid:27a73120-98f8-40c5-ad48-b17892981e1b>"],"error":null}
{"question":"How do zinc and clay tile roofing materials compare in terms of their durability against weather damage?","answer":"Zinc and clay tile have different durability characteristics against weather damage. Zinc naturally develops a protective patina (zinc hydroxyl-carbonate) that blocks moisture and chemicals, allowing it to self-heal from scratches over 2-5 years and resist corrosion. Clay tile is highly durable and fire resistant, standing up well to UV sunlight damage and won't rot or rust. However, while zinc is malleable and flexible, clay tile is weak against impact damage and can shatter from hail, falling tree branches, or even someone walking on the roof. Both materials are long-lasting, with zinc lasting 80-100 years and clay tile lasting around 50 years.","context":["Many times, the most innovative minds in architecture aren't the architects themselves. They can come in the form of students, researchers and in this case - providers. We recently asked VMZINC, a company that provides material solutions for architects, a few questions about the use of zinc in architecture, the Stonehenge Visitors Center and more.\nAD: How do you create intricate shapes with zinc, as seen in the roof at the Stonehenge Visitors Center?\nVMZINC: We customized our bent metal panels to meet our clients’ needs and specifications for seamless construction of intricate shapes, whether they are straight, curved, folded or all of those. When they have determined the specific needs of their project, then we look specifically at the size and angle specifications, as well as how and where the panel will be used, as we help them make the best determination of what amount of material they will need and as we place the order. It is of critical importance to ensure that panel lengths are correct, because they are too short, adding more material may be difficult and detract from the desired aesthetic. Panels are too large for a project means the zinc will be wasted unnecessarily, which we try to avoid both for environmental impact as well as for keeping the product affordable for our clients.\nAs soon as our customer accepts our terms, conditions and pricing for the products that they want to purchase, their order is entered into our fabrication system, where one project coordinator (PC) will assume responsibility for the project as well as for all communication with customers to answer any questions or address any concerns that they might have.\nWe have very high standards for each of our quality checks and a low tolerance for any mistakes made in the processing of customers’ orders and handling of their materials. These quality controls have allowed us able to create an award-winning assortment of unique designed protects in all types of terrain and climates.\nAD: What are the benefits of using Zinc in an architectural setting?\nVMZINC: Zinc performs the basic duties you would expect of any quality metal used for building. It keeps harmful elements out, safely protects what is housed within the unit and contributes to a building’s overall excellence. It is lightweight but strong. Zinc is compatible in association with many materials such as wood, brick and glass. In fact, its capacity to blend with or even highlight other materials, and to heighten their minerality (their natural aspect or their industrial dimension) is one of its advantages over other materials. Additionally, zinc is incredibly malleable. A 30-foot zinc panel can form to a gradual radius without the panel needing to be pre-curved using any metal tools or machinery.\nAlso making zinc appealing to architects is its relatively low startup costs, especially when measuring its environmental and maintenance savings. Zinc requires little to no maintenance over the life span of the panel due to the self-protective patina that forms on the material to heal any scratches on the surface.\nFinally, zinc is naturally smooth and shiny when it comes out of the mill. It is also available pre-weathered or in our newest product, AZENGAR, which gives zinc an engraved look and feel. We also offer colored zinc called PIGMENTO, which is available in red, green, blue, and brown and allows architects and homeowners to create colorful designs with an expanded palette that blends perfectly with other materials. QUARTZ-ZINC, is a factory formed pre-weathered zinc that gives the metal an aged look, while ANTHRA-ZINC is dipped into an acid bath that turns the product into a rich charcoal black color.\nAD: What different architectural applications does zinc have?\nVMZINC: Many architects incorporate zinc into their roofing needs because it is more durable and cost effective than other metals. Zinc is malleable, flexible and suitable for all roof pitches above 5 percent. These characteristics allow architects a great deal of freedom of expression for roof design when using zinc.\nZinc also is popular in façade applications as an extension of the building envelope from the roof down to the walls. All VMZINC facade systems belong to the rain screen family, cladding with a ventilated air space for sustainable performance. They come in straight, curved and complex designs that can be laid horizontally or vertically.\nAdditionally, we offer a standard range of roof ornaments such as weather vanes and dormers with our partner, Ornametals. Ornametals also helps us provide zinc rainwater systems that feature special kits with gutters, concealed brackets, stop-ends, corners, running outlets, downpipes and self-locking pipe brackets. Another partnership with W.P. Hickman allows our portfolio of products to include zinc coping caps featuring a two-piece design with a snap-on fascia and rigid termination bar to assure that wind and water stay out of the roof membrane. These coping caps come in both standard and customized sizes.\nA recent development has been the inclusion of our residential zinc countertops and commercial bars. Zinc provides a pristine surface for various areas in the kitchen and other areas of the home, ranging from breakfast bars to bathroom vanities. These versatile countertops give architects and homeowners surfaces that are of high quality, and provide a clean and modern look and permanence.\nAD: Is Zinc considered a sustainable product?\nVMZINC: Most definitely. Zinc ranks as the 24th most abundant element in the Earth’s crust. It exists naturally in air, water, and soil. Most rocks and many minerals contain zinc in varying degrees. It is relatively easy to mine thanks in part to being so abundant.\nA big sustainability advantage for zinc over other metals is that it takes much less energy to refine zinc than aluminum, copper, or stainless steel. For instance, the energy required to produce zinc from ore is a quarter of that needed to make aluminum and half of that needed for copper and steel.\nAD: Is Zinc Recyclable?\nVMZINC: Yes, zinc is 100 percent recyclable even at the end of its life. Because scrap of VMZINC has a high metal content, many new products can be created from recycled zinc, including but not limited to zinc oxides used in paint, rubber production, and pharmaceutical products.\nAD: What is the life span of Zinc in an exterior setting?\nVMZINC: Zinc will typically last between 80 and 100 years in exterior settings, depending on its application and location. This includes zinc used for roofing. Though results can vary due to outside factors, typically zinc roofs can last up to 100 years in rural areas, while zinc walls may last more than a century. In Europe, where zinc use is the most prevalent, roofs, gutters and wall systems have been known to last for generations.\nHelping zinc’s durability is the fact that it naturally develops a protective patina that increases its lifespan as well as allows it to withstand harsh elements over decades. This process makes zinc low maintenance over the years as well, as zinc redevelops or “self-heals” any imperfections thanks to the patina. Officially called zinc hydroxyl-carbonate, this patina blocks moisture and chemicals from penetrating it. If that protective layer is ever scratched, the hydroxyl-carbonate will reform over time (typically taking two to five years, depending on the climate), making zinc naturally resistant to corrosion.","Where do you start when choosing a new roof for your home? There are many materials to look at, each with its own set of pros and cons. Take a look at this guide to help you figure out which roof is the right choice for your home.\nAn Overview of Roofing Materials\nThese days, you can find asphalt shingle roofs in both organic and fiberglass varieties. Organic asphalt shingles are made with a mixture of paper, ceramic and asphalt. Fiberglass shingles are made with the glass fiber mat, a waterproof filler and a coat of asphalt.\nAsphalt shingles are one of the most common roofing choices for a variety of reasons. First of all, they are inexpensive to purchase and easy to install. Since shingles are a durable material, they often come with up to a 30-year warranty. They also come in a wide variety of colors and styles, and they are flexible if your roof has a curved design.\nIf your climate is subject to extreme temperature shifts, it can be hard on an asphalt shingle roof. The shingles will sometimes warp or crack. There is also the potential for shingles to lift or blow away in a windstorm. In humid climates, asphalt roofs often grow moss or algae. If left unchecked, the growth will deteriorate the roof in a matter of years.\nAsphalt Copper Shingles\nOne of the newest roofing varieties is asphalt shingle that has been treated with copper granules. These shingles come in mostly neutral shades such as grays and browns. They cost a bit more than regular asphalt shingles. Asphalt and copper shingles last anywhere between 20 and 30 years.\nThe major benefit to asphalt copper shingles is their ability to prevent moss and algae. The copper granules in this type of roof stop moss growth and the dark streaks that are caused by algae.\nSince the copper is designed to leach out over time, you will only see about 10 to 12 years of protection against moss and algae. Just like regular asphalt shingles, these shingles can lift or blow away in high winds. Temperature extremes can cause warping and cracking.\nWood roofs come in two types: shingle and shake. Wood shingles are machine-made so that each shingle is uniform in size and shape. Shake roofing is hand-split from wooden blocks, which gives the material a more rustic look. Wood shingles and shakes are typically made of red cedar, although it is possible to find pine shingles and shakes. Many people enjoy wooden roofing because of the weathered look it develops over the years.\nOn the right home, a wood shingle or shake roof can be gorgeous. Since wood is a variable material, each wooden roof is unique. Another advantage is that wood roofing tends to be a good insulator while allowing a house to breathe. These roofs last for anywhere between 30 and 50 years.\nWooden roofing material is expensive and more difficult to install than most other roofing types. Wood shingles and shakes also require a lot more maintenance. The wood can mold, rot, grow moss or suffer insect damage.\nTile and Cement Roofing\nIf you’re going for a Spanish, Southwestern or Italian look, clay roofing might be the right choice. This type of roofing material is generally made from clay, but you can also find tiles that are made of sand, ceramic or cement.\nClay roofing tile offers more choices than any other roofing material. The tiles can be curved, diamond-shaped, hexagonal or square. You’ll also have access to a huge range of colors, from natural shades to rich jewel tones. It stands up well to damage from UV sunlight, and it won’t rot, rust or suffer insect damage. Tile roofs are fire resistant and highly durable, often lasting as much as 50 years.\nClay roofing is very costly and it is extremely heavy. If your home needs structural additions to support the weight of a tile roof, that will drive up the installation costs even more. While clay roof holds up well to weather, it is weak against impact damage. Hail, tree branches or even someone walking across the roof can shatter the tiles. You will also have a more difficult time if you plan to install any rooftop equipment such as a satellite dish or antenna.\nSlate roofing is a type of shingle made from stone. Slate roofs are commonly found on older homes. Today, owners of upscale homes like to install natural stone roofing. Slate ranges in color from dark gray to reddish-brown, and it can be cut to a variety of shapes. While square slate shingles are most common, you can also find diamond cuts and rounded slates.\nSlate is fire resistant and it won’t rot, rust or sustain damage from insects. It also lasts for a very long time. Different slate quarries offer different life expectancies. Slate mined in Pennsylvania is usually expected to last at least 50 years, while Buckingham slate from Virginia has been known to last up to 175 years. Vermont slate roofs have an indefinite lifespan. There are accounts of homes with Vermont slate roofing that has lasted more than 200 years.\nSlate is similar to clay tile in that can shatter easily. While slate is a little stronger against hail damage, it can break if you walk on it. Like clay tile, slate is also very heavy – your home may need additional support to bear the extra weight.\nWhen people think of metal roofs, an image of old corrugated tin comes to mind. However, roofing technology has come so far that the metal roofs of today are quite different. You can still find the traditional corrugated styles in a variety of durable materials. You can also find metal roofing materials that emulate stone, wood and shingle.\nAluminum is becoming more popular as a roofing material. It is available in many different styles, from shingles to a tiled look or even the old-fashioned look of a tin roof. Aluminum roofing materials also come in many colors, from natural shades to bright tones. You can even find aluminum shingle that emulates the look of copper.\nAluminum roofing is a corrosion-resistant material. Since it is a highly recycled metal, most modern aluminum roofs are made of between 25 and 95-percent recycled aluminum. Another advantage to an aluminum roof is its longevity – these roofs often last for 50 years or more.\nWhile aluminum roofs aren’t as costly as some of the high-end roofing materials, they will cost you more than asphalt shingles or steel. They can also be very slick, which makes it dangerous to walk on them, particularly if they are wet. During a rainstorm, an aluminum roof can make quite a bit of noise.\nSteel roofing is one of the most popular choices available these days. Most steel roofing comes in a corrugated design, but it is possible to find steel shingles or textured styles. It comes in nearly any color you can imagine and it is very durable. Some insurance companies will give homeowners deep discounts for the installation of steel roofing.\nSteel roofs are easy to install, which lowers your installation costs. They are fireproof and they are resistant to wind, hail, and other types of damage. Modern steel roofing material is rust and corrosion resistant. Like most metal roofs, a steel roof should last at least 50 years.\nThere are a few places that still sell low-quality steel roofing that is prone to rusting. You’ll need to make sure you get a good quality product with a long warranty. Steel roofs also tend to make a lot of noise when it rains. The biggest issue with steel roofing is snow. When you have a thick snow cover that starts to melt, it will break away in huge chunks that can tear down your gutters. You can install snow guards as a precaution, but they detract from the uniform look of a steel roof.\nCopper roofing is usually used on high-end architecture. You’ll find copper on steeples and ornate gabled homes. Copper shingles are a popular style on upscale country cottages. It is an interesting roofing material in that it will never look the same from year to year. With time and weather, it will build a patina that ranges between dark reddish-brown and pastel green or blue. On a copper shingle roof, the patina can be different from shingle to shingle, creating its own interesting natural pattern.\nCopper roofing is a beautiful addition that can seriously increase the appeal and resale value of your home. It is are also resistant to fire, wind and water damage. A properly maintained copper roof can last indefinitely. In Europe, there are many structures that boast copper roofing dating back to the 18th century. Unlike aluminum and steel roofing, copper roofs are not particularly noisy during rainstorms.\nCopper roofing is one of the most expensive materials. As temperatures rise and fall, copper – more than any other material – will expand and contract. You will need to hire a very experienced contractor that is capable of accounting for temperature shifts to get the most from your copper roof.\nCosts of Roofing Materials\nRoofing costs are highly variable depending on the materials you select, their quality and the size of your home. The following estimates are based on an average-sized ranch home with a 1,700 to 2,100-square foot roof. These estimates also account for professional installation costs and the removal of your existing roof.\n- Asphalt shingle will cost between $1,700 and $8,200. For asphalt roofing treated with copper, your costs will be towards the high end of this average.\n- Cedar shakes and shingles can run anywhere between $6,800 and $20,000. Pressure treated shakes and shingles will cost more, but they will also last longer.\n- Clay and cement tile roofing ranges between $9,000 and $21,000. Clay tiles are usually on the cheaper end while cement and ceramics cost more.\n- Slate roofing is costly at between $17,000 and $84,000. Much of this cost will depend on the quality of the slate – you can expect Vermont slate to cost more. Another factor that drives the price up is the structural reinforcement that may be necessary.\n- An aluminum roof can cost between $7,000 and $20,000. However, the longevity and durability of an aluminum roof can offset the extra cost compared to less expensive roofing types.\n- Steel roofing can cost between $6,000 and $17,000 to install. Despite the fact that it costs more than shingle, it is a popular choice because of its extreme durability.\n- As the most expensive choice, copper roofing is typically reserved for high-end homes and historic buildings. A 1,700 to 2,100-square foot copper roof can range between $16,000 and $42,000.\nHow Location and Climate Affects Your Roofing Choice\nYour location plays a huge role in the type of roofing that will be best for your home. For instance, people living in coastal areas sometimes avoid metal roofing options because salt spray can cause them to corrode more quickly. Shingle roofs tend to deteriorate faster in areas with extreme temperature shifts, and humid climates can take their toll by causing moss and algae growth. Copper roofing is also susceptible to cracking and shifting as temperatures rise and fall. In hot climates, such as the southern United States, options like steel and aluminum roofing are often preferred as a reflective heat barrier.\nNatural Disasters and Your Roof\nThere is no roofing material that is completely impervious to natural disasters. However, each type has its own strengths and weaknesses. Consider the types of natural disasters that are common in your area and choose your roof accordingly.\nFor example, if you live in an area that is prone to wildfires, choose metal, slate or tile roofing since it has higher fire resistance than asphalt or wood. Areas with high seismic activity should avoid slate and clay tile roofing – a minor seismic shift can cause a lot of damage and put you at risk for further damage or even injuries due to falling stone or tile. If you live in an area with a high likelihood of high winds from thunderstorms and hurricanes, shingle roofing may not be the best option. Heavy materials like stone and clay will not blow around, while sheet metal roofing is nearly impervious to wind damage.\nHail is another consideration. Because of their flexibility, asphalt shingles are the least likely to suffer massive hail damage. Wood shingles and shakes are also relatively hail resistant. While hail may not puncture metal roofing, it can leave dents. In hail-prone areas, you may prefer to use a textured metal roof to hide the marks left by hailstones. Clay, ceramic and stone roofing should be avoided if you have regular hailstorms since even small hailstones can shatter slates and tiles.\nMaking Your Decision\nFor most people, cost is the prime consideration. Once you’ve thought about cost versus value, consider your climate, location and common natural disasters to make the best choice. Aesthetics are also important, but keep in mind that each roofing type comes in a wide variety of colors, textures and styles – you should be able to find a design you like in the material you need.\nLatest posts by Jake Hardy (see all)\n- Roof Snow Removal 101 – Preventing Collapse - December 16, 2014\n- Common Winter Roofing Problems You May Experience in Utah - November 24, 2014\n- Seven Things You Must Know Before Choosing a Roofing Company in Utah - October 19, 2014"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a205c261-aed0-402b-9058-1e3c875bd3dd>","<urn:uuid:5dc45ff9-a1c2-4c0d-add0-031fc053166b>"],"error":null}
{"question":"Which countries blamed other nations for Spanish Flu? Want examples","answer":"Several countries blamed others for the flu: Persia called it British Flu, Brazil called it German Flu, Senegal called it Brazilian Flu, and Japan called it 'Sumo Flu' (because it first appeared at a sumo wrestling tournament).","context":["Reading Pale Rider, Laura Spinney’s 2017 book about the 1918 flu pandemic, during the weeks of the UK government-imposed coronavirus lockdown has drawn my attention to several historical parallels.\nFor many decades after World War One, the influenza pandemic which came in its final year was a missing part of the narrative. In 1975, Paul Fussell’s widely-praised work of history The Great War and Modern Memory gave it one single mention. It was never included in any of those many World War One dramas until Downtown Abbey in 2011. Spinney points out that the subject as a whole has attracted sustained academic interest only since the 1990s.\nThere were actually three waves of the pandemic. The first in spring 1918 was generally mild, the second in the latter half of the year was mostly severe and the third in the first part of 1919 was moderate, somewhere in between. However not all countries experienced three waves. The global number of deaths has always been undercalculated: in the 1920s it was thought that figure was 21 million, more than had been killed on the battlefields, but, by the turn of the millennium 80 years later, that had been revised to, at minimum, 50 million.\n“There is only one thing we can say with something close to certainty: the Spanish flu did not start in Spain,” says Spinney. Spain was granted that dubious accolade by the victorious powers of Britain, France and the USA because it was neutral during the war and therefore did not censor press reports. This was not the sole example of blaming, however. Persia called it British Flu, Brazil called it German Flu, Senegal called it Brazilian Flu – and Japan called it “Sumo Flu” because it first broke out at a sumo wrestling tournament.\nThe disease may have originated on the Western Front battlefields. Equally it may have begun in an army camp in Kansas, USA, and travelled to Europe, or in China among the Chinese Labour Corps who went to serve in Europe. Only if the first scenario is correct, then, might the pandemic properly be described as a true product of World War One. However, it was certainly more international than the war as well as more deadly, killing more people on every continent except Europe. Spinney provides detailed synopses about the outbreaks in such disparate locations as Brazil, China, Spain, New York, Persia, Russia, Alaska, South Africa and India.\nThere are several places in the book where Spinney’s words prefigure eerily those of 2020’s politicians, medical advisers and journalists. For example, “We’re all in this together”, that popular government rallying-call, is a phrase used by Spinney to summarise the human tendency of “collective resilience”, joining a group in times of individual danger and seeing that group as part of him or herself. One reason for the development of “collective resilience”, she suggests, was the earlier centuries of preaching and practice about the importance of charity and family and community by the three major monotheistic religions of Christianity, Judaism and Islam. So it is interesting, and maybe ironic, then, to recall the comment of the Conservative politician Nigel Lawson thirty years ago that the National Health Service was now the nearest thing which secular Britain had to a national religion. Evidence of this “religious” practice in 2020 has been widespread celebration of the work of doctors and nurses and, equally, widespread criticism that their governments have failed to provide appropriate equipment and protective clothing.\n“Social distancing” is a phrase most of us had never heard until this year. Spinney credits the British scientist Ronald Ross as one whose research encouraged the idea as a measure to quell the spread of Spanish flu. This included closure of schools and churches and places of entertainment and the banning of large gatherings. “This was a time before civil rights movements,” she reminds us, “when authorities had more licence to intervene in private citizens’ lives, and measures that would be perceived as invasive or intrusive today were more acceptable”. I share her perspective, so I was astonished that the UK government introduced such restrictions so quickly – and that they were so willingly and widely accepted.\nIn 1918-1919, “the vast majority… experienced nothing more than symptoms of ordinary flu.” Death tended to follow when Spanish flu was aggravated by bacterial pneumonia. However, the spread was invariably facilitated by prevailing conditions of overcrowding, poor hygiene, and poor health, whether in the tenements of New York or the mines of South Africa or the ships and trains carrying the Chinese Labour Corps across the Pacific Ocean and Canada towards Europe. The UK has seen many improvements in public health and housing within the last century, but it was striking that the government’s Office for National Statistics felt it could not ignore the evidence that coronavirus has had a worse impact within the black and minority ethnic communities, who tend to live in areas of greater social deprivation and to earn lower wages.\nEvidence has also been published that a high percentage of the current deaths have been of people who already suffered from weaknesses caused by obesity and poor diet, even though the media are often more attracted to the dramatic exceptions where the cause of death is harder to find.\nWith Spanish flu, such factors as your country, your underlying health and your living conditions explained to a great extent why you became a casualty, but there did seem also a large element of tragic accident. After all, in some parts of Asia you were 30 times more likely to die than in some parts of Europe. Within the last twenty years, says Spinney, scientists have found evidence that some people have a genetic weakness to the flu virus, for example in the way they are unable to naturally produce interferon, the protein which defends the body against viruses. Other areas of similar genetics research have been reported recently about coronavirus.\nSpinney describes the Spanish flu pandemic as something “thoroughly ancient” which undid centuries of progress and took us back to the Middle Ages. “It was if the entire population of the globe…had been transported back several millennia.” No surprise that our own further advanced world has been traumatised even more thoroughly by coronavirus.\nHowever, back then, people were more accepting of life’s trials. opines Spinney. Perhaps, too, they were a bit braver, because death was usually closer and more frequent, and maybe, too, because religious faith was stronger.\nAt time of writing, most people in Britain seem to prefer that the state-imposed quarantine, despite all its stresses and restrictions, be extended, and this seems to be emboldening each of the UK and Scottish governments, both now nervous rather than cautious, to follow them.\nReference : Spinney, Laura (2017) Pale Rider: the Spanish Flu of 1918 and how it Changed the World London: Jonathan Cape"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:2ec378b8-e1f8-4456-92eb-0dea9b15978f>"],"error":null}
{"question":"What innovations in facade attachment systems exist today, and how do they compare to historical Norwegian architectural techniques?","answer":"Modern facade systems like Eclad 1 use an aluminum grid system with concealed undercut anchors for thin stone panels, allowing quick installation by simply slipping slabs into place and sinking screws into predrilled holes. This is twice as fast as traditional systems that use cumbersome cramps, straps, and clips. In historical Norwegian architecture, buildings relied on different attachment techniques, primarily using log construction introduced from Finland and Russia during the Viking Age. These structures evolved to use corner details with long grooves lined with moss to keep out wind, and later incorporated decorative paneling that protected the logs while allowing for elaborate carved designs. This historical evolution shows a progression from basic structural solutions to more sophisticated attachment systems.","context":["Back in 2014 Hugo Vega, vice president of sales North America at Polycor, noticed that the architects he was calling on were lacking a thin stone veneer that was light enough and strong enough for cladding large scale architectural projects. After some R&D within the company, Polycor went on to release its 1 cm reinforced slabs and Vega returned to his architects in triumph. Only their response was, “That’s great, but we need a way to hang it.”\n“The 1 cm product was a great innovation, but there was no way to apply it quickly and easily on large scale projects,” Vega said.\nSo the Polycor team dove back into development.\nMeanwhile another response began to percolate in the A&D world. In a bit of a surprise to Vega, the 1 cm slabs sales took off in the residential market where designers and their clients jumped at the chance to do feature walls in showers, full slab backsplashes and seamless vertical fireplaces. (You can see those designs in this lookbook.) At a third of the weight of the usual 3 cm material they were dealing with, fabricators were no longer breaking their backs to muscle a full slab up over a counter to install a backsplash. At 10 times the flexural strength, (thanks to its polycarbonate composite backing) gone was worry that the vertically oriented slab on the fireplace would crack on install.\nThe residential market was onboard for thin stone.\nAn example of a backsplash fabricated from a continuous slab of ultra-thin White Cherokee American marble.\nThat was great news, but Vega’s customers are in commercial not residential. So he continued to mull over this problem of adhering thin stone cladding to the exteriors of architectural projects. From time to time he would bump into the team from eclad at job sites where thicker panels of Polycor marble and granite were being installed with existing eclad systems, structural supports laid over existing facades in a modular fashion. A world leader in stone cladding systems, eclad has been creating and refining cladding systems since the 1990s. They too were seeing the same need in the market as the Polycor team - a fast and efficient way to clad with ultra-thin slabs. And so together the companies decided it was time to team up to bring a comprehensive thin stone cladding system to market.\nWhat they developed is a seamless system that saves time, labor and money: Eclad 1.\nUltra-thin American Black granite appears to float, supported by the invisible Eclad 1 structure.\nThe new design is based on an aluminum grid system in combination with undercut anchors attached to the back of the 1 cm panels so they remain concealed when using such thin stone. The panels are available up to 9 feet by 5 feet and weigh only six pounds per square foot on average, making the installation process an easier task.\nLEARN MORE ABOUT STONE FACADE SYSTEMS\nAnchors remain hidden for an unobstructed surface.\nInstallation at twice the pace\nThe complete system provides pre-drilled lightweight stone panels over a protective cladding structure that makes once heavy stone panels easier to install. Traditional cladding systems rely on thicker stone combined with cumbersome cramps, straps and clips. With Eclad 1 installers simply slip slabs into place and sink screws into the predrilled holes.\nAn example of a small scale Eclad 1 system mock up.\n“It’s basically a different way of installing the stone,” Vega said. “With traditional cladding systems, the anchors have to be installed one-by-one. The process is more labor intensive. On average, it’s twice as fast installing panels using the Eclad grid system.”\nOne system, three versions\nOn interior walls the system enables designers to define spaces with floating walls or elevate the look of existing walls with dramatically veined large format slabs of marble or granite. Granite panels are suitable for both interior and exterior applications, marble for interior only.\nAs soon as employees and guests walk in to the new Quebec headquarters, they’re greeted by a floating wall of 1 cm American Black granite.\nThe grid components are made of durable, corrosion resistant, non-combustible aluminum which withstands extremely high heat and will not burn. “The advantage of our thin panels against other systems such as honeycomb panels is that the stone is mechanically anchored to the system, and not glued,” Vega said.\nOpen system prolongs facade life\nOn a building’s exterior the Eclad system acts as a ventilated rainscreen, which assists in keeping the structural wall dry. This open system design allows outside air in and helps divert water and humidity that may penetrate behind the natural stone panels. The moisture is diverted through the horizontal and vertical joints and channels on the grid, helping to maintain the load bearing wall integrity.\nImprovement over older methods\nDesigned for flexibility the system includes three versions suitable for cladding over different surface materials. All are suitable for large scale architectural projects, new construction, retrofits and restorations.\nPolycor got a chance to use the new Eclad 1 recently for its own retrofit project when the company moved its headquarters into a 18-century building in Quebec city. A sprawling open space with floor to ceiling windows, exposed brick and wide open industrial spaces. Those elements are the stuff of a designer’s dream, but present a challenge when you want to showcase a portfolio of the world’s oldest and heaviest building materials, and define spaces for the staff to actually work.\nOther feature walls are incorporated throughout the building in various materials, finishes and sizes including 1 cm Pearl Grey honed marble from the Georgia quarry, that’s bookmatched to emphasize the dramatic veining. Take a virtual tour of the Polycor headquarters here.\nPolycor’s architect Étienne Bernier of Hatem + D mounted American Black granite slabs with the Eclad 1 to create an interior feature wall that shows off the unique linear veining of the stone. Mitered corners give the appearance of weight but the slabs are actually only ⅜” inch thick.\n“Before we got together with eclad, it was extremely difficult explaining to people the new system — explaining how it’s attached to the wall, how it works,” Vega said. “We’re always trying to work with (those who are) open minded, easy to work with — who have the same kind of thinking out of the box innovation. ”\nNow when Vega calls on architects interested in thin stone cladding he has a complete solution. Stone at a third of the weight and a installation system at twice the speed.\nLearn more about the thin stone Eclad 1 system. Request a demo.","The detailing around the door is influenced by medieval church art that helps to date the loft to around 1300 AD. The detailing was then highly coloured with yellow ochre and rust red pigmented paint, traces of which can still be seen.\nThe door on the lower level, leads to the bu where food was stored. It was painted with tar crosses to protect the contents. The simple stair to the right leads to the gallery above where valuables, for example textiles, were kept. There is also a bed dating from medieval times and was used as a guest room in summer.\nThe upper gallery of the loft is lit by three small openings with simple carvings and detailing. This would provide the main source of daylight and ventilation to the sleeping area. This later loft and bur from Rofshus in Telemark, built in 1754, also stand in the traditional Telemark style side by side. The log construction was introduced to Norway from Finland and Russia during the Viking Age and has improved over the ages. The corner detail, the way the logs were held together, developed over time to ensure a tighter fit with the long grooves being lined with moss to keep out the wind. In the Telemark region the use of enormous logs became a status symbol in the 1600's. The larger the logs the more prosperous were the inhabitants. The loft was traditionally the most prestigious building on the farmstead, it's style and construction however, changed very little over the years. There are very few differences between this example and the previous one at the Sondre Tveito farmstead despite being built some 450 years apart.\nWith the introduction of panelling, the detailing of the logs was greatly simplified as the logs were hidden from view and also protected more from the elements. The carvings on the panels however, became more elaborate as can be seen from this example. Being the most prestigious building, the loft's panels were decorated with great care. These carved floral motives were done in 1754 AD, the date being scribed above the door.\nThis single storey house has had it's logs hewn to give a tighter fit. The rooms were generally very simply furnished with long benches and a long table, but allowed the family to comfortably carry out all their daily activities. This main room (stua) was where the family would cook, eat, celebrate, work and sleep. Norwegian's used birch bark on the roofs to keep out the rain and the snow. Birch bark is waterproof and has a very long life and can withstand wet conditions. It is however, a very lightweight material and requires something a bit heavier to hold it in place. The most common and easiest material was turf that also helped to insulate the roofs. This could be seen on the previous pages. The photograph below shows an example of a birch bark roof held in place with timber boarding. Timber boarding in itself can be watertight and was sometimes used on its own. In other cases it was placed on top of turf to protect it and so help retain the heat. An interesting comparison can be found in the Glencoe Visitor Centre where a modern day interpretation by Gaia architects in Scotland designed a similar roof from untreated larch. (See menu below) The previous buildings can be found at the Norsk Folkemuseum in Oslo, which features 155 buildings. The buildings have been gradually added to the museum since it was established in 1894.\nHistory of rain screen siding – Norway is the place where rain screen cladding had its birth. It was not a scientific breakthrough, however, but more a gradual discovery that happened centuries ago in a largely intuitive way. Norwegian builders, probably through trial and error, found a way to utilize drained and back-ventilated cladding with joints that were both closed and open. The first buildings to have this type of rain screen cladding were large barns. This is why they called it “the open-jointed barn technique.” The timber cladding had closed joints with opening at the top and at the bottom of the timber to allow for water drainage, and also for the evaporation of any rain moisture that managed to penetrate inside.\nScientific research of the underlying principles of a rain screen didn’t start until as late as the 1940s. It was quickly recognized that the principles involved in rain screen cladding were vastly superior to anything else in use at the time. That still holds true today. Green Aspects of Rain Screen Siding – Rain screen siding is considered a green building practice for two reasons. Utilizing a well designed rain screen vastly extends the life of your siding and minimizes thermal transfer.\nBy incorporating the rain screen principle into building design, siding will last considerably longer because moisture will never be trapped between your exterior cladding and exterior wall envelope. A longer life for your siding equates to less material consumption due to material replacement.\nRain screen systems also minimize thermal transfer by creating an air cavity behind the siding. This keeps the heat generated by sunlight on the outside of your house not allowing the heat to directly transfer inside -ultimately significantly reducing energy consumption and cooling costs.\n1. Anderson, J.M. & Gill, J.R. (1988). Rainscreen Cladding: A Guide to Design Principles and Practice. Butterworth-Heinemann.\n2. Johansson, C.H. (1946). The influence of moisture on the heat conductance for bricks.\nToll Free: 855.545.4284"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:5f4be6d6-e9e1-49eb-9b1d-d2ec4dba1293>","<urn:uuid:449ef92b-2aaf-42bd-a2b0-935f15306233>"],"error":null}
{"question":"How do both professors' research interests relate to materials science and what areas do they specialize in?","answer":"The Cornell professor focuses on microfabrication and nanotechnology, specifically building microfluidic labs-on-chips and implantable sensors, as well as controlling surfaces at micro/nano scale. The Chinese Academy of Sciences professor specializes in superconducting materials, investigating non-Fermi liquid behavior, unconventional pairing mechanisms in cuprates and iron pnictide superconductors, and vortex dynamics.","context":["Joint Professor of Electrical Engineering and Bioengineering\nOffice: EEB 253I and Fluke Hall 215\nMy lab uses microfabrication and nanotechnology to build microfluidic labs-on-a-chip and implantable sensors.\nIn our lab, we work on many aspects of micro and nano electro mechanical systems (MEMS / NEMS). This research tends to be very interdisciplinary, and we have collaborations in biology, chemical engineering, bioengineering, computer science, and other departments. We have built, for example, self-assembling microstructures, biomedical implants, systems for docking of picosatellites, and walking microrobots.\nThere are two major research themes in our work:\nControlling surfaces and interfacial forces at the micro and nano scale: this includes systems for controlled self-assembly of microcomponents, “programmable” surfaces whose local properties (for example, hydrophobicity) can be changed on demand, and MEMS actuator arrays and microrobots for moving tiny objects.\nJoining MEMS and biology: this includes integrating new biomaterials into MEMS processes and devices, biomedical sensor implants, and microfluidic chips for handling and analyzing biological samples.\nPhD Cornell University, 1997\nMS Cornell University, 1993\nDipl.-Inform., Technical University Karlsruhe, Germany, 1990\nUniversity of California at Berkeley, 1996-1998\n2011 IEEE Fellow\n2010 John M. Fluke Distinguished Chair of Engineering, University of Washington\n2004-2005 Japan Society for the Promotion of Science (JSPS) Invitational Fellowship for Research in Japan\n2004 IEEE Robotics and Automation Society Academic Early Career Award\n1999 National Science Foundation CAREER Award\n1997 National Science Foundation Postdoctoral Associateship\nBowen Cheng, Dirk De Bruyker, Chris Chua, Kunal Sahasrabuddhe, Ivan Shubin, John E. Cunningham, Ying Luo, Karl F. Böhringer, Ashok V. Krishnamoorthy, Eugene M. Chow, “Microspring characterization and flip-chip assembly reliability.” IEEE Transactions on Components, Packaging and Manufacturing Technology 3(2):187-196, February 2013.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Optimization of Angular Alignment in Self-assembly of Thin Parts at an Air-water Interface.” ASME/IEEE Journal of Microelectromechanical Systems (letter) 22(1):13-15, February 2013.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Parallel heterogeneous integration of chip-scale parts by self-assembly.” ASME/IEEE Journal of Microelectromechanical Systems (letter) 21(6):1273-5, December 2012.\nChang-Ching Tu, Ji-Hao Hoo, Karl F. Böhringer, Lih Y. Lin, Guozhong Cao, “Surface passivation dependent photoluminescence from silicon quantum dot phosphors.” Optics Letters 37(22):4771-4, 15 November 2012.\nKwang Soon Park, Ji Hao Hoo, Rajashree Baskaran, Karl F. Böhringer, “Orientation-controlled parallel assembly at air-water interface”, Journal of Micromechanics and Microengineering 22(10):105028, October 2012.\nTodd A. Duncombe, James F. Parsons, Karl F. Böhringer, „Directed Drop Transport Rectified from Orthogonal Vibrations via a Flat Wetting Barrier Ratchet”, Langmuir 28(38):13765-13770, 30 August 2012.\nShaghayegh Abbasi, Sathana Kitayaporn, Michael J. Siedlik, Daniel T. Schwartz, Karl F. Böhringer, “Electrodeposition modeling and optimization to improve thin film patterning with orchestrated structure evolution”, IOP Nanotechnology 23(30):305301, 3 August 2012.\nTodd A. Duncombe, E. Yegân Erdem, Ashutosh Shastry, Rajashree Baskaran, Karl F. Böhringer, “Controlling Liquid Drops with Texture Ratchets.” Advanced Materials 24(12):1545-1550, 22 March 2012.\nAnupama V. Govindarajan, Sujatha Ramachandran, Genevieve D. Vigil, Paul Yager, Karl F. Böhringer, “A low cost point-of-care viscous sample preparation device for molecular diagnosis in the developing world; an example of microfluidic origami.” Lab-on-a-Chip 12(1):174-181, 2012.","Professor of Nanjing University, Chang Jiang Scholarship Professor, group leader, director of Center for Superconducting Physics and Materials of Nanjing university. Published more than 340 scientific papers in internationally recognized journals, received over 7000 citations, h-index 46. Gave about 100 speeches or invited talks at international conferences.\nWorking field: Exploration of new superconducting materials, investigation on non-Fermi liquid behavior, unconventional pairing mechanism of cuprates and iron pnictide superconductors, vortex dynamics, mixed state properties, critical fluctuation, etc.\nEducation and working background\n1. Sept.1981 — July 1985. Received the BS and the outstanding graduation award (1/100 probability) from Department of Physics, Anhui University;\n2. Sept. 1985 — March 1988 Obtained the MS from the Chinese Academy of Sciences;\n3. March 1988 — March 1991 Obtained the Ph.D. ( Philosophiae Doctor Condensed Matter Physics ) from Chinese Academy of Sciences.\n4. Oct. 1991 — Oct.1993, Post-Doctor in Vrije Universiteit, Amsterdam.\n5. Nov. 1993 — Aug. 1995, Assistant professor in Institute of Physics, Chinese Academy of Sciences.\n6. Aug. 1995 — Aug. 1996, Associate professor in Institute of Physics, Chinese Academy of Sciences.\n7. Aug. 1996 – Dec.2010, Full Professorship Research Fellow in Institute of Physics, Chinese Academy of Sciences.\n8. Oct. 1996 — April 1998, Alexander von Humboldt fellow in Ulm university , Germany\n9. Oct. 2000—Sept. 2009, Director of The National Lab for Superconductivity, Chinese Academy of Sciences.\n10. Oct.10, 2003—Dec.16 2003, Physics Department, University of Tennessee, Visiting Scholar.\n11. Jan. 2011—present, Professor of Nanjing University.\n1. Won the Outstanding Chinese Young Scholar’s prize by Chinese Academy of Sciences in 1995 (2nd grade).\n2. Received Outstanding Chinese Young Scholar’s foundation by Natural Science Foundation in China ( NSFC ) in 1998.\n3. Received the 10-outstanding-youth award from Chinese Academy of Science in 2002.\n4. Received the National young scholar’s Science and Technology award in 2000.\n5. Received the 2003 Beijing City Award for Science and Technology (second grade, order No#1)\n6. 2004, National Award for Natural Science (second grade)\n7. 2009, Award of Hong Kong Qiu Shi Science & Technologies Foundation\n8. 2010, Achievement in Asia Award (Robert T. Poe Prize)\n9. 2012, Chang Jiang Scholarship professor\n10. 2013, APS fellow\n11. 2013, National Award for Natural Science (first grade, order No# 4)\n● Since June 4, 2011, Director of Center for Superconducting Physics and Materials of Nanjing University\n● Since Jan. 2011, Professor of Nanjing University, Group leader, Tutor for Ph.D students；\n● Chairman of the National project for Superconductivity Research (materials and fundamental mechanism) (2006-2010); Project expanded and renewed in 2011-2015.\n● Chairman of the project “International Team on Superconductivity and Novel Electronic Materials” (2006-2010) of Chinese Academy of Sciences.\n● Editorial member of Physica C (Holland), Phylosophical Magzine (UK), Chinese Physics Letters, Science in China G and Physics ( in Chinese ).\n● Coordinator of Canada Institute for Advanced Research (CIFAR) in the Far East region.\nSelected published papers\n1. Zengyi Du, Xiong Yang, Dustin Altenfeld, Qiangqiang Gu, Huan Yang, Ilya Eremin, Peter J. Hirschfeld, Igor I. Mazin, Hai Lin, Xiyu Zhu, Hai-Hu Wen, Sign reversal of the order parameter in (Li1-xFex)OHFe1-yZnySe, Nature Physics 14, 134-139(2018).\n2. Mingyang Chen, Xiaoyu Chen, Huan Yang, Zengyi Du, Xiyu Zhu, Enyu Wang, Hai-Hu Wen, Discrete energy levels of Caroli-de Gennes-Matricon states in quantum limit in FeTe0.55Se0.45, Nature Communications 9, 970(2018).\n3. C. H. P. Wen, H. C. Xu, Q. Yao, R. Peng, X. H. Niu, Q. Y. Chen, Z. T. Liu, D. W. Shen, Q. Song, X. Lou, Y. F. Fang, X. S. Liu, Y. H. Song, Y. J. Jiao, T. F. Duan, H. H. Wen, P. Dudin, G. Kotliar, Z. P. Yin, D. L. Feng, Unveiling the Superconducting Mechanism of Ba0.51K0.49BiO3, Physical Review Letters 121, 117002(2018).\n4. Yue Zhang, Wenhao Liu, Xiyu Zhu, Haonan Zhao, Zheng Hu, Chengping He, Hai-Hu Wen, Unprecedented high irreversibility line in nontoxic cuprate superconductor (Cu,C)Ba2Ca3Cu4O11+δ, Science Advances 4, eaau0192(2018).\n5. Mingyang Chen, Xiaoyu Chen, Huan Yang, Zengyi Du, Hai-Hu Wen, Superconductivity with twofold symmetry in Bi2Te3/FeTe0.55Se0.45 heterostructures, Science Advances 4(6) eaat1084 (2018).\n6. Guan Du, Jifeng Shao, Xiong Yang, Zengyi Du, Delong Fang, Jinghui Wang, Kejing Ran, Jinsheng Wen, Changjin Zhang, Huan Yang, Yuheng Zhang, Hai-Hu Wen, Drive the Dirac electrons into Cooper pairs in SrxBi2Se3, Nature Communications 8, 14466(2017).\n7. Zengyi Du, Xiong Yang, Hai Lin, Delong Fang, Guan Du, Jie Xing, Huan Yang, Xiyu Zhu, Hai-Hu Wen, Scrutinizing the double superconducting gaps and strong coupling pairing in (Li1-xFex)OHFeSe, Nature Communications 7, 10565(2016).\n8. Philip J. W. Moll, Xiyu Zhu, Peng Cheng, Hai-Hu Wen, Bertram Batlogg, Intrinsic Josephson junctions in the iron-based multi-band superconductor (V2Sr4O6)Fe2As2, Nature Physics 10: 9, 644-647(2014).\n9. M. Yi, Y. Zhang, Z.-K. Liu, X. Ding, J.-H. Chu, A.F. Kemper, N. Plonka, B. Moritz, M. Hashimoto, S.-K. Mo, Z. Hussain, T.P. Devereaux, I.R. Fisher, H.H. Wen, Z.-X. Shen, D.H. Lu, Dynamic competition between spin-density wave order and superconductivity in underdoped Ba1-xKxFe2As2, Nature Communications 5, 3711 (2014).\n10. Philip J. W. Moll, Luis Balicas, Xiyu Zhu, Hai-Hu Wen, Nikolai D. Zhigadlo, Janusz Karpinski, Bertram Batlogg, Critical Current Oscillations in the Intrinsic Hybrid Vortex State of SmFeAs(O,F), Physical Review Letters 113, 186402(2014).\n11. T. Böhm, A. F. Kemper, B. Moritz, F. Kretzschmar, B. Muschler, H.-M. Eiter, R. Hackl, T. P. Devereaux, D. J. Scalapino, Hai-Hu Wen, Balancing Act: Evidence for a Strong Subdominant d-Wave Pairing Channel in Ba0.6K0.4Fe2As2, Physical Review X 4, 041046(2014).\n12. Zhenyu Wang, Huan Yang, Delong Fang, Bing Shen, Qiang-HuaWang, Lei Shan, Chenglin Zhang, Pengcheng Dai and Hai-Hu Wen, Close relationship between superconductivity and the bosonic mode in Ba0.6K0.4Fe2As2 and Na(Fe0.975Co0.025)As, Nature Physic 9:1, 42–48(2013).\n13. Huan Yang, Zhenyu Wang, Delong Fang, Qiang Deng, Qiang-Hua Wang, Yuan-Yuan Xiang, Yang Yang, Hai-Hu Wen, In-gap quasiparticle excitations induced by non-magnetic Cu impurities in Na(Fe0.96Co0.03Cu0.01)As revealed by scanning tunnelling spectroscopy, Nature Communications 4, 2749(2013).\n14. Xiaxin Ding, Delong Fang, Zhenyu Wang, Huan Yang, Jianzhong Liu, Qiang Deng, Guobin Ma, Chong Meng, Yuhui Hu, Hai-Hu Wen, Influence of microstructure on superconductivity in KxFe2−ySe2 and evidence for a new parent phase K2Fe7Se8, Nature Communications 4, 1897(2013).\n15. Y. M. Dai, B. Xu, B. Shen, H. Xiao, H. H. Wen, X. G. Qiu, C. C. Homes, and R. P. S. M. Lobo, Hidden T-Linear Scattering Rate in Ba0.6K0.4Fe2As2 Revealed by Optical Spectroscopy, Physical Review Letters 111, 117001(2013).\n16. J. Jiang, S. Li, T. Zhang, Z. Sun, F. Chen, Z.R. Ye, M. Xu, Q.Q. Ge, S.Y. Tan, X.H. Niu, M. Xia, B.P. Xie, Y.F. Li, X.H. Chen, H.H. Wen, D.L. Feng, Observation of possible topological in-gap surface states in the Kondo insulator SmB6 by photoemission, Nature Communications 4, 3010(2013).\n17. E. C. Blomberg, M. A. Tanatar, R. M. Fernandes, I. I. Mazin, Bing Shen, Hai-Hu Wen, M. D. Johannes, J. Schmalian, R. Prozorov, Sign-reversal of the in-plane resistivity anisotropy in hole-doped iron pnictides, Nature Communications 4, 1914(2013).\n18. F. Kretzschmar, B. Muschler, T. Böhm, A. Baum, R. Hackl, Hai-Hu Wen, V. Tsurkan, J. Deisenhofer, A. Loidl, Raman-Scattering Detection of Nearly Degenerate s-Wave and d-Wave Pairing Channels in Iron-Based Ba0.6K0.4Fe2As2 and Rb0.8Fe1.6Se2 Superconductors, Physical Review Letters 110, 187002(2013).\n19. L Shan, J Gong, YL Wang, B Shen, XY Hou, C Ren, CH Li, H Yang, HH Wen, SL Li, PC Dai, Evidence of a Spin Resonance Mode in the Iron-Based Superconductor Ba0.6K0.4Fe2As2 from Scanning Tunneling Spectroscopy, Physical Review Letters 108, 227002(2012).\n20. Lei Shan, Yong-Lei Wang, Bing Shen, Bin Zeng, Yan Huang, Ang Li, Da Wang, Huan Yang, Cong Ren, Qiang-Hua Wang, Shuheng H. Pan, Hai-Hu Wen, Observation of ordered vortices with Andreev bound states in Ba0.6K0.4Fe2As2, Nature Physics 7:4, 325-331(2011).\n21. YM Xu, P Richard, K Nakayama, T Kawahara , Y Sekiba, T Qian, M Neupane, S Souma, T Sato, T Takahashi, HQ Luo, HH Wen, GF Chen, NL Wang, Z Wang, Z Fang, X Dai, H Ding, Fermi surface dichotomy of the superconducting gap and pseudogap in underdoped pnictides, Nature Communications 2, 392(2011).\n22. M. L. Teague, G. K. Drayna, G. P. Lockhart, P. Cheng, B. Shen, H. -H. Wen, N. -C. Yeh, Measurement of a Sign-Changing Two-Gap Superconducting Phase in Electron-Doped Ba(Fe1-xCox)2As2 Single Crystals Using Scanning Tunneling Spectroscopy, Physical Review Letters 106, 087004(2011).\n23. B. Zeng, G. Mu, H. Q. Luo, T. Xiang, I. I. Mazin, H. Yang, L. Shan, C. Ren, P. C. Dai & H.-H. Wen, Anisotropic structure of the order parameter in FeSe0.45Te0.55 revealed by angle-resolved specific heat, Nature Communications 1, 112(2010).\n24. C Tarantini, M Putti, A Gurevich, Y Shen, RK Singh, JM Rowell, N Newman, DC Larbalestier, P Cheng, Y Jia, HH Wen, Suppression of the Critical Temperature of Superconducting NdFeAs(OF) Single Crystals by Kondo-Like Defect Sites Induced by alpha-Particle Irradiation, Physical Review Letters 104, 087002(2010).\n25. FL Ning, K Ahilan, T Imai, AS Sefat, MA McGuire, BC Sales, D Mandrus, P Cheng, B Shen, HH Wen, Contrasting Spin Dynamics between Underdoped and Overdoped Ba(Fe1-xCox)2As2, Physical Review Letters 104, 037001(2010).\n26. HH Wen, G Mu, HQ Luo, H Yang, L Shan, C Ren, P Cheng, J Yan, L Fang, Specific-Heat Measurement of a Residual Superconducting State in the Normal State of Underdoped Bi2Sr2-xLaxCuO6+delta Cuprate Superconductors, Physical Review Letters 103, 067002(2009).\n27. H Yang, Y Liu, CG Zhuang, JR Shi, YG Yao, S Massidda, M Monni, Y Jia, XX Xi, Q Li, ZK Liu, QR Feng, HH Wen, Fully band-resolved scattering rate in MgB2 revealed by the nonlinear Hall effect and magnetoresistance measurements, Physical Review Letters 101, 067001(2008).\n28. C Ren, ZS Wang, HQ Luo, H Yang, L Shan, HH Wen, Evidence for Two Energy Gaps in Superconducting Ba0.6K0.4Fe2As2 Single Crystals and the Breakdown of the Uemura Plot, Physical Review Letters 101, 257006(2008).\n29. JH Ma, ZH Pan, FC Niestemski, M Neupane, YM Xu, P Richard, K Nakayama, T Sato, T Takahashi, HQ Luo, L Fang, HH Wen, ZQ Wang, H Ding, V Madhavan, Coexistence of Competing Orders with Two Energy Gaps in Real and Momentum Space in the High Temperature Superconductor Bi2Sr2-xLaxCuO6+delta, Physical Review Letters 101, 207002(2008).\n30. DW Shen, BP Xie, JF Zhao, LX Yang, L Fang, J Shi, RH He, DH Lu, HH Wen, DL Feng, Novel mechanism of a charge density wave in a transition metal dichalcogenide, Physical Review Letters 99, 216404 (2007).\n31. Q Li, BT Liu, YF Hu, J Chen, H Gao, L Shan, HH Wen, AV Pogrebnyakov, JM Redwing, XX Xi, QQ Wang, J Shi, Y Wang, HH Wen, Large anisotropic normal-state magnetoresistance in clean MgB2 thin films, Physical Review Letters 96, 167003(2006).\n32. HH Wen, XH Chen, WL Yang, ZX Zhao, Same superconducting criticality for underdoped and overdoped La2-xSrxCuO4 single crystals, Physical Review Letters 85, 2805-2808(2000).\n33. HH Wen, WL Yang, ZX Zhao, YM Ni, Josephson-coupling origin for the upward curvature of the pseudo-upper-critical field in Bi2Sr2-xLaxCuO6+delta crystals, Physical Review Letters 82:2, 410-413(1999).\n34. HH Wen, HA Radovan, FM Kamm, P Ziemann, FM Kamm, P Ziemann, SL Yan, L Fang, MS Si, 2D vortex-glass transition with T-g=0 K in Tl2Ba2CaCu2O8 thin films due to high magnetic fields, Physical Review Letters 80, 3859-3862(1998).\n35. HH Wen, AFT Hoekstra, R Griessen, SL Yan, L Fang, MS Si, Field induced vanishing of the vortex glass temperature in Tl2Ba2CaCu2O8 thin films, PhysicaL Review Letters 79:8, 1559-1562(1997).\n36. R Griessen, HH Wen, Ajj Vandalen, B Dam, J Rector, HG Schnack, S Libbrecht, E Osquiguil, Y Bruynseraede, Evidence for mean free-path fluctuation-induced pinning in YBa2Cu3O7 and YBa2Cu4O8 films, Physical Review Letters 72:12, 1910-1913(1994)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:6fb06661-b03c-4bda-ada4-ab151e1b6440>","<urn:uuid:4f12ede1-d1a2-423f-b909-51bf5523bfdc>"],"error":null}
{"question":"Can you explain what the Encyclopedia of Scientific Dating Methods covers? I need reliable reference material for my studies.","answer":"The Encyclopedia of Scientific Dating Methods is a comprehensive treatise containing almost 200 articles that cover widely accepted dating methods. It provides an overview of both the physical and chemical foundations of dating methods and their applications. The work is included in Springer's Encyclopedia of Earth Sciences Series and has been recognized with the GSIS Mary B. Ansari Best Geoscience Research Resource Work Award.","context":["Springer encyclopedia for dating methods of fossil finds singled out for scientific research award\nEncyclopedia of Scientific Dating Methods wins Mary B. Ansari Best Geoscience Research Resource Work Award 2016\nHeidelberg | New York, 26 September 2016\nThe Geoscience Information Society (GSIS) has chosen Springer’s Encyclopedia of Scientific Dating Methods edited by William Jack Rink and Jeroen W. Thompson as the recipient of this year’s GSIS Mary B. Ansari Best Geoscience Research Resource Work Award. The award will be presented on 27 September 2016 during the Geological Society of America’s Annual Meeting and Exposition in Denver, Colorado.\nDiscovering an ancient artefact, mineral or fossil is only half the job in geological sciences, biology and archaeology. Dating this find and finding out where it fits in the geological time scale makes up the second half and is crucial for understanding its significance in the context of earth’s history. The Encyclopedia of Scientific Dating Methods provides an overview of the physical and chemical foundations of dating methods and their applications. The comprehensive treatise contains almost 200 articles covering widely accepted dating methods to determine the timing and rate of various processes, such as sedimentation (terrestrial and marine), tectonics, volcanism, geomorphological change, cooling rates, crystallization, fluid flow, glaciation, climate change and evolution. The reference work is published in Springer’s renowned book series Encyclopedia of Earth Sciences Series.\nPetra van Steenbergen, Executive Editor of Earth Sciences, Geography and Environment at Springer, said, “Springer is extremely proud that the Encyclopedia of Scientific Dating Methods is honored with the prestigious GSIS Mary B. Ansari Best Geoscience Research Resource Work Award. We extend our cordial congratulations to the editors, associate editors and contributors of this excellent work.”\nThe content in Springer Nature’s Major Reference Works is internationally commissioned and led by editors who are acclaimed and recognized scholars and scientists from top universities in all fields. Editorial boards that ascribe quality and validate the content consist of internationally recognized academic peers and leaders in their disciplines. Contributors include Nobel Prize winners, ground-breaking researchers and high-impact scientists. Over 100,000 scientists and researchers contribute the content that makes up the Springer Nature reference program.\nThe Geoscience Information Society is a member society of the American Geosciences Institute (AGI) and an associated society of the Geological Society of America (GSA). GSIS connects scientists, librarians, editors, cartographers, educators and information professionals in the geosciences from all over the world, encouraging the continuous exchange of information in this research field. Mary B. Ansari, whose generous donation enables this award, has been a member of the Geoscience Information Society for years and was its president in 1990.\nSpringer is part of Springer Nature, a leading global research, educational and professional publisher, home to an array of respected and trusted brands providing quality content through a range of innovative products and services. Springer Nature is the world’s largest academic book publisher, publisher of the world’s most influential journals and a pioneer in the field of open research. The company numbers almost 13,000 staff in over 50 countries. Springer Nature was formed in 2015 through the merger of Nature Publishing Group, Palgrave Macmillan, Macmillan Education and Springer Science+Business Media. Visit www.springernature.com and follow @SpringerNature.\nOn the book:\nWilliam Jack Rink, Jeroen W. Thompson (Eds.)\nEncyclopedia of Scientific Dating Methods\nPrint ISBN 978-94-007-6303-6\nAlso available as an eBook\nJoan Robinson | Springer Nature | Communications\ntel +49 6221 487 8130 | email@example.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:a2661306-6118-4746-9e0a-c849cce62fe1>"],"error":null}
{"question":"For antibiotic-resistant infections, which is more effective in breaking down bacterial cell walls - Penicillin or Cephalothin?","answer":"Cephalothin is more effective against antibiotic-resistant infections, particularly against β-lactamase producing microorganisms. It is the most efficient first-generation cephalosporin against resistant microorganisms. In contrast, Penicillin has become ineffective against certain bacteria as many bacteria and viruses have developed resistance to it over time. Some bacterias have developed ways to prevent Penicillin from breaking down their protective walls.","context":["Cephalothin is a first-generation cephalosporin, that shows great activity against Gram-positive microorganisms. Its effect is bactericidal, and due its action, it is the most efficient first-generation cephalosporin against resistant microorganism (β-lactamase producers). Although this drug has been clearly studied and researched about its antimicrobial activity, pharmacokinetics and pharmacodynamics, there are a few studies in literature regarding the development of analytical methodology for this cephalosporin. The aim of this work was to develop and validate a new method of analysis, using high performance liquid chromatography, resulting in an innovative method, quick and using solvents of low toxicity, minimizing, in this way, its toxic actions to the operators and leavings in the environment. The method developed and validated for the quantification of sodium cephalothin in lyophilized powder for injectable solution used high performance liquid chromatography (HPLC). The mobile phase consisted in water with 0.7% of glacial acetic acid and ethanol (70:30 v/v), wave-length of 237 nm, Zorbax Eclipse Plus C18 AgilentTM column and room temperature of 25°C, retention time of 4,20 minutes. The method was linear in the concentrations of 20, 40, 60, 80 e 100 μg/mL, selective, accurate and robust towards these modifications: ethanol brand, water source, mobile phase rate, glacial acetic acid proportion, flow rate, room temperature and wave length. The dosing for CET was of 106.72%.\nKeywords: Cephalotin sodium; Green method; HPLC; Method validation; Quality control\nWith the advance of the infection diseases and the increase in the world mortality rate, because of pathogenic microorganisms, there came the necessity of discovering substances that would be able to fight this. The last decades were dedicated to the search of new drugs, with great importance the period of 1950 a 1970, known as “The golden era” for the discovery of antimicrobials, emerging several classes of them . However, after this period, there was a decrease in the development of new molecules, what brought the worry about resistant microorganisms. In this way, a new approach to fight bacterial infections was through the improvement of this molecules already used .\nAn important class of antimicrobials is the cephalosporins, originally produced by Cephalosporium acremonium. The cephalosporins are classified as beta-lactam antibiotics, however, they show a broader action spectrum when compared to penicillins, because they are resistant of penicillinases. Changes in its structure gives a higher potency to this substance .\nCephalothin was one of the first modification obtained from the 7-aminocephalosporanic acid, the pharmacological structure of cephalosporins, classifying as a first-generation cephalosporin. This drug shows higher activity against Gram positive and less against Gram negative . Due its instability in acid, it is administered parenterally. Figure 1 shows the chemical structure of the drug.\nThe development and validation of analytical methods for the determination of the quality of final products is extremely important, mainly when related to pharmaceutical products [4,5]. The evaluation of quality will determine the efficacy of cephalosporin pharmaceutical products and avoid damage in the patient health [6-11]. Some studies relating the quality control of cephalothin were found in literature for its quantification in biological matrices [12-20], and for analysis of CET in pharmaceutical dosage form [21-24]. Its monograph is in pharmacopoeias like Brazilian , United States , United Kingdom , European  and Japanese Pharmacopoeia .\nBesides the importance of quality control, a crescent worry about environment and the worker makes necessary the development of conscious methods and less pollutants, in this way, the aim of this work is to improve the already validated technic for HPLC with a green chemistry approach, using less toxic solvents and a decrease in formation of residues .\nThe CLAE method was performed using a Waters system, model 1525 (Waters Chromatography Systems, California, USA), connected to a UV/VIS detector Waters 2487 and manual injector 7725i with 20 μL loop (Rheodyne BreezeTM, California, USA), The separation was in isocratic form with a reversed phase column Zorbas Eclipse Plus C18 AgilentTM (150 × 4,6 mm; 5 μm) (Santa Clara, California, USA). It was used analytical balance model DV215CD (Discovery, OhausTM, São Paulo, Brazil); ultrasound bath model USC2800A (Unique, São Paulo, Brazil); purified water Milli-QTM (Direct-QTM 3, Merck Millipore, Germany); micropipette model ResearchTM Plus 100-1000 μL (Eppendorf, Hamburg, Germany) UV chamber with mirrors in the interior and UVC lamp (254 nm); UV-VIS spectrophotometer ShimadzuTM (Tokyo, Japan), model UV-1800, using quartz cells of 1 cm of optical path.\nCephalothin reference substance (CET RS), with declared content of 99.6% were kindly provided by the laboratory União Química Farmacêutica Nacional S/A (São Paulo, Brazil).\nThe samples used was commercial sodium cephalothin (generic) in lyophilized powder for injectable solution in ampoule containing 1000 mg of active substance. The samples have an adjuvant, sodium bicarbonate. The samples were kindly provided by the laboratory ABL Antibióticos do Brasil Ltda (Cosmópolis, Brazil).\nAll solutions and the mobile phase used in this method were prepared from ultrapure water obtained through Milli-Q (Direct-QTM 3, Merck Millipore, Germany) equipment. HPLC grade ethanol and glacial acetic acid was used for the mobile phase and its brand was Baker JT (Mexico). For selectivity, it was used: 0.1 M hydrochloric acid solution (SynthTM, São Paulo, Brazil), 0.01 M sodium hydroxide solution (Vetec Química FinaTM, São Paulo, Brazil) and 0.3% hydrogen peroxide solution (Vetec Química FinaTM, São Paulo, Brazil).\nPreparation of CET SQR and CET sample solutions: The CET SQR solution was prepared weighting 5.02 mg and transferring to a 25 mL volumetric flask and adding purified water that will give 200 μg/mL stock solution. All other solutions used in the tests were prepared by this stock solution, taking the necessary volume to obtain the desired concentration and transferring to a 10 mL volumetric flask.\nThe content of 5 vials of CET sample in lyophilized powder for injectable solution were mixed and 5.27 mg was weighted and transferred to a 25 mL volumetric flask that was added purified water. It was obtained a 200 μg/mL stock solution. All solutions used in the tests were prepared taking the necessary volume from the stock solution, to obtain the desired concentration, and transferring to a 10 mL volumetric flask.\nHPLC method: The method was performed in isocratic mode and at room temperature of 25°C. The mobile phase consisted in ethanol and acidified water with 0.7% glacial acetic acid (30:70, v/v) that was degassed by ultrasonic bath for 30 minutes before use. The injection volume was of 20 μL at flow rate of 1.0 mL/minute, using UV detection at 237 nm. The solutions tested were filtered through 0.45 μm membrane (Pall Corporation, Michigan, USA) before the injection.\nThe parameters evaluated for the validation of the HPLC method were: system suitability, linearity, selectivity, precision (repeatability and intermediate precision), accuracy, robustness and limits of detection (LOD) and quantification (LOQ). The method was validated according to what is recommended by ICH guidelines literature .\nSystem suitability: A 60 μg/mL CET sample solution was prepared and injected in sextuplicate. All chromatograms were analyzed, and the parameters evaluated such as retention time (tR), peak area, number of plates (N), peak asymmetry (As), retention factor (k) and tailing factor (TF). It was calculated the relative standard deviation (RSD). The results are shown in Table 1.\n|Retention factor (>2.0)||Retention time (min)||Peak asymmetry (= 2.0)||Number of plates (>2000)||Area|\naRSD%: Relative Standard Deviation\nTable 1: Navigation lock (operation time) vessel movement upstream to downstream.\nLinearity: It was prepared a 200 μg/mL CET RS solution from whom was taken aliquots to prepare solutions of 20, 40, 60, 80 e 100 μg/mL and perform injections in triplicate. The equation of the line was determined by linear regression study, by the least squares method, analysis of variance (ANOVA) and residues analysis.\nPrecision: It was determined by repeatability precision and intermediate. The repeatability precision consisted in six injections of CET RS solution in the concentration of 60 μg/mL. It was done in the same day and carried by the same analyst. The intermediate was done in two ways. The first was performed by the same analyst but in three different days and following the same experimental conditions. In the second way, the analyst was changed, and the six injections were done in the same day and in the same experimental conditions. Statistical analysis was performed for each test through RSD values.\nAccuracy: The accuracy of the method was performed by contaminated placebo, in which known amounts of a CET SR solution were added to a solution prepared with sodium bicarbonate. All injections were made in triplicate for each concentration. First, it was injected a 30 μg/mL solution of CET SR in water, and then, three different concentrations of the contaminated placebo that corresponded to 80, 100 and 120% respectively. Aliquots of 4.8; 6.0 e 7.2 mL of CET SR solution were added to an excipient solution to determine the accuracy in the feedstock. The solutions were prepared as shown in Table 2.\n|Volume added of CET SR solution (100 μg/mL) (mL)||Volume added of placebo solution (mL)||Nominal concentration (μg/mL)|\nVolumetric flask: 10 mL\nTable 2: Procedure to determine the accuracy.\nRobustness: The robustness was evaluated by small variations in seven parameters organized in eight experiments and followed the Youden and Steiner method. To determine the robustness, it was used CET SR and sample solutions in the concentration of 60 μg/mL and performed in triplicate. Table 3 shows the parameters and the variations for each one, where the capital letter represents the conditions used in the method and the lower case when there was a variation and in Table 4 there is the range of variation.\n|C/c||Proportion of mobile phase||C||C||C||c||C||c||C||c|\n|D/d||Proportion of glacial acetic acid (%)||D||D||D||d||d||d||D||D|\n|E/e||Flow rate (mL/min)||E||E||E||e||e||E||E||E|\n|F/f||Room temperature (ºC)||F||F||F||F||F||f||F||F|\nbRSD%: Relative Standard Deviation\nTable 3: Shear strength prediction models for SFRC beams used in this study.\n|S. No||Factors||Unit||Limit||Varied Condition (1)||Normal Condition (0)||Varied Condition (-1)|\n|A||Ethanol brand||-||-||Scharlau||J. T. Baker||Scharlau|\n|C||Proportion of mobile phase||%||2||68:32 (v/v)||70:30 (v/v)||72:28 (v/v)|\n|D||Proportion of glacial acetic acid||%||0.1||0.8%||0.7%||0.6%|\nTable 4: Range of variations for the determination of cephalothin.\nThe difference between the normal values and the ones changed in module should be lower than the value resulted from 2xS in order to infer that the effects achieved with the variations of the parameters were not significant and therefore the method is robust for all selected factors.\nSpecificity: Specificity can be accessed by different analysis which can be easily found in the literature [31-36]. CET sample solution in the concentration of 60 μg/mL was submitted to forced degradation in acid, alkaline, oxidative, photolytic and neutral conditions. This parameter was performed to evaluate if there was any interference of degradation products in the quantification of CET sample. The solutions used as degradation solvents were: 0.1 M HCl, 0.01 M NaOH, 0.3% H2O2 and purified water, used in acid, basic, oxidative and neutral/photolytic degradation, respectively. The acid, oxidative and neutral conditions were heated to 60°C while basic and photolytic conditions were maintained at 25°C and, the photolytic degradation was induced by exposure to ultraviolet light (UVC, 254 nm). Aliquots were taken from 10 to 10 minutes until degradation above 10%.\nDetection (LOD) and Quantification (LOQ): According to the ICH, the LOD and the LOQ are studies based on the standard deviation of intercept and in the slope of the analytical curve. After obtaining three analytical curves, LOD and LOQ were calculated as:\nWhere σ is the standard deviation and S is the slope of the calibration curve.\nDifferent chromatographic conditions were tested to develop a quantification method for CET SR e CET samples. The choice of the chromatographic column was based in the peaks resolution. The Zorbax Eclipse Plus C18 AgilentTM (150 × 4.6 mm; 5 μm) showed a better peak symmetry and lower system pressure.\nFor the determination of the mobile phase there was made several tests varying the concentration of the glacial acetic acid in water and the proportion of ethanol. All mobile phases tested showed appropriate peaks, but just one of them covered all parameters settled in system suitability. The mobile phase used was water with 0.7% glacial acetic acid and ethanol (70:30, v/v). The use of water and a less toxic organic solvent reduced the formation of waste and damage for the chromatographic system. The chromatogram obtained in the method conditions is shown in the Figure 2.\nTable 5 shows the area values obtained for each concentration used for the determination of linearity. The residue analysis showed that the regression model used is appropriate. The area values were plotted in each concentration and linearity was observed in the range of 20 to 100 μg/mL. The results were analyzed using test of variance (ANOVA) that showed no deviation from linearity and the regression model is appropriate. The ANOVA calculated is in Table 6. The analytical curve and residue analysis are on Figure 3.\n|Concentration (μg/mL)||Areasa (AU)||Average area||RSD%c|\na: Average value of three determinations; cRSD%: Relative Standard Deviation\nTable 5: Peak area obtained for each concentration of CET SR solution.\n|Source of variation||Degree of freedom||Sum of squares||Variability||F calculated||F critical|\n|Deviation of linearity||3||4514583352.89||1504861117.63||0.43||3.71|\n*Significant at p<0.05%\nTable 6: Analysis of variance (ANOVA) for linearity.\nThe precision of the method was determined by repeatability, when six solutions of CET sample in the concentration of 60 μg/mL were injected by the same analyst, in the same day and under the same experimental conditions, providing a RSD of 1.91%. For the determination of interday precision, analysis was performed on three consecutive days, and RSD% was 4.90%. For precision between analysts it was made six solutions of CET sample in the same concentration as before, under same experimental conditions, in the same day, bust with a different analyst. The value of RSD% was 1.90. All results for precision are shown in Tables 7 and 8.\nbRSD%: Relative Standard Deviation\nTable 7: Intraday precision for the analytical method developed for HPLC.\naMean of 6values; bRSD%: Relative Standard Deviation\nTable 8: Interdays and between analysts precision for the analytical method developed for HPLC.\nThe results obtained in interday precision was statistically evaluated by analysis of variance and according to ANOVA there was no significant deviation, as shown in Table 9.\n|Source of variation||Sum of squares||DF||Average squares||F calculated||F critical||P-value|\nTable 9: Analysis of variance (ANOVA) for interday precision.\nThe accuracy of the method was made by the contaminate placebo method by adding a known quantity of a CET SR solution to the placebo solution. It was determined in three different concentrations predetermined and resulted in 95.38%, lower than what is recommended in literature [37-39]. The average percentage is shown in Table 10.\n|Accuracy (days)||Recuperation (%)||Average recuperation (%)||RSD (%)a|\naRSD%: Relative Standard Deviation\nTable 10: Results for CET SR method accuracy.\nThe robustness was evaluated by the Youden and Steiner method that consists in small variations in seven parameters organized in eight experiments, previously shown on Tables 3 and 4. The effects resulting from the changed parameters were evaluated in comparison to the values obtained as reference for the test 1+=5.39 and 1-=7.83. All effects are shown in Table 11.\n|Factor||(1)||Content (%)a,b Effects||(-1)||Content (%)a,b Effects|\n|A- Ethanol brand||Scharlau||104.12-106.50=-2.38||Scharlau||103.14-98.93=4.21|\n|B- Source of water||CFQ||105.53-105.08=0.45||CFQ||101.81-100.26=1.55|\n|C- Proportion of mobile phase (v/v)||68:32||105.25-105.36=-0.10||72:28||102.28-99.79=2.49|\n|D- glacial acetic acid concentration (%)||0.8||107.12-103.49=3.63||0.6||102.50-99.56=2.94|\n|E- Flow rate (mL/min)||1.1||104.85-105.76=-0.91||0.9||102.65-99.42=3.23|\n|F- Room temperature (ºC)||27||106.02-104.59=1.43||23||99.65-102.42=-2.77|\n|G - Wavelength (nm)||239||104.37-106.24=-1.86||235||100.66-101.41=-0.75|\naAverage contents obtained in normal conditions - Average contents obtained in altered conditions;\nbReference criteria calculated: 5.39 to test +1 e 7.83 to test -1\nTable 11: Results for sodium cephalothin method robustness.\nThe selectivity of the method was evaluated by forced degradation observing the chromatograms of CET SR to make clean if there would be any degradation substance. The chromatograms on Figures 4 and 5 shows that the degradation products have negligible interference with CET peak.\nLimit of detection and quantification\nThe sensitivity of the method was determined by chromatographic detection (LOD) and quantitation (LOQ) limits. The value calculated for the lowest concentration detected by analytical procedure was 1.95 μg/mL. In turn, the calculated LQ was 5.90 μg/mL. The calculated values for the LOD and LOQ indicated the ability of the method to detect and quantify reliably CET.\nThe qualitative analysis of CET sample in lyophilized powder for injection solution was performed by the organoleptic characteristics and high-performance liquid chromatography (HPLC) that demonstrated that these methods are appropriate to identification.\nThe mainly objective of this work was the development and validation of an analytical method for the quantification of CET sample, with a green chemistry approach. The developed analytical method used ethanol as organic solvent reducing, in this way, the toxicity to the professional and environment. Moreover, it was used a few quantities of organic solvent and no buffering solutions, reducing the waste .\nThe proposed method can be considered as innovative because it wasn´t found in the literature any approach for the quantification of CET sample with the view of waste and toxic solvents reduction.\nSo, we can conclude that the developed and validated method can be used in quality control for CET sample in lyophilized powder for injection because it demonstrated to be effective to quantification.\nThis work was supported by FAPESP and CNPq-Brazil.\nThe authors report no declarations of interest.","Difference between Amoxicillin and Penicillin\nAmoxicillin vs Penicillin:\nWhen it comes to tackling diseases or conditions that are caused by bacteria or viruses, many a medical practitioner would prescribe antibiotics as treatment for the affected. Antibiotics which are produced from microbes of bacteria and actinomycetes which are effective against other kinds of bacterias and thus fight against infections and ward them off. Amoxicillin and penicillin are two of the most popular antibiotics that are prescribed by doctors and these two have many differences which set them apart.\nWhat is Amoxicillin?\nAmoxicillin, or C18H19N3O5S, is a semi-synthetic penicillin drug and is an improved version of Penicillin as the latter needed some improvement from the initial drug itself. It is taken orally as a broad-spectrum antibiotic and contains similar structural analogs including ampicillin and is thus effective against a large variety of Gram-positive bacteria and a limited number of Gram-negative microbes as well. Amoxicillin is generally used to treat pneumonia; bronchitis; gonorrhea, ENT infections, infections of the urinary tract, skin infections, etc.\nWhat is Penicillin?\nDiscovered first by Alexander Flemming in 1928, Penicillin or C16H19N3O5S, is synthetically produced from the mold penicillium. It prevents the bacteria from building up protective walls around themselves thus making them susceptible for attack. Penicillin is effective against infections caused by streptococcus, staphylococcus and pneumococcus caterias and yet, however, it is not recommended for patients with asthma, diarrhoea, allergy and blood clotting disorder. It is cheap and is has also proven to be effective against many susceptible Gram-positive or Gram-negative cocci and bacilli and is also effective against certain spirochetes.\nWhat is the difference between Penicillin and Amoxicillin?\nIt is a little known fact that Amoxicillin is an advanced and a more developed form of Pennicillin. Both drugs belong to the antibiotics category and both the drugs react to disease in a similar way as well. And yet, there are certain distinct differences which sets the two apart.\nFirst and foremost, the two drugs are blatantly different in the way that they are being produced. While Penicillin is synthetically produced from the mold penicillium, Amoxicillin is known to be a semi-synthetic penicillin drug of which the semi-synthetic nature is considered as highly beneficial is breaking down bacterial walls. Amoxicillin is also known to be affective to a wider range of bacterias than Penicillin and is considered a much safer drug than Penicillin as well.\nAlthough Penicillin was the first to be discovered and is considered the very first of medications which treated severe cases of Syphilis and other infections, Penicillin has today become uneffective against certain bacterias as a number of bacterias and viruses have developed resistance to it. As opposed to Penicillin, Amoxicillin is considered as more easily absorbed by the body cells and are thus easily able to penetrate the body tissues and thus break down the cell walls of the bacterias more easily. Thus without the cell walls, the bacterias are defenceless and they eventually whither away and die. Thereby, because of its semi-synthetic nature, Amoxicillin is considered to be more effective and more penetrating than Penicillin and is considered as more effective against many kinds of bacteria which includes Staphylococci, Pneumococci, E.Coli, Streptococci, H, influenza, N. gonorrhea, etc."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4f4acc76-4629-40bd-9d8b-749d39aba681>","<urn:uuid:8c42368e-8f36-4cc5-bdcc-76395680c9a2>"],"error":null}
{"question":"What are the key differences between the storage methods used for Field Museum thangkas versus the display approach for the Weituo Buddhist guardian statue?","answer":"The Field Museum thangkas require specialized storage methods to prevent damage - some are stored flat in map cases, while others are rolled on padded dowels in clamshell archival boxes to protect from dust, light and pressure. Unmounted paintings are stored in drop-front archival boxes with rigid support. In contrast, the monumental Weituo Buddhist guardian statue, which stands over 6 feet tall, was displayed prominently outdoors near a pond in a private park setting, before being exhibited at the Gagosian Gallery. This highlights the dramatic differences in preservation needs between delicate thangka paintings that require careful climate-controlled storage versus a large bronze sculpture that can withstand outdoor display.","context":["Thangkas are iconographically and structurally complex devotional images used in households, monasteries, temples and other traditional locations. The two primary sections, the painting and the mounting, are integral to the icon. The painting is done on a fine cloth, which is sewn into a textile mount comprising multiple layers and sections of textile finished at the top and bottom with a stiff bar. The face is protected with a fine cover cloth secured at the top and held open with cords and ribbons. The thangka is consecrated repeatedly; empowerment symbols may be written on the reverse of the painting.\nDuring use, thangkas acquire patinas of soot, oil and water stains from exposure in temples and the elements. The thangka is rolled up for carrying. The repeated rolling and unrolling of a multilayered structure of brittle paint and stiff textile causes considerable damage to the paint and textiles. If handled in their current state, many of the Field Museum thangkas would loose more paint particles and textile fragments, and develop further breaks in the painting and mount textile.\nThe Field Museum thangka collection comprises 382 thangkas, of which 373 were collected by Anthropology Curator Dr. Berthold Laufer in1909 during the 3-year Blackstone Expedition to China and Tibet. The collection, amongst the three largest in the United States, has a number of thangkas of very high cultural and artistic significance, but is especially important for its anthropological value. Collected during one year, the collection provides a cross section of thangka use in eastern Tibet at that time: it includes stylistic and functional thangka sets, household and temple thangkas, new and well used thangkas, thangkas of exquisite and humble aspect. The collection is also significant for containing a high proportion of Bon thangkas – those whose images relate to popular folk culture in which features of the earlier indigenous religions in Tibet were incorporated into Buddhist iconography. These have received less scholarly attention than other categories of thangkas and it would be valuable to make the collection stable enough to be available to scholars.\nThe Anthropology thangka collection comprises about 140 traditionally mounted paintings, 175 unmounted paintings, 30 scroll mounted paintings, and 15 framed or otherwise previously restored or nontraditionally mounted paintings. We are conducting a condition survey of the whole collection to determine the treatment needs of each and develop a budget and strategy for funding. During the survey, preservation of the thangkas and paintings is improved by archival housing, and high resolution digital imaging is done to document their condition, document the painted images to make them more available to scholars. The survey includes details on presence and condition of each component of the mount and painting, and an estimate of the type and time of conservation treatment needed.\nRehouse to reduce factors causing damage to the thangkas.\nA unique housing system was designed and implemented for the traditionally mounted thangkas. Most damage to the thangka occurs as it is rolled and handled, cracking and flaking the paint and creasing and tearing the textile. A special set of 12 thangkas depicting the Dalai Lamas and Arhats are stored flat in map cases to eliminate rolling. Some other thangkas were previously restored and mounted in such a way that they are extremely stiff; these can only be housed flat in boxes. We do not have room to store the whole collection flat, so remaining mounted thangkas have been rolled on dowels with 2” of soft padding (soft to accommodate cockles in the painting, and large diameter to reduce the curvature of the rolled painting). The dowels are suspended in clam shell archival boxes so that the weight of the thangka is not carried by parts of the painting. The clam shell boxes protect the thangkas from dust, light and pressure and allow efficient use of storage space.\nThe 175 unmounted paintings were previously held with corner strips on heavy paper sheets and put in a glassine folder. These materials were tested and found to be archival (neutral pH), but provided insufficient support and were difficult to handle. Drop front archival boxes were made to accommodate the folders, providing a rigid, dust and light free storage environment.\nHigh resolution digital imaging of obverse and reverse of each thangka to assist scholars in low impact access to the thangkas.\nDuring the survey, high resolution tiff images are made of obverse and reverse, in incident and raking light, of every thangka surveyed. The availability of digital images renders the thangkas potentially accessible to scholars around the world and reduces the amount of damaging handling to which the thangkas are subjected. Imaging also documents the thangkas’ condition at this point in time, greatly increasing our ability to detect changes in condition over time.\nImages of the Field Museum paintings are posted on the website of Himalayan Art Resources (www.himalayanart.org) curated by Jeff Watt and David Pritzker.\nDevelop conservation treatment protocols appropriate to the specific thangka styles and types of damage to allow accurate estimate of resources needed.\nTraditionally mounted thangkas were of two styles: those mounted in plain, moderately course cotton or wool cloth; and those mounted in brocade fabric with fine silk covers and ribbons. The coarse cloth tended to have considerable loss from rodent eating, and to have tears at the top and bottom. The brocade and silk tended to have tears and losses where the fabric was stressed. In both cases, standard treatment comprised flattening and aligning the fabric and attaching a support patch or lining to the reverse using sewing techniques. The fabrics and threads used matched the weight of the original fabric – polycotton for the coarser cloth and silk crepeline and hair silk thread for the finer silks and brocades.\nThe support fabric of some paintings was torn or fractured. These were treated with a heat reactivated patch on the reverse; the weight of the patch paper matching the weight of the painting support fabric and thickness of ground.\nMany paintings suffered losses and instability from cracking, cupping and flaking of the ground and paint. Most paint is matte and thick, considerably restricting the type and application of consolidant that can be used to secure the paint but not cause darkening or sheen. Repeated applications of a very dilute methylcellulose paste in water/ethanol was found to penetrate the paint, provide enough plasticity to allow setting down of cupped paint, and dry without altering the paint appearance.","- 10 Years of Asian Art\n- Asian Art History\n- Participant News\n- Asian Art News\n- In the Press\n- Photos & Videos\n- AWNY Contemporary\n- Interior Design\n- 2017 Gallery Hop\nWhat’s happening in Asian art\nLooking Ahead, Looking Back: Ten Years of Asian Art (Part 4)\nSeptember 17, 2018\nAs we look towards Asia Week New York's tenth anniversary in March 2019, we're also taking a look back at the event's most memorable works of art. This is part 4 of a multi-part series in which we are showcasing the most important objects sold by our participants over the last decade. Check back here often or subscribe to our newsletter to stay updated. Below, the stories of four incredible objects:\nAN IMPORTANT PORTRAIT FROM CARLO CRISTI\nPortrait of Phagmotrupa\nDistemper on cotton\nTibet, 13th c.\n15 3/8 x 11 5/8 in. (39 x 29,5 cm)\nThis important tangka shows the portrait of Lama Phagmotrupa (1110-1170 CE), guru of Tashipal, founder of the Taklung monastery (founded in 1180 CE), and one of the main seats of the Kagyu sect. On the reverse, a dedication by Onpo Rimpoche, a successor of Tashipal who was shortly abbot of the Taklung monastery, confirm the date of this tangka to the 13th century. Portraiture of the gurus (masters), produced as realistically as possible but with iconic signs of divinity, coincided with the development of monasticism in Tibet. Phagmotrupa seems to have been one of the first Lamas to be represented on canvas, and there exist only a small number of his portraits. These paintings were made for meditation, to pass on teachings to disciples, and to recognize the historical importance of the master.\nThe tangka was sold in 2008-2009 to a private European collection.\nAN EXQUISITE VASE AT ZETTERQUIST GALLERIES\nIn the words of dealer Eric Zetterquist:\n\"In 2012, on the occasion of the 20th anniversary of my gallery, I had the immense pleasure of handling this exquisite and rare Guan-Yao vase. Authentic Guan-Yao (“Official Ware”) is one of the most rare and sought-after of all Chinese ceramics. Produced exclusively for the Imperial Court in the Southern Song Dynasty, adjacent to the Imperial Palace grounds in the Laohudong Kiln Site. Very few of these pieces survived, and most are in public collections. Originally influenced by the Northern Song Dynasty Ru-Ware forms, this piece possesses not only a fine Southern Song form, (often repeated in Longquan celadons), but an ideal Guan-Yao glaze. It is bubbly and slightly opaque with a blue-green hue, and is covered overall with an intentional craquelure that has colored to alternating translucent silver and brownish-gold colors. An identical piece is in the collection of the Victoria and Albert Museum, London.\"\nA SYMBOLIC CONTEMPORARY DRAWING AT KAI GALLERY\nHui Chi Lee\nGraphite & color pencil on paper\n28.5 x 36.5 inches\nDuring Asia Week New York 2018 at Kai Gallery, this piece by Taiwanese artist Hui Chi Lee caught the eye of a curator from the Herbert F. Johnson Museum of Art, Cornell University, and the museum subsequently acquired the work. Lee’s name for this series, “Lián: Liàn,” derives from a pair of Chinese homophones, which, depending on the context, mean “to connect” and “to enchain.” To Lee, the seemingly powerless mannequins evoke life in contemporary society. The artist writes, “I am interested in the obscure and anonymous quality of the human form, and I want to guide the viewer to consider the subject matter in a critical, holistic manner… In Chinese tradition, lengthy hair symbolizes longevity. Hair signifies the duration of a life span, an expanse of time of which we are often hardly aware. While we may acknowledge the finitude of life, time is envisioned as somehow endless. Humans favor stability and continuity. Thus, even when one’s comfort and status is threatened or entangled by a chaotic environment, ambivalence seems inevitable. Color is introduced here in a symbolic, metaphorical way. Red symbolizes both a warning and an awakening moment in life.”\nA MONUMENTAL GUARDIAN AT GISÈLE CROËS S.A.\nBuddhist Guardian Weituo, courtesy of Gisèle Croës S.A.\nMing Dynasty (1368–1644), most likely 15th century\nTotal height of figure with stand: 93 in (236.22 cm)\nPhoto by Maggie Nimkin Photography\nThis monumental bronze, standing over six feet in height even without its stone base is a representation of Weituo, a Buddhist guardian figure whose mythology emerged well before the Ming. Dressed in full armor and wearing the ornate helmet of a Chinese general, Weituo, his hands pressed together in prayer and reverence, stands resolute on a square stone base, faithfully guarding the Buddha, his teachings (the dharma), and Buddhist monasteries, monks, and treasures. This extraordinary, powerfully conceived bronze sculpture of General Wei exquisitely captures in his stance and facial expression his dual role as fierce defender of and unwavering believer in the Buddha, the dharma and the daily life of the Buddhist community or sangha. Appropriately, Weituo was also considered a symbol of determination in spiritual training and practice.\nThe following is an account of the history of the object by dealer Gisèle Croës:\nA few years ago, I was walking through a very large private park in company of the owner, and suddenly we arrived at a beautiful pond where I saw from a distance a magnificent sculpture (above). I had a visual shock and asked about this wonderful piece in such a remote area. After a while, I had the good fortune to acquire it.\nWith enthusiasm we decided to study the piece and researched its provenance. This piece was purchased in Beijing in 1918 by a prominent Japanese dealer from Kyoto, Yamanaka Sadajirō. More than a hundred years ago, Yamanaka & Company opened its first shop at 20 West 27th Street in New York City. Several decades later, the main overseas branch of Yamanaka & Company in New York occupied a five-story building, on fashionable upper Fifth Avenue, and there were equally prestigious branch offices in Boston, Chicago, London, Peking, Shanghai, Nara and Kyoto. The main Japanese office remained in Osaka. Many people contributed to the success of Yamanaka & Company, but Yamanaka Sadajirō provided the vision that guided the international operation through its formative years.\nWe asked Professor Annette Juliano from Rutgers University to write an essay on the Weituo. This essay was published in “Gisèle Croës, Matter and Memory Part II,” Brussels, Belgium, 2014, (pages 9–28).\nWhen exhibited in New York at Gagosian Gallery in March 2014, the piece raised an immense interest and was admired by a very large audience of collectors, Institutions and museums. The object was sold in 2014 to a private Chinese collector."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4af72f61-62dd-4c79-96a3-eb379d6423e5>","<urn:uuid:b25b6cf8-d912-4f8e-acb8-265299776de5>"],"error":null}
{"question":"How did the development of the Tristan chord in classical music progress from Spohr to Wagner? What were the similarities between Spohr's Jessonda and Wagner's Tristan und Isolde?","answer":"In Spohr's opera Jessonda (1823), the main character's entrance aria contains a passage that is almost identical to Wagner's famous Tristan chord. Both works share the same key, the same 6/8 meter, and similar rhythms. The only difference is in one note - where Spohr uses a C-natural as an upper-neighbor embellishment, Wagner later used an A-sharp as a chromatic passing tone. This similarity raises questions about whether Wagner might have been influenced by Spohr's work when composing Tristan und Isolde, though this connection is not widely acknowledged, and no one refers to it as 'The Jessonda Chord.'","context":["Before reading up a bit for this post I was aware that Louis Spohr (named Ludwig, but he preferred the French equivalent Louis), was a virtuoso violinist and younger contemporary of Beethoven, widely known for inventing the violin chin rest in 1820. I’m very grateful to him for that; I really don’t enjoy playing my violin without one. It’s a fabulous creation. I can’t imagine how violinists managed during the baroque era!\nHe also had the foresight in 1812 to use letters on musical scores as an aid to rehearsal. So he was quite the innovator in many ways. When our conductor at Aylesbury would bark something like: “You made a real hash of the passage between D and E, so let’s go back to D again,” you’d be able to find the place in the music very quickly and easily. I had no idea that this bright idea was down to Louis Spohr.\nLouis Spohr: (5 April 1784 – 22 October 1859)\nI was pleasantly surprised and impressed to say the least, when I started to discover his many other accomplishments. His musical compositions were largely unknown to me, despite his immense popularity in classical circles during his lifetime. His fame dwindled after his death and only a small portion of his work remains in modern repertoire.\nIt couldn’t have been easy crafting your notes in the shadow of Mozart and at the same time as the likes of Beethoven, Hummel and Schubert, but to his credit he followed his own path within the parameters of early romanticism.\nHe was widely known and respected in Europe during the early 19th century as a virtuoso violinist, conductor, teacher and composer. He was probably the most famous violinist in Europe until Paganini arrived on the scene with his own fiery brand of pyrotechnics.\nLike his friend Beethoven, he also believed in democratic freedoms and was known to possess a noble character. He was unusually tall for the time, being over six-foot. Unlike Beethoven, who was the epitome of the lonely, tortured artist, Spohr was a family man who enjoyed a happy social life and varied pursuits like swimming, ice-skating, hiking, gardening, as well as considerable skill as a painter.\nWhilst Beethoven was creating music that was innovative, immortal and ‘new’ to the ears of early 19th century concert goers, Spohr appears to blend in with the tastes of the zeitgeist, certainly nothing that would upset the apple cart. But when tastes changes, as they invariably do over time, his more traditional music became eclipsed by Beethoven and Schubert.\nThe so called Biedermeier period (1812 – 1848), saw the rise of the middle class in Europe, paralleling urbanisation and industrialisation, when access to the arts expanded to attract a larger number of people. Biedermeier encompassed literature, music, the visual arts, interior design and architecture.\nIt seems that Louis Spohr was a product of his era, whereas Beethoven was a musician for all-time. Rather sadly he is sometimes referred to as the ‘forgotten master’.\nAs I’ve discovered, his music was mostly written in the romantic genre and I was surprised at the many different instruments he wrote for aside from the violin. I believe his music should be more widely heard and performed than it is. He may not be a Mozart or a Beethoven, but his achievements are worthy of admiration.\nLouis was born to musical parents; his mother being a talented singer and pianist whilst his father was an amateur flutist. The young Spohr however, despite starting out on the harp, took to the violin. His first tutor was a violinist named Dufour, who saw an opportunity for his pupil to further his musical learning at the Duke of Brunswick’s court. He joined the ducal orchestra aged 15.\nThree years later he was sent on a year-long study tour of St Petersburg and Moscow with his tutor, violinist Franz Anton Eck. He also wrote his early compositions during this time.\nAfter Spohr returned to Brunswick the duke allowed him to make a concert tour of northern Germany. An influential music critic, Friedrich Rochlitz happened to be in the audience during his recital in Leipzig in December 1804, and wrote a glowing review of both his virtuosity and his opus 2 violin concerto in D minor. Hence Spohr was promptly catapulted into the pantheon of revered violinists of the early 19th century.\nSpohr became orchestral director at the court of Gotha between 1805-1812 until he landed the job of leader of the orchestra at the Theater an der Wien in Vienna from 1813-15, where he met Beethoven.\nHis career progressed as he moved to Frankfurt where he took up the post of Opera Director between 1817-19, and thanks to the recommendation of fellow composer, Carl Maria von Weber, he was appointed Court Kapellmeister at Kassel from 1822 until his death on 22nd October 1859. Incidentally, Kassel was also the place where the Brothers Grimm wrote most of their fairy tales in the early 19th century.\nDuring his career and at the height of his popularity Spohr travelled to England on five separate occasions, and was named in an aria from Act 2 of Gilbert and Sullivan’s Opera, The Mikado.\nSpohr was a prolific composer of many genres: violin concertos, symphonies, clarinet concertos, harp and chamber music, lieder, cantatas, oratorios and operas. I’ve selected a few pieces from each genre to give an overview of his style and talents. He composed a total of 290 works.\nAlthough he wrote eighteen violin concertos, six violin sonatas and various duos for violin and harp he did not set out to write purely for the violin in the same way that Viotti, Kreutzer, Vieutemps or Wieniawski did.\nOf particular note is his Violin Concerto No. 8 in A minor, Op. 47 ‘In modo d’un scena cantate’ that just sings in the most mournful, lyrical melody when performed by the incomparable Jascha Heifetz:\nViolin Concerto No. 2 in D minor, Op. 2 by Christiane Edinger and the Slovak Radio Symphony Orchestra:\n‘Duo für 2 Violinen’ with David and Igor Oistrakh:\nViolin Concerto No. 7 in E minor, Op. 38 (3rd movement) with Takako Nishizaki, Libor Pesek and the Bratislava Philharmonic Chamber Orchestra:\nSonata in D Major for Violin and Harp, with Sophie Langdon and Hugh Webb:\nDuo for Violin and Viola in E minor, Op. 13 with Antje Weithaas and Tabea Zimmermann:\nSonata for Violin and Harp in C minor, a delightful recital by Jean-Jaques Kantorow and Susanna Mildonian:\nConcertante No. 1 in G Major, WoO 13 for Violin, Harp and Orchestra (Adagio) with Ursula Holliger, Hansheinz Schneeberger and English Chamber Orchestra:\nConcertante No. 2 in E minor, WoO 14 for Violin, Harp and Orchestra, (3rd movement) performed by English Chamber Orchestra, Ursula Holliger and Christoph Poppen under the baton of Heinz Holliger:\nLike Beethoven, Louis Spohr has nine symphonies to his name, and a tenth unfinished!\nHis Symphony No. 4 in F Major, Op. 86 ‘Die Weihe der Töne’ (The Consecration of Sound), was based on the poems of the same name by Carl Pfeiffer.\nOverview from Naxos:\nThe first movement opens with a slow introduction, illustrating the profound silence before the creation of sound. The Allegro that follows, in traditional sonata form, includes the gentle sound of the breeze and woodwind bird-song, before the storm that forms the central section of the movement, to die out in the distance in the final bars. The second movement demonstrates the function of music as lullaby, dance and serenade, the last with a solo cello. All three finally combine in a conductor’s nightmare of varying bar-lines and tempi.\nThe third movement shows the role of music as an inspiration to courage, here with a narrative element. Soldiers depart for battle, while in a central trio section those remaining behind express their anxiety, followed by the victorious return of the marching troops and the song of thanksgiving. The final movement buries the dead, to the sound of the chorale ‘Begrabt den Leib’, leading to ultimate consolation in tears.\nHere is a recording by the Budapest Symphony Orchestra and Alfred Walter:\nSymphony No. 6 in G Major, Op.116 ‘Historical Symphony in the style and taste of four different periods’ composed in 1840, performance by Concertgebouw Amsterdam and Ton Koopman:\n- Largo-Grave (Bach-Händel’sche Periode, 1720)\n- Larghetto (Haydn-Mozart’sche Periode, 1780\n- Scherzo (Beethoven’sche Periode, 1810)\n- Allegro vivace (Allerneueste Periode, 1840)\nSymphony No. 9 Op. 143 ‘The Seasons’ performed by Slovak State Philharmonic Orchestra under Alfred Walter:\nHarp and clarinet works\nSpohr wrote a significant number of works for, and including the harp, which is entirely understandable as his first wife, Dorette Scheidler, was a renowned harp virtuoso. They were married for 28 years until her death in 1834.\nFantasie for Harp in C Major, Op. 35 with Lena-Maria Buchberger:\nClarinet Concerto No. 1 in C Major, Op. 26 (3rd movement) with Paul Meyer and the OCL:\nClarinet Concerto No. 2 in E-Flat Major, Op. 57 with Julian Bliss:\nClarinet Concerto No. 4 WoO 20, ‘Rondo al espagnol’ with Paul Meyer and Orchestre de Chambre de Lausanne:\nAmong his output of chamber music are 36 string quartets, 7 string quintets, a string sextet and 5 piano trios. Probably the most performed in modern repertoire are the Nonet and the Octet, for your listening pleasure below.\nOctet in E Major, Op.32 with the Vienna Octet:\nNonet for Wind Quintet and Strings in F Major, Op. 31 with the Consortium Classicum Conducted by Dieter Klöcker:\nPiano Trio No. 2 in F Major, Op. 123 with the Hartley Trio :\nConcerto for String Quartet & Orchestra Op. 131 (1st movement), composed in Kassel during the last three months of 1845, performed here by Leipziger Streichquartett, Leipziger Kammerorchester and Sebastian Weigle:\nSix German Songs\nSpohr’s Six German Songs for Soprano, Piano and Clarinet, Op. 103 are a delightful indulgence of his romantic side! These lovely performances are by Helen Donath, Klaus Donath and Dieter Kloecker:\n- Be still my heart\n- In a lilac bush sat a little bird\n- Longing: I look into my heart\n- Cradle Song: All is quiet in sweet peace\n- The Secret Song: There are secret pains\n- Awakening : Why do you stand and ponder\nOf the ten operas Sphor composed the two most popular are Jessonda and Faust.\nJessonda was written in 1822 to the libretto by Eduard Gehe, based on Lemiere’s novel, La veuve de Malabar. Under Spohr’s baton it was first performed on 28th July 1823 in Kassel, and tells the story of an Indian princess (Jessonda), who is condemned to burn on her husband’s funeral pyre; as was the custom for a widow of a recently departed Rajah. She is ultimately spared by a young Brahmin (Nadori) and eventually rescued by the Portuguese General she was in love with (Tristan d’Acunha). It was popular in 19th and 20th century repertoire until it was banned by the Nazis.\nOverture to Jessonda:\nJessonda – Selected highlights with Gerd Albrecht leading the Hamburg State Philharmonic Orchestra and Opera Chorus and Julia Varady and Dietrich Fischer-Dieskau in the main roles:\nThe Tristan Chord\nSo, the big question is, did Wagner take inspiration from Spohr to create his famous chord?\nI might ignite some controversy here!\nMuch has been made of the Tristan Chord in the opening bars of Wagner’s beautiful, romantic opera, Tristan und Isolde; but composer and musician Dr. Dick Strawser, who was quite taken with aspects of Jessonda noticed the following:\nNow, what I found in the vocal score of Spohr’s Jessonda – opening the main character’s entrance aria – was an almost identical passage: the same key, the same 6/8 meter and (as I recall) the same rhythms but, more importantly, virtually all the same pitches but one – the next-to-last note in Spohr is a C-natural, an “upper-neighbor” embellishment, where Wagner’s A-sharp is a chromatic passing tone.\nSpohr composed his opera in 1823.\nYet no one calls it “The Jessonda Chord.” Nor does anyone accuse Wagner of plagiarism, either.\nWas Jessonda so forgotten 25 years later that Wagner could steal this, even subconsciously, without anyone noticing? Hmmmm…\nWagner aficionado Stephen Fry:\nA beautiful aria ‘Ich bin allein’ from Act Two of Faust:\nSpohr the conductor\nLouis Spohr was one of the first musicians to use a baton when conducting. Imagine the orchestra’s surprise when their leader, instead of using his bow, put his violin down, took a wooden stick out of his pocket, got up and turned the music stand to face the orchestra where he proceeded to wave it about in time with the music.\nLater in his musical career after he had scaled back his violin performance schedule, his reputation as an eminent conductor meant that he continued to receive many invitations to music festivals and various events, including the unveiling of Beethoven’s statue in Bonn in 1845.\nHe championed Wagner’s music and also played Beethoven’s late quartets, even though it seems he was as baffled by them as audiences were at the time. He also played with Beethoven during rehearsals of the Piano Trio No. 1 in D Major, Op. 70 ‘The Ghost’ in 1808, commenting on how Beethoven, almost devoid of his earlier technical abilities, hammered away on the ivories and that his piano was out of tune, but he must have made allowances for Ludwig’s hearing loss.\nA wonderful recording of ‘The Ghost’ with Daniel Barenboim, Jacqueline du Pre and Pinchas Zukerman:\nSpohr made many valuable contributions to violin technique in the early 19th century and was a proponent of the Mannheim School. He taught around 200 pupils during his career. If I ever find myself in Kassel I’ll be sure to visit his museum there!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:2c0213af-6493-42ca-8f3a-74f9b6521841>"],"error":null}
{"question":"How can FODMAPs affect both regular food consumption and tube feeding, and what are the specific components to watch out for in both cases?","answer":"In both regular food consumption and tube feeding, FODMAPs can cause symptoms like bloating, gas, and diarrhea. For regular foods, key components to watch include wheat (containing fructans), lactose from dairy, excess fructose, and polyols. In tube feeding formulas, the main concerns are fiber components such as inulin (chicory root extract), fructo-oligosaccharides (FOS), trans galactooligosaccharides, and raffinose from legumes. While protein is generally not problematic unless associated with high lactose content, and fats are usually well-tolerated, the fiber additions in tube feeding formulas can increase gas and stool frequency. Portion size and the cumulative effect of consuming multiple high FODMAP items over time are important considerations in both feeding methods.","context":["Could It Be FODMAPs Triggering Your Gut Symptoms?\nSo you know you are not celiac, and you notice you feel less bloated, less discomfort and generally have a happier tummy when you avoid gluten – but you do still get some symptoms occasionally.\nIt could actually be FODMAPs and not gluten that is the culprit. Wheat contains fructans which are poorly absorbed and a gluten free diet will substantially lower your FODMAP intake. Naturally gluten free grains such as rice, maize and quinoa are all low FODMAP too.\nModifying FODMAP intake is a highly effective approach for irritable bowel syndrome (IBS) and other functional bowel disorders (bloating, wind, pain). This approach can as been shown to help 3 out of 4 people control over their symptoms rather than the symptoms controlling them.\nFODMAPs is an acronym for the group of sugar molecules that can be poorly digested.\nOligo-saccharides (fructans and galacto oligosaccharides/ GOS)\nDi-saccharides (lactose – milk sugar)\nMono-saccharides (excess fructose)\nPolyols (eg sorbitol, mannitol)\nFODMAPs can be poorly absorbed in the small intestine and are then fermented by our gut bacteria in the large intestine to produce gas, and cause more water to pass through the bowel. This happens in all of us, some gas means our gut bugs are being fed! However in people with IBS this can trigger symptoms such as bloating, pain, and diarrhea. This video shows how\nRemember when modifying your FODMAP intake portion size and the ‘additive effect’ of eating multiple higher FODMAP foods over the same day or a few days are very important.*\nFor example a small serve of broccoli may be tolerated but a large serving may not. Eating wheat once a day may be tolerated but including wheat/ rye more frequently may not be.\nThe best resource is the Monash University FODMAP diet with a comprehensive food guide and recipes. The Monash team test the FODMAP content of foods in their lab and regularly update the app.\nAs the FODMAP diet can seem complex and confusing, advice is best provided by a dietitian who has received training and is experienced in this area. They can help you to individualize the process and also ensure that your diet is nutritionally balanced.\nThe first phase is the low FODMAP diet which restricts these sugars for 2-6 weeks. The second phase is to re-challenge with some of the high FODMAP foods. Once you have identified the major triggers the third phase is to move to a long term modified FODMAP diet which allows symptom control. It is essential not to get stuck in the low FODMAP phase for too long as the diet can lower levels of some of the beneficial bacteria.\nThe chart below will give you a good idea of common problem foods and substitutions during the diet. Eggs, fresh meat, fish and chicken are naturally low FODMAP\nWhat About Non-Celiac Gluten/ Wheat Sensitivity\nDiagnosing NCGS remains complicated – the current research shows that it does exist but that we do not have an accurate test for it yet. It may be other non-gluten proteins in wheat (eg amylase trypsin inhibitors) that cause symptoms, the research is on-going.\nThe gold standard for food intolerance testing is to exclude the suspected food, followed by gradual reintroduction to see if symptoms reoccur – in research this would be blinded which of course is tricky in the real world!\nConsider that it may be FODMAPs that are contributing to symptoms. If FODMAPs allow adequate control of symptoms you have your answer.\nIf you do feel better on a gluten free diet then naturally you are likely to stick to it but do ensure that you are not restricting your diet un-necessarily.\nLastly, remember it is vital to be properly tested for celiac disease before you eliminate gluten from your diet .","- Meet Consumers/Patients\n|Nutrition and You: FODMAPs|\nFODMAPs is an acronym for “fermentable oligo-, di- and mono-saccharides and polyols.” FODMAPs are short-chain, small molecular weight carbohydrates that are poorly absorbed in the GI tract. As a result, they are rapidly fermented by normal bacteria in the intestine, causing GI distention, increased gas production, pain, and fluid delivery to the GI tract.\nPeople with irritable bowel syndrome (IBS) and some other GI disorders have used a diet low in FODMAPs to treat symptoms with great success. The diet includes eliminating or limiting certain foods containing fructose, fructo-oligosaccharides (fructans and galacto-oligosacccharides), lactose, and polyols (sugar alcohols). Modification of the diet to limit FODMAPs (less than 4 gm/day) has been shown to be effective in decreasing symptoms in 75 percent or more IBS patients. After four to six weeks, higher FODMAP foods are re-introduced, carefully and systematically, as tolerated.\nSo how does this affect you as an Oley consumer? If you are receiving tube feeding and you have noticed bloating or diarrhea that you cannot seem to manage and cannot attribute to another cause, it might be too many FODMAPs in your formula. If you are a home parenteral nutrition (HPN) consumer and you get these GI symptoms after eating, it may be because you have consumed too many FODMAP foods.\nFODMAP DietManaging FODMAPs food intake is a little tricky. You need to work with a registered dietitian who has expertise in this area. While primarily problematic for people with IBS, some of these foods can be troubling for anyone with a compromised GI tract.\nFor the most part, protein is not a culprit unless the protein is associated with higher lactose content. When supplementing, look for protein with low or zero lactose, like brown rice protein, egg protein, or whey protein isolate. Fat also is not usually a problem—just not too much! Food sources of FODMAPs are more pronounced in “real food” or blenderized formulas, so check the formula label and consider the contents.\nCarbohydrates in tube feedings that can cause symptoms are most often associated with the fiber components. Some examples of the high FODMAPs fibers are inulin (chicory root extract), fructo-oligosaccharides (FOS), trans galactooligosaccharides, and raffinose, which is a fiber from legumes. Added fiber may be good for most people, but depending on your GI system you may need to limit these types of fibers. These fibers have been added to enteral (tube feeding) formulas because they have a shorter chain and do not increase the viscosity (thickness) of the formula. However, formulas supplemented with FOS have been associated with increased gas and stool frequency. This will not happen for everyone, but it is something to consider if you aren’t tolerating your formula when everything else looks right.\nBottom line: What you eat matters, whether it is administered through a tube or as a supplement to HPN. Keep in close contact with your dietitian, who can assist you in managing GI symptoms.\nCarol Ireton-Jones, PhD, RDN, LD, CNSC FASPEN; reviewed by Laura Matarese, PhD, RDN, LD, CNSC, FASPEN.\n2/6/2017 » 2/10/2017\nFeeding Tube Awareness Week\n2/18/2017 » 2/21/2017\nOley exhibit at A.S.P.E.N.'s Clinical Nutrition Week"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:58d9c109-4933-4d58-8591-c0411f57f029>","<urn:uuid:86c0d1e8-7140-45a5-9f5e-5f10de4faab8>"],"error":null}
{"question":"As a researcher in machine learning, I'm curious - do both the traffic sign recognition systems and the ATLAS trigger system utilize neural networks for their core functionalities?","answer":"Yes, both systems utilize neural networks but in different ways. The traffic sign recognition systems employ neural networks for detection and recognition tasks, with some implementations specifically using deep quadruplet networks for unseen traffic sign recognition. The ATLAS trigger system uses Graph Neural Networks (GNNs), which are specifically deployed on FPGAs for online track reconstruction. GNNs are used because they are particularly well-suited for track reconstruction tasks by learning on structured graph representations of hit data.","context":["Stellenangebote - Master Arbeiten\n- Alle Stellen\n- Master Arbeiten\n- HiWi Jobs\n|Experimentelle und theoretische Tests der Quantenmechanik bei niedrigsten Energien||Maarten DeKieviet|\nDas in Heidelberg entwickelte 3He-Atomstrahl-Spinecho-Spektrometer ermöglicht das Vermessen kleinster Energieänderungen (~100 peV) in der Wechselwirkung zwischen Atomen und Felder oder Materie. Das Experiment ist in seiner Form weltweit einzigartig und erforscht derzeit:\nFür diese spannenden und grundlegenden physikalischen Fragen können wir Hilfe gebrauchen, sowohl am Experiment, als auch bei der Theorie. Interesse mit zu machen? Dann meldet euch bei:\n|New Physics using Geometric Algebra||Maarten DeKieviet|\nInstead of having special mathematics for all the different fields of physics, Geometric Algebra (GA) supplies a unified and unifying mathematical language for the whole of physics. It not only allows for a geometric interpretation of the constituent elements, it uncovers hidden connections between the otherwise seemingly unrelated mathematical descriptions. „Why hasn‘t anyone told me that before?“ is a regularly heard, awing reaction of students being exposed to this language for the first time. We are currently searching for discrepancies with and extensions of the regular mathematical approach, both theoretically and experimentally. These are exciting times, come and join us!\n|Development of a new pixel detector in High Voltage-MAPS technology||André Schöning|\n|Our group is developing new pixel detectors based on the High-Voltage MAPS (monolithic active pixel detector) technology. Monolithic pixel detectors are in many respects superior compared to standard hybrid silicon detectors. High Voltage-MAPS will be used for the new Mu3e experiment are also considered for the LHC-High Luminosity Upgrades. We offer several bachelor and master theses in this area.|\n|Muon Physics at PSI||André Schöning|\nThe Paul Scherrer Institute (PSI, Switzerland) operates the highest intensity proton accelerator (HIPA) in the world. This facility also provides high intensity muons beams where the muons are produced in pion decays. An upgrade of the accelerator is planned for 2026 with the goal to increase the muon beam intensity by almost two orders of magnitude, thus providing 10^10 muons per second. This upgrade will dramatically increase the discvery potential for rare physics processes like mu->eee (Mu3e Phase II) and mu-> e gamma which are also called the \"golden\" muon decay channels. Simulation and design studies need to be performed to reduce background and optimise the sensitivity.\n|Track Reconstruction with Graph Neural Networks for the ATLAS Trigger System||André Schöning,|\nOur group explores the feasibility of applying modern machine learning algorithms such as Graph NeuralNetworks (GNN) deployed on FPGAs for online track reconstruction within the ATLAS online server farm for the High-Luminosity LHC (HL-HLC) upgrade. GNNs are a powerful class of geometric deep learning methods for modelling spatial dependencies via message passing over graphs. They are well-suited for track reconstruction tasks by learning on an expressive structured graph representation of hit data. A\n|Development of a new pixel detector for 50 MeV photon conversion||André Schöning|\nThe next generation of experiments searching for the charged lepton flavor violating decay mu -> e gamma will potentially exploit an Active Photon Converter for precise measurement of the energy, position and direction. Active photon converters are believed to be superior over standard calorimeters for photon energies of interest (53 MeV). Our group is developing an active photon converter based on High Voltage Monolithic Active Pixel Sensors (HV-MAPS). The group is developing HV-MAPS since more than 10 years and the task of the thesis is to demonstrate the proof of this new concept. If successful, this work would lead to a new experimental concept for the detection of rare mu -> e gamma decays with unprecedented sensitivity. Such an experiment could be installed at the new High Intensity Muon Beamline at the Paul Scherrer Institute which which starts operation in 2028.\nBitte beachten sie:\nUm neue offene Stellen einzutragen, benutzen Sie bitte das Formular auf den internen Seiten.","Traffic sign recognition thesis\nVision based traffic sign detection and analysis for intelligent driver assistance systems møgelmose, andreas trivedi, mohan m moeslund, thomas b trafﬁc sign recognition (tsr) research needs to take into. Road traffic sign recognition: traffic sign recognition: traffic sign recognition code compound urdu/arabic character recognition all the necessary code with images are included in the zip file beside the thesis based on this code can also be found in the pdf formatrun the softare. We are taking a big step towards recognizing traffic signs all over the world by adding support for more than 500 traffic signs globally, together with an appearance-based taxonomy for traffic signs this is the beginning of our journey of recognizing every road sign in the world, no matter where it is. Traffic sign recognition (gtsrb) german traffic sign reco bench 992% accuracy house number recognition (google) street view house numbers 943 % accuracy y lecun object recognition [krizhevsky, sutskever, hinton 2012] conv 11x11/relu 96fm local contrast norm. Study on traffic sign recognition international journal of research studies in computer science and engineering (ijrscse) page 35 representation of an image into something that is more meaningful and easier to analyze [5. Fpga-based traffic sign recognition for advanced an efficient real-time traffic sign recognition system for intelligent vehicles with smart phones, proceedings of 2010 international real time traffic sign recognition system on fpgas, ms thesis, middle east. This thesis report is submitted in partial fulfillment of the requirements for the degree of master of science in computer science and engineering, 2016. International journal of computer applications (0975 - 8887) volume 120 - no24, june 2015 10 traffic road sign detection and recognition for automotive vehicles md safaet hossain.\nThis thesis report is submitted in partial fulfillment of the requirements for the degree of warning traffic sign detection using learning vector quantization & hough transform and recognition abstract: traffic sign recognition (tsr) is used to regulate traffic signs, warn a driver. The research on traffic sign recognition based on deep learning the identification of the traffic signs, as a key component of the intelligent transit system, has a prospect of widespread use in self-driving and driver assistance system. Abstract this thesis presents a new approach, based on human visual perception, for detecting and recognising traffic signs under different viewing conditions. Fpga-based traffic sign recognition for advanced driver assistance systems sheldon waite and erdal oruklu illinois, 60616 email: [email protected] abstract this paper presents the implementation of an embedded automotive system that detects and recognizes traffic signs within a video stream.\nHighway sign recognition study sheet missouri department of revenue no left turn no weather drive carefully in these conditions added lane traffic from another road will be entering the road you are on no merging is necessary because a lane has signs, sign, recognition, study. A real-time traffic sign recognition system based on local structure features kwangyong lim, hyeran byun department of computer science yonsei university. 1 traffic sign recognition using neural network on opencv: toward intelligent vehicle/driver assistance system auranuch lorsakul1 and jackrit suthakorn,2 center for biomedical and robotics technology (bart lab).\nLow cost passive dampers for highway traffic signs by lea ljumanovic for the thesis requirement for the master of science with the construction of the model in addition, recognition is given to kyle hudson for. Co-domain embedding using deep quadruplet networks for unseen traffic sign recognition junsik kim, seokju lee 30th ipiu 2018 2017 vpgnet: vanishing point guided network for lane and road marking detection and recognition seokju lee, junsik kim, jae shin yoon ms thesis, 2015. Thesis title: traffic sign detection based on hsv 'traffic sign detection and recognition: review and analysis', international conference on intelligent and interactive computing (iic 2015), 11-12 august, melaka.\nDesign of traffic light and traffic sign detection system for autonomous vehicle thesis submitted in partial fulfilment of the requirements of. The effect of the poor conditions on the overall traffic sign recognition accuracy is shown and the performances of the classifier for various conditions of roadside images are. This paper introduces a fast traffic sign recognition system developed for a robot 2015 abstract: this paper proposes a unique system for the automatic detection and recognition of traffic signs thesis writing paper publishing research publishing authors guidelines.\nTraffic sign recognition thesis\nAbstract the aim of this thesis is to develop a method able to obtain images of road scenery and recognize traffic signs from these images an almost unlimited source of traffic sign images can be found in google street view, thus the method is able to download images from google street view for any. Honda motor co,ltd honda worldwide traffic sign recognition the system displays traffic sign information to assist safe driving the monocular camera recognizes traffic signs. Citeseerx - document details (isaac councill, lee giles, pradeep teregowda): approval of the thesis: traffic sign recognition.\nWhat is the honda traffic sign recognition system it is a system that remembers the road signs that your honda passes to remind you of the speed limit. Today we are happy to announce another feature requested by our faithful mapillary users - traffic sign recognitionwe have processed over 6 million images and detected all the traffic signs in the united states and europe from now on every image added in these regions will go through our system and will be categorized with traffic sign data. The primary aim of mathematical problems in engineering is rapid publication and dissemination of important [phd thesis], charles iii university a study on traffic sign recognition in scene image using genetic algorithms and neural networks, in proceedings of the. Traffic sign recognition (ijcnn 2012) neuronal membrane on february 1, 2017 i will quit my position at idsia to focus on my startup dan claudiu cireșan senior researcher - dalle molle institute for artificial intelligence phd thesis recunoașterea șirurilor numerice scrise de.\nTrafﬁc sign recognition - how far are we from the solution markus mathias, radu timofte, rodrigo benenson, and luc van gool abstract—trafﬁc sign recognition has been a recurring appli. Traffic sign recognition is the second part of traffic sign detection and shape, and pictogram classification using support vector machines authors authors and affiliations ahmed madani (2005) clustering and prototype based classification phd thesis, fakultät iv elektrotechnik. More and more devices are used in assisting drivers on the road one of them is traffic sign recognition system it uses a front mounted video camera and informs the driver of incoming dangers and restrictions on the road, mainly by speech or special dashboard display traffic sign recognition system is used to avoid accidents, caused by. Traffic and road sign recognition hasan fleyeh this thesis is submitted in fulfilment of the requirements of napier university for the degree of. Abstract— in a visual driver assistance system, traffic sign detection and recognition are important functions this paper presents automatic traffic sign detection and recognition systems based on neural networks and particle swarm optimization."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d0f540bc-e768-435d-a46b-1c03c60c7d9d>","<urn:uuid:3f350b5e-2c9c-470e-954e-a4c4eb24d5af>"],"error":null}
{"question":"What are the key benefits of genetic counseling for BRCA mutation carriers, and what emotional challenges might they face during genetic testing?","answer":"Genetic counseling provides several key benefits for BRCA mutation carriers, particularly increasing their likelihood of pursuing potentially life-saving preventive measures. Research shows that women who receive genetic counseling are more likely to undergo genetic testing (32.5% vs 8.6%) and preventive ovary removal surgery. Additionally, genetic counseling can help women understand their cancer risks without causing undue concern. However, genetic testing can bring emotional challenges, including depression, anxiety, or guilt after receiving results. Some people may feel sick even if they never develop cancer, while others might experience guilt if they don't have a mutation that other family members have. The process can also create family tension when sharing results with relatives.","context":["SAN ANTONIO – April 24, 2015 – Genetic counseling is a preferred method for informing women with BRCA1 or BRCA2 genetic mutations and/or a family history of breast or ovarian cancer about their risks for getting such cancers, but little has been known about the impact of genetic counseling on efforts to reduce the risk of ovarian cancer.\nA randomized clinical trial conducted by researchers at Fred Hutchinson Cancer Research Center has found, for the first time, that proactively offering genetic counseling to women at high risk of ovarian cancer can increase the use of prophylactic oophorectomy, or preventive ovary removal.\nM. Robyn Andersen, Ph.D., M.P.H., a member of the Public Health Sciences Division at Fred Hutchinson Cancer Research Center in Seattle, led the study, which was funded by the National Cancer Institute of the National Institutes of Health.\nMany women with a family history of breast cancer who carry a BRCA mutation know they are at increased risk of breast cancer but may not realize they are also at an increased risk for ovarian cancer, Andersen said.\n“Most high-risk women with BRCA mutations have a family history of breast cancer and know about their relatives who died of breast cancer. Fewer have had relatives with ovarian cancer. … It isn’t an intuitively obvious jump to go from, ‘My mom and her sisters had breast cancer’ to ‘I’m at high risk for ovarian cancer,’” Andersen said. “Many women, even those who know a lot about their elevated risk for breast cancer, haven’t heard much or anything, really, about their ovarian cancer and their elevated risk for it.”\nAccording to the National Cancer Institute, women with a BRCA1 or BRCA2 mutation are at highly elevated risk for both breast and ovarian cancer. It is estimated that 55 to 65 percent of women with a BRCA1 mutation and about 45 percent of women with a BRCA2 mutation will develop breast cancer by age 70. Estimates of lifetime risk of ovarian cancer among BRCA1/2 carriers range from 16 to 45 percent. Unfortunately ovarian cancer is often diagnosed at a late stage and has a poor prognosis.\nPreventive surgery, known as prophylactic risk-reducing bilateral salpingo-oopherectomy, or RRSO, greatly reduces the risk of ovarian cancer among women with genetic risk factors. However, a lack of knowledge that a family history of breast cancer confers a risk for carrying a genetic mutation that also increases ovarian cancer risk, coupled with a lack of genetic testing to identify such mutations, can be a significant barrier to informed decision-making.\nAndersen and colleagues aimed to determine whether non-directive genetic counseling could increase women’s knowledge of their ovarian cancer risk and their use of preventive surgery. She also sought to determine the impact of genetic counseling on perceived levels of worry about ovarian cancer risk.\nThe study involved 458 Seattle-area women who reported a BRCA1/2 mutation and/or a personal or family history of breast cancer, no prior ovarian cancer diagnosis and no prior preventive ovary-removal surgery. Half of the study participants were randomized to receive genetic counseling and half to receive standard care. The study participants were then followed over four years and surveyed about their use of genetic counseling, genetic testing and pelvic surgery, including RRSO.\nWomen offered genetic counseling were more likely than the standard-care group to undergo genetic testing (32.5 percent versus 8.6 percent) and ovary-removal surgery in the two-year period of study follow up (10 women in the intervention group versus three in the usual care group). There was, however, some concern that these positive changes could be associated with potentially distressing elevations in women’s levels of worry about and perceived risk of ovarian cancer.\n“We found that genetic counseling can provide women with what they need to pursue risk-reduction activities that could save their lives,” Andersen said. “We also found that it appears to be possible without causing women undo concern about their risk for ovarian cancer.”\nAndersen will present these study results at 8:45 a.m. CT on April 25 during the Society of Behavioral Medicine’s 2015 Annual Meeting & Scientific Sessions in San Antonio. Her presentation is titled: “Changes in Ovarian Cancer Worry and Risk among High-Risk Women after Genetic Counseling.” Other study authors are Jason Thorpe, Kate Watabayashi and Nicole Urban, of Fred Hutchinson, and J. David Beatty, Robert Resta and Charles Drescher, of Swedish Hospital in Seattle. The authors report no financial or other conflicts of interests.\nThe Society of Behavioral Medicine (SBM) is a 2,200-member organization of scientific researchers, clinicians and educators. They study interactions among behavior, biology and the environment, and translate findings into interventions that improve the health and well-being of individuals, families and communities (www.sbm.org).\nThis study does not reflect the policies or the opinion of SBM. Given that this study will be presented at a scientific meeting, the data and conclusions reached should be regarded as preliminary until they are published in a peer-reviewed journal. Funding agencies played no role in this study. There are no conflicts of interest for the investigators.\nAt Fred Hutchinson Cancer Research Center, home to three Nobel laureates, interdisciplinary teams of world-renowned scientists seek new and innovative ways to prevent, diagnose and treat cancer, HIV/AIDS and other life-threatening diseases. Fred Hutch’s pioneering work in bone marrow transplantation led to the development of immunotherapy, which harnesses the power of the immune system to treat cancer. An independent, nonprofit research institute based in Seattle, Fred Hutch houses the nation’s first National Cancer Institute-funded cancer prevention research program, as well as the clinical coordinating center of the Women’s Health Initiative and the international headquarters of the HIV Vaccine Trials Network.","Genetic testing helps estimate your chance of developing cancer in your lifetime. It does this by searching for specific changes in your genes, chromosomes, or proteins. These changes are called mutations.\nGenetic tests are available for some types of cancer. These include:\nGenetic testing may help:\nPredict your risk of a particular disease\nFind if you have genes that may pass increased cancer risk to your children\nProvide information to guide your health care\nNo genetic test can say if you will develop cancer for sure. But it can tell you if you have a higher risk than most people.\nOnly some people with a gene mutation will develop cancer. What does this mean? A woman may have a 45% to 65% chance of breast cancer. But she may never develop the disease. Meanwhile, a woman with a 25% chance may develop breast cancer.\nRisk factors for hereditary cancer\nA hereditary cancer is any cancer caused by an inherited gene mutation. An inherited gene means it is passed from parent to child within a family. The following factors suggest a possible increased risk for hereditary cancer:\nFamily history of cancer. Having 3 or more relatives on the same side of the family with the same or related forms of cancer.\nCancer at an early age. Having 2 or more relatives diagnosed with cancer at an early age. This factor may differ depending on the type of cancer.\nMultiple cancers. When one relative develops 2 or more types of cancer.\nRare cancers. Some types of cancer, such as ovarian cancer, adrenocortical cancer, or sarcoma, are linked to inherited genetic mutations.\nReasons to consider genetic testing for cancer\nGenetic testing is a personal decision made for different reasons. It is also a complex decision best made after talking with your family, health care team, and genetic counselor.\nASCO recommends considering genetic testing in the following situations:\nA personal or family history suggests a genetic cause of cancer.\nA test will clearly show a specific genetic change.\nThe results will help with diagnosis or management of a condition. For example, you may take steps to lower your risk. Steps may include surgery, medication, frequent screening, or lifestyle changes.\nASCO also recommends genetic counseling before and after genetic testing. Learn more about these recommendations on genetic testing for cancer susceptibility on a separate ASCO website.\nOther factors to consider\nGenetic testing has limitations and emotional implications. These may include:\nDepression, anxiety, or guilt. A positive test result means a gene mutation exists. This result may bring difficult emotions. Some people may think of themselves as sick, even if they never develop cancer. Negative test results may also cause difficult emotions. For example, some people may experience guilt if they do not have a gene mutation that other family members have.\nFamily tension. People are generally encouraged to tell family members about test results because they can be important for the health of family members. But this information could also complicate family dynamics. Learn more about sharing genetic test results with your family.\nA false sense of security. A negative result means a specific genetic mutation is not present. But people with negative results may still develop cancer. A negative result only means the person’s risk is average. Each person’s risk for cancer is also affected by other factors. For example, lifestyle, environmental exposure, and medical history.\nUnclear results. A gene may have a mutation not linked with cancer risk. This is called a variant of unknown significance. It means that it is unclear whether the mutation will increase risk. Or people may have mutations that current tests cannot find. Many cancers are not yet tied to specific gene mutations. Also, some genes may interact unpredictably with other genes or environmental factors. And these interactions may cause cancer. So it may be impossible to calculate the cancer risk.\nHigh cost. Genetic testing can be expensive. It is particularly expensive if health insurance does not pay for it.\nDiscrimination and privacy concerns. Some people fear genetic discrimination from test results. Others worry about the privacy of their genetic information. The Genetic Information Nondiscrimination Act (GINA) protects against employment and health insurance discrimination. Discuss related concerns with a genetic counselor or doctor.\nQuestions to ask yourself about genetic testing\nBefore having genetic testing, learn about its risks and limitations. Identify your reasons for wanting a test. And consider how you will cope with test results.\nHere are some questions to help you make a decision:\nDo I have a family history of cancer?\nHave I developed cancer at an earlier-than-average age?\nHow will I interpret the results of genetic testing? Who will help me use this information?\nWill the test results affect my medical care? The medical care of my family?\nIf I have a genetic condition, can I lower my cancer risk?\nA genetic counselor can help you answer these questions. He or she can give you advice and information about the risks and benefits of genetic testing. A genetic counselor also helps people through the genetic testing process. Learn more about what to expect when meeting with a genetic counselor."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:200653cc-1acd-42b2-aa9b-7e158211e169>","<urn:uuid:1c356b00-d237-47b5-ad3a-558703f64326>"],"error":null}
{"question":"As a theology student, I'd like to know how Vatican II views the relationship between Scripture and Tradition compared to modern evangelization needs?","answer":"Vatican II establishes that Scripture and Tradition form a single sacred deposit of the Word of God, which must be accepted and honored with equal devotion and reverence. This theological foundation directly connects to modern evangelization needs, as the ministry of the Word is described as a fundamental element of evangelization at all stages because it involves proclaiming Jesus Christ, the eternal Word of God. The word of God serves a dual purpose in modern evangelization - it nourishes both the evangelizers and those being evangelized, enabling both groups to grow in their Christian life. This unified approach to Scripture and Tradition supports the Church's current missionary mandate to proclaim Christ to all people.","context":["A. What is it?\n“Dei Verbum” is the Vatican II Dogmatic Constitution on Divine Revelation, promulgated by Pope Paul VI on 18 November 1965\nThe phrase “Dei Verbum” is Latin for “Word of God”\nIt is one of the smallest Vatican II Documents (26 paragraphs or roughly 3,000 words in Latin)\nB. What does it speak of?\n“Dei Verbum” addresses the Catholic Church’s beliefs in regards to Sacred Scripture.\n“Dei Verbum” is laid out into 6 Chapters:\n- Chapter 1: Divine Revelation Itself\nSpeaking on the Nature of Revelation, this chapter demonstrates God’s desire to communicate with human beings, revealing the mystery of the Divine Will.\nIt offers a summary of the Salvation History\nIt also emphasizes the Truth of this Revelation and the fact that it is accomplished in such a way that human beings can comprehend it.\n- Chapter 2: Transmission of Divine Revelation\nThe Truth of Revelation, is rooted in Christ’s very person and in his own proclamation of the Gospel; having commissioned the Apostles to carry it forward…, the truth of the\nThe Gospel also lies in the Apostolic Tradition.\nBoth Scripture and Tradition must be accepted and honoured with equal devotion and reverence”.\nTradition and Scripture make up a single Sacred deposit of the Word of God\n- Chapter 3: Sacred Scripture: Its Divine Inspiration and Interpretation\nIt affirms the importance of both- the Old Testament and New Testament\nIt adopts the threefold-process of the Formation of the Gospels with the three levels: (i) the time of the Historical Jesus (ii) The oral preaching of the earliest apostles (iii) The time of the Evangelists\n- Chapter 4: The Old Testament\nThe plan of salvation was spoken through the authors of the Old Testament.\nIts purpose was to prepare for the coming of the Christ and to show to all, how God interacts and deals with mankind in justice and mercy.\nGod wisely arranged for the New Testament to be hidden in the Old, and the Old to be made manifest in the New. While Christ made the new covenant with His blood, the Old Testament sheds light on and explains this mystery.\n- Chapter 5: The New Testament\nThe New Testament stands as a Perpetual and Divine Witness to the Reality of Salvation.\nThe Gospel Authors wrote about things handed on by word of mouth or in writing, sometimes a synthesis, sometimes as a proclamation, but always the honest truth about Jesus.\n- Chapter 6: Sacred Scripture in the Life of the Church\nThe Church has always venerated the scripture together with the Tradition as the supreme Rule of Faith.\nThe Church encourages the study of the Church Fathers as well as those exegetes who so well illuminate the teaching within the scriptures.\nIndividuals should read with enthusiasm, following the mind of the Church.\nAll clergy must read the scriptures with diligence. The same is encouraged for the laity and Religious. All faithful should not forget that prayer should always be the companion to reading God’s Word.\nC. Pointers for Reflections\n- “Dei Verbum” is considered as one of the important achievements of the Vatican Council II since its implications is for the treatment of Sacred Scripture itself.\nIt accords rightful significance to the Bible as the special locus of Divine Communication or Divine Revelation.\n- It presents three key principles of Catholic biblical interpretation:\n(i) Pay attention to the content and unity of all the Sacred Scriptures.\n(ii) Read and interpret the Bible within the living tradition of the Church.\n(iii) Keep in mind the coherence of all the truths of revelation\n- The understanding from “Dei Verbum” is enshrined in the Catechism of the catholic Church (CCC), affirming reading Scripture for its four classical sense – the literal sense, and then the spiritual sense divided into three: the allegorical, topological, and anagogical senses.\nThe allegorical sense (Typology) concerns how the Old and New Testaments relate, the topological sense is the moral sense, and the anagogical sense concerns the soul’s progress to heaven.\nD. What virtues/points can we pick up from the “Dei Verbum” for this Season of Lent?\n- Making it a Daily Habit to Read the Bible\n- Studying the Bible and going deeper into understanding the meaning of Scripture in our daily life\nE. Tips to practice these virtues\n- Set apart a time, daily, to read God’s Word.\nJust as our meals become a daily “must”, so should the Bible be part of our daily “sustenance for strength”\n“Ignorance of Scriptures is ignorance of Christ” says St Jerome\nChrist is the primary and ultimate revelation of God. So the more we read and reflect on Scripture, the more we can know Him and love Him\n- Learn, practise and revive the Catholic Tradition of the “Lectio Divina”(= a Latin term, means “divine reading”)\n(i) The first stage is LECTIO (reading): Read any passage of the Word of God, slowly and reflectively so that it sinks into us\n(ii) The second stage is MEDITATIO (reflection): Think about the text we have chosen and ruminate upon it so that we take from it what God wants to give us\n(iii) The third stage is ORATIO (response): Leave thinking aside and simply let the heart to speak to God.\n(iv) The final stage is CONTEMPLATIO (rest): Let go of our own ideas, plans and meditations and also holy words and thoughts. Simply rest in the Word of God and listen, to God, who speaks within us with a still small voice.\nAs we listen, we are gradually transformed from within and this will have a profound effect on the way we actually live.\nMay this Lent and the familiarity with “Dei Verbum” – the Vatican II Dogmatic Constitution on Divine Revelation – help us to grow in our acclamation: “Eureka – I have found the Lord”\n(The Full Text of “Dei Verbum” can be found at:\nGod Bless! Live Jesus!","- Prayer and Worship\n- Beliefs and Teachings\n- Issues and Action\n- Catholic Giving\n- About USCCB\nThe ministry of the Word is a fundamental element of evangelization through all its stages, because it involves the proclamation of Jesus Christ, the eternal Word of God.\n“The word of God nourishes both evangelizers and those who are being evangelized so that each one may continue to grow in his or her Christian life”\n(National Directory for Catechesis [NDC] [Washington, DC: United States Conference of Catholic Bishops, 2005], no. 17).\nby Alan Schreck, PhD\nProfessor of Theology\nFranciscan University of Steubenville\nIn his apostolic letter on the beginning of the new millennium, Blessed John Paul II pointed to the Second Vatican Council \"as the great grace bestowed on the Church in the twentieth century: there we find a sure compass by which to take our bearings in the century now beginning\" (Novo Millennio Ineunte [NMI], no. 57). Pope Benedict XVI, in his first papal homily in April 2005, referred to this statement and affirmed his own commitment as pope to implement faithfully the teachings of Vatican II.\nHowever, Pope Benedict XVI realizes that not everything done in the Church over the past fifty years in the name of Vatican II has been constructive. In an address given on December 22, 2005, the Holy Father distinguished between a proper interpretation of Vatican II—a \"hermeneutic of reform,\" a renewal in continuity with past tradition—and false interpretations—\"a hermeneutic of discontinuity and rupture.\" He also said \"the Council had to determine in a new way the relationship between the Church and the modern era\" (Address to the Roman Curia Offering Them His Christmas Greetings).\nOften in the past, popes felt compelled to condemn the errors of the \"modern world,\" giving the impression that the Church opposed all change and modern ideas. In calling the Second Vatican Council, Blessed John XXIII believed that a different approach was necessary. As he explained in his opening address to the Council on October 11, 1962:\nNowadays, however, the Spouse of Christ prefers to make use of the medicine of mercy rather than that of severity. She considers that she meets the needs of the present day by demonstrating the validity of her teaching rather than by condemnations. . . . To mankind, oppressed by so many difficulties, the Church says, as Peter said to the poor who begged alms from him: \"I have neither gold nor silver, but what I have I give to you; in the name of Jesus Christ of Nazareth, rise and walk\" (Acts 3:6). (Walter Abbott, SJ, ed., The Documents of Vatican II [New York: American Press, Inc., 1966], 716)\nWhat the Church Offers to the World Through Vatican II\nPope John XXIII's statements remind us that the greatest gift that the Church offers to all people is Jesus Christ, and faith in him. The Year of Faith calls us to focus on the great treasure Catholics receive and offer to the world: faith in Jesus Christ and the promise of eternal life through him. The documents of Vatican II affirm the centrality of this message in many ways.The central document of the Council opens with the words \"Christ is the light of humanity\" and continues \"it is, accordingly, the heart-felt desire of this sacred Council . . . that, by proclaiming his Gospel to every creature (cf. Mk 16:15), it may bring to all men that light of Christ which shines out visibly from the Church\" (see Second Vatican Council, Dogmatic Constitution on the Church [Lumen Gentium (LG)], no. 1,in Vatican Council II: Volume 1: The Conciliar and Post Conciliar Documents, ed. Austin Flannery [Northport, NY: Costello Publishing, 1996]). Likewise, the Pastoral Constitution on the Church in the Modern World (Gaudium et Spes [GS]) closes each chapter with an explanation of how Jesus Christ is the key to a proper understanding of each aspect of the human situation and how he provides the solution for the problems and challenges the world faces.\nThe Church believes that Christ, who died and was raised for the sake of all [see 2 Cor 5:15], can show man the way and strengthen him through the Spirit in order to be worthy of his destiny: nor is there any other name under heaven given among men by which they can be saved [see Acts 4:12]. The Church likewise believes that the key, the center and the purpose of the whole of man's history is to be found in its Lord and Master. She also maintains that beneath all that changes there is much that is unchanging, much that has its ultimate foundation in Christ, who is the same yesterday, and today, and forever [see Col 1:15]. (GS, no. 10, in Vatican Council II: Volume 1)\nIn reexamining the teaching of the Second Vatican Council today, any interpretation or implementation of the Council that takes Jesus Christ out of the center is distorted. It is true that Vatican II, in Gaudium et Spes and other documents, recognizes and affirms the goodness and positive values that are present in the modern world (such as scientific progress and the richness and diversity of human cultures) and urges Catholics to work together with all people of good will for constructive solutions to the world's problems (GS, nos. 3, 21, 42, 44). Yet it also states that the teachings of Christ and his Gospel can enrich, guide, purify, and elevate human endeavors and cultures (GS, nos. 37, 39, 58). It is for this reason, among others, that Catholics and other Christians can and must bring their faith into the affairs and discussions of the world and not pretend that they are guided just by secular (\"this-worldly\") considerations. That is why Jesus tells us that we are \"the salt of the earth\" (as long as we do not lose our \"saltiness\") and \"the light of the world\" (Mt 5:13-14).\nBesides bringing Jesus Christ and the wisdom of his teaching into the secular world, the Second Vatican Council clearly taught the primary mission of the Church to proclaim Jesus Christ to all people that all might come to believe in him as their Savior and Lord. The Decree on the Church's Missionary Activity (Ad Gentes Divinitus [AG]) declares, \"The Church, the salt of the earth and the light of the world (cf. Mt 5:13-14), is even more urgently called upon to save and renew every creature, so that all things might be restored in Christ, and so that in him men might form one family and one people of God. . . . The Church on earth is by its very nature missionary\" (AG, nos. 1-2, in Vatican Council II: Volume 1).\nConsiderable confusion exists regarding the Catholic Church's approach to non-Christians, but the Council's teaching is clear and consistent. The best summary may be found in the Declaration on the Relation of the Church to Non-Christian Religions (Nostra Aetate [NA]), which states, \"The Catholic Church rejects nothing of what is true and holy in these religions. . . . [which] often reflect a ray of that truth which enlightens all men. Yet she proclaims and is in duty bound to proclaim without fail, Christ who is the way, the truth, and the life (Jn 14:6)\" (NA, no. 2, in Vatican Council II: Volume 1). Lumen Gentium explains that non-Christians who do not yet believe in Christ through no fault of their own have the possibility of salvation, but, \"very often\" they fall prey to \"the Evil One\" or to \"ultimate despair,\" and the Church must proclaim the Gospel to them \"to procure the glory of God and the salvation of all these\" (LG, no. 16). Vatican II did strongly condemn anti-Semitism and any form of discrimination or harassment based on religion (NA, nos. 4, 5), but it would certainly be a \"hermeneutic of discontinuity\" to claim that Vatican II teaches that there is any person or group who should not hear the Gospel and be given the possibility of conversion to Jesus Christ, the only way to the Father (Jn 14:6). In response to confusion on this issue, the popes since Vatican II have applied a \"hermeneutic of reform\" with regard to the Church's mission to proclaim Jesus Christ.\" Pope Paul VI's 1975 apostolic letter On Evangelization in the Modern World (Evangelii Nuntiandi) and Pope John Paul II's 1990 encyclical letter On the Permanent Validity of the Church's Missionary Mandate (Redemptoris Missio) make the Council's teaching unmistakably clear. Vatican II certainly began a new quest for reconciliation with non-Christians and for the restoration of Christian unity (ecumenism), but this is not contrary to the major emphasis of the Council: to lead all people to unity in Jesus Christ.\nThe Anthropocentric Focus\nAs we reexamine Vatican II, the emphasis on the value and dignity of the human person stands out as particularly critical today. It is notable that the doctrine of the human person is presented most fully in the Council's only pastoral constitution, Gaudium et Spes, but proper pastoral practice is always founded on sound doctrine. GS declares, \"In reality it is only in the mystery of the Word made flesh that the mystery of man truly becomes clear. . . Christ the Lord, Christ the new Adam . . . fully reveals man to himself and brings to light his most high calling\" (GS, no. 22). Humans discover their dignity when they give themselves in love and service to others. \"Man can fully discover his true self only in a sincere giving of himself\" (GS, no. 24).\nUnderstanding each human person's unsurpassed dignity and worth, created in God's image and likeness, is the foundation of the Catholic Church's teaching on life issues, reproductive issues, and social teaching, since society ultimately exists to protect and promote the welfare of the individual (GS, nos. 25-26). Although the Church \"is not committed to any one culture or to any political, economic or social system\" (GS, no. 42), she favors systems that allow the greatest participation of its citizens and protects their rights (GS, no. 31). At the same time, each person must fulfill social obligations and take on social responsibilities as necessary for salvation, and cannot separate faith from daily life (GS, nos. 30, 43).\nOne of the gravest errors of our time is the dichotomy between the faith which many profess and the practice of their daily lives. . . . Let there, then, be no such pernicious opposition between professional and social activity on the one hand and religious life on the other. The Christian who shirks his temporal duties shirks his duties toward his neighbor, neglects God himself, and endangers his eternal salvation. Let Christians follow the example of Christ who worked as craftsman; let them be proud of the opportunity to carry out their earthly activity in such as way as to integrate human, domestic, professional, scientific and technical enterprises with religious values, under whose supreme direction all things are ordered to the glory of God. (GS, no. 43)\nThis balance between the mandate of the Council to evangelize and encourage holiness in others with serving in society in order to transform it according to Christian values as well as to meet human needs is portrayed in the three objectives of the lay apostolate described in the Decree on the Apostolate of the Laity (Apostolicam Actuositatem [AA]). Catholics have the general impression that the Council's call for a more active laity meant that lay people were to be more involved in liturgical or catechetical ministries. Although this involvement is good, Vatican II in Lumen Gentium stresses that the \"special vocation\" of the laity is \"to seek the kingdom of God by engaging in temporal affairs and directing them according to God's will.\" Lay people are \"called by God\" to be in the world, in order to transform and sanctify it \"as from within like leaven\" by engaging \"in each and every work and business of the earth and in the ordinary circumstances of social and family life\" (LG, no. 31). There they carry out their threefold apostolate or mission:\nAmong the sixteen documents promulgated by Vatican II, the four constitutions have special importance; they have been called the \"pillars\" of the Council. While I have spoken of two of these constitutions, the other two constitutions show us both the power of and guidance for the Church, including the laity, to carry out her mission in the world and the final \"end\" or purpose of the Church's temporal work. The Dogmatic Constitution on Divine Revelation (Dei Verbum) [DV]) explains how Catholics understand the Word of God (Dei Verbum). \"Sacred Tradition and sacred Scripture make up a single sacred deposit of the Word of God, which is entrusted to the Church\" (DV, no. 10). This Word of God is the source of guidance and power to carry out and fulfill the Church's mission on earth.\nSuch is the force and power of the Word of God that it can serve the Church as her support and vigor, and the children of the Church as strength for their faith, food for the soul, and a pure and lasting fount of spiritual life. Scripture verifies in the most perfect way the words: \"The Word of God is living and active\" (Heb 4:12), and \"is able to build you up and to give you the inheritance among all those who are sanctified\" (Acts 20:32; cf. 1 Thes 2:13).(DV, nos. 21-22, in Vatican Council II: Volume 1)\nBesides being nourished by the Word of God, Catholics receive the spiritual nourishment and grace to carry out their mission in the world through the charisms—ministry gifts of the Holy Spirit (see LG, no. 12; AA, no. 3)—and through the sacraments, especially the Eucharist.The fourth constitution, the Constitution on the Sacred Liturgy (Sacrosanctum Concilium [SC]) speaks of the sacred liturgy as the \"source\" and \"summit\" of the Christian life: \"The liturgy is the summit toward which the activity of the Church is directed; it is also the fount from which all her power flows. For the goal of apostolic endeavor is that all who are made sons of God by faith and baptism should come together to praise God in the midst of his Church, to take part in the Sacrifice and to eat the Lord's Supper\" (SC, no. 10, in Vatican Council II: Volume 1).\nTo carry out her mission in the world, the Church needs the spiritual nourishment of her liturgy: \"the fount from which all her power flows.\"The goal of the Church's apostolic endeavors is the glory of God; our endeavors in the world are our spiritual sacrifice offered to the Father through Jesus, the Son, in the Holy Spirit. This is our Eucharist, our thanksgiving to God for his gifts and graces. In the liturgy, our daily efforts and sacrifices are united with the one perfect sacrifice of Jesus Christ (LG, no. 34). Hence, \"the liturgy is the summit toward which the activity of the Church is directed,\" since all we do is an offering for the glory of God.\nCopyright © 2013, United States Conference of Catholic Bishops, Washington, DC. All rights reserved. Permission is hereby granted to duplicate this work without adaptation for non-commercial use.\nExcerpts from Vatican Council II: The Conciliar and Post Conciliar Documents edited by Austin Flannery, OP, copyright © 1975, Costello Publishing Company, Inc., Northport, NY, are used with permission of the publisher, all rights reserved. No part of these excerpts may be reproduced, stored in a retrieval system, or transmitted in any form or by any means—electronic, mechanical, photocopying, recording, or otherwise—without express written permission of Costello Publishing Company.\nExcerpts from Pope John Paul II, Novo Millennio Ineunte, copyright © 2001, Libreria Editrice Vaticana (LEV); Pope Benedict XVI, Address, December 22, 2005, copyright © 2005, LEV. Used with permission. All rights reserved.\nScripture excerpts used in this work are taken from the New American Bible, rev. ed.© 2010, 1991, 1986, 1970 Confraternity of Christian Doctrine, Inc., Washington, DC. All rights reserved. No part of this work may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system, without permission in writing from the copyright owner.\nExcerpts from The Documents of Vatican II, Walter M. Abbott, SJ, General Editor, copyright © 1966 by America Press, Inc. Reprinted with permission. All rights reserved.\nBy accepting this message, you will be leaving the website of the\nUnited States Conference of Catholic Bishops. This link is provided\nsolely for the user's convenience. By providing this link, the United\nStates Conference of Catholic Bishops assumes no responsibility for,\nnor does it necessarily endorse, the website, its content, or"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:e1eb2ac5-1ad6-4020-8e78-9d9696640152>","<urn:uuid:a6d09ca3-178d-41e1-b7a1-9517c26e2a31>"],"error":null}
{"question":"What types of professionals attend global health courses versus neuro rehabilitation conferences?","answer":"Global health courses are primarily attended by students preparing for careers in healthcare, focusing on understanding population health, infectious diseases, and cultural factors including religion. In contrast, neuro rehabilitation conferences attract practicing healthcare providers including nurses, social workers, vocational specialists, counselors, administrators, physical therapists, occupational therapists, speech language pathologists, psychologists, case managers, claims specialists, community living specialists and direct care staff who work specifically in brain injury rehabilitation.","context":["Global Health (Shenandoah University)\nProfessor Audra L. Gollenberg\nCourse description: Global health is the study of the biological, social, and environmental factors that contribute to the creation and maintenance of health and disease in populations around the world. These factors include infectious disease, nutrition, economic factors, clean water, pollution, and globalization, among others. This course introduces students to the study of global health by exploring these topics in detail. This course is designed to give an overview of health problems from an international perspective. Students will learn to analyze risk factors for global health problems and explain the complex interactions between behavior, context, and disease.\nLearning outcomes for this course include:\n- Explain the health needs of special population groups (like women and children, prisoners, refugees, etc.)\n- Identify major organizations that are involved in global health and illustrate the importance of individual involvement in global health affairs\n- Demonstrate how religion intersects with public health practice\nCase study activity:\nStudents read “Religion and Ebola: Learning from Experience” by Katherine Marshall and Sally Smith (The Lancet, 2015) and examine recommendations from the World Health Organization reflecting on how religion and religious practices played an integral role in the 2014-2015 western African Ebola epidemic and its containment. Students also watch an assigned video on safe and dignified burials of those with Ebola in Sierra Leone. They then discuss ways to integrate religious leaders and religious practices into the control of infectious disease outbreaks, working through the following questions:\n- In what ways did religion play a role in the Ebola outbreak in west Africa (2014-2015)?\n- Imagine you are one of the global health outreach workers whose primary objective is to work with villages in west Africa to halt the spread of Ebola during the 2014-2015 epidemic. In what ways should you incorporate religion, religious leaders, and religious practices in your outreach methods? Explain in detail.\n- Why is it important to consider religion when working in diverse communities to improve health? Give examples in your answer.\n- Imagine you are asked to give advice to a young student who wishes to work in global health as a career. The student asks how he/she should prepare to consider religion in his/her future work. What is your response?\nPersonal and Community Health (Bridgewater College)\nProfessor Jill Lassiter\nCourse description: This course examines the multiple determinants of health and wellness from a personal and community perspective. Students will work toward obtaining knowledge and skills to critically analyze individual, social, and environmental factors that influence health, while focusing on their application to individual and community health improvement.\nLearning outcomes for this course include:\n- Demonstrate knowledge of the many factors that influence personal health choices (including religious and worldview considerations)\n- Critically analyze social, religious, and political factors that impact personal and community health\n- Develop strategies to initiate community change that are culturally sensitive and support healthy lifestyles\nSample course assignment: Perspectives paper\nOver the course of the semester students reflect on their own role within the complex healthcare system. The outcome of this exploration and writing assignment is a clearer vision of themselves as healthcare consumers, providers, and community members.\nStudents are required to conduct a series of interviews with a specific set of questions. They must interview a healthcare provider, someone from their service learning site, and someone who has had a significant experience as a patient. All interviews must be oral, so that the conversation involves interaction and further questioning beyond the listed set of questions. In addition to notes on each interview, students must write a final synthesis paper, including self-reflection from the beginning and end of the semester in which students answer the questions: What is my role as a healthcare consumer, responsible citizen, and (eventual) healthcare provider? And how does my identity (social, family, demographic, etc.) impact my views about health? These final papers must comment on intersectionality and determinants of individual and community health.\n- How do you feel about the current state of healthcare in the United States?\n- What do you think is the impact of the community on the health of your patients/your health?\n- How do your patients/you take personal responsibility for their/your health? What are some ways that they/you do not take personal responsibility for their/your health?\n- How does faith play a role in health?\n- How does provider personality, communication style, personal views, etc. impact patients?\nHealth, Healing, and Religion (Barton College)\nProfessor Jane Webster\nCourse description: In this course, Students develop their ability to interact with people of other religious and cultural traditions. They do this by reading about, participating in, and observing various healing methods, through a wide range of cultures. They identify religious features such as meaning-making, ritual, pilgrimage, and sacrifice, as well as ways that healers build confidence in those they heal. These religious features appear overtly in some religious contexts, and covertly in “non-religious” modern medical and psychological contexts. Students are asked to compare and contrast these features across cultures and to reflect upon their implications for health care in America today. They are also asked to reflect on their own response to interfaith understanding, its value, and its implications for their future career.\nLearning outcomes for this course include:\n- Identify, analyze, and compare worldviews/ideologies of different people groups, including your own\n- Identify, describe, interpret, and compare healing philosophies, values, assumptions, and rituals of several different cultures\n- Identify the potential challenges, features, and implications of interfaith understanding\n- Discuss the interrelationship between religion and healing and its implications\nSample class activity: BáFá Simulation Game\nThe following sequence of exercises is used to invite students to examine their own cultural and religious attitudes or assumptions, to understand the urgency and relevance of empathetic understanding, and to work effectively and collaboratively in the midst of cultural difference. These exercises prepare students to examine healing modalities from multiple cultures with empathy and curiosity.\nThe game requires two groups—the Alphas and the Betas—to communicate. Both groups have markedly different values, taboos, agenda, etc. (All materials are developed from “BaFá BaFá: A Cross-Cultural Simulation” by Jennifer Robertson, Valencia College).\nSession 1: Students are assigned to Alphas or Betas and receive a description of their role. They are assigned to read their role description and be ready to enter the next class as their Alpha or Beta persona.\nSession 2: Alphas and Betas are separated so they can discuss, clarify, and practice their cultural markers. One member of each group is invited to visit the other group to learn as much as they can; after a few minutes, they must return to their own group and report what they have learned. The two groups are then brought together to mingle. Students learn that due to an environmental crisis, the two groups must build a bridge together in order to survive, forcing them to engage the other group for the common social good.\nAt the end of this class, students are invited to share some observations, or what they felt during the exercise. They are assigned to write a response paper describing their experience of the game and to identify any “big ideas” they have learned.\nSession 3: Students complete a norm-setting exercise on active listening. The class then moves into a discussion of the “big ideas” named after the previous class, putting the skills of active listening to use. Over the course of discussion, students are asked to extract principles and techniques of intercultural communication or interfaith leadership. The class generates a list of these practices, including examples like doing research, asking questions, seeking the common good, being open to learn, sharing experiences, speaking for one’s self (rather than representing a group), refraining from judgment, and avoiding generalizations.\nStudents are assigned to write a description of a conversation (real, borrowed, or imagined) with someone of another religion and to connect the big ideas from the BáFá simulation exercise.\nSession 4: Students share their homework conversations with a classmate to exchange feedback. As a large group, students share what they have learned about effective interfaith dialogue from this exercise, from the BáFá simulation, their homework assignments, and class discussions thus far.\nFor homework, students write a response to the prompt: What is your (religious) worldview? With a very brief reference to your larger tradition (e.g. that of your family or ethnic group), describe what you personally believe about the world, including some of these questions: How did the world come into existence? How is the world sustained? Is the world basically good or bad? How can we know? What is the role of humans in the world? What is the purpose of community? How should we live? What happens when people are born and die? This paper should connect these questions to the “big ideas” from the BáFá simulation exercise.\nThroughout the remainder of the course, students return to this experience again and again to remind themselves why interfaith understanding is both urgent and relevant. At the midterm and the final, students write a summary of what they have been learning about various healing modalities, framed by this prompt: Given your experience in BáFá game and in the course more generally, why is interfaith understanding urgent and relevant in your chosen profession?\nIntroduction to Medical Ethics (Shenandoah University)\nProfessor Barry Penn Hollar\nCourse description: This course introduces students to five moral principles and their basis in the Western philosophical tradition. We consider how those principles are relevant to work in various health professions. Topics to be considered include: professional codes of ethics and ethics committees in various institutional settings; the nature of the professional-patient relationship; truth-telling and informed consent; confidentiality; end of life decisions; euthanasia; and justice as it relates to access to health care. An emphasis of the course is case analysis involving the identification of moral dilemmas and the reasoning that supports their resolution.\nLearning outcome for this course:\nStudents will be able to demonstrate awareness of how diverse religious perspectives affect moral judgements related to withholding, withdrawing, or refusing life-sustaining treatment and active euthanasia.\nLearning activity to achieve this outcome:\nStudents write a 250-500 word reflection on how they think or would expect religious perspectives to affect moral judgments and decisions about end-of-life care. This exercise invites students to assess what perceptions they already have about the ways in which religious beliefs and practices could impact decisions about end-of-life care. These reflections will also “prime” students’ minds for engaging the ideas presented in the lecture presentation and readings on these topics.\nFollowing an in-class lecture on moral judgments and end-of-life care, students read several articles related to religion, spirituality, and end-of-life care. Sample readings include:\n- “Placing Religion and Spirituality in End-of-Life Care” by Daleeman and VandeCreek (Journal of the American Medical Association, 2010)\n- “Religion, Spirituality, and Health Care: Social, Ethical, and Practical Considerations” by Astrow, et al (American Journal of Medicine, 2001)\n- “Families, Patients, and Physicians in Medical Decision Making: A Pakistani Perspective” by Moazam F. (Hastings Cent. Rep, 2000)\n- Cancer is Funny: Keeping Faith in Stage-Serious Chemo, Chp. 1 “I Thought I Had Cancer”, by Jason Miceli (Fortress Press, 2016)\nStudents use these readings to design a series of interview questions, which they then use to interview three persons with diverse perspectives (from at least two different religious traditions) about their moral judgments related to end-of-life care. Finally, students will prepare a report on those interviews analyzing the attitudes they encountered and comparing them with the readings, and what they had expected as reflected in their original essay.","On With Life's Annual Neuro Rehabilitation Conference\nFull refunds will be given through September 1. Partial refunds will be given through September 9.\nThe On With Life Neuro Rehabilitation Conference brings together experts from around the U.S. to discuss recent trends in the field of brain injury rehabilitation. Intended for healthcare providers including nurses, social workers, vocational specialists, counselors, administrators, physical therapists, occupational therapists, speech language pathologists, psychologists, case managers, claims specialists, community living specialists and direct care staff.\nPlease join us for two half days of online learning about recent trends in the field of neuro rehabilitation, coming from experts all over the United States. This year's conference will cover discussions on pelvic floor, return to living, Parkinson's Disease, and more. Intended for healthcare providers, the On With Life Conference brings together experts from around the United States to discuss recent trends in the field of neuro rehabilitation. Attendees include nurses, case managers, social workers, vocational specialists, claims specialists, counselors, administrators, physical therapists, occupational therapists, speech-language pathologists, psychologists, community living specialists and direct care staff.\n- Delineate key characteristics of TBI that behavioral health professionals should understand, describe why TBI is associated with behavioral health problems and list three ways that TBI affects successful behavioral health treatment\n- Define effective feedback and its value, demonstrate giving effective feedback using a simple model and receive feedback without excuses, blame, or defensiveness\n- Define the basic anatomy of the pelvic floor, list three reasons you might initiate a referral to a pelvic floor therapist and identify three ways to incorporate pelvic floor treatment into practice\n- Identify three ways mindset can impact progression towards independence and describe how can therapy can include salient activities to help overcome perceived barriers related to independence\n- Identify the five aspects of Living LARGE with PD, identify the interplay between motor and non-motor symptoms on the psychosocial impact of PD and understand the importance of social and community support with various resources discussed and identified\n- Describe the etiology of complex concussion, name two symptoms and three interventions of post-concussive syndrome and discuss how psychosocial factors can influence recovery\n- Define post-traumatic growth, recognize how pre-injury characteristics can impact rehabilitation and identify three ways survivors have re-established meaning and purpose in their lives\nTHURSDAY, SEPTEMBER 10\nReturn to Living: Getting On With Life\nBlack Women, Brain Injury and Domestic Violence/Intimate Partner Violence\nLiving LARGE and Informed with Parkinson’s\nGiving and Receiving Effective Feedback\n|4:30 pm||Closing Remarks|\nFRIDAY, SEPTEMBER 11\nWhat Healthcare Professionals Should Know About TBI\nOvercoming Adversity, Post-Traumatic Growth, and Finding Meaning After Brain Injury\nThe Neurogenic Pelvic Floor – Let’s Give You Something to Talk About\nReturn to Work after mTBI: Therapy’s Role After Acute or Delayed Recovery\n|4:30 pm||Closing Remarks|\n|Ashley Coop - Ashley is a licensed occupational therapist who has worked at On With Life for the past 2 years. She has a bachelors degree in rehabilitation science and a masters degree in occupational therapy from Concordia University Wisconsin. Ashley is a certified brain injury specialist with a passion to serve those who have sustained a brain injury return to independent living.|\n|John Corrigan, PhD, ABPP - Dr. Corrigan is a Professor in the Department of Physical Medicine and Rehabilitation at Ohio State University in Columbus Ohio, and Director of the Ohio Valley Center for Brain Injury Prevention and Rehabilitation. He is Editor-in-Chief of the Journal of Head Trauma Rehabilitation. Dr. Corrigan is the Director of the Ohio Brain Injury Program, which is the designated lead agency in the state of Ohio for TBI policy and planning. He founded and for 25 years directed the TBI Network at Ohio State––a specialized substance use disorder treatment program for persons with brain injury. Dr. Corrigan is a member of the Board of Directors of the Brain Injury Association of America and has previously served national organizations, including CARF, the Injury Control Center at CDC, the Veterans Administration and the U.S. Department of Defense, Defense Health Board. He has received many awards for his service and research in brain injury rehabilitation, including the Brain Injury Association of America’s William Fields Caveness Award, the 2007 Robert L. Moody Prize and the Gold Key Award from the American Congress of Rehabilitation Medicine.|\n|Libby Roberts, M.S. Ed - Libby came to ATW Training Solutions in 2016 after spending almost nine years in the banking industry, where she started in front-line operations and moved to management and training. Libby brings experience in supervision and management, customer service, communication skills, workplace etiquette, and sales to her role at ATW. Libby holds a bachelor’s degree from Simpson College and a master’s degree in Adult Learning and Organizational Performance from Drake University. In addition to her role at ATW, Libby is a member of the Central Iowa Chapter of the Association for Talent Development and serves on the membership committee.|\n|Megan Ihrke, MA, CCC-SLP, CBIST - Megan is a speech-language pathologist who has been practicing in the area of neurological rehabilitation with adolescent and adult populations for the last 10 years, all of which have been served at On With Life. She completed her bachelors and masters in speech-language pathology at the University of Iowa. Megan serves as a Clinical Team Lead at On With Life in Ankeny. She is a Certified Brain Injury Specialist Trainer and also maintains MANDT certification. Megan provides assessment and treatment in the areas of neurogenic swallowing, speech production, expressive and receptive language, and cognitive-linguistic skills in a transdisciplinary approach providing individual, co-treatment, and group therapy for individuals who are survivors of acquired brain injury.|\n|Jillian Jones, DPT, CBIS - Jillian is an Iowa-licensed physical therapist who received her doctorate degree in physical therapy from St Ambrose University. She is passionate about working in the field of neurological rehabilitation, particularly in the field of brain injury. At On With Life she has had the opportunity to work in a multitude of settings which include inpatient, outpatient and long-term care, exposing her to brain injury rehabilitation through the continuum. She is a certified brain injury specialist through the Brain Injury Association of America and is a member of the Outpatient Complex Concussion Committee at On With Life. She has participated in research in collaboration with Drake University, specifically in the area of post-concussion syndrome. Jillian also serves on the Concussion Specialist Certification Committee through the BIAA.|\n|Lindsay Maltas, DPT, C/NDT, CBIS, ACCI - Lindsay is a physical therapist and has worked at On With Life for the past 9 years. Lindsay received her undergraduate degree in Kinesiology & Health with minors in Spanish and Psychology from Iowa State University (go Cyclones!). She received her doctorate degree in physical therapy from the University of Iowa. Lindsay is certified in Neurodevelopmental Treatment (NDT), is a credentialed clinical instructor, and has received training in pelvic floor assessment and treatment through Herman & Wallace.|\n|Gail McGaughy, MPT, C/NDT, CLT CBIS - Gail has been a Physical Therapist for 22 years with emphasis and specialty in neurological disorders. Gail worked in Inpatient Rehab for 15 years and Outpatient Neuro Rehab at On With Life since 2012. As a certified LSVT BIG provider, member of the Parkinson’s Committee at On With Life and board member for the Iowa Chapter of APDA, Gail has a keen interest in the treatment and wellbeing of Parkinson’s patients through the lifespan from early diagnosis to end stage Parkinson’s. Gail and staff have committed their time, energy, and efforts to be the provider of choice for persons with PD and their care-partners. Gail is also certified in Neuro-Developmental Treatment (NDT), Lymphedema Treatment (CLT) and Certified Brain Injury Specialist (CBIS). Gail has also provided various lectures and CEU opportunities in NDT for Adult Hemiplegia in IA, IL and WI.|\n|Laura McPike, PTA, CBIS - Laura is a licensed Physical Therapist Assistant who has worked in Neuro Rehab for 8 years and at On With Life for over 2 years. She has a bachelors degree in communications from Bowling Green State University in Ohio. She is a certified brain injury specialist with a passion for women’s health. Laura has received training in Pelvic Floor health and Neurological conditions affecting the Bowel and Bladder from Herman and Wallace.|\n|Monique Pappadis, MEd, PhD - Since 2004, Dr. Pappadis has been conducting patient-centered outcomes research in the area of stroke and traumatic brain injury research. Her research includes the role of neurocognition on outcomes after brain injury, minority aging, health disparities, and addressing gender, language, and cultural differences related to outcomes after injury. She has won several research awards, made several national and international presentations, published more than 30 peer-reviewed publications, and disseminated several educational materials for persons with brain injury and their caregivers.|\n|Magen Rainey, CTRS, CBIS - Magen is a certified therapeutic recreation specialist who has worked at On With Life for about 4 years. She has a bachelor’s degree from Northern Iowa in Leisure Youth and Human Services, specializing in recreation therapy. Magen has worked in a wide variety of settings, but her passion for neuro continues to challenge her on a personal and professional level.|\n|Emily Summerfield, DPT, CBIS - Emily is an occupational therapist who has worked at OWL for 3 years and has practiced in neurorehabilitation for 4 years. Emily received her graduate/undergraduate degrees at College of Saint Mary in Omaha, NE. She is a certified brain injury specialist and certified LSVT clinician. Emily is a team member of the complex concussion committee at OWL.|\n|Lindsay Vaux Eldredge, MS, CBIST - Lindsay received her Bachelor’s degree in psychology with a minor in biology from the University of Iowa. She completed her Master’s degree in clinical psychology from the University of Utah with an emphasis in neuropsychology. Lindsay has been serving brain injury survivors for 11 years. She has been working at On With Life for over 7 years, where she provides psychological services to inpatient persons served and their families.|\n|Sam Williams, OTR/L, CBIS - Samantha is an occupational therapist and has worked in the area of neurological rehabilitation for the past 8 years and has been at On With life for the past 3 years. Samantha received her undergraduate degree in psychology and a master’s degree in occupational therapy from St. Ambrose University. She is a certified brain injury specialist and has received training in pelvic floor assessment and treatment through Herman & Wallace. Samantha is passionate about keeping up with the world of neuro rehab.|\n|Natasha Winterbottom - Natasha currently runs the Iowa Chapter of the American Parkinson Disease Association. Natasha has worked previously one on one to set up in home cares for PD patients as well as running two separate support groups for patients and caregivers in Iowa. She has a passion for finding new ways to support our local PD community and sharing the APDA mission that every day, we provide the support, education, and research that will help everyone impacted by Parkinson’s disease live life to the fullest.|\nRelevant to the content of this educational activity, the following individuals have no financial conflicts with commercial interest companies to disclose.\n- Abby Bogaards, MHA - activity director and planning committee member\n- Ashley Coop - speaker\n- John Corrigan, PhD, ABPP - speaker\n- Madison Denkinger - activity coordinator and planning committee member\n- Megan Ihrke, MA, CCC-SLP, CBIST - speaker\n- Jillian Jones, DPT, CBIS - speaker\n- Lindsay Maltas, DPT, C/NDT, CBIS, ACCI - speaker\n- Gail McGaughy, MPT, C/NDT, CLT CBIS - speaker\n- Laura McPike, PTA, CBIS- speaker\n- Monique Pappadis, MEd, PhD - speaker\n- Magen Rainey, CTRS, CBIS - speaker\n- Kim Reed, CBIS, CTRS - planning committee member\n- Sonni Ricklefs, BSN, RN, CRRN - planning committee member\n- Libby Roberts, M.S. Ed - speaker\n- Emily Summerfield, DPT, CBIS - speaker\n- Lindsay Vaux Eldredge, MS, CBIST- speaker\n- Nicole Weidman, PTA, MHA - planning committee member\n- Sam Williams, OTR/L, CBIS - speaker\n- Natasha Winterbottom - speaker\nContinuing Education Credit\n- MD: This activity has been planned and implemented in accordance with the accreditation requirements and policies of the Iowa Medical Society (IMS). Des Moines University (DMU) is accredited by the IMS to provide continuing medical education for physicians. DMU designates this live activity for a maximum of 8.0 AMA PRA Category 1 Credit(s)™. Physicians should only claim credit commensurate with the extent of their participation in the activity.\n- DO: Des Moines University (DMU) is accredited by the American Osteopathic Association (AOA) to provide osteopathic continuing medical education for physicians. DMU designates this program for a maximum of 8.0 AOA Category 2-A credits and will report CME and specialty credits commensurate with the extent of the physician’s participation in this activity.\n- Nurse: Des Moines University is Iowa Board of Nursing approved provider #112. This live activity has been reviewed and approved for 8.0 continuing education contact hour(s). Nurses must attend the entire session within each day to receive credit. Partial session credit is prohibited and will be forfeited.\n- Other Health Professionals: This live activity is designated for a maximum of 8.0 AMA PRA Category 1 Credit(s)™.\nNo commercial interest company provided financial support for this continuing education activity.\nEveryone in a position to control the content of this educational activity will disclose to the CME provider and to attendees all relevant financial relationships with any commercial interest. They will also disclose if any pharmaceuticals or medical procedures and devices discussed are investigational or unapproved for use by the U.S. Food and Drug Administration (FDA). Determination of educational content and the selection of speakers is the responsibility of the activity director. Firms providing financial support did not have input in these areas. The information provided at this CME activity is for continuing education purposes only and is not meant to substitute for the independent medical judgment of a healthcare provider relative to diagnostic and treatment options of a specific patient’s medical condition. The content of each presentation does not necessarily reflect the views of Des Moines University.\n- 8.00 AMA PRA Category 1 Credits™\n- 8.00 AOA Category 2A\n- 8.00 CE Contact Hour(s)\n- 8.00 IBON\nThis conference will be held using Zoom. We will host one continuous webinar each day that will include all four presentations and time for breaks. Registered individuals will be emailed the login information prior to the event.\n- Attend both days: $50\n- Attend one day: $30\nFull refunds will be given through September 1. Partial refunds will be given through September 9."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:690b434d-c174-4eb8-8897-6a5c22337984>","<urn:uuid:2be6278c-c263-47dd-9624-e67d67eba335>"],"error":null}
{"question":"As a cybersecurity professional, I'm wondering how MITRE ATT&CK and NIST frameworks differ in their approach to providing security guidance?","answer":"MITRE ATT&CK and NIST take distinctly different approaches to security guidance. MITRE ATT&CK is grounded in practical, actionable advice based on real-world threat details, providing specific recommendations for mitigation and remediation tactics. It details exact techniques attackers use, such as specific ways they might escalate privileges or bypass user access control. In contrast, NIST offers a more comprehensive, multidimensional approach to cybersecurity, focusing on broader guidelines for risk assessment, cybersecurity techniques, and secure system development. NIST's framework is particularly notable for its flexibility, allowing organizations to adapt the guidance to their individual needs across various sectors and company sizes.","context":["July 29, 2019\nHardening defenses with MITRE ATT&CK and osquery: Lessons from Singapore Health breach\nThere's a big disconnect between best practice frameworks and the real-life nitty gritty. Many of these frameworks broadly approach the overarching principles that a robust security program should encompass and why these principles are important; however, they don't usually say specifically what kind of attacker behavior a defender should anticipate when building their security programs, nor do they detail how an attacker would work to thwart those vaulted best practices. Often, that's left up to the security practitioner to suss out themselves in their copious spare time.\nBridging the gap between theory and practice can be challenging even for the most seasoned security veterans, which is why we recommend enterprises take a serious look at the MITRE ATT&CK framework for guidance.\nWhat's MITRE ATT&CK and why is it a useful framework?\nMITRE, being well-known in security circles for their vulnerability identification and threat research, created the ATT&CK framework based on all the real-world threat detail they encounter and study, which are all threats that enterprises frequently face. As such, their ATT&CK framework is grounded in practical, actionable advice and gives specific recommendations for mitigation and remediation tactics. It spans operating systems and device types, with 12 columns naming specific attack tactics frequently used against enterprises. Each of those columns then contains numerous techniques that attackers often use to carry out their tactic, and exactly how the attacker would carry out that technique.\nFor example, under the Privilege Escalation column, ATT&CK lists 28 possible techniques an attacker may try, such as bypassing user access control (UAC). ATT&CK then details exactly how bypassing UAC tends to occur, what operating systems tend to be vulnerable — in this case, Windows — and what tools are frequently deployed by attackers to escalate privileges. The framework then details mitigation recommendations, from making policy-level changes to monitoring specific registry keys that attackers tend to modify.\nWhile many other security frameworks are quite high-level and strategic, and in some cases proprietary, MITRE ATT&CK merges well the tactical and practical to help organizations build a robust roadmap to better organizational security. It's also completely free to use.\nWhy osquery is a great fit for the ATT&CK framework\nOsquery is an open-source, cross-platform tool that allows you to query and monitor your endpoints as if they were a relational database. It's fast, flexible, and highly extensible, allowing you to glean a wealth of information from endpoints in comparison to older monitoring tools with slower agents. The comprehensive real-time results that osquery generates make it a natural fit in a defender's arsenal, enabling easy continuous monitoring of the endpoints on the network for events, what the endpoints are connected to, what processes they are (or aren't) running — just to name a few. Since osquery essentially turns your endpoint operating systems into a fast virtual database, the logs that osquery generate also slot well into your existing security ecosystem for analysis and action.\nTo demonstrate how organizations can leverage osquery's powerful monitoring capabilities and pair it with the MITRE ATT&CK framework's recommendations, it’s helpful to use a real-world example: The 2018 Singapore Health breach. In this incident, nearly 1.5 million patients had their personal health information (PHI) stolen. Singapore Health’s breach is a learning opportunity for security professionals thanks to its comprehensive post-breach report, which painstakingly outlines the specific attack vectors used and the multiple failure points that lead to the breach. (Download a PDF copy of the breach report here.)\nLet’s take a look at one example point in the Singapore Health breach, and how an organization using osquery within the ATT&CK framework might prevent similar missteps.\nExamples from the Singapore Health breach: What happens after initial infiltration\nWhile there's some disagreement about how it got there in the first place, the Singapore Health breach investigators determined that attackers first got a foothold in the network by placing malware on at least one workstation, and the malware in question seemed to exploit a vulnerability in Microsoft Outlook. Once exploited, the attacker was able to download a payload onto the infected machine, which included malicious PowerShell scripts masquerading as innocuous .jpg files. From there, the attacker gained a foothold on the compromised machine, from which they were able to download additional payloads and quietly lurk until they were able to move laterally to another machine and eventually infiltrate servers containing PHI nearly half a year later. Being able to stop that initial foothold via malicious PowerShell scripts could have seriously disrupted, if not stopped, this attack.\n- ATT&CK framework mapping: PowerShell (T1086) is one of the many specific techniques outlined in ATT&CK's Execution tactic.\n- How osquery could be used here: Osquery can be set up to detect and flag obvious malicious Powershell events on monitored endpoints, but it can also look at the overall number of lines, or total size, of PowerShell scripts running on endpoints in the first place. Malicious payloads are often larger than the typical PowerShell script, and often contain encoded shellcode. By detecting large scripts and investigating their content, we can detect potential attempts to run malicious codes. So if normally innocuous PowerShell scripts are pretty concise, and suddenly they become verbose and balloon in line size, even if what they're doing is not obviously malicious to your average anti-virus, it's clear that something is up that's worth flagging. Osquery can keep an eye on the size of PowerShell code it normally sees, and send up a flag if that number starts to suddenly deviate from the norm.\nFree Webinar: Get more information on using osquery with MITRE ATT&CK\nThe Singapore Health post-incident report outlines six key points in the breach, where more robust defensive measures could and should have stopped the attacker's progress. This post is just a taste of what osquery could do, and how it could help organizations harden their defenses and monitor their endpoints in real-time.\nLearn more about osquery:\nMaria Varmazis is an information security marketing consultant based in the Boston area. Prior to her consultancy career, she managed content and social media marketing for major security companies, including Rapid7 and Sophos. She is passionate about security and privacy advocacy, and her mission is to share security...\nOther posts you might be interested in\n8 min read | January 8, 2020\nHow To Use MITRE ATT&CK For Endpoint SecurityRead More\n6 min read | November 11, 2020\nFast, consolidated, and context-rich detections from Uptycs will keep security analysts saneRead More\n6 min read | February 25, 2021","In the present digital era, where the risk of data breaches and cyber threats emerges significantly, the key role of IT compliance frameworks in strengthening organizational security and keeping operational integrity cannot be underestimated.\nThese frameworks serve as process standards, guiding businesses through complex regulations and best practices to ensure a robust foundation for data protection and process efficiency.\nBusinesses nowadays heavily depend on technology to develop new ideas and move forward. This has made following the defined business rules more complicated. Therefore, choosing the right rules is vital to get the most out of this situation. Organizations that do more than check off tasks must be smart about their choices.\nWhat is IT Compliance?\nThe IT compliance framework is a set of guidelines and procedures organizations use to ensure their IT systems and processes comply with regulatory requirements and industry standards. Such frameworks must be implemented for technology to be managed systematically while minimizing risks and guaranteeing data security. The IT compliance framework allows organizations to avoid legal disputes, reputational damage, and fines by following technology practices that align with the legislation’s requirements.\nHow IT Compliance Frameworks Protect Data and Enhance Trust\nThe IT compliance framework covers many laws, including the Data Protection Act, industry-specific standards, and cybersecurity-related legislation. To demonstrate compliance with the rules and regulations, it lays down a plan for implementing security measures, periodic audit work, and maintenance of documentation.\nThe IT compliance frameworks also help to foster an organizational culture of accountability because they necessitate cooperation between IT, law, and business units. These frameworks are important to protect confidential data and build trust between customers and enterprises operating under a complex and constantly evolving digital landscape.\nIt is paramount for enterprises to choose a proper compliance framework, which can give them numerous benefits that are more than just about complying with the law. Some benefits include:\n- improved security measures;\n- disciplined operational procedures;\n- enhanced stakeholder confidence;\nBy aligning with a suitable framework for compliance, companies can strengthen their security measures and data protection protocols. This can prevent the possible leakage of sensitive information and protect against threats to cybersecurity, thereby curtailing financial losses and reputational damages. In addition, implementing a suitable framework can help simplify and enhance effectiveness in managerial operations while reducing operational complexity.\nIt is equally essential to develop compliance frameworks that consider specific business needs. The reason is that the specificities of the sector or unique operating requirements may not be addressed in general frameworks. To ensure that businesses comply with their obligations and consider their specific problems, defined frameworks can result in more efficient risk management and strategic alignment.\nOverview of Key IT Compliance Frameworks\n- National Institute of Standards and Technology (NIST) Framework\nNIST offers a comprehensive approach to cybersecurity through its multidimensional guidelines. The guidelines pertain to:\n- assessment of risks;\n- use of cybersecurity techniques;\n- development of secure systems;\nFlexibility, which allows organizations to tailor their guidance to individual needs, is a strong feature of NIST. This adaptability ensures that deploying cybersecurity measures can be effective in various sectors and sizes of companies, considering their particular needs.\n- ISO 27001\nISO 27001 presents an information security management framework for protecting sensitive data. The flexible architecture of the framework permits businesses to deploy information security measures tailored to their risk evaluations and business objectives, thereby ensuring the proficient safeguarding of critical information assets.\n- General Data Protection Regulations (GDPR)\nGDPR focuses on protecting the data and privacy rights of European Union citizens. It emphasizes transparent data collection, user consent, and robust security measures. GDPR compliance is vital for any business handling personal data, as it ensures legal adherence and promotes customer trust. Non-compliance can result in heavy fines and reputational damage, highlighting the significance of aligning data practices with GDPR.\nFactors Affecting the Selection of an IT Compliance Framework\nSeveral factors influence the selection of an IT compliance framework for any organization. Some of the factors are as below:\n- Domain-Specific Regulations\nThe distinct regulations of an industry heavily influence the choice of a compliance framework. Industries such as healthcare, finance, and telecommunications have specific compliance requirements due to the data sensitivity they handle. For instance, healthcare organizations must adhere to Health Insurance Portability and Accountability Act (HIPAA) regulations. At the same time, financial institutions follow Sarbanes-Oxley Act (SOX) and Payment Card Industry Data Security Standard (PCI DSS) regulations. These unique demands dictate the selection of a framework that directly addresses the industry’s specific challenges.\n- Harmonized Compliance Goals and Business Objectives\nIt is vital to align the compliance goals with the objectives of the business. The chosen framework should guarantee compliance with regulation and promotion of strategic achievement. Adequate risk tolerance is also essential, as established sectors demand a more stringent framework to prevent catastrophic breaches; while new industry entrants may seek flexibility in this area.\n- Business Scale and Available Resources\nAn appropriate framework should be able to adapt to the scale of operation so that it does not have to undergo costly repairs down the line. The same importance is associated to the allocation of resources. Financial, human, and technological resources are required to implement and maintain compliance frameworks. Therefore, businesses must assess whether they have sufficient resources for efficient framework deployment and, if so, what adjustments must be made.\n5-Step Framework Selection Process\nBelow, a 5-step IT compliance framework selection process is described:\n- Step 1: Self-Assessment of Current Processes\nAn understanding of the current practices and needs is required to implement an IT compliance framework initiative. This means existing compliance measures must be assessed and gaps identified to correct them. Organizations can assess whether they are in a good position when meeting their obligations and how much improvement is needed.\n- Step 2: Benchmarking against Existing Frameworks\nThe second step is to identify and compare different frameworks aligned with identified needs. A thorough analysis helps choose the appropriate framework based on its unique features. A framework should be flexible enough to accommodate specific requirements, and it should be able to be customized. It is also important to consider regular updates and the ability to adapt to evolving regulations.\n- Step 3: Expert Opinion\nIt is important to rely on the expertise of compliance professionals. Consultation with experts helps gain access to specialized knowledge and insights that help informed decision-making. Experts can adjust their recommendations to optimize the framework selection process by considering an organization’s industrial structure, size, and unique challenges.\n- Step 4: Stakeholder Feedback\nIt is essential to consider the feedback of all the involved stakeholders in the framework selection process. Collaborating with the stakeholders ensures consensus and a shared understanding, promoting commitment to the chosen compliance framework.\n- Step 5: Implementation and Continuous Improvement\nThe final step of this selection process is the implementation. For that, it is essential to develop a plan, arrange the training of the employees, and then execute the plan as per a documented procedure. To determine the effectiveness of the chosen framework, the organizations must perform assessments and improve the processes regularly.\nChoosing an appropriate framework for IT compliance is essential in the complex world of digital operations. It supports data security and ensures coherence with strategic objectives about cyber threats.\nA systemic approach relying on self-assessment, benchmarking, expert opinion, and stakeholder feedback is required to select a suitable IT compliance framework.\nThis commitment to strong compliance and the resilience needed in a dynamic digital environment is reinforced by implementing it with a focus on continuous improvement."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:57237e93-13cf-46d1-94e5-ec868de664b7>","<urn:uuid:6646cbc3-78f9-4c46-a132-be353930d00e>"],"error":null}
{"question":"What are the differences between imaging-based inspection systems and phase-measurement based 3D systems?","answer":"Imaging-based inspection systems typically use cameras, lenses, and specialized lighting to acquire images, followed by digital image processing to extract information and make pass/fail decisions. They can use various imaging types including 2D visible light, multispectral, hyperspectral, and infrared imaging. In contrast, phase-measurement based 3D systems use fringe projection methods where the object is illuminated with a grating structure from multiple directions using a telecentric system. The phase-measurement system captures coordinates with equal accuracies in all dimensions (x,y,z) and uses rotation tables to view the object from different angles, eliminating problems with specular reflection or shadows.","context":["The basis of the described 3D-measurement system is the method of fringe projection in combination with the principle of uniform scale representation /1 1, /2/, /3/. The measurement is characterized by the exclusive use of phase-measurement values for the coordinates of each point. To obtain the phase-measurement values the object under test is successively illuminated with a grating structure from at least three different directions with a telecentric system. A CCD-camera records the intensity distribution of the fringes intersected by the object. It should be pointed out that the values of all coordinates (x,y,z) have the same accuracies. The object to be measured and the CCD-camera are both mounted on a rotation table, turning both of them with respect to the fringe-projector about an axis. The rotation axis makes a constant angle c with the projection direction. With at least i = 3 different angle values e the linearly independent absolute phase values are obtained, which are necessary for the coordinate calculation (applying gray-code in conjunction with four 90 degs phase-shifts). In our setup we can choose up to I = 1 5 rotation angles, so that awkward areas of the surface, like zones with either specular reflection or shadows, are shifted over the surface and have nearly no influence on the results of measurements. So, we obtain the 3D-coordinates of a single patch of the object. We have expanded the system to include a second rotation axis, where the object can rotate within the measurement volume. The CCD-camera will then get different views of the object. The second rotation axis is tilted against the first axis by an angle S. Depending on the class of objects there are different possible tilt-angles S. Typically we choose ö = 300. By rotating the object around the first axis with a rotation angle -y (j = number of patches) it is possible to measure the object from different viewpoints, whereby for each patch or viewpoint the procedure described above is used. For a convenient handling of the data-set the restriction j 8 is used. The actual rotation angles e and y were measured with angle encoders. The free geometric parameters (grating periode A and projection angle a) and the orientation in space of the second axis are gauged before the measurement procedure as described below. By measuring a number of patches we have the problem of transforming them from a local coordinate system into a global one. The combination of the different patches into one coordinate system without interactive user help was solved by developing a calibration method for the second axis. In combination with an absolute phase measurement all patches were measured in a known space orientation, so that we obtain the 3D-picture by rotation of each patch around the second axis. Correlation methods or special points are not necessary. To obtain the 3Dorientation of the second axis one can use a calibration procedure with a special calibration body whose 3Dcoordinates are measured for different rotation positions whereby the second axis has to cross the plane near the centre.","Machine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision describes many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be looked at distinct from computer vision, a type of computer science. It tries to integrate existing technologies in new ways and apply them to solve real life problems. The phrase is the prevalent one for these functions in industrial automation environments but can also be utilized for these functions in other environments such as security and vehicle guidance.\nThe entire Top Machine Vision Inspection System Manufacturer includes planning the specifics from the requirements and project, and then developing a solution. During run-time, this process begins with imaging, followed by automated research into the image and extraction of the required information.\nDefinitions of the term “Machine vision” vary, but all are the technology and techniques used to extract information from a picture on an automated basis, as opposed to image processing, where output is yet another image. The information extracted can become a simple good-part/bad-part signal, or maybe more an intricate set of data including the identity, position and orientation of each and every object within an image. The details can be utilized for such applications as automatic inspection and robot and process guidance in industry, for security monitoring and vehicle guidance. This field encompasses a huge number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision is practically the only term used for these functions in industrial automation applications; the term is less universal for these functions in other environments including security and vehicle guidance. Machine vision being a systems engineering discipline can be looked at distinct from computer vision, a type of basic computer science; machine vision efforts to integrate existing technologies in new ways and apply these to solve real-world problems in a manner in which meets the prerequisites of industrial automation and other application areas. The phrase can also be used in a broader sense by trade events and trade groups such as the Automated Imaging Association and the European Machine Vision Association. This broader definition also encompasses products and applications most often connected with image processing. The main ways to use machine vision are automatic inspection and industrial robot/process guidance. See glossary of machine vision.\nImaging based automatic inspection and sorting\nThe main ways to use machine vision are imaging-based automatic inspection and sorting and robot guidance.;:6-10 within this section the former is abbreviated as “automatic inspection”. The general process includes planning the details from the requirements and project, then making a solution. This section describes the technical method that occurs during the operation in the solution.\nMethods and sequence of operation\nStep one inside the automatic inspection sequence of operation is acquisition of the image, typically using cameras, lenses, and lighting which has been created to provide the differentiation essental to subsequent processing. MV software programs and programs created in them then employ various digital image processing methods to extract the necessary information, and frequently make decisions (like pass/fail) based on the extracted information.\nThe constituents of your automatic inspection system usually include lighting, a camera or any other imager, a processor, software, and output devices.3\nThe imaging device (e.g. camera) can either be separate from the primary image processing unit or along with it in which case a combination is usually known as a smart camera or smart sensor When separated, the bond may be made to specialized intermediate hardware, a custom processing appliance, or a frame grabber in a computer using either an analog or standardized digital interface (Camera Link, CoaXPress) MV implementations also have digital camera models competent at direct connections (without having a framegrabber) to some computer via FireWire, USB or Gigabit Ethernet interfaces.\nWhile conventional (2D visible light) imaging is most frequently used in MV, alternatives include multispectral imaging, hyperspectral imaging, imaging various infrared bands,line scan imaging, 3D imaging of surfaces and X-ray imaging. Key differentiations within MV 2D visible light imaging are monochromatic vs. color, frame rate, resolution, and whether or not the imaging process is simultaneous within the entire image, making it suitable for moving processes.\nThough the vast majority of machine vision applications are solved using two-dimensional imaging, Automated Vision Inspection Machines utilizing 3D imaging are a growing niche within the industry. By far the most frequently used technique for 3D imaging is scanning based triangulation which utilizes motion from the product or image throughout the imaging process. A laser is projected on the surfaces nefqnm an object and viewed coming from a different angle. In machine vision this can be accomplished using a scanning motion, either by moving the workpiece, or by moving your camera & laser imaging system. The line is viewed by way of a camera from the different angle; the deviation from the line represents shape variations. Lines from multiple scans are assembled right into a depth map or point cloud. Stereoscopic vision is utilized in special cases involving unique features found in both views of a set of cameras. Other 3D methods used for machine vision are time of flight and grid based.One method is grid array based systems using pseudorandom structured light system as used by the Microsoft Kinect system circa 2012."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ca26c70d-5d2c-439a-aecc-9c6f4fcfc59c>","<urn:uuid:8a7da364-e6d2-4468-9fca-80826a23460d>"],"error":null}
{"question":"What is the relationship between continuous improvement approaches in production management versus project quality management, specifically regarding their implementation and goals?","answer":"In production management, continuous improvement focuses on optimizing the 6M's (men, money, machines, materials, methods, and markets) to enhance production efficiency and satisfy customer wants. In project quality management, continuous improvement is based on the plan-do-check-act cycle and incorporates initiatives like TQM and Six Sigma to improve both project execution and product quality. Both approaches emphasize customer satisfaction and efficiency, but production management applies these principles to ongoing manufacturing processes, while project quality management applies them to specific project deliverables and outcomes.","context":["Production Management; means planning, organizing, directing and controlling of production activities. Also, in other words, P.M. involves an application of planning, organizing, directing and controlling the production process. P.M. deals with converting raw materials into finished goods or products. It brings together the 6M’s i.e. men, money, machines, materials, methods, and markets to satisfy the wants of the people. Also learn, the Financial Management, What is Definition of Production Management?\nLearn, Explain, Definition of Production Management.\nProduction and operation management is the science-combination of techniques and systems. That guarantees the production of goods and services of the right quality, in the right quantities and at the right time with the minimum cost within the shortest possible time. The essential features of a production and operation function are to bring together people, machines, and materials to provide goods and services for satisfying customer needs.\nProduction management also deals with decision-making regarding the quality, quantity, cost, etc., of production. It applies management principles to production. Production management’s a part of business management. It is also called “Production Function.” Production management is slowly being replaced by operations management. The main objective of production management is to produce goods and services of the right quality, right quantity, at the right time and at minimum cost. It is also trying to improve efficiency. An efficient organization can face competition effectively. P.M. ensures full or optimum utilization of available production capacity.\nMeaning of Production Management:\nProduction is the creation of goods and services. It is concerned with transforming the inputs in the form of raw materials, labor, machines, men and money into output i.e. goods and services with the help of certain production processes.\nThe production function is the most important function in an organization around. Which other activities of an enterprise (viz., marketing, financing, purchasing, and personnel, etc.) revolve. It is pertinent to note that production function should manage in an efficient and effective manner for the achievement of the organizational goals.\nIn a departmental type of organization, production management is concerned with carrying out the production function. Production management becomes the process of effectively planning and regulating the operations of that part of an enterprise. Which is responsible for the actual transformation of raw materials into finished products. Also, Production managing department more helps in the Business.\nSimply stated, production management is concerned with decision making relating to processes for producing goods and services in accordance with the predetermined specifications and standards by incurring minimum costs.\nThe result of at least three developments:\n- First is the development of the factory system of production. Until the creation of the concept of manufacturing, there was no such thing as management, as we know. It is true that people operated one type or another business, but for the most part, these people were business owners and did not consider themselves as managers.\n- Essentially stems from the first, namely, the development of the large corporation with many owners and the necessity to hire people to operate the business.\n- Stems from the work of many of the pioneers of scientific management. Who was able to demonstrate the value, from a performance and profit point of view, of some of the techniques they were developing.\nDefinition of Production Management:\nIt is observed that one cannot demarcate the beginning and end points of Production Management in an establishment. The reason is that it is interrelated with many other functional areas of business, viz., marketing, finance, industrial relation policies, etc.\nAlternately, Production and Operation Management is not independent of marketing, financial, and personnel management due to which it is difficult to formulate some single appropriate definition of Production and Operation Management.\nThe following definitions, also explain main characteristics:\nBy the words of Mr, E.L. Brech:\n“Production Management is the process of effective planning and regulating the operations of that section of an enterprise. Which is responsible for the actual transformation of materials into finished products.”\nThis definition limits the scope of production management to those activities of an enterprise which are associated with the transformation process of inputs into outputs. & the definition does not include the human factors involved in a production process. It lays stress on materialistic features only.\nProduction Management deals with decision-making related to the production process. So, the resulting goods and services are producing in accordance with the quantitative specifications and demand schedule with minimum cost.\nMain functions of production management:\nAccording to this definition design and control of the production system are two main functions of production management.\nProduction Management is a set of general principles for production economies, facility design, job design, schedule design, quality control, inventory control, work study and cost, and budgetary control. This definition explains the main areas of an enterprise where the principles of production management can apply. This definition clearly points out that production management is not a set of techniques.\nIt is evident from the above definitions that production planning and its control are the main characteristics of production management. In the case of poor planning and control of production activities, the organization may not be able to attain. Its objectives and may result in loss of customer’s confidence and retardation in the progress of the establishment.\nThe main activities of production management can list as:\n- Specification and procurement of input resources namely management, material, and land, labor, equipment, and capital.\n- Product design and development to determine the production process for transforming. The input factors into the output of goods and services.\n- Supervision and control of the transformation process for the efficient production of goods and services.","IT definitions, discussions and more\nTag Archives: project managment\nProject quality management includes the processes and activities that determine quality policies, objectives, and responsibilities so that the project will satisfy the needs for which it was undertaken.\nProject Quality Management Processes include the following:\n- Plan Quality: Identifying quality requirements or standards for the project and the product\n- Perform quality assurance: Auditing the quality requirements and the results from quality control measurements\n- Perform quality control: Monitoring and recording results of executing quality activities to assess performance and recommend changes\nThese processes interact with processes in other knowledge areas and each process can involve efforts from one or more person or groups based on project requirements. Each process occurs at least once in every project and occurs in one or more of the project phases.\nProject Quality Management addresses the management of the project and its product. Product quality measurements are specific to the product and failure to meet product or project quality requirements can have serious negative consequences for the stakeholders.\nQuality vs. Grade\nQuality: is the degree to which a set of inherent characteristics fulfill requirements.\nGrade: is a category assigned to products having the same functional use but different technical characteristics.\nA product can be of high quality (no obvious defects) and low grade (a limited number of features), or of low quality (many defects) and high grade (numerous features). The project manager and the project management team are responsible of the tradeoffs involved to deliver the required levels of quality and grade.\nPrecision vs. Accuracy\nPrecision means the values of repeated measurements are clustered and have little scatter (show the same results under the same conditions).\nAccuracy means that the measured values are very close to the true value (degree of closeness to true value).\nPrecise measurements are not necessarily accurate. A very accurate measurement is not necessarily precise. The project management team must determine the levels of accuracy and precision.\nProject Management and Quality management recognize the importance of:\n- Customer satisfaction: Understanding and managing expectations do that customer requirements are met.\n- Prevention over inspection: Quality is planned, designed, and built in – not inspected in. The cost of preventing mistakes is much less than of correcting them when inspected.\n- Continuous improvement: The plan-do-check-act is the basis for quality improvement. Other initiatives such as TQM and Six Sigma, should improve the quality of the project and the product.\n- Management responsibility: Success requires participation of all team members, but remains the responsibility of the management to provide the resources needed to succeed.\nCost of Quality (COQ)\nRefers to the total cost of all efforts related to quality through the product life cycle. Project decisions can impact operational costs of quality.\nThe Quality Management Processes:\n1. Plan Quality\nIs the process of identifying quality requirements and standards for the project and its product, and documenting how the project will demonstrate compliance and should be performed in parallel with the other project planning processes. For example, proposed changes in product to meet a quality standard may require cost or schedule adjustments and a risk analysis of the impacts.\n1.1 Plan Quality: Inputs\na. Scope base Line:\n- Scope statement: Project description, deliverables, acceptance criteria.\n- WBS: The work packages and the control accounts to measure performance.\n- WBS Dictionary: technical information for WBS elements\nb. Stakeholder Register: Identifies the stakeholders with particular interests in, or impact on quality.\nc. Cost Performance Baseline: Documents the accepted time phase used measure cost performance\nd. Schedule Baseline: Documents the accepted schedule performance measures.\ne. Risk Register: Contains information on threats and opportunities impact quality requirements.\nf. Enterprise Environmental Factors: Factors that influence the Plan Quality Process such as Governmental regulations, rules, standards, working conditions.\ng. Organizational Process Assets: Assets that influence the Plan Quality Process such as, quality polices/procedures, lessons learned from previous projects, quality policy as endorsed by senior management. If there is no quality policy, the project management team needs to develop a quality policy for the project and to ensure that the project stakeholders are fully aware of the policy used.\n1.2 Plan Quality: Tools and Techniques\n- Cost – Benefit Analysis: A business case for each quality activity compares the cost of the quality step to the expected benefit.\n- Cost of Quality (COQ): Costs incurred over the life of the product by investment in preventing non-conformance to requirements, appraising the product for conformance to requirements and rework. Failure costs (cost of poor quality) categorized into internal (by the project) and external (by the customer).\n- Control Charts: Charts used to determine whether or not a process is stable or has predictable performance. Control limits are set by the project management and appropriate stakeholders to reflect points of corrective actions.\n- Benchmarking: Comparing actual or planned practices to other projects to identify best practices, generate ideas for improvements, and provide basis for measuring performance.\n- Design of Experiments: DOE is a statistical method for identifying which factors may influence variables of a product. DOE should be used to determine the number and type of tests and their impact on costs and quality.\n- Statistical Sampling: Involves choosing part of a population of interest for inspection\n- Flowcharting: A graphical presentation of a process showing the relationship among processes. Flowcharting can help anticipate quality problems that might occur.\n- Quality Management Methodologies: Six Sigma, CMMI, etc…\n- Additional Quality Planning Tools: Such as Brainstorming, Affinity Diagrams, Force field analysis, Matrix Diagrams, Prioritization Matrices.\n1.3 Plan Quality: Outputs\na. Quality Management Plan: Describes how the project management team will implement the quality policy. The plan should be reviewed early in the project to ensure that decisions are based on accurate information to reduce cost of rework.\nb. Quality Metrics: Operational definitions that describe a project or product attribute and how quality control will measure it. Example metrics: on-time performance, budget control, defect frequency, failure rate, availability, reliability.\nc. Quality Checklists: Structured tools used to verify that a set of required steps has been performed. Quality checklists are used in the quality control process.\nd. Process Improvement Plan: Details the steps for analyzing processes to identify activities which enhance their value, the following areas are considered:\n- Process boundaries: Purpose of processes, start and end, inputs and outputs, data required owner and stakeholders.\n- Process Configuration: Graphic depiction of processes with interfaces to facilitate analysis.\n- Process metrics: With control limits allows analysis of process efficiency.\n- Targets for improved performance: Guides to improvement activities.\ne. Project Document Updates: Stakeholder register and responsibility assignment Matrix.\n2. Perform Quality Assurance\nQuality Assurance is the process of auditing the quality requirements and the results from quality control measurements to ensure appropriate quality standards and operational definitions are used. Quality Assurance may be provided by the project team, the management, the customer, or sponsor as well stakeholders.\nQuality Assurance also provides process improvement to reduce waste and eliminate activities do not add value which results increased level of efficiency and effectiveness.\n2.1 Perform Quality Assurance: Inputs\na. Project Management Plan: which also include the following:\n- Quality management plan: Describes how quality assurance will be performed.\n- Process improvement plan: Steps for analyzing processes to identify activities which enhance their value.\nb. Quality Metrics: As described in 1.3.b\nc. Work Performance Information: Performance information collected from the activities to support auditing. It includes:\n- Technical performance measures\n- Project deliverables status\n- Schedule progress\n- Costs incurred\nc. Quality Control Measurements: They are the results of quality control activities and used to analyze and evaluate the quality standards and processes.\n2.2 Perform Quality Assurance: Tools and Techniques\n- Plan Quality Tools: as described in 1.2 can also be used for quality assurance\n- Quality Audits: A structured, independent review to determine whether project activities comply with policies processes, and procedures. They may be scheduled or random and may be conducted by internal or external auditors to confirm the implementation of approved change requests, corrective actions and preventive actions.\n- Process Analysis: Which follows the steps in the process improvement plan to identify needed improvements. This analysis examines problems, constrains and non-value-added activities identified. This analysis also includes problem identification to discover causes and develop preventive actions.\n2.3 Perform Quality Assurance: Outputs\n- Process Assets Updates: Elements of the organizational process assets that may be updated.\n- Change Requests: Created and used as input into the Perform Integrated Change Control to allow full consideration of the recommended improvements. Change requests can be used to take corrective action or preventive action.\n- Project Management Plan Updates: May update Quality, Schedule or Cost Management Plans.\n- Project Document Updates: Updates quality audit reports, training plans and process documentation.\n3. Perform Quality Control:\nThe process of monitoring and recording results of executing the quality activities to assess performance and recommend necessary changes. Quality standards include project processes and product goals. Project results include deliverables and project management results. Quality control activities identify causes of poor process or product quality and recommend/take action to eliminate them. The project team must know the difference between:\n- Prevention (keep errors out of the process) and inspection (keep errors out of the customer’s hands)\n- Attribute Sampling (the result conforms or does not conform) and Variables Sampling (the result measures the degree of conformity)\n- Tolerance (range of accepted results) and Control Limits (thresholds)\n3.1 Perform Quality Control: Inputs\na. Project Management Plan: This includes the quality management plan which is used to describe how quality control will be performed.\nb. Quality Metrics: As described in 1.3.b\nc. Quality Checklists as described in 1.3.c\nd. Work Performance measurements: Produce metrics to evaluate planned vs. actual for:\n- Technical performance\n- Schedule performance\n- Cost performance\ne. Approved Change Requests: The timely implementation of approved changes needs to be verified.\nf. Process Assets: Assets that can influence the Perform Quality Control Process which includes:\n- Quality standards and policies\n- Standard work guidelines\n- Issue and defect reporting procedures and polices\n3.2 Perform Quality Control: Tools and Techniques\n- Cause and Effect Diagrams: illustrate how factors linked to problems or effects. A possible root cause can be uncovered by asking “why” or “how” along one of the lines.\n- Control Charts: As described in 1.2.c. Control Charts illustrate how a process behaves over time. They answer the question “Is the process variance within acceptable limits?” and help assess whether the application of process changes resulted in desired improvements.\n- Flowcharting: As described in 1.2.g. Used to determine a falling process steps and identify improvement opportunities.\n- Histogram: Vertical bar chart showing how often a variable state occurred. Each column represents an attribute or characteristic. Helps illustrates the cause of problem by a number and heights of the bars.\n- Pareto Chart: Shows how many defects were generated by type or category of causes.\n- Run chart: A line graph shows data points plotted in the order in which they occur and shows the history and pattern of variation. Trend analysis is performed using run charts to monitor Technical performance and Cost and Schedule Performance.\n- Scatter Diagram: Shows the relationship between two variables and allows the quality team to study and identify the possible relationship between changes in two variables.\n- Statistical Sampling: Samples are selected and tested as defined in the quality plan.\n- Inspection: The examination of a work product to determine whether it conforms to documented standards.\n- j. Approved Change Requests Review\n3.3 Perform Quality Control: Outputs\n- Quality Control Measurements: Documented results of quality control activities.\n- Validated Changes: Changed or repaired items are inspected and accepted or rejected. Rejected items may require rework.\n- Validated Deliverables: This is the goal of quality control.\n- Process Assets Updates: Such as Completed checklists and lessons learned.\n- Change Requests: If a corrective or preventive action requires a change in the project management plan, a change request should be initiated.\n- Project Management Plan Updates including the quality management and process improvement plans.\n- Project document Updates including the quality standards."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:45d413e2-f4bc-4fee-a0d0-5ab13097652c>","<urn:uuid:5bcd8717-3166-4900-802a-79e454f64cee>"],"error":null}
{"question":"What methods are available for fixing cracks in asphalt, and which method should be used depending on crack size?","answer":"For cracks less than 3/4 of an inch wide, asphalt sealant should be used. For cracks wider than 3/4 of an inch, crack filling is the appropriate method. The process involves cleaning out debris from the cracks first. For smaller cracks up to 3/4 inch, you can use either pourable or tubed asphalt patch products, with tubed products requiring a caulk gun for application.","context":["How to Repair Asphalt Driveway Cracks | HowStuffWorks\nThe most frequent cause of cracks in your asphalt driveway is water that has seeped below the surface and expanded or contracted with freezing and thawing temperatures. After patching and filling any cracks or dips in your asphalt driveway, you may also wish to seal .\nDo It! How to Repair and Seal an Asphalt Driveway | Today .\nAsphalt driveways can develop cracks and deteriorate over time. To keep your driveway in good shape, it's important to clean, repair, and seal it every few years. It's best to work on an asphalt driveway during warm weather so the repair material and sealer will set faster. Be sure to fill all .\nAsphalt Maintenance and Repair - Asphalt Preservation\nAsphalt maintenance and repair is essential to the life of existing asphalt. Asphalt sealing, such as slurry seal, seal coat, and crack seal, provide a protective coating for asphalt-based pavements which protect it from breaking down from water, oils, U.V. damage, and normal wear and tear.\nAsphalt Seal Your Own Driveway! - YouTube\nAug 26, 2016 · This is a simple step by step instructional video to help guide you through the steps of sealing your driveway. CHECK OUT OUR LATEST VIDEOS 🎥Cleaning Large.\nAsphalt Sealcoating Equipment and Asphalt Sealing .\nAt Asphalt Sealcoating Direct, you will find an assortment of high quality, budget priced Asphalt Sealcoating Equipment for sale including Heat Lances, 10 Gallon Melter / Applicators, Asphalt Sealcoating Spray Equipment for sale and asphalt crack filler, along with many other asphalt sealing equipment products we offer for sale.\nAsphalt Crack Repair Treatments: Crack Sealing Vs. Crack .\nSealing Asphalt Cracks Crack sealing is a localized treatment that prevents water and other debris from getting inside of asphalt abrasions and causing further damage to roadways. In order to properly seal a crack, all debris must be blown out and cleaned away.\nAsphalt Crack Filling and Sealing - Local Sealcoating\nCrack Sealing is an Integral Component of an Effective Pavement Preservation Program. A scheduled program of pavement maintenance that includes sealcoating and crack filling will more than double the life cycle of your asphalt pavement and provide maximum curb appeal.\nUtah Crack Sealing Services | Extend the Life of Your Asphalt\nMore cracks will form allowing more water to enter the base. Without maintenance, this cycle will destroy your asphalt. Crack seal seals cracks from water. Crack seal is made of rubberzed asphalt which expands and contracts with the crack. Crack sealing is one of the most important things you can do to extend the life of your asphalt.\nProperly sealing asphalt cracks as they appear can more\nProperly sealing asphalt cracks as they appear can more than double the effective service life of the pavement. Pavement is a costly investment. Just as you would seal leaks in the roof of your building, you should seal cracks in your pavement. Water infiltration through cracks causes deterioration of the pavement and substructure.\nCrack Sealing - Fix Asphalt\nApplication of a hot rubberized asphalt crack sealer will prevent water penetration into the sub-base thereby prolonging the life of your pavement and elimination of potential liabilities. Hot rubberized crack sealing, if installed properly, is the foundation of a successful maintenance program for any type of .\nSeal Coating - Extraordinaire Asphalt & Grounds .\nExtraordinaire is a new asphalt and grounds maintenance company located in Baltimore Maryland, but with years of experience behind us. We specialize in asphalt seal coating, asphalt crack sealing, line striping, pot hole repair, and crack sealing, among other services in Baltimore.\nShould You Seal Or Fill Asphalt Cracks . - Kleenco .\nSealing or filling asphalt cracks is directly dependent upon the extent of asphalt cracks measured in width. Generally, asphalt sealant is used for cracks that measure less than ¾ of an inch in width and crack filling is used for cracks that measure wider than ¾ of an inch.\nThe Basics of Crack Filling / Crack Sealing | Paveman Pro\nThe Basics of Crack Filling by Greg Walters, Sealmaster It's no secret that cracks in asphalt surfaces allow water to penetrate the surface, softening the stone base and leading to alligator cracking/spider webbing, pot holes and other forms of degradation.\nConcrete Crack Filler, Best Asphalt Crack Sealant for .\nKOLD-FLO ® Pourable Crack Filler is a modified asphalt emulsion concrete and asphalt crack filler & sealer that is ideal for asphalt crack repairs in roads, pavements, driveways, concrete foundations, parking lots and other asphalt or concrete surfaces. It is a cost-effective and fast-curing asphalt concrete crack repair solution for filling and sealing cracks from weather's destructive .\nAsphalt Sealcoating Contractor - Crack Repair | Portland OR\nAsphalt Crack Repair. Cracks usually form from expansion and contractions in the ground. Paying close attention to these cracks is always a good idea; this is because the sooner you seal or fill these cracks, the less likely they will continue to grow.\nAsphalt Patching and Crack Repair | The Family Handyman\nFix driveway cracks. In cold climates, water seeps in and destroys the asphalt when it expands during freezing. If you plan to topcoat your driveway, you'll need to fix the cracks first and do asphalt repair later. You can buy squeeze bottle and caulk tube–style crack filler products from any home center.\nCrack Sealing | Asphalt Repair Solutions\nAsphalt cracks are normal and occur over time. Your maintenance program should include professionally sealing cracks that are 1/8\" wide or greater with a proper crack sealant, to mitigate damage to your asphalt caused by water intrusion.\nHow to Seal an Asphalt or Concrete Driveway | Apply .\nHomeAdvisor's Driveway Sealing Guide instructs on how to use asphalt blacktop or concrete sealer to sealcoat, reseal, tar, or repair cracks. Discover the best process for DIY driveway sealing, or how to clean and prepare your pavement when working with a professional. Questions anwered including whether it needs to be done for new driveways, and what supplies and materials are needed.\nRepair Asphalt Driveway Cracks - Lowe's Home Improvement\nThe forces of nature do their best to break down asphalt, creating cracks and potholes. Repairing driveway cracks is a lot simpler than you may think. Cracks up to 3/4 of an inch wide can be repaired with a pourable or tubed asphalt patch product. A tubed product requires a caulk gun for application .\nThe Difference between Crack Filling and Crack Sealing .\nWhile the terms crack filling and crack sealing are often used interchangeably, there are differences between these methods of pavement crack repair. Understanding these differences will help you make the most cost-effective choice and long-lasting solution for crack treatment of your asphalt surfaces. Crack sealing\nAsphalt Crack Sealing Near Me | Randazzo Paving | New Hope PA\nSealing cracks prevents water and moisture penetration below your asphalt and eliminates the most common cause of premature asphalt pavement failure and deterioration. We use a hot applied crack and joint sealant that is engineered to stay flexible in hot and cold temperature extremes to ensure the crack stays sealed. Please note- Crack sealing .\nHow To Fill Cracks In Asphalt Using Asphalt Crack Filler .\nHow to Fill Asphalt Cracks In Driveways and Parking Lots. . Sealcoating is not designed to be an asphalt crack filler, it is made for sealing the surface and protecting it from the elements such as UV light, salts and oils. It is not flexible enough to fill a crack without re-cracking and it does not have adhesion quality's that Asphalt Crack .\nCrack Sealing Company Pavement Crack Filling Crack Repair\nCrack Filling & Sealing Overview. Pavement cracks need to be filled and/or sealed to prevent water from entering sub-grade layers. Special rubberized materials are applied to the cracks, which seals the pavement and helps to prolong its life.\nHow Much Does it Cost to Seal an Asphalt Driveway? | Angie .\nHow Much Does it Cost to Seal an Asphalt Driveway? . Sealing the cracks and the pavement properly is absolutely essential to prevent the damage from occurring. While a poor sealant job has no affect and only looks good, a good sealant job can add years to your driveway in .\nAsphalt Crack Repair - asphaltkingdom\nPreparing for Crack Filling Asphalt This is the most time-consuming and most important part of the job, but it's also easy for any DIY'er. To get the longest-lasting repair, the crack seal needs a clean and solid area to stick to. Any debris or weeds that are left in the crack will degrade your crack repair job.\n7 Different Types of Pavement Failure | Types of Cracks in .\nCracks need to be repaired right away or else they can expand and become costlier in the long run. Perhaps the hardest part of repairing cracks in your pavement is identifying the exact cause and type of crack in your asphalt pavement. 7 Types of Cracks in Asphalt Pavement\nCrack Sealing | A-Line Asphalt Maintenance, Inc.\nCrack sealing is the process of pouring a hot rubberized tar into isolated cracks in the asphalt pavement to prevent water from penetrating the asphalt base. Benefits of Crack Sealing Protects the Base – Effective crack sealing keeps water from entering and weakening the base of the asphalt.\nDifference Between Crack Filling and Crack Sealing .\nCrack sealing is an important procedure that must be addressed yearly. Crack sealing is one of the most cost-effective ways to prolong the pavement life, as much as 3- 8 years. The terms crack filling and crack sealing are often confused as being one and the same. Not true, they are different.\nProfessional Tips for your Next Asphalt Crack Sealing Project.\nCrack sealing is one of the most cost effective methods for repairing damaged pavement before it progresses too extensively. Depending upon the climate conditions, traffic volume and load, type of sealant used, and the application method – asphalt crack sealing can extend the life of damaged pavement by between six months and four years."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:2b894d97-6371-468e-8e52-b8fe6e167898>"],"error":null}
{"question":"How does the MVA technology contribute to personalized medicine, and what are the HIPAA compliance requirements for managing the resulting patient data?","answer":"MVA technology enables creation of personalized immunoprofiles that define individual humoral immune response profiles, with a metadatabase of over 2000 individuals' profiles allowing disease susceptibility prediction. For biomaterials, it helps identify patient-specific immune responses and generates a biomaterial-specific immunoprofile database. For managing such patient data, HIPAA requires implementation of strict security measures. These include access control limiting authorized users to minimum necessary information, audit controls, authentication systems, and secure data storage. While encryption isn't explicitly required by HIPAA, it's recommended as the best way to protect patient information and ensure authorized access only.","context":["Beginning of June the 17 PANBioRA partners have met in Brussels for their first official periodic review meeting in month 18 of the project. In the presence of a representative of the European Commission each partner has presented the performed work since the beginning of the project as well as main results achieved.\nThe PANBioRA system is composed of four different types of analyses allowing biomaterial risk assessment at nano-, micro- and milliscale level. During the first 1 ½ years of the project, different project partners have worked on these analyses with significant progress.\nAmong these different testing systems, there is the Mimotope Variation Analysis (MVA) developed and patented by the Estonian biotechnology company Protobios which has never been used in the framework of biomaterial assessment before. Immune responses contribute to the development of many common disorders (e.g. Alzheimer’s disease) but also to immunologic diseases including infectious diseases (e.g. tuberculosis, hepatitis, pneumonia). With the MVA technology, Protobios has developed a method to create a personalised outline, called immunoprofile, allowing to define an individual’s humoral immune response profile. Over the past years Protobios has developed a metadatabase of immunoprofiles of more than 2000 individuals. This enables them to predict an individual’s susceptibility to specific diseases.\nFor PANBioRA, the MVA technology is being further developed and adapted to identify a patient-specific response of the immune system to biomaterials. A biomaterial specific immunoprofile database is being generated allowing to comprehensively understand the properties, interaction and fate of engineered biomaterials in relation to human health and environment. This will pave the way for the design of new generations of biomaterials with tailored functional and immunological properties. As PANBioRA is aiming to provide a faster, reliable and quantitative assessment of new biomaterials, the MVA technology plays a substantial part in the overall PANBioRA system.\nOne of the major challenges for the upcoming project months will be to successfully integrate the different test modules, developed within the project in one comprehensive system. Until the beginning of next year, a first prototype of the whole system will be ready in order to further test, improve and validate. PANBioRA will provide substantial health benefits for millions of patients who are candidates for receiving medical implants by improving their function and reducing the chance of failure due to inflammation.\nFurther details about Protobios and their innovative MVA technology are available on the company’s website: http://www.protobios.com/\nSteinbeis 2i GmbH\nTimo Doll | firstname.lastname@example.org |+49 721 93519141\nPANBioRA is coordinated by Steinbeis 2i GmbH in cooperation with 16 partners: Protip Medical (Scientific Coordinator), Dolmen Design and Innovation Limited, Biodevice Systems, Protobios, Elvesys SAS, Steinbeis Advanced Risk Technologies Institute doo Kragujevac, Steinbeis Advanced Risk Technologies GmbH, Commissariat A L’Energie Atomique Et Aux Energies Alternatives, Pro-active, Aalto-Korkeakoulusäätiö, Turgut Ozal Education SHA, The University of Nottingham, Agencia Estatal Consejo Superior de Investigaciones Científicas, Dublin City University, Institut National de la Santé et de la Recherche Medicale, Centre Hospitalier Universitaire de Liège\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 760921.\ncontact for scientific information:\nProject coordinator: Dr. Nihal Engin Vrana | email@example.com","RESOURCES - ARTICLE LIBRARY\nStaying HIPAA Compliant with Online Data Storage\nKeeping patient records secure and private is the concern of every hospital and health care provider, but they are often overwhelmed with years and years of patient information and the lack of adequate storage space. Destroying these health records in order to make room for more storage is often not an option. Patients want access to all of their health care records, and physicians need them in order to better diagnose patients. Online data storage is a way to satisfy all of these issues. Using online storage for these records allows easier access for patients, and offers easier sharing of patient information from hospital to physician, as well as from physician to physician. Storing health records online isn’t, however, without security concerns. Patients, hospitals, and physicians want assurance that these confidential records will remain safe, private, and secure, and will only be accessed by those authorized to do so.\nWhat is HIPAA?\nHIPAA or the Health Insurance Portability and Accountability Act of 1996 was created in order to protect health information and give patients certain rights regarding their private health information. It also allows for disclosure of health information necessary for patient care. This act specifies safeguards necessary for administrative, and physical and technical handling of patient health information.\nAccording to the U.S. Department of Health and Human Services (HHS.gov) HIPAA has many requirements and restrictions. It requires safeguards for:\n- Access Control\n- Audit Controls\n- Person or Entity Authentication\nAccess control is defined in the HIPAA Privacy Rule as “the ability or the means necessary to read, write, modify, or communicate data/information or otherwise use any system resource.” It should allow authorized users to only access the minimum amount of information necessary to complete job functions. The Access Control specification also requires the implementation of an exclusive user identification or user ID, and immediate access in case of an emergency.\nWhat Type of Security is Necessary?\nWhen dealing with patient records in an office, maintaining privacy and security usually involves storing patient files in locked cabinets where the files can be physically secured and visibly monitored at all times. When you are storing patient information online, certain precautions must be met in order to maintain the same security and privacy guaranteed each patient.\nWhile HIPAA permits patient records to be transmitted over the Internet, businesses will want a service that offers file encryption, authentication and password protection in order to secure the information. Although HIPAA does not require online data storage services to have encryption, it does require that patient information be adequately protected and accessible only to authorized persons. Encryption is the best way to protect that information and ensure authorized access to those records. It is also important to offer backup services in case of a virus attack, flood, or fire. Finally, the service must offer a method of tracking any security breach, as well as the ability to lock out former employees after they have left or been terminated.\nWhen storing patient information, it is important to stay HIPAA compliant, as the fines for not doing so are expensive. While online storage for health care businesses guarantee less worry, work, and expense for health care providers, the service is only as good as the security offered. Remaining HIPAA compliant is vital in order to continue a good business relationship with the health care industry.\nContent by Managed Services Provider University"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c7941e16-d68f-45e5-bfcc-7dabdc53c49d>","<urn:uuid:53c1e085-512d-454f-869c-1a294b08602e>"],"error":null}
{"question":"What was the historic significance of Milosevic's trial at the ICTY, and how did later cases like Gotovina impact the tribunal's credibility?","answer":"Milosevic's trial was historically significant as he became the first former head of state to face the U.N. war crimes tribunal. The tribunal charged him with crimes against humanity for atrocities in Kosovo that left thousands of ethnic Albanians killed or missing. However, later cases like Gotovina's damaged the tribunal's credibility, particularly when the Appeals Chamber's 3-2 split decision to acquit him contradicted the Trial Chamber's unanimous conviction. This controversial decision, especially regarding the existence of a joint criminal enterprise by Croatian leadership, led to the tribunal losing credibility in Serbia, where it came to be viewed as a political court rather than an instrument of justice.","context":["Milosevic arrives in The Hague to face U.N. war crimes charges\nBELGRADE, Yugoslavia (AP) – Slobodan Milosevic, who lost four wars, impoverished his people and turned Yugoslavia into an international pariah, was delivered to the U.N. war crimes tribunal early Friday to face charges of ”crimes against humanity.” He will be the first former head of state to face the court.\nAuthorities in Belgrade handed over the former dictator to a tribunal official Thursday, ignoring a court ruling that barred his extradition to stand trial for alleged atrocities in Kosovo.\nIt was an abrupt end to his battle to evade international justice and the prelude to trial before the tribunal which has indicted him for his alleged role in the Kosovo atrocities that left thousands of ethnic Albanians killed or missing and made refugees of hundreds of thousands more.\n”This is the ultimate case,” Jim Landale, spokesman for the tribunal, said after Milosevic arrived. He told CNN that it would be a ”relatively lengthy trial” with complex legal issues.\nMilosevic’s transfer could free up generous allotments of financial aid that Washington has linked to his extradition, which came the day before a Belgium conference to discuss those funds.\nPraising the move to extradite Milosevic, President Bush called it proof the Balkan nation wants to turn away from ”its tragic past and toward a brighter future.”\nThe swift move by Serbia – by far the most powerful of Yugoslavia’s remaining two republics – reportedly caught lawyers for Milosevic by surprise.\nThe state Tanjug news agency said that President Vojislav Kostunica, Milosevic’s successor, was informed of the hand-over only after it happened. His lawyers expressed astonishment of news that their client had been surrendered.\nAn attorney for Milosevic, Branimir Gugl, accused the authorities of kidnapping his client.\n”The process of extradition without the presence of attorneys is tantamount to an abduction,” he said. Another of his lawyers, Toma Fila, said: ”I cannot believe that this has happened.”\nAs word spread of the transfer, about 3,000 pro-Milosevic supporters gathered in downtown Belgrade. ”Uprising, uprising,” the crowd chanted. Some took swings at television crews covering their demonstration. Several people were badly beaten.\nMilosevic’s wife, Mirjana Markovic, briefly appeared at the central prison’s gate but turned back without entering.\nSerbian Prime Minister Zoran Djindjic said there was no choice for Yugoslavia but to surrender Milosevic or face renewed international isolation and a freeze on financial aid, leading to ”unprecedented humiliation.”\nMilosevic was apparently flown aboard a Serb aircraft to Tuzla, headquarters for the American peacekeeping operation in Bosnia, where he was transferred to a British aircraft and flown to the The Hague. A U.S. defense official in Washington told the AP that there was no direct American military involvement in Milosevic’s transfer.\nShortly before Landale spoke, reporters saw a police helicopter land inside the prison walls about 1:16 a.m. (7:16 p.m. EDT) while a second helicopter hovered overhead.\nMilosevic was expected to be interviewed by tribunal officials who will read the charges against him and explain his rights. There was no word on when he will be arraigned.\n”The forthcoming trial of a former head of state is a new and irreversible step in relation to the international community’s resolve to fight against impunity,” tribunal President Claude Jorda said.\nNATO Secretary-General Lord Robertson called the move a ”wise and courageous decision.”\nThe U.N. war crimes tribunal was founded in 1993. Milosevic would be the first former head of state tried by the court. Former Japanese Prime Minister Tojo was tried convicted and executed by a war crimes court from 1946-47, but it was not a U.N. court.\nMilosevic, 59, has been in jail since April while local allegations of abuse of power and corruption were investigated. He was indicted by the U.N. tribunal for alleged atrocities committed in Kosovo during an offensive two years ago against the province’s ethnic Albanian population. About 10,000 ethnic Albanians were estimated to have died in the crackdown, which ended after NATO’s 78-day bombing campaign.\nThe charges in the May 1999 indictment include crimes against humanity and violation of the laws and customs of war. The war crimes tribunal has said it is preparing a possible case against Milosevic for genocide in connection with atrocities committed in the wars in Bosnia and Croatia.\nHe once described himself as the ”Ayatollah Khomeini of Serbia,” declaring that ”the Serbs will follow me no matter what.” For years, they did – through wars which dismembered Yugoslavia. But in the end, his people abandoned him.\nAhead of a key aid conference in Brussels, Belgium, on Friday, Yugoslavia’s pro-democracy government had intensified its efforts to extradite Milosevic in order to meet international demands.\nWashington on Wednesday announced it would send representatives to the donors’ conference, after weeks of waiting to see how serious Yugoslavia’s efforts were. Yugoslavia is in need of billions of dollars worth of foreign aid after 13 years of rule by Milosevic, which ended in October after riots forced him to concede losing elections.\nEarlier Thursday, Milosevic appeared to have won more time in his fight to avoid trial by the tribunal when judges on the Constitutional Court suspended a federal government decree allowing his extradition.\nThe court – made up of judges appointed under Milosevic – ruled it needed more time to consider the government decree enabling the handover.\nThe decision to bypass the court decision came from the government of Serbia, which together with Montenegro makes up the Yugoslav federation. Senior Serbian officials had served notice they would surrender Milosevic to the tribunal even if the federal Constitutional Court suspended the extradition decree.\nIn a statement explaining the move, Djindjic said his government had decided to take over the jurisdiction from federal authorities on the extradition law.\nHe called the Constitutional Court decision ”an attempt to compromise the entire future of our country … a sellout of Serbia’s future.”\nSupport Local Journalism\nSupport Local Journalism\nReaders around the Lake Tahoe Basin and beyond make the Tahoe Tribune's work possible. Your financial contribution supports our efforts to deliver quality, locally relevant journalism.\nNow more than ever, your support is critical to help us keep our community informed about the evolving coronavirus pandemic and the impact it is having locally. Every contribution, however large or small, will make a difference.\nYour donation will help us continue to cover COVID-19 and our other vital local news.\nStart a dialogue, stay on topic and be civil.\nIf you don't follow the rules, your comment may be deleted.\nUser Legend: Moderator Trusted User\nINCLINE VILLAGE, Nev. — The Incline Village General Improvement District Board of Trustees voted to move forward with remodeling the upstairs bathrooms at the recreation center, but delayed the locker room remodel project.","Friday’s judgment in Gotovina and Markac by the ICTY Appeals Chamber (summary; judgment), in which it by 3 votes to 2 reversed a unanimous Trial Chamber and acquitted the defendants, is a disaster at almost every level. I say this not as an aggrieved Serb lamenting the selectiveness of international justice and its failure to punish crimes against his own people – I have long since developed antibodies to all forms of nationalism, including the very virulent type thriving on self-victimization, and I have no personal axe to grind here. I say this rather as an international (human rights) lawyer who has always thought of the ICTY as an indispensable, if imperfect, instrument of justice for the atrocities of the Yugoslav conflicts. That said, how and why then is the Gotovina appeals judgment so bad? To my mind, the problem is not with the acquittal as such – even though as far as public opinion in the former Yugoslavia is concerned the bottom line is all that mattered. Rather, the problem is with the process, the reasoning, the appearances, and the broader repercussions that all these will have.\nFirst, with regard to process: as the dissents by Judges Agius and Pocar correctly point out, the majority make a complete mess of the appellate standards for review. Readers will recall that in the common law-inspired procedure of the ICTY the main task of the Appeals Chamber is to correct errors of law made by the Trial Chamber. The Trial Chamber is owed deference with regards to its findings of fact, which are not to be disturbed lightly on appeal, but only if no reasonable trier of fact could have made the relevant finding on the strength of the record. In short, unlike in most continental systems, the appellate process should not amount a retrial, a de novo examination of the entire case. This ensures both procedural economy and the integrity of the exhaustive fact-finding process in the trial court.\nWhile the majority endorses these standards as they are set out in the ICTY’s long-established jurisprudence, it does not actually follow them – to the extent that its approach to standards of review is actually even discernible, as I will now explain. The whole case ultimately turned around the Trial Chamber’s unfortunate finding that in assessing the shelling by the Croatian artillery of the four Serb towns in the separatist Serb entity in Croatia, chief of them Knin, any shell that fell further than 200 meters from a legitimate military target in the towns should be presumptively considered as evidence of an unlawful indiscriminate attack. The Appeals Chamber was actually unanimous that this rigid standard was not supported by the evidence in the trial record and was not given adequate reasons for by the Trial Chamber.\nSo far so good. But what the majority then does with this finding turns appellate review on its head. The majority does not explain whether the Trial Chamber’s error with regard to the 200 meter standard itself was an error of law or an error of fact. If it was the former, the majority would have had to articulate a new, proper legal standard for testing the facts established in the record, upon whose application we could know whether the shelling of Knin and the other towns was indiscriminate or not. If it was the latter, the majority would have had to pay due deference to the totality of the factual findings made by the Trial Chamber and should only have disturbed them if no reasonable trier of fact could have found that the shelling was indiscriminate on the basis of all of the evidence in the record.\nBut this is not what the majority of the Appeals Chamber in fact did. It rather treated the Trial Chamber’s error with regard to the 200 meter standard as a fatal flaw that unraveled the entire trial judgment, and proceeded with a rather bizarre form of de novo review of the facts on the apparent basis of the Trial Chamber’s failure to provide adequate reasoning as a legal error. It then went on to examine each item of evidence in clinical isolation, determined that essentially all other evidence depended on the core issue of the 200 meter standard, even though the Trial Chamber never assigned that standard such importance in the trial judgment itself, and proceeded to discard one by one almost all of the Trial Chamber’s principal factual findings (the majority does most of this work in paras. 64-67 of the judgment).\nThus, in the majority’s view the Trial Chamber’s error with regard to the 200 meter standard made it impossible to establish beyond a reasonable doubt whether the shelling of Knin and the other Serb towns was indiscriminate or not. In effect, the majority thought there was no standard that could supplant the erroneous 200 meter standard. The consequence of this agnosticism was the majority holding that no reasonable trier of fact could have found that Knin was shelled indiscriminately, even though, as Judge Agius pointed out in his dissent, it was undisputed that over 900 artillery projectiles fell on Knin in the course of one and half days in the absence of any resistance from the town itself (Agius dissent, para. 18). The majority similarly overturned the Trial Chamber’s finding that the distribution of projectile impacts in Knin could not be explained by the Croatian army engaging moving targets of opportunity. In this respect, as Judge Agius correctly observed (Agius dissent, para. 32), the majority’s reasoning becomes simply contradictory:\nWith respect to the one police car that was hit in Knin, the Majority assumes that HV [Croatian Army] artillery weaponry could be so accurate as to obtain a direct hit, but with regard to all of the military targets which had been pre-established with proper co-ordinates, the Majority effectively gives the HV the benefit of the doubt ad infinitum. I would be enlightened by an explanation from the Majority as to how, if the HV could be so accurate with regard to a moving object, it could miss military targets by hundreds of metres?\n(Note the tone of the dissent, to which I will turn later).\nThe majority then went on to give the 200 meter standard a key role in its assessment of the existence of a joint criminal enterprise on the part of the Croatian leadership to ethnically cleanse the Croatian Serbs. In its view, the Trial Chamber considered unlawful shelling to have been an indispensable component of this enterprise, as this was the primary way of forcibly displacing civilians. Accordingly, as there was no way to establish beyond a reasonable doubt that the shelling was unlawful, there was also no way for a reasonable trier of fact to establish that a JCE even existed. In doing so, the Appeals Chamber essentially imputed to the Trial Chamber views that it had never adopted, and overturned yet one more factual finding by using de novo review of individual items of evidence taken in isolation, which it found lacking in the absence of the context of unlawful shelling. These included the transcripts of the meetings of the Croatian leadership at Brioni, evidence of numerous individual crimes on the ground perpetrated by Croatian forces, inflammatory speeches by Croatian president Tudjman, and the subsequent implementation of discriminatory measures designed to prevent the return of Serbs to Croatia.\nWith the JCE gone, the convictions had to be vacated. In a rather remarkable turn of irony, the majority then refused to enter convictions on alternate grounds (such as aiding and abetting), which were not pleaded in the original indictment or considered by the Trial Chamber, finding that this would effectively amount to a retrial and would exceed the proper scope of appellate review – which is precisely what they did in the remainder of the judgment.\nOf course, there is nothing wrong with de novo appellate review as such, if we forget for a moment the majority’s assessing of individual items of evidence in isolation rather than in their totality. The problem here is one of institutional design. Not only did the majority of the Appeals Chamber think that they better appreciated the facts of the case than the Trial Chamber, but they did so from the absolutely passive position of not remedying in any way the (alleged) deficiencies in the evidentiary process at trial, which is what de novo review in a continental system would do. If, for instance, the evidence collected at trial really made it impossible to conclusively establish whether the shelling was indiscriminate, a court exercising proper de novo review could have reopened evidentiary proceedings and called new witnesses and experts. In the way it went about the matter, however, the majority combined the worst of both worlds.\nSo much about the law. What makes matters worse is that the majority’s reasoning is lacking in all practical wisdom. It not only overturns the Trial Chamber’s factual findings lightly, it does so casually, in a couple of paragraphs for issues where the trial judgment ran at hundreds of pages, and with apparent disregard for the consequences. Perhaps the individualized guilt of Gotovina and Markac really wasn’t there. Perhaps they should have been acquitted because they were not members of the JCE or their contribution to it was not a substantial one, or convicted on different grounds and given a lesser sentence. Without having sat in court for the many months of trial and without looking in detail at the whole record one simply cannot be sure, and those who say they are nonetheless are more likely than not victims of their own prejudice. But even if Gotovina and Markac were to be acquitted, was it truly necessary for the majority to disregard the considered views of the Trial Chamber on so many issues, and especially with regard to the existence of the JCE?\nThis is not only where the majority’s reasoning is especially problematic, but where it is most consequential. Note that people in Croatia and Serbia didn’t really care about the two generals as individuals; what they did care about was about how the Tribunal characterized the systemic nature of the crimes (or not), and it is here that the Appeal Chamber’s decision is at its most dramatic. From a unanimous Trial Chamber declaring that the highest ranks of the Croatian leadership, including President Tudjman, formed a joint criminal enterprise with the purpose of ethnically cleansing Serbs from Croatia, to a divided, 3 to 2 decision by the Appeals Chamber that no reasonable trier of fact could have found that JCE to exist on the evidence heard by the Trial Chamber. Not only is this outcome hard to rationally explain to non-specialists, it only serves to harden the conflicting nationalist narratives in Croatia and Serbia.\nIn Croatia, the appeals judgment is conclusive evidence that the war they fought with the Serbs was not only defensive and just, but also pure and unsullied. Yes, there were some crimes perpetrated against the Serbs, but this was done by a few bad apples, to an extent understandably inspired by revenge, not the Croat leadership and state apparatus. Rather than being ethnically cleansed, the Serbs left Croatia willingly, only because their own leaders told them to do so in order to make the Croats look bad in the eyes of the world. In Serbia, the judgment only confirms the perpetual victim narrative – the ICTY and the international community never really cared about crimes against Serbs, and the Tribunal has shown itself to be nothing more than a political court. Its credibility is now not just in tatters – it is nil. In assessing these two narratives, if that is even possible, the Croatian, victorious brand of nationalism is even more poisonous and harder to cure. To the Serbs, whose leadership were objectively undoubtedly the greatest villains of the wars of the 1990s, these wars ultimately brought nothing but misery and defeat. To the Croats, however, the war was a foundational event in the creation of their modern state, with all its accompanying paraphernalia, so that even those on the more liberal side of the political spectrum, like President Ivo Josipovic, have to pay it due obeisance, and have indeed done so in the wake of the generals’ acquittal.\nThe entrenchment of these narratives means that in Croatia in particular, but also in Serbia, there will be no appetite for any further attempts for reconciliation or the search for a common, if possibly objective, truth. Each will continue to live in their own bubble, their own little alternate reality. Civil society attempts to establish a regional truth and reconciliation commission (REKOM), whose success was unlikely to begin with, will now probably falter no matter how laudable they substantively are. Voices dissenting from the two official narratives will continue to be marginalized, and ever more so.\nAnd finally, it is at the same time both fascinating and deeply disturbing to observe how the gloating Croats and crying Serbs (see this BBC report for some flavor) could so easily have found their respective roles reversed, if only one judge of the Appeals Chamber had changed his mind. While the Croatian nationalists will always say that a majority is a majority and that is that, dismissing the dissents completely, Serbs will also equally be able to point to the 5 international judges (3 judges of the unanimous Trial Chamber and 2 judges of the minority on appeals) who looked at the same record and voted to convict. Add to this the sheer vehemence of the disagreement between the majority and the minority, which is probably unprecedented in the ICTY’s jurisprudence. I personally cannot recall a judgment in which dissenting judges so bluntly stated that the majority was misinterpreting, ignoring and dismissing the findings of the Trial Chamber (Judge Agius), characterized the majority’s reasoning as ‘grotesque’ (Judge Pocar), suggested that its reasoning had other than purely legal motives (Pocar dissent, para. 30), and characterized the judgment as ‘contradict[ing] any sense of justice’ (ibid, para. 39). It is indeed a collective failure of the ICTY as an institution, and a professional failure of all those involved in the case, regardless of their good intentions, to have had a decision as important as this one ultimately decided by 3 votes to 2, and in such poor form."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:96d60f4a-fb6d-4c13-87e4-b708e46cbedb>","<urn:uuid:f70c5a5c-ce0e-48d1-a8a2-fc6bfa137218>"],"error":null}
{"question":"What are common legal violations to avoid in Islamic business practices?","answer":"Common legal violations to avoid in Islamic business practices include: engaging in unlawful transactions for big profits, using interest-based funding, failing to document agreements and debts, making obscure agreements, delaying payments despite having the ability to pay, encroaching on others' property, entrusting property to incompetent people, setting unjust conditions that lead to monopoly, failing to pay required Zakah and taxes, and relying on weak legal verdicts that contradict majority scholarly opinion.","context":["Making intellectual decisions can positively or negatively shape our financial existence, but what about the decisions that encroach on our ethical lives?\nThere is more to transactions than simple trade; in Islam, there is a certain and specific “code of business ethics” that must be followed. These rules and regulations for business conduct have been detailed and played out over the centuries by our ancestors and by our brethren in Islam who have followed them unwaveringly.\nGrowingly, new methods of using usury are knocking on our doors and we have got to acquire the knowledge to grasp the roots of those essential values dug deep by Islamic law. We not only need to aspire to follow them unwaveringly, but we also need to understand that applying this “Islamic code of ethics” goes much deeper – we must begin to understand the “why’s” of what is unlawful and comprehend the result of Satan’s whispering that this sin is only transitory.\nThe Islamic principles in transactions refer to those principles and rules that have been derived from the sources of the Islamic Law, or Shari‘ah, and which pertain to transactions in general and include such things as commercial transactions, economic transactions and financial transactions.\nThese principles have, as a reference, the decisions and fatawa (legal verdicts) issued by the jurisprudence academies relating to contemporary issues. Individual opinions are not to be considered if they contradict those of the Muslim scholars or those of the jurisprudence academies. Understanding and observing the Islamic principles in transactions has a host of benefits which include the following:\n- Seeking Allah’s pleasure, guidance and mercy and avoiding contradicting His Laws.\n- Increasing one’s wealth.\n- Avoiding falling into prohibited acts and thereby eschewing sins and evil deeds.\n- Avoiding suspicion among Muslims and establishing justice amongst all people in transactions.\n- Stressing the comprehensiveness of Islam—which is not only a set of religious duties, but is also a way of life.\n- Calling others to the fold of Islam with knowledge and translating words into reality.\n- Setting an ideal example for the Muslim businessman in the arena of life.\n- Enabling the implementation of Allah’s Law with a view to saving humanity from misery and suffering.\nIslamic Principles in Transactions\n❋ Ensuring that one has a sincere intention, namely that the object of transactions is to earn a living through lawful means in order to satisfy one’s basic needs and thus to enable oneself better to worship Allah and to shun the worship of wealth—which may otherwise have detrimental effects.\n❋ Observing high morals in transactions, for doing so testifies to one’s true worship of, and obedience to, Allah and this will certainly help increase one’s earnings.\n❋ Choosing only lawful projects, even if the profits may be minimal.\n❋ Choosing good partners and laborers on the basis of good moral conduct, as this is bound to lead to success and help gain more profits.\n❋ Giving the laborers their dues “before their sweat dries up” to motivate them and to prompt them to work vigorously.\n❋ Discharging one’s financial obligations to Allah, such as giving Zakah and charity in order to purify one’s wealth and cause it to grow.\n❋ Discharging one’s duty towards society by paying the required tax, which is a form of social responsibility.\n❋ Giving priority to fellow Muslims in one’s dealings over others, as the believers are protecting friends one of another, in the words of the Quran.\n❋ Avoiding dealing with those who openly oppose our religion, as doing so may lead to trials and tribulations.\n❋ Solving disputes by reaching amicable settlement to be supervised by righteous people.\n❋ Retaining friendliness and love for Allah’s sake in case of the dissolution of a business partnership.\n❋ Turning to Allah in repentance and seeking His forgiveness for the transactions that may have been conducted contrary to the Islamic dictates.\nExamples of Common Legal Infractions that Must be Avoided\n❋ Engaging in unlawful transactions in search of big profits and blindly following non-Muslims in this respect.\n❋ Engaging in transactions that involve the exchange of interest to fund projects.\n❋ Failing to document agreements between partners—which can lead to suspicion and disputes.\n❋ Failing to document debts and having others to witness them—which is also bound to lead to suspicion and disputes.\n❋ Making obscure agreements—which also leads to bitter disputes and the dissolution of business partnerships.\n❋ Being shy to count cash upon giving or receiving it—thus causing conflicts.\n❋ Using one’s partners’ shyness to one’s advantage—and thus not giving them their dues. Or, taking away their rights unlawfully—which is strictly forbidden.\n❋ Delaying giving any person their due despite one’s financial ability to do so, usually using a person’s goodwill to one’s advantage.\n❋ Encroaching upon other people’s property or upon public property; in fact, this is a blatant form of injustice and corruption.\n❋ Entrusting property to incompetent and irresponsible people—who may then squander it or destroy it and thus cause conflicts and suspicion.\n❋ Choosing partners on the basis of social status and authority and not on the basis of righteousness and high morals.\n❋ Employing people on the basis of emotional feelings and neglecting the importance of moral values and technical expertise.\n❋ Setting unjust conditions and exploiting the needs of the employees and other people—which can lead to monopoly.\n❋ Failing to discharge one’s financial obligations, such as paying Zakah, charity and tax.\n❋ Changing the original purpose of the agreement in the event of disputes and interpreting the agreement texts to one’s advantage.\n❋ Reliance on independent or weak legal verdicts that contradict the opinion of the majority of Muslim scholars in order to gain worldly profits.\n❋ Belittling the commission of minor as well as major sins on the plea that Allah is “Most Forgiving, Most Merciful.”\n❋ Imitating sinners in their dealings.\n❋ Allowing wealth and authority to rule one’s life and neglecting moral and spiritual education.\nObserving Islamic Principles in Transactions\nIslamic principles can be achieved by following these few simple guidelines:\n❋ Develop a good understanding of the rules of transactions by educating yourself in the subject and then abide by them under all circumstances.\n❋ Set up a reference library to help provide continual guidance on the rules of transactions which can be used as the need arises.\n❋ Consult competent authorities whenever a new question is raised for the first time and which requires an Islamic ruling or guidance.\n❋ Refer transactions and contracts that have been concluded to competent jurists and knowledgeable Muslim scholars to be on the safe side, whenever possible.\n❋ Provide yourself with a continuous purification of your earnings through charity, seeking Allah’s forgiveness and repentance.\nCall to Conscience\nWe must become responsible for our own transactions, large and small, and prepare to incur the consequences of the decisions we make in this life and in the Hereafter—understanding that all what we do will not go hidden and forgotten in the end. By educating ourselves and our community, we will begin to develop a society of Muslims who will not only be fair dealers to fellow Muslims, but will also become prime examples to all who deal with our Ummah.\nA good business man or woman, who is known as fair, honest and trustworthy, is someone that provides one of the best forms of dawah there is… Never forget, our beloved Prophet, was a renowned merchant known for his ethical dealings with all people. We must endeavor to emulate his excellent example by upholding the sanctity of Islamic business ethics – because business principles will ultimately be our professional legacy.\nToday is the day to make the choice and decide what will be your own legacy: Will you be:\n- A follower of money who is morally bankrupt?\n- Or, will you become an astute leader of the business community that leaves behind a commendable legacy?"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:76ec9544-cf35-4807-88d8-f91ca1551160>"],"error":null}
{"question":"What are the proper feeding guidelines for orphaned kittens, and what household items should be kept away from them to ensure their safety?","answer":"For feeding orphaned kittens, they should be given kitten milk replacement formula (like KMR or Just Born) using a sterilized bottle or syringe, never cow's milk or human baby formula. Feeding should occur every 2 hours for 5-10 minutes, never exceeding 4-hour gaps. At 5 weeks, kittens weighing 454g need 128cc of formula daily across three feeds. By week 7, they can transition to kitten food. Regarding safety, kittens must be kept away from several household hazards: lilies (which can cause fatal kidney failure), small items like batteries, hairpins, and rubber bands that pose choking risks, Easter grass and tinsel that can damage intestines, and toxic substances like cleaning supplies, antifreeze, and pesticides. Additionally, human foods like chocolate, candy, grapes, raisins, and macadamia nuts should be kept out of reach.","context":["Kittens that weigh around 454 grams and are five weeks old need 128cc of the emergency kitten formula. They should be fed three times per day (approximately 42cc per feed). At 7 th week the kitten is now able to feed on its own. It should thus be introduced to kitten food. Bottle feeding is the standard method for feeding orphaned kittens, but if you're having difficulty feeding a kitten under 2 weeks old, you may want to consider switching from a bottle to a syringe. Before you get started, learn about the benefits and risks of syringe feeding! A syringe can be greatly beneficial for kittens 0-2 weeks of age.\nTo feed a baby kitten without a mother, feed it a kitten milk replacement formula using a sterilized feeding bottle. For a particularly small kitten, you may want to use a syringe or dropper to feed it instead. You should feed the kitten for 5-10 minutes every 2 hours, and never go longer than 4 hours in between feedings.\nWhat to feed baby kittens in an emergency. But kittens have a higher requirement for protein, amino acids, and minerals, as well as for some vitamins. For example, kittens should get about 30% of their energy from protein. For these reasons, most experts recommend you feed your kitten specially formulated kitten food until age 1. Although some cat foods are labeled as appropriate for. What to feed newborn kittens without a mother in an emergency abandoned give kitten orphaned for constipation milk bottle my baby a formula by hand do food eat how and kind of 4-week old stray use video with syringe your. Human Formula for Emergency Use. In an emergency, you may be able to use powdered or concentrated human baby formula to sustain an orphaned kitten until you can get some regular feline milk replacer. However, you need to ignore the directions on the package. Instead, make up the formula at twice the strength indicated for human babies.\nIf we do find someone to bottle-feed, you might still be responsible for taking the kittens back when they no longer require bottle-feeding. You also might be responsible for paying for veterinary visits, which might include emergency medical care, and will definitely include spay or neuter surgery, disease testing, and vaccinations. Feed very slowly so the fluid does not go down the wrong tube and choke your kitten. After two hours, if your kitten has warmed up, you can begin to feed him formula. Never feed a kitten cow’s milk or human baby formula as this causes stomach upset and severe diarrhea. He can have a commercially available formula such as KMR or Just Born, but. What Do Kittens Eat Besides Milk? When the orphaned kittens are three to four weeks old, begin to offer milk replacer in a shallow bowl, then introduce a moist, easily chewable diet. You can make gruel from warmed milk replacer and a high-quality dry or canned kitten food. Serve it in a shallow bowl and feed the kittens several times each day.\nHave Emergency Vet Clinic number handy.(Ask if they have experience with orphaned kittens) Feeding Instructions KMR (Kitten Milk Replacer) or Just Born are the best formulas to feed a neonatal kitten. Do not give a kitten cow's milk, except in an emergency. If you cannot obtain KMR immediately, use the following emergency recipe for up to 24. A new litter of kittens is often an exciting event, but if something goes wrong, and you need to care for the kittens, you may suddenly need kitten formula. Homemade milk replacer recipes often feature ingredients you probably already have in your kitchen and can help you to get by in an emergency. How Do You Feed a Newborn Kitten? If you need to bottle feed a kitten, you'll need to use special baby bottles.Tiny baby bottles with tiny nipples for kittens can be purchased online or in pet stores. These bottles typically hold small amounts of formula so they are easy to handle while also holding a small kitten.\nHomemade Formula for Emergency Feeding. 6 Easy-to-Prepare Homemade Formula for Kittens.. How to Feed Kittens. Rescued or orphaned kittens have to survive without a mother, so it's up to us to lend a helping hand.. Human infant formula milk is formulated for a human baby. Kittens are entirely different from humans. Ways to feed the baby kitten Ideally, kittens ought to reside close to as well as a nurse through. The way you can their mother’s cat with regard to around eight days prior to becoming separated as well as or even used. In how to bottle feed newborn baby. Suppose you find orphaned kittens after all the pet stores in your area have closed. You can mix up an emergency formula to feed the little ones for a short period. Such formulas, usually made with ingredients you may have in your home, won't upset your fuzzy friends' tummies when you feed them, overcoming the risk of problems like diarrhea.\nIt is not recommended to use a microwave. Once it passes the skin temperature test, you are ready to feed kittens. Homemade Formula #2 (for emergencies) 8 ounces homogenized whole milk 2 egg yolks 1 teaspoon salad oil 1 drop liquid pediatric vitamins (optional) Mix well and warm before using. Keep refrigerated. Emergency Formula #3 (for. Many people feed their new kitten by simply filling a bowl with dry food and leaving the food available the entire day. However, establishing a feeding schedule for your kitten is a good idea. A feeding schedule for your kitten allows you to control the kitten’s diet more easily and make sure your kitten does not overeat and gain too much weight. 6. Feed the kittens. If it’s your first time bottle feeding, don’t panic! You can do it, but you’ll want to know some tricks so you don’t hurt them. Watch my YouTube video on How to Bottle Feed a Kitten for tips on proper preparation and feeding posture. And please, never feed cow’s milk to a kitten, as this is extremely dangerous to.\nFeed every two hours around the clock. If you don't have kitten bottles, use a clean dropper. First thing in the morning, run to Walmart or similar store, by kitten bottles and formula, Then call several vets and rescue groups to see if they have a nursing mother cat who can foster this kitten until old enough to be weaned. Squeeze the syringe very gently, but not while it is in their mouth. Only push out enough so that it seeps out the syringe nozzle, not enough so it squirts out. Place it near the kittens mouth so that it can sense it. Once the rabbit baby starts to feed on the milk, you can squeeze a little more, but not while it is actually in the rabbit's mouth. Consult with local veterinarians and shelters to find out if there is a nursing mother cat who may be able to take on the kitten. Mother's milk is best for any baby mammal, and prior to attempting to bottle feed a kitten with supplemental formulas, it is recommended to seek out nursing cat that could take the place of the absent or unable mother.\nAt the first few feedings, the kittens will probably only consume a few cc’s worth of milk. (There are 5 cc's in a teaspoon.) You will need to feed every couple of hours at first and gradually build up time between feedings as they begin to eat more at each meal. Start by offering a small amount.\n“Happiness is a warm puppy.” – Charles Shultz","11 of the Most Common Household Health Risks For Pets\nCould the biggest danger to your fur baby be in your own home? Here are 11 health risks for pets you can eliminate right now.\nFlowers and plants that pose a health risk to pets\nWhile they may be pretty, lilies are one of the most poisonous plants for cats. Petside suggests keeping them out of the house (or better yet, purchase artificial flowers). Be aware of symptoms of lily poisoning which include vomiting, lethargy, and loss of appetite. Call your vet as soon as possible if you think your pet has ingested lily. The American Society for the Prevention of Cruelty to Animals (ASPCA) says that without immediate care, cats who eat lily may develop life-threatening kidney failure within 36 to 72 hours of ingestion. (By the way, here are 50 secrets your pet wishes they could tell you.)\nHoliday poinsettias are also dangerous for pets, though not as worrisome as the lily. This doesn’t mean your pet should eat this pretty red Christmas decoration, since doing so will likely lead to stomach pain and discomfort, including vomiting.\nThe ASPCA’s compiled a searchable plant database of dangerous plants (listing over 400 items). Check it out if you are considering bringing a new plant home.\nPsst—this is why cats are afraid of cucumbers.\nFoods that pose a health risk to pets\nChocolate might have plenty of health benefits for humans, but it’s a harmful food for pets. Petside says most adults know this, but that it’s adults’ responsibility to make sure children know, too. Keep little ones from giving chocolate to pets and do your best to supervise.\nAll kinds of candy—including candy wrappers\nToo much sugar can give your pet a bellyache, but worse, if wrappers are swallowed, your pet risks tearing of the esophagus or intestines. Clean up as best and frequently as you can when candy is being unwrapped.\nMore harmful foods\nYour pets should also steer clear of chewing gum, grapes, raisins, macadamia nuts, avocados, onions, garlic, salt, raw yeast dough, and fatty foods.\nNext, learn to read the signs your dog is mad at you.\nHoliday health risks for pets\nEaster and Christmas decorations\nPlastic eggs, if ingested, can rip tears in the digestive system. Likewise, spoiled hard boiled eggs, if ingested, can make pets ill. Easter grass and tinsel are attractive, but deadly. Pets who attempt to eat these garlands and garnishes can choke, or lethally damage their intestines. At Easter, try real grass or crumpled paper instead. At Christmas, cat-proof your tree by avoiding tinsel.\nOther holiday safety tips for pets:\nNew Year’s: Forego confetti and keep an eye on balloons. If they deflate, they become a choking hazard.\nValentine’s Day: Keep their paws off the chocolates and far from the flowers.\nThanksgiving: Throw turkey bones in the trash.\nHalloween: Use flameless candles, and keep candy out of harm’s way. (Speaking of Halloween, you won’t want to miss these adorable Halloween dog costume ideas.)\nChristmas: Keep pets out of tree water, and be attentive when they show interest in ornaments, decoration hooks and ribbon. Here are more holiday safety mistakes you didn’t realize you were making.\nToys can pose a health risk to pets\nSmall, brightly coloured toys hold the same appeal for pets as they do children. The problem is that they are choking hazards. Petside’s advice is to keep small toys in a place safely hidden from pets.\nLearn how to spot the signs of cancer in cats.\nDrinks that are health risks for pets\nCoffee, tea, and alcohol\nCoffee and tea leaves are on the ASPCA’s list of People Foods to Avoid Feeding Your Pet, as is alcohol. Alcoholic beverages can cause vomiting, diarrhea, decreased coordination, central nervous system depression, and breathing difficulty, among other things.\nCould you be stressing out your pooch without realizing it? Check out these surprising reasons your dog is anxious.\nBatteries (and other small items) can be health risks for pets\nMany small items can lead to choking—even things you would never expect your pet would attempt eating. Be mindful of buttons, small batteries, twist ties, and rubber bands. In the bathroom, keep hairpins, cotton swabs, and dental floss out of reach from your pet. Cut down on clutter throughout your home with these organization tips from Marie Kondo.\nHealth risks for pets in the garage\nIf your pet is your shadow and frequently follows you around the house, remember that garage and storage areas need special attention, too. Keep cleaning supplies, antifreeze, fertilizer, de-icing materials and pesticides in a place pets can’t easily access. “Products containing metaldehyde, such as some slug pellets and firelighters, are extremely toxic, and should be kept away from pets,” according to Blue Cross. “Antifreeze and de-icer fluids taste sweet, but are also poisonous.” (As if having a furry companion wasn’t enough of a reason to grin, did you know that your dog actually loves when you smile?)\nBones pose a health risk for pets\nWhile eating meat off the bone might be tastier, if your pet gets a hold of one of those bones it could be bad news. Just like hazardous objects that might be laying around the house, it’s especially important to keep an eye on where your food leftovers end up. “Cooked bones splinter and can cut your dog’s mouth,” says Dana Humphrey, A.K.A. The Pet Lady. “If swallowed, they can puncture their stomach or esophagus too.” The same goes for bone “toys” you find in pet stores—which is why you should never, ever buy one.\nLearn how to read your dog’s facial expressions.\nSticks can be a health risk for pets\nYour dog might love to play fetch, but you might want to think twice before you pick up that stick outside. Sticks, especially small ones, can pose as serious choking hazards. Instead, Blue Cross suggests throwing a plastic, indestructible object that’s too big for your pet to accidentally swallow.\nPsst—this is why dogs spin around before they poop.\nIt’s easy to forget that trash cans can be health risks for pets, too. Your garbage can might have bones, chocolate, coffee grounds—essentially, a checklist of dangerous items that your fur baby should be nowhere near. “Make sure your garbage pail comes with a secure lid so you don’t have to worry about Fido or Fluffy getting their paws on discarded rib bones or leftover chocolate cake,” says The Pet Lady, Dana Humphrey.\nThink you’re raising a four-legged genius? Here’s how to tell if your dog is smart.\nMedication can pose a serious health risk for pets\nJust like humans, if you take medication that isn’t meant for you, it’s probably not a good idea. Human medication isn’t meant for your pets, and might even cause more harm than good. “Painkillers such as ibuprofen and paracetamol are particularly dangerous,” says Blue Cross. “Vitamin and mineral supplements can also be dangerous, particularly iron tablets and products containing zinc.” The same concept applies to different animals: never give your dog cat medication, and vice versa.\nBe sure to check out the ASPCA site for tips on keeping your pet safe and poison-proofing your home.\nNext, find out the secrets your dog’s tail is trying to tell you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:16b002c3-6b8a-4a5a-ae32-c20a52491d81>","<urn:uuid:1082d9f9-b7d7-4f14-ac3d-56dd4f59394c>"],"error":null}
{"question":"What are key health checkups for a new puppy and their timing requirements?","answer":"A new puppy requires a well-puppy check with a veterinarian within 72 hours of bringing them home. For vaccinations, they need their first round at 6-8 weeks old, followed by booster shots at 10-12 weeks of age. The puppy should not be brought home before 9 weeks old, and preferably at 12 weeks old if getting from a breeder, as this ensures better socialization. Yearly booster vaccinations will be needed after the initial series to maintain immunity from diseases.","context":["You have decided it is time to add a new puppy to your family. After you have done your homework and found the perfect puppy the big day comes for you to bring home your new furry friend. Are you prepared? There is a lot to think about when you add a new puppy to the home.\nGeneral tips to be prepared for your new puppy:\n- If buying from a breeder, don’t bring home the puppy before it is 9 weeks old and preferably 12 weeks old. The longer the puppy stays with its siblings and mother the better socialized it will be. A good breeder will not let you bring home the puppy before 9 weeks.\n- Whether you adopt from the shelter/rescue group or buy a puppy from a breeder, make an appointment with your veterinarian to have a well puppy check. This should be done within 72 hours of getting the puppy.\n- Have a crate for your puppy as it does several things:\n- Keeps your puppy safe when you can not watch him\n- Helps with the potty training\n- Prevents chewing and other destructive behaviors\n- Gives the puppy a safe place to rest (your pup should view it as his den)\n- When using a crate there are several things to keep in mind:\n- A good rule when crate training is the age plus one, so a two month old puppy can stay 3 hours in the crate, three month old puppy 4 hours, four month old puppy 5 hours etc.\n- If you work away from home having a professional pet sitter come in mid-day to give the puppy a potty break, exercise and attention is essential.\n- The crate should be big enough for the pup to stand in and be able to turn around and lie down. It should not be bigger than that. You might have to buy a new crate as your puppy grows depending upon the size.\n- Don’t use the crate for punishment. It should be a good place, a safe place.\n- Having a schedule for your puppy is imperative. Every time the puppy wakes up from a nap it should be taken out to potty. The puppy should be given lots of play time and attention.\n- Puppy proof your house and yard.\n- Diet is important for puppies. You might want to switch from what the breeder feeds but the process should be done slowly. Your puppy is already under stress being in a new home, new routine etc. and its body needs to adjust to the new food. If you are going from a kibble to raw diet you want to prepare your puppy’s digestive system for the switch.\n- Your new puppy needs to learn how to wear a collar and walk on a leash. Start early with the collar and introduce the leash as they get older.\n- Plenty of toys are needed to occupy the puppy and aids in the development of your puppy. Toys should be durable and the puppy will need to have chew toys they can not destroy or eat.\n- Remember bringing a puppy into your home requires time from you for training and socializing. Be sure your life is at the stage where you have the time to give your new addition.\n- Don’t plan vacations until the puppy is at least 6 months old or older.\n- Realize puppies will cost money for veterinarians, pet sitters, toys, food etc.\nEnjoy the puppy stage as a puppy will grow into an adult dog fast.\nHowever, they never outgrow the need for you!","Our furry friends bring incredible joy to our lives, and, in return, we must ensure that they remain happy and healthy. One of the crucial steps in guaranteeing their well-being is to vaccinate our pets regularly. Are you curious about how often you should vaccinate your pet and what vaccines they need?\nIn this comprehensive guide, we delve into various aspects of pet vaccinations to strengthen your understanding and help you follow the best practices for safeguarding your pet’s health.\nUnderstanding Vaccination Schedules for Pets\nVeterinary professionals carefully design vaccination schedules for pets to ensure optimal protection against various diseases and illnesses. Following prescribed schedules is of the utmost importance since it strengthens your pet’s immune system and preserves their overall health. The immunization guidelines differ for puppies, kittens, adult dogs, and adult cats, considering their specific needs and vulnerabilities.\nVaccination Schedules for Dogs\nDog vaccinations are a crucial aspect of maintaining your canine companion’s health. Puppies typically receive their first round of vaccinations at 6 to 8 weeks old, followed by booster shots at 10 to 12 weeks of age. Afterward, yearly booster vaccinations help ensure continued immunity from potentially life-threatening diseases. In addition to core vaccines, special situations such as boarding, dog shows, and exposure to other animals might require additional preventative measures.\nVaccination Schedules for Cats\nSimilar to dogs, kitten vaccinations start at 6 to 8 weeks of age, with boosters following at 10 to 12 weeks. Adult cats receive annual booster vaccinations to keep their immunities fully functional. Vaccination needs for outdoor cats or those participating in cat shows may differ from those strictly kept indoors.\nCommon Types of Pet Vaccines\nPet vaccinations broadly fall into two categories: core and non-core vaccines. Core vaccines are essential for every pet, while non-core vaccines depend on your pet’s lifestyle, risk factors, and the prevalent issues in your geographical region. Your local veterinarian can help you decide which vaccines your pet will benefit from the most.\nCore vaccines protect your pet from life-threatening diseases that are common and easily transmissible. Some examples include rabies, parvovirus, and feline leukemia vaccines.\nNon-core vaccines might not be necessary for every pet but can provide added protection based on their needs. Examples include canine influenza, Lyme disease, and feline immunodeficiency virus vaccines.\nTiter Testing for Pets\nIf you are uncertain about administering booster shots for your pet or feel that they may not need them, consider turning to titer testing. This process gauges your pet’s immunity levels, enabling you to make an informed decision about whether or not they require additional vaccinations. However, this method does not apply to rabies vaccines, which should be administered as your veterinarian prescribes.\nPotential Side Effects of Pet Vaccinations\nLike humans, pets can also experience some short-term side effects from vaccines. Some common symptoms include difficulty breathing, small red bumps or itchiness on the skin, persistent vomiting or diarrhea, collapse or fainting, and severe coughing. If you notice any concerning side effects after your pet’s vaccination, consult your veterinarian immediately to determine if further action is necessary.\nJust as immunizations are vital to your pet’s well-being, so is their dental health. Veterinary dentistry plays an essential role in ensuring your pet maintains their overall health by examining your pet’s mouth, diagnosing any issues, and making relevant treatment recommendations. Dental health contributes significantly to a pet’s quality of life and should not be overlooked. To learn more about vet dentistry, click here and discover how you can better support your pet’s oral health.\nCosts Associated with Pet Vaccinations\nThe cost of vaccinations varies depending on factors like vaccine type, your pet’s age, and your geographical location. Generally, the costs of vaccines can range from $25 to $100 per shot or more. It’s important to consult your local veterinarian for accurate pricing and to plan your pet’s vaccinations accordingly.\nUnderstanding the importance of pet vaccinations and adhering to proper schedules is essential to being a responsible pet owner. Be sure to consult your veterinarian for guidance on which vaccines your furry friend needs, and keep up to date on their immunizations to provide them with the best possible care. Together, we can ensure that our beloved companions enjoy a long, healthy, and happy life by our side."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:2b06100a-ca0f-44f7-accc-7e35ae4d3624>","<urn:uuid:04ceb87f-3418-4688-bb8f-43dc92960aca>"],"error":null}
{"question":"I want to upgrade my PC cooling. How do closed-loop water cooling systems compare to custom water cooling setups?","answer":"Closed-loop systems like the Corsair Cooling Hydro Series H50 offer near-custom cooling performance with less hassle and cost. They consist of a CPU water block connected to a small one-fan radiator, with the pump and reservoir built into the radiator. While they don't provide quite the same level of CPU cooling as custom setups, they're easier to install, take up less case space, and don't require fluid replacement. Installation simply involves attaching the radiator and fan to an exhaust grate and mounting the water block. They cost more than air coolers but significantly less than building a custom water-cooling system.","context":["In the pantheon of nerd achievement, water cooling ranks near the top—somewhere between installing Linux and becoming fluent in Klingon. And there’s a reason the hardest of the hardcore prefer water cooling: It’s incredibly effective at lowering the temperatures of core system components. With higher thermal conductivity and specific heat capacity than air coolers, water cooling can mean double-digit drops in CPU and GPU temperatures.\nHowever, water cooling isn’t exactly a walk in the park. You’ve got two challenges ahead of yourself: Designing the water-cooling system that’s right for your PC, and actually putting it together. Both tasks will take some time and effort, but neither has to be daunting. Every first-time water-cooling build is a learn-as-you go experience, but we’ll walk you through the details and help you avoid the mistakes that would take the biggest toll on your system and your wallet.\nThe advantage of a custom water-cooling system is that it’s just that—custom. By picking out exactly which parts you want, you’re able to create a system that matches your cooling needs and your aesthetic sensibilities. To get you started building your system, we’ll go through every major component of a water-cooling system, describing what each one does, and what your options are.\nEven though there’s no fluid touching your case, it’s one of the most important parts of a good water-cooling setup. For water cooling, you’ll need a case with plenty of room on the inside and a large fan grate, ideally on the top or bottom of the case. Although it can be a little hard on the wallet, getting a case that’s been designed with water cooling in mind will ensure that your install goes as smoothly as possible. In our build, we used the Corsair Obsidian 800D full-tower case.\nA block is the piece of hardware responsible for drawing heat out of your computer hardware (your CPU and GPU, for instance) and into the liquid coolant in a water-cooling system. A block of heat-conducting metal makes contact with your CPU or GPU (aided by thermal paste) on one side, while water is forced across the other, literally flushing away excess heat.\nYou need a separate block for each component you want to cool. The obvious component to water cool is your CPU, which will see some of the greatest benefit in the form of increased overclocking potential. The GPU on your videocard is another good candidate for water cooling, as is your chipset. For this build we’ve chosen to focus on CPU and GPU cooling.\nAs for actually picking which water block to use, it’s generally a matter of brand and the right block for your part. For instance, if you’re using a socket 1156 CPU, a quick Internet search for “socket 1156 water block” will turn up a handful of compatible water blocks, as well as some performance comparisons. We’ve chosen CPU and GPU blocks made by DangerDen ( www.dangerden.com ).\nIn a water-cooling setup, the radiator is the water block’s complement, releasing heat absorbed from the block into the air. It accomplishes this by forcing the liquid coolant through an array of thin tubes attached to metal fins. Traditional case fans pull air through the capillary-like radiator, absorbing heat from the liquid and forcing it out of the case.\nThere are radiators big enough to support one, two, or three fans. Of course, bigger radiators and more fans amount to better cooling, so we generally recommend going with the biggest radiator that fits your case and your budget.\nThe fanciest water-cooling equipment in the world won’t do a thing unless the water’s moving through it, and that’s accomplished with a pump. There are quite a few pumps on the market, and although it’s on the pricier side, we recommend the Laing DDC 3.25 for its reliability and small formfactor. If you go with a different pump, make sure to read user reviews before you buy—a shoddy pump will wear out or break down over time.\nIn water cooling, a reservoir is a pretty simple thing—it’s a tank of water, with an inlet and an outlet. You might wonder why, exactly, you need a big tank of water in your system, since it doesn’t have an immediate function, like absorbing or dispelling heat. However, the reservoir performs a number of important duties:\nAs for which reservoir to use—well, it’s really just a tank; pick one that fits in your case and looks nice. For this build, we used a double optical-drive bay acrylic reservoir from Danger Den, which comes with a pair of Molex-powered LEDs to light up the front of your case.\nFinally, you need tubing to combine all the other parts. The most common sizes of tubing used are 1/2-inch and 3/8-inch diameter. The demonstrable performance difference between the two sizes of tubing is slim, and 3/8-inch tubing can bend more without kinking, so we used that for our system. Whichever you pick, just make sure that all the rest of your water-cooling hardware has fittings of the same size. Most all hardware is available with either 1/2-inch or 3/8-inch fittings; if you get a size that doesn’t match your tubing, you’re hosed.\nBeyond the diameter of the tubing, you just need to pick a color. Most sites that deal in water cooling sell pretty much the same PVC-based tubing. It works well, it’s fairly cheap, and it’s available in a bunch of UV-reactive colors. Some sites offer slightly more expensive Tygon tubing, which is more flexible and durable. Fittings come in barbed or compression styles. Both will work just fine, though compression fittings look nicer and are a bit more expensive.\nYou’ll also need coolant to put into your system. Although it’s commonly referred to as “water cooling,” most modern cooling systems use some sort of coolant with anti-corrosive and anti-conductive properties. This fluid is available from any distributor of liquid-cooling products, and comes in various UV-reactive colors.\nOnce you’ve picked out the individual components, you’ll need to design the layout of your water-cooling system. For this, it helps to make a simple diagram, showing how you want everything to be hooked up. A simple system has the water passing from the radiator to the CPU, then to the GPU, the reservoir, pump, and finally, back to the radiator. This design works well because then the water passes over the CPU while it’s at its coolest, and also because the CPU and the GPU tend to be physically near each other.\nWe recommend starting with the CPU water block, because it usually installs with a mounting backplate, so you’ll need to install it before you can screw the motherboard to the case, and you don’t want to install any other component until the motherboard is securely in place.\nNow we can move onto the radiator, the largest component. The radiator can be installed over any fan grate that’s large enough, and most simple radiators are designed with screw holes that have the same dimensions as standard case fans. Thus, if your case has a grate big enough for two fans, you can mount a double fan–size radiator onto it. You can mount a radiator inside or outside of a case, but make sure that the fans blow hot air out of the case and away from your PC’s hardware. If you mount the fans outside the case, make sure you’ve got a plan for how you’re going to plug them into your power supply.\nContinuing with the practice of installing largest parts first, it’s time to install the reservoir. There’s a wide variety of reservoirs available to suit your setup. They can be attached inside or outside of the case, to the radiator, or into a drive bay. No matter what style of reservoir you have, take note of the location of the fill port, and have a plan for how to access that port when it comes time to fill your cooling system with liquid.\nFinally, we’ll close off the loop by installing the GPU block, assuming you want one. Some of the highest temperatures in your system can be found on your videocard, so there are definite advantages to water cooling it. At the same time, it’s also one of the riskier aspects of liquid cooling, since you have to remove your videocard’s existing cooler, directly exposing its processing cores.\nThe pump should be really easy to install. Most pumps are fairly small and can be attached almost anywhere in your case, using screws or Velcro tape. Next, complete your loop by connecting the output barb of the pump (usually marked with an arrow pointing away from the pump) to the radiator, and the input barb to the reservoir. When you’re cutting tubing, don’t just go with the shortest amount possible—also consider how the tubing will affect how you access your PC hardware. You don’t want to have to dismantle half your water-cooling system just to swap out a hard drive.\nNow that your water-cooling loop is completed, it’s time to add the coolant. Give your system one last sanity check, making sure that each fitting is tightly connected, and that all components are hooked up in one continuous loop. Once you’re confident that your system won’t leak, and without plugging anything in, start filling up the reservoir to the manufacturer’s recommended level. Keep an eye on the rest of the water-cooling system, and be ready with a towel in case anything springs a leak.\nWith the reservoir filled with coolant, you can now fire up the pump. You don’t want to actually turn on your motherboard yet, so you’ll need to trick the power supply into powering the pump. Generally, this is done by shorting the green wire on the power supply’s ATX connector to one of the black wires, although it’s wise to consult the manual for your specific power supply.\nAs long as nothing is leaking, let the pump run for 10 minutes or so to let air bubbles escape. Slowly rock your case back and forth, to let any air that’s trapped in the water blocks or radiator escape. Once all the air bubbles are out of the liquid (you should be able to see them in the tubes if there are any left), you may need to add more liquid to the reservoir to reach the recommended level.\nAs you can see, although water cooling provides exceptional cooling power, it isn’t the easiest—or cheapest—way to cool your PC. If you just want to cool your CPU more effectively and quietly than a performance air cooler but without the hassle of building a custom water-cooling rig, there’s an alternative: a prebuilt, closed-loop system like the\nCorsair Cooling Hydro Series H50\nor the CoolIT ECO.\nThese systems achieve cooling performance near that of a custom water-cooling rig, but save you the hassle of building one yourself, or ever having to replace the fluid. They cost more than an air cooler, but significantly less than building your own water-cooling system. And, unlike full-blown water coolers, they’re easy to install and don’t take up much room in your case.\nInstallation for this type of cooler is simple. It’s a backplate-mounted water block for the CPU, connected to a small, one-fan radiator. You just attach the radiator and fan to an exhaust grate on your case, and that’s it; the pump and reservoir are built into the radiator.\nYou won’t get quite the same CPU cooling as in an all-out water-cooling system—or the nerd cred that comes from a tower full of tubes—but closed-loop coolers are definitely an excellent alternative for enthusiasts who want some extra cooling performance without a lot of hassle.\nWhether you overclock, or just want to make sure your processor lasts as long as possible, it’s important to keep an eye on your system’s temperatures. These two free programs help you do just that.\nSpeedFan uses the built-in temperature-monitoring hardware in your chips to display temperatures for all of your individual components, and it allows you to control fan speeds in your case automatically, based on temperature readings. SpeedFan also monitors S.M.A.R.T. readings and analysis, so you can make sure your hard drives are healthy. ( www.almico.com/speedfan.php )\nFrom CPUID, the makers of CPUZ, HWMonitor keeps track of all the temperatures and voltages in your system. It doesn’t have the advanced S.M.A.R.T. features or fan-speed controls of SpeedFan, but its temperature-reporting functionality is top-notch. ( www.cpuid.com/hwmonitor.php )"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:fe4ad2d9-bdb7-411e-840f-7f51b9b7096e>"],"error":null}
{"question":"What installation tools are needed to fix a loose brick versus setting up a basic water heater timer?","answer":"For fixing a loose brick, you need safety goggles, cold chisel, wide brick chisel, wire brush, garden hose, mixing bucket, stir stick, sharp trowel, mortar jointer or thin metal rod, and stiff scrub brush. In contrast, installing a basic water heater timer typically only requires a couple of screwdrivers, particularly for models that plug directly between the water heater and wall outlet.","context":["A loose or broken brick in a wall or chimney not only looks bad, and it can provide an opening (literally!) for further damage. Act quickly to prevent moisture from entering the wall or chimney. And don't worry -- the repair is an easy one.\n- safety goggles\n- cold chisel\n- wide brick chisel\n- wire brush\n- garden hose\n- mixing bucket and stir stick\n- sharp trowel\n- mortar jointer or thin metal rod\n- stiff scrub brush\n- mortar mix\n- mortar coloring\n- corrugated cardboard scrap\n- replacement brick\nTime: 1 to 2 hoursWearing safety goggles, begin by using a cold chisel and sledgehammer to remove the mortar around the loose brick. Work carefully to avoid damaging the loose brick or surrounding bricks. Lift the loosened brick out from the wall or chimney and set it in a bucket of water to soak.If a loose brick cannot be easily removed, or if the brick is broken, break it up to remove it. Wearing safety goggles, chop out the damaged brick with a wide brick chisel and sledgehammer. But still be careful not to damage the surrounding bricks. Fill a bucket with water and set the replacement brick in it to soak.Still wearing safety goggles, remove all remaining mortar from the hole where the brick was removed; use the sledgehammer and cold chisel to remove large chunks of old mortar and then wire-brush the cavity to remove any debris still adhering to the bricks. Flush the cavity thoroughly with the garden hose.Mix a small batch of mortar according to the directions on the package. To see what color the mortar will be when it dries, spread a little mortar on a scrap piece of corrugated cardboard; as the cardboard absorbs water from the mortar and it dries, the mortar's color will lighten. Add mortar coloring as necessary, experimenting with mortar applied to the cardboard, until the new mortar matches the old. Mix enough mortar to secure the brick, and add coloring in the proportion used in the test batch.Before replacing the brick in the wall, spray the cavity again with the garden hose to dampen it; the cavity should be wet but not streaming. Spread a thick bed of mortar on the bottom surface of the cavity, smoothing it roughly level.\nRemove the replacement brick -- salvaged or new -- from the bucket of water and shake it to remove excess water. Apply mortar generously to the top and ends of the brick, but don't mortar the back. Set the brick carefully into place in the prepared hole, pressing it in firmly. It should align with the bricks on each side of it; adjust it to match, and apply more mortar as necessary. Make sure the face of the new brick is flush with the surface of the wall.\nWhen the new brick is firmly in place, force mortar into the top and side joints of the brick to fill them completely. Smooth the mortar all around the new brick, making sure there are no gaps. Scrape excess mortar from the wall with the side of the trowel. Then, using the trowel, a brick jointer, or a thin metal rod bent to form a handle, tool the new mortar joints to match the joints in the rest of the wall.\nTo keep the new mortar from drying too quickly, spray it lightly with the garden hose several times a day for 2 or 3 days. When the mortar has set completely, use a stiff scrub brush to remove any excess mortar from the face of the wall.\nIf you can handle repairs with mortar, concrete is the natural next step. On the next page, learn how to repair crumbling concrete stairs.\nFor further tips and instructions on how to fix things around the house:\n- Home-Repair Safety Tips: Doing the job yourself doesn't save you money if you end up in the emergency room. Read these tips to make sure you work smart when doing home repairs.\n- Home-Repair Materials Basics: Stock up on these frequently used items and you can be Mr. or Ms. Fix-It without six trips to the hardware store.\n- Home-Repair Tool Basics: Does your tool box have what it takes? Learn what tools you're likely to be looking for when making repairs around the house.","When it comes to making the home more energy efficient, your water heater should be a bigger concern than most people give it. According to energy.gov, your water heater accounts for as much as 12 percent of your home’s energy usage. Unfortunately, more time is spent encouraging people to buy new water heaters, install solar heaters, etc., and not much time is spent explaining the virtues of installing a water heater timer.\nTypes of Hot Water Heater Timers\nWater heaters can be controlled using a wide variety of devices, ranging from a simple On/Off switch to controllers that can be accessed from anywhere using your smartphone. The type of timer used is often determined by the application, such as residential or industrial, but there are no hard and fast rules regarding which types must be used in which settings. The most common types of water heater timers include:\nWhile a simple On/Off switch is not technically a timer, it is an efficient, manual method of reducing energy costs related to your water heater. Since it takes a while for water to heat, this method requires more planning than any other, making it quite a bit less efficient.\nThis type of timer has to be activated similar to the switch method, but has the advantage of automatically turning itself off after your desired length of usage. This method is more efficient than a switch, but still requires forethought and manual execution.\nFor household use, a programmable timer is probably the most efficient method of all. This type of timer allows you to create various presets– the number is determined by the device installed– that control power to your water heater. For example, you could set one timer for weekdays when no one is home and another for the weekend when the whole family is using hot water.\nA box timer is the most common and in our opinion, the best water heater timer style. These digital and electronic water heater timers have the advantage of allowing you to set different usage periods based on the hour and day of the week or month. Intermatic water heater timers are far and away the most well-known and trusted brand out there. For situations where the amount of hot water needed varies according to the day, a box timer is highly recommended in most situations.\nWiFi and Z-Wave Timers\nSmart homes rely on smart technology, and there are devices available to turn your water heater on or off, or even set a particular timer function, straight from your phone. This type of connected timer is more expensive than other types, but offers you total control over the system, no matter where you go.\nTimers for Gas Water Heaters\nEven if your water heater uses some type of gas as the primary fuel source, it probably relies on electricity to determine when to turn on or off. To this end, there are various types of timers available, up to and including WiFi and Z-Wave timers.\nBenefits of a Water Heater Timer\nA water heater timer will save you a bit of money under any circumstances, but offer greater savings on tanks that are not inside a heated are of your home, or in areas where utility costs are determined by peak usage rates. You can increase the savings by adding insulation or moving the water heater inside the heated portion of the home, but a timer can still help you maximize the amount of hot water you need versus keeping the unit operating 24/7. If your electric company charges more during evening hours for example, using a timer to turn the heater off during peak energy usage times will reduce the cost per kilowatt of using the water heater.\nInstalling a Water Heater Timer\nInstallation varies based on the type of timer being used. If your water heater uses a standard 110 or 240 volt AC plug, there are timers which can be quickly plugged in between the water heater and the wall outlet and are ready to use. Other types of timers may require disconnecting the wiring where it enters the water heater and connecting those wires to the timer and then connecting the timer to the water heater. Even this type of installation should only take a few minutes and can be accomplished with a couple of screwdrivers in most cases. Be sure to check the way your water heater is wired and buy the appropriate timer.\nReducing the Costs of Water Heater Usage\nThere are other ways to reduce energy costs related to your water heater. By using a combination of methods, you can achieve the most efficiency. Here are some of the most commonly used ways to reduce water heater power consumption:\nInstall a timer – This can cut up to 25% of your annual water costs.\nTake shorter showers – This has the advantage of reducing both hot water heater operation and overall water consumption in the home.\nLower the water heater thermostat – Reducing the temperature of the water you use, as with reducing your bathing time, saves power and reduces water consumption. By using cooler water, there is less need to mix cold water into the hot water flow, and that could cut water consumption by as much as 50% depending on your family size and typical usage. See our recommended temperature guide for more info.\nInsulate your water heater – Whether the unit is inside the home or out, adding a thick layer of insulation helps keep your water hot longer, reducing the time that the water heater has to operate to maintain the desired temperature. Simple yet effective.\nWater heater maintenance – Performing annual maintenance, including draining and flushing your tank or even replacing a heating element on an older electric model, will help your water heater operate more efficiently and prevent costly and unexpected malfunctions.\nReplace the water heater – Unfortunately, water heaters are not designed to last forever. If your water heater is more than 10-15 years old, it may be time to replace the unit with a more efficient model. When shopping for a new water heater, compare energy star ratings, tank sizes, and options such as installing an on-demand water heating system. Choosing the right unit for your home gives you ample hot water and reduces energy usage.\nA water heater timer will not save you as much as timer manufacturers may hint at, but it is a fantastic way to help you manage your energy usage more efficiently. A timer alone provides a significant amount of annual savings, but combining some of the methods above can cut your water heating bill be as much as 50 percent."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:fb0387b8-4599-4328-9a03-34340f286328>","<urn:uuid:b3c1a944-338c-4c43-885b-562613be149d>"],"error":null}
{"question":"What are the professional focus areas of Ann Schneider and Matt DuPee in relation to studying international conflicts?","answer":"Ann Schneider is a Latin America expert who focuses on issues of reckoning with dictatorships and state violence in Latin America, having written her doctoral dissertation on this topic at the University of Chicago before joining ICE to pursue war criminals. Matt DuPee has specialized in studying political and security events in Afghanistan since 1999, serving as managing editor for Afgha.com and conducting research on political, security, and geographical issues pertaining to Afghanistan at the Naval Postgraduate School's Program for Culture and Conflict Studies.","context":["Mark Shaffer’s title is “Special Agent.” He works as a unit chief of a department that tracks down criminals from around the globe, many of whom have evaded capture across the years and across borders. His department recently made the news because of their success in tracing suspected Bosnian war criminals who had escaped to the United States. But despite that action-movie-worthy background, he works with a group of people whose reputation is usually far less tough: historians.\n“History’s always been a very big interest of mine,” Shaffer says. “[Working with historians] has been the most interesting work of my almost 20 years in law enforcement.”\nAfter any conflict or crisis, some war criminals manage to escape to other countries and build new lives — often with fake identities. Too often they pose as refugees, fleeing the horrors of which they were guilty. They have lied on paperwork, and avoided admitting their past involvement in crimes overseas.\nCommitting that kind of fraud during immigration procedures would be grounds for deportation, but it’s not easy to uncover the truth in those situations. Many of the cases are to do with human-rights violations a long time ago, which is where historians come in. In the United States, the Immigration and Customs Enforcement department — where Shaffer heads the Human Rights Violators and War Crimes Unit — works with the FBI and the Department of Justice to build cases against these suspects, and prosecute them in immigration court. Just last week, the verdict was passed on one of their more high-profile targets: a panel declared that Carlos Eugenio Vides Casanova, who was a high-level official in El Salvador during a time of severe human-rights abuses in that country, could be deported.\nHistorians have the background in the culture, language and politics of particular regions of the world; they know where to look for the sources that might prove who was responsible for particular crimes. This kind of research relies on personal testimony, written records, photographs — and today, social media.\nAn important part of their work is creating the narrative, the story that the U.S. Attorney can present in court. This means putting together the threads that link pieces of evidence, building a full picture of a person’s role in human rights abuses. A particular challenge is state-sponsored abuses, where historians must contend with (in some cases) decades of cover-ups and silencing.\nFor perpetrators from some countries, the U.S. immigration trial will be the only trial they face. That decision, in a U.S. court, may be the only legal acknowledgment their victims ever receive. The ICE team knows this, and they want to make very case count. “Our historians are absolutely critical to getting these cases moving forward,” Shaffer told me.\nToday there are three historians building these cases: one who focuses on Africa, one on the Balkans and one on Latin America.\nAt the Balkans desk is Michael MacQueen, who has made news over the last 25 years for his work pursuing war criminals: first Nazis, now those from more recent Balkans conflicts. He joined the Justice Department in 1988, when he left a PhD program at the University of Michigan in Eastern European studies. Fluent in German and Polish, his task was to trace former Nazis who had operated in Eastern Europe. Among his many coups, it was his research that led to concentration-camp guard John Demjanjuk (AKA Ivan the Terrible) being stripped of his U.S. citizenship and sent to Munich to face trial in 2009.\nToday, younger historians are following in his footsteps. Latin America expert Ann Schneider is a prime example of how knowledge of history can help enforce global justice. When Schneider was a doctoral student at the University of Chicago, she wrote a dissertation about issues of reckoning with dictatorships, and legacies of state violence in Latin America. Five years ago, the opportunity came up to join ICE and pursue some of the criminals she had studied. “I can’t turn this down,” she recalls thinking, “to have my work really contribute to judicial processes.”\nSchneider’s interest in human rights and accountability goes back to her childhood. In the fourth grade, she heard of four Catholic missionaries being murdered in El Salvador The nuns who ran her school held annual remembrances, and over the years the lack of justice for this brutal attack was part of what led Schneider to pursue a career examining how we deal with human rights abuses.\nTwo of the men in power when the death squads carried out this and other attacks were José Guillermo García (the minister of defense) and the aforementioned Vides Casanova (a national guard general, later also defense minister). Both had found their way into the United States in the late 1980s, having avoided prosecution.\nVarious groups campaigned against these men, but the challenges of proving their knowledge and culpability were compounded by the fact that these men had been part of a regime that previous U.S. administrations had supported. Finally in 2014, an immigration court in Miami found they were guilty of immigration fraud and human rights abuses, liable for “countless unnamed victims.” Vides Casanova’s appeal was denied on March 11, 2015. Garcia’s case is still pending, but the decision on Vides Casanova suggests his appeal will also be denied.\nVides Casanova and Garcia may never be tried in El Salvador. That country’s Amnesty Law prevents prosecution of crimes committed during their decades-long civil war.\nSince its founding in 2003, ICE has deported more than 650 known or suspected human-rights violators from the United States. And while their mandate was originally to remove offenders, they are now working preemptively as well, helping NGOs to produce lists of suspects and put them in the lookout system to prevent them gaining access to the USA.\nThey continue to gather information from refugee groups in the USA too. A major concern for victims who have fled to safety elsewhere in the world is that they might encounter their attackers in the very same refugee communities in which they live.\nThe unit is working today to prevent this happening. Over the last four years, ICE’s Human Rights Violators and War Crime Center has issued more than 67,000 lookouts for people from more than 111 countries and stopped 140 human rights violators or war-crime suspects from entering the United States.\nTheir work sends a message to victims: you will be heard. And to perpetrators: you cannot simply walk away. It’s a lesson that, appropriately, fits neatly with the historians’ perspective. Just because something is past doesn’t mean it’s over.\n- For People With Disabilities, Losing Abortion Access Can Be a Matter of Life or Death\n- Inside the Clandestine Efforts to Smuggle Starlink Internet Into Iran\n- How to Help the Victims and Community After the Monterey Park Shooting\n- The Biggest Snubs and Surprises of the 2023 Oscar Nominations\n- Talking Less Will Get You More\n- Kamala Harris Subtly Emerges as Powerful White House Asset\n- How Avatar: The Way of Water Became the 6th Movie in History to Make $2 Billion\n- Is There Really No Safe Amount of Drinking?\n- How Our Cells Strategize To Keep Us Alive","Bill Roggio is the Managing Editor of The Long War Journal; the president of Public Multimedia Inc., a nonprofit media organization with a mission to provide original and accurate reporting and analysis of the Long War; a Senior Fellow at the Foundation for the Defense of Democracies; a Hoover Institute Media Fellow; and a contributor to the The Weekly Standard. His coverage includes the wars in Afghanistan, Pakistan, Somalia, Lebanon, and Iraq, as well as al Qaeda's operations, tactics, and strategy.\nBill has embedded with the US Marine Corps, the US Army, the Georgian Army, the Iraqi Army, and the Iraqi police in Iraq in 2005, 2006, 2007, and 2008, and with the Canadian Army in Afghanistan in 2006. His articles have been published in The Washington Times, The New York Post, The National Review, The Toronto Times, and Die Weltwoche. His photographs have been published in the Wall Street Journal and the Washington Post. He also presents regularly at the US Air Force's Contemporary Counterinsurgency Warfare School on the media and embedded reporting. Bill served as a signalman and infantryman in the US Army and the New Jersey National Guard from 1991 to 1997. Bill can be reached at [email protected]\nThomas Joscelyn is the Senior Editor of The Long War Journal. Thomas is a senior fellow at the Foundation for Defense of Democracies (FDD). He is also the executive director of the Center for Law and Counterterrorism at FDD. He is a terrorism analyst, economist, and writer living in New York. Most of Thomas's research and writing has focused on how al Qaeda and its affiliates operate around the world. He is a regular contributor to the Weekly Standard and its online publications, the Daily Standard and Worldwide Standard. His work has also been published by National Review Online, the New York Post, and other media outlets. Thomas is the author of Iran's Proxy War Against America, a short book published by the Claremont Institute that details Iran's decades-long sponsorship of America's terrorist enemies. He makes regular appearances on radio programs around the country and has appeared on MSNBC and FOX News.\nIn 2006 he was named one of the Claremont Institute's Lincoln Fellows. Thomas served as the senior terrorism adviser for Mayor Rudolph Giuliani's 2008 presidential campaign. He holds a Bachelor of Arts degree in Economics from the University of Chicago.\nBill Ardolino is an Associate Editor and Overseas Correspondent for The Long War Journal. His reporting includes embeds with the US Marine Corps, the US Army, the Iraqi Army, and the Iraqi Police in Fallujah, Habbaniyah, and Baghdad in 2006, 2007, and 2008. He later embedded with the Marine Corps, the Afghan Police, the Afghan Army, and the US and Afghan Air Forces in Kabul, Nimroz, Helmand, Khost and Kandahar provinces in 2010, 2011 and 2013.\nBill's reports, columns and photographs have been published in The Washington Examiner, Wired, Small Wars Journal and The Weekly Standard. His reporting has focused on combat operations, the development of indigenous security forces, civil affairs work, and Iraqi politics. He is the author of Fallujah Awakens: Marines, Sheikhs and the Battle Against al Qaeda (Naval Institute Press).\nChristopher Radin is an Associate Editor at The Long War Journal. Chris tracks the development of the Afghanistan National Security Force and publishes a periodic status report at The Long War Journal. Chris has a Bachelor of Science degree and a Master of Science degree in Chemical Engineering from Columbia University and began his career as an engineer at Intel Corporation. He was a product manager in Silicon Valley, California, for 15 years. Chris has a lifelong interest in politics, international affairs, and the military.\nChris’s articles on Afghanistan National Security Forces have been used at the NATO Special Forces Command and the US Army War College as training material. Chris was invited to present a paper on the subject at “The Prospects for Reconciliation in Afghanistan” conference at Tufts University in 2010.\nWes Bruer monitors and reports on terrorists and terror-related activities in the US for The Long War Journal, and also assists with LWJ's communications and news media. He graduated from the University of Georgia's School of Public & International Affairs with a Bachelor of Arts in Political Science and currently works in politics and resides in Georgia. He intends to pursue graduate studies in international affairs with an emphasis on Middle Eastern affairs.\nMatt DuPee has independently studied political and security events happening in Afghanistan since 1999. Mr. DuPee worked as the managing editor for Afgha.com since 2004. Mr. DuPee earned a Bachelor of Fine Arts degree in Film and Video Production from Point Park University and received intelligence analyst training from Neumann College. He is currently enrolled in the Master of Arts program in Regional Security Studies at the Naval Postgraduate School. He now serves as a Research Assistant with the Program for Culture and Conflict Studies and continues his research on political, security, and geographical issues pertaining to Afghanistan. He also serves as an assistant to the web and journal editor for the program.\nMr. DuPee's articles have been published in a variety of publications including the Frontier Pots, the Middle East Times, e-Ariana, The Center for Conflict and Peace Studies, and others. His work was cited in the Military Review's May-June 2008 edition, in an article titled \"The Taliban: An Organizational Analysis.\" He also participated in post-production research for the Canadian Broadcasting Corporation's documentary, \"Afghanistan: Between Hope and Fear,\" broadcast in the spring of 2008. His work was previously cited in the Stockholm International Peace Research Institute article, \"The Taliban's propaganda activities: How well is the Afghan insurgency communicating and what is it saying?\".\nAlexander Mayer assists in analyzing counterterrorism efforts against the Taliban and Al Qaeda in Pakistan's Northwest Frontier Province for The Long War Journal. A Chicago native, Alex is currently pursuing his MA degree in Security Policy Studies at George Washington University, with a focus on terrorism and Middle East regional security. He is also working as a Research Department intern at Foundation for Defense of Democracies (FDD) in Washington, DC during the Fall 2010 semester. Alex recently completed a year-long fellowship with Radio Free Europe/Radio Liberty in Prague, Czech Republic, and graduated summa cum laude from the College of William and Mary with a Bachelor of Arts degree in Government and Middle Eastern Studies in 2009."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f325b00f-369a-4a3c-a7df-7361372a18a0>","<urn:uuid:04cedff9-6219-4633-8440-b19edf778ef8>"],"error":null}
{"question":"What are the symptoms of arrhythmia, and how does it relate to the broader issue of cardiovascular disease mortality in the US?","answer":"Arrhythmia symptoms include fluttering in the chest, sudden weakness, fainting or near-fainting, chest pain, focus difficulties, fatigue, shortness of breath, and issues when exercising. However, sometimes there may be no symptoms at all, and a doctor might discover arrhythmia during a routine examination. This condition is part of the broader cardiovascular disease problem in the United States, where one person dies every 36 seconds from cardiovascular disease. In fact, cardiovascular disease is the leading cause of death for men, women, and people of most racial and ethnic groups in the United States, with about 659,000 people, or 1 in 4 people, dying each year from it.","context":["Heart arrhythmia treatment could control or remove abnormal heartbeats. Additionally, since difficult heart arrhythmias are frequently worsened — or are actually triggered — by a weak or broken heart, you might be in a position to lower your arrhythmia risk by using a heart-healthy lifestyle.\nHowever, some heart arrhythmias could cause annoying — sometimes even lethal — signs and symptoms. It generally does not suggest there’s a significant issue, even when an individual sees signs.\nWhat’re the signs and outward indications of arrhythmia? Some individuals might have no symptoms whatsoever. A health care provider may discover an indication of arrhythmia throughout a routine evaluation.\nSymptoms of tachycardia include: (Occasionally you will find no signs)\n- Syncope (fainting, or near-fainting)\n- Fluttering in the chest\n- Sudden weakness\nSymptoms of bradycardia include: (Occasionally you will find no signs)\n- Angina (chest pain)\n- Focus difficulties\n- Issues when training\n- Fatigue (tiredness)\n- Shortness of breath\n- Syncope (fainting or near-fainting)\nSigns for atrial fibrillation : (Occasionally you will find no signs)\n- Angina (chest pains)\n- Syncope (fainting, or near-fainting)\nCauses of Arrhythmia\nA lot of things can result in, or trigger, an arrhythmia, including:\n- A coronary attack that is happening at this time\n- Scarring of heart muscle from the previous heart attack\n- Blocked arteries in your heart (coronary artery disease)\n- High blood-pressure\n- Overactive thyroid gland (hyperthyroidism)\n- Consuming an excessive amount of alcohol or caffeine\n- Health supplements and herbal remedies\n- Electric shock\n- Polluting of the environment\nWhat is an ordinary heartbeat?\nThe electrical signals that make it contract follow an exact path during your heart, whenever your heart beats. An arrhythmia can be caused by any interruption in these impulses.\nThroughout a pulse, the atria contract and complete the ventricles with blood.\nYour heart is divided in to four chambers. The chambers on each 1 / 2 of your heart type two adjacent pumps, by having an upper chamber (atrium) and a diminished chamber (ventricle).\nThe impulse then travels towards the middle of one’s heart, for the atrioventricular node that lies on the path between your ventricles and atria. From here, the intuition travels during your ventricles and leaves the atrioventricular node. Because their minds are so effective trained players at-rest generally possess a heartbeat significantly less than 60 beats one minute.\nKinds of arrhythmias\nPhysicians identify arrhythmias not just by where they begin (atria or ventricles) but additionally by the pace of heartbeat they cause:\nTachycardia – This describes a quick pulse — a resting heart-rate more than 100 beats one minute.\nBradycardia – This describes a slow pulse — a resting heart-rate significantly less than 60 beats one minute.\nNot totally all tachycardias or bradycardias suggest you’ve cardiovascular disease. For instance, during exercise it is normal to produce tachycardia whilst the heart increases to supply your cells with increased oxygen-rich blood.\nA heartbeat is clearly an additional beat, even though it usually feels as though a missed heartbeat. It rarely indicates you’ve a far more severe issue, despite the fact that you may experience an occasional rapid beat. Nevertheless, a rapid beat may induce an extended sustained arrhythmia — particularly in individuals with cardiovascular disease.\nCertain factors may raise your threat of developing an arrhythmia. These generally include:\n- Congenital cardiovascular disease. Being blessed with a heart problem might affect your heart’s beat.\n- Coronary artery disease, prior heart surgery and other heart problems. Previous heart surgery, heart attack, abnormal valves, concentrated heart arteries, cardiomyopathy and other heart injury are risk factors for every type of arrhythmia.\n- High blood-pressure. It might also cause the walls of one’s left ventricle to become heavy and rigid, which could alter how electrical signals travel during your heart.\n- Medicines and products. Over-the-counter cough and cold medications containing pseudoephedrine and certain prescription medications may subscribe to arrhythmia development.\n- Diabetes. Your threat of developing high blood-pressure and coronary artery disease significantly increases with uncontrolled diabetes.\n- Chemical imbalance. Materials in your body called chemicals — such as for instance sodium, potassium, calcium and magnesium — support trigger and perform the electrical impulses in your heart. Chemical levels which are too large or too low can impact your heart’s electrical signals and subscribe to arrhythmia development.\n- Consuming an excessive amount of alcohol. Actually, development of atrial fibrillation after a bout of heavy-drinking may also be called “holiday heart syndrome.” Chronic alcohol abuse could cause your heart to beat less efficiently and can result in cardiomyopathy.\nComplications of Arrhythmia\nSpecific arrhythmias may raise your threat of developing problems such as for instance:\n- Heart failure. This could result if your heart is working ineffectively for an extended period because of bradycardia or tachycardia, including atrial fibrillation.\n- Sroke. Whenever your heart quivers, it is not able to pump blood efficiently, which could cause blood to pool. This could cause blood clots to create. If your clot breaks free, it may go and block a brain artery, causing a stroke. This might harm some of one’s mind or result in death.","Did you know in 1964 President Lyndon B. Johnson was the first to proclaim the month of February as American Heart Month in efforts to bring awareness to the American people? Heart disease is, and has been, the leading cause of death for men, women, and people of most racial and ethnic groups in the United States. One person dies every 36 seconds in the US from cardiovascular disease (CVD). About 659,000 people in the US, or 1 in 4 people, die each year from CVD.\nWhat is cardiovascular disease? Cardiovascular disease refers to several types of heart conditions with the most common being coronary artery disease (CAD). CAD is associated with plaque buildup in our arteries; the tiny little ones that provide blood flow to our heart, known as our coronary arteries. Without enough blood flow to an area means there will be a lack of oxygen to that area as well. When there isn’t sufficient oxygen being provided, that is when we start experiencing signs and symptoms of discomfort; worst cases scenario being tissue death and/or mortality.\nOur heart is a muscle, just like the ones in our arms and legs. If we pull a muscle in our arm or leg, it generally stays in that local area. We normally see a bruise, or we move in a certain way and we feel a pop, either way it’s obvious and we know we must tend to it if we want to feel better. If we get an injury to our heart muscle, you won’t see a bruise on your chest telling you there’s something wrong and it may not be super obvious. That’s why paying attention to signs and symptoms is a priority and going to your primary care doctor annually for labs and a check up are important. If we know something is going on we can help prevent it or monitor it, CVD does not need to be a death sentence.\nThree big takeaways that I want you to leave a good understanding with are: Signs and Symptoms, Risk Factors, and Prevention.\nSigns and Symptoms: Chest pain or discomfort, upper back or neck pain, indigestion, heartburn, nausea or vomiting, extreme fatigue, upper body discomfort, dizziness, shortness of breath, fluttering feeling in the chest, swelling in the feet, ankles, legs, abdomen, or neck veins.\nThere are two types of risk factors, modifiable and non-modifiable. Modifiable risk factors are things that are in our control to change; lifestyle, activity, nutrition, weight. Non-modifiable risk factors are things you cannot control; age, race, gender, genetics. Genetics only account for 20% of our health outcomes and 50% is attributed to our lifestyle choices. After we change our modifiable risk factors and notice things aren’t changing favorable over time, consult with your doctor to possibly consider medication. Medications can be helpful to maintain desirable ranges for things like blood pressure, cholesterol, A1C, etc.\nSome risk factors for CVD are high blood pressure, high cholesterol, smoking, diabetes, overweight and obesity, unhealthy diet, physical inactivity, and excessive alcohol use.\nWays to help prevent CVD are stop smoking or using tobacco, move for at least 30 minutes a day, eat a heart healthy diet, maintain a healthy weight, get good sleep, manage stress, and get regular health screenings.\nPreventing heart disease can be challenging and overwhelming. If that sounds like you, please reach out to a professional for help. Be sure to utilize our free educational resources at: https://fullsclaefit.com/resources/ for various health and wellness calculators, nutrition database, exercise database, and so much more.\nAuthor: Quinn Butler, MS, CEP, Full Scale Fitness Personal Trainer\n- Centers for Disease Control and Prevention. (2021, September 27). About heart disease. Centers for Disease Control and Prevention. Retrieved February 11, 2022, from https://www.cdc.gov/heartdisease/about.htm\n- Centers for Disease Control and Prevention. (n.d.). Prevalence of heart disease — United States, 2005. Centers for Disease Control and Prevention. Retrieved February 11, 2022, from https://www.cdc.gov/mmwr/preview/mmwrhtml/mm5606a2.htm#:~:text=Heart%20disease%20has%20been%20the,a%20major%20cause%20of%20disability\n- Mayo Foundation for Medical Education and Research. (2022, January 14). Top strategies to prevent heart disease. Mayo Clinic. Retrieved February 11, 2022, from https://www.mayoclinic.org/diseases-conditions/heart-disease/\n- Proclamation 3566-american heart month, 1964. Proclamation 3566-American Heart Month, 1964 | The American Presidency Project. (1963, December 30). Retrieved February 11, 2022, from https://www.presidency.ucsb.edu/documents/proclamation-3566-american-heart-month-1964#:~:text=Johnson%2C%20President%20of%20the%20United,States%20to%20issue%20similar%20proclamations"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:1931717f-fe4f-4acc-8a5d-4b3c54b03be2>","<urn:uuid:d875f919-75c7-4493-bd35-9a6690a4dd9d>"],"error":null}
{"question":"How does the Microsoft Kinect help evaluate walking difficulties in MS patients?","answer":"The Microsoft Kinect, a 3D depth-sensing camera, helps evaluate walking difficulties in MS patients by detecting movement and using computer algorithms to quantify walking patterns. The system captures patient movement data and analyzes gait characteristics, mathematically defining them at different severity levels. This method reduces potential human error compared to traditional subjective clinical evaluations, as it can accurately determine the level of gait abnormality and provide reproducible results that correlate with clinical measures.","context":["A device commonly found in living rooms around the world could be an inexpensive and effective means of evaluating the walking difficulties of multiple sclerosis (MS) patients. The Microsoft Kinect is a 3D depth-sensing camera used in interactive video activities such as tennis and dancing. It can be hooked up to an Xbox gaming console or a Windows computer.\nA team of McGill University researchers collaborated with the Montreal Neurological Institute and Hospital to test whether the Kinect could detect the differences in gait of MS patients compared with healthy individuals. In current clinical practice, the walking movement of MS patients is usually assessed by their doctors, and subjective evaluations may distort results because two different clinicians may give the same patient different evaluations. Using a camera that detects movement and computer algorithms that quantify the patients’ walking patterns can reduce the potential for human error.\nThe researchers were led by postdoctoral fellow Farnood Gholami and supervised by Jozsef Kövecses from the Department of Mechanical Engineering and Centre for Intelligent Machines. Gholami captured the movement of 10 MS patients and 10 members of an age-and-sex-matched control group using the Kinect device. The MS patients had previously been assessed for gait abnormalities using the traditional clinician method.\nHow It Works\nUsing the data, the team then developed computer algorithms that quantified gait characteristics of MS patients and healthy people. The investigators found that gait characteristics measured with the Kinect camera and analyzed with the developed algorithms were reproducible, and were different between MS patients and the healthy individuals. Moreover, the gait characteristics of MS patients obtained by the algorithm were correlated with clinical measures of gait. The algorithms could mathematically define the characteristics of gait in MS patients at different severity levels, accurately determining the level of gait abnormality.\nGholami says he became interested in using motion-capture technology for clinical purposes as a PhD student, but the equipment he was using at the time was very expensive, difficult to use, and nonportable, making widespread clinical use prohibitive. The Kinect device gave him an inexpensive tool that appears accurate enough to do the job.\n“This tool may help the clinician provide a better diagnosis of gait pathology, and may be used to observe if a prescribed medication has been effective on the gait of the patient or not,” he says. “Our developed framework can likely be used for other diseases causing gait abnormalities as well, for instance Parkinson’s disease.”\nDaria Trojan, a physiatrist in the Department of Neurology and Neurosurgery working at the Montreal Neurological Institute and Hospital, says the tool could be useful “to assess treatment effects of certain interventions, such as rehabilitation or medication, and to document MS disease progression as reflected by gait deterioration. It may also be useful as a measure in clinical trials.” The next step is to conduct a study with a larger group of MS patients, including evaluation in a gait laboratory, using a newer version of the Kinect device that promises to improve accuracy.\nFor more information, visit www.mcgill.ca/neuro/Kinect-MS-Device-Gait ."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:ff9025dc-4486-49bb-bbaf-30bae3ef36eb>"],"error":null}
{"question":"What's the difference between IGI and GIA diamond certification?","answer":"Both IGI and GIA are reputable diamond grading laboratories, but GIA is considered the most respected worldwide as they developed the 4C's grading system in the 1940s which became the industry standard. IGI is the largest organization of its kind with laboratories in multiple cities including Antwerp, New York, Hong Kong, and Mumbai. Both laboratories inspect diamonds using microscopes and grade up to 'Internally Flawless' (IF) as their highest clarity grade, though GIA additionally includes a 'Flawless' (FL) grade to distinguish between internally flawless diamonds and those that are also unblemished on the surface.","context":["How can you be sure that the diamond you bought has the quality it was advertised for? This is the mission and purpose of the diamond grading laboratories. They examine and measure the diamonds and provide a certificate that matches internationally agreed standards. The diamond certificate or diamond passport guarantees and describes the quality for each unique diamond. In the certificate, you will find the values to capture the 4 c's of diamond quality: clarity, color, carat, and cut. The certificate also allows to identify each unique diamond and can be used in case of theft or loss to prove the diamond ( or engagement ring) is your property.\nAround the world, IGI certificates bring confidence when buying or selling diamonds, gemstones and jewelry. Total commitment to understanding consumer concerns has motivated IGI to develop a comprehensive analysis and clear documentation for consumers. This empowers jewelry buyers to focus on finding precisely what they want, with full assurance in the integrity and quality of the IGI certification. IGI is the largest organization of its kind, with operating laboratories & offices in Antwerp, New York, Hong Kong, Mumbai, Bangkok, Tokyo, Dubai, Tel Aviv, Toronto, Los Angeles, Kolkata, New Delhi, Thrissur, Jaipur, Surat, Chennai, Ahmedabad and Hyderabad\nThe HRD Antwerp diamond lab is the largest diamond lab in the world that issues reliable diamond certificates, grading and jewelry reports fully compliant with the rules for grading polished diamonds set by the International Diamond Council (IDC)\nEstablished in 1931, GIA is the world’s foremost authority on diamonds, colored stones, and pearls. A public benefit, nonprofit institute, GIA is the leading source of knowledge, standards, and education in gems and jewelry.\nWe did not only select these because they are reputable diamond grading labs but in addition, they provide the diamonds with a certificate and packaged in a sealed container.\nSealing guarantees that a given certificate and the corresponding diamond belong together, and it keeps the diamond safe from oil and dirt. The sealing is tamper-proof and will show when opened or tampered with.\nAlthough all Diamond Grading Laboratories will inspect diamonds using microscopes at very high magnification ( 40x +) , the actual grading only occurs at 10x magnification, this is why HRD will not grade higher than 'Loupe Clean' ( LC).\nOther Gemological Institutes ( IGI, GIA etc ), have opted for 'Internally Flawless' ( IF) as their highest grade. Since some diamonds may be slightly 'blemished' on their surfaces ( although this does not really affect the Clarity of the diamond ), GIA has added 'Flawless' to its grading in order to distinguish between the Internally Flawless ( Loupe Clean ) and those who are NOT only IF/LC but also totally 'unblemished'.\nOnce that a diamond is worn or handled, the chances are that it will lose its FL grading, since as soon as a diamond makes contact with any hard object, it will be slightly 'blemished'. As far as Clarity is concerned, IF and LC are the highest.\nWith our diamonds graded by IGI,GIA or HRD you can add the 5th \"c\" to your diamond purchase: Confidence\nOur diamond education pages help you to understand the 4 C's of diamond quality so you can choose your next diamond wisely.","Bespoke Craftsman Jeweller and Diamond Dealer Based in Bawtry, South Yorkshire\nIn white or colourless diamonds, colour is universally graded using an alphabetical scale from D to z with D being the highest grade.\nBefore the GIA colour grading system was universally accepted, various other systems were used including letters of the alphabet starting at A,B and C with multiple A’s used to grade the best stones, number systems and Roman numerals and the result of all of these grading systems was confusion and inaccuracy and so, to make a fresh start and to avoid any association with previous systems it was decided to us D as the highest grade.\nThe grader will assess the colour of the diamond using a master set of diamonds of each colour grade. Without comparing diamonds side by side it is very difficult to see the difference between a D colour and a G colour stone.\nStones graded D,E,F are described as colourless, G,H,I ,J are near colourless, K,L,M are faint.\nDiamonds and other gemstones are weighed in metric carats. One carat is 0.2 grams and a carat is split further into 100 points (a diamond weighing 50 points is a half carat diamond).\nAny diamond weighing more than 1 carat is described in carats and decimals. For example a diamond weighing 1.72 carats would be expressed as one point seven two carats.\nThe quality of the cut of a diamond is important, because it dictates a diamond’s fire, sparkle and brilliance. A standard round brilliant cut will have 57 facets and how these facets relate to each other will determine how the light will pass through the diamond. The scale for the quality of the cut goes through Excellent, being the highest grade, Very good, Good, Fair or Poor.\nAs with colour, unless you have stones side by side of differing cut grades, it would be very difficult to tell an Excellent from a Very good cut.\nDiamonds are formed under extreme heat and pressure deep inside the earth and often display internal inclusions or external blemishes.\nThe clarity of a diamond refers to the absence of these inclusions and blemishes. The scale for grading clarity usually contains 11 grades starting with FL (flawless) and IF (internally flawless) followed by VVS1, and VVS2 (very, very slightly included), VS1 and VS2 (very slightly included), SI1 and SI2 (slightly included) and I1, I2 and I3, these are stones with quite noticeable inclusions often able to be seen with the naked eye.\nWe sell certified diamonds graded from 5 different laboratories.\nGIA (Gemological Institute of America)\nIGI (International Gemological Institute)\nHRD (Hoge Raad Voor Diamant)\nEGL (European Gemological Laboratory)\nIIDGR (International Institute of Diamond Grading and Research)\nGIA developed the 4C’s system of diamond grading in the 1940’s and as such is the most respected certificate worldwide. Their system is now accepted as the industry standard and is used by all grading labs, therefore I would recommend to buy your stone with a GIA certificate. That is not to say that certificates issued by other labs are less worthy, consistency in the grading of the different 4C’s is the key."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:78082e19-d934-4890-86b2-87836cb2b2e2>","<urn:uuid:1fba9f86-857c-49f8-a16e-4f7a14a08267>"],"error":null}
{"question":"What are the key responsibilities of mediators in maintaining impartiality, and how do ADR complaint commissions differ in their binding nature?","answer":"Mediators must remain impartial and even-handed throughout the process, avoiding partiality, prejudice, or any conduct that gives the appearance of bias. They should avoid behaviors like spending more time with one party or socializing with them, and must withdraw if unable to maintain impartiality. For complaint commissions in ADR systems, their settlements have varying binding nature - they are mostly non-binding for parties, though in some systems they may bind the entrepreneur. Some complaint commissions can even handle consumer cases without entrepreneur consent, and while their decisions may not be binding, they can impact the entrepreneur's reputation.","context":["Types of ADR systems\nMain types of ADR systems occur in the European Union:\nMediation and conciliation\nBy means of mediation or conciliation the parties try to reach an amicable agreement with the participation of third party.\nThe mediator’s task is to make easier for the parties to find a solution satisfactory for them. The mediator himself doesn’t impose any solution but takes care that the parties reach compromise independently. In conciliation system the third party, after listening to the arguments of both parties, tries to propose the best solution for them. This proposal doesn’t have to be binding for the parties. In mediation and conciliation proceeding the parties are not limited by the provisions of substantive law and rules of procedure. The dispute settlement, therefore, doesn’t have to be based on a specific legal norm, but may refer to the rules of honesty, legitimacy, loyalty or good morals. Most often the amicable agreement concluded in such a proceeding additionally requires granting an enforcement clause by court.\nArbitration is a method of out-of-court disputes settlement mostly close to court procedures. The most important legal instrument regulating arbitration is Convention of the United Nations on Identification and Acceptance of Foreign Arbitration Decisions as of June 10th, 1958.\nArbitration is a type of procedure within which the parties select one or more neutral individuals to whom they present the case in order to obtain a final legally binding settlement. It may be of single or institutionalised nature. In temporary arbitration each party of dispute selects its own arbitrator (or arbitrators) and then these appoint a super arbitrator. Selected in such a way composition settles a dispute on the basis of previously agreed rules. Institutional arbitration most often functions on the basis of professional organisation dealing with arbitration. In some models of arbitration there may be formed a necessity of conducting additional enforcement proceeding before a civilian court.\nConsumer organisations, associations of entrepreneurs or commercial institutions may jointly or independently organise complaint commissions basing on provisions of common law or solutions based on soft-law. Complaint commissions are of collective nature with equal representation of consumers and entrepreneurs community.\nThe commission’s settlements are mostly not binding for parties, unless in some systems they bind the entrepreneur.\nSome complaint commissions may conduct consumer cases even without the entrepreneur’s consent. Such a decision, although not binding one, is significant for his reputation.\nOmbudsman is a single-person institution appointed to settle disputes between entrepreneurs and consumers. For this position there is appointed a person with high subject-matter qualifications, enjoying a high prestige and spotless opinion. Most often this ADR type is formed from initiative of entrepreneurs of certain branch and constitutes one of the instruments of soft-law. Although ombudsman is appointed by entrepreneurs, usually in his settlements, he is an independent authority. Ombudsman is usually competent in the indicated scope of cases and works out decisions on the basis of legal provisions, rule of equality or guidelines accepted in the branch. His decisions are usually binding for entrepreneur or are not binding for any party.","Ethical Guidelines for Mediators\nTable of Contents\nETHICAL GUIDELINES FOR MEDIATORS\nMediation is a process in which an impartial person - a mediator - facilitates the resolution of a dispute by promoting uncoerced agreement by the parties to the dispute. A mediator facilitates communication, promotes understanding, assists the parties to identify their needs and interests, and uses creative problem solving techniques to enable the parties to reach their own agreement.\nA mediator should explore with the parties prior to the mediation commencing that each party will have the necessary authority to conclude any settlement.\n- A mediator should provide information about the process, and help the parties identify their real concerns and all their options. The primary role of the mediator is to facilitate voluntary resolution of disputes by the parties themselves.\n- A mediator cannot personally ensure that each party has made a fully informed decision when reaching an agreement to resolve a dispute, but it is good practice for the mediator to make the parties aware of the importance of consulting other professionals, where appropriate, to help them make informed decisions.\n- The mediator must address with the parties any instances of deceit, fraud and misleading statements before any settlement is reached.\nA mediator may mediate only those matters in which the mediator can remain impartial and even handed. If at any time the mediator is unable to conduct the process in an impartial manner the mediator should withdraw.\nAccordingly, a mediator must avoid:\n- partiality or prejudice; and\n- conduct that gives any appearance of partiality or prejudice.\n- Whatever their own views and standards mediators should not only not be partial or prejudiced but should avoid the appearance of partiality or prejudice by reason of such matters as the parties' personal characteristics, background, values and beliefs or conduct at the mediation.\n- Mediators should be conscious of behaviour which, however innocent, may be interpreted as indicating partiality or prejudice, such as spending more time with one party than another without good reason, socialising with a party and adopting different modes of address.\n- Even if all the disputants agree that they would like the mediator to express an opinion on the merits, there is a substantial risk in giving such an opinion that the mediator may no longer appear to be impartial. As a result the mediator may be obliged to withdraw.\n- Should the disputants agree to terminate the mediation and enter an alternative process, using the mediatior, the mediator must consider the suitability of continuing as the appointed resolver and may need to withdraw altogether notwithstanding the parties’ wishes.\n3. CONFLICTS OF INTEREST\nBefore the mediation begins, the mediator must disclose all actual and potential conflicts of interest known to the mediator. The mediator should:\n- discuss any circumstances that may, or may be seen to, affect the mediator’s independence or impartiality; and\n- at all times be transparent about the mediator’s relations with the parties in the mediation process.\nDisclosure must also be made if conflicts arise during the mediation.\nAfter making disclosure the mediator may proceed with the mediation if all parties agree and the mediator is satisfied that the conflict or perception of conflict will not preclude the proper discharge of the mediator's duties. The mediator must be certain of:\n- the parties’ agreement; and\n- the mediator’s ability to undertake the mediation with independence and neutrality so as to ensure impartiality.\n- Conflicts of interest may arise in recommending the services of others. It may be preferable to recommend referral services or associations which maintain rosters of qualified persons.\n- External pressures should never influence the mediator. The mediator's commitment should be to the parties and the process.\n- Interests which should be disclosed include any association with a party or adviser or representative of a party, which could reasonably be seen to affect the impartiality of the mediator.\n- The mediator should disclose to the participants any circumstances which may cause, or have tendency to cause, a conflict of interest. In particular a mediator who is a partner or an associate of any representative retained by either of the parties should not act as mediator without the fully informed consent of all the parties.\n- The mediator should not establish a professional relationship with one of the parties in relation to the same dispute.\nA mediator must not mediate unless the mediator has the necessary competence to do so and to satisfy the reasonable expectations of the parties.\nA person who agrees to act as a mediator holds out to the parties and the public that she or he has the competence to mediate effectively.\n- Competence comprises appropriate knowledge and skills which would normally be acquired through training, education, and experience.\n- Mediators should have available for the parties information regarding their training, education and experience.\n- When a person is appointed or nominated to a panel or list of mediators, the appointing court, tribunal, institution, or agency should ensure that the mediator has through training, education and experience acquired the necessary knowledge and skill for inclusion on the particular panel or list.\n- The qualifications for inclusion on a list of mediators should be made public and available to interested persons.\nSubject to the requirements of the law a mediator must maintain the confidentiality required by the parties.\n- As the parties' expectations regarding confidentiality are important, the mediator should discuss those expectations with the parties and endeavour to meet them. The mediator should clarify when the mediation begins and when it ends, and whether conversations on the telephone, in meetings and communications by email and other means are also confidential.\n- The parties' expectations of confidentiality depend on the circumstances of the mediation and any agreements they, and any other persons present at the mediation, and the mediator may make.\n- A mediator should not disclose any matter that a party requires to be kept confidential (including information about how the parties acted in the mediation process, the merits of the case, any settlement offers or agreed outcomes) unless:\n- the mediator is given permission to do so by all persons attending at the mediation with an interest in the preservation of the confidence; or\n- the mediator is required by law to do so.\n- The parties and the mediator may make their own rules with respect to confidentiality, or the accepted practice of the mediator or an institution may mandate a particular set of expectations.\n- If the mediator intends to hold private sessions with a party, the mediator should before such sessions discuss with the parties the confidentiality attaching to them.\n- Any reporting which requires a subjective judgment by the mediator of the conduct of the parties is likely to destroy the integrity of the mediation process.\n- Under appropriate circumstances, researchers may be permitted to obtain access to statistical data.\n- With the permission of all of the parties, researchers may be permitted access to individual case files, to observe mediations, and to interview participants.\n- A mediator should render anonymous all identifying information. When materials emanating from a mediation are used for research, supervision, or training purposes, the mediator should remove all identifying information from them.\n6. TERMINATION OF MEDIATION\nA mediator may terminate the mediation if the mediator considers that:\n- any party is abusing the process; or\n- there is no reasonable prospect of settlement.\nThe mediator if appropriate should inform the parties, and may terminate the mediation if:\n- a settlement is being reached that to the mediator appears illegal having regard to the circumstances of the dispute; or\n- the mediator considers that continuing the mediation is unlikely to result in a settlement.\n7. RECORDING SETTLEMENT\nIf the mediation results in a settlement between the parties, the mediator should encourage the parties to continue the mediation until the parties have:\n- addressed any enforceability issues; and\n- recorded terms of settlement in writing.\n- Normally agreement to record the terms of any settlement in writing should be made prior to the commencement of the mediation.\n- The mediator ought to be cautious about direct involvement in drafting the terms of agreement, as their involvement in drafting may be construed as providing legal advice.\n- The mediator should however assist the parties to take whatever steps may be necessary to formalise any settlement agreement, and satisfy themselves regarding its enforceability. The mediator may consider seeking to reconvene the mediation at a later time to allow the parties to finalise a settlement deed or any necessary court orders.\n8. PUBLICITY AND ADVERTISING\nA mediator must not engage in misleading or deceptive publicity or advertising.\nA mediator must not make any false or misleading statement including statements or claims as to the mediation process, its costs and benefits, or the mediator's role, skills, or competence.\nA mediator must fully disclose his or her fees to the parties.\n- As early as practicable, and before the mediation session begins, a mediator should obtain the agreement of the parties regarding all fees and other expenses to be charged for the mediation, and by whom and when the fees and expenses are to be paid.\n- The better practice is to record in writing the arrangements in respect of fees and costs.\n- A mediator should not agree to a fee which is contingent upon the result of the mediation or amount of settlement."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:a8c44f42-1e82-4e8d-bf62-75bcab2a7fd9>","<urn:uuid:eda71d1f-741b-4f71-bdad-e586991a8436>"],"error":null}
{"question":"How do the raw materials used in modern float glass compare to the historical glass manufacturing formulas?","answer":"Modern float glass is made from precisely controlled raw materials including silica sand (SiO2), sodium oxide from soda ash (Na2O), dolomite (MgO), feldspar (Al2O3), and recycled cullet. Historical glass formulas were simpler, consisting primarily of silica, lime, and soda or potash for common glass, while more expensive flint glass replaced lime with lead oxide. Additionally, modern float glass can incorporate specific metal oxides like NiO, CoO, SeO, and Fe2O3 to create different color tints, while historical glass color was mainly influenced by the purity of ingredients, with green tints coming from iron impurities.","context":["NRF promotes and invests in the architectural heritage of the Newport community, the traditional building trades, and Doris Duke’s fine and decorative arts collections, for the enjoyment, education and inspiration of all.\nAs a leader in the preservation of early American architecture, NRF supports research and education in areas directly related to its collections and issues of critical concern to the field of historic preservation.\nTour Doris Duke’s art-filled mansion and enjoy panoramic ocean views from the extensive grounds, designed by Frederick Law Olmsted. Open April to November.\nExperience the only museum in the world specializing in 18th-century Newport furniture and related decorative arts.\nExplore 40 acres of open space, a tribute to the agrarian heritage of Aquidneck Island. The site is open daily from dawn to dusk for public enjoyment.\nNewport Restoration Foundation holds one of the largest collections of period architecture owned by a single organization anywhere in the United States.\nCelebrate excellence in historic preservation efforts within the City of Newport, Rhode Island.\nLive amidst history by renting one of our many historic properties.\nHelp us to continue a lived-in legacy by making a contribution to our Annual Fund today.\nFollowing a recent lecture on Newport architecture, I felt compelled to do a bit of research to back up my knowledge that glass is not a flowing, “supercooled liquid” at all. The characteristics of historic glass are all due to its methods of manufacture, but the nature of the material itself required confirmation from the experts. I contacted Stephen Koob and Dr. Brill at the Corning Museum of Glass, two renowned scientific experts in the conservation of glass and the nature of the material.\nThey stated that glass is not really a supercooled liquid at all, since it does not display any characteristics of a liquid. It is an amorphous solid, meaning that it does not have the more common crystalline structure of minerals; despite that difference, the material most chemically and physically similar to glass is quartz – definitely a solid also.\nAs to the notion – often observed to be true – that old window glass is thicker at the bottom than at the top, that quality is due to the original form of the glass, which in turn is due to its method of manufacture. The manufacturing processes will be described first, followed by the construction principles for windows. These generalities apply equally to glass in medieval stained glass as well as to eighteenth-century windows everywhere.\nEarly glass could be made in three ways: crown, cylinder, and rolled. The latter can be discounted for window glass, because the process was only applied to very costly luxury items. Crown glass was first blown by mouth: a gather of molten glass from the furnace was blown on a blowpipe until it became a proper sized bubble (limited by the glassworker’s breath and arm strength), at which point, the side of the bubble away from the pipe was cut open, and the worker spun the hot glass into a flat disk after transferring the piece from the blowpipe to a pontil rod (stuck to the center of the disk, where the blowpipe had been; this also closed up the hole in the middle). This\noften entailed a number of steps, mostly to keep the glass hot and malleable. This was the “crown” of glass. The disk tapered from the center to the edges, but if the worker was skilled, at least it was fairly flat; often, curved lines resulting from the spinning may be seen in pieces of crown glass. Cutting usable panes or “quarries” from the disk involved some waste, but cutting small panes, some of them triangular (for a diamond-pattern window) wasted the least. Unusable scraps of glass were returned to the furnace. The center of the disk, called the bull’s eye, was not as transparent as the rest, and was often used in transoms above doorways, where one would not be able to look out anyway, and some light was admitted. Now, of course, bull’s eyes are much sought after.\nA type of flat glass that was a bit thicker was produced by the broad or cylinder method. A large bubble of glass was blown, roughly forming a cylinder, which was cut open along its length and ironed flat while still hot and pliable from the furnace. Although cylinder glass could produce larger rectangular pieces than crown glass, it was more difficult to make, and it was not as clear, due to the ironing process; elongated striations may be seen in it.\nThe first method for making plate glass was developed in the late seventeenth century, but it seldom pertains to window glass. Early plate glass was made by rolling molten glass on metal tables, producing heavy pieces of dull, uneven glass that needed considerable grinding and polishing to be usable in large windows and mirrors. Needless to say, it was much more costly than either crown or cylinder glass. By the mid-nineteenth century, a method for making drawn sheet glass created smooth glass by pressing the molten material between two rollers. By 1923, high quality thick plate glass could be produced, and in 1959 the float glass process was perfected, producing a continuous ribbon of glass floated on molten tin.\nAs a brief note, there are two common formulas for glass. In the simplest terms, common glass is made of silica, lime, and soda or potash. Its color and clarity depend on the purity of the ingredients; it was frequently green from iron content. Common glass was often known as “bottle glass.” More expensive flint glass was made substituting lead oxide for lime. The resulting material was heavy, sparkling, and somewhat softer than common glass. Flint glass was well suited to cutting and engraving, while common glass was used in creating decorative pressed glass pieces.\nAs for the construction of windows, the method is straightforward. Panes are cut from the crown, and often display some taper, due to the nature of the crown itself. Logically, the thicker edge of the glass is placed toward the bottom. Sometimes, lines or smears can be seen in the glass, also as a result of the production method. If cylinder glass was used, those lines may be straighter, but not necessarily. Consider that old mirrors, bottles and other vessels do not change shape over time, and we know that window glass does not, either. The glass does not sag over time, nor does its surface become wavy as it ages. It cannot become “less solid” unless it is placed in a furnace and melted.\nAbout the only change that occurs in old glass – and then, only in rare and extreme cases – is sometimes called glass disease. A fault in the original composition of the glass causes it to be vulnerable to attack by moisture. Alkali is leached out of the glass, droplets of moisture might be seen on the surface (hence the expression, weeping glass), and there may be a slight vinegar smell. The glass may take on a hazy appearance, developing over time into a mass of microscopic cracks; this condition is called crizzled glass. Also rarely, old glass with an excess of magnesium in the mix can develop a purple cast, although in modern times this condition is generally regarded as a status symbol rather than a problem. It does not materially affect the glass.\nBy Bruce MacLeish, Former Director of Collections, Newport Restoration Foundation.\nCopyright Bruce Macleish, Newport Restoration Foundation, June 2007.\nNewport Restoration Foundation is deeply saddened by the recent passing of our Chair Emerita, Mrs. Marion Oates “Oatsie” Charles on December 5, 2018.\nJoin us this Fall for more events celebrating this year’s theme of NRF at 50!\nThe month of August is jam-packed with fun activities that continue to celebrate the 50th anniversary of the Newport Restoration Foundation. We’ll be looking back at — and sometimes reviving — what Doris Duke and others were up to around the time of its founding and much more! Please mark your calendars for special events including...\nThis spring and summer, NRF will offer public programming that celebrates the 50th anniversary of the Newport Restoration Foundation by looking back at — and sometimes reviving — what Doris Duke and others were up to around the time of its founding. Please mark your calendars for special events including:","Have you ever wondered how is glass made? The glass – float glass as we know – is manufactured by the PPG process. This process was invented by Sir Alistair Pilkington in 1952 and is the most popular and widely used process that describes how to make glass for architectural purposes in the world today.\nIt consists of the following steps:\nStage 1- Melting & Refining:\nFine grained ingredients closely controlled for quality, are mixed to make a batch, which flows into the furnace, which is heated up to 1500 degree Celsius. This temperature is the melting point of glass.\nThe raw materials that float glass is made up of are:\n- SiO2 – Silica Sand\n- Na2O – Sodium Oxide from Soda Ash\n- MgO – Dolomite\n- Al2O3 – Feldspar\nThe above raw materials primarily mixed in batch helps to make clear glass. If certain metal oxides are mixed to this batch they impart colors to the glass giving it a body tint.\n- NiO & CoO – to give grey tinted glasses (Oxides of Nickel & Cobalt)\n- SeO – to give Bronze tinted glasses (oxide of Selenium)\n- Fe2O3 – To give Green tinted glasses (oxides of iron which at times is also present as impurity in Silica Sand)\n- CoO – To give blue tinted glass (oxides of Cobalt)\nApart from the above basic raw material, broken glass aka cullet, is added to the mixture to the tune of nearly 25% ~ 30% which acts primarily as flux. The flux in a batch helps in reducing the melting point of the batch thus reducing the energy consumed to carry out the process.\nStage 2 – Float Bath:\nGlass from the furnace gently flows over the refractory spout on to the mirror-like surface of molten tin, starting at 1100 deg Celsius and leaving the float bath as solid ribbon at 600 deg Celsius.\nStage 3 – Coating (for making reflective glasses):\nCoatings that make profound changes in optical properties can be applied by advanced high temperature technology to the cooling ribbon of glass. Online Chemical Vapour Deposition (CVD) is the most significant advance in the float process since it was invented. CVD can be used to lay down a variety of coatings, a few microns thick, for reflect visible and infra-red radiance for instance. Multiple coatings can be deposited in the few seconds available as the glass flows beneath the coater (e.g. Sunergy)\nStage 4 – Annealing:\nDespite the tranquillity with which the glass is formed, considerable stresses are developed in the ribbon as the glass cools. The glass is made to move through the annealing lehr where such internal stresses are removed, as the glass is cooled gradually, to make the glass more prone to cutting.\nStage 5 – Inspection:\nTo ensure the highest quality, glass manufacturers carry out inspection at every stage. Occasionally a bubble that is not removed during refining, a sand grain that refuses to melt or a tremor in the tin puts ripples in the glass ribbon. Automated online inspection does two things. It reveals process faults upstream that can be corrected. And it enables computers downstream to steer round the flaws. Inspection technology now allows 100 million inspections per second to be made across the ribbon, locating flaws the unaided eye would be unable to see.\nStage 6 – Cutting to Order:\nThe entire process of glass making is finally complete when diamond steels trim off selvedge – stressed edges- and cut ribbon to size dictated by the computer. Glass is finally sold only in square meters."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ecf1833f-dc5b-429d-b100-ea080e7b5299>","<urn:uuid:7e89ca18-8896-411c-aa4b-3ac07f6b77ea>"],"error":null}
{"question":"What are the key differences in frequency representation between analog and digital signals versus DSP filter responses?","answer":"Analog signals are represented as continuous sine waves with varying amplitude and frequency, while digital signals are represented as square waves with discrete values. In DSP systems, filters like the Chebyshev filter use z-transform to convert discrete-time signals into frequency domain representation. The Chebyshev filter allows ripple in the passband (Type 1) and provides the best approximation to ideal response for specified order and ripple. Its frequency response is generally linear, unlike nonlinear filters which can introduce new frequency components not present in the input signal.","context":["File Name: comparison between analog and digital signal .zip\nAnalog and digital signals are different types which are mainly used to carry the data from one apparatus to another. Analog signals are continuous wave signals that change with time period whereas digital is a discrete signal is a nature. The main difference between analog and digital signals is, analog signals are represented with the sine waves whereas digital signals are represented with square waves. In some way, an electronics project mainly interacts by the real analog world whereas digital signals with computers, microprocessors, and logic units. These two kinds of signals are similar to different electronic languages.\nAnalog and digital signals are used to transmit information, usually through electric signals. In both these technologies, the information, such as any audio or video, is transformed into electric signals. The difference between analog and digital technologies is that in analog technology, information is translated into electric pulses of varying amplitude. In digital technology, translation of information is into binary format zero or one where each bit is representative of two distinct amplitudes. An Analog signal is any continuous signal for which the time varying feature variable of the signal is a representation of some other time varying quantity, i. It differs from a digital signal in terms of small fluctuations in the signal which are meaningful. A digital signal uses discrete discontinuous values.\nWhen we talk about analogue or digital, we are referring to the type of transmission of a signal. There are a number of key differences between analogue and digital signal transmission. An analogue signal otherwise known as a wave form is characterised by being continuously variable along both amplitude and frequency. In the case of telephony, when we speak into a handset, our voice is converted into current, or voltage fluctuations. Those fluctuations in current are an analogue transmission of the actual voice pattern. To transmit an analogue signal effectively, we need to define the frequency in which is operates. In telephony, the usable voice frequency band ranges from approximately Hz to Hz, and so the network provider phone company will allocate a bandwidth of around 4,Hz for voice transmission.\nSound can be recorded and stored and played using either digital or analog techniques. Both techniques introduce errors and distortions in the sound, and these methods can be systematically compared. Musicians and listeners have argued over the superiority of digital versus analog sound recordings. Arguments for analog systems include the absence of fundamental error mechanisms which are present in digital audio systems, including aliasing and quantization noise. The bandwidth of the digital system is determined, according to the Nyquist frequency , by the sample rate used. The bandwidth of an analog system is dependent on the physical capabilities of the analog circuits.\nAny type of data is transferred in analog signal. Any data is converted into electric form first and after that it is passed through communication channel. Analog communication uses a continuous signal which varies in amplitude, phase, or some other property with time in proportion to that of a variable. They digital signal consists of discrete values rather than continuous values. In digital communication physical transfer of data occurs in the form of digital bit stream i. In digital communication the digital transmission data can be broken into packets as discrete messages which is not allowed in analog communication.\nTrack My Order. Frequently Asked Questions. International Shipping Info. Send Email.\nA signal is an electromagnetic or electrical current that is used for carrying data from one system or network to another. The signal is a function that conveys information about a phenomenon. In electronics and telecommunications, it refers to any time-varying voltage that is an electromagnetic wave which carries information. A signal can also be defined as an observable change in quality such as quantity In this tutorial, you will learn: What is Signal?\nAnalog and Digital are the different forms of signals.","What is Digital Signal Processing?\nDSP manipulates different types of signals with the intention of filtering, measuring, or compressing and producing analog signals. Analog signals differ by taking information and translating it into electric pulses of varying amplitude, whereas digital signal information is translated into binary format where each bit of data is represented by two distinguishable amplitudes. Another noticeable difference is that analog signals can be represented as sine waves and digital signals are represented as square waves. DSP can be found in almost any field, whether it's oil processing, sound reproduction, radar and sonar, medical image processing, or telecommunications-- essentially any application in which signals are being compressed and reproduced.\nSo what exactly is digital signal processing? The digital signal process takes signals like audio, voice, video, temperature, or pressure that have already been digitized and then manipulates them mathematically. This information can then be represented as discrete time, discrete frequency, or other discrete forms so that the information can be digitally processed. An analog-to-digital converter is needed in the real world to take analog signals (sound, light, pressure, or temperature) and convert them into 0's and 1's for a digital format.\nA DSP contains four key components:\n- Computing Engine: Mathematical manipulations, calculations, and processes by accessing the program, or task, from the Program Memory and the information stored in the Data Memory.\n- Data Memory: This stores the information to be processed and works hand in hand with program memory.\n- Program Memory: This stores the programs, or tasks, that the DSP will use to process, compress, or manipulate data.\n- I/O: This can be used for various things, depending on the field DSP is being used for, i.e. external ports, serial ports, timers, and connecting to the outside world.\nBelow is a figure of what the four components of a DSP look like in a general system configuration.\nThe Chebyshev filter is a digital filter that can be used to separate one band of frequency from another. These filters are known for their primary attribute, speed, and while they aren't the best in the performance category, they are more than adequate for most applications. The design of the Chebyshev filter was engineered around the matematical technique, known as z-transform. Basically, the z-transform converts a discrete-time signal, made up of a sequence of real or complex numbers into a frequency domain representation. The Chebyshev response is generally used for achieving a faster roll-off by allowing ripple in the frequency response. These filters are called type 1 filters, meaning that the ripple in the frequency response is only allowed in the passband. This provides the best approximation to the ideal response of any filter for a specified order and ripple. It was designed to remove certain frequencies and allow others to pass through the filter. The Chebyshev filter is generally linear in its response and a nonlinear filter could result in the output signal containing frequency components that were not present in the input signal.\nWhy Use Digital Signal Processing?\nTo understand how digital signal processing, or DSP, compares with analog circuitry, one would compare the two systems with any filter function. While an analog filter would use amplifiers, capacitors, inductors, or resistors, and be affordable and easy to assemble, it would be rather difficult to calibrate or modify the filter order. However, the same things can be done with a DSP system, just easier to design and modify. The filter function on a DSP system is software-based, so multiple filters can be chosen from. Also, to create flexible and adjustable filters with high-order responses only requires the DSP software, whereas analog requires additional hardware.\nFor example, a practical bandpass filter, with a given frequency response should have a stopband roll-off control, passband tuning and width control, infinite attenuation in the stopband, and a response within the passband that is completely flat with zero phase shift. If analog methods were being used, second-order filters would require a lot of staggered high-Q sections, which ultimately means that it will be extremely hard to tune and adjust. While approaching this with DSP software, using a finite impulse response (FIR), the filter's time response to an impulse is the weighted sum of the present and a finite number of previous input values. With no feedback, its only response to a given sample ends when the sample reaches the \"end of the line\". With these design differences in mind, DSP software is chosen for its flexibility and simplicity over analog circuitry filter designs.\nWhen creating this bandpass filter, using DSP is not a terrible task to complete. Implementing it and manufacturing the filters is much easier, as you only have to program the filters the same with every DSP chip going into the device. However, using analog components, you have the risk of faulty components, adjusting the circuit and program the filter on each individual analog circuit. DSP creates an affordable and less tedious way of filter design for signal processing and increases accuracy for tuning and adjusting filters in general.\nADC & DAC\nElectric equipment is heavily used in nearly every field. Analog to Digital Converters (ADC) and Digital to Analog Converters (DAC) are essential components for any variation of DSP in any field. These two converting interfaces are necessary to convert real world signals to allow for digital electronic equipment to pick up any analog signal and process it. Take a microphone for example: the ADC converts the analog signal collected by an input to audio equipment into a digital signal that can be outputted by speakers or monitors. While it is passing through the audio equipment to the computer, software can add echoes or adjust the tempo and pitch of the voice to get a perfect sound. On the other hand, DAC will convert the already processed digital signal back into the analog signal that is used by audio output equipment such as monitors. Below is a figure showing how the previous example works and how its audio input signals can be enhanced through reproduction, and then outputted as digital signals through monitors.\nA type of analog to digital converter, known as the digital ramp ADC, involves a comparator. The value of the analog voltage at some point in time is compared with a given standard voltage. One way to achieve this is by applying the analog voltage to one terminal of the comparator and trigger, known as a binary counter, which drives a DAC. While the output of the DAC is implemented to the other terminal of the comparator, it will trigger a signal if the voltage exceeds the analog voltage input. The transition of the comparator stops the binary counter, which then holds the digital value corresponding to the analog voltage at that point. The figure below shows a diagram of a digital ramp ADC.\nApplications of DSP\nThere are numerous variants of a digital signal processor that can execute different things, depending on the application being performed. Some of these variants are audio signal processing, audio and video compression, speech processing and recognition, digital image processing, and radar applications. The difference between each of these applications is how the digital signal processor can filter each input. There are five different aspects that varies from each DSP: clock frequency, RAM size, data bus width, ROM size, and I/O voltage. All of these components really are just going to affect the arithmetic format, speed, memory organization, and data width of a processor.\nOne well-known architecture layout is the Harvard architecture. This design allows for a processor to simultaneously access two memory banks using two independent sets of buses. This architecture can execute mathematical operations while fetching further instructions. Another is the Von Neumann memory architecture. While there is only one data bus, operations cannot be loaded while instructions are fetched. This causes a jam that ultimately slows down the execution of DSP applications. While these processors are similar to a processor used in a standard computer, these digital signal processors are specialized. That often means that, to perform a task, the DSPs are required to used fixed-point arithmetic.\nAnother is sampling, which is the reduction of a continuous signal to a discrete signal. One major application is the conversion of a sound wave. Audio sampling uses digital signals and pulse-code modulation for the reproduction of sound. It is necessary to capture audio between 20 - 20,000 Hz for humans to hear. Sample rates higher than that of around 50 kHz - 60 kHz cannot provide any more information to the human ear. Using different filters with DSP software and ADC's & DAC's, samples of audio can be reproduced through this technique.\nDigital signal processing is heavily used in day-to-day operations, and is essential in recreating analog signals to digital signals for many purposes.\nI hope that this article has provided enough information to get a general understanding of what DSPs are, how they work, and what they are specifically used for in a plethora of fields. If you have any questions or thoughts, please leave a comment below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:eeb6c2c7-ba12-40f5-866a-13d00e5d77dd>","<urn:uuid:d338626b-3cf1-4c35-9111-c24e55c1827b>"],"error":null}
{"question":"As a student of royal residences, who commissioned the original construction of Nymphenburg Palace and what was the occasion?","answer":"Elector Ferdinand commissioned the palace in 1662 as a gift to his wife, Henrietta Adelaide of Savoy, to celebrate the birth of their heir Maximilian Emanuel. The foundation stone was laid in 1664 for a villa suburbana in the Italian style.","context":["Nymphenburg Palace, the extensive summer residence of the\nBavarian electors and kings is one of the most individualistic but significant palaces in\nEurope, and its excellent decoration has remained almost intact.\nNymphenburg Palace History\nOn the birth in 1662 of his heir Maximilian Emanuel, Elector Ferdinand gave\nhis wife, Henrietta Adelaide of Savoy (1636–1676), some land for a palace west of Munich. In\n1664 the foundation stone was laid for a villa suburbana in the Italian style, the ‘Borgo delle\nUnder the direction of Agostino Barelli and from 1673 of Enrico Zuccalli,\nthe central cubic block was erected, with domed pavilions linked to it by galleries. Its five\nstoreys were separated by string courses; and for a short time it had a roof with dormers. A\nsymmetrical external staircase with twin flights led to the central portal. An Italian garden\nlay to the east.\nThe internal arrangement has changed little. At the centre was the\nSteinerner Saal, adjoined by the electoral suites. Antonio Triva (1626–1699), Joseph Werner II,\nAntonio Zanchi, Stefano Catani and others collaborated in the decorative paintings.\nBuilding work slackened on the electress’s death and ceased in 1680, to be\nresumed only in 1701 when Maximilian II Emanuel returned from the Netherlands and commissioned\nEnrico Zuccalli to plan the next stage, directed from 1702 by Giovanni Antonio Viscardi. To the\nexisting main building the Elector added two slightly projecting cubic side pavilions, probably\non the foundations of the earlier ones, connected to the central pavilion by arcaded\nThe loose linking of the buildings and the view of the park from the arcades\nand the passage through the socle storey of the middle building distinguish the\nNymphenburg, as a garden-palace, from such closed structures as Versailles Palace. Its models were not French, however,\nbut Netherlandish, Maximilian II Emanuel’s ideas having been influenced, in particular, by\nHet loo at Apeldorn.\nDuring the same period the middle pavilion was altered on both the town and\nthe park side, the five middle bays being reduced to three and large, round-arched windows\nadded on the first and second floors. Inside, arcades opened from the two rooms one above the\nother on the garden side, into the Steinerner Saal, which thus received more light.\nArticulated with colossal pilasters, the Steinerner Saal had a decorative\nscheme by Johann Anton Gumpp (1701–3; apparently replaced 1726 by paintings on leather [destr.]\nby Domenico Valeriani: d before 1771; and Giuseppe Valeriani: d 1761), similar to that in\nSchloss Lustheim, showing scenes from the myth of Diana.\nFurther enlargement was delayed by the outbreak of the War of the Spanish\nSuccession (1704) and fully resumed only in 1715, under Josef Effner. He converted the lesenes\nof the main pavilion into pilasters and on the garden side extended them to the cornice.\nThe four bays on either side of the central section were reduced to three,\nand large triangular pediments (removed 1826) were added above the three central bays. In\n1715–16 Effner elaborated the interior and produced plans (unexecuted) for subsidiary buildings\non each side. One of the first rooms to receive Régence panelling, by Johann Adam Pichler, was\nthe north anteroom, from around 1716.\nThe Marstall (now Marstallmuseum) with courtiers’ accommodation was built to\nthe south, the orangerie with other buildings to the north, while the galleries over the\ncanals, which linked the whole complex together, were not constructed until 1739 and 1747. The\nlong façades with middle and corner pavilions, the blind arcades and the segmental arched\nwindows clearly show the influence of such French architects as Robert de Cotte and,\nespecially, Germain Boffrand.\nUnder Elector Maximilian III Joseph, some rooms were altered and\nredecorated. The most important work of this phase is the decoration of the Steinerner Saal by\nZimmermann and de Cuvilliés, focusing on Zimmerman’s fresco Nymphs Paying Homage to the Goddess\nFlora. The decoration of the Gartensaal and the Emporensaal was designed by de Cuvilliés.\nFrom 1804 to 1823 the Baroque garden, made up of symmetrical groups of\nshrubbery, was extensively modified in the style of an English landscape garden. The last phase\nof alterations to the palace took place under Maximilian IV Joseph, who chose it as his\nfavourite residence after he became king (1806). Karl Ludwig Puille began remodelling the rooms\nof the first pavilions to the north and south from 1806. In 1826 Klenze adapted the central\nbuilding to the Neo-classical style.\nAddress: Schloss Nymphenburg 19, 80638 München, Germany. Get directions using the map\nView Larger Map"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:fa8e9fe0-0a45-4c20-963b-9527e1fa2563>"],"error":null}
{"question":"What are the recommended practices for interacting with marine wildlife in Hawaii's waters, and what are the primary threats these ecosystems face from human activities?","answer":"For safe marine wildlife interaction in Hawaii, maintain appropriate distances (50 yards from monk seals), avoid touching any marine life, and move away if animals show signs of distress like rapid swimming direction changes or escape tactics. Do not feed marine life as it can cause illness and disrupt natural populations. For sea turtles, allow space for them to surface and breathe. As for threats, coral reefs face significant challenges from human activities including overfishing, which affects over 55% of world's reefs, and tourism impacts such as coral breakage from direct contact during activities like snorkeling. Coastal development, wastewater pollution, and recreational activities also pose serious threats through sediment runoff, pathogen transmission, and physical damage to coral colonies.","context":["Show Hawaiian Marine Life Aloha\nTake nothing but pictures – touch nothing but hearts. The desire to care for our natural environment is instinctual, and that desire becomes stronger the closer we are to that environment, physically or emotionally. Every year, over 5 million people will venture into Hawaii’s near shore waters.\nPlease do not attempt to touch, ride, hug, kiss, feed or pet Hawaii’s marine wildlife.\nAlthough we can’t guarantee the “mood” or sightings of particular marine life, we can guarantee that by practicing the guidelines below you will ensure the best wildlife encounters possible during your stay in Hawaii.\nCautiously move away if you observe any of the following behaviors:\n- Rapid changes in swimming direction or speed\n- Escape tactics such as prolonged diving, underwater course changes, or rapid swimming away from your location at the surface.\n- An adult attempting to shield calf with body or by movements.\n- Sudden stop in important breeding, nursing, feeding or resting activities upon your arrival.\n- Review swim encounter guidelines\n- Sea Turtles require air to live. If you see them rising to surface, give them room to breath. Ensure that your presence does not disturb them.\n- Cautiously move away if you observe any of the following behaviors:\n- Sudden awakening from a sleep-like state on the seafloor.\n- Movement away from or increase in swim speed upon disturbance.\n- “Yawning” motions or “Flipper Swipe” over their eye\nFish and Coral Reefs\n“It takes 1000 years for a 1000 year old reef to repair itself.” – Dave Gulko\n- No peas please! Feeding fish frozen peas or any other food can cause illness or death and disrupt natural fish populations.\n- Do not attempt to touch fish or any marine life.\n- In shallow water, avoid stepping on or touching coral heads.\n- Watch “Hawaii Reef Etiquette” for a head start on showing our oceans Aloha.\nHawaiian Monk Seals\n- Hawaiian monk seals are one of the most endangered seal species in the world.\n- Ensure that your presence does not disturb them; observe them from at least 50 yards away .\n- In the ocean, monk seals may show inquisitive behavior. Do not attempt to approach these seals or “play” with them. The seals may misread your actions, can move quicker than you think which can result in serious injury. Watch them from a safe vantage point.\nIn-Water Best Practices\nAs marine operators, we will attempt to avoid influencing the natural behavior of any marine life, and will encourage our guests to do the same. It is our intention to provide as natural a viewing experience as possible for our guests while interacting with the marine environment.\n- Avoid touching or taking any marine organism or their habitat, living or not, (rocks, turtles, dolphins, whales, fish, or invertebrates, etc.).\n- Be aware of our body and equipment so as to avoid contact with the reef or other marine life.\n- Not approach any marine life within arm’s reach for any reason.\n- Not approach turtles within 3 body lengths and will avoid impeding a turtle’s path to the surface for breathing.\n- Protect ourselves and marine life and their habitat by not FEEDING them to; make animals less agressive, protect marine life from injesting harmful foods, protect the balance of the marine ecosystem, and let the fish do their job.\n- Apply sunscreen 30 minutes before entering the water. Reef safe sunscreen is preferred, we have this onboard.\n- Use biodegradable defog.","Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:fa19c2e2-b97f-4509-a1ea-dc860177765b>","<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>"],"error":null}
{"question":"I'm a construction site supervisor. How does choosing the right equipment affect project success, and what Personal Protective Equipment (PPE) should workers wear for maximum safety?","answer":"Choosing the right equipment is crucial for project success as it can mean the difference between a successful job and an expensive disaster. The equipment must be reliable, efficient, and able to complete work within timeline and budget. Important factors include rental costs, maintenance needs, equipment availability, and proper insurance coverage to protect against accidents. For worker safety, essential PPE includes hard hats to protect against falling objects, safety glasses and face shields for eye protection, ear plugs and muffs for hearing protection, back support belts for lifting, and protective gloves and steel-toed boots for hands and feet protection. Additionally, workers must wear fall protection equipment like safety harnesses, respiratory protection against hazardous materials, and high visibility vests for easy spotting in emergencies.","context":["Making the right choice for construction equipment is an important decision when starting any kind of project. It can mean the difference between a successful job and an expensive disaster, so it’s important to be informed about which type of equipment will be best suited for your particular task. From choosing appropriate machines such as cement mixers or earth movers to understanding what rental rates and features are available, there are plenty of factors that must be considered before selecting any given piece of construction hardware. In this blog post, we explore some key points that you need to know in order to make sure you’re getting the best value from your next construction purchase.\nResearch the Available Construction Equipment\nPlanning a construction project requires a lot of thought and consideration. One of the most important decisions you’ll have to make is deciding on the right equipment to use. With so many options available, it can be overwhelming trying to determine which will work best for your project. Acquire top-notch construction equipment at Deslauriers, for example, and ensure a foundation of excellence for your building projects. You want your equipment to be reliable, and efficient and can get the job done within your timeline and budget. By researching your available options and considering factors such as the size and complexity of your project, you can make an informed decision that will set you up for success. The right equipment can make all the difference, so take the time to thoroughly assess your needs before making a final decision.\nConsider Rental Costs, Maintenance Needs, and Availability of the Equipment\nWhen it comes to renting equipment, there are a few key factors to consider before making your final choice. It’s important to think about rental costs and ensure that the equipment you need fits within your budget. Additionally, you’ll want to consider maintenance needs and ensure that any necessary upkeep won’t be too expensive or time-consuming. Finally, the availability of the equipment is crucial – make sure you can get your hands on it when you need it so you don’t end up stuck without the tools you require. By taking all of these factors into account, you can ensure that your rental experience is a success.\nDiscuss Safety Features and Accessories\nWhen it comes to workplace safety, there are many precautions that can be taken to keep employees protected from harmful conditions. Using safety accessories such as noise reduction headsets, protective goggles or dust masks can help to reduce the risk of injury and illness. These accessories can protect workers from loud noises, flying particles, toxic fumes, and harmful dust. Noise reduction headsets, for example, can help prevent hearing loss in noisy environments. While protective goggles and dust masks can be used to safeguard against dust particles and other contaminants in the air. When workers feel safe and protected on the job, they can focus on getting their work done without worrying about potential hazards.\nConfirm that You Have the Right Insurance\nOperating heavy construction equipment comes with inherent risks, which is why having the right insurance in place is crucial. You need to make sure that your liabilities are covered in the event of accidental damage or injury caused by the machinery. The last thing you want is to face hefty financial consequences that could have easily been avoided with sufficient insurance coverage. With the right policy in place, you can focus on getting the job done while having peace of mind that you are protected in the event of an unfortunate incident. Don’t leave your construction business vulnerable – take the necessary steps to confirm that you have the right insurance coverage in place.\nMake Sure you Know How to Properly Operate the Chosen Equipment\nBefore beginning to use any equipment, it is important to first ensure that you have a thorough understanding of how to properly operate and maintain it. This can help prevent accidents, and damage to the equipment, and ensure that you accomplish the task at hand efficiently and effectively. By taking the time to read the owner’s manual and watching any instructional videos, you can gain a comprehensive understanding of the equipment’s capabilities, limitations, and potential hazards. Additionally, regularly maintaining equipment, such as cleaning and lubricating moving parts, can prolong its lifespan and prevent breakdowns. Overall, taking the necessary steps to properly operate and maintain equipment can make all the difference in ensuring a safe and successful operation.\nOne of the most important steps in taking on any construction project is selecting the right equipment. Choosing the correct equipment for the job can help to ensure that it is completed on time and within budget while also protecting safety and reducing liabilities. Taking into account factors such as rental rates, maintenance needs, power source requirements, safety features, and insurance coverage can help guide you toward making the best decision based on your needs. Understanding how to properly use the chosen construction equipment is also essential for ensuring it is used safely and correctly while achieving maximum efficiency. With careful research and planning of these items, you can select suitable construction equipment that will take your project to new heights.","When you imagine a construction worker’s daily uniform, typical things that may come to mind are the safety glasses, the reflective vest, and the classic hard hat. While they might appear to be just part of a uniform, all of these accessories are essential to conducting workplace safety and are some of the main items worn as a part of a construction worker’s personal protective equipment. Personal protective equipment is crucial to safety on the job site, so below we’ll fill you in on what it is, why it’s worn, and how you can use it to better improve safety for your workers.\nWhat is Personal Protective Equipment\nIt’s a no-brainer that personal protective equipment is designed to protect oneself from the dangers they may encounter during work in their field, but what exactly is it defined as? Personal protective equipment (PPE) is a general term for wearable equipment and gear that’s meant to protect the wearer from hazards and dangers on the job site. Although construction is one of the largest industries utilizing personal protective equipment, it is used in many other fields too, like the military, the police, and firefighting.\nIn the construction industry, workers use all kinds of equipment and materials. Because of this, there is an abundance of protective gear available to protect against the possible dangers a worker can encounter on site. Below are the main areas of the body requiring protective gear and items used to protect each region:\nThe head > includes hard hats to protect against falling objects and blows to the head.\nThe eyes > includes safety glasses, face shields, and chemical splash goggles to protect against particles from getting into the eyes when working in certain areas like welding or handling hazardous materials.\nThe ears > includes ear plugs and ear muffs to protect against loud noise that can occur on site.\nThe back > includes support belts to protect against muscle strains that can come from lifting heavy objects.\nThe hands and feet > includes snug, insulated gloves and puncture-resistant, steel-toed boots to protect against shocks from working with electrical wires or objects from crushing the feet..\nPersonal protective equipment includes more than just protective clothing. There are three other important categories including fall protection, respiratory protection, and visibility. Falling is one of the leading causes of death on construction sites, so using appropriate protective gear like a safety harness is one way to prevent this danger. Protecting the lungs is also important, so using respirators to prevent the inhalation of hazardous gases, vapors, and particles is another way to practice good safety. The visibility of your workers is also important on construction sites, so enforcing the usage of high visibility vests keeps workers easy to spot, ensuring the ability to quickly find anyone in case of an emergency.\nThe Risks Involved\nDue to the many potential dangers that workers can face in the construction industry, wearing personal protection equipment is necessary. Depending on the job site, workers can face a number of risks including falling, electrical shocks, blunt trauma, and getting caught in between objects, all of which can cause death in serious cases. Construction workers can experience a number of serious nonfatal injuries too, like chemical burns, broken bones, and severed fingers. Due to the severity of these risks, companies can suffer huge fines for the non-usage of safety equipment, and in some cases can face massive legal trouble.\nImproving Construction Site Safety\nConstruction site safety should be a priority for employers, so it’s important to practice good safety procedures through the usage of personal protective equipment. One way to practice this is to make sure your employees understand how to use their equipment, what it protects them from, and even more important: what it does not protect them from. Personal protective equipment has to fit properly to ensure its maximum protective ability, and proper care and maintenance are also equally important in practicing good safety measures. Workers also need to know when to replace equipment, especially when it comes in contact with hazardous materials, or gets damaged as this reduces its maximum protective ability.\nUnderstanding why PPE needs to be worn is important as big fines, legal trouble, serious injuries, and even death can occur from a lack of or improper usage. Ultimately, personal protective equipment improves construction site safety which is why it’s important to promote a safe working environment with fit testing and ensuring all occupational and health safety requirements are met."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:226b0251-74ac-4bce-9dcb-f2060dd70123>","<urn:uuid:0f74e8e6-fd9c-42c9-8a94-b4c3edb30984>"],"error":null}
{"question":"Which city has more complete historical founding story - Rome or Carthage?","answer":"Rome has a more detailed founding story, with multiple interconnected legends including the tale of Romulus and Remus (twins raised by a wolf), and the story of Aeneas, a Trojan prince who came to Italy and whose descendants would lead to Rome's foundation. These stories are supported by extensive myths, multiple historical sources debating the exact founding date, and archaeological evidence like fortification walls from the 8th century BCE. In contrast, Carthage's founding story is simpler, primarily centered around the legend of Queen Dido (also known as Elissa) who fled from Phoenicia and established the city around 814 BCE, with the story of how she cleverly obtained land by cutting an ox hide into thin strips to claim the hill called Byrsa.","context":["Founding of Rome\nThe founding of Rome can be investigated through archaeology, but traditional stories handed down by the ancient Romans themselves explain the earliest history of their city in terms of legend and myth.\nThe most familiar of these myths, and perhaps the most famous of all Roman myths, is the story of Romulus and Remus, the twins who were suckled by a she-wolf. This story had to be reconciled with a dual tradition, set earlier in time, the one that had the Trojan refugee Aeneas escape to Italy and found the line of Romans through his son Iulus, the namesake of the Julio-Claudian dynasty.\nThe national epic of mythical Rome, the Aeneid of Virgil, tells the story of how Trojan prince Aeneas came to Italy. The Aeneid was written under Augustus, who claimed ancestry through Julius Caesar from the hero and his mother Venus. According to the Aeneid, the survivors from the fallen city of Troy banded together under Aeneas and underwent a series of adventures around the Mediterranean Sea, including a stop at newly founded Carthage under the rule of Queen Dido, eventually reaching the Italian coast. The Trojans were thought to have landed in an area between modern Anzio and Fiumicino, southwest of Rome, probably at Laurentum or, in other versions, at Lavinium, a place named for Lavinia, the daughter of King Latinus whom Aeneas married. This started a series of armed conflicts with Turnus over the marriage of Lavinia. Before the arrival of Aeneas, Turnus was engaged to Lavinia, who then married Aeneas, starting the war. Aeneas won the war and killed Turnus. The Trojans won the right to stay and to assimilate with the local peoples. The young son of Aeneas Ascanius, also known as Iulus, went on to found Alba Longa and the line of Alban kings who filled the chronological gap between the Trojan saga and the traditional founding of Rome in the 8th century BC.\nToward the end of this line, King Procas was the father of Numitor and Amulius. At Procas' death, Numitor became king of Alba Longa, but Amulius captured him and sent him to prison; he also forced Numitor's daughter Rhea Silvia to become a virgin priestess among the Vestals. For many years, Amulius was the king. The tortuous nature of the chronology is indicated by Rhea Silvia's ordination among the Vestals, whose order was traditionally said to have been founded by Romulus's successor Numa Pompilius.\nRomulus and Remus\nThe myth of Aeneas was of Greek origin and had to be reconciled with the Italian myth of Romulus and Remus, who would have been born around 771 BC if taken as historical figures. They were purported to be sons of Rhea Silvia and either Mars, the god of war, or the demi-god hero Hercules. They were abandoned at birth, in the manner of many mythological heroes, because of a prophecy that they would overthrow their great-uncle Amulius, who had overthrown Silvia's father Numitor. They were abandoned on the Tiber River by servants who took pity on the infants, despite their orders. The twins were nurtured by a she-wolf until a shepherd named Faustulus found the boys and took them as his sons. Faustulus and his wife Acca Larentia raised the children. When Remus and Romulus became adults, they killed Amulius and restored Numitor. They decided to establish a city; however, they quarreled, and Romulus killed his brother. Thus, Rome began with a fratricide, a story that was later taken to represent the city's history of internecine political strife and bloodshed.\nThe ancient Romans were certain of the day Rome was founded, April 21, the day of the festival sacred to Pales, goddess of shepherds, on which date they celebrated the Par ilia (or Palilia). However, they did not know the exact year the city had been founded; this is one reason they preferred to date their years to the presiding consuls over using the formula A.U.C. or Ab Urbe Condita. Several had been proposed by ancient authorities, and Dionysius of Halicarnassus records these: the Greek historian Timaeus, the first to write a history of the Romans, stated that Rome was founded in the 38th year prior to the first Olympiad, or 814 BC; Quintus Fabius Pictor, the first Roman to write the history of his people, stated Rome was founded in the first year of the eighth Olympiad, or 748/7 BC; Cincius Alimentus claimed Rome was founded in the fourth year of the twelfth Olympiad, or 719/8 BC; and Cato the Elder calculated that Rome was founded 432 years after the Trojan War, which Dionysius stated was the first year of the seventh Olympiad, or 752/3 BC. Dionysius himself provided calculations showing that Rome was founded in 751 BC, starting with the Battle of the Allia, which he dated to the first year of the ninth Olympiad, or 390 BC, then added 119 years to reach the date of the first consuls, Junius Brutus and Tarquinius Collatinus, then added the combined total of the reigns of the Kings of Rome (244 years) to arrive at his own date, 751 BC. Even the official Fasti Capitolini offers its own date, 752 BC.\nThe most familiar date given for the foundation of Rome, 753 BC, was derived by the Roman antiquarian Titus Pomponius Atticus, and adopted by Varro, having become part of what has come to be known as the Varronian chronology. An anecdote in Plutarch where the astrologer Lucius Tarrutius of Firmum provides an argument based on a non-existent eclipse and other erroneous astronomical details that Rome was founded in 753 BC suggests that this had become the most commonly accepted date. Through its use by the third-century writer Censorinus, whose De Die Natali was the ultimate influence of Joseph Justus Scaliger's work to establish a scientific basis of ancient chronology, it became familiar.\nRecent discoveries by Andrea Carandini on Rome's Palatine Hill have also yielded evidence of a series of fortification walls on the north slope that can be dated to the middle of the 8th century BC. According to the legend, Romulus plowed a furrow (sulcus) around the hill in order to mark the boundary of his new city.\nThe name of Rome\nThere is no consensus on the etymology of the city's name. Jean-Jacques Rousseau (1712–1778) suggested Greek \"ῥώμη\" (rhōmē), meaning \"strength, vigor\". A modern theory of etymology holds that the name of the city is of Etruscan origin (and perhaps the city itself, though this cannot be proven), derived from rumon, \"river\".\n|This section needs additional citations for verification. (December 2011) (Learn how and when to remove this template message)|\nThe historical Latins were originally an Italic tribe who inhabited the Alban Hills.[when?] They later moved down into the valleys, which provided better land for agriculture. The area around the Tiber river was particularly advantageous and offered notable strategic resources; the river was a natural border on one side, and the hills could provide a safe defensive position on the other side. This position would also have enabled the Latins to control the river and the commercial and military traffic on it from the natural observation point at Isola Tiberina. Moreover, road traffic could be controlled, since Rome was at the intersection of the principal roads to the sea coming from Sabinum (in the northeast) and Etruria (to the northwest).\nThe development of the town is presumed to have started from the development of separate small villages located at the tops of hills that eventually accreted to form Rome. In any case, the location that became the city of Rome was inhabited by Latin settlers from various regions, farmers and pastoralists, as evidenced by differences in pottery and burial techniques.\nRecent studies suggest that the Quirinal hill was very important in ancient times, although the first hill to be inhabited seems to have been the Palatine (therefore confirming the legend), which is also at the center of ancient Rome. Its three peaks, the minor hills (Cermalus or Germalus, Palatium, and Velia), were united with the three peaks of the Esquiline (Cispius, Fagutal, and Oppius), and then villages on the Caelian Hill and Suburra.\nThese hills had expressive names. The Caelian Hill was also called Querquetulanus (from \"quercus\", oak) and \"Fagutal\" (pointing to beech-woods, from \"fagus\" meaning \"beech\"). Recent discoveries revealed that the Germalus on the northern part of the Palatine was the site of a village (dated to the 9th century BC) with circular or elliptical dwellings. It was protected by a clay wall (perhaps reinforced with wood), and it is likely that this is where Rome was really founded.\nThe territory of this federation was surrounded by a sacred border called the pomerium, which enclosed the so-called e Servian expansion of Rome.\nFestivals for the Septimontium (literally \"of the seven hills\") on December 11 were previously considered to be related to the foundation of Rome. However, April 21 is the only date for Rome's foundation upon which all the legends agree, and it has recently been argued that Septimontium celebrated the first federations among Roman hills. A similar federation was, in fact, celebrated by the Latins at Cave, Italy or at Monte Cavo (in Castelli).\nDuring the Italian Renaissance, a group of humanists affiliated with the Roman Academy formed a sodality to pursue antiquarian interests, celebrating the \"birthday of Rome\" annually on April 20. In 1468, the Academy was suppressed by Pope Paul II for fomenting \"republicanism, paganism, and conspiracy\", but the sodality was reinstated about ten years later under Sixtus IV as the Societas Literatorum S. Victoris in Esquiliis (\"Literary Society of Saint Victor on the Esquiline\"). The reformed group placed itself under the new patronage of saints Victor, Fortunatus, and Genesius, \"whose feast day was conveniently proven to coincide with the Palilia\". Their \"Palilia\" was organized by Pomponio Leto and featured speeches, a communal meal, and a poetry competition.\n- Livy (1797). The history of Rome. George Baker (trans.). Printed for A. Strahan.\n- Livy (2005-05-26). The Early History of Rome. Penguin Books Ltd. ISBN 978-0-14-196307-5.\n- \"Turnus\". Encyclopaedia Britannica. Retrieved 2013-09-13.\n- Dionysius of Halicarnassus, Roman Antiquities, 1.74\n- Dionysius of Halicarnassus, 1.75\n- Gary Forsythe, A Critical History of Early Rome (Berkeley: University of California, 2005), p. 94\n- Anthony Grafton and Noel Swerdlow, \"Technical Chronology and Astrological History in Varro, Censorinus, and Others\", Classical Quarterly, N.S. 35 (1985), p. 454-65\n- Cf. Jean-Jacques Rousseau, The Social Contract, Book IV, Chapter IV, written in 1762, where he writes in a footnote that the word for Rome is Greek in origin and means force. \"There are writers who say that the name 'Rome' is derived from 'Romulus'. It is in fact Greek and means force.\"\n- Baldi, Philip (2002). The Foundations of Latin. Walter de Gruyter. p. 106-7.\n- Angela Fritsen, \"Ludovico Lazzarelli's Fasti Christianae religionis: Recipient and Context of an Ovidian Poem,\" in Myricae: Essays on Neo-Latin Literature in Memory of Jozef Ijsewijn (Leuven University Press, 2000), pp. 121–122.\n- Carandini, Andrea (2011). Rome: Day One. Princeton, N.J: Princeton University Press. ISBN 978-0-691-13922-7.\n- Forsythe, Gary (2005). A Critical History of Early Rome: From Prehistory to the First Punic War. Berkeley: University of California Press. ISBN 978-0-520-22651-7.\n- Raaflaub, Kurt A. (2005). Social struggles in archaic Rome: new perspectives on the conflict of the orders. Malden, Mass: Blackwell Publishing. ISBN 978-1-4051-0060-1.","Carthage was a Phoenician city-state on the coast of North Africa (the site of modern-day Tunisia) which, prior the conflict with Rome known as the Punic Wars (264-146 BCE), was the largest, most affluent, and powerful political entity in the Mediterranean. The city was originally known as Kart-hadasht (new city) to distinguish it from the older Phoenician city of Utica nearby. The Greeks called the city Karchedon and the Romans turned this name into Carthago.\nIt was founded c. 814 BCE by the legendary Phoenician queen Dido, increased in size after an influx of refugees from the city of Tyre following Alexander the Great’s conquests of 332 BCE, and afterwards expanded until it was the seat of the Carthaginian Empire with colonies (such as Sabratha) along the North African coast, in Sicily, Spain, and elsewhere; these would all be lost following the Punic Wars which elevated Rome to Carthage’s former position as the greatest Mediterranean power.\nThe history of the ancient city is usually divided into five periods:\n- Ancient Carthage (Punic Republic) – c. 814-146 BCE\n- Roman Carthage – 146 BCE - 439 CE\n- Vandal Carthage – 439-534 CE\n- Byzantine Carthage (Exarchate of Africa) – 534-698 CE\n- Muslim Arab Carthage (Islamic Carthage) – 698-1270 CE\nOwing to limitations of space, this article will primarily deal with Ancient Carthage/the Punic Republic.\nIn 698 CE, the city was conquered during the Muslim Arab invasion of North Africa and destroyed. It would be rebuilt, though on a modest scale compared with the city at its height, until it was completely destroyed under the reign of Muhammad I al-Mustansir (r. 1228-1277 CE) after defeating the European Christian invasion of the Eighth Crusade of 1270 CE. The site would continue to be inhabited, though the ancient ruins were neglected until the 1830s CE when modern excavations began.\nFoundation & Expansion\nAccording to legend, Carthage was founded by the Phoenician Queen Elissa (better known as Dido) c. 814 BCE; although Dido's historicity has been challenged, the founding does date to about this time. Dido was allegedly fleeing the tyranny of her brother Pygmalion of Lebanon, landed on the coast of North Africa, and established the city on the high hill later known as the Byrsa. The legend claims that the Berber chieftain who controlled the region told her she could have as much land as an ox hide would cover; Dido cut a single ox hide into thin strips and lay them end-to-end around the hill, successfully claiming it for her people.\nDido’s reign is described by the Roman poet Virgil (l. 70-19 BCE), and others, as impressive, noting how the city grew from the small community on the hill to a grand metropolis. This account, and others like it, are legendary but Carthage, which seems to initially have been a minor port on the coast where Phoenician traders stopped to resupply or repair their ships, was clearly a major center of trade by the 4th century BCE.\nThe city developed significantly following Alexander's destruction of the great industrial and trade center of Tyre (considered Carthage’s mother-city) in 332 BCE when Phoenician refugees fled from there to Carthage. These Tyrians arrived with whatever wealth they had and, since many whom Alexander spared were those rich enough to buy their lives, they landed in the city with considerable means which established Carthage as the new center of Phoenician trade.\nThe Carthaginians then established a working relationship with the tribes known as the Masaesyli and the Massylii of the North African Berber (Imazighen) Kingdom of Numidia who would fill the ranks of their military, primarily as formidable cavalry troops. From a small town on the coast, the city grew in size and grandeur with enormous estates covering miles of acreage. Carthage quickly became the richest and most powerful city in the Mediterranean.\nCarthaginian government, formerly a monarchy, was a republic based on meritocracy (rule of the elite) by the 4th century BCE. The top position was held by two elected magistrates known as suffetes (“judges”) who governed in conjunction with a senate of between 200-300 members who held the position for life. Laws were passed by an assembly of citizens who would vote on measures proposed by the suffetes and senate. The aristocrats lived in palaces, the less affluent in modest but attractive homes, and the lower classes in apartments or huts outside the city.\nTribute and tariffs regularly increased the city’s wealth on top of the lucrative business in maritime trade. The city’s harbors were immense, with 220 docks, and gleaming columns which rose around it in a half-circle, in front of towering arches and buildings ornamented with Greek sculpture. There were two harbors, one for trade and the other for warships, which operated constantly in resupplying, repairing, and outfitting vessels. The Carthaginian trading ships sailed daily to ports all around the Mediterranean Sea while their navy, supreme in the region, kept them safe and, also, opened new territories for trade and resources through conquest as the Carthaginians built their empire.\nThe city had four residential sections, which grew up around the citadel of the Byrsa in the center, and was surrounded by walls which stretched 23 miles (37 kilometers) in length from the harbors inland. The city had all the accommodations and refinements of any great ancient city – a theater for entertainment, temples for religious observances, a necropolis, an agora (marketplace) – but on a much grander scale. Its patron deity was the goddess of love and fertility, Tanit who was worshipped alongside her consort Baal-Hamon. It is possible that children were sacrificed to Tanit in the sacred precinct known as the Tophet, but this claim has been challenged, and it is equally likely that the Tophet of Carthage was simply a necropolis reserved for infants and the young.\nAffluence & Invasion\nThe city’s wealth was due not only to its advantageous position on the North African coast, from which it could control sea traffic between itself and its colony on Sicily, but also to the people’s skill in agriculture. The writer Mago of Carthage (dates unknown) wrote a work of 28 volumes devoted to agriculture and veterinarian science which was considered the most comprehensive on the subject of its time and reflects the Carthaginian’s intense interest in farming and animal husbandry. Mago’s works were considered so important that they were among the few that would be spared by the Romans after Carthage’s final defeat in 146 BCE. Roman references to the books are now all that remain of them.\nThe Carthaginians planted fruit trees, grapes, olive trees, and vegetables in a ring of gardens irrigated by small canals and then expanded their cultivation outward beyond the city walls to fields of grains. The fertility of the land, and their expertise in cultivation, increased the city’s wealth through trade with the interior as well as maritime trade elsewhere as Carthage continued to flourish.\nIt was this expansion that first brought Carthage into conflict with others. In 310-307 BCE, North Africa was invaded by Agathocles of Syracuse (r. 317-289 BCE) who sought to subdue Carthage and use her wealth to fund his wars. Agathocles was able to feed his army easily off the land because the crops grew in such abundance. He was only defeated because the Libyans and Berbers, who worked the land, sided with the Carthaginians who had treated them well. Agathocles was driven from North Africa and Carthage continued to prosper until it became involved in a conflict with Rome, then just a small city-state on the Tiber River in Italy, in 264 BCE.\nThe Punic Wars\nControl of Sicily was divided between Rome and Carthage who supported opposing factions on the island which quickly brought both parties into conflict directly with each other. These conflicts would be known as the Punic Wars from the Phoenician word for the citizens of Carthage (given in Greek as Phoinix and in Latin as Punicus). When Rome was weaker than Carthage, they posed no threat. The Carthaginian navy had long been able to enforce the treaty which kept the Roman Republic from trading in the western Mediterranean. When the First Punic War (264-241 BCE) began, however, Rome proved far more resourceful than Carthage could have imagined.\nThough they had no navy and knew nothing of fighting on the sea, Rome quickly built 330 ships which they equipped with clever ramps and gangways (the corvus) which could be lowered onto an enemy ship and secured; thus turning a sea battle into a land battle. After an initial struggle with military tactics, Rome won a series of victories and finally defeated Carthage in 241 BCE. Carthage was forced to cede Sicily to Rome and pay a heavy war indemnity.\nFollowing this war, Carthage became embroiled in what is known as The Mercenary War (241-237 BCE) which started when the Carthaginian army of mercenaries demanded the payment Carthage owed them. This war was finally won by Carthage through the efforts of the general Hamilcar Barca (l. c. 285 - c. 228 BCE), father of the famous Hannibal Barca (l. 247-183 BCE) of the Second Punic War.\nCarthage suffered greatly from the First Punic and Mercenary War and, when Rome occupied the Carthaginian colonies of Sardinia and Corsica, there was nothing the Carthaginians could do about it. They tried to make the best of their situation by expanding holdings in Spain but again went to war with Rome when Hannibal attacked the city of Saguntum, an ally of Rome in Spain, in 218 BCE.\nThe Second Punic War (218-202 BCE) was fought largely in northern Italy as Hannibal invaded Italy from Spain by marching his forces over the Alps. Hannibal won every engagement against the Romans in Italy. In 216 BCE he won his greatest victory at the Battle of Cannae but, lacking sufficient troops and supplies, could not build on his successes. He was finally drawn from Italy and defeated by the Roman general Scipio Africanus (l. 236-183 BCE) at the Battle of Zama, in North Africa, in 202 BCE and Carthage again sued for peace.\nPlaced, again, under a heavy war indemnity by Rome, Carthage struggled to pay their debt while also trying to fend off incursions from neighboring Numidia under the king Masinissa (r. c. 202-148 BCE). Masinissa had been Rome's ally in the Second Punic War and was encouraged by Rome to raid Carthaginian territory at will. Carthage went to war against Numidia and, in so doing, broke the peace treaty with Rome which forbid Carthage from mobilizing an army.\nCarthage felt it had no choice but to defend itself against Masinissa's invasions but was censured by Rome and ordered to pay a new war debt to Numidia. Having only recently paid off their debt to Rome, they now owed a new crippling war debt. Rome was not concerned with whatever conflict Carthage and Numidia were involved in but did not care for the sudden revitalization of the Carthaginian military.\nCarthage believed that their treaty with Rome was ended when their war debt was paid; Rome disagreed. The Romans felt that Carthage was still obliged to bend to Roman will; so much so that the Roman Senator Cato the Elder ended all of his speeches, no matter what the subject, with the phrase, “Further, I think that Carthage must be destroyed.” In 149 BCE, Rome decided upon just that course of action.\nA Roman embassy to Carthage presented a list of demands which included the stipulation that Carthage be dismantled and then rebuilt further inland, thus negating the long-recognized advantage it had in trade from its position on the coast. The Carthaginians, understandably, refused to do so and the Third Punic War (149-146 BCE) began.\nThe Roman general Scipio Aemilianus (l. 185-129 BCE) besieged Carthage for three years until it fell. After sacking the city, the Romans burned it to the ground, leaving not one stone on top of another. A modern myth has grown up that the Roman forces then sowed the ruins with salt so nothing would ever grow there again but this claim has no basis in fact. It is said that Scipio Aemilianus wept when he ordered the destruction of the city and behaved virtuously toward the survivors of the siege.\nUtica now became the capital of Rome’s African provinces and Carthage lay in ruin until 122 BCE when Gaius Sempronius Gracchus (l. 154-121 BCE) the Roman tribune, founded a small colony there. Gaius’ political problems, and the memory of the Punic wars still being too fresh, however, caused the colony to fail. Julius Caesar proposed and planned the rebuilding of Carthage and, five years after his death, Carthage rose again. Power now shifted from Utica back to Carthage – which became Rome’s breadbasket owing to the same agricultural success which had enriched it before - and it remained an important Roman colony until it fell to the Vandals under their king Gaiseric (r. 428-478 CE) in 439 CE.\nCarthage had risen in prominence as Christianity grew and Augustine of Hippo (St. Augustine, l. 354-430 CE) contributed to its prestige by living and teaching there. The city was considered so illustrious, in fact, that the Council of Carthage of 397 CE was held there; the series of synods which would confirm the biblical canon for the Western Church, legitimizing the narratives which would come to be known as the Bible. The Vandal invasion of North Africa did nothing to halt Christianity’s development there, but tensions would rise between the Arian Christians (the Vandals primarily) and Trinitarian Christians just as they did elsewhere.\nThe Vandals under Gaiseric took full advantage of the location of their new city and plundered passing ships at will while also raiding coastal cities. Roman attempts to dislodge them failed and so a treaty was signed in 442 CE between Gaiseric and Valentinian III (r. 425-455 CE) acknowledging the Vandal Kingdom of North Africa as a legitimate political entity and establishing peaceful relations. When Valentinian III was assassinated in 455 CE, however, Gaiseric disregarded the treaty, believing it was an agreement only between himself and the emperor, and sailed for Rome. He looted the city but, in accordance with the request of Pope Leo I (served 440-461 CE), did not damage it nor harm the populace. The Vandals would continue to hold Carthage, and profit from its location, until after Gaiseric’s death.\nThe later Vandal king Gelimer (r. 530-534 CE), an Arian Christian, reinstituted the persecution of Trinitarian Christians which enraged the Eastern Roman emperor Justinian I (r. 527-565 CE), a trinitarian, who sent his great general Belisarius (l. 505-565 CE) to North Africa. Belisarius won the short-lived Vandalic War (533-534 CE), brought Gelimer back to Constantinople in chains, and restored Carthage to the Byzantine Empire (330-1453 CE) under which it continued to flourish.\nUnder the Byzantines, Carthage prospered through trade and as a major source of grain for the Eastern Roman Empire (the Western Roman Empire having fallen c. 476 CE). Around 585 CE, Carthage became the seat of the Exarchate of Africa under the Byzantine emperor Maurice (r. 582-602 CE), a separate administrative region established for more effective rule of the western areas of the empire.\nIn 698 CE, the Muslims defeated the Byzantine forces at the Battle of Carthage, destroyed the city completely, and drove the Byzantines from Africa. They then fortified and developed the neighboring city of Tunis and established it as the new center for trade and governorship of the region. Under the Arab Muslims, Tunis fared better than Carthage, but the city continued to thrive until the Eighth Crusade of 1270 CE when it was taken by the European Crusaders who fortified the citadel of the Byrsa. Once they were defeated, Muhammad I al-Mustansir had the city’s defenses torn down and many of the buildings razed to prevent any further such occupation.\nThe site of the ancient city continued to be inhabited and was included in the region taken by the Ottoman Empire (1299-1922 CE) who had no interest in excavating the ruins. The stones of the fallen houses, temples, and walls were carried off for personal or administrative building projects or left where they had been found. Modern archaeological excavation began in the 1830s CE through the efforts of the Danish consulate and continued under the French between c. 1860-1900 CE.\nFurther work at the site was undertaken throughout the first part of the 20th century CE but, as at Sabratha and other sites, the archaeologists were more interested in the Roman history of Carthage. The political and cultural zeitgeist of the time defined the Carthaginians, who were Semites, as a people of little value, and anti-Semitism significantly influenced not only the interpretation of physical evidence but the choice of what was kept for placement in museums or discarded.\nThe history of the period of Ancient Carthage, therefore, suffered as much from these modern-day excavations as from the city’s destruction by Rome or later conflicts. It was not until after World War II that systematic, unbiased, work at Carthage would begin; a paradigm consistent with the excavation and interpretation of many other ancient sites.\nCarthage still lies in ruin in modern-day Tunisia and remains an important tourist attraction and archaeological site. The outline of the great harbor can still be seen as well as the ruins of the homes, public baths, temples, and palaces from the time when the city of Carthage ruled the Mediterranean as the most opulent jewel of the North African coast."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:94a21baf-85b5-4e6a-98f6-cbaeef15d566>","<urn:uuid:98ea1cde-1a21-4563-a4c6-88da15ad2cb2>"],"error":null}
{"question":"What are the therapeutic applications and targeting methods for neuromodulation in pain management and depression treatment? 神经调节在疼痛和抑郁症治疗中如何应用？","answer":"Neuromodulation is used differently for pain and depression conditions. For pain management, spinal cord stimulation targets specific spinal nerves and treats conditions like failed back surgery syndrome, CRPS, and chronic back/limb pain. The stimulator is implanted under the skin to block pain signals. For depression treatment, deep brain stimulation requires precise targeting of specific brain areas through image-guided therapy, combining anatomy and functional neuroanatomy to activate regions responsible for treating severe depression.","context":["Spinal Cord Stimulation\nSpinal cord stimulation delivers electrical pulses to specific spinal nerves that are causing pain.\nSpinal cord stimulation is a type of neurostimulation. In this interventional pain management treatment, a permanent spinal cord stimulator is implanted under the skin. This spinal cord stimulator delivers regular electrical impulses to the spinal nerves that are causing pain, blocking these pain messages before they can reach the brain. The implanted device is small, like a pacemaker, and does not interfere with normal daily activities. Spinal cord stimulation therapy may help to reduce dependence on oral pain medications.\nConditions treated by spinal cord stimulation\nSpinal cord stimulation is used in the treatment of several pain conditions, including:\n- Failed back surgery syndrome\n- Complex regional pain syndrome (CRPS)\n- Chronic low back pain that does not respond to other therapies\n- Chronic arm and leg pain\nYour Henry Ford Pain Management specialist will work with you to determine whether a neurostimulation therapy (spinal cord stimulation, peripheral nerve stimulation or peripheral field stimulation) or other interventional pain management treatment is best for your pain condition.\nSpinal cord stimulation is performed in two separate procedures\nThe two spinal cord stimulation procedures include:\n- Initial trial electrodes: An initial procedure to connect an external, trial pulse generator. This is used to test the effectiveness of the spinal cord stimulation treatment before proceeding to the permanent spinal cord stimulator.\n- Permanent spinal cord stimulator implant: If your pain is reduced by 50 percent during the initial test period, you will undergo a second procedure to implant a permanent spinal cord stimulator under your skin for long-term pain management.\nProcedure 1: Initial trial electrodes\nDuring this minimally invasive procedure:\n- You lie on your stomach.\n- Your skin is sterilized at the injection site.\n- You are administered a local anesthetic to numb the injection area.\n- Your pain management physician inserts wires (leads) through a needle, under your skin and into the spinal cord space near the targeted spinal nerves.\n- These leads are then connected externally to a smartphone-sized electrical pulse generator that you carry with you during the initial test period.\nProcedure 2: Implanting the permanent spinal cord stimulator\nThis surgery is performed under general anesthesia. During this procedure:\n- You lie on your stomach.\n- Your skin is sterilized at the surgical site.\n- You are administered general anesthesia.\n- Your pain management physician connects the existing leads from the test pulse generator to a smaller spinal cord stimulator and implants this permanent device under your skin.\nFollowing each spinal cord stimulation procedure\nYou will be moved to a recovery room and monitored by your care team. In most cases, spinal cord stimulation procedures are performed on an outpatient basis and you will go home the same day, although in rare cases some patients may require admission to the hospital following the procedure.\nThe Henry Ford Pain Center approach\nAt the Henry Ford Pain Center, pain treatment begins with a thorough medical history and physical exam. All patients are evaluated by board-certified pain medicine physicians, physician assistants and certified nurse practitioners to identify the cause of their pain. Following initial assessment, we work with you to develop a personalized treatment plan that may include interventional pain procedures, medication therapy, physical therapy, massage and other complementary options.","Neuroimaging makes inroads toward quantifying and treating the complex, widespread disease\nAs depression continues to afflict millions of people worldwide, researchers in a number of medical disciplines — including radiology — are working together to accelerate progress in treating and diagnosing the crippling disease.\nIn a Monday Special Interest Session, Paths to Recovery: What Can Neuroimaging Teach Us about Depression and Its Treatment, experts in neuroimaging, psychoradiology and psychiatry will discuss the increasingly critical role neuroimaging plays in furthering the understanding of the complex disease that impacts more than 300 million people across the globe.\n\"Depression is a huge clinical problem,\" said session moderator David B. Hackney, MD, a professor of radiology at Harvard Medical School and chief of neuroradiology at Beth Israel Deaconess Medical Center in Boston. \"Millions of people have depression, and a substantial subset of those people have severe, ongoing depression that doesn't get better with standard talk therapy and medication. This session will focus on how, when you can identify areas of the brain that respond to treatment, you can do these people a great service.\"\nPresenters Helen S. Mayberg, MD, a professor of neurology, neurosurgery, psychiatry and neuroscience at Icahn School of Medicine at Mount Sinai in New York, and Qiyong Gong, MD, PhD, a professor of radiology at West China Hospital of Scichuan University, China, will demonstrate how neuroimaging can guide antidepressant therapies like deep brain stimulation. (See sidebar.) Both presenters are currently associate editors of the American Journal of Psychiatry.\nDr. Mayberg is renowned for her study of brain circuits in depression. Dr. Gong is a leader in the new field of psychoradiology, which explores interactions between what radiologists can see on imaging using purely anatomical techniques and understanding the brain function that underlies depression and other psychiatric diseases.\n\"Psychiatrists and physicians are good at making a diagnosis of depression, but we have limited knowledge of objective criteria for subtypes,\" Dr. Mayberg said. \"We can tell what symptoms are dominant, but we need biomarkers that can help stratify treatment outcomes.\"\nIn mapping how the brain responds to treatment, imaging has helped conceptualize mental health issues as actual brain disorders and is taking research beyond brain lesions to brain circuits, Dr. Mayberg said.\n\"Imaging continues to be at the forefront of how we conceptualize what goes wrong in the brain in even the most complex behavioral disorders,\" she said. \"And imaging gives us some methodological anchors for building models of how the brain works, which will lead to better diagnoses and better treatment.\"\nThe Promise of Deep Brain Simulation\nUnlike with movement disorders, brain stimulation techniques for depression are gradual rather than immediate, Dr. Hackney said.\n\"In people who have movement disorders, when you turn on the brain stimulators, the tremors disappear right away, and if you turn them off, the tremors come back right away,\" he said. \"For people with depression, it can take weeks to months before results become apparent.\"\nDeep brain stimulation is true image-guided therapy, Dr. Hackney said.\n\"Targeting where you put the deep brain stimulator, and specifically the part of the brain you're attempting to activate, is very much a combination of anatomy and functional neuroanatomy. It leads to an understanding of how that part of the brain is responsible for— or at least can be stimulated to help treat — severe depression.\"\nDr. Mayberg, known for her pioneering work in deep brain stimulation research, will speak on the technique during the RSNA Special Interest Session. Dr. Mayberg is also a pioneer in minmally invasive interventional psychoradiology which may be achieved in the future through technical advances such MR-guided focused ultrasound.\n\"I'm not a psychiatrist; I'm a neurologist who started doing imaging in the mid '80s as a research tool,\" Dr. Mayberg said. \"Imaging remains an important investigation tool to understand how the brain changes over time and to help select patients who can benefit from targeted therapies. The takeaway is that we're working on quantitative ways to make diagnoses and customize treatments.\"\nSession Tackles Depression\nThe Special Interest Session: Paths to Recovery: What Can Neuroimaging Teach Us about Depression and Its Treatment, SPSI25, will be held Monday at 4:30 p.m. David B. Hackney, MD, will moderate.\n- Depression 101: Current Approaches to Diagnosis and Treatment, Helen S. Mayberg, MD\n- Current Psychoradiology for Depression, Qiyong Gong, MD, PhD\n- Strategies to Develop Imaging Biomarkers for Treatment Selection, Dr. Mayberg\n- Theory to Practice: Targeted Modulation of Depression Circuits using Deep Brain Stimulation, Dr. Mayberg"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c6d1a8dd-0f1c-40b9-8a59-fea6e0429ced>","<urn:uuid:e98c356d-4931-44d2-819e-ae00023958e3>"],"error":null}
{"question":"What are the key differences between whale feeding mechanisms when comparing Minke whales from Scottish waters and Gray whales from Monterey Bay?","answer":"Minke whales and Gray whales use different feeding mechanisms. Minke whales are baleen whales that filter feed by taking in large quantities of water and using baleen plates made of keratin bristles to trap fish like herring and sand eels as they expel the water. They belong to the rorqual group, having throat grooves that allow expansion when gulping water. In contrast, Gray whales are bottom feeders that use their baleen (also made of keratin) to strain food from the seafloor, specifically targeting sand crab-like crustaceans called amphipods. They open their mouths underwater to let in water, krill, and other tiny creatures, then push the water out as their baleen filters the food.","context":["Our range of tours offer you the opportunity to see a range of wildlife including whales, porpoise, dolphins, basking sharks, otter, golden and white tailed sea eagles and much more in our European Special Area of Conservation - Firth of Lorn. However, with wildlife there can be no guarantees, please call us on 01852 300003 or email us at firstname.lastname@example.org to ask us what we've been seeing recently - we will always give you an honest answer !\nOur routes throughout the Firth of Lorn pass through some incredibly biodiverse ecosystems, on land and at sea, supporting a great range of animals from the smallest of fish to the largest land mammal and bird of prey the UK has to offer. Our year round residents are spectacular enough but we also have some incredible seasonal visitors too. This page showcases some of the wildlife we regularly get to see on our trips.\nYear Round Wildlife - Residents\nThe White Tailed Sea Eagle\nThis is the UK's largest raptor and are often thought of as the fourth largest eagles in the world, their wingspan can be up to 2.5m, they are so large they are commonly referred to as ‘flying barn doors’. Less aerially agile than the golden eagle they soar on flat wings but they are still formidable predators, taking mammals, birds and fish as prey, they are opportunistic feeders and will even feed on carrion. They develop their white tail when they reach adulthood at around 5 years of age and can live up to 25 years.\nIt is a joy to see them back in the UK after human persecution lead to their local extinction in the UK in the early 1900s. A reintroduction effort started on the Isle of Rum in 1975 and now 40 years on they seem to be flourishing and spreading down the islands and on to the mainland. The gaelic name for the white tailed sea eagle is \"iolaire sùil na grѐine\" meaning ‘eagle of the sunlit eye’, for its beautiful golden eye.\nThe Golden Eagle\nThe second largest raptor in the UK with a wing span of around 2m, although not the largest of our raptors they do have an air of majesty and have long been symbolic of power and strength throughout their range. These large birds are expert hunters and feed on small mammals, birds and sometimes carrion. Their keen eyesight helping them hone in on prey from great distance, they have awe inspiring agility in flight and during courtship displays perform undulating flight, flying upwards then swooping at great speed.\nThey will have up to 5 eyries in their extremely large territories which can be up to 150Km², nesting on craggy cliffs and in tall trees, they will add nesting material each year they reuse a nest, which can make nesting structures incredibly large! Tending to lay 2-3 eggs per clutch it is usually only one chick that fledges, although in some rare cases twins and even triplets have been reared to fledging.\nThe Buzzard is the most numerous bird of prey in the UK, it is a medium sized raptor with a large range of colour morphs, from very pale to almost black, they have a paler patch on the underside of the wings that can help identify them during flight.\nThe Common Seal\nThe common seal is smaller than the grey seal, the larger males being up to 1.9m, they have a very dog like profile with their large eyes and pronounced muzzle. Named common as they have a wide geographic range they are less numerous than the grey seal in the UK. They pup in early summer and the pups are able to swim hours after birth, but they do have a little encouragement from mum and if they get a little tired mum's there for a piggyback! As adults they are able to dive for up to 30 minutes and have been found to go to a depth of 500m, however their normal dive profile tends to be less than 4 minutes at a depth of less than 20m to get their favourite prey of small fish like sand eels.\nThe Grey Seal\nThe grey seal is the larger of the two species of seal that breed in the UK, the bulls (males) can reach over 2m in length with the females being slightly smaller at around 1.8m. They are superbly adapted to life at sea, being able to hold their breath for over 1 hour and like all true seals don’t have external ear structures and have small forelimbs. They can only shuffle about on land but have fantastic agility underwater. They spend most of their lives at sea but we often see them hauled out resting and regulating their temperature, because although their blubber helps them stay warm underwater sometimes they need to warm up on land.\nThe Harbour Porpoise\nThe harbour porpoise is the smallest cetacean we see in British waters, they are a coastal species never ranging further than 10km from land. Growing up to 1.9m, they have a small triangular dorsal fin in the middle of their back. Locally they are also known as the puffing pig as when they surface to breath they make a sneezing like sound. Like all the toothed cetaceans they use echolocation to hunt their prey and will often feed collaboratively, numerous individuals rounding up a shoal of fish into a tight ball and forcing them close to the surface, the porpoise then take it in turn to pick fish off from the shoal.\nThe Bottlenose Dolphin\nThe bottlenose dolphin is a large dolphin reaching sizes of up to 3.8m and can weigh up to 650kg, with a dark grey dorsal surface, paler underside and short beak they are an instantly recognisable species. They are a surface active species, breaching and spy hopping and performing great aerial acrobatics, they can be very inquisitive and will often come and ride the bow waves next to boats! Usually swimming at speeds of around 6mph they can reach up to 30 mph for short periods of time. Although resident in our waters all year they have an extremely large home range, from the southern tip of Islay all the way up to Skye. Please scroll to the bottom of this page to see some video footage, taken in early 2016 by crew member James Gilpin, of dolphin encounters on our Corryvreckan Wildlife Tours.\nShags and Cormorants\nThese very closely related seabirds can be quite hard to tell apart, unless side by side as the cormorant is about a third larger than the shag. Shags tend to be a jet black in colour whereas the cormorants are more brown in colour and have a larger creamy patch on their throat and chest. During the breeding season the shags have a pronounced crest which is why they are so named. For seabirds they have relatively little waterproofing oil in their feathers so they get waterlogged wings quite quickly making them less efficient fliers and swimmers so they need to haul out to dry off, they are often seen with their wings out; increasing the surface area for speeding up the drying process.\nThe red deer is the largest land mammal in the UK, a stag can stand at around 137cm at the shoulder and weigh up to 190kg! They breed during the rut in autumn where the males defend a territory and battle it out for breeding rights to a harem of hinds. They fight as a last resort, bellowing and parallel walking to infer dominance and it's only if the males are fairly evenly matched will they fight, using their antlers as weapons. They cast their antlers each year and grow new ones towards the end of spring, it is the fastest growing mammalian tissue and while growing is covered in a velvet like fur, filled with blood vessels supplying energy and nutrients for growth.\nThe fallow deer are not a native species to the UK and were introduced in the 11th century, the fallow deer on Scarba being introduced as a game species much later. Standing at just under a metre at shoulder height the males, called bucks will weigh about 100kg less than the red deer stags. They have palmate antlers rather than the branched antlers of the red deer and primarily feed on grass.\nThe European otter, growing up to 1.1m in length, its tail makes up a third of its total length helping propel them through the water. When they live in fresh water habitats European otters display what is known as crepuscular activity, being most active around dawn and dusk, however when they live in a coastal habitat their activity fluctuates with the tide so we often see them in the middle of the day. Hunting for their favourite quarry of small fish, but they will also eat crustaceans and frogs. Unlike the other marine mammals in the area otters lack a layer of blubber so have 2 layers of fur to provide insulation whilst foraging in the water.\nSeasonal Wildlife - Visitors\nThe Minke Whale\nReaching our area towards the end of June each year they spend up to 8 weeks feeding in the food rich water around the Garvellach Islands and Gulf of Corryvreckan.\nThe minke whale is a baleen whale, a group of whales that, rather than having teeth, have large baleen plates to filter fish like herring and sand eels from the water. The Baleen plates are made up of keratin bristles that trap fish as they expel the large quantities of water they have just taken in. Minke whales are the smallest rorquals, a group within the baleen whales that have large groves on the underside of their throats that allow expansion when taking in gulps of water when feeding. We only see the minke whales in the summer; usually July and August, during the winter they are further south, breeding in warmer waters. They can reach up to 10m in length and weigh in at around 8 tons.\nWe have dedicated Whale & Wildlife Watching Tours once we have been regularly seeing whales in our area. Please contact us on 01852 300003 to ask whether we have been seeing whales, we will always give you an honest answer.\nThe Basking Shark\nBasking sharks also pass through our area during the summer. They are second largest shark in the world and can grow over 10m in length. Similar to the whale shark which is the largest shark in the world the basking shark is also a filter feeder, using gill rakers that catch plankton out of the water it takes in, they can filter up to 1.5 million litres of water per hour! They often feed at the surface where plankton is abundant at certain times of the day and often their nose, tip of their dorsal fin and caudal (tail) fin can be seen above the water.\nComing to the coast to breed during the spring and summer we see lots of these seabirds down in the gulf of Corryvreckan which is a great feeding site before they head back out to sea towards the end of August where they spend the majority of the year.\nThe kittiwakes are the UKs most delicate looking gull, as adults they have black wing tips and black legs and feet, as juveniles (like the one pictured) they have a dark patch behind the eye and a black band on the leading edge of their wing.\nThey have a diet of small fish, primarily sand eels and their breeding success fluctuates in relation to sand eel abundance.\nThe common guillemot is a member of the auk family and relative to the black guillemot that is resident all year. They are slender seabirds, with a white belly and black back, and use their wings to propel them through the water to catch fish. They come to the coast during the spring to nest on ledges on cliff faces, they defend a small nesting territory and will literally be touching their nearest neighbour; to prevent them from falling off the cliff-face their eggs are cone shaped.\nThe razorbill is another auk, named because its beak has the same shape as an old fashioned cut-throat razor, which also comes to the coast to breed. Although they nest further up on the cliff-faces than the common guillemot so they don’t need the pyriform egg shape.\nThe gannet is a large seabird with a wingspan of 1.8m, as an adult they are predominantly white with black wing tips, a creamy face and neck and icy blue eyes. In flight they look pterodactyl like with a bend in the wing at their carpal (wrist) joint. They soar high above the water often at about 30m high looking for fish, on spotting a fish they plummet like arrows at speeds reaching up to 60mph into the water after their prey, they can dive down to 20m depth after a fish and stay underwater for 30 seconds. They often feed in association with cetaceans as they bring shoals of fish close to the surface of the water. The gannets we see in our area breed down on Ailsa Craig in the firth of Clyde and travel up to feed in the rich waters in the gulf of the Corryvreckan.\n*Page content - J. Finnigan.","Scientific Name: Eschrichtius robustus\nWhen is the best time to view Gray Whales in Monterey Bay and where is the best viewing?\nGray whales are typically seen in the Monterey Bay from December through May. In the winter they travel South to Baja California for reproduction. They peak in numbers traveling through the Monterey Bay around mid-January.\nIn the Spring, February through May, the whales are traveling North with their babies through the Monterey Bay. These number peak around mid-March. During this migration, the whales travel closer to shore to protect their babies.\nViewing these whales from a boat is the best viewing and there are many whale watching tours available. You can still see the whales from land most most Monterey Bay shorelines. One great place to do this is Pt. Lobos State Natural Reserve because of its high view point along the trails.\nAbout Gray Whales\nThe gray whale is mostly gray with some white patches and scratches. Their skin feels like a peeled, hard-boiled egg and bumpy where there are patches of barnacles, whale lice and other organisms along for the ride. Gray whales have streamlined bodies with narrow heads. They have a natural overbite, as the upper jaw slightly overlaps the lower jaw. The gray whale does not have throat pleating or a dorsal (top) fin. However, about 2/3 of the way back on the body is a dorsal hump. The dorsal hump is followed by 6–12 knuckles along the dorsal ridge, which extend to the tail.\nGray whales travel in groups called pods and surface to breathe. An adult can stay submerged up to 15 minutes. Gray whales have a double blowhole. A gray whale spout or blow can reach up to 15 feet, and looks like a heart from the front or back. Instead of teeth, gray whales have baleen. Baleen is made of keratin (same substance found in human hair and nails) and is similar to bristles of a toothbrush. Gray whales use their baleen to strain food from the seafloor. The gray whale opens its mouth underwater, letting in water, krill and other tiny creatures. The whale then pushes the water out as its baleen filters the food from the water. Baleen was once used to make corsets and umbrella ribs.\nWhat do Gray Whales and elephants have in common and other Gray Whale fun facts.\n- Same size and weight as 10 good-sized elephants\n- Tail-flukes can weigh up to 400 lbs. alone, 10–12 feet across\n- The heart of a gray whale weighs over 285 lbs.\n- Hunted to the edge of extinction, the Gray whale was given protection by the International Whaling Commission (IWC) in 1937 & 1947\n- Can dive up to 395 ft., travel at 3–6 mph and be submerged for up to 15 min.\n- Calves nurse 7–8 months on milk that is 53% fat (human milk is 2% fat)\n- Migrate 10,000–14,000 miles, which is the longest known migration route for a mammal\n- Their crusty ocean rock appearance is due to several hundred pounds of parasites covering their skin.\nHabitat: Migrate along the North American coastline from Alaskan waters to the Mexican coast. Mate and give birth in warmer waters of Baja, California.\nSize: Newborn calves average 16 ft. Males average 43 ft. and Females average 46 ft. Can be up to 50 ft.\nDiet: bottom feeder, consists mainly of sand crab-like crustaceans called amphipods\nLifespan: About 40 years"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:b4597882-ae6b-4257-9a28-0bf67fef2fda>","<urn:uuid:7b786516-fbc8-4d1e-894c-5c3beae9c17f>"],"error":null}
{"question":"What are the career benefits for environmental organization employees, and what motivates businesses to maintain environmental compliance?","answer":"Working for environmental organizations offers benefits like travel opportunities, diverse work experience, and the chance to work for a passionate cause, though salaries may be lower than other fields. Organizations like Greenpeace provide international work opportunities with offices in over 28 countries. As for businesses, they are motivated to maintain environmental compliance primarily to avoid serious consequences like investigations and hefty fines for non-compliance. Additionally, failing to follow regulatory compliance practices can damage a company's public image. Companies must implement comprehensive compliance systems including regulatory, data, and health and safety compliance to protect both the environment and their business interests.","context":["Jobs for the Environment\nAn environmental organization is either a government or private organization which monitors, analyzes and works to preserve the environment. Environmental organizations exist on international, national, regional and local levels with thousands of organizations working for the common cause of a better environment worldwide.\nBecause of the large number of organizations that exist, some target specific issues like environmental disasters or radioactive waste, while others tackle environmental issues on a broader level.\nEnvironmental Organizations employ thousands of employees and work with even more volunteers every year. The jobs that are involved with an environmental organization vary extremely because of the range of jobs that exist within thousands of different organizations. These jobs can include administration workers, scientists, engineers, environmental lawyers, grant writers, human resource professionals, analysts, toxologists, fundraisers and public affairs workers. For someone interested in working for an Environmental Organization, there are endless possibilities.\nThe Environmental Protection Agency\nThe EPA or Environmental Protection Agency is perhaps the United States’ most popular Environmental Organization. Started in the 1970s, it is a governmental protection agency run by an Administrator who is appointed by the President. The EPA enforces environmental laws, researches better and cheaper ways for business to interact with the environment and works with other world organizations to promote the health of people and environments worldwide. The EPA also works with the United States government to provide research for legislation that relates to the environment. The EPA employs over 17,000 people in the United States with a diverse field of engineers, scientists and administrative workers. The EPA even offers summer and temporary work internships for students who want to work with environmental agencies.\nLearn More about the EPA\nWhile the EPA is a large example of a world-renowned environmental organization there are literally thousands or organizations that are eager to employ people willing to help the environment.\nGreen Peace is an example of a large Environmental Organization that is in many ways a household name. It was started in Canada but now has offices in over 28 countries. It is perhaps one of the most popular Environmental Organizations worldwide that isn’t a government affiliate. Its focus on global campaigning is very well known and its members commonly protest whaling, deforestation and global warming.\nLearn More about Green Peace\nGreen Peace is an excellent example of how working for an environmental organization can provide travel opportunities and work experience.\nJobs for Environmental Organizations\nBecause so many Environmental Organizations are non-profit and run based on donations and grants, the salaries for workers of these organizations can be lower than they might be in another field.\nThis isn’t true for every environmental organization job and the size and sheer number of organizations is a testament the variety of jobs and salaries that are available. Internships and volunteer opportunities are excellent segues into work for an environmental organization and some even run summer camps for children. Perhaps the most important aspect in working for an environmental organization is that its workers are working towards a cause they feel passionate about. Finding a paying job in an organization might take some ingenuity, but with a passion to help the environment, there is most certainly an organization that will want your contribution.\nTypes of Environmental Jobs\nThere are people with a variety of backgrounds who find themselves employees of an Environmental Organization. Some are formally trained environmental lawyers, accountants and even lobbyists. Others have education in parks and recreation or are grant writers and researchers. The size and number of these organizations are enormous and as a result there are the typical jobs associated with running large companies. This includes businessmen, accountants and PR representatives. However, because of the environmental focus of these organizations there is an additional need for people with an interest in helping the environment as well as people who have training in environmental issues.\nThe best way to discover a job through an environmental organization is to research organizations independently. Many offer internships and volunteer programs for people who want to help, when there are no open positions.","Complete Guide to Business Environmental Regulatory Compliance\nClimate change is considered the biggest existential threat ever faced in human history. It affects every living person and every country. Therefore, efforts to mitigate catastrophe can’t be further delayed.\nIn 2019, the Emissions Gap report found that to keep global temperatures from going up to more than 1.5 degrees Celsius; we need to reduce total emissions by 7.6% every year until the year 2030. It becomes the responsibility of all organizations within goods-producing industries to work towards reducing their environmental impact and protecting the resources we still have.\nWhat is environmental and regulatory compliance? They refer to the laws, standards, and guidelines used to regulate businesses’ operations. The last few years have seen an increased passing of federally mandated regulations related to the environment.\nRead on to discover how to prepare your business to maintain regulatory compliance in efforts to make industries greener.\nWhy We Need Business Environmental Regulatory Compliance\nThe production of goods has a major impact on the environment. It takes a lot of natural resources to make products and synthetic materials and the machines that produce them. The Industrial Revolution birthed mass production, which accelerated scientific and social advancements.\nBecause of this, for the last 250 years or so, rates of industrial output have skyrocketed. People consumed more, which led to businesses using more resources to meet rising demands and make a profit. Large corporations were doing this unchecked for decades which caused the climate change crisis we face today.\nConcerns about how human industry would harm the planet started as early as 1962 when scientist Rachel Carson published “Silent Spring.” She showed some of the earliest documented evidence of pollution harming water systems and animal populations because of the agriculture industry’s excessive use of pesticides.\nShe called attention to how technology was disrupting ecosystems and would eventually cause permanent damage.\nGlobal Commitments to Reduce Emissions\nBy 1988, climate change became a center-stage political issue because of the gradual rise in global temperatures. This came about as a consequence of the weakening of the Earth’s ozone layer. The sense of urgency about the potential of climate disasters found its way into public awareness.\nGovernments started to discuss how to regulate industries to make them more sustainable.\nThe first agreement to reduce emissions through regulatory compliance came about in 1997 with the Kyoto Protocol. Even though President Bill Clinton signed this agreement, which was a commitment to lower greenhouse gas levels by 2012, President George W. Bush pulled the US out of the agreement due to concerns over the economy.\nThis would happen again with the 2015 Paris Agreement. President Barack Obama, along with 197 other countries, committed to implementing their own regulations to reduce emissions. The goal was to make sure global temperatures would not rise more than 2 degrees Celsius.\nPresident Donald Trump immediately pulled the US out of this agreement after his election in 2016.\nUnderstanding Environmental and Regulatory Compliance\nTo commit to reducing emissions and preserving natural resources, the federal government has passed different laws and regulations. Federal laws set standards that companies must follow in their business practices. It refers to a legal requirement to go green.\nSome of the laws that have been passed over the years in the US include the following:\n- Toxic Substances Control Act (TSCA)\n- Clean Water Act (CWA)\n- Clean Air Act (CAA)\n- Resource Conservation and Recovery Act (RCRA)\nThese laws put the responsibility on companies to make sure they follow these regulations so that they have a reduced impact on the environment, make better use of resources, and mitigate the harm caused by hazardous materials. Laws exist to regulate processes like how those hazardous materials are disposed of and handled.\nMost corporations see these federal mandates as a burden. However, non-compliance carries serious consequences. For instance, companies that fail to meet certain regulatory standards have to undergo investigations and potentially pay hefty fines.\nFailure to follow regulatory compliance practices due to neglect and cutting corners also makes companies look very bad in the public eye.\nHow to Create an Environmental and Regulatory Policy Business Plan\nIt takes a lot of work for businesses to set up practices to reduce their impact on air emissions and water contamination. Fundamentally changing processes at every level of production also involves setting up protocols to ensure business practices stay compliant.\nKeeping track of your company’s regulatory processes might involve assistance from third-party organizations. These agents perform some of the following tasks, including:\n- On-site inspections\n- Data collection\nSometimes companies can work with inspectors off-site with the following processes:\n- Reporting audits\n- Public records review\n- Special programs\nYour business also can put systems in place that ensure you have the most up-to-date information and that your staff remains fully educated about compliance regulations, as well as methods for keeping track of compliance. Understanding the environmental regulation policy serves as the most critical undertaking.\nThere are several ways your business can be informed about policies as well as policy updates:\n- Employee and manager training about environmental policies\n- Getting staff members the credentials to perform audits\n- Keep information posted on-site in work areas as reminders\n- Assigning managers to do oversight\nOne main way to keep staff informed includes making public information available in these ways:\n- Brochures, flyers, and pamphlets\n- Posters and infographics\n- Online content like blog posts and email newsletters\nDifferent Ways that Businesses Can Stay Compliant\nCompliance is just a part of doing business. Whether business owners do it begrudgingly or not, multiple compliance systems operate to keep businesses up to code. Different types of compliance include the following:\n- Regulatory compliance\n- Data Compliance\n- Health and safety compliance\nFollowing environmental policies requires the above forms of compliance practices in some form or another.\nRegulatory compliance refers to the laws and regulations for how businesses perform operations. To protect the environment, companies must constantly keep their production and use of resources in check and follow laws that have been put in place to prevent further environmental damage.\nData compliance controls laws centering on how businesses collect data and store and maintain database records. This is for tracking operations to make sure they are environmentally friendly. Data gives references for audits and thorough oversight.\nFinally, health and safety compliance measures put protocols in place to protect employees from harm. They also keep the surrounding communities protected. For environmental regulations, this means safety protocols surrounding pollution prevention and handling hazardous materials. The main goal is to reduce harm and environmental impact.\nTake Steps to Help the Environment\nBusiness environmental regulatory compliance is one of the most critical steps businesses in the goods industries can take. It’s not just about avoiding trouble and staying in business; it’s about protecting future generations and the one planet we call home.\nContact Atlantic Environmental to learn how we help companies follow environmental policies."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:6d79b824-0de6-4a9b-b0a6-725e5059bb55>","<urn:uuid:5153a3fc-d954-4836-972b-d77e796d647c>"],"error":null}
{"question":"What are the main architectural characteristics of traditional dwellings in Luang Prabang?","answer":"Traditional dwellings in Luang Prabang were built from wood and bamboo, raised on piles. They featured a lightweight framework of wood or bamboo with panels of woven bamboo strips for infill, and thatched roofing. Later, a plaster finish made from lime, straw, sand, palm sugar and boiled buffalo skin was introduced by the Tai Dum people.","context":["Located on a peninsula at the confluence of the Mekong and Nam Khan Rivers in the north of Laos, Luang Prabang is the nation's oldest continually occupied city. Its present population is about 20,000.\nLuang Prabang's recorded history begins in the fourteenth century, when it became the original capital of Lan Xang (Kingdom of a Thousand Elephants). Founded by Fa Ngum (1316-1374), Lan Xang was the first unified Lao kingdom. Fa Ngum sought to unify the diverse ethnic groups who populated the kingdom by promoting Theravada Buddhism, the predominant religion of continental Southeast Asia. In 1358, he asked the Khmer king to send a Buddhist mission consisting of elders, scholars, and artisans to the city, then known as Muang Sua. They brought with them the sacred Pra Bang image of the Buddha, which remains the city's palladium, as well as a symbol of the authority of the Buddhist Sangha.\nIn 1563, in response to threats from Burma, Settathirath I (r. 1548-71) moved the capital from Muang Sua to Vientiane and renamed it Luang Prabang (Royal City of the Pra Bang) in the Pra Bang's honor. After the transfer, Luang Prabang's political importance receded. In the late seventeenth century, the kingdom's end approached. In 1707, the northern provinces became the separate kingdoms of Luang Prabang and Vientiane. In 1713, the southern provinces became the Champassak kingdom.\nAfter Lan Xang's demise, Luang Prabang sought to maintain its independence in response to threats from Siam and Burma. Taking advantage of the division of the Lao kingdoms, Burma attacked Luang Prabang in 1753 and 1771. In 1778, Siam declared it a dependency. After subduing Vientiane in 1828, Siam began to exert greater influence over Luang Prabang, which ceremoniously reaffirmed its allegiance in 1836. During this period, the city also suffered from frequent disastrous fires that destroyed much of its architectural heritage.\nIn the late nineteenth century, France began to extend its imperial ambitions in Indochina to the Lao territories. Siam signed a treaty with France in 1893 in which it reluctantly renounced sovereignty over all Lao territories east of the Mekong. This treaty opened the way for France to establish a protectorate over Luang Prabang, direct colonial administration over central and southern Laos east of the Mekong, and control of the Mekong itself. In 1904 and 1907, further agreements effected the transfer of two Lao provinces west of the Mekong. France now controlled about half the territories of the former Lan Xang.\nThroughout the period of French control, the Lao territories remained France's least important possessions in Indochina. When France selected Vientiane as the seat of colonial administration, Vientiane usurped Luang Prabang's status as the Lao territories' center of political power. The Luang Prabang king increasingly became a figurehead who executed his duties in parallel with the French administrators.\nFollowing the Geneva Conference of 1954, France withdrew from Indochina. A constitutional monarchy that restored Buddhism to the position of official state religion, which inspired a Buddhist revival, was established. In the 1950s and 1960s, a period of political instability, rightists, neutralists, and communists struggled for control of the government.\nIn the 1960s, the Vietnam War escalated, and the United States bombed eastern Laos. Luang Prabang, however, was spared from destruction. In 1975, Sisavong Vatthana (r. 1959-75), the last king, abdicated, and the Lao PDR, to be governed by the Lao People's Revolutionary Party, was established.\nThe Lao PDR has attempted to establish socialism \"with Lao characteristics.\" The king's removal was accompanied by Buddhism's rejection as the official state religion. But in the 1990s, the state turned to Buddhism in a search for a reformulated Lao nationalism and promoted the Sangha as the national culture's caretakers. The nomination of Luang Prabang as a UNESCO World Heritage Site, a status it achieved in 1995, exemplified this tendency.\nLuang Prabang Architecture\nThe following text is reprinted from Tourism and Heritage Site Management in the World Heritage Town of Luang Prabang, Lao PDR (Bangkok: Office of the Regional Advisor for Culture in Asia and the Pacific, UNESCO Bangkok and School of Travel Industry Management, University of Hawa'i, 2004), pp. 27-28.\nThe early dwellings in the Luang Prabang area, similar to vernacular houses throughout Lane Xang and neighbouring kingdoms, were built from wood and bamboo, and raised on piles. A lightweight framework of wood or bamboo was constructed, with panels of woven bamboo strips used for infill. Thatched roofing provided protection against the elements and could be easily replaced as necessary. Later, a plaster finish made from lime, straw, sand, palm sugar and boiled buffalo skin was introduced by the Tai Dum, a Tai-speaking group whose homeland is in northern Viet Nam. These construction techniques and materials are still predominant in the villages surrounding Luang Prabang and in some areas of the town itself.\nThe construction technique of vernacular dwellings is consistent throughout different levels of society. Higher status is revealed only through location, larger size and better quality of construction materials. As such, a village chief's house was, traditionally, stylistically identical to the house of an ordinary resident.\nNew secular building styles were introduced between 1893 and 1907 as the French gradually assumed administrative control of Laos. In constructing administrative buildings and houses the French introduced European construction techniques and materials. For instance, the restriction on the use of bricks, which were previously used only for temples, was lifted. The French, however, did not merely transplant European styles into Luang Prabang. Instead, they employed styles developed in Viet Nam and produced designs inspired by vernacular temple architecture and secular wooden structures that were better suited to the warm and humid Laotian climate.\nAs a result, a new Laotian architectural style emerged, based on indigenous domestic architecture but freely incorporating French and Vietnamese design elements along with European and Chinese technical innovations. The Laotian royalty and aristocracy, who had previously lived in wooden houses, had their new masonry residences constructed in this style. The former Royal Palace, which today houses the Luang Prabang National Museum, was built between 1904 and 1909 and serves as a fine example of the French-inspired architecture that was popular at that time.\nThe French introduced some elements of Chinese architecture and urbanism indirectly. To execute French public works, skilled Vietnamese labourers were imported. These labourers settled near the foot of the peninsula and built their own commercial quarters which were brick, Chinese-style shop houses in rows that faced directly onto the street, with living accommodations on the upper floors. All of these architectural styles can still be seen today in Luang Prabang."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:92a6ad4b-ec71-4b35-8fb2-01a4d1cb769d>"],"error":null}
{"question":"How do particle physicists compare the structural complexity of pions versus muons?","answer":"Pions and muons represent different levels of structural complexity. Pions are composite particles consisting of a quark and an anti-quark pair, plus a complex quark-gluon sea where quarks, anti-quarks and gluons pop in and out of existence, making their structure more complicated. This structure is mapped through a form factor that shows how charge is distributed inside the pion. In contrast, muons are fundamental particles like electrons - they are not made up of quarks and do not have an internal structure to study. Instead, their importance lies in their properties as heavy cousins of electrons, making them useful for probing the accuracy of the Standard Model.","context":["Probing the Pion's Structure\nIn research carried out at the Thomas Jefferson National Accelerator Facility (Jefferson Lab), the Fπ Collaboration has provided significant new data on the structure of the pion, the lightest particle built of quarks. The pion, arguably the most important of the mesons due to its unusually small mass, can be naively pictured as consisting of one each of the lightest quarks and anti-quarks. As with all quark-based particles, however, a more realistic description of the pion also includes the quark-gluon sea: a strong-force driven bevy of quarks, anti-quarks and gluons popping into and out of existence and providing the foundation of the pion's structure. This structure is mapped out by a single quantity (known as a \"form factor\" Fπ), which provides information about the distribution of electric charge inside the pion. By measuring Fπ at ever shorter distances, it is possible to study the pion's transition from a particle in whose structure the quark-gluon sea plays a significant role, to one that behaves like a simple quark-antiquark system.\nFigure 1. The t-channel diagram of interest in this experiment, where the electron scatters off the virtual π+ inside the proton, knocking it on shell. The scattered electron and the pion are detected in coincidence, providing a snapshot of the pion at the moment of scattering.\nThe importance of the measurement is to be seen in the context of understanding in detail the mechanisms that bind quarks, which do not exist as free particles in nature, into nucleons (three-quark objects such as protons and neutrons) and mesons (quark-antiquark pairs such as pions). These two families of bound quark objects are collectively called hadrons. The underlying theory describing the quarks and gluons is Quantum Chromo-Dynamics (QCD), which has been successfully applied to high energy processes which are able to resolve hadron structure in fine detail. However, a peculiar property of the strong nuclear force is that it grows stronger when the energy involved in the interaction between quarks and gluons gets smaller. In this case, precision calculations with perturbative theoretical methods (known as perturbative QCD) break down, and theorists resort to ''effective'' models of hadron structure. While these models are well constrained at very low energies, corresponding to low-resolution in the hadronic structure, their predictions differ significantly at higher energies, where their range of validity should give way to that of perturbative QCD. It is this breakdown of precision calculations that the Jefferson Lab experiment set out to probe, significantly increasing the range of precision data for the pion form factor.\nThe pion is not an easy target to have experimentally, since it is unstable with a life-time of only 26 billionths of a second. To get around this difficulty, the scientists involved in the experiment used a proton target, since the proton sometimes fluctuates into an intermediate state of a pion and a neutron. The process of interest is therefore the scattering of a multi-giga-electron volt (GeV) electron from the virtual pion inside the proton (Fig. 1). A snapshot of the pion at the moment of scattering was taken by measuring the scattered pions and electrons within a set energy range and at a set angle in the Jefferson Lab Hall C magnetic spectrometers.\nThe new data shows that the pion form factor remains high in the the resolution range probed (up to Q2=2.45 GeV2), with no sign of turning around to levels predicted by perturbative QCD [Fig. 2]. Thus, the highest Q2=2.45 GeV2 probed by the experiment is still far from the resolution region where the pion behaves like a simple quark-antiquark pair. These new high-precision data provide a stringent test for models that attempt to incorporate the important ``softer'' quark-gluon sea contributions, serving as a benchmark for understanding the strong interaction at its most basic level. It remains unclear at what energies the pion actually behaves as its simplistic picture implies, and plans are now being made to study the pion with the higher-energy electron beam proposed for the 12 GeV upgrade at Jefferson Lab. The upgrade will allow an extension of the Fπ measurement to Q2=6 GeV2, which will probe the pion at double the resolution.\n T. Horn, et. al., Phys. Rev. Lett. 97 (2006) 192001.\n V. Tadevosyan, et al., Phys. Rev. C 75 (2007) 055205.\n J. Volmer, et al., Phys. Rev. Lett. 86 (2001) 1713.","The muon—the short-lived cousin of the electron—could be the key to understanding relationships between other fundamental particles. And it holds a mystery all its own.\nIn the 1930s, scientists thought they had matter figured out. Matter was atoms; atoms were protons, neutrons and electrons; and that was that. Then they discovered the muon—a surprisingly heavy cousin of the electron with no apparent purpose other than to baffle scientists. The muon was so unexpected that, regarding its discovery, Nobel laureate Isidor Isaac Rabi famously quipped, “Who ordered that?”\nOut of the 16 particles in the Standard Model, the muon is becoming the focus of research for more and more physicists, who seek both to understand its unique properties and to use it as a probe of the rest of the subatomic world. “Muons are special,” says Chris Polly, a Fermilab physicist involved in muon research. “They are light enough to be produced copiously, yet heavy enough that we can use them experimentally to uniquely probe the accuracy of the Standard Model.”\nEven though the muon is one of the most accessible subatomic particles, it has its share of characteristics that make it enigmatic and exciting.\nMuons are about 200 times heavier than the electron. While this larger mass makes them interesting, it also makes them unstable. Whereas electrons live forever, muons exist for only about two microseconds—or two millionths of a second—before they decay. But for particle physicists, who work with particles traveling close to the speed of light, two microseconds is, essentially, forever.\nFrom an article of Sarah Charley – Symmetry a Fermilab/SLAC pubblication\nWe wish to thank Prof. Coan and Prof. Ye for the software we have used and for their work on muons detection which has inspired the present work.\nCosmic rays, at sea level, have plenty of muons. They come from the interaction of primary cosmic with nuclei of atmosphere. Cosmic muons can be detected rather simply with scintillation detectors and it is also possible to measure the muon lifetime.\nIn PhysicsOpenLab we already did experiments with cosmic muons, described in the following posts :\nNow we want to describe the last equipment we built with the aim to measure with precision the muon lifetime. It consists of a plastic scintillator coupled with a PMT and an electronic circuit based on a PSoC which performs the time measure between the muon passage in the scintillator and the muon decay. The data are stored in a SD Card and sent via serial communication to a PC which runs the software to acquire data and to make the statistical analysis.\nThe image below shows the generation of the two light pulses (short arrows) used in determining the muon lifetime. One light pulse is from the slowing muon (dotted line) and the other is from its decay into an electron or positron (wavy line).\nThe muons that come to rest then live a relatively long time, on the order of order microseconds, inside the scintillator; but eventually each of them decays in an electron (positron) plus a neutrino and a antineutrino. Nearly all of the rest energy (105 MeV) of the stopped muons appears as kinetic energy of the three particles; on average, the electron (positron) gets a third of this energy, about 35 MeV. (The two neutrinos carry away the rest of the energy undetectably). But such an electron is a charged particle, itself certain to cause ionization as it moves through the scintillator. Conveniently, the typical energy deposited in this ionization process is about the same as that deposited by a muon-in-transit, or a muon stopping, so the very same scintillator/PMT configuration is also suitable for detecting stopped muons subsequently decaying at rest.\nTo measure the muon’s lifetime, we are interested in only those muons that enter, slow, stop and then decay inside the plastic scintillator. Such muons have a total energy of only about 160 MeV as they enter the tube. As a muon slows to a stop, the excited scintillator emits light that is detected by a photomultiplier tube (PMT), eventually producing a logic signal that triggers a timing clock. A stopped muon, after a bit, decays into an electron, a neutrino and an anti-neutrino. Since the electron mass is so much smaller that the muon mass, mμ/me ~ 210, the electron tends to be very energetic and to produce scintillator light essentially all along its path. The neutrino and anti-neutrino also share some of the muon’s total energy but they entirely escape detection. This second burst of scintillator light is also seen by the PMT and used to trigger the timing clock. The distribution of time intervals between successive clock triggers for a set of muon decays is the physically interesting quantity used to measure the muon lifetime.\nThe images below shows the experimental setup with the “scintillator can” and the signal processing electronic part.\nThe image below shows the signal processing chain, from the PMT to the PC for data acquiring and data processing.\nTrans Impedance Amplifier\nThe -negative- signal from the PMT, which is powered with a high voltage at around 1 kV, is amplified with a trans impedance amplifier (TIA). The scheme below shows the schematic of the TIA, the operational amplifier has to be very fast (wide band op amp) in order to preserve the timing of the original signal produced by the PMT. The signal produced in output by the amplifier is inverted with respect the input, so it has a positive swing from 0 level to about the maximum level of 2 V.\nThe amplified signal is sent to a fast comparator, with adjustable threshold, which has the aim to produce a square pulse signal for the pulses generated by the muon passage inside the plastic scintillator. Acting on the threshold it is easy to select only those pulses coming from muons excluding the “noise” from background radioactivity, because the muons generate pulses rather high while the background pulses remain below 100-200 mV. So we set the threshold at 300 mV. With this value we obtain a pulse rate of around 5-7Hz, that is 300 – 400 CPM.\nThe aim of the PSoC component is mainly to acquire the pulse signals from the comparator, count them and select the pulses which are coming from a muon decay; in this latter case the aim is to measure the time interval between the two pulses, the first pulse from the muon and the second pulse from the electron (positron) generated in the muon decay : this duration is the decay time of the muon.\nIn details the tasks accomplished by the PSoC are the followings :\n- Read and show on display the fast comparator threshold;\n- Acquire the muon pulses from the comparator and generate for each pulse a 40ns-fixed pulse;\n- Measure the time interval between two consecutive pulses (40ns pulses) : if this time is less than 20μs it is a muon decay event, otherwise it is not;\n- Count the muon pulses and calculate the rate value and the σ value;\n- Count the muon decay events and calculate the rate value and the σ value;\n- Show on display the data;\n- Write on SD card the acquired data;\n- Send the acquired data to the PC via serial communication;\nThe PSoC 5LP (CY8CYKIT-059) has been programmed with the development system PSoC Creator, freely downloadable from the manufacturer’s website. Using this tool, with a rich and simple graphical interface, the chip is programmed and all the used components are defined and configured.\nWe go through the main points of the project.\nThe system is synchronized by means of a series of clock signals, the main of which is the BUS_CLK that has a frequency of 50MHz (this frequency is set in configuration and depends on the characteristics of the circuit that is being achieved), it synchronizes the operation of all the components. The period of this signal is 20ns , this is important because it corresponds to the minimum duration of the signals that can be managed correctly by the system. Signals shorter than 20ns may be managed incorrectly or may not be read by the system. In our project we have established for the signals a minimum duration of 40ns, well above the limit of 20ns.\nThe pulses produced by the fast comparator are the input signals of the PSoC processing chain. Since the pulses may have variable durations, downstream it is placed a D-type Flip Flop with external RC network to obtain a clean pulse of 40ns. The latter also produces a further pulse of 0.01s used to turn on a LED in order to give a visible feedback of the capturing event.\nThe pulses are sent to digital counters which perform the pulse counting. There is also a counter that counts the seconds so as to measure the duration of the counting operations.\nTime Interval Measurement\nThe time interval between two pulses is measured with a timer. The time interval is measured between the rising edge of the pulses.\nThe system is equipped with three buttons and a LED that lights up when you press any of the three buttons.\nThe functions of the three buttons are as follows :\n– Reset Counters\n– Start / Stop Counting\n– Display Switch\nThe following images show the electronic part and the data shown on the display :\nIn the picture below you can see the typical PMT pulse produced by the muon passage in the scintillator. The FWHM is about 40ns, while the amplitude is about 200mV.\nThe picture below shows various PMT pulses, with amplitude ranging from 100mV to 300mV and duration ranging from 40ns to about 120ns.\nThe picture below shows both the PMT pulse and the output pulse of the amplifier. The duration is the same while the signal amplitude, positive, goes up to 1,5V.\nThe picture below shows both the PMT pulse and the output pulse of the fast comparator.\nThe picture below shows both the PMT pulse and the 40ns pulse generated by the PSoC. The latter is delayed with respect to the PMT pulse of around 40ns.\nThe picture below shows the jitter of the 40ns pulse generated by the PSoC. From the traces it can be estimated in less than 20ns.\nThe picture below (obtained increasing the persistence of the scope display) shows the muon pulse plus two pulses generated by the muon decay and the corresponding PSoC pulses.\nThe PSoC board is interfaced to the PC by means of a serial communication. In the PSoC project this is done with the UART component. The serial connection is the same connection used for programming and debugging.\nThe image below shows the entire setup with scintillator, electronics and PC for data acquisition.\nThe software used for data analysis is the application realized by Prof. Coan and Prof. Ye which can be downloaded, with documentation, from their site www.mtphys.com.\nThe image below is a screenshot of the software.\nThe application reads and records the data which are sent from the PSoC. It performs the counting of muons, the counting of muon decays, it calculates the rates and, mainly, it fits the decay data with exponential law determining the muon lifetime.\nIn the image below we report the results after collecting about 5000 events.\nThe decay constant value is equal to a 2.08 ± 0.03 μs\nThis value is between the value measured in vacuum, 2.197 μs, which is valid for the positive muons and the characteristic value of negative muons equal to 2.043 μs, slightly less because of the interaction of negative muons with nuclei of the scintillator.\nWhen a negative muon passes through and stops inside a material there is a not null likelyhood that it is “absorbed” by a proton of a nucleus causing a “capture” reaction:\nμ– + Z -> (Z – 1)* + νμ\nTherefore the actual mean life is shortened and is given by:\n1/τe = 1/τμ + 1/τc\nWhere 1/τμ and 1/τc are the rate for decay and for capture, respectively. Of course, it must be taken into account the ratio μ– / μ+ present in cosmic rays and the capture cross-section in the matter. It turns out that the correction is about 4%. Taking this correction, we get the following value:\nτμ = 2.16 ± 0.04 μs\nThe measurements are in excellent agreement with the expected and predicted by theory.\nIf you liked this post you can share it on the “social” Facebook, Twitter or LinkedIn with the buttons below. This way you can help us! Thank you !\nIf you like this site and if you want to contribute to the development of the activities you can make a donation, thank you !"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:8d37f012-c63f-481a-b3cb-a18b575f10d5>","<urn:uuid:f9b04e22-d8b8-4ed0-814d-90920280b616>"],"error":null}
{"question":"How do the information processing approaches differ between the Experience Recorder and Reproducer (ERR) model of consciousness and a computer's CPU architecture?","answer":"The ERR model and CPU process information very differently. The ERR model does not require processing units, algorithms, or stored programs - it functions more like a non-linear random-access data recorder that stores experiences using content-addressable memory. In contrast, a CPU acts as the brain of a computer system, performing calculations, comparisons, and controlling operations through specific components like the control unit and arithmetic logic unit (ALU). While the ERR model reproduces experiences when stimulated by similar current experiences, a CPU executes instructions sequentially based on its instruction set and uses registers for temporary information storage. The ERR model focuses on recording and reproducing experiences with associated emotions, while CPUs focus on mathematical and logical operations through structured processing.","context":["Consciousness can be defined in information terms as a property of an entity (usually a living thing, but we can also include artificially conscious machines or computers) that reacts to the information (and particularly to changes in the information) in its environment. In the context of information philosophy, we can define this as information consciousness. Thus an animal in a deep sleep is not conscious because it ignores changes in its environment. And robots may be conscious in our sense. Even the lowliest control system using negative feedback (a thermostat, for example) is in a minimal sense conscious of (aware of, exchanging information about) changes in its environment. This definition of consciousness fits with our model of the mind as an experience recorder and reproducer (ERR). The ERR model stands in contrast to the popular cognitive science or \"computational\" model of a mind as a digital computer. No \"information processing\" (no processing units, algorithms, or stored programs) is needed for the ERR model, although we also see mind as immaterial \"software\" in the material brain \"hardware.\" The physical metaphor for ERR is a non-linear random-access data recorder, where data is stored using content-addressable memory (the memory address is the data content itself). Simpler than a computer with stored algorithms, a better technological metaphor might be a video and sound recorder, enhanced with the ability to simultaneously record smells, tastes, touches, and - critically essential - feelings, something no computer machine can do. The biological model is neurons that wire together during an organism's experiences, in multiple sensory and limbic systems, such that later firing of even a part of the wired neurons can stimulate firing of all or part of the original complex experience. The psychological aspect of ERR is that \"subjective experience\" depends on the diverse and unique past life experiences of each individual, leading to what David Chalmers calls the \"hard problem\" of consciousness, Thomas Nagel's \"what it's like to be...\". Where neurobiologist Donald Hebb famously argued that \"neurons that fire together wire together,\" our experience recorder and reproducer ERR model assumes that \"neurons that have been wired together will fire together.\" If just some of those wired-together neurons are fired by a new experience, many more of them may fire again, explaining many aspects of memory, feelings, and the association of ideas. Neuroscientists are investigating how diverse signals from multiple pathways can be unified in the brain. We offer no specific insight into these \"binding\" problems. Nor can we shed much light on the question of philosophical \"meaning\" of any given information structure, beyond the obvious relevance (survival value) for the organism of remembering, and thus learning from, past experiences. A conscious being is constantly recording information about its perceptions of the external world, and most importantly for ERR, it is simultaneously recording its feelings. Sensory data such as sights, sounds, smells, tastes, and tactile sensations are recorded in a sequence along with pleasure and pain states, fear and comfort levels, etc. All these experiential and emotional data are recorded in association with one another. This means that when the experiences are reproduced (\"played back\" when some of their interconnected neurons are fired by something in current experience), the accompanying emotions are once again felt, in synchronization. The capability of reproducing experiences is critical to learning from past experiences, so as to make them guides for action in future experiences. The ERR model is the minimal mind model that provides for such learning by living organisms. ERR also explains the uncontrollable and unpleasant recall of past negative experiences that generates post-traumatic stress disorders. The ERR model does not need a single \"central processor unit (CPU) or even several \"parallel processors.\" It does not use computer-like \"data retrieval,\" based on the \"address\" of the data, to reproduce past experiences. All that is required is that past experiences \"play back\" (are reproduced) whenever they are stimulated by present experiences that resemble the past experiences in one or more ways. When the organism recreates past experiences by acting them out, they become \"habitual\" and \"subconscious\" information structures. This repetition, with the random variations caused by noise in recall, subtly changes the recorded experiences over time. It is critical that the original emotions also play back, along with any variations in current emotions that are experienced on playback. ERR then becomes an explanatory basis for conditioning experiments, classical Pavlovian and behaviorist operant conditioning, and in general a model for associative learning. Bernard Baars's Global Workspace Theory uses the metaphor of a \"Theater of Consciousness,\" in which there is an audience of purposeful agents calling for the attention of the executive on stage. In the ERR model, vast numbers of past experiences clamor for the attention of the central executive at all times, whenever anything in current experience has some resemblance. Global Workspace Theory is a version of the \"blackboard\" model of Allan Newell and Herbert Simon, concepts written on the blackboard call up similar concepts by association from deep memory structures. The ERR model supports this view, and explains the mechanism by which concepts (past experiences) are retrieved and come to the blackboard. In Daniel Dennett's consciousness model, the mind is made up of innumerable functional homunculi, each with its own goals and purposes. Some of Dennett's homunculi are information structures in the genes, which transmit \"learning\" or \"knowledge\" from generation to generation by heredity alone. Others are environmentally and socially conditioned, or consciously learned through cultural transmission of information. If we define \"current experience\" as all afferent perceptions plus the current contents of consciousness itself, we get a dynamic self-referential system with plenty of opportunities for negative and positive feedback. William James's description of a \"stream of consciousness\" together with a \"blooming, buzzing confusion\" of the unconscious appear to describe the ERR model very well.\nThe Elements of Consciousness\nFour \"Levels\" of Consciousness\nInstinctive Consciousness - by animals with little or no learning capability. Automatic reactions to environmental conditions are transmitted genetically. Information about past experiences (by prior generations of the organism) is only present implicitly in the inherited reactions. Learned Consciousness - for animals whose past experiences guide current choices. Conscious, but mostly habitual, reactions are developed through experience, including instruction by parents and peers. Predictive Consciousness - The Sequencer in the ERR system can play back beyond the current situation, allowing the organism to use imagination and foresight to evaluate the future consequences of its choices. Reflective (Normative) Consciousness– in which conscious deliberation about values influences the choice of behaviors.All four levels are emergent, in the sense that they did not exist in the lower, earlier levels of biological evolution.","CPU is the brain of a computer system. All major calculations and comparisons performed by a computer are carried out inside its CPU. CPU is also responsible for activating and controlling the operations of other units of a computer system. Hence, no other single component of a computer determines its overall performance as much as its CPU.\nIn order to be able to evaluate a computer’s capabilities quickly, it is important to know how CPUs are internally structured, how different CPUs differ from each other, and how CPU speed is evaluated.\n- [#1] – Introduction to Computer Fundamentals\n- [#2] – Basic computer organization\n- [#3] – Number systems\n- [#4] – Computer Codes\n- [#5] – Computer Arithmetic\n- [#6] – Processor and Memory\n- [#7] – Secondary Storage Devices\n- [#8] – Input-Output Devices\n- [#9] – Computer software\n- [#10] – Planning the Computer Program\n- [#11] – Computer Languages\n- [#12] – System Implementation and Operation\n- [#13] – Operating Systems\n- [#14] – Application Software packages\n- [#15] – Business Data Processing\n- [#16] – Data Communications and Computer Networks\n- [#17] – The Internet\n- [#18] – Multimedia\n- [#19] – Classification of Computers\n- [#20] – Introduction to C Programming Language\nPoints To Remember:\n- CPU is the brain of a computer system. All major calculations and comparisons performed by a computer are carried out inside its CPU. CPU is also responsible for activating and controlling the operations of other units of the computer system. Hence, no other single component of a computer determines its overall performance as much as its CPU.\n- The two basic components of a CPU are the control unit and the arithmetic logic unit.\n- The control unit (CU) of a computer’s CPU acts as the central nervous system for all other components of the computer. It manages and coordinates the entire computer system including the input and output units. It obtains instructions from the program stored in the main memory, interprets the instructions, and issues signals that cause other units of the system to execute them.\n- The Arithmetic Logic Unit (ALU) of a computer’s CPU is the place where the actual execution of the instructions takes place during data processing operation.\n- Every CPU has built-in ability to execute a set of machine instructions, called its instruction set.\n- As instructions are interpreted and executed by a computer’s CPU, there is movement of information between various units of the computer. In order to handle this process satisfactoriluy and to speed up the rate of information transfer, a number of special memory units called registers are used. These registers are used to hold information on a temporary basis and are part of the CPU (not main memory).\n- The speed of a processor is related directly to a computer’s clock speed. Which is the number of pulses produced per second by the built-in electronic clock. This clock speed is measured in megahertz (MHz) or gigahertz (GHz).\n- The three commonly known processor architectures are CISC (Complex Instruction Set Computer), RISC (Reduced Instruction Set Computer), and EPIC (Explicitly Parallel Instruction Computing).\n- Multicore processor technology enables building of computers with better overall system performance by handling more work in parallel. In this technology, a processor chip has multiple cooler-running, more energy-efficient processing cores instead of one increasingly powerful core. Multiple programs/threads can be run at the same time on a multicore chip each core handling a separate program/thread. To take advantage of multicore chips, applications must be redesigned so that the processor can run them as multiple threads.\n- Every computer has a temporary storage area built into the computer hardware. Instructions and data of a program reside in this area mainly when the CPU is executing the program. This storage space is known as primary storage, main memory, or simply memory.\n- Any storage unit of a computer system is characterized and evaluated based on following properties – storage capacity, access time, cost per bit of storage, volatile, and random access.\n- A primary storage or main memory of a computer system is made up of several small storage areas called locations or cells. Each of these locations can store a fixed number of characters (equal to its word-length in bytes). In these computers, storage space is always allocated in multiples of word-length. On the other hand, in a character-addressable computer each numbered address can store only a single character (A,B,1,2,+,- etc.).\n- Main memory capacity of large computer systems is normally more than that of smaller systems. This capacity is defined in terms of the number of bytes a computer system can store. Memory capacity of a computer system is stated normally in terms of kilobytes (KB), which is equal to 1024 (210) bytes of storage, or megabytes (MB), which is equal to 1,048,576 (220) bytes of storage, or gigabytes (GB), which is equal to 1,073,741,824 (230) bytes of storage.\n- A computer’s main memory is built of volatile RAM chips.\n- A special type of RAM, called read-only memory (ROM), is non-volatile memory chip in which data is stored permanently and cannot be altered by the programmer. There are two types of read-only memory (ROM) – manufacturer-programmed and user-programmed. The latter is commonly known as Programmable Read-Only Memory (PROM) because a user can program it.\n- Once information is stored in a ROM or PROM chip it cannot be altered. However, Erasable Prgrammable Read-Only Memory (EPROM) chips can be reprogrammed to store new information. EPROM chips are of two types – Ultra Violet EPROM (UVEPROM) and Electrically EPROM (EEPROM). EEPROM is also known as flash memory.\n- Cache memory is an extremely fast and small memory between CPU and main memory. Its access time is closer to the processing speed of CPU. It acts as high-speed buffer between CPU and main memory and is used to temporarily store very active data and instructions during processing.\nList of Questions\n- List the main functions of CPU in a computer system.\n- What are the two main components of CPU of a computer system? List the main functions of each of these components.\n- Describe the role of decoder of a CPU.\n- What is a microprocessor?\n- What is an instruction set of a computer system? Do computers made by different manufacturers generally have the same or different instruction sets?\n- Machine language programs written for one computer will generally not run on another computer with a different CPU. Explain why.\n- What is a family of CPU? When do two CPUs belong to the same family?\n- When do we say that a computer is backward compatible with another computer? How this feature is useful for the users of these computers?\n- What are registers? Name some commonly used registers and briefly describe the function of each.\n- What does length of a register mean? What are the commonly used register-lengths in modern computer systems? Why is a larger register length desirable?\n- List the main steps involved in execution of an instruction by a computer’s CPU.\n- What is clock speed of a computer system? How does it influence the overall speed of a computer system?\n- Differentiate between CISC and RISC processors.\n- List the relative advantages and disadvantages of CISC and RISC processors.\n- What is a multicore processor? How does it differ from a single-core processor?\n- Explain the limitations of current transistor technology that has forced processor manufacturers to shift form making more powerful single-core processors to multicore processors.\n- Explain how a multicore processor can handle the execution of multiple simultaneous tasks more efficiently than a single-core processor.\n- Multicore processor technology adds to extra work for programmers. Explain why.\n- List the main advantages and current limitations of multicore processor technology as compared to single-core processor technology.\n- Why multicore processor technology is also referred to as energy-efficient or power-aware processor technology?\n- Why software licensing is a key issue with multicore chips?\n- List the key properties used to characterize and evaluate storage units of computer systems.\n- Explain the difference between volatile and non-volatile memory. Write an example of each type of memory.\n- Explain the difference among random access, pseudo-random access, and sequential access storage units. Write an example of each type of storage unit.\n- Differentiate among a bit, a byte, and a word.\n- Explain the difference between memory read and write operations.\n- Explain why it is advantageous to have more number of bit per word instead of having more words of shorter length in the memory architecture of a computer system.\n- Distinguish between word-addressable and character-addressable computers. Discuss their relative advantages and disadvantages.\n- How many bytes will be required to store the word ‘MEMORY’ in (a) a character-addressable computer, (b) a word-addressable computer having word-length of 64 bits?\n- Name some commonly used units for stating memory capacity. Roughly, how many characters does each unit represent?\n- A computer has 512 MB of memory. How many characters can be stored in its memory at a time?\n- What is a ROM? Why it is so called? Write few typical uses of ROM.\n- What is a micro-program? Write an example of a micro-program that is a good candidate for storage in the ROM of a computer system.\n- Differentiate between PROM and EPROM.\n- Differentiate between UVEPROM and EEPROM.\n- What is a flash memory? why it is so called?\n- Differentiate among RAM, ROM, PROM, and EPROM.\n- What is a cache memory? How it is different from a primary memory?\n- Explain how a cache memory helps in improving the overall processing speed of a computer system?\n- Write short notes on:\nCentral Processing Unit (CPU)\nArithmetic Logic Unit (ALU)\nStorage evaluation criteria\nVolatile and Non-volatile storage\nFixed and Variable word-length memory\n- Write the full form of the following abbreviations:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:0cb95cf4-e110-4e61-9914-53302d54bb7d>","<urn:uuid:ed12f433-2551-4de4-b984-5ff61d8a7f7f>"],"error":null}
{"question":"How do you properly dry water-damaged photos and books, and what essential steps in creating wax figures involve drying processes? As someone recovering from a house flood, I need detailed guidance.","answer":"For water-damaged photos and books, you should dry them in a cool indoor area with good air circulation. Books should be dried slowly - large volumes flat, smaller ones upright - using absorbent materials between pages. Photos should be dried face up on a table or non-rust screen, avoiding direct sunlight. For both items, freezing can stabilize them for later treatment. For wax figures, drying processes are critical in multiple steps: after creating the mold, the wax mixture must be left to thoroughly harden before opening. Later, when applying skin coloring, the wax and oil paint mixture must be carefully dried to achieve the exact skin tone match. The process requires controlled conditions to prevent warping or damage in both restoration and figure creation.","context":["How to Make a Wax Figure\nEach life-size, life-like wax figure is made by our Hollywood Wax Museum team of seven different artists, and each one takes at least three months to create. Click the photos to see our step-by-step wax figure creative process.\nStep 1: Creating a Clay Sculpture\nArmed with photographs, actual measurements, and scanned images, the sculptor transforms a common lump of clay into a perfect likeness that captures the look and feel of life. From facial expressions right down to every mole and wrinkle, the clay head and face are created in perfect proportion.\nStep 2: Making a Mold\nOur fabricator forms an algenate mold from the clay sculpture. Then, he carefully pours a molten mixture of natural and petroleum-based wax inside. The wax is swirled, layer upon layer, until the mold is evenly coated with a two-inch thickness of wax. The mixture is left to thoroughly harden before the mold encasing the head is opened to reveal a rough waxen image.\nStep 3: Whittling the Wax\nUsing specially-designed instruments fashioned after surgical tools, the wax artist removes excess wax from the nostrils, ears and mouth. He then makes intricate refinements by delicately etching the wax for days. First, an authentic facial expression is created with the appropriate wrinkles, fine lines and unique markings in each forehead and face. The masterpiece is completed by carving the detailed features of the eyes, ears, nose and lips.\nStep 4: Bringing the Wax to Life\nFirst, the craftsman mixes colored wax and oil paint to create an exact skin tone match. He gently applies the color like make-up, adding details such as freckles, birthmarks and age marks. Next, hair is inserted with a special needle, one strand at a time, including eyebrows, lashes and facial hair. Finally, medical glass eyes and porcelain teeth are added to duplicate the person's actual characteristics.\nStep 5: Handling Hands & Other Parts\nThe wax artist creates hands using the same sculpting, molding, carving and painting techniques as crafting the head. But, there is an added challenge of matching the person's stance and hand gestures. Right down to each vein, knuckle and fingernail, our team matches the person's skin tone, paints the distinctive markings and inserts hairs with a tiny needle.\nStep 6: Crafting the Costume\nThe tailor places linen over the figure's body and creates a pattern for each piece of clothing. With the goal of complete authenticity in mind, the costumer locates fabrics and decorative materials (metal studs, buckles, beads, and sequins) to hand-make each item, including coats, dresses, boots, headdresses, belts, capes, chaps, bathing suits, and more. Cleverly-concealed Velcro closings are sewn in to help the costumer dress the figure.\nStep 7: Prepping the Props\nMatching the look and placement of every accessory is top priority for the prop master. Prop houses, vintage stores, auctions, and the studios, are resources for items such as military medals, crests, distinctive buttons, and logos. Often our prop master borrows an original sword, pair of sunglasses or other prop from the studio. He makes a mold and finishes the piece exactly as it appeared on screen.","COLLECTIBLES — Column #674 Copyright © Rinker Enterprises, Inc. 1999\nWater Damage with Special Emphasis on Flood Damage – Part III\nThis column is the third of a four-part series on dealing with water damage with special emphasis on flood damage. Part I presented information about Heritage Preservation (www.heritagepreservation.org), “The Emergency Response and Salvage Wheel,” planning ahead, and weathering the immediate emergency. Part II focused on the actions required to clean up a site and the initial steps in recovering water-damaged property. This column deals specifically with water damage recovery techniques relating to paper, everything from books to family photographs.\nBasic Steps That Apply To All Materials\nWhen objects have been heavily water-damaged, there are a few basic steps that apply to all objects whether books, ceramics, glass, furniture, paper, etc.\nEstablish a work area. Ideally, it will be on site. If not, you will have to move objects to it. The site needs plenty of workspace, an adequate water supply, and an indoor drying area that can be humidity controlled. You may have to build the latter.\nWhile an object is still wet, rinse it with clear water. A very fine, low-volume hose spray is ideal. Use soft brushes or damp cloths to clean off silt and debris. Be gentle. Avoid grinding the dirt and silt into the object. Dirt and silt is an abrasive. Over energetic cleaning will scratch an object. Dry the object with a clean, soft cloth or paper towel.\nDo not, repeat, not dry objects in sunlight or other high temperature environments. Drying an object too quickly can cause it to buckle, split, or warp. Air dry objects indoors where you have some control over the temperature and humidity.\nIf you transported the objects from the site to the work area in plastic bags, remove them from the bags as quickly as possible. In fact, do not seal them at all. Keep the bags open to allow air to circulate. These steps are designed to prevent mold from forming.\nThe development of mold and mildew are major concerns. Use these steps to delay or hopefully to prevent their occurrence: (1) after determining which tact is needed, increase airflow using fans, open windows, air conditioners, or dehumidifiers and (2) provide moderate exposure to light (which kills some mildew) by opening window shades or leaving lights on.\nWet paper tears easily. If possible use a plastic or other form of rust proof screen to support paper during the removal and recovery process. Carefully rinse the paper item to remove as much dirt and silt as possible. Be extremely careful with folded paper. It may make more sense to dry the object first before trying to unfold it.\nIf space permits, dry each sheet individually. If space is limited, dry documents in one-quarter inch or less piles, placing absorbent material between the sheets. Remove and replace when the sheets become wet.\nIf the number of documents is large, place a piece of wax or freezer paper between each document, create stacks (ranging between one and three inches), and wrap in freezer paper. Place these stacks supported in an upright position in a plastic or wire container in preparation for freezing. Never pack a container more than ninety percent full. Allow room for expansion due to freezing.\nFreezing stabilizes paper objects for months. It stops dye transfer, ink running, mold growth, and swelling. A sub-zero commercial freezer is the best choice. A home freezer is an adequate second choice. For extremely large paper collections, consider a refrigerated truck.\nFocus your efforts on your most important books. Assign the lowest priority to books that can be easily replaced or have little meaning to you.\nWater-damaged books can be very heavy and fragile. Use both hands when picking up one. Transport books to the recovery site in a plastic crate (milk crates are great) or wire basket. Do not use a paper container, e.g., cardboard box.\nKeep books closed until you are ready to work on them. The goal is to create a slow drying process that allows recovery. Temperature and relative humidity are critical. Work on books only in areas that are dry, cool and comfortable (warm or hot is very bad), and have plenty of air circulation. Never attempt to dry books in an oven or microwave or with a hair dryer or iron.\nDry large volumes flat, smaller volumes upright. Begin by placing sheets of absorbent material (blotters, plain newsprint, paper towels, etc.) between the pages. If the page count is high, you have to do this part of the drying process in several steps. Beware of adding too many sheets and destroying a book’s binding. Change the absorbent material as it becomes wet.\nWhen wet has been reduced to damp, stand the book upright on its driest edge and fan the pages. If you have fans, make certain the book’s spine or binding, not the open pages, is facing the breeze. When the book is dry but still cool to the touch, lay it flat and put a small weight on it. Check it twice a day for mold growth. Remove any mold you find immediately.\nBooks printed on coated paper, e.g., most coffee table and illustrated volumes, need to be treated differently. The coated, smooth, shiny paper will stick together when wet. In order to avoid this, insert a piece of wax paper instead of absorbent paper between each page in the drying process.\nFreeze any books that you cannot air dry within the first two days of your recovery operation. Wrap the books in wax paper and pack them spine down in a sturdy plastic or wire container. Defrost and work on the books as time permits.\nChances are you will not have the time or energy to save every photograph. You have tough decisions to make. Once you make them, do not look back. Remember, saving some photographs is far better than losing them all.\nDamaged photographs for which there are no negatives have the highest priority. If photographs stick together or become moldy, they are extremely difficult, if not impossible, to save. Immediately remove those photographs you wish to save from albums and frames.\nAs with water-damaged books, photographs become fragile when wet. Handle them carefully. Immediately rinse with clean water. Cut wax paper and place it between the photographs. Place the photographs in small stacks, eight to ten per stack, into a Zip-Lock type plastic bag and put them in a freezer to be worked on later. This step allows you to deal with other immediate preservation concerns. When the initial crisis is over, the photographs can be defrosted, separated, and air-dried. Never freeze glass plate negatives.\nIf you have the space and time, you can avoid the freezing process. Dry the photographs face up on a large table, non-rust window screen, or large piece of plastic laid on the ground and covered with absorbent paper if available. Do not dry them in direct sunlight. The photographs will curl as they dry. You can contact a photograph expert later about flattening them.\nFollow the same steps for negatives.\nPart IV, the final installment in this series, completes the steps necessary to recover water-damaged objects by concentrating on ceramics, furniture, glass, metal, and textiles."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:f8d17fc0-ecef-4059-9bb1-2d2bec8ef822>","<urn:uuid:1b21e7ae-e572-4bba-b81d-5f9def877810>"],"error":null}
{"question":"What specific evidence must entrepreneurs present in problem and solution slides, and how does this relate to their exit planning?","answer":"In problem slides, entrepreneurs must present specific evidence including: customer needs, problem importance, target customer identification, independent market research proving market existence, specific market size, and growth expectations. For solution slides, they must demonstrate: product specifics, functionality, unique features, cost-effectiveness, proprietary technologies, patents, manufacturing details, and required investments. This initial presentation of evidence relates directly to exit planning, as research shows that entrepreneurs with more innovative opportunities tend to develop financial harvest exit strategies, while those with less innovative businesses typically pursue voluntary cessation strategies. The strength of the problem-solution fit and innovation level influences whether founders can pursue profitable exits through IPOs or acquisitions versus other exit paths.","context":["If marketing is the art of understanding customer needs and then creating solutions to meet those needs at a profit; then nowhere is marketing more important for an entrepreneur than at seeking funding from investors at start up and subsequent rounds.\nPitching to invetors is about understanding what investors need to know about you and your venture.\nFollow The 10-20-30 Rule:\n10 Slide Presentation\n- Title – see below\n- Problem – see below\n- Solution – see below\n- Competitive Position (real analysis)\n- Team (industry knowledge, track record, expertise,)\n- Business Strategy (the plan to grow beyond launch)\n- Financial Projections\n- Funding Sought (amount, use)\n- Milestones (product launch, next funding, breakeven, etc)\n- Exit Strategy (IPO, acquisition, who?)\n20 minutes duration\n‘Tell them what you are going to tell them’ opening\n1.Title Slide: show them where you are going to take them- the agenda. It tells the skeleton of your whole pitch in a nutshell. The key elements might be numbered and subsequent Slides/Points reinforce the elements in turn. Put forward a clear, simple case\n2.Problem Slide: show the simple ABC situation/gap analysis:\n- A = Today (the current market situation)\n- B = Tomorrow (the place where the market should be / the big opportunity)\n- C = Gap (what’s missing to get there/ the special play you have to fill the gap)\nTell them why your play is better than everyone else’s\n3.Solution Slide: set out your clear market positioning – ‘We have the only X product that solves Y customers problem in Z unique way and we can back this up with – team’s track record, customer traction, real competitive analysis, etc’\n- X = Product Category (the specific product type market for your business)\n- Y = Target Buyer (the person who actually writes the cheque)\n- Z = Differentiation (advantage or positive distinction over the competition)\nProofs are better than ‘claims’.\nGo with the best foot first\nAdapt the organisation of the pitch and how you tell it to the stage of your company’s development. Always put the strongest case elements first.\nAt Seed Financing\nRelatively small amounts of capital are made available to support the entrepreneur’s exploration of an idea or product concept.\nPitch first about initial market validation (quotes from prospects in target industry), then about the product specification, the team (such as it is) and then the other slides.\nAt Start Up Financing\nA commitment of more significant funds is made to an organisation that is prepared to commence operations. The company has a clear competitive advantage and a prototype. Capital is primarily for production and initial marketing.\nPitch first about initial customer traction (some demonstration of willingness to try and pay for the product), then the best real numbers you have, the product specification, the team and then the other slides.\nAt First-Stage Financing\nFunding for an up and running business. The venture is normally not yet profitable, an established organisation, a working product and preferably some revenues. Capital is primarily for the company’s first major marketing efforts, to hire sales and support personnel in anticipation of higher sales volumes. Product enhancement and line expansion is also typical.\nPitch first about the momentum of the business (the progress made, milestones achieved), then show the sales numbers and the trends, then the product specifications, the team and then the other slides.\nWrap up, recap and go for the close\nTell them again in summation what your pitch story was and deliver your prepared, though-out, aggressive enough ask.\nHandle Questions competently\nAlways show progress as a team, product and with customers. Understand the management, market, financial, technological and operational risks to your venture and the industry it will compete in – Investors will ask.\nKnow the 10 companies across the different verticals who will want to acquire your venture and why – Investors will ask.\nKeep the business model specific, simple and non-aggressive. It shouldn’t be a total innovation.\nThe deal must make lots of money for the investor – multiples of 5 to 10 x initial capital to be earned in an exit 5 to 8 years down the line\nSlide 2 ‘Problem’\n- What specific problem or need do customers have?\n- Why is the problem important?\n- Who, specifically is the customer?\n- How do we know the market exists? What independent evidence can you cite, such as independent market research?\n- How large is the specific (narrowly defined) market for your product?\n- What growth is expected in this market?\n- Are the market size estimates realistic?\nFor industrial product companies…\n- What 2-3 industries comprise the most important prospects in Year-1? In Year-3?\n- What are the job titles of the buyers (decision-makers) in these prospects?\nFor consumer product companies…\n- What are the demographics of the 2-3 most important customer segementsw in Year 1? Year 3?\nSlide 3 ‘Solution’\n- What, specifically, are the company’s products?\n- What do the products do?\n- Why would the customer buy these products?\n- What makes the products unique or special?\n- In general, how are they better than other products or alternative methods of solving the problem?\n- How much better are they than other solutions?\n- Can we demonstrate that they are cost effective?\n- What, if any, proprietary technologies are used to make them? Any proprietary process?\n- Are there patents? If so, what, specifically, do they protect?\n- Why will they be of value to the company?\n- What special issues relate to manufacturing the product(s)? Any special materials or processes?\n- What special equipment or facilities are required?\n- What investment is required to set up manufacturing? For what capacity?\n- How do you know you can manufacture the product at a cost that will yield acceptable gross margins?\nSlide 4 ‘Competitive Position’\n- How else can the customer solve the problem your products solve?\n- What are the alternatives?\n- How does your product compare to each?\n- Why is it better?\n- In what ways is it worse?\n- Who are the vendors of these other solutions?\n- How do they compete with each other?\n- Where will you fit into the industry?\n- Why will you be able to compete effectively against them for the next ten years?\n- Why are you confident that no new entrant will come along with a better solution and blow you away?\n- Why do you think you can dominate your market niche?\n- How does it meet the 5 criteria for early Adoption\n- Better than current idea\n- Compatible with existing people, processes and technology\n- Ease of Use\n- Easy to try out\n- Easy to see benefits\nSlide 5 ‘Team’\n- What is your background and previous experience?\n- Where did the idea for the company come from?\n- How did you get involved with the company?\n- Who is presently involved in managing the company?\n- What are their credentials?\n- Why will they be able to build a successful company?\n- If not all management spots are filled, what is the plan for filling them?\n- What kind of people are you seeking? To fill what roles?\n- f you do not expect to be the CEO that builds the business to $10 or 20 million, what kind of person would you bring in? When?\n- Who is on your board of directors and board of advisors ?\n- How does the board function?\nSlide 6 ‘Business Strategy’\n- How has it been funded to date?\n- What is the business model? (i.e. what will produce the company’s revenue?\n- What kind of gross margins will the company have?\n- What level of operating profit can the business generate?\n- Do you have (or plan) any corporate partnerships in place?\n- What are the significant risks your business faces?\n- What needs to be done to finish your first product(s)? What’s your next act?\n- Do you rely on outside contractors? How much do you license from others?\n- What expertise do you have at developing this kind of product?\n- What development challenges are most important or difficult to overcome? How do you intend to do so?\nSlide 7 ‘Financial Projections’\n- What kind of revenues can the business produce, on an annual basis, over the next five years?\n- What investment is required to carry the company to the next major level of valuation?\n- When do you expect the next rounds to take place?\n- What specific tasks need to be accomplished to do that?\n- How long will it take? (Try to identify a “next level” that can be achieved in less than 18 months.)\n- What investment will be required beyond that?\n- To the extent possible, explain key assumptions behind your forecast. And make sure the forecast relates in a logical way to the market forecasts you described previously.\n- How will the investor get his money back? Through an IPO? Acquisition? When?\nSlide 8 ‘Funding Sought’\n- How much hard-money (cash) have the founders put in?\n- How much cash have Directors and Advisory Board members invested?\n- What equity is available to recruit key executives?\n- How did you arrive at your pre-money valuation for this round?\n- What comparables are you using for your proposed IPO/exit round?\nSlide 9 ‘Milestones’\n- What is your track record at hitting schedules on similar efforts?\n- Are you fully-staffed for the work indicated in the schedule?\n- How are you going to get your partners to meet your schedule?\n- What makes you think you can achieve this schedule when “X” failed?\n- What contingencies have you built into the schedule? The budget?\nSlide 10 ‘Exit Strategy’\n- Why won’t one of your established competitors step in and leapfrog you?\n- How long do you think you can maintain your lead, thus preserving your company’s value?\n- Why would this be an exciting business opportunity for an acquirer?\n- Why would it an exciting IPO opportunity?\n- What are the three most serious risks the company faces?","Author Information : Alexander McKelvie (Whitman School of Management, Syracuse University)\nDawn R. DeTienne (College of Business, Colorado State University)\nGaylen N. Chandler (Barton School of Business, Wichita State University)\nYear of Publication : Journal of Business Venturing (2015)\nSummary of Findings : While exit strategies are important to entrepreneurs, there is little understanding of what might drive a founder to develop one exit strategy over another; we predict and examine the factors that lead to the development of financial harvest, stewardship and voluntary cessation exit strategies.\nResearch Questions : 1. What are the main factors attributed to an exit strategy for entrepreneurial founders?\n2. What are the unique personal and business predictors for each of the exit strategy types: financial harvest, stewardship and voluntary cessation?\nWhat we know : Entrepreneurial exits, and in particular exit strategies, are an important part of entrepreneurship. But how these develop in entrepreneurs is not well understood.\nWe provide a lens through which to better understand how entrepreneurs consider how they are going to exit their business at some time in the future. These factors are known to influence the decisions and behaviors that the entrepreneur engages in.\nDevise that types of exit strategies can be put into categories, with predictors of each. This takes into consideration that not all entrepreneurs want to “cash out” by selling their business, but rather captures a diversity in personal motives and business factors. These categories are predicted by different sets of factors, which shows that many complex decisions are being made.\nNovel Findings : Show that the seven types of exit strategies can be put into consistent categories.\nEntrepreneurs with financial harvest strategies have higher levels of extrinsic motivation, larger founding teams, more innovative opportunities and use causation.\nEntrepreneurs with stewardship exit strategies have lower levels of extrinsic motivation, higher levels of motivation for autonomy, smaller founding teams size but larger numbers of employees.\nFounders with voluntary cessation exit strategies had smaller numbers of employees, lower innovative businesses and tend not to use causation-based decision making practices.\nNovel Methodology : Surveyed 189 founders during the formative years of their business. Usually exit strategies are studied at a later time in business development or after the founder has already exited the firm.\nImplications for Practice : Provides new insights into how individual- and business factors relating to the entrepreneur's motivation, decision-making process, opportunity, team size and number of employees affect the development of an exit strategy.\nSuggests important differences between financial harvest, stewardship, and voluntary cessation exit strategies.\nImplications on Research: First research to offer a systemic analysis of the factors that will predict the type of exit strategies different founders will pursue.\nContribute early stage study of an important part of the overall entrepreneurial process, but one that is not well understood.\nProvides further evidence of founder motives, although first to see that applied to the study of exit strategies\nFull Citations : DeTienne, Dawn R., McKelvie, Alexander and Chandler, Gaylen N., 2015, Making sense of entrepreneurial exit strategies: A typology and test, Journal of Business Venturing 30: 255-272.\nAbstract : Entrepreneurial exit is a major event in the development of a venture. However, we have little understanding of the factors that drive the development of an important pre-cursor to exit: the exit strategy of the founder. Based on the existing literature, we develop a typology of entrepreneurial exit strategies consisting of three higher-level exit categories (i.e., financial harvest, stewardship, and voluntary cessation) and develop an initial test of our typology. Specifically, we examine entrepreneurs' perceived innovativeness of their opportunity, motivational considerations, decision-making approach, founding team, and firm size. Our results show different predictors for each of the three exit strategy types and represent a significant contribution to the understanding of exit strategies in new ventures.\nThis study predicts and examines the factors that lead to the development of financial harvest, stewardship and voluntary cessation exit strategies.\nLatest posts by Alexander McKelvie (see all)\n- Signaling for More Money: The Roles of Founders’ Human Capital and Investor Prominence in Resource Acquisition across Different Stages of Firm Development - March 13, 2018\n- Variable risk preferences in new venture growth and survival - May 16, 2016\n- Making sense of entrepreneurial exit strategies: A typology and test - August 13, 2015"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:b9b225e4-557f-4991-b76a-398e6a894b7b>","<urn:uuid:77ce7266-e4df-435c-85db-2d3fe990ff9c>"],"error":null}
{"question":"What are the primary contact points between a shooter and their shotgun compared to how imaging machines interface with patients?","answer":"A shotgun connects with the shooter at five specific points: the recoil pad against the shoulder pocket, the cheekbone on the stock's comb, the back hand gripping the stock's wrist, the trigger finger on the trigger blade, and the leading hand on the forearm or barrels. In contrast, imaging machines interface with patients primarily through a bed or table that the patient lies on. In both CT and MRI scans, the patient must lie completely still on a bed that moves through the machine's gantry. For CT scans, the X-ray tube rotates around the patient while they lie on the table, while in MRI, the patient remains stationary while magnetic fields and radio waves create the images. Some patients may require light sedation if they have trouble staying still or are claustrophobic.","context":["But according to seasoned gun fitter Bryan Bilinski, owner of FIELDSPORT, “I know Mr. Average is out there somewhere and he may basically exist, unfortunately I have never met him”.\nThe good news is that if you’re fortunate enough to order and purchase a truly custom bespoke over/under or side-by-side shotgun that has been fitted to within a whisker of perfection, don’t bother to read on. Just count your blessings and enjoy using a gun that is effortless to mount and one that you’re confident shoots exactly where your eyes are looking.\nFor the rest of us wingshooters and clay target disciples, sooner or later we realize a visit to an experienced, master gun fitter will take our shooting to the next level. It’s the ultimate quest to gain comfort and confidence in our shotgun, with our ability to properly shoot of paramount importance.\nA shotgun connects with your body at five points: 1) the recoil pad or butt against your shoulder pocket, 2) your cheek bone ledge mated to the comb of the stock, 3) your back, trigger hand firmly gripping the wrist of the stock, 4) your index finger tip touching the trigger blade and 5) the leading hand, the left hand for us right handed shooters, holding and properly gripping the forearm or barrels.\nIf all of these points of contact are correct, your shotgun should feel effortless to mount and swing with relative ease but most importantly position your dominant eye perfectly down the rib — making your shotgun shoot exactly where you’re looking.\nHowever, if you need to 1) crush your face down into the stock to achieve eye alignment, 2) lift your face to see over the standing breech, 3) assume some kind of unnatural and uncomfortable stance or posture in order to accommodate yourself to a stock length, (that is unfortunately either way too long or way too short for you), 4) experience way too much felt recoil in either your shoulder pocket or on your cheek bone, 5) you have way too much muscle tension in your neck, shoulders or back, or 6) you consistently miss some type of target presentation and can’t seem to miss others, then you’re a good candidate for a custom gun fitting.\nEnter seasoned gun fitter and wingshooting instructor, Mr. Bilinski.\nHe founded and owns FIELDSPORT Ltd. located in Traverse City, Michigan. He is ranked as one of the top wingshooting instructors and gun fitters in America. More than 30 years of experience doesn’t hurt either. He also claims to be the man who introduced Sporting Clays to the U.S. when he worked for the Orvis Company in the early 1980s.\nDespite all the current press and rhetoric about gun fit, he explains that, in fact, the craft of gun fitting is actually quite new to Americans. Our shotgun industry has almost always based gun fit on the principal of Henry Ford’s Model T… “You can have any color Model T you want, as long as it’s black.”\nToday, American wingshooters and clay target shooters are more discerning and demanding than ever before. Very few are happy to accept mediocrity in any sport they enjoy. Simply put, they want to hit with “a load of shot” what they should be able to hit with “a load of shot.” When you also add all the new advancements in gun technology, an influx of more women and youngsters into the sport and the search for perfection that consumes more and more shotgun shooters today, you can begin to see why gun fit is quite possibly the last frontier in American wing and clays shooting.\nUnfortunately the men or women who actually make a year-round living conducting live-fire gun fittings for clients in the U.S. are ultimately quite rare. From that viewpoint, gunfitting in America is still novel to the shotgun sports.\nAt the turn of the last century, the bespoke shotgun was simply a matter of course in the United Kingdom for the affluent. If you lived and worked in England, you had a personal tailor and a bespoke shotgun made by the London trade. The aristocracy held a veritable monopoly on shotgun ownership. In fact, most of the best London and Birmingham gun makers employed a gun fitter who worked exclusively for them. This elite group of men conducted private shooting instructions and custom gun fittings at shooting grounds that functioned under prestigious gun maker’s names. These shooting instructors also became experts in a new fitting tool called the “try gun.” Many of these gun fitters and shooting coaches became famous in their own right.\nWhile the British were shooting driven pheasants on their private estates with their bespoke London Best shotguns, gun ownership and the functional use of shotguns here in the U.S. proved more egalitarian. Double guns made by the likes of Parker Brothers, A.H. Fox and L.C. Smith were often sold in hardware and sporting-goods stores. These firearms were primarily used for putting meat on the table while functioning under the roughest conditions. The shotguns were tools, not elegant bespoke art forms. Just as there were no bespoke shovels or picks, the notion of ordering a custom fitted shotgun for most rural based Americans was an unlikely extravagance and a cost-prohibitive luxury.\nFrom the perspective of a professional gun fitter, Mr. Bilinski believes America is still basically “a nation of riflemen.” It’s a fairly easy phenomenon to track. Most likely the first gun you were given was a BB gun with iron sights. The second was a pump or CO2 pellet gun, again with open, iron sights. The third was your 16th birthday present — perhaps a bright and shiny.22 bolt action rifle also with open sights for aiming. By now, you have become a rifleman, learning to shoot by aiming every gun you owned.\nWhen you finally received some shotgunning instructions with a hand trap behind the barn, you were probably told to “put the bead on the clay target” and pull the trigger. And the slippery slope of aiming a shotgun begins. A shotgun is meant to be “pointed” not aimed. You can aim at a sitting rabbit or squirrel out stretched on a limb, but try aiming at a fast flushing grouse or rooster pheasant with a 30-mile-per-hour tail wind. A gun you point must fit perfectly in order to shoot where you look and follow your gift of eye/hand coordination.\nWe are also not the same sized people or shooters we were pre-World War II. Mr. Bilinski notes that shotgun stocks made for “Mr. Average” from the 1800s to the 1950s were totally different than the stock dimensions needed for the shooters of today. Stocks made for that era of shooter were relatively short in length of pull, 14 inches or less. Stock drop at heel was more pronounced, often averaging 2 ½ inches to 3 inches or more. Cast of the stock was virtually irrelevant, with most stocks Cast “0” or just a 1/16 inch Off +/-. Not only have body types and physiques drastically changed, but shooting styles and wingshooting techniques have evolved as well.\nToday, we're entering a new age of shotgunning in America — a transition where having a custom fitted shotgun is becoming more the norm than the exception.\n“Mr. Average” today has come to the realization that it’s somewhat foolish to invest $ 3,000 to $ 6,000 (or more) for a fine over/under or side-by-side shotgun that isn’t properly fitted. We’re rapidly focusing in on having a shotgun properly fitted, regardless of the price or grade of gun. What good is a shotgun that isn’t comfortable to shoot and even more importantly doesn’t shoot exactly where you’re looking? The good news is that the shotgun industry is beginning to put more and more focus on gun fit, regardless of the price of the gun.\nThe most obvious reason to seek out help with a gun fitting is usually either disappointing scores on clay target disciplines or miserable percentages of hits on live game. Nobody likes to come back to camp with an empty game bag, especially if they had a few good shots they should have made. Unfortunately, many shotgun enthusiasts may not consider a poorly fitted shotgun as the source of their problems. They may simply believe they’re poor shots.\nMr. Bilinski’s 30-plus years in the gun business indicate that folks often make conscious efforts to adjust their body fit a shotgun. The common observation is shared daily in gun shops throughout America. A prospective customer is either handed or picks up the shotgun of choice, they mount it improperly to their shoulder first, they then look down and focus at the bead or front “sight” of the gun, adjust their eye alignment down the barrel and then says “Ohhh, this gun fits me.”\nThis ready-to-buy affirmation is often confirmed by the commissioned salesman. “Yes sir, I agree, that gun looks like it fits you perfectly sir, should I wrap it up?”\nFor Mr. Bilinski and his clients, a complete try gun fitting is a journey that could take a couple hours up to an afternoon to perform properly. When asked, “How long does a fitting take?” he typically answers “I really don’t know yet. The time it takes to properly perform a gun fitting directly interrelates to your level of skill. It is difficult to fit someone who actually needs more help perfecting their gun mounting skills before being able to take advantage of the value of a gunfitting session.”\nOne of the first elements he always checks before starting the fitting session is “eye dominance.” Knowing which eye is dominant is critical to shooting where you are looking and being able to be fitted properly.\nAnalyzing gun mounting abilities is also critical in gun fitting. If the components of the shotgun aren’t properly anchored at the five key contact points, more time will be needed to readjust and alter the try gun. A gun fitting simply cannot be rushed. If it takes four hours or more, so be it.\nWhen a new client arrives for a custom gun fitting, Mr. Bilinski needs to evaluate their shooting style, including stance, forward posture and whether or not they begin every gun mount from a “ready position.” Even if a client shows up with a near-perfect gun mount, it’s still going to take time to navigate the gun-fitting process. Mr. Bilinski says that he tries not to radically change an individual’s shooting style providing there aren’t matters of glaringly bad technique. Every fitting is a process of discovery and evolution.\nHe likes to summarize gun fit down to two simple, words, “anchor point.” The top, rounded edge of the comb perfectly complements the underside ledge of your cheek bone (also known as the zeugmatic arch). When your gun fits you well, you should be able to anchor the comb under your zeugmatic arch and have perfect eye alignment related to the rib or center line of your shotgun. Unfortunately, too many shotgun shooters think they should mount the gun to the shoulder first, and then move their head and attempt to align their dominant eye to the rib. This type of gun mounting technique is fraught with problems. Mounting the stock to your face first promotes consistently good shooting.\nDetermining your ideal “length of pull” is also critical to a well fitted gun. When you shoot a gun with an ideal length of pull for you, the center of the comb, also known as the “drop at face,” allows you to anchor the gun properly under the zeugmatic arch and with minimal head movement. When all the critical measurements of a properly fit gun are established the gift of a perfectly fit gun launches you with confidence on your journey as a wingshooter.\nMr. Bilinski’s primary tool of choice for a live fire gun fitting is a fully articulated, operational shotgun called a try-gun. In fact, FIELDSPORT owns five different try-guns: two over/unders and three side by sides. A try gun features a fully articulated stock that can be altered to move up and down and left to right, while the length of pull and pitch are adjusted through two rails that slide in and out of a metal carriage fitted inside the butt stock.\nAfter a gun-fitting session is completed, the client is given a fitting sheet that lists all the crucial measurements that a gun maker or stock maker needs to know in order to make a custom fitted stock for a “bespoke” shotgun.\nIn experienced hands, the try-gun is a forgiving tool, allowing the gun fitter to change dimensions relative to his observations during the fitting and also the actual pattern placement he sees and reads on the steel patterning plate. Mr. Bilinski explains the process is one that basically starts with a rough fit and end with a perfect fit. After the measurements are perfect for the person being fitted the results are confirmed on the fitting plate positioned 16 yards in front of the shooters eyes. One thing for sure, the fitting plate takes no prisoners and does not lie. Confidence comes from seeing that image of perfect pattern placement clearly evident on the fitting plate.\nWhile shotgun fittings are increasing in demand among the male population of American shotgunners, women and youngsters also represent a growing segment of shotgun shooters in search of perfectly fitted shotguns.\nTypically, most women are introduced to shotgunning by a well-intentioned but ill-advised husband, boyfriend or father. When gun fit finally enters into the scenario, the “significant other” assumes that by simply shortening the stock all the gun fit ills will be rectified. Normally, shortening the length of pull does help a high percentage of lady shooters, but that is akin to assuming that all women are the same height and body type (Ms. Average) and no other critical stock dimensions need to be changed.\nCompared to the gun fit needs of men, women have an entirely different set of gun fit needs and standards to follow, but that’s another chapter on gun fit.\n“Ultimately, an individual should leave a gun fitting session with one very important element, confidence,” says Mr. Bilinski. “Confidence that your gun not only shoots exactly where they are looking, but also is quite comfortable to shoot, the felt recoil being absorbed properly at all the key contact points of the body”.\nSo what does this service cost? When Mr. Bilinski utilizes his try guns, the cost averages around $ 450, a figure that is largely dependent on the time and effort it takes to complete the program. A person with a poor gun mount and a novice skill level takes a lot longer to fit than a Master Class shooter. When Mr. Bilinski does an “impact test” live fire gun fit analysis using your existing gun, the price is usually less. Regardless of the shooters skill level, he allocates three to four hours per fitting session.\nWhen you think of all the money you spent on guns that didn’t fit and you couldn’t shoot worth a darn, the investment in a custom gun fitting is money well spent. Most shooters only need to have one or two fittings done throughout their entire life. Ultimately, a custom gun fitting is a small price to pay in the search for shotgunning perfection.","Doctors routinely request diagnostic imaging tests to monitor what is happening inside your body. The various imaging tests help the doctor make a clear diagnosis and decide on the best treatment option.\nYour doctor can identify specific medical conditions with the use of images produced by these imaging tests.\nWhat is a CT Scan?\nCT, computerized axial tomography, produces images of the body, including bone, using rotating x-rays. The patient lying on the table is rotated around by the x-ray tube. The x-ray detector is located on the patient’s opposite side. This detector picks up the beam that passes through the patient.\nThe best uses for a CT scan (also called a CAT scan) are evaluating bone damage, identifying lung and chest conditions, and finding cancer.\n- Bone structure imaging using CT is effective.\n- Some people cannot have an MRI but can have a CT scan if they have specific surgical clips, cardiac monitors, metallic fragments, or pacemakers.\n- Compared to MRI, the total testing time is less with CT.\n- For claustrophobic people, CT may be more comfortable.\nWhat is an MRI?\nA strong magnet and pulsed radio waves are used in MRI (Radio Frequency or RF). Any axis of the body is used to reconstruct the acquired data into a two-dimensional image.\nSince bone is devoid of water, it does not produce any picture data. As a result, the photos have a black area. Therefore, the imaging of soft tissue is best suited for MRI scanners.\nDoctors can use MRI technology to see soft tissues like muscles, ligaments, tendons, bones, and organs. The spinal cord and nerves can also be seen.\nMRI is also used:\n- To identify sports injuries such as strained or sprained muscles, torn anterior cruciate ligaments, and ruptured Achilles tendons\n- Discover malignant and benign tumors\n- Recognize issues with the circulatory, digestive, circulatory, and respiratory systems.\n- Check for any abnormalities in the brain, such as aneurysms and tumors\n- Identify bone and cartilage diseases\n- See how much your joints are inflamed.\nThe noninvasive diagnostic imaging process known as computed tomography (CT scan or CAT scan) creates horizontal or axial images of the body (commonly referred to as slices) using a mix of X-rays and computer technology. The CT scan is a noninvasive, pain-free, and relatively safe process that doesn’t require recovery time.\nThe noninvasive medical imaging procedure known as magnetic resonance imaging, or MRI, creates precise images of every internal bodily structure, including the organs, bones, muscles, and blood arteries. MRI scanners create images of the body by using a powerful magnet and radio waves.\nSoft tissue, bone, and blood vessel details can all be seen in great detail in CT scans. MRI scans are better and more accurate at visualizing the ligaments, soft tissue, or organs.\nSoft tissue injury, ligament damage, and herniated discs may be simpler to detect as issues after an MRI scan. Medical professionals can use a CT scan to obtain images of organs, fractured heads, or physical components.\nIn a CT scan and an MRI scan, the patient is on a bed that gently rotates across the gantry as a narrow beam of x-rays is shot into the body by the x-ray tube in case of a CT scan. For MRI, the magnetic field momentarily realigns your body’s water molecules. Then, radio waves induce these aligned atoms to emit weak signals to make cross-sectional MRI pictures.\nYou must lie still and motionless throughout the scan. If you move, the MRI or CT scan images might need to be clarified. If you are claustrophobic (afraid of enclosed places), have trouble staying still, or suffer chronic pain, your primary care doctor might prescribe you a light sedative.\nAre CT and MRI contrast the same?\nMRI and CT use the same contrast materials, except iodine, which is only present in CT.\nWhat are the differences between MRI and FMRI?\nMRI and fMRI scan both employ the same fundamental atomic physics concepts. However, MRI scans show anatomical image structure, and fMRI scans show metabolic function. As a result, MRI scan results resemble three-dimensional photographs of anatomical structures.\nWhat can an MRI show that a CT scan cannot?\nA CT scan cannot detect some disorders, which is where MRI excels. Some tumors, including liver cancers, uterine cancers, and prostate cancers, are virtually undetectable or extremely difficult to find on a CT scan. An MRI is also better at showing bone and brain metastases.\nWhich is safer, MRI or CT scan?\nA tiny dosage of ionizing radiation is used in CT scans to create the images. However, an MRI scan doesn’t operate in this manner. Instead of ionizing radiation, it creates images using strong magnets and radio waves. So, in contrast to a CT scan or x-ray, you are not exposed to radiation when you undergo an MRI scan although it does take longer.\nWhy would a doctor order a CT scan instead of an MRI?\nThe doctor might order a CT scan instead of an MRI if a patient cannot have an MRI. Due to the strong magnet inside the machine, those with metal implants, pacemakers, or other implanted devices shouldn’t undergo an MRI.\nIs MRI more expensive than CT?\nMRIs are substantially more expensive than CT scans and other imaging procedures due to the higher equipment expenses. Additionally, reading these intricate images by radiologists requires additional time.\nWhy do they inject dye for an MRI?\nDye is injected because MRI scans sometimes require it as a contrast agent. Certain tissues and blood arteries become more distinct and detailed as a result.\nCan you do CT and MRI at the same time?\nCT and MRI data are spatially and temporally registered in a perfect CT-MRI scanner. Therefore, CT and MRI scans that were collected separately could be combined to mimic a simultaneous acquisition."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:6de256e6-016b-4bd3-a646-91bd190a7947>","<urn:uuid:6d8ca417-2ebb-431c-9184-233b2a5424fd>"],"error":null}