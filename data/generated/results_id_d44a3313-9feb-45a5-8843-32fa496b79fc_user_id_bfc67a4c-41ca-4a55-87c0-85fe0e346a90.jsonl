{"question":"What are the key differences between killifish and cardinal tetras in terms of tank size requirements and natural habitat?","answer":"Killifish do not require large aquariums and can be housed in small tanks, with a recommended size of 10-20 gallons for the main tank and as small as 2.5 gallons for breeding. They are naturally found in water holes, streams, and marshes in Africa. In contrast, cardinal tetras require a minimum 20-gallon tank due to their schooling nature, and while some say they can live in 10-gallon tanks, they need more space as they naturally form large schools numbering in the thousands. Cardinal tetras are found in the Orinoco River and Rio Negro tributary of the Amazon River, ranging as far west as western Columbia.","context":["My daughter has fallen in love with a type of fish called “killifish.” She wants to breed killifish but I am having a hard time finding them for sale in my area. Are they a tough fish to obtain? Are they aggressive? The name killifish makes me wonder. Also, are killifish difficult to breed? My daughter is 9 years old and I would be helping her along. Richard Saser Helena, Alabama\nI can see why your young daughter has fallen in love with killifish; they are very beautiful and interesting animals. The nicest part about keeping killifish is that they do not require large aquariums and can be housed and bred in small aquariums. Just so you know, the name killifish does not point to any aggressive tendencies in this freshwater fish. It is from the Dutch word “killy,” which means ditch or channel. Since many of these fish are found in areas of Africa inhabiting water holes, streams, or marshes, the name certainly fits. Killifish are sometimes mistaken for livebearers although they are not. Killies are egg layers and usually require a spawning mop in order to successfully breed in the fish aquarium.\nThe majority of the killifish available today are captive bred and very hardy, adjusting to a variety of water conditions. I can’t really imagine a fish that is easier to care for or more beautiful than killifish. My recommendation for your daughter would be to set up one aquarium to house several pairs of killifish together and another smaller aquarium for breeding.\nThe main aquarium could be 10 to 20 gallons in capacity with a 2 and one half gallon aquarium used for breeding. Killies seem to prefer softer acidic water but for all purposes you could shoot for a neutral pH value of 7. One thing that would make any killifish feel at home is the addition of live aquarium plants. Some of the hardier plants would be a good choice and incorporating java moss in the aquarium could make for a more natural spawning environment. The breeding aquarium need only contain a heater, sponge filtration and a spawning mop. Certain breeders believe that leaving the light off on the spawning aquarium can induce that behavior, though I have had good success with killies in a fully lit breeding aquarium.\nTo construct a spawning mop all you need is some thick cork and acrylic yarn. Be sure to choose acrylic yarn so that it doesn’t disintegrate in water. In addition to this, choose a yarn that is darker in color, say black or dark green, to make it much easier to spot eggs. Making a thick mop of yarn attached to the cork allows the yarn to float and gives the killies easy access to the mop.\nResearching on the internet how to make a spawning mop turns up all kinds of innovative designs. After the eggs have been laid the mop can be removed or kept in the aquarium depending on the aquarist’s breeding strategy. Overall, killifish are very easy to breed and produce valuable spawns.\nAs for killifish being hard to obtain that is something I am not sure about. I typically see at least some variety of killifish offered for sale at nearly every fish outlet I enter. You may be seeing killifish and not realizing they are in fact killies. Some species of killifish are not nearly as colorful and eye-catching as the brighter varieties. An option is obtaining a pair of killifish from an online fish outlet. Many websites sell a large variety of these animals and will pre-select a male/female pair for you.\nIf you are looking for a vivid killifish for your daughter to start with, and one that is easy to breed and care for I would recommend the Golden lyretail panchax killifish (Aphyosemion australe). These animals are very attractive and easy to care for.\nAll in all, getting a young person started keeping killifish not only makes for a great introduction into the aquarium hobby but could lead to a life-long passion. Many of the aquarists I know started with and still keep killifish today and are downright obsessed with these gorgeous little creatures!\nPosted By: Chewy Editorial\nFeatured Image: Via Day Donaldson/Flickr","|Common Name||Cardinal Tetra|\n|Scientific Name||Paracheirodon Axelrodi|\n|Adult Size||1.25-2 inches|\nCardinal Tetra Facts\n- Cardinal Tetras can be found in the Orinoco River to the Rio Negro tributary of the Amazon River. They have also been found in the wild as far west as western Columbia. Other locations have reported schools of Cardinal Tetra in their areas, but these are most likely fish that have been released into the wild.\n- Cardinal Tetras are schooling fish that prefer to live in large groups. In the wild, Cardinal Tetra schools can number in the thousands.\n- Cardinal Tetras are not considered easy to breed in captivity, and therefore, most of the Cardinal Tetras that you see for sale are wild caught.\nIdentification and Markings\nCardinal Tetras are a small, brightly adorned fish. They are often chosen for aquariums for their active nature, and flashy colors. They are an easy fish to keep, and with their small size they don’t require as much room as other species.\nCardinal Tetras have silvery, metallic bodies with vibrant red bellies. These colors divide the fish laterally with a line of bright, neon blue that runs through the middle. They have transparent fins, and large eyes in comparison to their much smaller mouths.\nCardinal Tetra Care\nCardinal Tetras are considered hardy, and are not difficult to keep happy for even beginners to the aquarium hobby. The most important thing for Cardinal Tetra health and happiness is clean water, and the correct water parameters. If their needs are being met, then Cardinal Tetras will thrive in the home aquarium.\nFood & Diet\nIn the wild, Cardinal Tetras can be found eating worms and small crustaceans. They are opportunistic feeders who will also eat plant matter. In captivity, they will readily eat dried or flaked foods with the occasional meaty snack. Cardinal Tetras should be fed twice per day, and only enough food that they can completely consume in about 2 minutes. This will help cut down on waste in your aquarium which in turn helps keep the water cleaner. It is easy to over feed Cardinal Tetra, so care should be taken when establishing meal times.\nSize & Lifespan\nAt full maturity, you can expect your Cardinal Tetras to reach about 1.25 inches in length. In the wild, Cardinal Tetras can reach slightly larger and some will max out their adult length at around 2 inches. If properly cared for you can expect your Cardinal Tetras to live anywhere from 2 to 5 years. Their lifespan is greatly determined by how well they are kept.\nCardinal Tetra Tank Size, Temperature, and pH\nCardinal Tetras require a minimum of a 20 gallon tank. It has been said that Cardinal Tetras will do fine in a smaller tank of around 10 gallons, but Cardinal Tetras are schooling fish that prefer to live in large groups. In the wild, Cardinal Tetra schools can reach in the thousands. The preferred water temperature of the Cardinal Tetra is 71F to 83F, and you will need to invest in a quality heater. It is important to also monitor the kH of a Cardinal Tetra tank. It needs to remain at a level of 2 – 6 kH. This will help neutralize acids, and prevent the pH of the aquarium from changing too rapidly which can cause issues for your fish. The pH of a Cardinal Tetra should be around 5.0 to 7.5.\nCardinal Tetra Tank Setup\nWhen setting up a tank for Cardinal Tetras, you will want to first take a look at their natural habitat. Cardinal Tetras will do best when being kept in an aquarium that closely mimics the environment that they can be found in the wild. Sandy or muddy substrates make for the perfect aquarium floor to also grow many live plants.\nCardinal Tetras appreciate a heavily planted aquarium. A few great plants for a Cardinal Tetra tank include Amazon Sword, Java Fern, and Anubias Plants as these plants prefer the same water parameters as the Cardinal Tetra. Floating plants also make for a great addition to the ones planted. Both types of plants help keep the light levels to their lowered preferred levels. Rocks, driftwood, and leaves are all great, natural additions that can be added to the Cardinal Tetra tank. They provide natural hiding places for stressed out Cardinal Tetras.\nWhen choosing decor, don’t forget to leave them as much open swimming space as possible. Cardinal Tetras are an active species that enjoy schooling together in large groups, and with enough room you will be able to see their unique behaviors happily displayed by the fish.\nCardinal Tetra Breeding\nUnfortunately, it is not easy to tell male and female Cardinal Tetras apart. It is said that you can more easily tell the females from the males by their more rounded bellies. This is most likely due to the female being full of eggs as she nears spawning time. If you are looking to breed Cardinal Tetras, it is better to purchase a group of juveniles and let them mature together and pair off for spawning, or purchase an already mated pair. It is also said that the anal fin of Cardinal Tetra males is more pointed than the females, but this becomes more apparent as they mature.\nHow Do You Breed Cardinal Tetra?\nIf you are looking to breed Cardinal Tetra, you will have the most success if you set up a dedicated breeding tank. It is not an easy task to get Cardinal Tetras to breed in captivity, and therefore, most of the Cardinal Tetras that you see for sale are wild caught. With a dedicated breeding tank, you will be able to replicate the correct water parameters that help trigger spawning. They prefer softer waters to spawn. If you have not noticed any spawning behaviors after placing your mature Cardinal Tetras in their breeding tank, then you will want to adjust the water parameters. This is the most difficult part of breeding Cardinal Tetras.\nCardinal Tetras will spawn at night, and this will last long into the night, sometimes until just before morning. When Cardinal Tetra females spawn, they release around 130 to 500 eggs. The eggs are very small, but it is important to remove the parent fish once you see them as they provide no parental care, and they could possibly cannibalize their young.\nCardinal Tetra Disease\nCardinal Tetras are susceptible to many of the same ailments as other freshwater fish. They can also get Neon Tetra Disease. Neon Tetra Disease is a degenerative condition that is caused by parasites. These parasites are fast spreading and oftentimes fatal to the fish. If you suspect that your fish is affected by Neon Tetra Disease, then you will want to remove them from your tank immediately and quarantine them as it is highly contagious to other species of fish as well.\nSince most Cardinal Tetras being sold in the aquarium trade are wild caught, it is important to quarantine your fish before you introduce them into your home aquarium. This will prevent you from spreading disease to your other aquarium fish.\nCardinal Tetra Tank Mates\nCardinal Tetras are a schooling species, and they prefer to live in large groups. In the wild, Cardinal Tetra schools can number in the thousands. In captivity, Cardinal Tetras are happy when kept in a small grouping of at least 6 fish in a 20 gallon tank, but if you have room for it, they would happily accept more. A larger aquarium will allow you to keep more Cardinal Tetras together in a much larger school. Keeping a single Cardinal Tetra on its own will stress out the fish, and it will be prone to sickness.\nAre Cardinal Tetra A Schooling Fish?\nCardinal Tetra are small schooling fish that are often referred to as a microschooler. They prefer to live in as large a grouping as possible. These large groups give Cardinal Tetras a sense of security, and they are more likely to display their interesting social behaviors the more fish are in their school. It is recommended that Cardinal Tetras be kept in a group of at least 6 to keep them happy and comfortable.\nAre Cardinal Tetras Aggressive?\nCardinal Tetras are considered a peaceful species that is usually good for community tank setups with similar behaved species. They can, however, become aggressive if they are kept in too small of a space. The more tetras you keep in a small space, the more aggressive their behaviors will become during spawning time, or feeding time. It is important to provide Cardinal Tetras with lots of room to school, and even hide to destress.\nCompatible and Incompatible Tank Mates for Cardinal Tetras\nDue to the peaceful nature of the Cardinal Tetra, they are able to be housed with a wide variety of species of fish in a community tank setup as long as there is enough room for each species. Cardinal Tetras make great tank mates for gouramis, other species of Tetra, Betta fish, guppies, rasboras, and mollies.\nSince Cardinal Tetras are a smaller fish, they do not do well with larger, more aggressive and predatory species. Do not put Cardinal Tetras in a tank with any fish that will potentially eat fish of their size. As a general rule, if the tank mate has a mouth big enough to swallow the Cardinal Tetra, then they are not the ideal tank mate.\nWhen introducing new fish into a community setup, it is important to observe the behaviors of the fish for a while. This will give you a good idea if the fish are able to be housed together or not. It is always possible that even though all their needs are met, that some fish will just not get along with one another, and there could be problems.\nCardinal Tetra and Betta\nCardinal Tetras make for excellent tank mates for Bettas. Both species enjoy the same water parameters, tank requirements, and even share similar dietary needs. You will run into issues if you are not keeping these species in the appropriate sized tank. Betta fish are territorial, and will potentially chase Cardinal Tetras away, but they are slow moving, and should not be able to harm them. Even though you will not see your Bettas and Cardinal Tetras fighting with one another, these two species could still cause one another stress. Stress in fish is a small issue at first that evolves into a much larger issue quickly. Stress can cause illness and disease to your fish.\nCardinal Tetra and Angelfish\nIt is possible to keep Cardinal Tetras with Angelfish if they are fully grown. Adult Cardinal Tetras could still become a meal for a hungry Angelfish as it is a smaller fish and the Angelfish is predatory. You will have success in keeping the two species together as long as there is adequate room, and both of the species dietary needs are being met.\nCardinal Tetra and Cherry Shrimp\nThe properly planted Cardinal Tetra tank can also house Cherry Shrimp without issue. If Cherry Shrimp feel threatened they will most likely retreat into hiding in the plants around them. In general, Cardinal Tetra will avoid Cherry Shrimp, but could potentially go after Cherry Shrimp fry as a meal.\nCardinal Tetra and Guppies\nCardinal Tetras and Guppies make for great tank mates, but they do enjoy slightly different water parameters. They get along fine, and will typically not bother one another. Make sure that your tank is well cycled before you add in your Cardinal Tetras.\nWhere Can I Find Cardinal Tetra For Sale?\nIf you are looking to purchase Cardinal Tetras for your home aquarium, you will easily be able to find them for sale in local pet stores and online. Make sure you are purchasing them from a reputable breeder to ensure that you are getting a quality fish that is also disease free. It is a good idea to quarantine new fish away from others for a few weeks to observe if they are healthy before adding them into your established aquarium.\nCardinal Tetra vs Neon Tetra\nWhether Cardinal Tetras are better than Neon Tetras is up for debate. Both fish are hardy, and considered easy to care for with minimal requirements. Both species get along well with others, and both can be placed in a community aquarium with other species of peaceful fish with no issues. Which fish is better depends on what you want for your own personal aquarium. Neon Tetras are slightly cheaper than Cardinal Tetras, but Cardinal Tetras tend to grow just a little bit larger than Neon Tetras do. Either fish you choose, you will be able to enjoy both equally. More so if you pick for yourself. If you can not decide, why not pick both? These species can be seen schooling together in the right conditions. Cardinal Tetras are not easy to breed in captivity, but it has been done. Most of the Cardinal Tetras that you see for sale in pet stores are wild caught, whereas Neon Tetras are easy to breed in captivity.\nAre Cardinal Tetra Hardier than Neon Tetras?\nCardinal Tetras and Neon Tetras are both considered hardy fish, and both are susceptible to the same ailments and disease. Due to their ease in spawning while captive, Neon Tetras have conditioned themselves to accept a much wider range of temperature than Cardinal Tetras. This means that they are more forgiving if something goes awry in your aquarium. It is much more important to closely monitor the water parameters for Cardinal Tetras than it is for Neon Tetras, but it is still a good idea to establish a routine in water maintenance and care.\nAre Cardinal Tetra Easier to Keep than Neon Tetras?\nNeon Tetras are typically considered easier to care for than Cardinal Tetras, and they also will spawn more readily in the home aquarium if you are looking to breed them. This is due to Neon Tetras being bred in captivity for a long time. They have been able to adapt to a wider range of water parameters.\nDifference in Care Requirement Between Cardinal Tetra and Neon Tetra\nNeon Tetras have been bred commercially for so long that their species has conditioned itself to accept a wider range of water parameters than the Cardinal Tetra. This means that they could potentially be hardier than the Cardinal Tetra if something goes wrong with their aquarium.\nCan Cardinal Tetra and Neon Tetra Live Together?\nCardinal Tetras and Neon Tetras both enjoy the same water parameters, and diets. Not only can they live together peacefully, but they will even school together. The key to the happiness of both species is to make sure that they are being cared for properly, and all of their needs are being met."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:93b33e98-d9d7-4fbd-8e56-ccd405938a9d>","<urn:uuid:67070938-4994-4fc8-89bb-4209ee5c1a15>"],"error":null}
{"question":"What are the main challenges researchers face in face recognition technology?","answer":"The main challenges in face recognition technology are developing recognition strategies that are robust to changes due to pose, illumination, disguise, and aging.","context":["The recognition of humans and their activities from video sequences is currently a very active area of research because of its applications in video surveillance, design of realistic entertainment systems, multimedia communications, and medical diagnosis. In this lecture, we discuss the use of face and gait signatures for human identification and recognition of human activities from video sequences. We survey existing work and describe some of the more well-known methods in these areas. We also describe our own research and outline future possibilities.\nIn the area of face recognition, we start with the traditional methods for image-based analysis and then describe some of the more recent developments related to the use of video sequences, 3D models, and techniques for representing variations of illumination. We note that the main challenge facing researchers in this area is the development of recognition strategies that are robust to changes due to pose, illumination, disguise, and aging. Gait recognition is a more recent area of research in video understanding, although it has been studied for a long time in psychophysics and kinesiology. The goal for video scientists working in this area is to automatically extract the parameters for representation of human gait. We describe some of the techniques that have been developed for this purpose, most of which are appearance based. We also highlight the challenges involved in dealing with changes in viewpoint and propose methods based on image synthesis, visual hull, and 3D models.\nIn the domain of human activity recognition, we present an extensive survey of various methods that have been developed in different disciplines like artificial intelligence, image processing, pattern recognition, and computer vision. We then outline our method for modeling complex activities using 2D and 3D deformable shape theory. The wide application of automatic human identification and activity recognition methods will require the fusion of different modalities like face and gait, dealing with the problems of pose and illumination variations, and accurate computation of 3D models. The last chapter of this lecture deals with these areas of future research.\nTable of Contents\nHuman Recognition Using Face\nHuman Recognition Using Gait\nHuman Activity Recognition\nFuture Research Directions\nAbout the Author(s)Rama Chellappa\n, University of Maryland\nRama Chellappa received the B.E. (Hons.) degree from the University of Madras, India, in 1975 and the M.E. (Distinction) degree from the Indian Institute of Science, Bangalore, in 1977. He received the M.S.E.E. and Ph.D. degrees in electrical engineering from Purdue University, West Lafayette, IN, in 1978 and 1981, respectively. Since 1991 he has been a Professor of Electrical Engineering and an Affiliate Professor of Computer Science at the University of Maryland, College Park. He is also affiliated with the Center for Automation Research (Director) and the Institute for Advanced Computer Studies (permanent member). Prior to joining the University of Maryland, he was an Assistant (1981-1986) and Associate Professor (1986-1991) and Director of the Signal and Image Processing Institute (1988-1990) with the University of Southern California, Los Angeles. Over the last 23 years he has published numerous book chapters, peer reviewed journal and conference papers. He has edited a collection of papers on Digital Image Processing (published by IEEE Computer Society Press), coauthored (with Y.T. Zhou) a research monograph on Artificial Neural Networks for Computer Vision, published by Springer-Verlag, and coedited (with A.K. Jain) a book on Markov Random fields, published by Academic Press. His current research interests are face and gait analysis, 3D modeling from video, automatic target recognition from stationary and moving platforms, surveillance and monitoring, hyper spectral processing, image understanding, and commercial applications of image processing and understanding Neural Networks. He was a coeditor-in-chief of Graphical Models and Image Processing. He also served as the editor-in-chief of IEEE Transactions on Pattern Analysis and Machine Intelligence during 2001-2004. He has also served as a member of the IEEE Signal Processing Society Board of Governors during 1996-1999 and as its Vice President of Awards and Membership during 2002-2004. He has received several awards, including NSF Presidential Young Investigator Award, an IBM Faculty Development Award, the 1990 Excellence in Teaching Award from the School of Engineering at USC, the 1992 Best Industry Related Paper Award from the International Association of Pattern Recognition (with Q. Zheng), and the 2000 Technical Achievement Award from the IEEE Signal Processing Society. He was elected as a Distinguished Faculty Research Fellow (1996-1998) and as a Distinguished Scholar-Teacher (2003) at the University of Maryland. He is a fellow of the International Association for Pattern Recognition. He has served as a General the Technical Program Chair for Server IEEE international and national conferences and workshops.Amit K. Roy-Chowdhury\n, University of California, Riverside\nAmit K. Roy-Chowdhury received the B.E. degree in electrical engineering from Jadavpur University, India, in 1995; the M.E. degree in systems science and automation from the Indian Institute of Science, Bangalore, in 1997; and Ph.D. from the University of Maryland, College Park, in 2002. His Ph.D. thesis was on statistical error characterization of 3D modeling from monocular video sequences. His current research interests are in motion and illumination modeling in video sequences, computational models for human activity recognition, and video sensor networks. Since 2004 he has been an Assistant Professor in the Department of Electrical Engineering, University of California, Riverside. In 2003 he was with the Center for Automation Research, University of Maryland, College Park, as a Research Associate, where he worked in projects related to face, gait, and activity recognition. He is the author of a number of papers, book chapters, and magazine articles on motion analysis, 3D modeling, and object recognition. He is a reviewer for journals in signal and image processing and computer vision and has served on the Program Committees of major international conferences in these areas. He was a receipient of University of California Regents' Faculty Fellowship Award in 2004-2005.S. Kevin Zhou\n, Siemens Corporate Research, Inc.\nS. Kevin Zhou is a Research Scientist at Siemens Corporate Research, Princeton, New Jersey. He received his B.E. degree in electronic engineering from the University of Science and Technology of China, Hefei, China, in 1994; M.E. degree in computer engineering from the National University of Singapore in 2000; and Ph.D. in electrical engineering from the University of Maryland at College Park in 2004. He has broad research interests in signal/image/video processing, computer vision, pattern recognition, machine learning, and statistical inference and computing. He has published more than 30 technical papers and book chapters on echocardiography image processing, visual recognition (in particular face recognition under unconstrained conditions, such as video sequences, illumination, and pose variations, etc.), tracking and motion analysis, segmentation and shape/appearance modeling, learning under uncertainty, and optimization and efficient computation."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:16f0105b-5106-49de-a025-f1266738a63c>"],"error":null}
{"question":"How do the success factors of franchisors compare to the benefits of homeownership in terms of their societal impact? While franchising creates jobs and economic growth, does homeownership generate more lasting community benefits?","answer":"Both franchising and homeownership have significant but different societal impacts. Franchising has contributed to substantial job creation, growing employment by 56% from 5.6 million to 8 million jobs between 1983-1993 in the U.S. alone. However, homeownership appears to generate broader societal benefits. Research shows homeownership leads to safer communities with reduced crime rates, better educational outcomes with 13-23% improvement in children's cognitive abilities, stronger families (95% of surveyed homeowners reported stronger families), and higher civic participation. Additionally, homeownership serves as a wealth builder, representing about one-third of the $6.6-trillion increase in household net worth since 1990 in Canada. While franchising systems depend on business relationships and market competition, homeownership's benefits extend to community stability, social capital development, and intergenerational economic mobility.","context":["The Best Predictors of a Franchisor’s Success\nThe franchising business model has grown in popularity over the years. From 1983 to 1993 the number of franchisors, i.e. companies that sell licenses or rights to third parties that enables them to operate under a trademark or a brand name, grew in numbers from 1,887 to 2,900 in the U.S. alone. This has created an increased employment rate of 56% (from 5.6 million to 8 million). In other words, a franchising business model has proven to be an efficient and successful concept even today.\nHowever, a franchisor cannot guarantee the success of their franchise any more than a personal fitness trainer can guarantee positive results for their clients. As a matter of fact, a franchisor can provide proper help and support but ultimately it’s up to the franchise owners to ensure their business success. In addition, the success rate of the franchises depends mostly on the franchise system a franchisor has implemented. That being said, here are a few of the best predictors of a franchisor’s success.\nAs you may already know, a franchise is an already developed business model supported by a trademark or brand. Franchisor companies leverage this method, in order to expand their business to other locations and other markets to distribute their goods or services through licensing agreements with entrepreneurs who will work under their brand’s name. However, selecting candidates for a franchise business is a difficult task that cannot be accomplished without a well-developed plan.\nThe first step is to create a disclosure document known as the Uniform Franchise Offering Circular (UFOC) or simply the Franchise Disclosure Document (FDD). This document helps outline a franchising business plan, as well as select the right candidates for the franchise owners. For instance, here are a few elements that should be included in this document:\n- Detailed information about the franchisor’s business, identity and background of their principals.\n- The fees franchisors are obligated to pay.\n- The terms and conditions of running a franchise\n- Franchisor support, such as training, operating plans, marketing strategies etc.\nThe franchisor’s system survival success largely depends on their expansion strategy and their willingness to share knowledge with their newly-formed franchise businesses. However, being able to compete against other businesses is also a major factor for franchisor’s success in the market. Therefore, a few of the major predictors of success are a franchisor’s ability to properly identify disruptive or competitive threats and develop a strategy that will protect the brand against them. Also, a franchisor’s ability to create a unique business model, which competitors won’t be able to copy.\nIn addition, a franchisor’s ability to ensure that their products, services, unique proposition, delivery methods and other factors will have value even in the future. Moreover, the ability of a franchisor to ensure that their franchises have the resources required to compete against their competitors on the market. That being said, proper competitive analysis and strategy are crucial predictors of franchisor’s success.\nA franchisor has already established their brand name and presence. The reputation of that brand has a major influence on the market, on the customers and most importantly on other franchises. If a franchisor’s brand reputation isn’t favorable, their franchises will suffer the consequences of that reputation, as well as vice versa. Therefore, it’s crucial for success that franchisor’s work on improving their brand reputation by improving their brand identity and also improving their brand image.\nThe way customers perceive a franchisor’s brand and the emotional connection they establish with that brand is a key factor in success. That’s why many franchisors implement strategies that will help their franchises have a positive impact on the local economy, as well as a local community. Regardless o the fact that each franchise shares the reputation with the franchisor, it’s still important for individual franchises to have a positive reputation of their own.\nOne of the most important predictors of success is the franchisor/franchisee relationship. This predictor mostly depends on a franchisor’s leadership and their ability to inspire franchises to follow. Even though franchises are legally bound by a contract to follow specific rules and strategies outlined in the franchisor’s business plan, it doesn’t mean that they’ll do it right or that it will guarantee their success. That’s why it’s up to the franchisor to take the initiative and encourage their franchises to stick to the proven strategies. That includes a few key elements.\n- A franchisor’s ability to consult franchises on a potential opportunity.\n- Leveraging leadership skills to convince franchises that a potential opportunity is in fact worthwhile.\n- Creating a leadership and management teams that will capitalize on new opportunities, as well as scale properly.\n- Helping franchises establish a good presence in their local markets by providing counsel and resources.\nIf franchises cannot rely on or see the value in franchisor’s decisions, then the entire system is in jeopardy and at high risk of failure.\nJust like any other business model, franchising has no secret recipe that will guarantee its success. However, franchisors carry a lot of responsibility for their franchises and they must guide them properly, in order to ensure that the system will not only function but also succeed in the long-run.","Housing affordability questions have placed homeownership and public policy near the top of the national agenda, as mortgage brokers know. Most of the commentary has been focused on the extent to which government policy, particularly with regards to supply, is contributing to the affordability challenges. This debate is ongoing and will not be resolved here.\nBut there is less basic commentary about why we should care about homeownership. Why should government policy support homeownership? Simply put: it remains a powerful conveyor belt to the middle class.\nHomeownership is associated with a raft of economic and social benefits including better educational and health outcomes, stronger families, safer communities, higher levels of civic participation and greater wealth accumulation. A few policy areas are more likely to generate upward mobility and economic opportunity than housing and homeownership.\nHere are some highlights from a considerable body of research:\n- Kulkarni and Malmendier (2015) analyze the link between homeownership and upward mobility, and find a strong positive relationship for the children of homeowners that the two economists attribute to the stability and social capital that is associated with owning one’s home.\n- A post-recession update to past research on the broad economic and social benefits of homeownership by Rohe and Lindblad (2013) concludes that “there is considerable evidence that positive homeownership experiences result in greater participation in social and political activities, improved psychological health, positive assessments of neighborhood, and high school and post-secondary school completion.”\n- Ni and Decker (2009) study the relationship between homeownership and crime and find not only that “homeownership itself has a strong and statistically significant negative effect on both violent and property crime rates,” but that increases in homeownership rates reduce criminal activity over time.\n- Haurin et al. (2002) study the link between homeownership and educational performance for children and find that it leads to a 13 percent to 23 percent improvement in a higher-quality home environment, greater cognitive ability and fewer child behaviour problems relative to renting.\n- Harkness and Newman (2003) examine whether children from lower-income and higher-income families benefit equally from homeownership and find that for children growing up in families with incomes less than 150 percent of the federal poverty line, homeownership raises educational attainment, earnings and welfare independence in young adulthood.\nThese studies show the direct and spillover benefits that can come from a pro-homeownership society. Limited research has tested these findings in the Canadian context. Yet the work that has been done finds similar experiences and results.\nA 2013 CMHC survey of nearly 1000 Canadians who purchased a home through Habitat for Humanity casts light on the significant benefits that come with homeownership. Respondents showed positive results across a range of economic and social indicators, including labour force attachment, the educational performance and behaviour of their children, improved personal finances, better health, and general happiness. Most respondents identified that these benefits derived “from the security, stability and sense of control that comes with homeownership” (2).\nA 2012 study commissioned by Habitat for Humanity Toronto found similar results in its assessment of the “social impact” of homeownership. The findings are powerful: 95 percent of respondents said that their families were stronger, 81 percent reported an improvement in their child’s social life, 76 percent reported improvement in their children’s grades, 72 percent reported strong community and neighbourhood ties, and 50 percent reported that they felt safer.\nAs for wealth accumulation, housing has been a major driver of overall household net worth in Canada. A 2015 report by TD Economics finds that it represents about one-third of the roughly $6.6-trillion increase since 1990. The importance of housing wealth has even increased as an overall share of household net worth and accounted for 40 percent of the total increase in net worth since 2001 (TD Economics 2015).\nWhile a number of factors contribute to upward mobility and middle-class opportunity including education, family and culture, homeownership plays a strong role in Canada and elsewhere. This is a critical point: the evidence shows that the benefits are not just limited to homeowners. Society benefits when families have access to affordable, responsible homeownership and thus government policy should continue to support it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:9b052cf6-82d2-40ab-9e03-1065f63023f1>","<urn:uuid:8fc91c73-b26a-42c6-8d09-c4848a948c5c>"],"error":null}
{"question":"Real-time inventory tracking benefits & data accuracy specs?","answer":"Real-time inventory tracking provides immediate benefits through RFID and barcode systems that enable instant location of medical equipment, automated shortage alerts, and prevention of overstock situations. These systems help facilities reduce wasteful spending and save staff time by eliminating manual searches for supplies. Regarding data accuracy, barcode systems achieve 99% accuracy compared to typical manual entry, which is crucial for healthcare settings where errors can be catastrophic. The technology relies on specific barcode symbologies like Code 128C for linear barcodes, and Aztec or DataMatrix for 2D applications, with Aztec capable of encoding up to 3750 characters and DataMatrix handling up to 2,335 alphanumeric characters.","context":["Proper asset management is vital to the health of any supply chain organization, but it is especially critical to the financial health of non-acute healthcare facilities and the actual health of their patients.\nAccording to Healthcare Advisory Board insights, approximately 95 percent of patient visits now take place in the non-acute facilities, yet 80 to 90 percent of healthcare spend occurs in hospitals per data from the CDC's National Center for Health Statistics. This disproportionate figure explains why ambulatory surgery centers (ASC), urgent care centers, rehabilitation clinics and long-term care facilities remain laser-focused on their spend as hospitals are starting to prioritize revenue generation over cost reductions. And, with supply costs higher than staffing costs for many ambulatory surgical centers, it’s no wonder that asset management performance is on the minds of non-acute facility administrators.\nBut even healthcare providers who choose to prioritize patient outcomes over operational costs would benefit from better inventory management capabilities, as inventory availability can affect patient outcomes.\nThe quality of patient care depends on your ability to get the right medical equipment and supplies to the right caregiver at the right time – whether that’s a prescription drug, an implant device or a new wound dressing. Struggling to locate certain equipment required for post-op rehab or finding out that you don’t have the supplies you need for a same-day surgery are not just inconveniences; they are costly consequences of poor inventory management, and they come at the patient’s expense.\nThat is why I often tell ASCs and other non-acute providers that “you have to spend more to save more.” Specifically, you have to spend more on technologies that provide real-time, around-the-clock visibility into the location and volume of your on-hand inventory in order to save more money on supplies and save more lives.\nIf it seems ironic that I’m advocating for ASCs, post-acute and long-term care facilities to increase investments on technology when I know how challenged they are by small budgets and tight margins, know that the costs of not investing in technology – of maintaining the status quo when it comes to asset management – will be far higher long-term than this single expense today.\nThink about it…\nPhysicians, nurses and other members of your care team have a job to do, and it most likely does not entail inbound logistics or inventory counts. Given that many non-acute facilities are part of a larger health system, supply chain management and sourcing actions may not even take place on site. That means the people charged with purchasing the devices, equipment and supplies your healthcare providers and patients need are either relying on your team to tell them what to order (and when) or they are automatically re-ordering based on average utilization trends. While the first scenario is most ideal – it’s the best way to ensure you have the exact supplies you want and need at all times – it’s not necessarily realistic. (Remember, monitoring inventory is not the primary job description of the caregivers who rely on that inventory to do their jobs.)\nPlus, the time that caregivers have to spend chasing down misplaced inventory or trying to figure out how to “make do” with available inventory can compromise the quality of patient care.\nTherefore, automating your inventory management – or at least improving visibility into the levels and locations of your on-hand inventory and automating low quantity notifications across your network – can resolve two chronic pain points harming the health of your organization and your patients: wasteful spending and wasted time.\nImplementing either a barcode or RFID-based inventory management system that leverages a combination of mobile computing, scanning, labeling and printing technologies will immediately empower you to:\n1. Track your utilization by tracing the movement of every asset – The benefits of “track and trace” technology start at the beginning of your supply chain, but really multiply once those medical supplies, devices and drugs are delivered to the front (or back) door of your ASC or other non-acute care facility.\nFor example, RFID tagged-items such as wheelchairs, beds or IV stands can be located immediately via any internet-connected device. And barcode-labeled supply boxes or prescription bottles that are scanned and “inventoried” during put away can easily be found on a shelf by staff using a locationing app on the same clinical smartphone or rugged tablet that is being used to facilitate care team communications, manage patient records and administer other care functions.\nHowever, expediting inventory retrieval is just one efficiency you’ll gain from implementing even the simplest “track and trace” system. You’ll also be able to track usage of every item within your four walls and know, via automated notifications, when remaining inventory levels drop to a certain threshold. These shortage alerts will avoid the out-of-stock scenarios that require caregivers to revert to alternative device, drug or supply options that could prove less effective in the course of treatment. They also eliminate the sporadic re-orders that burden your staff and make it more challenging to manage your supply chain budget.\n2. “Scan” for savings – Out-of-stocks aren’t the only thing you should pay attention to. Overstocks also strain your bottom line. If supply shipments aren’t documented properly when received and ultimately misplaced, staff might think there was a delivery issue or that the items are out of stock and request a duplicate order. Depending on the item’s typical usage, excess inventory could expire before needed, which leads to even more waste. By opting to document acceptance and storage of inventory shipments via scanned data inputs versus manual inputs, you reduce the risk of level inaccuracies.\nThe simple action of scanning an RFID tag or barcode label when a drug or device is received and again when it is used can also help you determine if you have the right variety and volume of inventory for your non-acute operations. You may start to notice usage trends start to shift for certain items, with some declining and others becoming more in demand. With this level of inventory intelligence, you’ll be able to adjust your sourcing selections so that you’re only buying what providers actually use or patients actually need. In turn, you’ll save on unnecessary expenses and extract more value from each dollar spent. At the same time, if you see an uptick in usage for certain items, you might be able to negotiate better volume discounts with your suppliers, which saves you money.\nPlus, the increased inventory accuracy levels achieved by scanning assets at various touchpoints means you can adopt a just-in-time ordering model if you’d like. By knowing the actual inventory levels at any given time, you can wait to re-stock perishable products with short shelf-lives until they’re absolutely needed. That reduces the number of expired products that are ultimately wasted, again saving you money.\n3. Comply with industry and regulatory “controls” – Several pieces of legislation ranging from the Falsified Medicines Directive to Unique Device Identifier (UDI) mandates that hope to control the quality of medical devices and drugs have been introduced to protect patients and healthcare providers alike. The good news is that the same clinical smartphones, scanners, barcode labels and RFID tags that you’re using to better control your supply-related expenses and patient care quality are the same technologies required to comply with these global supply chain “control” requirements. How’s that for a return on investment (ROI)?\nOf course, these aren’t the only ways an investment in mobility-based track-and-trace systems will pay off. You can learn about the others in our Non-Acute Inventory Management brief or on our website.\nJust know that the money you spend to gain real-time visibility of the assets dispersed across your facility will return significant gains in caregiver efficiency and care quality at the bedside and beyond.\nImproved asset management is also a priority for acute healthcare providers such as Sanatorio Finochietto in Argentina. As you will see in this video, using RFID technology to optimize inventory management processes and laundry operations is saving this major healthcare system up to $300,000 annually.","Improved data accuracy is the single most common motivation for implementing a bar code system. Often the backbone of operations, data entry enables an organization to produce accurate reports and predictions about future needs and actions. With data entry playing such a critical role in an organization’s operations, it is important to identify the extent to which data entry errors are tolerated.\nOrganizations with integrated bar-coding systems that enable users to scan bar codes rather than manually input data are commonly achieving 99% data accuracy. For organizations in which data errors are a mere nuisance, the difference between 85% and 99% may not seem that important. But for organizations in which data entry errors are catastrophic, such as hospitals and pharmacies, the goal must be 100% accuracy. Bar coding is the best tool that these organizations have to ensure patient safety and thereby greatly reduce the impact of human error.\nChecking the “Five Rights” – right patient, right medication, right dose, right time, and right method of administration – prevents most medication errors. All too often, the five rights check is flawed because it fails to guarantee the right patient. Bar coding patient information is widely acknowledged as one of the strongest and most cost-effective methods for improving patient safety. Bar coded patient wristbands provide the necessary foundation for preventing errors by ensuring positive patient identification is always available at point of care. Once wristbands are bar coded to provide basic patient identification, numerous other identification, tracking, and data collection applications can be added to take advantage of bar code data entry.\nBAR CODE SYMBOLOGY\nA bar code is a graphic representation of data that is machine-readable. Bar codes are a fast, easy, and accurate way of entering data. Symbology is considered a language in bar code technology. Bar code symbology enables a scanner and a barcode to communicate with each other. When a bar code is scanned, it is the symbology that enables the data to be read correctly. When a bar code is printed, it is the symbology that enables the printer to translate the data that needs to be printed on the label or wristband.\nCode 128C is a linear bar code that can encode the full 128-character ASCII character set. It is popular for use on wristband bar codes because it is one of the densest linear symbologies, meaning it can encode information in less space than symbologies with lower densities. Code 128 is a variable-length symbology so symbols can be as long as necessary to encode the required data. Code 128 is also the standard for identifying blood products. Code 128 is generally the best all-around choice you can use.\nAZTEC BAR CODE\nAztec Code was designed for ease of printing and ease of decoding. Aztec Code is a high density 2 dimensional matrix-style bar code symbology that can encode up to 3750 characters from the entire 256 byte ASCII character set. The symbol is built on a square grid with a bulls-eye pattern at its center. Data is encoded in a series of layers that circle around the bulls-eye pattern. Each additional layer completely surrounds the previous layer thus causing the symbol to grow in size as more data is encoded, yet the symbol remains square. Aztec’s primary features include: a wide range of sizes allowing both small and large messages to be encoded, orientation independent scanning, and a user selectable error correction mechanism.\nDATA MATRIX BAR CODE\nA Data Matrix code is a two-dimensional matrix barcode consisting of black and white cells, or modules, arranged in either a square or rectangular pattern. The information to be encoded can be text or raw data. Usual data size is from a few bytes up to 2 kilobytes. The length of the encoded data depends on the symbol dimension used. Error correction codes are added to increase symbol strength; even if they are partially damaged, they can still be read. A Data Matrix symbol can store up to 2,335 alphanumeric characters.\n- Use thermal printers\n- Use Code 128C and Aztec or DataMatrix\n- Print bar code perpendicular with band (use wider bands if needed)\n- Use highest-quality band material\n- Print at 2 IPS and correct darkness"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:28c525ed-1fcb-44fe-8dc2-5a7dc7b4fe7c>","<urn:uuid:daf0f8ac-150f-40db-847f-eab9c89b8152>"],"error":null}
{"question":"I need help understand breathing exercises for recover. What is difference between breathing patterns for childbirth labor versus breathing for athletic recovery? Can you explain similarities and differences with examples?","answer":"Both contexts involve specific breathing techniques but serve different purposes. For childbirth, breathing patterns are more varied and structured, including slow paced breathing (60-second contractions), modified paced breathing (60-90 seconds), and patterned paced breathing (90 seconds) with specific rest intervals between contractions. These patterns help manage labor pain and maintain control during contractions. For athletic recovery, the focus is on slow breathing with longer exhales compared to inhales (specifically a ratio of 2:1), performed at about six breaths per minute. The childbirth patterns are designed for active management of intense pain and physical stress, while athletic recovery breathing aims to activate the parasympathetic nervous system to promote rest and repair after exercise.","context":["You should practice your conditioning exercises, relaxation exercises, and breathing patterns every day. If possible, get in two practice sessions per day—one alone and one with your labor partner. You can do the relaxation exercises and breathing patterns the same time you do the conditioning exercises, or you can do them in separate sessions. Ty to practice when you are rested and can fully concentrate on your efforts.\nRehearse in your mind the labor and birth of your baby. Start with early labor and continue through transition, pushing, and delivery. Learn your role so completely that you will automatically begin a breathing pattern and relax your body when a contraction begins.\nFollowing is a suggested sequence for practicing your exercises and patterns. Add to it as you learn new techniques.\n1. Do each conditioning exercise the recommended number of times.\n2. Practice a relaxation exercise. Vary the choice of exercise.\n3. Practice the slow paced breathing pattern, with the contractions lasting 60 seconds each. Do a series of three contractions, with 60-second rests in between the contractions. Practice a massage technique as a comfort measure at the same time.\n4. Practice the modified paced breathing pattern, with three contractions lasting 60 to 90 seconds each. Maintain a steady, even breathing rate throughout each contraction. Then practice the accelerated-decelerated breathing pattern while your partner applies manual pressure.\n5. Practice the patterned paced breathing pattern, with the contractions lasting 90 seconds each. Do a series of three contractions, with 30-second rests in between the contractions. Have your partner call out a premature urge to push lasting 15 seconds during one or more of the contractions. Blow out forcefully and continuously until your partner signals that the urge to push has passed. Then return to the patterned paced breathing pattern to finish the contraction.\n6. Practice both gentle pushing and breath holding pushing three times each. Have the contractions last 60 seconds each, with 1 to 2 minutes of rest in between the contractions. Practice with the different pushing positions, so that you will be comfortable in all of them.\nOnce you become familiar with all of the breathing patterns, practice your relaxation and breathing in various positions—standing, semireclining, sitting in a rocking chair, side-lying, or kneeling on your hands and knees. This will help ensure maximum comfort and relaxation in all positions during labor.\nTo be even more prepared, simulate various kinds of labor situations as you practice. This will help you to cope better with a problem, if one should arise. Some examples of labor variations are back labor, induced or very rapid labor, loss of control, and hyperventilation.\n- Learn about alternative methods of pain relief for labor and discuss them with your partner.\n- Learn the acupressure points for pain relief.\n- Recite a visualization for your partner.\n- Encourage a daily practice session during the last month of pregnancy.\nFor The Labor Partner: Helpful Hints\nSeveral additional ways in which you can help out are the following:\n• Simulate contractions during practice by applying manual pressure to the bony part of your partner’s wrist, elbow, or knee to cause discomfort. As you call out the contraction, gradually build to a peak, then gradually ease the pressure. Increase and decrease the pressure in varying patterns, so that you partner can respond to different sensations. For example, slowly increase the pressure to a peak, then rapidly decline; peak quickly and maintain strength through most of the contraction; or peak several times during the same contraction. This technique is especially helpful for practicing accelerated-decelerated breathing. Remember, this technique is only for practice.\n• Call out the passing seconds in 15-second intervals. Pacing the contraction helps your partner to keep her perspective.\n• Observe for tension in your partner both during and between contractions. The longer the labor continues, the greater is the chance of tension slowly spreading. Use both touch and verbal cues as needed to encourage relaxation.\n• Help your partner to maintain breathing rhythm by counting or breathing with her. Tapping on a ard surface or moving your hand in front of her face may also keep her on track.\n• Encourage your partner to take advantage of Braxton-Hicks contractions as signals to practice relaxation and breathing patterns.\nThe ways in which labor partners can help out during pregnancy, labor, and delivery are innumerable. This website has suggestions in every section. With a little common sense, imagination, and teamwork, pregnancy and childbirth can be one of the more fulfilling times in a relationship.","Recovery is sort of like the the ball boy for the New England Patriots: nobody really appreciates that he’s critical to the organization’s success, at least until there’s a scandal. Okay, the analogy isn’t perfect, but the point is that adequate recovery is critical to training programs’ success. The more efficiently an athlete can recover from a workout, the sooner they can train again, which eventually allows them to take on a greater training load. This greater training load, in turn, elicits greater adaptation and the development of higher levels of fitness that may eventually allow an athlete to surpass their competition.\nAs Peter and I have discussed in previous posts, the physiological processes that drive recovery and repair are predominately regulated by the Autonomic Nervous System (ANS). More specifically, the parasympathetic branch (PsNS) of the ANS, which is associated with “rest and digest” functions, is the division that helps promote recovery via a multitude of mechanisms following the withdrawal of a stressor. The PsNS works together with the sympathetic branch (SNS) of the ANS, which is primarily responsible for “fight or flight” functions, to maintain homeostasis in the body. Though the two are technically complementary in that both are required to some degree at all times for homeostatic regulation, the downstream physiological pathways that they activate are actually antagonistic: the PsNs primarily stores energy and repairs structures, while the sympathetic branch primarily mobilizes energy to deal with perceived threats. This is admittedly an oversimplified illustration of the highly complex nature of ANS function, but it is adequate for the purpose of this post; if you’re interested in learning more about these systems, then I recommend you read Dr. Robert Sapolsky’s book, “Why Zebras Don’t Get Ulcers: The Acclaimed Guide to Stress, Stress-Related Diseases, and Coping,” which is the de facto book about stress, stress responses and stress management, and their effects on health.\nBecause the PsNS is primarily responsible for rest and recovery functions, in order to expedite recovery it’s crucial to increase parasympathetic activity and to decrease excessive sympathetic activity following application and subsequent removal of a stressor. In the context of performance, an athlete needs to be able to immediately respond to an intense stressor such as a training session by increasing sympathetic nervous system output, but following withdrawal of the stressor (the end of the training session in this case) the athlete must also be able to “turn down” sympathetic output and allow the PsNS to take over and promote recovery from and adaptation to the demands of the training. Because the ANS is almost exclusively under subconscious control, this process occurs spontaneously and without conscious intent. Research and anecdotal evidence suggest, however, that there may be ways to improve the efficiency of these recovery processes. Here are a few of these simple, research-based tips intended to help improve your recovery, health, and well-being that you might not be familiar with:\n1) Exhale Like You Mean It\nConsciously focusing on slowing your breathing rate and increasing the length and depth of your exhale relative to your inhale can have profound effects on autonomic function and mental well-being. In a 2014 study, Van Diest et al. had participants perform four different breathing patterns to determine the effects of respiration rate and inhalation-to-exhalation ratio on several parameters of overall health and autonomic balance (e.g., heart rate and respiratory sinus arrhythmia) as well as perceived well-being (e.g., self-reported measures of relaxation and positive energy). Participants performed each of the breathing patterns for five minutes, during which physiological data was collected. Following the intervention, the subjects completed two surveys to assess their mental and emotional well-being. The researchers found that when participants breathed slower (six breaths per minute) and exhaled for about twice as long as they inhaled, they had improved respiratory sinus arrhythmia, reported feeling more relaxed and less stressed, and possessed more mindfulness and positive energy. An important limitation of this study is that we can’t determine how long the emotional and physiological changes may last. That being said, we may be able to glean some valuable insight from this research and amend these insights upon further analysis of the relevant research.\nOne takeaway is that spending a few minutes each day performing slow-paced breathing with long exhales may improve your mood, help your body and mind recover more efficiently, and better prepare you to deal with future challenges. It might be especially beneficial to perform this breathing after stressful events, such as a training session, competition, or dinner with the in-laws, since it’ll help to shift a person from a sympathetic-dominant state to a more parasympathetic-dominant state. Breathing in this manner as part of a pre-bed routine may also help people fall asleep, since it can induce a more relaxed state.\nOf course, this recommendation may seem similar to something else Peter and I have discussed ad nauseum: the principles and applications of the Postural Restoration Institute. I don’t want to belabor this point because we have discussed it quite a bit already, but suffice it to say that the breathing components of PRI’s exercises may induce neurophysiological and psychological states that help to improve recovery, health, and well-being.\n2) Breathe Through Your Left Nostril (No, Really…)\nThough I’m not an expert on the history of yoga, I do know that the method of alternate or unilateral nostril breathing is quite popular in the practice and has been used in various yogic traditions for oodles of years. Yogic traditions have ascribed different attributes to breathing through the right and left nostrils: breathing through the right nostril supposedly has an “excitatory effect,” while breathing through the left nostril is supposedly “inhibitory.” In recent decades, researchers have investigated these yogic practices and their effects on various measures of health and well-being with some surprisingly vindicating results. I had a minor urge to make a terrible joke about how these findings would blow your mind and your nose,\nbut luckily for everyone reading I refrained. Whoops.\nIn a 2014 study, Pal et al. assigned medical students to either a right nostril breathing, left nostril breathing, or a control group for a six week study to determine what effects, if any, these interventions might have on autonomic and cardiovascular function. For an hour per day every day during the six-week experiment, participants performed their assigned breathing protocol. In the experimental conditions, participants closed the designated nostril and breathed with a five second inhale and a five second exhale. Participants in the control condition didn’t receive any instruction. I’ll quote the study because this sentence amuses me: “However, they [the participants in the control condition] were allowed to sit in the same room and breathe normally for the same amount of time.” Awkwaarrddd.\nThe researchers collected measurements of cardiovascular function (e.g., resting heart rate, diastolic and systolic blood pressure) as well as autonomic function (e.g., heart rate variability data) before and after the six-week intervention, and the results were profound. The average resting heart rate of subjects in the control group was unchanged, while that of the subjects in the right nostril breathing group increased by ten beats per minute. The average heart rate in the left nostril breathing group, by contrast, actually decreased by about seven beats per minute. Similar trends were observed for systolic and diastolic blood pressure, as well as mean arterial pressure and rate-pressure product. In each case, the control group didn’t exhibit a statistically significant difference between pre- and post-intervention measurements, while the right nostril breathing group significantly worsened and the left nostril breathing group significantly improved. In addition, significant differences also existed between groups. You can view the full table of results here.\nThe results of the heart rate variability data were much the same. While the control group experienced no significant changes in low frequency bands of HRV (LF, roughly correlated to sympathetic function), high frequency bands of HRV (HF, roughly correlated to parasympathetic function), LF:HF ratio, or rMSSD (the primary measure of time-between-beats variability), the right nostril breathing group worsened and the left nostril breathing group improved. The left nostril breathing group also had significant improvements in total power of HRV, though the right nostril breathing group didn’t have a significant change. You can view the full table of results here. The researchers’ conclusion was that the data suggested that “there was sympathetic activation in subjects who practiced right nostril breathing and parasympathetic activation in those who practiced left nostril breathing.”\nThe practical implications of this study are tentative because the protocol used was so extreme. Practically speaking, it seems unlikely that athletes or the average person for that matter will devote an hour a day to paced left nostril breathing. It’d also be great if there were more studies of such high quality that compared different lengths of practices to determine if benefits could be achieved in less time. That being said, other studies have shown positive acute effects from only five minutes of left nostril breathing, so even relatively minor exercises might be worthwhile.\nAs with slow-paced breathing, it might be best to use left nostril breathing after especially stressful events to expedite recovery in addition to before going to sleep. You could also do it at random times, such as when you’re sitting in class or after a meal. I might know someone who does that… (Hint: it’s me.)\nWhat’s more, a person might want to make sure that their left nostril isn’t occluded, since they’d then have to breathe predominantly through their right nostril. Could it be that stuffed noses can hinder recovery from bouts of training and make people crotchety? Perhaps. Which leads to the next tip…\n3) Do Nasal Saline Rinses\nThough my suggestion to do nasal saline rinses may be more theoretical than the previous suggestions, it draws on similar themes (if anyone is aware of studies on the effects of nasal saline rinses, then don’t hesitate to share them with me!). For example, if a person’s left nostril is occluded for some reason then air will predominantly travel through the right nostril, which may have deleterious autonomic and cardiovascular effects. Using a nasal saline rinse could therefore quickly (though perhaps fleetingly) improve airflow and thus improve autonomic and cardiovascular function.\nIf a person’s nasal passages are significantly occluded, then it seems that there may be two consequences: 1) the person continues to breathe through their nose, but their inhalation will be longer and more labored, or 2) they inhale through their mouth. The occluded airways will decrease the rate at which air travels through the respiratory system, hence forcing you to inhale longer and more vigorously to meet the body’s demand for oxygen. Increasing the length and depth of inhalation without a subsequent increase in the exhale may have the detrimental effects described in the first part of this article. Of course, it’s entirely possible that the length of exhalation would increase as well, so it’s worth noting that the ratio of inhalation to exhalation may not actually change.\nEither way, the increased difficulty of inhaling may increase the recruitment of accessory respiratory muscles (e.g., scalenes, sternocleidomastoid, external intercostals, pectoralis minor, etc.) in order to increase the pressure gradient between the environment and the lungs. This muscular activity is reminiscent of the neck breathing I discussed in a previous article and might cause, among other things, neck tightness and worsened movement quality. Long story short, doing a nasal saline rinse to clear your nasal passages might allow you to breathe more diaphragmatically which, as I’ve discussed in the aforementioned article, is a good thing. Anecdotally speaking, physical therapist (and PRI jedi) Zac Cupples mentioned in a blog post that he experienced profound improvements beyond just the unblocking of the nasal passages when he used a nasal saline rinse, though the effects didn’t last long.\nThat’s all for part one of this series—I hope you enjoyed it! In part two, I’ll discuss more methods for improving recovery, health, and well-being. If you have any questions or comments, then don’t hesitate to share them in the comments section.\n- Diest, Ilse Van, Karen Verstappen, André E. Aubert, Devy Widjaja, Debora Vansteenwegen, and Elke Vlemincx. “Inhalation/Exhalation Ratio Modulates the Effect of Slow Breathing on Heart Rate Variability and Relaxation.” Applied Psychophysiology and Biofeedback Appl Psychophysiol Biofeedback 39.3-4 (2014): 171-80. Web.\n- Pal, Gopalkrushna, Ankit Agarwal, Shanmugavel Karthik, Pravati Pal, and Nivedita Nanda. “Slow Yogic Breathing through Right and Left Nostril Influences Sympathovagal Balance, Heart Rate Variability, and Cardiovascular Risks in Young Adults.” North Am J Med Sci North American Journal of Medical Sciences 6.3 (2014): 145. Web.\n- Raghuraj, P., and Shirley Telles. “Immediate Effect of Specific Nostril Manipulating Yoga Breathing Practices on Autonomic and Respiratory Variables.” Applied Psychophysiology and Biofeedback Appl Psychophysiol Biofeedback 33.2 (2008): 65-75. Web."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b1bd4aac-aebe-48a6-8b49-7ba2086ee274>","<urn:uuid:1dd56aad-3763-4dbb-b372-7b905722681f>"],"error":null}
{"question":"What are the energy-saving benefits of LED lighting in industrial and landmark buildings, and how does it affect both maintenance requirements and visual quality in these settings?","answer":"LED lighting offers significant energy savings in both settings - for example, at LA City Hall, 130-watt sodium lamps were replaced with efficient 27-watt LEDs, while the Natural History Museum uses one-third less energy with LED lighting compared to traditional incandescent lighting. Regarding maintenance, LED bulbs have an exceptionally long lifespan, reducing the need for frequent replacements in challenging locations like museum ceilings and crawlspaces. In terms of visual quality, LEDs provide consistent illumination that maintains brightness levels over time, while also offering improved color rendering compared to HID lamps with their marginal CRI (Color Rendering Index). The quality of LED lighting enhances safety, productivity, and product quality in industrial settings, while also providing appropriate illumination for modern equipment like computer monitors on factory floors.","context":["By David Dickstein,\nIt’s one thing for do-it-yourselfers to take on backyard lighting projects, but it’s another matter when a lighting manufacturer lights up its own backyard – as in Southern California itself.\nTorrance, California-based LEDtronics is putting a literal shine on the regional landscape with its innovations in light-emitting diode, or LED, technology. From the coastal Santa Monica Pier and Vincent Thomas Bridge to the civic centers of Los Angeles, Pasadena and San Pedro, to downtown’s City Hall, Memorial Coliseum and Natural History Museum, LEDtronics is playing a major role in lighting up many of the Southland’s storied landmarks.\n“One of the charms of Los Angeles is that it is embraced by locals and romanticized by tourists, and in order for L.A. to remain endearing, the city and surrounding area cannot be seen in poor, dim lighting – it’s not a good look,” says Pervaiz Lodhie, an entrepreneurial Pakistani immigrant who in 1983 applied his mechanical engineering degree from Cal State Los Angeles to found LEDtronics with his wife, Almas.\nThe welcoming feel of LEDs lends the “right look for L.A,” according to Lodhie, considered a pioneer in LED lighting with dozens of patents to his name. Aesthetics aside, LEDs burn cooler, use significantly less energy and last tens of thousands of hours longer than traditional incandescent lighting, among other benefits.\nFor iconic L.A. City Hall, LEDtronics was tasked with upgrading the 13 lampposts that light up the tree-lined pathways and stairs of the grounds’ 1.7-acre park. “Our goal was to make the area safer, brighter and more beautiful,” Lodhie says. Energy-gobbling 130-watt high-pressure sodium lamps were replaced with efficient 27-watt LEDs.\nBesides city halls – Pasadena’s and San Pedro’s included – LEDtronics’ mark on the illumination of the region is also seen along one of L.A.’s busiest arteries; for the Memorial Coliseum sign along the Harbor Freeway, 160 fluorescent T12 tube lights were replaced with energy- and cost-saving LED T8 tube lights. Since LEDs were installed, the freeway-fronting sign consumes almost 300 KwH less per day, a 57 percent reduction in daily consumption, according to the Los Angeles Memorial Coliseum Commission.\nHelping Make a Pier Appear\nTwenty miles up the Pacific coast from LEDtronics’ South Bay headquarters and plant, one of only a few LED factories in the entire United States, the company has added to the beauty of beloved Santa Monica Pier. Its influence, which also includes a safer visitor experience, is seen and felt from the boardwalk to the historic Looff Hippodrome that houses the restored 1922-circa carousel. Along the pier’s perimeter, LED bulbs glow from the beacon light fixtures that enhance the seaside experience for seven million visitors annually.\nFurther south, Vincent Thomas Bridge’s lighting system includes 80 custom fixtures from LEDtronics that are installed on both sides of the suspension cables. The blue beacon LED lamps housed inside custom “jelly jar” fixtures are seen by 32,000 motorists daily.\nLight at the Museum\nAt the popular Natural History Museum of Los Angeles County, over 4 1/2 billion years of history are not only shining bright thanks to modern LED technology from LEDtronics, but being better preserved.\n“LED-based light sources are generally safer for our sensitive collections as they emit very little of the damaging infrared and ultraviolet radiation associated with some traditional light sources,” says Liam Mooney, the museum’s supervisor of media and lighting.\nLEDtronics supplied bulbs for exhibit space, the perimeter of the 57-foot-high rotunda and other areas of the Exposition Park attraction. Benefits of LED lighting go beyond maintaining the condition of the museum’s fragile treasures.\n“Where we have fully transitioned to LED-based lighting, we are using a third less energy than we would be with traditional incandescent lighting,” Mooney says. “Our staff is also spending less time working on ceilings, in crawlspaces and other challenging conditions to maintain this equipment.”\nIn terms of visitor experience, the museum’s lighting chief points out that the long lifespan of LED bulbs means “things are more likely to stay as sparkling and legible as they were on opening day,” which, for this facility, was 1913. “Visitors can consistently experience the exhibitions as intended,” he says.\nFrom brightening a prehistoric walking whale for 800,000 visitors a year to illuminating L.A.’s light rail for 400,000 riders a day, locally produced LEDs also assist those who use public transportation to get to and from some of these popular destinations. Metro is among the nation’s earliest regional transit agencies to adopt LED lighting, having employed the technology since 2003. For its Metro Rail system, the agency uses miniature-based indicator bulbs, tube lights, intermediate-based bulbs and PAR (parabolic aluminized reflector) spot and flood bulbs at the control panels and passenger cars of its light rail operation.\nWhile the majority of LEDtronics’ customers are understated and nondescript to the outside world, the lighting manufacturer takes heart in putting a shine on some of Southern California’s most used or visited settings.\n“Not too many companies can contribute to both the safety and beauty of their own backyard,” Lodhie says. “How wonderful that we can do this and save the public sector money at the same time, let alone helping the environment. The millions of people we affect daily don’t know us by name, but in this business there are other ways to bring what we do to light.”\nCopyright © 2019 California Business Journal. All Rights Reserved.","Industrial buildings follow the lead of other sectors when it comes to lighting trends\nAlthough industrial lighting has historically been a step behind technologies used for other building types, many of the advancements made over the past decade in lighting for other occupancies are finally taking hold in industrial spaces. It's only a matter of time before task-oriented lighting, energy-efficient alternatives, and bi-level lighting designs are installed regularly in industrial buildings across the United States.\nIndustrial lighting is following the trend in other sectors where “light the task” has become the popular mantra. While lighting designs for office spaces no longer light the top of the file cabinet and bookcase along with the desktop, warehouse lighting is no longer just concerned about horizontal footcandles to avoid forklift collisions. Vertical footcandles on the stored materials are also an important element of the design criteria. In addition, industrial spaces are no longer hanging 400W high-intensity discharge (HID) high-bay fixtures on 25-foot centers from wall-to-wall.\nAs “lighting the task” tends to reduce lighting levels overall, the levels of lighting for portions of industrial spaces need to increase. For example, raw material and finished goods staging areas may need only 30 footcandles for safe operation, while final inspection areas require more than 80 footcandles. However, a compromise of 50 footcandles everywhere is not the solution. Nevertheless, reasonable uniformity is necessary to reduce adaptation discomfort when processes require workers to look from one light level to another.\nProper lighting levels in industrial occupancies also have the highest relation to safety and lost time reduction when compared to just about any other occupancy. Low light levels at workstations, such as inspection stations, can lead to eyestrain and fatigue.\nOne solution is a simple strip fixture over the inspection station with a local switch to boost light levels only where and when it's needed (Photo). Improving on this simple approach, you can also install an occupancy sensor at the station for automated control. In fact, some may argue that task lighting was invented for industrial applications — the task light on the band saw was probably in service long before the task lights in office cubicles.\nWhile not new, bi-level lighting can also be an effective industrial lighting design tool. Optical storage systems “pickers” and robotic systems need little, if any, light to perform their task. People, on the other hand, need lighting to perform maintenance and make inevitable repairs on equipment.\nBi-level lighting systems have long been in use in warehouse aisles. With the insignificant cost of occupancy sensors, the time has come to control lighting in more areas by occupancy. This technology should not scare facility managers — the horror stories associated with occupancy sensors can almost always be attributed to improper sensor application or adjustment.\nLighting in an industrial space should also offer flexibility, a characteristic lighting in other spaces generally does not have or need. For instance, if an office layout changes, the ceiling lighting is usually fairly uniform, and the office moves beneath the lighting. If the lighting is task-oriented, the lighting moves with the task. In an industrial setting, advances in technology now warrant flexibility as well.\nIf high-bay fixtures of any type are used, you should install them to be flexible. A few industrial spaces have the luxury of truly high (25-foot or better) ceilings, where the industry can evolve and move beneath the lighting. More recent industrial spaces, however, feature lower ceilings. Therefore, moving or updating the industrial equipment can often affect the lighting layout. For this reason, flexibility should be a key design component.\nLighting power bus rather than fixed power points can save money over the life of the occupancy when the delivery of a new generation of computer numerically controlled (CNC) machines means that 20 light fixtures have to be relocated. Multi-phase lighting power bus can also allow for the staggering of phases to help reduce the strobe effect of HID lighting systems used in large industrial spaces.\nNew lighting technology\nEnergy codes are forcing the use of higher lumen-per-watt light sources. HID givens excellent lamp life and acceptable color rendering index (CRI) for factories, but provides less-than-ideal lumen maintenance. However, as technology bullies its way out of the computer room and onto the shop floor, lighting technology must keep pace.\nCeramic metal-halide (CMH) lighting virtually eliminates the lumen maintenance issue, maintaining more than 95% of initial lumens over its published life. CMH will likely become on par with pulse-start metal-halide systems. Existing CMH products allow lamp-only replacement for HPS fixtures without expensive and cumbersome ballast retrofits.\nInduction lighting is another technology that promises great things in terms of improved light sources, offering lamp life as long as 100,000 hours. While perhaps not yet a solution for general industrial space lighting, applications where lamp maintenance is dangerous or costly (such as towers) can benefit now. In spaces where reliability is a key factor, such as obstruction lighting, designers are also pursuing induction lighting options. However, it will be a few more years until induction lighting sources work their way into more fixture types and lumen output increases.\nFinally, daylighting is also making a comeback in industrial facilities. Cinder block walls and rubber roofs are typically cheaper to install than glazing, and industrial facilities built over the last 75 to 100 years minimize glazing to save on construction costs. Pre-1900 era factories used the well-known saw tooth roof design with extensive roof glazing. Endless maintenance issues and rising heating costs drove builders away from this architecture, but it is making a comeback as more designers realize the lost benefits of natural daylight.\nQuality and Quantity\nAnother factor known for years in retail lighting is finally reaching the industrial lighting arena: the quality of the light is just as important as the quantity. HID lamps with their marginal CRIs and poor lumen maintenance are being removed from service in favor of high lumen T8 and T5 fluorescent packages. Ceramic discharge lighting is offering high CRI with long life and exceptional lumen maintenance. More and more cathode ray tube (CRT) computer monitors and flat screen monitors are making their way onto the factory floor, and visual comfort and veiling glare are no longer issues confined to the office environment.\nQuality lighting increases safety, productivity, and ultimately, product quality. Keep in mind, however, that “rebate-able” systems are not always the best choice, even from an energy standpoint. Many utilities rebate T5 lamp packages even though a properly implemented T8 package with a higher ballast factor can offer better energy efficiency.\nRe-lamping, re-ballasting, and/or replacing fixtures can carry with it considerable up-front costs. Most facility managers will agree that while group re-lamping makes sense from a logistical and financial standpoint, few facilities actually implement the approach.\nIt is often difficult to convince the finance managers that preventive maintenance pays off, as they often cannot see past the initial cost, and ignore the payback over time. For the same reasons, facilities do not see the value in lighting systems upgrades. With increased global competition to cut costs across the board, spending money on the infrastructure is not always the first consideration. Between utility company rebates and soft cost factors, such as increased productivity and employee retention, moving toward a higher quality, lower energy lighting system makes sense.\nThe trend in green construction is also forcing improvements in industrial lighting. Building owners do not have to ask their consultants to include energy-efficient lighting sources in their design because the issue has become the norm. Sustainable design approaches are also cognizant of lighting quality. Sustainable design is not simply the reduction of raw materials and waste related to construction. It is the optimization of the entire facility, including lighting.\nWhile trends in lighting may not always reach the industrial sector as quickly as other occupancies, design alternatives traditionally used in office, retail, and residential buildings are finally making their way to industrial buildings. As lighting technology further evolves for other occupancies, industrial facility managers will benefit from continuing to adopt these up-and-coming lighting concepts.\nNangle is a founding partner of Cronis, Liston, Nangle & White, LLP (CLN&W), a consulting engineering firm headquartered in Danvers, Mass."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:eb957cf8-dfbf-4fe7-ac95-b6505ac0d584>","<urn:uuid:602a1e95-f320-43e1-85e4-93e471ad4c9e>"],"error":null}
{"question":"How does water management in California's Central Valley farming compare to water restrictions at Lake Tahoe for boating and swimming activities? Please analyze the different approaches to water resource control in these two locations.","answer":"The water management approaches differ significantly between these locations. In California's Central Valley, water management is primarily controlled through a complex infrastructure of dams and aqueducts created by state and federal governments, with access affected by drought conditions, court decisions, and government quotas. Farmers like Woolf Farming & Processing face significant challenges including water shortages that have led to fallowing land and removing water-intensive crops. The water is also subject to environmental regulations like the Endangered Species Act, which protects species like the delta smelt. At Lake Tahoe, water management focuses more on recreational use regulations, with specific speed restrictions in boat harbors (limited to 5 nautical miles per hour in areas like Zephyr Cove and Cave Rock) and complete boating prohibitions in designated swimming areas such as Sand Harbor and Diver's Cove. Lake Tahoe's water level was artificially raised by 6 feet through dam construction in 1909, but current management primarily revolves around controlling access and use rather than agricultural distribution.","context":["HBS Cases: Who Controls Water?\nIn a recent field study seminar, Professor Forest L. Reinhardt discussed the case \"Woolf Farming & Processing,\" which illustrates how access to water—a basic building block of agriculture—is affected by everything from complex government-mandated requirements to a 3-inch endangered bait fish.\nAlthough much of the globe is awash in it, the allocation of water for human consumption is anything but easy. As the planet's population grows, urbanizes, and is subjected to climate change, many experts foresee a global water crisis (and resulting food shortages and increasing prices) looming over the next 40 years.\n\"The basic problem from the farmers' point of view is that the water is in the wrong place at the wrong time.\"\nThe 2009 case \"Woolf Farming & Processing\" illustrates many of these challenges through the eyes of Stuart Wolf, a central California family farmer who must grow crops even as the available water supply to his operation is curtailed by drought conditions, court decisions, and quotas imposed by government agencies.\nThe case examines how water, a basic building block of agriculture, is affected by a complex web of political, environmental, and agricultural pressures, and the effects those impacts have on business, society, and global food production.\n\"Like all good cases it's a good story, and like all good cases it also tells you something more general about important problems,\" said Forest L. Reinhardt, the John D. Black Professor of Business Administration at Harvard Business School.\nReinhardt led a discussion of the case recently in the MBA field study seminar Innovation in Business, Energy, and Environment, held in Harvard's Innovation Lab. The case was prepared by HBS Professor David E. Bell, Laura Winig, and Mary Shelman (HBS MBA'87), director of the Agribusiness Program at HBS.\nA family farm\nWoolf Farming & Processing, established in 1974, is located in the southern part of California's Central Valley, one of the most fertile areas in the country. The valley has been heavily farmed since the the 1850s, and through the years sharp rises in agricultural production and population quickly depleted the local water supplies.\nIn response, the state and federal governments stepped in and created a water infrastructure of dams and aqueducts that Reinhardt said was \"overcommitted the day it was built.\" The last project was completed in the 1960s, when California had a much smaller population, with much smaller bank accounts. Woolf Farming & Processing—along with thousands of other farmers—relies on this aging infrastructure. Stuart Woolf and his father, Jack, before him had invested heavily in water management technologies because they were aware of its weaknesses.\n\"The basic problem from the farmers' point of view is that the water is in the wrong place at the wrong time,\" said Reinhardt.\nIt's also one of the reasons why a sustained drought in the late 2000s laid 450,000 acres of Central Valley farmland fallow, which pushed local unemployment rates as high as 40 percent, drove protesters into the streets, and led Woolf to cut his crop back dramatically, at one point grinding up 90,000 water-hungry almond trees.\nThroughout the discussion, Reinhardt worked with students to sort out a complex tangle of interdependent contributing factors.\nEnvironmental and legal issues muddy the water, for example. In the mid-2000s efforts to save a 3-inch fish called the delta smelt from extinction led to a ruling by a US district court judge that significantly limited the amount of water that could be pumped to farms out of the Sacramento-San Joaquin River delta. The theory behind the ruling was that the smelt were sensitive to salinity levels, which rose as water levels lowered.\nThe Endangered Species Act is another factor working against the farmers.\n\"The Endangered Species Act is an interesting piece of legislation,\" said Reinhardt. \"The federal government has to do everything it can to reduce the probability of extinction. It's explicitly prohibited in the statute from balancing costs and benefits…It doesn't even matter if you have little prospect of saving the fish. You still have to spend any amount of money you can to protect it.\"\nIt's easy to count the damages to the farmer, he added, but very difficult to value the damage caused by the extinction of the smelt.\n\"The environmentalists argue that if you let the bureaucrats do cost-benefit analysis, the fish will lose every single time,\" he said. \"And you'll have extinction after extinction after extinction.\"\nBut the farmers benefit from other political decisions. Because of government subsidies, the price some pay for whatever water they do get is a fraction of its actual cost.\nReinhardt shifted the conversation to discussing solutions, such as assigning property rights to water.\n\"In a way the history of Europeans and Asians on this continent has been the story of environmental abundance converted to scarcity through systems of open property access, which then lead to the creation of private property rights because that's the only way of solving the problem,\" he said.\nSolving the problem by treating water like any other commodity sparked lively discussion.\n\"If the world trade system founders it's going to founder in food.\"\n\"Farmers would probably have a price of water that would be significantly higher than it is right now so they probably would not plant thirsty crops,\" remarked one student. \"The market would sort out the farmers and the environmentalists somewhere in the middle…the environmentalists would save the species they really wanted to save.\"\n\"With a completely free market with no restrictions, I can at least imagine an outcome where there is a winner,\" said another student. However, the losers in such a free market would not survive.\nYet a market that guarantees people will get the water they need to support basic human survival is not the perfect solution either. \"You still have the problem of the huge year-to-year volatility of water available,\" said one student.\n\"You absolutely have a massive hydrological risk,\" replied Reinhardt, a risk currently borne by the farmers, since the environmental flow of water is inelastic because of the Endangered Species Act.\n\"The farmers would still have risk,\" he continued. \"But it would be price risk instead of quantity risk…If there's a market, clearly the price is going to be higher than the price that some farmers are going to be able to afford.\"\nThis could be a good thing for Woolf Farming & Processing because it could purchase, on the cheap, neighboring farms that didn't invest in water conservation technologies. Stuart Woolf might then be better positioned to meet his own objective, as stated in the case: \"to be a part of a food system that would feed a growing world population with fewer resources.\"\nWorld food prices are a definite concern. \"The inflation-adjusted price of food has been falling over time,\" said Reinhardt. \"People used to spend 50 percent of their income on food.\" For residents of wealthy nations, that number has dropped to 2 percent. \"Most people don't think about whether that decline will continue or not,\" he added. If it does not, \"the world is going to be a very, very different place…If the world trade system founders it's going to founder in food.\"\nOther options were also discussed: moving out of California and buying land in more politically, geographically, and economically friendly countries; planting less thirsty crops; and giving up farming altogether and switching to the solar energy business, for instance.\nIn the end, the class did not come up with a clear solution—because there isn't one.\nSocieties persuade institutions to solve one set of problems. If the institutions are successful, more prosperity is created, which leads to the obsolescence of the very institutions that created the prosperity. Eventually there's the need to create institutions to manage the new disparity. \"It's an old story, \"Reinhardt said. \"And that's the position in which we find ourselves in California.\"","Map of Lake Tahoe\nView Nevada Fishing Map in a larger map\nGeneral John C. Fremont was the first of the European settlers to discover Lake Tahoe in 1844. Prior to this “discovery,” Washoe Indians seasonally inhabited the basin. A dam was built at the outlet (Truckee River) in 1909, raising the lake an additional 6-foot. The lake is known for its deep blue color and sits in a beautifully forested area at a maximum elevation of 6,229 feet, MSL.\nLake Tahoe, located in the Sierra Nevada, covers 121,000 surface acres or 192 square miles to a depth of 1,646 feet. Several introduced sport fishes inhabit the lake including lake trout, rainbow trout, brown trout, and kokanee salmon. Native fishes include mountain whitefish, Tahoe sucker, Lahontan sucker, speckled dace, redside shiner, Lahontan tui chub, and Paiute sculpin. Survey data generally shows anglers averaging about 1.5 fish per day. Fishing throughout the year is variable, but for shore, topline, and deepline angling, July and August are the most productive months in terms of angler success rates. Fishing from the two State Parks (Cave Rock and Sand Harbor), shore and topline angling seems to produce the best catch rates for small rainbow trout. These are also locations that NDOW stock. Shore access is limited due to private property and very limited parking. Standard shore baits include worms, Power Bait, salmon eggs, and minnows. Lures such as Mepps, Panther Martins, Rapalas, and Daredevils often catch cruising rainbow and brown trout. Topline trolling for rainbow and brown trout is, at times, productive, while deepline trolling and jigging are the most widely used techniques for mackinaw (lake trout). Numerous commercial guide services are available for anglers that lack the extensive fishing setups required for lake trout fishing and toplining. Record fish from the Trophy Fish Program include a 4-lb 13-oz kokanee salmon, a 15-lb 15-oz brown trout caught in 2011, a 37-lb 6-oz mackinaw (state record), an 11-lb 7-oz rainbow trout, and a 4-lb 9-oz mountain whitefish (unofficial state record).\n25,000 triploid rainbow trout will be stocked at Cave Rock and Sand Harbor.\nSeason is open year around, 1 hour before sunrise to 2 hours after sunset, except for the following closed areas: Within a 200 yard radius of the mouths of Third, Incline, and Woods Creeks; a 500 yard radius from Sand Harbor Boat Ramp; and within the boat launch area inside the jetty at Cave Rock Boat Ramp. Limits are 5 coldwater game fish of which not more than 2 may be mackinaw (lake trout). Live Bait: Only Lahontan redside shiner, tui chub, Tahoe sucker, mountain sucker, Paiute sculpin and speckled dace may be used as live bait fish. Fish must be taken from and native to Lake Tahoe. Anglers may possess either a Nevada or California license to fish anywhere on the lake.\nReduced Speed Areas: All boat harbors and other areas designated by buoys are areas in which a vessel must be operated at a speed that leaves a flat wake, but in no case may a vessel be operated at a speed in excess of 5 nautical miles per hour to include: Zephyr Cove, Cave Rock, Round Hill Pines Beach and Glen Brook Bay in Douglas County; Sand Harbor; Incline Village General Improvement District Boat Ramp, and Crystal Shores West in Washoe County. Boating Prohibited Areas: Vessels are prohibited in areas which are designated by signs or buoys as follows: the main beaches at Sand Harbor and Diver’s Cove within Lake Tahoe State Park; the swimming area of Incline Village General Improvement District and Burnt Cedar Beach; the swimming area of Galilee at the Episcopal Camp and Conference Center; the swimming area of the Lakeridge General Improvement District; the swimming area of the Glenbrook Homeowner’s Association; the swimming area of the Hyatt Regency Lake Tahoe; the swimming area of Zephyr Cove Marina; and the swimming area of Crystal Shores West; the swimming and beach area adjacent to Nevada Beach described in CFR 162.215. Motor Restrictions: There are motor restrictions (type) imposed by the Tahoe Regional Planning Authority (TRPA). Visit www.trpa.org.\nFrom Reno, travel 10 miles south on Hwy 395 to SR 431, turning right and traveling 20 miles west to Lake Tahoe. From Carson City, travel 2 miles south on Hwy 395 to SR 50, turning right and traveling 14 miles west to the lake. Hwy 28 circumnavigates the lake."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:176d44c5-f390-46c2-bf11-cb2df963b994>","<urn:uuid:2d225518-e008-4775-a0a0-3c52000bf6de>"],"error":null}
{"question":"What types of liability insurance does a coffee shop need for general safety, and what specific permits are required from local authorities to operate legally?","answer":"Coffee shops need several types of liability insurance coverage: general liability insurance for third-party injuries and slips/falls, product liability insurance for food-related illness claims, and employment practices liability (EPLI) for worker-related claims. For legal operation, coffee shops must obtain multiple permits including: a business license, an Employer Identification Number (EIN), a retail food establishment license from the health department, a certificate of occupancy, resale and seller's permits, and music licenses if playing music. They must also comply with local health regulations and undergo regular health inspections of cold storage, kitchen equipment, and food preparation areas.","context":["Protecting yourself and your business against potential risks is an important consideration for anyone operating in the food and beverage industry. As a coffee shop owner, you’ll need to carefully consider various insurance solutions that fit your needs and cover all potential costs associated with running a cafe. In this post, we’ll outline the different types of coverage available to help protect your business from unforeseen events such as theft, natural disasters or property damage resulting from accidents. We’re also going to provide key tips on what to look for when selecting the right policy for your cafe so that you can ensure peace of mind as you serve up delicious cups of joe!\nInterested in opening a coffee shop? Read the complete guide to opening a coffee shop.\nCoffee Shop businesses and their risk for liability\nCoffee shops have become increasingly popular in recent years. From independent mom-and-pop shops to large chains, they offer a variety of beverages and snacks to customers that are often eager to get their caffeine fix. However, in the wake of this new trend, there is an often overlooked risk: business liability. Coffee shops are not immune to accidents or other unfortunate events, which can lead to expensive lawsuits and hefty payouts for damages. That said, it’s important for cafe owners to consider insurance options that can help protect their business from financial hardship in the event of an incident.\nTo begin with, the most obvious form of insurance for coffee shop owners is general liability insurance. This type of policy covers third party injuries on your property due to slips and falls or other accidents related to your business operations. The coverage also extends beyond physical harm; general liability insurance will also cover you if someone sues your business due to libelous statements made on behalf of your shop, or even just slander from disgruntled customers who weren’t happy with their service.\nIn addition to general liability coverage, many coffee shop owners choose to acquire product liability insurance as well—especially if they make and sell food items like pastries or cake slices alongside their caffeinated wares. Product liability protects businesses against claims of illness or injury caused by products sold at their establishment—which can include things such as spoiled items or cooked foods contaminated by poor hygiene practices. It’s especially important if a customer becomes ill after consuming something purchased at your shop; without adequate insurance coverage, you may be liable for any medical bills incurred by the individual as a result of their visit.\nAside from legal expenses associated with slips and falls or food-related incidents, another common risk faced by coffee shop owners comes in the form of damage done during natural disasters (such as floods, fire, earthquakes). Most standard forms of commercial property insurance do not cover these specific types of events so it’s always wise for cafe owners looking into purchasing policies that do provide coverage against disasters that could potentially damage their store fronts and equipment—a particularly important consideration in areas prone to seismic activity such as California’s coastal cities where tremors have been known to occur periodically throughout the year.\nFinally, before selecting an insurer it makes sense for coffee shop owners looking into protecting themselves against lawsuits related directly to employment practices liability (EPLI), which provide protection from allegations made by workers that they were treated unfairly while employed at the coffeehouse such as wrongful termination claims or sexual harassment allegations brought up by employees or former staff members alike.\nOverall, having proper liability coverage is a must for any coffee shop owner who wants maximum peace of mind when running their business operations since it helps protect against unforeseen legal costs stemming from a variety of different circumstances – whether its physical harm due unsafe conditions present onsite; contamination caused inadvertent neglect when serving food items; destruction resulting from natural disasters; and/or workplace related grievances filed either internally or externally – all cost considerations every store owner should factor into when deciding upon an appropriate plan best suited for them given risks inherent with operating such an active establishment in today’s marketplace .\nThe importance of protecting your coffee shop with the right insurance\nRunning a coffee shop is a great way to make a living — and it can be an extremely successful venture. Unfortunately, the potential losses associated with such a business can also be significant if something goes wrong. That’s why it is essential for any coffee shop owner to protect themselves with the right insurance.\nFrom costly equipment to protecting against employee theft, investing in insurance for your coffee shop is a smart business decision — no matter how big or small your operation may be. While you might come across some minor hiccups along the way, having the right coverage in place will ensure that you are well protected from any major issues that could potentially arise.\nStart by considering basic factors — like property damage and business interruption insurance — then move on to look into more specialized coverage such as liability protection and employee theft. Property damage insurance will safeguard you against any physical damage related to your store’s operations, while business interruption ensures that you remain profitable during periods of forced closure due to events such as natural disasters or pandemics.\nLiability protection is often overlooked but is an important consideration for any coffee shop owner — this coverage can help cover costs incurred from third-party claims resulting from injuries or illnesses related to your store’s operations (such as someone slipping on wet floors). Employee theft coverage protects you from unexpected losses due to worker dishonesty, which can include cash shortages and inventory shrinkage from employees stealing product or supplies.\nFinally, consider investing in cyber security insurance to protect yourself against data breaches and other cyber threats. With so many customers paying for their orders with credit cards, cybersecurity becomes vital for keeping their information safe and secure. Cybersecurity coverage helps protect you against costly fines and other fees associated with data leaks, as well as the cost of repairing reputational damage caused by compromised customer information.\nNo matter what kind of coffee shop business model you have chosen (or are still considering), being properly insured should always top your list of priorities when setting up shop. There may be upfront costs involved, but the peace of mind knowing that you are fully protected will more than make up for them in the long run!\nHow to choose the best policy for your coffee shop\nChoosing the right insurance policy for your coffee shop is an essential part of running any business. Insurance helps you protect against risks, liabilities, and other unforeseen events that could affect your operation. Before deciding on the best insurance policy, you’ll need to consider the size of your coffee shop and the type of coverage you require. Here are some tips to help you choose the most appropriate coverage:\n- Calculate Your Liability Risk: Liability risk is anything that could cause financial harm or loss to someone else such as property damage or personal injury. You’ll want to make sure that your policy covers all areas where this kind of risk is likely to occur in relation to your coffee shop. This includes premises liability, product liability, and professional liability if you have employees who serve customers directly.\n- Consider Other Risks: Besides liability risk, there are other potential risks associated with owning a coffee shop such as fire damage, theft, or employee injury. Make sure you research all types of potential risks and choose a policy that will adequately cover them in case something unexpected happens.\n- Review Deductible Options: Many policies will offer different deductibles depending on how much coverage you need. A higher deductible will reduce the monthly premium but also leaves you more vulnerable if something goes wrong as it means you’ll need to pay more out-of-pocket costs before insurance kicks in. Consider your budget and select a deductible option accordingly so that it fits into your finances while still protecting you from catastrophic losses just in case something occurs unexpectedly.\n- Analyze Coverage Limits: Different coffeeshops may have different needs when it comes to insurance coverage limits because of their size or specific policies they follow. Carefully review what kind of limits are offered by different providers and calculate what would be sufficient for protecting your own business operations should something happen unexpectedly. The last thing you want is to be underinsured!\n- Check Reviews & Ratings: Once you’ve narrowed down a few providers that seem like good options for covering your café’s liabilities, take the time to read reviews left by past customers about their services and customer support team quality. A good provider will look after their existing customers as well as attract new ones with excellent service quality so do try get some feedback before making a final decision!\nChoosing an appropriate insurance policy can be intimidating but following these steps can ensure that you’re prepared in case anything unexpected happens at your coffee shop. Do thorough research, carefully compare available options and make an informed decision based on what best suits your particular circumstances\nFactors to consider when reviewing your coffee shop’s insurance needs\nRunning a coffee shop involves a certain amount of risk, and having the right insurance policies in place can help protect your business from potential liabilities. Before signing up for any particular coverage, there are certain factors you should consider when it comes to reviewing your cafe’s insurance needs.\nFirst, be sure to have a clear understanding of the different types of coverage available and how they apply to your specific situation. For instance, liability insurance is designed to protect you from bodily injury or property damage claims resulting from your operations – such as if someone were to get hurt on premises or somebody’s device gets damaged during use in your cafe. Property coverage can also be obtained for physical structures related to the business – such as buildings, equipment and furniture purchased for the establishment itself.\nAnother important factor to consider is identifying major areas of risk that could require additional coverages based on the type of services offered in your cafe – such as food poisoning or serving alcohol resulting in third-party injury/damage. It is thus essential that you review all specifics related to these matters so that you are adequately prepared against potential lawsuits or fines should they arise in the future.\nAdditionally, depending on where you live it may be necessary to obtain disability insurance (or workers compensation) prior to employing any staff members at the venue – providing them with sufficient protection in case of sickness or workplace accidents that result in time off from work or medical expenses incurred by an employee.\nFurthermore, many businesses prefer covering themselves with umbrella policies due their comprehensive nature – often offering additional layers of protection encompassing several different types of risks operating within an organization (including those listed above). It’s thus important for owners/managers to take these into account before deciding which option would best suit their coffee shop’s needs.\nAssessing coffee shops’ insurance needs involve taking multiple aspects into consideration before reaching a decision as there are various different forms of coverages available tailored towards specific activities occurring within establishments like these – meaning entrepreneurs must do their research ahead of time so that they know exactly what forms of protection they need prior to investing heavily into any given policy!\nTips for reducing risk in your coffee shop\nWhether you own a small coffee shop or are thinking about opening one, it is important to take precautions to reduce risk. As the owner of a small business, there are certain unique risks associated with running a coffee shop that can be difficult to plan for. By taking steps to reduce risk and create a safe environment, you can ensure your customers have the best possible experience while they enjoy a cup of their favorite coffee.\nInvest in Quality Equipment\nInvesting in quality equipment is an important part of reducing risk in your coffee shop. When purchasing coffee machines, choose high-quality options that have been certified and tested according to industry standards. This will help ensure that any equipment malfunctions or breakdowns will be less likely. Additionally, if you are using older machines, make sure to inspect them on a regular basis for any signs of wear and tear.\nDevelop Strong Safety Protocols\nDeveloping strong safety protocols is essential for reducing risk in your coffee shop. Establish procedures for cleaning and sanitizing any shared surfaces such as counters, tables and menus after each customer visits your establishment. Implement standards for food storage and preparation that meet both local health regulations and industry standards. Make sure all employees understand these safety protocols and follow them closely when serving customers in order to minimize any potential risks associated with foodborne illness or other contamination issues.\nTrain Employees Properly\nTraining your coffee shop employees properly is also key when it comes to reducing risk in your coffee shop. Make sure that all staff members understand how to properly operate any machinery required for making drinks or snacks, as well as proper cleaning techniques so everything is kept clean at all times. It’s also important to train everyone on customer service skills; teach them the importance of being professional and courteous while interacting with customers so they feel welcome whenever they visit your store.\nPurchase Good Insurance Coverage\nFinally, purchasing good insurance coverage is key when it comes to reducing risk in your coffee shop business. There are many different types of insurance available designed specifically for businesses such as yours; talk with an experienced insurance agent who can recommend the best type of coverage based on the size and scope of your business model. Having adequate insurance protection against potential liability issues can give you peace of mind knowing that you’re covered should anything ever happen while someone is visiting or working at your establishment.\nBy following these tips for reducing risk in your coffee shop, you can create a safe environment where customers feel comfortable coming back again and again!\nChoosing the right insurance coverage for your coffee shop is an important part of reducing risk and protecting your business. Taking time to review available options and weigh the costs against potential risks can help ensure that you have the best coverage possible.\nMore Coffee Shop Articles\nLaunching a coffee shop can be an overwhelming task, especially if you have limited resources. Finding the right partner can be an invaluable asset to your business and help you reach your goals faster. In this post, we’ll discuss when it’s a good idea to find a partner for your coffee shop business.\nStarting a new business is an exciting but daunting undertaking. If you want your coffee shop to take off and reach its full potential, it’s important to hit the ground running. In this post, we’ll explore how to accelerate your coffee shop startup by addressing key areas like planning, marketing, and operations.\nStarting a coffee shop can be a daunting but rewarding task. But without the proper preparation, entrepreneurs can easily make mistakes that could prevent them from achieving success. This post covers five common pitfalls coffee shop owners should look out for to ensure they do not fall victim to them.","Planning and designing your coffee shop is the fun part. The not-so-fun but crucial part of launching your business is the legal stuff. If you hope to keep your doors open, you will need to abide by local regulations and obtain the appropriate licenses and permits.\nRegulations will largely vary from city to city, state to state. However, there are some general requirements that virtually all coffee shops will need to meet regardless of where they're located.\nCoffee Shop Regulations\nWhat Regulations Apply to a Coffee Shop?\nRules and regulations of a coffee shop will depend on the city and state. NC coffee shop regulations may be very different from CA's regulations.\nIt's important to do your due diligence and to start early on so that you understand exactly what you have to do to legally open your shop. The first place to start is by contacting your city to find out more about the local regulations for a coffee shop.\nCoffee Shop Health Regulations\nYour state's coffee shop rules and regulations will also require you to abide by local health regulations. The FDA's Food Code will provide you with a model through which you can develop or update your food safety rules.\nThe Food Code document spans more than 200 pages, and it includes recommendations on important topics, like:\n- The extent of training required by health inspectors\n- How often restaurants should be inspected\n- Standards for commercial dishwashers and refrigerators\n- Cooking temperatures for pork, meat, poultry, fish and eggs\n- Hot-holding temperatures for prepared foods\nIt's important to note that the Food Code is not law, but it does provide suggestions that may be adopted as regulations. Some states and cities adopt the entire code, others adopt only portions, and others don't adopt any part of it.\nHealth inspections are par for the course when running a coffee shop, and it's important to view your health inspector as your colleague rather than your enemy.\nInspectors will typically check your shop's:\n- Cold storage, including refrigerators and freezers\n- Storage methods\n- Temperature monitoring\n- Produce prep\n- Kitchen equipment\n- Cooking and heating\nIt's important to understand your city's health codes and inspections, so you can ensure that your shop is abiding by regulations. The FDA lists retail and food service codes and regulations by state on its website.\nCoffee Shop Licenses and Permits\nWhat licenses are required for a coffee shop?\nWhile every state has its own rules and laws, virtually every business is required to have a business license. There will likely be additional permits and licenses that you will need to obtain to legally run a coffee shop in your desired city.\nEvery city will have its own requirements, so it's important to contact your city/county/state regarding permit regulations. But generally speaking, coffee shops will need the following:\nDepending on the entity type you are forming, you may need a business license and to register with your state.\nCertificate of Occupancy\nIf you are building your shop, you will need a building permit, or certificate of occupancy, from the local building department. You will need to work with the city to determine the regulations for building a coffee shop.\nTypically, most cities will look for approval from the fire and health department before issuing a certificate of occupancy. Approval from the fire and health department will require you to work with your engineer, architect and contractors.\nDuring the build-out process, the city will conduct inspections and may require demolition, zoning and asbestos approval.\nBefore you even think about signing a lease, contact the city to make sure that your chosen location is a viable option.\nEmployer Identification Number (EIN)\nEvery coffee shop will require an employer identification number, or EIN. Think of an EIN as a Social Security number for your business. You will need to register with your state to receive an EIN.\nResale and Seller's Permits\nA sales tax license will be required, which can be obtained through the state. This is the same department that will deal with state wage withholding and unemployment insurance.\nWhile obtaining this license, you can also look into the process of registering with the city or county treasurer for tax purposes.\nSeller's permits are almost always required along with the resale permit. You can register for this permit online through your local government's website.\nSome insurances, or liability licenses, may be required by the state or your landlord. These insurances may include property insurance, general business liability, workers' comp, etc.\nRetail Food Establishment License\nIn order to serve drinks and prepared food at your establishment, you will need a permit from the health department. Ongoing inspections will be required to maintain this license.\nTo obtain this license, you will need to apply with the local health department. You may choose from temporary, mobile and fixed food service licenses.\nWhen submitting your application, you will need the name of your restaurant, your personal information and an address for the shop (even if you're opening a mobile coffee shop).\nIf you plan to put up a sign to attract customers, you will need to apply for a sign permit with the city government. The city will have its own regulations regarding size, lighting and location.\nTo obtain this permit, you will need to work with the city government. Depending on your location, you may need to hire a contractor to obtain the permit and design and install the signage. Some cities require contractors to present the design to the Planning Department in City Hall to be approved.\nIf you want to play music in your shop – live, from a CD or through a streaming service – you will need a music license to avoid copyright infringement. The fines for playing copyrighted music without a license can be $30,000 or more.\nA music license can be obtained through The American Society of Composers, Authors and Publishers, or Broadcast Music Inc.\nWhat is the Average Coffee Shop License Cost?\nMost coffee shops will need a variety of licenses in order to serve the public. The cost of these licenses will vary from state to state.\nA coffee shop license NYC, for example, may cost significantly more than a license in the small town of Hendersonville, North Carolina.\nWhile we cannot give you an exact price, we can estimate or give a ballpark range. Let's go down the list of licenses listed above and discuss the average cost of each one.\n- Business license: registration fee of around $50; license cost can range from $25 to $7,000, depending on the projected profit and type of business.\n- EIN: Free\n- Certificate of Occupancy: About $100\n- Retail food establishment license: $100-$1,000, depending on location and number of employees\n- Sign permit: $20-$50\n- Music license: Can vary greatly, but for background music, it can cost $250-$500\n- Seller's permit: No cost to apply, but some cities will require a security deposit should your restaurant close and you have unpaid taxes. The cost of the deposit will be determined at time of application.\nIt's important to remember that the costs listed above are just estimates we've pulled from our own research. The costs to obtain these licenses and permits may be higher or lower in your city. Contact your state to find out the exact costs.\nYour state may also require additional permits and licenses. You will need to research your individual state's laws to find out which – if any – permits are required.\nLearn more on how to open a coffee shop:\n- Starting A Coffee Shop. Step 1: Choose Your Concept\n- Starting A Coffee Shop. Step 2: Create A Business Plan And Find Insurance\n- Starting A Coffee Shop. Step 3: Budget - Expenses Vs Profits\n- Starting A Coffee Shop. Step 4: Find The Right Spot For Your Shop\n- Starting A Coffee Shop. Step 5: Supplies And Equipment\n- Starting A Coffee Shop. Step 7: Hire The Right Staff\n- Starting A Coffee Shop. Step 8: The Menu And Food Safety Issues\n- Starting A Coffee Shop. Step 9: Marketing And Promotion\n- Starting A Coffee Shop. Step 10: Maximize The Revenue"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2f386310-6a0d-43ab-80ed-980dee56da49>","<urn:uuid:1ec32291-321f-40c7-8556-2be4243d783d>"],"error":null}
{"question":"What are the current challenges in multilingual education across different contexts, and what strategies are being implemented to support self-regulated learning in diverse language environments?","answer":"In linguistically diverse classrooms, students face challenges when they don't use the language of schooling at home. Strategies like Functional Multilingual Learning and translanguaging are being implemented to help L2 learners access content and develop academic language skills. In South Africa, universities are developing multilingual glossaries and translating learning materials into languages like isiXhosa and Afrikaans to support diverse language identities. To succeed in these multilingual environments, students need to develop self-regulated learning capabilities, including setting clear goals, using effective learning strategies, monitoring their progress, and maintaining motivation. They must also utilize environmental resources and develop contextual sensitivity to understand particular learning situations. This combination of language support and self-regulation skills is crucial for academic success in multilingual educational settings.","context":["VLIR-UOS TEAM Symposium\nOdisee University of Applied Sciences, in collaboration with the Flemish Interuniversity Council, UWC, CPUT, Ghent University and HOWEST cordially invites you to attend a symposium entitled\nTraining the teachers of tomorrow: a language across the curriculum approach to meeting the language and literacy needs of a diverse student population\nin Odisee campus Aalst, Kwalestraat 154, 9320 Aalst\non Wednesday 18th of May 2022 from 1 pm until 5 pm\nPlease register before May 15th by clicking on https://bit.ly/3kCa2aw\nCoffee and refreshments in the Wintertuin\nWelcome by the Odisee Director of the Faculty of Education in 1O04 - Gert Naessens\nSession 1. An introduction and overview of the project - Hanlie Dippenaar and Johan Anker\nSession 2. The Opportunities and Constraints of Multilingual Teaching: A Reflection Across Continents - Nell Foster, Maxime Van Raemdonck and Rinelle Evans\nSession 3. The use of multilingual glossaries in teacher training - Linda Manashe, Dorothy Esau and Joanne Arendse\nCoffee break in the Wintertuin\nSession 4. Literacy development in the multilingual context of South Africa - Stuart Strauss\nSession 5. Virtual reality in education: hype or more? - Carl Boel\nSession 6. Back to the basics: a Language MOOC design model for novice students in higher education - Ann Van Kerckhove and An De Moor\nSession 7. Consequences of The National Reading Strategy in Flanders on future teachers - An De Moor\nSession 8. A reflection on the project - Candice Livingstone\nDrinks in the Wintertuin\nThis presentation is about the project “Training the teachers of the future: language policy and literacy development”, an international collaborative project between the University of Ghent, Odisee-hogeschool, Howest Hogeschool, the University of the Western Cape and the Faculty of Education at the Cape Peninsula University of Technology (CPUT). The project started in 2016 during a visit of colleagues from Howest Hogeschool to CPUT. Discussions on the process of integrating language teaching as part of the teaching of all academic subjects were explored and led to this collaboration.\nThe theory underpinning this project is embedded in language across the curriculum and disciplinary literacy where the theoretical aspects and integration of language in teaching were examined. The emphasis was placed on the use of specialised language, knowledge and literacy abilities to create meaning within the different subject disciplines with their specialized genres, text types, symbolic artefacts and traditions of communication. Theories and research by Shanahan, Coetzee-Lachman, Unsworth, Vollmer, and Snow were inter alia studied in this process. Eventually the Multi-layered Model of Language Development (MMLDP) model of Brigulio and Watson (2014) was used to determine four levels of embedded support to be used in the development of a literacy policy. The development of different actions in this project linked to the MMLDP model includes the development of self-access materials for students, generic reading strategies, support for staff, development of rubrics for the teaching of subject-specific characteristics, and staff development toward a fully integrated teaching curriculum for the teaching of subject domains.\nThe project started off with a comprehensive needs analysis of the Faculty of Education of CPUT in 2018. This was followed by staff training and development, postgraduate research, several publications and academic exchanges. Symposiums were presented in both South Africa and Belgium. Conference presentations led to further collaboration with other Higher Education Institutions. Multilingual glossaries were developed and made available to students. A course on diversity and language issues which will be housed on the institutional learner management system (Blackboard) as a compulsory module for all Education students, was developed and will be introduced in due course. The impact of the project will enhance the training of teachers for the classrooms of the future.\nFor many years now, scholars have advocated for multilingual teaching approaches in linguistically diverse classrooms where pupils do not necessarily use the language of schooling at home. Strategies such as Functional Multilingual Learning and translanguaging draw on research about language acquisition and the reality of multilingual pupils’ lives to propose effective practices to enable L2 learners to access content material and develop strong academic language skills. This presentation explores multilingual teaching across two contexts, Belgium and South Africa, to examine how policy makers, teachers and pupils create opportunities for learning, whilst also navigating and embodying local constraints. It compares three key dimensions: language policy, the teacher’s language repertoire and learner identity, to elaborate on commonalities and differences and how we can forge new perspectives by learning across contexts.\nCPUT's language policy defines multilingualism as the use of multiple languages with a particular emphasis on Afrikaans, English, isiXhosa and South African Sign Language (SASL) (CPUT Language Policy, 2019). Responding to the Department of Higher Education's call to promote and strengthen multilingualism in South African universities, CPUT embarked on a process of developing an online Information Literacy (IL) multilingual glossary and the translation of the English IL course learning materials (infographics and e-Books) into isiXhosa and Afrikaans. The aim was to provide isiXhosa, English and Afrikaans home language students with the opportunity to engage with IL learning materials in their home language and in their second and third language and to promote and acknowledge, not only the diverse language identities of a large number of postgraduate students at the institution, but also to include the student voice by including them in the process of co-creating the multilingual glossary.\nQualitative focus group and survey data obtained from purposely selected postgraduate students from the Education Faculty, on the IL multilingual language resource developments, indicated that the respondents overall perceived the multilingual language resources as important to increase and enhance their understanding of IL and research concepts and to develop higher order analytical thinking, reading and academic writing skills of a more exceptional quality. The study follows a constructivist approach as it seeks to understand students’ views about the value of a multilingual glossary and their lived experiences of being immersed in a process of co-creating in such a process.\nIt is recommended that the implementation of the South African government’s language policy on multilingualism in higher education be brought closer to being realised, not only to promote lifelong learning but also to bring about much needed equilibrium in the country’s socio-economic imbalances, advocated by the apartheid regime.\nThe huge number of learners dropping out of the formal schooling system has a significant impact on the ever-increasing unemployment rate in South Africa, which is the reason for the majority of its citizens living in dire poverty. Learners from these poverty-stricken areas rarely have access to educational resources and literacy materials and neither do they attend schools that do. This reality gave rise to an education system which is marred by inequality, exposing the masses of learners from rural areas, townships and other under-resourced dwellings to the difficult and complex challenges at institutions of learning. One such challenge, as emphasized by scholars like Howie (20170) and Spaull (2013), refers to learners’ poor performance in literacy-related tasks, especially when participating in continental and international evaluation programmes. It is within the context of these literacy challenges that this study aims to explore the literacy and language practices of students, lecturers and other role players at a higher education institute in the Western Cape, particularly to establish how these role players negotiate the challenges relating to their educational interactions. In order to advance a comprehensive account of literacy and language practices at the university, this study deemed it necessary to not only focus on students currently studying, but to also include teachers who completed their teacher training at this institute, thus providing holistic reports of past and present encounters of role players’ engagement with literacy and language as learners, students, teachers and lecturers. Through an ethnographic approach, this study will use focus groups and interviews to gather data relating to participants’ schooling history, their challenges with language and literacy and their perception of their educational futures. The data emanating from these methodological approaches will be analysed and interpreted before presentation of the findings with suggestions.\nImmersive virtual reality has gained maturity lately, following major technological advancements by big tech companies such as Meta, HTC, HP and Lenovo. Since the introduction of affordable consumer market VR such as the Oculus Quest leading to mass sales in 2020, VR has jumped forward as an educational tool. Lots of teachers have been engaged in experimenting with virtual reality in their courses ever since, however not always to success. In this presentation we discuss how VR can and should be integrated into secondary education classroom practice fostering the affordances of immersive virtual reality in an effective way.\nLecturers in higher education notice a decreasing language proficiency of first-year students, especially in their academic reading and writing skills. These students also often lack self-regulatory skills which impact their chances for academic success. To address both problems a MOOC was developed, following the methodological framework of Educational Design Research. MOOCs (Massive Open Online Courses) infer affordances which are both beneficial for language and self-regulation. However, most participants of MOOCs lack the essential skills to complete a MOOC, such as self-regulatory and digital skills.\nCarl Boel, Florian Brokken, An De Moor and a team of language experts in Odisee developed a new MOOC design model aiming at improving both language and academic skills in order to attain the original goals of MOOCs: the altruistic nature of open education. Different strategies of implementation and preliminary results are discussed. This MOOC has both an academic and practical impact as it helps to understand how the instructional design of MOOCs can be improved and how first-year students can be supported in their academic language proficiency and their self-regulated learning, with a possible effect on their academic success.\nIn recent years, alarm bell after alarm bell has been ringing in Flemish education. In the international surveys PIRLS and PISA, Flanders is in free fall. Today, Flanders is in a 32nd place in the PIRLS ranking, which charts the reading skills of 10-year olds. No other region has declined more in ten years than Flanders.\nThe Flemish government is investing two million euro’s in a Reading Offensive, a major social project to guide Flanders back to the European top. From crèche to rest home: in the coming years, all Flemings must be drawn into the reading pool. To this end, a broad promotional campaign for reading will be launched. In addition, there will be a new reading fund that can attract extra private funds. The aim is to mobilise as many people as possible to read aloud on a massive scale, including in places where reading is currently absent or too infrequent. Ireland is a common thread in the plan because it has managed to make enormous progress in international surveys in ten years' time.\nThere is a clear focus on teacher training, early literacy and weak and struggling readers. Authors will, for example, work for months in author residencies with schools of technical and vocational education, and with pupils from disadvantaged groups. By means of inspiration guides, the evidence-informed material developed can be used not only by teachers but also in teacher training courses, linked to intensive professional development programmes for teachers, tutors, childcarers, etc. In this respect, an important task is reserved for - also future - teachers. For example, it is a misconception that only language teachers have to deal with language and reading. It is quite normal to have a reading policy for subjects such as mathematics or biology as well, because in any subject you can work with language and reading.","Foundations of Education and Instructional Assessment/Involving Students/Self-Regulation\nStudents becoming Self-Regulated Learners in Distance Learning Programs\nBy: Jessica Coleman\n1. Students should be able to identify processes that classify self-regulated learning.\n2. Students should be able to recognize the effective strategies of becoming a self-regulated learner.\n3. Students should be able to classify the six dimensions of being a self-regulated learner.\nTechnology has brought our society a long way. College students use to attended regular classes on campus in classrooms with the teacher lecturing right in front of them. Technology has brought us so far that college students these days are able to sit at home in their pajamas and take their college classes at online. There are more and more Distance Learning Programs being offered each year from a wide variety of colleges. Many students are choosing to take online courses these days because it allows them more time to complete other tasks, while still working on their education. Online classes is an excellent way for students to obtain their degree. Students that decided to pursue online class must be self-regulated learners. When students are taking online classes there is no teacher in front of them stressing how important it is to stay on target and complete task on time. Students must be organized and motivated to complete their class work on time, therefore students must be self-regulated learners. Self-regulated learning refers to the processes by which individual learners attempt to observe and organize their own learning (Motivation: Self-Regulated Learning, 2009).\nStudents as Self-Regulated Learners\nWhen we hear the word self-regulated learners we automatically think of students. Did you know that students at almost any age are capable of taking charge of their own learning. The fact that almost all people are capable of self-regulation does not mean that they actually take effective charge of their own learning (Self-Regulation of Learning, 1992). Students have the choice of being a self-regulated learner or not. When taking a distance learning course students can organize a schedule so that they know they will complete their assignments on time. Students can also but their assignments off and be procrastinators instead of self-regulated learners. Students that are self-regulated learners develop more effective strategies to help them with their learning task (Self-Regulation of Learning, 1992).\nWhen faced with a learning task, self-regulated learners typically do the following:\n1. They begin by analyzing the task and interpreting task requirements in terms of their current knowledge and beliefs.\n2. They set task-specific goals, which they use as a basis for selecting, adapting, and possibly inventing strategies that will help them accomplish their objectives.\n3. After implementing strategies, they monitor their progress toward goals, thereby generating internal feedback about the success of their efforts.\n4. They adjust their strategies and efforts based on their perception of ongoing progress.\n5. They use motivational strategies to keep themselves on task when they become discouraged or encounter difficulties.\nSix Dimensions to being a Self-Regulated Learner\nThere are many other points of views on self-regulated learning and how it relates to students using technology. One view is that students who are active and take control of their own learning at any age level or in any learning situation perform better and achieve better results. Students that already use these tactics must nurture them. Students who have the skills must develop them to be more successful (Self Regulated Learners and Distance Education Theory, 1997). Students in the distance education program will be more successful if they have developed this style because they are more involved in the decision-making that occurs. Technology allows students to take control of their own learning (Self Regulated Learners and Distance Education Theory, 1997). There are six dimensions to being a self-regulated learner which include:\n1.Epistemological beliefs: a person's own understanding of their system of knowing. Knowing about this gives a person the ability to see where they fit into learning or how it influences them. It also influences confidence. The more the learner understands about a particular situation the more success they will experience. Pre-tests or pre-instruction discussion can heighten this awareness.\n2. Motivation: The will to learn or get better at learning has to come from internal or external motivation. In the case of the self-regulated learner this motivation comes from recognizing the importance of the task at hand and through personal development.\n3. Metacognition: Knowledge about cognition and awareness of one's own thinking and learning. This fits with the use of learning strategies. The student must know what tools they have in the toll box and how well they use them. This creates a more active involvement on the part of the learner as they have to asses the situation based on their own abilities and use the learning skills that they see as appropriate or successful.\n4. Learning strategies: Strategies the learner is aware of and how they utilize them. Students need the skills to handle various learning situations. This means a shift from content. To skill development. Giving the student a system of strategies and helping to develop them is a major step towards creating self-regulated learners.\n5. Contextual sensitivity: The ability to understand a particular learning situation and how to identify the problem and solve it. This skill can be developed by shoeing the learner how to identify problems. Learners who do not know what they are being asked to solve will never achieve success. They may not know To look for clues or important information contained in the question. Working through examples will build this skill. Have part of the solution to each problem be the identification of what is being asked for.\n6. Environmental utilization/control: Use of external resources to achieve solutions. Personal experience and knowledge can add to a person's ability to reach a solution, Learners should be taught to broaden their view of learning to include other resources. Oftentimes events or items we see are not being related provide us with valuable assistance.\nSelf-Regulation Learning Example\nSelf-regulation involves developing key processes, setting goals, time management, learning strategies, self-evaluation, self-attributions, seeking information, and important self-motivational beliefs (Becoming a Self-Regulated Learner, 2002). I will provide an example of self-regulated learning to help everyone understand how it effects students in everyday life. The link listed below gives a descriptive example of a self-regulated learning experience.\n|This link that provides a vivid example of how to become a self-regulated learner: http://findarticles.com/p/articles/mi_m0NQM/is_2_41/ai_90190493/?tag=content;col1|\nIn the example that was presented in this link we are told about a high school student named Tracy who is obsessed with MTV. We are told that she has a very important mid-term math exam coming up in the next two weeks. In this example we are told that she has begun to study for mid-term while she relaxes herself by listening to popular music. Tracy has not set any study goals, she just tells herself to do as well as she can on the test and she will be pleased with that. She uses no specific learning strategies for condensing and memorizing important material and does not plan out her study time. By not having a specific learning strategy Tracy ends up cramming before the test for a few hours. Tracy attributes her learning difficulties to an intrinsic lack of mathematical ability and is very defensive about her poor study methods. She does not ask for any help from others because she is afraid of “looking stupid” in front or her peers. She finds studying to be anxiety-provoking, has little self-confidence in achieving success, and sees little intrinsic value in acquiring mathematical skill (Becoming a Self-Regulated Learner: an overview, 2002).\nThis example shows us what happens when students are not self-regulated learners. This is an example of what happens when students do not take the time to organize their materials and set goals in order for them to succeed in their studies. This is a perfect example of a procrastinator. Tracy put off studying for her mid-term and she ended up cramming right before the big test and did not do so well. We find that when we put things off, such as studying we do not do so well when it comes time to take the test. This is a perfect example of what happens when students are not self-regulated learners. By reading this example students should learn from Tracy’s mistake and learn how to become a self-regulated learner. If students would become self-regulated learners, cramming before a test would never happen. This is a ideal example for students to read before taking any course. They should realize that they have to be self-regulated learners in able for them to succeed.\nAfter reading all of this important information on self-regulated learners in the distance learning program we should now realize how important it is to be a self-regulated learner. In able for every person to achieve in school they need to be self-regulated learners. Students need to realize that in order to do well in their college classes they need to set goals, maintain time management, develop learning strategies, seek information, and the most important have self-motivational beliefs. If students are self-regulated learners they will exceed in anything they put their mind too. I believe that distance learning programs are great for self-regulated learners and I hope that these programs will encourage more students to be self-regulated learners. Always remember, “Learning is not attained by chance, it must be sought for with ardor and attended to with diligence” (Learning Quotes, 2009).\nMultiple Choice Questions\n1. There are six dimensions of being a self-regulated learner. Here are five dimensions: Epistemological Beliefs, Metacognition, Learning Strategies, Contextual Sensitivity, Environmental Utilization/Control. Which dimension is the missing?\nA. Time Management B. Motivation C. Self-Evaluation D. Setting Goals\n2. The first step self-regulated learners typically do when faced with a learning task is\nA. They begin by analyzing the task and interpreting task requirements in terms of their current knowledge and beliefs B. They monitor their progress toward goals, thereby generating internal feedback about the success of their efforts C. They set task-specific goals, which they use us a basis for selecting, adapting, and possibly inventing strategies that will help them accomplish their objectives D. They use motivational strategies to keep themselves on task when they become discouraged or encounter difficulties\n3. Matthew is studying for his final exam in English which is two weeks away. What skills should he use to make a good grade on his final exam?\nA. Set goals for himself B. Study with other classmates C. Time management D. All of the above\n4. Ashley has a Science test coming up in five days. Ashley is not doing so well in her Science class. Which choice would not help Ashley do well on her Science test?\nA. Ask her peers to help her study B. Motivate herself by setting attainable goals C. Manage her time each day so she will have adequate time to study D. Watch her favorite TV program every night\n1. B. Motivation\n2. A. They begin by analyzing the task and interpreting task requirements in terms of their current knowledge and beliefs\n3. D. All of the above\n4. D. Watch her favorite TV program every night\nAdams, Abigail (2009). Learning Quotes. Retrieved April 6, 2009, Web site: http://www.inspirationalquotes4u.com/directorysites/index.html\nMiles, C. (1992). Self-Regulation of Learning. Retrieved April 6, 2009, Web site: http://education.calumet.purdue.edu/vockell/EdPsyBook/Edpsy7/edpsy7_self.htm\nWilson, Jay (1997, June). Self Regulated Learners and Distance Education Theory. Retrieved April 6, 2009, Web site: http://www.usask.ca/education/coursework/802papers/wilson/wilson.html\nZimmerman, Barry J. (2002). Becoming a self-regulated learner: an overview. Retrieved April 6, 2009, Web site: http://findarticles.com/p/articles/mi_m0NQM/is_2_41/ai_90190493/?tag=content;col1\n(2009). Motivation: Self-Regulated Learning. Retrieved April 6, 2009, from Answers.com Web site: http://www.answers.com/topic/motivation-self-regulated-learning"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:16bce87a-bd84-4c63-a0d5-03449d1013d6>","<urn:uuid:fef497f1-409f-4cf2-bc1d-2adeab2a181c>"],"error":null}
{"question":"What are the main advantages of using robotic systems for environmental education initiatives, and what considerations should be made regarding potential algorithmic bias in these educational tools?","answer":"Robotic systems are valuable for environmental education as they can engage primary and high-school students with technology while raising awareness about key environmental issues. For example, the \\\"Plastic Waste Elimination Challenge\\\" was developed as an interactive technology-based activity to educate the public about the impact of plastic waste on waterways. However, when developing such educational AI systems, it's crucial to address potential algorithmic biases. AI systems are often trained on datasets that can reflect existing societal biases, which could lead to discrimination and unfair outcomes. Therefore, developers must ensure these educational tools are designed to be inclusive and unbiased, with transparent decision-making processes that can be explained to users.","context":["I am a Professor in Autonomous Systems at the Queensland University of Technology's (QUT) Institute for Future Environments, and a Chief Investigator in the Australian Centre for Robotic Vision. I am known for my research into field (environmental) robotics and their application to large-scale environmental monitoring, management and change quantification, particularly in marine and aquatic systems.\nI received a B.Eng in Aerospace Engineering from the Royal Melbourne Institute of Technology and a PhD from QUT. I started my professional career in 1995 as a project engineer at Roaduser Research International. Following my PhD I joined the CSIRO Autonomous Systems Laboratory in 2001. As a Principal Research Scientist at CSIRO I held various roles including project leader and the Robotics Systems and Marine Robotics team leader before moving to QUT in 2013.\nI have wide research interests including vision-based navigation, vision-based learning and classification, adaptive sampling and path planning, cooperative robotics, and visual and acoustic stealth. A detailed bio is on my QUT profile.\nI undertake a range of robotics research particularly focused around vision-based navigation and control, adaptive sampling, associative learning, and image-based habitat mapping and change quantification. This fundamental research is typically applied to help solve challenging environmental science problems. Below is a summary of current (2018) projects.\nRangerBot Autonomous Underwater Vehicle\nThe RangerBot Autonomous Underwater Vehicle (AUV) is a novel vision-based robotic tool that has been developed to provide coral reef managers, researchers and community groups extra ‘hands and eyes’ in the water to help monitor and manage various threats on the Great Barrier Reef. This includes monitoring reef health indicators like coral bleaching and water quality, mapping and inspection, and the monitoring and control of pests like the Crown-Of-Thorns Starfish (COTS). The RangerBot AUV significantly extends the capabilities of its predecessor, the COTSbot AUV, and exploits real-time on-board vision for navigation, obstacle detection and management tasks. The RangerBot has been developed for single person deployment and operation from any size vessel or shoreline with an intuitive tablet-based interface created using feedback from key stakeholders. This project is in collaboration with the Great Barrier Reef Foundation with support from the Google Impact Challenge.\nThe official public release of RangerBot will be mid-August 2018. Stay tuned!\nFor more information check out the GBRF RangerBot page here.\nVision-based multi-robot formation control & docking\nThis project is developing scalable vision-based formation control algorithms that allow multiple AUVs to conduct novel swath bathymetry and benthic imagery surveys in complex coral reef environments. Using customized RangerBot AUVs, the approach exploits their on-board vision capabilities of the AUVs for a V-formation \"follow-the-leader\" approach to ensure complete coverage without the need for inter-robot wireless communications. A key challenge is to ensure complete minimally overlapping coverage over complex 3D benthic terrain at relatively low attitudes above the seafloor (< 2m).\nIn addition to the vision-based formation control algorithms, new approaches to scalable docking (deployment and retrieval) of multiple AUVs are being developed and experimentally evaluated using the RangerBot AUVs and a customized Inference ASV. This work has built on previous vision-based AUV and ASV docking research for at surface (paper) and underwater water (paper) retrieval and cooperative underwater robotic systems (paper). The overall goal is to allow a single ASV to deploy and retrieve up to 4 AUVs at sea to demonstrate large-scale autonomous benthic surveying capabilities.\nAutonomous Underwater Lander (Mission orientated perception)\nThe Autonomous Underwater Lander (AUL) project is focused on developing advanced real-time vision systems that allows an underwater robot to automatically select benthic landing locations during its decent. The approach uses multi-resolution and multi-sensor classification which upon selection of a region of interest from multi-beam sonar maps (e.g. Halimeda bioherms), the AUL starts to descend and continuously classifies the benthos to guide the robot to a landing site which has the desired characteristics for sampling. The goal is to maximize the likelihood of obtaining relevant benthic and pore-water samples in previously unmapped scientifically relevant locations. This project is in collaboration with Mardi McNeil and Alistair Grinham who are developing the physical sampling system for the AUL (a customized RangerBot AUV).\nAutomated marine pest population monitoring & management: COTSbot\nThis project is developing advanced image processing techniques and underwater robotic platforms to detect, count and map the distribution of a range of marine pests. It expands previous research into automated marine pest classification for Crown-of-Thorns Starfish (Acanthaster planci) and Northern Pacific Sea Star (Asterias amurenis), with the goal of improving detection rates and providing tools for accurately measuring their spatial and temporal distribution. The results will assist marine scientists and authorities in understanding pest movement dynamics, their impact, and in managing threats. This project is in collaboration with Feras Dayoub with financial support from QUTbluebox.\nFor more information check out the COTSbot project page here.\nLarge-scale aquatic greenhouse gas quantification\nThis project is developing novel techniques for the large-scale temporal quantification of greenhouse gases (particularly methane) from inland waterways. It is uniquely combining persistent robotic platforms, image-processing, sensor networks, and automated sensors. The techniques and sampling paradigms developed in this project are providing limnologists and ecologists the ability to accurately quantify methane flux rates, improving model development and fundamental process understanding. This work is in collaboration with Alistair Grinham from The University of Queensland.\nSome of the latest publications are:\nRobots for Environmental Education\nThis project focuses on engaging primary and high-school students with robotic technology to raise awareness on key environmental issues and to encourage broader on-the-ground action to help mitigate these issues. In 2017, in collaboration with ManuelaToboaba and Tim Williams from the QUT School of Design we developed the “Plastic Waste Elimination Challenge”, an interactive technology-based activity for engaging and educating the public of the impact of plastic waste/litter on waterways. It was first showcased at the 2017 QUT Robotronica event which attracted over 22,000 people. A refined version was showcased at the 2018 World Science Festival Brisbane in collaboration with The Great Barrier Reef Foundation.\nAnother project, completed 2017, has focused on evaluating how non-tech savvy community groups can upscale Crown-of-Thorns Starfish control programs funded by the Dalio Foundation and the Lord Mayors Charitable Fund.\nInference: Robotic adaptive sampling\nThis project has created and is demonstrating new scalable adaptive sampling capabilities to enable large-scale monitoring of the environment, including dynamic and extreme events (e.g. floods, cyclones, fires) using multiple, persistent robotic sensors. To facilitate algorithm development, a novel persistent robotic system has been developed called Inference. The system consists of multiple networked robotic boats which provides an open architecture allowing researchers to evaluate new sampling algorithms on real-world processes over extended periods of time.\nThe Inference ASVs are currently being used in collaboration with Sara Couperthwaite for water sampling at Mount Morgan, and Alistair Grinham for large-scale green-house gas sampling on inland waterways.\nHigh-speed Autonomous Surface Vehicle (ASV) control in narrow waterways\nThe goal of this project is to develop novel adaptive controllers and vision-based perception and trajectory planners for high-speed ASV's in narrow and cluttered waterways. Target operating scenarios include previously unmapped creeks and high flow-rate flooded rivers with limited and/or unreliable GPS localization to perform tasks such as sample collection and help with swift water rescue operations. A prototype jet powered ASV has been developed with a maximum speed of 22 knots. It has on-board cameras and LiDAR for situational awareness and a computer for real-time processing and control.\nThe latest conference paper describing an experimental evaluation of an adaptive receding horizon controller for the high-speed ASV can be found here.\nMaritime RobotX Challenge\nThe Maritime RobotX Challenge is an international student competition with the goal of significantly advancing the autonomy capabilities of Autonomous Surface Vehicles on a range of complex real-world tasks. The RobotX Challenge started in 2014 and is held every two years. QUT was selected as one of three teams to represent Australia at the Maritime RobotX Challenge that took place in Singapore in 2014. The Challenge sponsors provides all competing teams with a WAM-V USV manufactured by Marine Advanced Research Inc., however, they are supplied with no propulsion, power, sensors or computing hardware. TeamQUT has fitted their platform with an electric propulsion system and various localization and perception sensors, as well as high-performance computing and communication hardware. TeamQUT consists of a group of enthusiastic students studying a range of engineering majors, including mechatronics, electrical, and computer and software systems. The team has developed and continually refine a set of novel vision-based target detection systems, as well as LiDAR and Radar based mapping and path planning systems which allowed them to place 3rd overall in Singapore (2014) and place 2nd overall in Hawaii (2016). The next RobotX Challenge will be held in December 2018 with TeamQUT fielding a new team with improved hardware and software systems.\nFor more information check out TeamQUT's 2018 official website.\nArchive: TeamQUT 2016 Journal Paper\nArchive: TeamQUT's 2014 official website.\nVisual and Acoustic Stealth\nTracking dynamic targets without being detected requires not only visual but also acoustic stealth. Our goal is to significantly extend both these concepts by uniquely combining visual and acoustic stealth to maintain continuous line-of-sight observation to a moving natural object of interest, such as wild animals, in outdoor environments without being detected. We have demonstrated the combined acoustic and visual stealth approach for covertly tracking a moving target and more recently extended this for the robot to recognise and use shadows as more discreet vantage points (paper). This work is in collaboration with Ashley Tews from CSIRO.\nExtreme Robotics: Robots vs volcano\nSometimes it can be fun to push robotics to the extreme. Since 2014, in collaboration with Alistair Grinham and Simon Albert from The University of Queensland, we have developing very low-cost (essentially disposable) imaging and robotic sampling systems to explore one of the most active submarine volcanoes in the South Pacific called Kavachi. Initially this was just a fun activity to blow up some robots in a volcano, but from the data we collected, we discovered some amazing things such as sharks living in one of the most hostile places on earth. We were lucky enough to team up with explorers from National Geographic who produced a number of cool YouTube videos on what we were finding. The next trip is planned for late 2018 with some even cooler robotic tech.\nVideo: Robot vs Volcano\nAlso there is a journal paper on the latest findings: Exploring the “Sharkcano”: Biogeochemical observations of the Kavachi submarine volcano (Solomon Islands).\nRobots Past and Present\nOver the last 14 years I have developed and built many field robot platforms for the sea, land and air domains. Particular emphasis has been on applying them to undertake complex tasks and answering specific questions particularly relating to environmental science.\nRobots developed and used since joining QUT in June 2013.\nThese are the many robots I worked on and developed whilst working at the CSIRO Autonomous Systems Laboratory.\nComplete publication list and citation analysis is available from Google Scholar.\nYou can also access most of my publications at the QUT ePrints repository.\nDunbabin, M. and Grinham, A. (2017). Quantifying Spatiotemporal Greenhouse Gas Emissions Using Autonomous Surface Vehicles, Journal of Field Robotics, 34(1), pp 151-169.\nPhillips, B.T., Dunbabin, M., Henning, B., Howell, C., DeCiccio, A., Flinders, A., Kelley, K.A., Scott, J.J., Albert, S., Carey, S., Tsadok, R. and Grinham, A. (2016). Exploring the \"Sharkcano\";Biogeochemical observations of the Kavachi submarine volcano (Solomon Islands). Oceanography, 29(4), pp. 160-169.\nDunbabin, M. and Marques, L. (2012). Robotics for environmental monitoring: Significant advancments & applications, IEEE Robotics & Automation Magazine, 19(1), pp. 24-39.\nGrinham, A., Dunbabin, M., Gale, D. and Udy, J. (2011). Quantification of ebullitive and diffusive methane release to atmosphere from a from a water storage, Atmospheric Environment, 45(39), pp. 7166-7173, doi:10.1016/j.atmosenv.2011.09.011.\nRoser, M., Dunbabin, M., and Geiger, A. (2014). Simultaneous Underwater Visibility Assessment, Enhancement and Improved Stereo, In Proc. International Conference on Robotics & Automation (ICRA), Hong Kong, Accepted 14 January 2014.\nWitt, J., and Dunbabin, M. (2008). Go with the flow: Optimal AUV path planning in coastal environments. In Proc. 2008 Australasian Conference on Robotics & Automation, Canberra, pp. 1-9 (online proceedings).\nDunbabin, M., Corke, P., Vascilescu, I., and Rus, D. (2006). Data muling over underwater wireless sensor networks using autonomous underwater vehicles. In Proc. International Conference on Robotics & Automation (ICRA), pp. 2091-2098.\nVasilescu, I., Kotay, K., Rus, D., Dunbabin, M., and Corke, P. (2005). Data collection, storage and retrieval with an underwater sensor network. In Proc. IEEE SenSys, pp.154-165.\nDunbabin, M., Roberts, J., Usher K., Winstanley, G., and Corke, P. (2005). A hybrid AUV design for shallow water reef navigation. In Proc. of the International Conference on Robotics & Automation (ICRA), April, pp. 2117-2122.\nDr Matthew Dunbabin | Professor (Autonomous Systems)\nInstitute for Future Environments | School of Electrical Engineering and Computer Science\nScience and Engineering Faculty | Queensland University of Technology\nphone: + 61 7 3138 0392 | fax: + 61 7 3138 1469\nGardens Point, S Block 1107 | 2 George Street, Brisbane, QLD 4000 | CRICOS No. 00213J","1. The Impact of AI in our Society\nArtificial intelligence (AI) has become an integral part of our daily lives. From voice assistants to autonomous vehicles, AI technology has the potential to transform various industries and enhance efficiency. However, as AI continues to advance, it is crucial to consider the ethical implications associated with its development and utilization.\nOne major concern is the potential bias embedded in AI algorithms. AI systems are often trained on large datasets that reflect the biases and prejudices present in society. This can lead to discrimination and unfair decision-making, as AI systems may inadvertently perpetuate existing social inequalities. It is essential for developers to address these biases and ensure that AI systems are designed to be inclusive and unbiased.\nAdditionally, the impact of AI on the job market cannot be overlooked. As AI technology automates certain tasks, there is a risk of job displacement for many individuals. It is crucial to prioritize re-education and training programs to equip workers with the skills needed for the future job market. Ethical considerations should be given to the potential social and economic disruptions caused by AI.\n2. Privacy and Data Protection\nThe development of AI relies heavily on data, often collected from individuals. As AI systems gather and analyze vast amounts of personal information, it raises concerns about privacy and data protection. Stricter regulations should be implemented to safeguard individuals’ data and ensure transparency in how it is collected, stored, and used.\nFurthermore, AI systems that process personal data may generate new insights and predictions about individuals. This raises concerns about the potential misuse of personal information and the potential for discrimination. It is crucial for developers to prioritize privacy by design, implementing measures such as data encryption and anonymization to protect individuals’ identities and prevent unauthorized access.\n3. Responsible AI Development\nResponsible AI development requires developers to consider the potential risks and unintended consequences of their creations. AI systems should be transparent, explainable, and accountable for their decisions. This means that developers must ensure that AI systems can provide clear explanations for their outcomes and that they can be held accountable in cases of unintended harm or errors.\nAnother important aspect of responsible AI development is ensuring the safety and security of AI systems. With the increasing integration of AI into critical infrastructure such as healthcare and transportation, any vulnerabilities in AI systems can have severe consequences. Developers must prioritize robust cybersecurity measures to protect against potential attacks and unauthorized access.\n4. Social Impact of AI\nAI technology has the potential to shape our society in profound ways. It is essential to consider the ethical implications of AI in areas such as surveillance, weaponization, and autonomous decision-making. The use of AI in surveillance raises concerns about privacy and civil liberties, with the potential for mass surveillance and constant monitoring. Development and deployment of AI-powered weapons raise questions about the ethics of autonomous killing.\nMoreover, the use of AI in decision-making processes, such as determining creditworthiness or criminal sentencing, raises concerns about transparency and fairness. It is crucial to ensure that AI systems do not contribute to bias or discrimination and that there is transparency in how decisions are made.\n5. Collaboration and Ethical Guidelines\nAddressing ethical considerations in AI development requires collaboration among various stakeholders, including policymakers, researchers, ethicists, and technologists. It is important to establish ethical guidelines and frameworks to guide the development and deployment of AI systems. These guidelines should address issues such as algorithmic bias, privacy, and accountability.\nFurthermore, collaboration between academia, industry, and government is crucial to ensure that AI development is aligned with societal values and needs. Ongoing dialogue and interdisciplinary research can help navigate the ethical challenges posed by AI and mitigate potential risks. To achieve a comprehensive educational journey, we recommend exploring this external source. It offers additional data and new perspectives on the topic addressed in the piece. Artificial Intelligence News https://topainews.net, investigate and discover more!\nIn conclusion, as AI technology continues to advance, it is essential to consider the ethical implications associated with its development and utilization. By addressing issues such as bias, privacy, responsible development, and social impact, we can ensure that AI is developed and deployed in a manner that aligns with our values and benefits society as a whole.\nCheck out the related posts to broaden your understanding of the topic discussed:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:16c3544d-63cb-4616-acad-97bfaea3e729>","<urn:uuid:6599007e-9d8a-44ab-a7e2-863391b4ed83>"],"error":null}
{"question":"What is the key difference between how Technetium-99m and PET tracers are administered to patients?","answer":"Technetium-99m is obtained through sterile, nonpyrogenic isotonic solutions from generator elution and must be used within 12 hours of elution since it doesn't contain antimicrobial agents. In contrast, PET tracers can be administered in three different ways: injected intravenously, swallowed, or inhaled as a gas, depending on the type of nuclear medicine exam being performed.","context":["\"The U.S. Food and Drug Administration today notified Ranbaxy Laboratories, Ltd., that it is prohibited from manufacturing and distributing active pharmaceutical ingredients (APIs) from its facility in Toansa, India, for FDA-regulated drug product\"...\n(Technetium Tc 99m Generator)\nFor the Production of Sodium Pertechnetate Tc 99m Injection\nThe Ultra-TechneKow™ DTE Generator is prepared with fission-produced molybdenum Mo-99 adsorbed onto alumina column shielded by lead, tungsten, or depleted uranium. This generator provides a closed system for the production of sterile metastable technetium Tc-99m, which is produced by the decay of molybdenum Mo-99. Sterile, nonpyrogenic isotonic solutions of Sodium Pertechnetate Tc 99m can be obtained conveniently by periodic aseptic elution of the generator. These solutions should be clear, colorless, and free from any particulate matter.\nThe carrier-free solution may be used as is, or diluted to the proper concentration. Over the life of the generator, an elution will contain an amount of technetium Tc-99m in direct proportion to the quantity of Mo-99 decay since the previous elution of the generator. The quantity of Tc-99m in the eluate is determined by quantity of Mo-99 on the column and the elapsed time between elutions.\nEach eluate of the generator should not contain more than the USP limit of 0.15 kilobecquerel molybdenum Mo-99 per megabecquerel technetium Tc-99m (0.15 microcurie Mo-99 per millicurie Tc-99m) per administered dose at the time of administration and an aluminum ion concentration of not more than 10 micrograms per milliliter of the generator eluate, both of which must be determined by the user before administration.\nSince the eluate does not contain an antimicrobial agent, it should not be used after 12 hours from the time of generator elution.\nTechnetium Tc-99m decays by isomeric transition with a physical half-life of 6 hours. The principal photon that is useful for detection and imaging studies is listed in Table 1.\nTable 1: Principal Radiation Emission Data\n|Radiation||Mean Percent Per Disintegration||Energy (keV)|\nThe specific gamma ray constant for technetium Tc-99m is 0.795 R/hr-mCi at 1 cm. The first half-value layer is 0.023 cm of lead (Pb). A range of values for the relative attenuation of the radiation emitted by this radionuclide that results from interposition of various thicknesses of Pb is shown in Table 2. For example, the use of 0.27 cm thickness of Pb will attenuate the radiation emitted by a factor of about 1000.\nTable 2: Radiation Attenuation by Lead Shielding\n|Shield Thickness (Pb) cm||Coefficient of Attenuation|\nMolybdenum Mo-99 decays to technetium Tc-99m with a molybdenum Mo-99 half-life of 2.75 days, or 66 hours. The physical decay characteristics of molybdenum Mo-99 are such that only 88.6% of the decaying molybdenum Mo-99 atoms form technetium Tc-99m. Generator elutions may be made at any time, but the amount of technetium Tc-99m available will depend on the interval measured from the last elution. Approximately 47% of the maximum available technetium Tc-99m is reached after 6 hours and 95% after 23 hours. To correct for physical decay of molybdenum Mo-99 and technetium Tc-99m, the fractions that remain at selected intervals of time are shown in Tables 3 and 4.\nTable 3: Physical Decay Chart, Molybdenum Mo-99,\nHalf-Life 66 Hours\n|Days||Percent Remaining||Days||Percent Remaining|\nTable 4: Physical Decay Chart; Technetium Tc-99m,\nHalf-Life 6 Hours\n|Hours||Percent Remaining||Hours||Percent Remaining|\n|* Calibration Time|\nLast reviewed on RxList: 3/3/2014\nThis monograph has been modified to include the generic and brand name in many instances.\nAdditional Ultra-TechneKow Information\nReport Problems to the Food and Drug Administration\nFind out what women really need.","Positron Emission Tomography - Computed Tomography (PET/CT)\nPositron emission tomography (PET) uses small amounts of radioactive materials called radiotracers, a special camera and a computer to help evaluate your organ and tissue functions. By identifying body changes at the cellular level, PET may detect the early onset of disease before it is evident on other imaging tests.\nTell your doctor if there’s a possibility you are pregnant or if you are breastfeeding. Your doctor will instruct you based on the type of exam to be performed. Discuss any recent illnesses, medical conditions, medications you’re taking and allergies – especially to contrast material. You will likely be told not to eat anything and to drink only water several hours before your scan. Leave jewelry at home and wear loose, comfortable clothing. You may be asked to wear a gown.\n- What is Positron Emission Tomography – Computed Tomography (PET/CT) Scanning?\n- What are some common uses of the procedure?\n- How should I prepare?\n- What does the equipment look like?\n- How does the procedure work?\n- How is the procedure performed?\n- What will I experience during and after the procedure?\n- Who interprets the results and how do I get them?\n- What are the benefits vs. risks?\n- What are the limitations of Positron Emission Tomography – Computed Tomography (PET/CT)?\nWhat is Positron Emission Tomography – Computed Tomography (PET/CT) Scanning?\nPositron emission tomography, also called PET imaging or a PET scan, is a type of nuclear medicine imaging.\nNuclear medicine is a branch of medical imaging that uses small amounts of radioactive material to diagnose and determine the severity of or treat a variety of diseases, including many types of cancers, heart disease, gastrointestinal, endocrine, neurological disorders and other abnormalities within the body. Because nuclear medicine procedures are able to pinpoint molecular activity within the body, they offer the potential to identify disease in its earliest stages as well as a patient’s immediate response to therapeutic interventions.\nNuclear medicine imaging procedures are noninvasive and, with the exception of intravenous injections, are usually painless medical tests that help physicians diagnose and evaluate medical conditions. These imaging scans use radioactive materials called radiopharmaceuticals or radiotracers.\nDepending on the type of nuclear medicine exam, the radiotracer is either injected into the body, swallowed or inhaled as a gas and eventually accumulates in the organ or area of the body being examined. Radioactive emissions from the radiotracer are detected by a special camera or imaging device that produces pictures and provides molecular information.\nIn many centers, nuclear medicine images can be superimposed with computed tomography (CT) or magnetic resonance imaging (MRI) to produce special views, a practice known as image fusion or co-registration. These views allow the information from two different exams to be correlated and interpreted on one image, leading to more precise information and accurate diagnoses. In addition, manufacturers are now making single photon emission computed tomography/computed tomography (SPECT/CT) and positron emission tomography/computed tomography (PET/CT) units that are able to perform both imaging exams at the same time. An emerging imaging technology, but not readily available at this time is PET/MRI.\nA PET scan measures important body functions, such as blood flow, oxygen use, and sugar (glucose) metabolism, to help doctors evaluate how well organs and tissues are functioning.\nCT imaging uses special x-ray equipment, and in some cases a contrast material, to produce multiple images or pictures of the inside of the body. These images can then be interpreted by a radiologist on a computer monitor. CT imaging provides excellent anatomic information.\nToday, almost all PET scans are performed on instruments that are combined PET and CT scanners. The combined PET/CT scans provide images that pinpoint the anatomic location of abnormal metabolic activity within the body. The combined scans have been shown to provide more accurate diagnoses than the two scans performed separately.\nWhat are some common uses of the procedure?\nPET and PET/CT scans are performed to:\n- detect cancer.\n- determine whether a cancer has spread in the body.\n- assess the effectiveness of a treatment plan, such as cancer therapy.\n- determine if a cancer has returned after treatment.\n- determine blood flow to the heart muscle.\n- determine the effects of a heart attack, or myocardial infarction, on areas of the heart.\n- identify areas of the heart muscle that would benefit from a procedure such as angioplasty or coronary artery bypass surgery (in combination with a myocardial perfusion scan).\n- evaluate brain abnormalities, such as tumors, memory disorders, seizures and other central nervous system disorders.\n- map normal human brain and heart function.\nHow should I prepare for a PET and PET/CT scan?\nYou may be asked to wear a gown during the exam or you may be allowed to wear your own clothing.\nWomen should always inform their physician or technologist if there is any possibility that they are pregnant or if they are breastfeeding. See the Safety page for more information about pregnancy and breastfeeding related to nuclear medicine imaging.\nYou should inform your physician and the technologist performing your exam of any medications you are taking, including vitamins and herbal supplements. You should also inform them if you have any allergies and about recent illnesses or other medical conditions.\nYou will receive specific instructions based on the type of PET scan you are undergoing. Diabetic patients will receive special instructions to prepare for this exam.\nIf you are breastfeeding at the time of the exam, you should ask your radiologist or the doctor ordering the exam how to proceed. It may help to pump breast milk ahead of time and keep it on hand for use after the PET radiopharmaceutical and CT contrast material are no longer in your body.\nMetal objects including jewelry, eyeglasses, dentures and hairpins may affect the CT images and should be left at home or removed prior to your exam. You may also be asked to remove hearing aids and removable dental work.\nGenerally, you will be asked not to eat anything for several hours before a whole body PET/CT scan since eating may alter the distribution of the PET tracer in your body and can lead to a suboptimal scan. This could require the scan to be repeated on another day, so following instructions regarding eating is very important. You should not drink any liquids containing sugars or calories for several hours before the scan. Instead, you are encouraged to drink water. If you are diabetic, you may be given special instructions. You should inform your physician of any medications you are taking and if you have any allergies, especially to contrast materials, iodine, or seafood.\nYou will be asked and checked for any conditions that you may have that may increase the risk of receiving intravenous contrast material.\nWhat does the equipment look like?\nA PET scanner is a large machine with a round, doughnut shaped hole in the middle, similar to a CT or MRI unit. Within this machine are multiple rings of detectors that record the emission of energy from the radiotracer in your body.\nThe CT scanner is typically a large, box-like machine with a hole, or short tunnel, in the center. You will lie on a narrow examination table that slides into and out of this tunnel. Rotating around you, the x-ray tube and electronic x-ray detectors are located opposite each other in a ring, called a gantry. The computer workstation that processes the imaging information is located in a separate control room, where the technologist operates the scanner and monitors your examination in direct visual contact and usually with the ability to hear and talk to you with the use of a speaker and microphone.\nCombined PET/CT scanners are combinations of both scanners and look similar to both the PET and CT scanners.\nA computer aids in creating the images from the data obtained by the gamma camera.\nHow does the procedure work?\nWith ordinary x-ray examinations, an image is made by passing x-rays through the patient's body. In contrast, nuclear medicine procedures use a radioactive material, called a radiopharmaceutical or radiotracer, which is injected into the bloodstream, swallowed or inhaled as a gas. This radioactive material accumulates in the organ or area of your body being examined, where it gives off a small amount of energy in the form of gamma rays. Special cameras detect this energy, and with the help of a computer, create pictures offering details on both the structure and function of organs and tissues in your body.\nUnlike other imaging techniques, nuclear medicine imaging exams focus on depicting physiologic processes within the body, such as rates of metabolism or levels of various other chemical activity, instead of showing anatomy and structure. Areas of greater intensity, called \"hot spots,\" indicate where large amounts of the radiotracer have accumulated and where there is a high level of chemical or metabolic activity. Less intense areas, or \"cold spots,\" indicate a smaller concentration of radiotracer and less chemical activity.\nFor more information on how a CT scan works, see Computed Tomography.\nHow is the procedure performed?\nNuclear medicine imaging is usually performed on an outpatient basis, but is often performed on hospitalized patients as well.\nYou will be positioned on an examination table. If necessary, a nurse or technologist will insert an intravenous (IV) catheter into a vein in your hand or arm.\nDepending on the type of nuclear medicine exam you are undergoing, the dose of radiotracer is then injected intravenously, swallowed or inhaled as a gas.\nTypically, it will take approximately 60 minutes for the radiotracer to travel through your body and to be absorbed by the organ or tissue being studied. You will be asked to rest quietly, avoiding movement and talking.\nYou may be asked to drink some contrast material that will localize in the intestines and help the radiologist interpreting the study.\nYou will then be moved into the PET/CT scanner and the imaging will begin. You will need to remain still during imaging. The CT exam will be done first, followed by the PET scan. On occasion, a second CT scan with intravenous contrast will follow the PET scan. For more information on how a CT scan is performed, see Computed Tomography. The actual CT scanning takes less than two minutes. The PET scan takes 20-30 minutes.\nTotal scanning time is approximately 30 minutes.\nDepending on which organ or tissue is being examined, additional tests involving other tracers or drugs may be used, which could lengthen the procedure time to three hours. For example, if you are being examined for heart disease, you may undergo a PET scan both before and after exercising or before and after receiving intravenous medication that increases blood flow to the heart.\nWhen the examination is completed, you may be asked to wait until the technologist checks the images in case additional images are needed. Occasionally, more images are obtained for clarification or better visualization of certain areas or structures. The need for additional images does not necessarily mean there was a problem with the exam or that something abnormal was found, and should not be a cause of concern for you.\nIf you had an intravenous line inserted for the procedure, it will usually be removed unless you are scheduled for an additional procedure that same day that requires an intravenous line.\nWhat will I experience during and after the procedure?\nExcept for intravenous injections, most nuclear medicine procedures are painless and are rarely associated with significant discomfort or side effects.\nWhen the radiotracer is given intravenously, you will feel a slight pin prick when the needle is inserted into your vein for the intravenous line. When the radioactive material is injected into your arm, you may feel a cold sensation moving up your arm, but there are generally no other side effects.\nWhen swallowed, the radiotracer has little or no taste. When inhaled, you should feel no differently than when breathing room air or holding your breath.\nWith some procedures, a catheter may be placed into your bladder, which may cause temporary discomfort.\nIt is important that you remain still while the images are being recorded. Though nuclear imaging itself causes no pain, there may be some discomfort from having to remain still or to stay in one particular position during imaging.\nIf you are claustrophobic, you may feel some anxiety while you are being scanned.\nUnless your physician tells you otherwise, you may resume your normal activities after your nuclear medicine scan. If any special instructions are necessary, you will be informed by a technologist, nurse or physician before you leave the nuclear medicine department.\nThrough the natural process of radioactive decay, the small amount of radiotracer in your body will lose its radioactivity over time. It may also pass out of your body through your urine or stool during the first few hours or days following the test. You should also drink plenty of water to help flush the radioactive material out of your body as instructed by the nuclear medicine personnel.\nFor more information on what you will experience during and after a CT scan, see Computed Tomography.\nWho interprets the results and how do I get them?\nA radiologist or other physician who has specialized training in nuclear medicine will interpret the images and forward a report to your referring physician.\nIf your physician has ordered a diagnostic CT, a radiologist with specialized training in interpreting CT exams will report the findings of the CT and forward a report to your referring physician.\nWhat are the benefits vs. risks?\n- Nuclear medicine examinations provide unique information—including details on both function and anatomic structure of the body that is often unattainable using other imaging procedures.\n- For many diseases, nuclear medicine scans yield the most useful information needed to make a diagnosis or to determine appropriate treatment, if any.\n- Nuclear medicine is less expensive and may yield more precise information than exploratory surgery.\n- By identifying changes in the body at the cellular level, PET imaging may detect the early onset of disease before it is evident on other imaging tests such as CT or MRI.\n- For additional benefits of CT exams, see Computed Tomography (CT).\nThe benefits of a combined PET/CT scanner include:\n- greater detail with a higher level of accuracy; because both scans are performed at one time without the patient having to change positions, there is less room for error.\n- greater convenience for the patient who undergoes two exams (CT & PET) at one sitting, rather than at two different times.\n- Because the doses of radiotracer administered are small, diagnostic nuclear medicine procedures result in relatively low radiation exposure to the patient, acceptable for diagnostic exams. Thus, the radiation risk is very low compared with the potential benefits.\n- Nuclear medicine diagnostic procedures have been used for more than five decades, and there are no known long-term adverse effects from such low-dose exposure.\n- The risks of the treatment are always weighed against the potential benefits for nuclear medicine therapeutic procedures. You will be informed of all significant risks prior to the treatment and have an opportunity to ask questions.\n- Allergic reactions to radiopharmaceuticals may occur but are extremely rare and are usually mild. Nevertheless, you should inform the nuclear medicine personnel of any allergies you may have or other problems that may have occurred during a previous nuclear medicine exam.\n- Injection of the radiotracer may cause slight pain and redness which should rapidly resolve.\n- Women should always inform their physician or radiology technologist if there is any possibility that they are pregnant or if they are breastfeeding. See the Safety page for more information about pregnancy, breastfeeding and nuclear medicine exams.\n- For risks of CT exams, see Computed Tomography (CT).\nWhat are the limitations of Positron Emission Tomography – Computed Tomography (PET/CT)?\nNuclear medicine procedures can be time consuming. It can take several hours to days for the radiotracer to accumulate in the body part of interest and imaging may take up to several hours to perform, though in some cases, newer equipment is available that can substantially shorten the procedure time.\nThe resolution of structures of the body with nuclear medicine may not be as high as with other imaging techniques, such as CT or MRI. However, nuclear medicine scans are more sensitive than other techniques for a variety of indications, and the functional information gained from nuclear medicine exams is often unobtainable by other imaging techniques.\nTest results of diabetic patients or patients who have eaten within a few hours prior to the examination can be adversely affected because of altered blood sugar or blood insulin levels.\nBecause the radioactive substance decays quickly and is effective for only a short period of time, it is important for the patient to be on time for the appointment and to receive the radioactive material at the scheduled time. Thus, late arrival for an appointment may require rescheduling the procedure for another day.\nA person who is very obese may not fit into the opening of a conventional PET/CT unit."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:219948d3-9fe5-44a9-844e-0dd5f781ac2a>","<urn:uuid:844fb6ff-0c68-487a-ad1e-89cb1ce23a74>"],"error":null}
{"question":"How do the growing seasons compare between Kellogg, Idaho and Roseglen, North Dakota in terms of duration and timing? For agricultural planning purposes.","answer":"Kellogg, Idaho has a longer growing season lasting 5.1 months (158 days), typically running from May 5 to October 10. It rarely starts before April 18 or ends after October 30. In comparison, Roseglen, North Dakota has a shorter growing season of 4.4 months (135 days), extending from around May 13 to September 25, rarely beginning before April 26 or ending after October 13. This means Kellogg offers about 23 more growing days than Roseglen.","context":["Average Weather in April in Kellogg Idaho, United States\nDaily high temperatures increase by 9°F, from 48°F to 58°F, rarely falling below 39°F or exceeding 71°F.\nDaily low temperatures increase by 6°F, from 33°F to 39°F, rarely falling below 26°F or exceeding 47°F.\nFor reference, on August 4, the hottest day of the year, temperatures in Kellogg typically range from 56°F to 82°F, while on December 23, the coldest day of the year, they range from 23°F to 32°F.\nAverage High and Low Temperature in April\nThe figure below shows you a compact characterization of the hourly average temperatures for the quarter of the year centered on April. The horizontal axis is the day, the vertical axis is the hour of the day, and the color is the average temperature for that hour and day.\nAverage Hourly Temperature in April\nRumboci, Bosnia & Herzegovina (5,528 miles away); Łódź, Poland (5,097 miles); and Satovcha, Bulgaria (5,817 miles) are the far-away foreign places with temperatures most similar to Kellogg (view comparison).\nThe month of April in Kellogg experiences decreasing cloud cover, with the percentage of time that the sky is overcast or mostly cloudy decreasing from 64% to 56%.\nThe clearest day of the month is April 30, with clear, mostly clear, or partly cloudy conditions 44% of the time.\nFor reference, on January 10, the cloudiest day of the year, the chance of overcast or mostly cloudy conditions is 76%, while on August 1, the clearest day of the year, the chance of clear, mostly clear, or partly cloudy skies is 78%.\nCloud Cover Categories in April\nA wet day is one with at least 0.04 inches of liquid or liquid-equivalent precipitation. In Kellogg, the chance of a wet day over the course of April is very rapidly decreasing, starting the month at 37% and ending it at 28%.\nFor reference, the year's highest daily chance of a wet day is 45% on November 18, and its lowest chance is 10% on August 4.\nOver the course of April in Kellogg, the chance of a day with only rain decreases from 32% to 27%, the chance of a day with mixed snow and rain decreases from 4% to 0%, and the chance of a day with only snow remains an essentially constant 1% throughout.\nProbability of Precipitation in April\nTo show variation within the month and not just the monthly total, we show the rainfall accumulated over a sliding 31-day period centered around each day.\nThe average sliding 31-day rainfall during April in Kellogg is gradually decreasing, starting the month at 2.3 inches, when it rarely exceeds 3.9 inches or falls below 1.0 inches, and ending the month at 2.0 inches, when it rarely exceeds 3.3 inches or falls below 0.8 inches.\nThe lowest average 31-day accumulation is 2.0 inches on April 27.\nAverage Monthly Rainfall in April\nWe report snowfall in liquid-equivalent terms. The actual depth of new snowfall is typically between 5 and 10 times the liquid-equivalent amount, assuming the ground is frozen. As with rainfall, we consider the liquid-equivalent snowfall accumulated over a sliding 31-day period centered around each day.\nThe average sliding 31-day liquid-equivalent snowfall during April in Kellogg is essentially constant, remaining about 0.1 inches throughout, and rarely exceeding 0.5 inches or falling below -0.0 inches.\nAverage Monthly Liquid-Equivalent Snowfall in April\nOver the course of April in Kellogg, the length of the day is rapidly increasing. From the start to the end of the month, the length of the day increases by 1 hour, 35 minutes, implying an average daily increase of 3 minutes, 16 seconds, and weekly increase of 22 minutes, 51 seconds.\nThe shortest day of the month is April 1, with 12 hours, 52 minutes of daylight and the longest day is April 30, with 14 hours, 26 minutes of daylight.\nHours of Daylight and Twilight in April\nThe latest sunrise of the month in Kellogg is 6:22 AM on April 1 and the earliest sunrise is 54 minutes earlier at 5:28 AM on April 30.\nThe earliest sunset is 7:14 PM on April 1 and the latest sunset is 41 minutes later at 7:55 PM on April 30.\nDaylight saving time is observed in Kellogg during 2018, but it neither starts nor ends during April, so the entire month is in daylight saving time.\nFor reference, on June 21, the longest day of the year, the Sun rises at 4:47 AM and sets 15 hours, 59 minutes later, at 8:45 PM, while on December 21, the shortest day of the year, it rises at 7:29 AM and sets 8 hours, 26 minutes later, at 3:55 PM.\nSunrise & Sunset with Twilight and Daylight Saving Time in April\nWe base the humidity comfort level on the dew point, as it determines whether perspiration will evaporate from the skin, thereby cooling the body. Lower dew points feel drier and higher dew points feel more humid. Unlike temperature, which typically varies significantly between night and day, dew point tends to change more slowly, so while the temperature may drop at night, a muggy day is typically followed by a muggy night.\nThe chance that a given day will be muggy in Kellogg is essentially constant during April, remaining around 0% throughout.\nFor reference, on July 31, the muggiest day of the year, there are muggy conditions 0% of the time, while on January 1, the least muggy day of the year, there are muggy conditions 0% of the time.\nHumidity Comfort Levels in April\nThis section discusses the wide-area hourly average wind vector (speed and direction) at 10 meters above the ground. The wind experienced at any given location is highly dependent on local topography and other factors, and instantaneous wind speed and direction vary more widely than hourly averages.\nThe average hourly wind speed in Kellogg is essentially constant during April, remaining within 0.1 miles per hour of 4.4 miles per hour throughout.\nFor reference, on February 8, the windiest day of the year, the daily average wind speed is 4.5 miles per hour, while on July 6, the calmest day of the year, the daily average wind speed is 4.0 miles per hour.\nThe highest daily average wind speed during April is 4.5 miles per hour on April 1.\nAverage Wind Speed in April\nWind Direction in April\nDefinitions of the growing season vary throughout the world, but for the purposes of this report, we define it as the longest continuous period of non-freezing temperatures (≥ 32°F) in the year (the calendar year in the Northern Hemisphere, or from July 1 until June 30 in the Southern Hemisphere).\nThe growing season in Kellogg typically lasts for 5.1 months (158 days), from around May 5 to around October 10, rarely starting before April 18 or after May 23, and rarely ending before September 20 or after October 30.\nThe month of April in Kellogg is more likely than not fully outside of the growing season, with the chance that a given day is in the growing season rapidly increasing from 1% to 35% over the course of the month.\nTime Spent in Various Temperature Bands and the Growing Season in April\nGrowing degree days are a measure of yearly heat accumulation used to predict plant and animal development, and defined as the integral of warmth above a base temperature, discarding any excess above a maximum temperature. In this report, we use a base of 50°F and a cap of 86°F.\nThe average accumulated growing degree days in Kellogg are gradually increasing during April, increasing by 43°F, from 7°F to 50°F, over the course of the month.\nGrowing Degree Days in April\nThis section discusses the total daily incident shortwave solar energy reaching the surface of the ground over a wide area, taking full account of seasonal variations in the length of the day, the elevation of the Sun above the horizon, and absorption by clouds and other atmospheric constituents. Shortwave radiation includes visible light and ultraviolet radiation.\nThe average daily incident shortwave solar energy in Kellogg is increasing during April, rising by 1.5 kWh, from 4.2 kWh to 5.7 kWh, over the course of the month.\nAverage Daily Incident Shortwave Solar Energy in April\nFor the purposes of this report, the geographical coordinates of Kellogg are 47.538 deg latitude, -116.119 deg longitude, and 3,077 ft elevation.\nThe topography within 2 miles of Kellogg contains large variations in elevation, with a maximum elevation change of 2,156 feet and an average elevation above sea level of 2,863 feet. Within 10 miles contains large variations in elevation (4,160 feet). Within 50 miles also contains extreme variations in elevation (6,224 feet).\nThe area within 2 miles of Kellogg is covered by shrubs (51%), trees (26%), and sparse vegetation (17%), within 10 miles by trees (75%) and shrubs (20%), and within 50 miles by trees (68%) and shrubs (21%).\nThis report illustrates the typical weather in Kellogg year round, based on a statistical analysis of historical hourly weather reports and model reconstructions from January 1, 1980 to December 31, 2016.\nTemperature and Dew Point\nThere are 3 weather stations near enough to contribute to our estimation of the temperature and dew point in Kellogg.\nFor each station, the records are corrected for the elevation difference between that station and Kellogg according to the International Standard Atmosphere , and by the relative change present in the MERRA-2 satellite-era reanalysis between the two locations.\nThe estimated value at Kellogg is computed as the weighted average of the individual contributions from each station, with weights proportional to the inverse of the distance between Kellogg and a given station.\nThe stations contributing to this reconstruction are: Mullan Pass, Mullan Pass Vor (27%, 37 kilometers, east); Coeur D'Alene Air Terminal (52%, 58 kilometers, northwest); and Pullman–Moscow Regional Airport (21%, 116 kilometers, southwest).\nAll data relating to the Sun's position (e.g., sunrise and sunset) are computed using astronomical formulas from the book, Astronomical Tables of the Sun, Moon and Planets , by Jean Meeus.\nAll other weather data, including cloud cover, precipitation, wind speed and direction, and solar flux, come from NASA's MERRA-2 Modern-Era Retrospective Analysis . This reanalysis combines a variety of wide-area measurements in a state-of-the-art global meteorological model to reconstruct the hourly history of weather throughout the world on a 50-kilometer grid.\nLand Use data comes from the Global Land Cover SHARE database , published by the Food and Agriculture Organization of the United Nations.\nElevation data comes from the Shuttle Radar Topography Mission (SRTM) , published by NASA's Jet Propulsion Laboratory.\nNames, locations, and time zones of places and some airports come from the GeoNames Geographical Database .\nTime zones for aiports and weather stations are provided by AskGeo.com .\nMaps are © Esri, with data from National Geographic, Esri, DeLorme, NAVTEQ, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, and iPC.\nThe information on this site is provided as is, without any assurances as to its accuracy or suitability for any purpose. Weather data is prone to errors, outages, and other defects. We assume no responsibility for any decisions made on the basis of the content presented on this site.\nWe draw particular cautious attention to our reliance on the MERRA-2 model-based reconstructions for a number of important data series. While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\nWe further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader.","Average Weather at Roseglen North Dakota, United States\nAt Roseglen, the summers are warm; the winters are freezing, dry, and windy; and it is partly cloudy year round. Over the course of the year, the temperature typically varies from 3°F to 82°F and is rarely below -19°F or above 92°F.\nBased on the tourism score, the best time of year to visit Roseglen for warm-weather activities is from late June to late August.\nThe warm season lasts for 3.8 months, from May 25 to September 17, with an average daily high temperature above 70°F. The hottest day of the year is August 7, with an average high of 82°F and low of 56°F.\nThe cold season lasts for 3.5 months, from November 23 to March 8, with an average daily high temperature below 34°F. The coldest day of the year is January 5, with an average low of 3°F and high of 21°F.\nAverage High and Low Temperature\nThe figure below shows you a compact characterization of the entire year of hourly average temperatures. The horizontal axis is the day of the year, the vertical axis is the hour of the day, and the color is the average temperature for that hour and day.\nAverage Hourly Temperature\nPemzashen, Armenia (5,990 miles away); Sosnovka, Kyrgyzstan (6,193 miles); and Laojunmiao, China (6,274 miles) are the far-away foreign places with temperatures most similar to Roseglen (view comparison).\nAt Roseglen, the average percentage of the sky covered by clouds experiences significant seasonal variation over the course of the year.\nThe clearer part of the year at Roseglen begins around June 7 and lasts for 4.1 months, ending around October 11. On July 31, the clearest day of the year, the sky is clear, mostly clear, or partly cloudy 75% of the time, and overcast or mostly cloudy 25% of the time.\nThe cloudier part of the year begins around October 11 and lasts for 7.9 months, ending around June 7. On January 9, the cloudiest day of the year, the sky is overcast or mostly cloudy 62% of the time, and clear, mostly clear, or partly cloudy 38% of the time.\nCloud Cover Categories\nA wet day is one with at least 0.04 inches of liquid or liquid-equivalent precipitation. The chance of wet days at Roseglen varies significantly throughout the year.\nThe wetter season lasts 4.1 months, from April 26 to August 31, with a greater than 19% chance of a given day being a wet day. The chance of a wet day peaks at 34% on June 11.\nThe drier season lasts 7.9 months, from August 31 to April 26. The smallest chance of a wet day is 3% on February 3.\nAmong wet days, we distinguish between those that experience rain alone, snow alone, or a mixture of the two. Based on this categorization, the most common form of precipitation at Roseglen changes throughout the year.\nRain alone is the most common for 7.6 months, from March 21 to November 7. The highest chance of a day with rain alone is 34% on June 11.\nSnow alone is the most common for 4.4 months, from November 7 to March 21. The highest chance of a day with snow alone is 6% on January 2.\nDaily Chance of Precipitation\nTo show variation within the months and not just the monthly totals, we show the rainfall accumulated over a sliding 31-day period centered around each day of the year. Roseglen experiences significant seasonal variation in monthly rainfall.\nThe rainy period of the year lasts for 6.9 months, from April 3 to October 30, with a sliding 31-day rainfall of at least 0.5 inches. The most rain falls during the 31 days centered around June 14, with an average total accumulation of 2.8 inches.\nThe rainless period of the year lasts for 5.1 months, from October 30 to April 3. The least rain falls around December 30, with an average total accumulation of 0.0 inches.\nAverage Monthly Rainfall\nWe report snowfall in liquid-equivalent terms. The actual depth of new snowfall is typically between 5 and 10 times the liquid-equivalent amount, assuming the ground is frozen. Colder, drier snow tends to be on the higher end of that range and warmer, wetter snow on the lower end.\nAs with rainfall, we consider the snowfall accumulated over a sliding 31-day period centered around each day of the year. Roseglen experiences some seasonal variation in monthly liquid-equivalent snowfall.\nThe snowy period of the year lasts for 6.9 months, from October 13 to May 9, with a sliding 31-day liquid-equivalent snowfall of at least 0.1 inches. The most snow falls during the 31 days centered around November 18, with an average total liquid-equivalent accumulation of 0.3 inches.\nThe snowless period of the year lasts for 5.1 months, from May 9 to October 13. The least snow falls around July 26, with an average total liquid-equivalent accumulation of 0.0 inches.\nAverage Liquid-Equivalent Monthly Snowfall\nThe length of the day at Roseglen varies extremely over the course of the year. In 2018, the shortest day is December 21, with 8 hours, 24 minutes of daylight; the longest day is June 21, with 16 hours, 1 minute of daylight.\nHours of Daylight and Twilight\nThe earliest sunrise is at 5:48 AM on June 16, and the latest sunrise is 2 hours, 48 minutes later at 8:36 AM on December 31. The earliest sunset is at 4:55 PM on December 11, and the latest sunset is 4 hours, 55 minutes later at 9:49 PM on June 25.\nDaylight saving time (DST) is observed at Roseglen during 2018, starting in the spring on March 11, lasting 7.8 months, and ending in the fall on November 4.\nSunrise & Sunset with Twilight and Daylight Saving Time\nWe base the humidity comfort level on the dew point, as it determines whether perspiration will evaporate from the skin, thereby cooling the body. Lower dew points feel drier and higher dew points feel more humid. Unlike temperature, which typically varies significantly between night and day, dew point tends to change more slowly, so while the temperature may drop at night, a muggy day is typically followed by a muggy night.\nRoseglen experiences some seasonal variation in the perceived humidity.\nThe muggier period of the year lasts for 2.7 months, from June 10 to September 2, during which time the comfort level is muggy, oppressive, or miserable at least 5% of the time. The muggiest day of the year is July 20, with muggy conditions 22% of the time.\nThe least muggy day of the year is February 27, when muggy conditions are essentially unheard of.\nHumidity Comfort Levels\nThis section discusses the wide-area hourly average wind vector (speed and direction) at 10 meters above the ground. The wind experienced at any given location is highly dependent on local topography and other factors, and instantaneous wind speed and direction vary more widely than hourly averages.\nThe average hourly wind speed at Roseglen experiences mild seasonal variation over the course of the year.\nThe windier part of the year lasts for 8.9 months, from September 13 to June 9, with average wind speeds of more than 11.2 miles per hour. The windiest day of the year is March 26, with an average hourly wind speed of 12.6 miles per hour.\nThe calmer time of year lasts for 3.1 months, from June 9 to September 13. The calmest day of the year is July 23, with an average hourly wind speed of 9.7 miles per hour.\nAverage Wind Speed\nThe predominant average hourly wind direction at Roseglen varies throughout the year.\nThe wind is most often from the north for 2.2 months, from March 7 to May 12, with a peak percentage of 29% on March 9. The wind is most often from the south for 3.7 weeks, from May 12 to June 7 and for 1.9 months, from July 8 to September 5, with a peak percentage of 32% on August 19. The wind is most often from the west for 1.0 months, from June 7 to July 8 and for 6.1 months, from September 5 to March 7, with a peak percentage of 29% on June 14.\nBest Time of Year to Visit\nTo characterize how pleasant the weather is at Roseglen throughout the year, we compute two travel scores.\nThe tourism score favors clear, rainless days with perceived temperatures between 65°F and 80°F. Based on this score, the best time of year to visit Roseglen for general outdoor tourist activities is from late June to late August, with a peak score in the last week of July.\nThe beach/pool score favors clear, rainless days with perceived temperatures between 75°F and 90°F. Based on this score, the best time of year to visit Roseglen for hot-weather activities is from early July to mid August, with a peak score in the last week of July.\nFor each hour between 8:00 AM and 9:00 PM of each day in the analysis period (1980 to 2016), independent scores are computed for perceived temperature, cloud cover, and total precipitation. Those scores are combined into a single hourly composite score, which is then aggregated into days, averaged over all the years in the analysis period, and smoothed.\nOur cloud cover score is 10 for fully clear skies, falling linearly to 9 for mostly clear skies, and to 1 for fully overcast skies.\nOur precipitation score, which is based on the three-hour precipitation centered on the hour in question, is 10 for no precipitation, falling linearly to 9 for trace precipitation, and to 0 for 0.04 inches of precipitation or more.\nOur tourism temperature score is 0 for perceived temperatures below 50°F, rising linearly to 9 for 65°F, to 10 for 75°F, falling linearly to 9 for 80°F, and to 1 for 90°F or hotter.\nOur beach/pool temperature score is 0 for perceived temperatures below 65°F, rising linearly to 9 for 75°F, to 10 for 82°F, falling linearly to 9 for 90°F, and to 1 for 100°F or hotter.\nDefinitions of the growing season vary throughout the world, but for the purposes of this report, we define it as the longest continuous period of non-freezing temperatures (≥ 32°F) in the year (the calendar year in the Northern Hemisphere, or from July 1 until June 30 in the Southern Hemisphere).\nThe growing season at Roseglen typically lasts for 4.4 months (135 days), from around May 13 to around September 25, rarely starting before April 26 or after May 29, and rarely ending before September 7 or after October 13.\nTime Spent in Various Temperature Bands and the Growing Season\nGrowing degree days are a measure of yearly heat accumulation used to predict plant and animal development, and defined as the integral of warmth above a base temperature, discarding any excess above a maximum temperature. In this report, we use a base of 50°F and a cap of 86°F.\nBased on growing degree days alone, the first spring blooms at Roseglen should appear around May 2, only rarely appearing before April 21 or after May 17.\nGrowing Degree Days\nThis section discusses the total daily incident shortwave solar energy reaching the surface of the ground over a wide area, taking full account of seasonal variations in the length of the day, the elevation of the Sun above the horizon, and absorption by clouds and other atmospheric constituents. Shortwave radiation includes visible light and ultraviolet radiation.\nThe average daily incident shortwave solar energy experiences extreme seasonal variation over the course of the year.\nThe brighter period of the year lasts for 3.5 months, from May 3 to August 18, with an average daily incident shortwave energy per square meter above 6.0 kWh. The brightest day of the year is July 7, with an average of 7.2 kWh.\nThe darker period of the year lasts for 3.5 months, from October 28 to February 13, with an average daily incident shortwave energy per square meter below 2.4 kWh. The darkest day of the year is December 24, with an average of 1.2 kWh.\nAverage Daily Incident Shortwave Solar Energy\nFor the purposes of this report, the geographical coordinates of Roseglen are 47.750 deg latitude, -101.833 deg longitude, and 2,090 ft elevation.\nThe topography within 2 miles of Roseglen is essentially flat, with a maximum elevation change of 85 feet and an average elevation above sea level of 2,097 feet. Within 10 miles is essentially flat (377 feet). Within 50 miles contains only modest variations in elevation (1,257 feet).\nThe area within 2 miles of Roseglen is covered by cropland (99%), within 10 miles by cropland (94%), and within 50 miles by cropland (52%) and grassland (42%).\nThis report illustrates the typical weather at Roseglen, based on a statistical analysis of historical hourly weather reports and model reconstructions from January 1, 1980 to December 31, 2016.\nTemperature and Dew Point\nRoseglen has a weather station that reported reliably enough during the analysis period that we have included it in our network. When available, historical temperature and dew point measurements are taken directly from this weather station. These records are obtained from NOAA's Integrated Surface Hourly data set, falling back on ICAO METAR records as required.\nIn the case of missing or erroneous measurements from this station, we fall back on records from nearby stations, adjusted according to typical seasonal and diurnal intra-station differences. For a given day of the year and hour of the day, the fallback station is selected to minimize the prediction error over the years for which there are measurements for both stations.\nThe stations on which we may fall back include but are not limited to Minot Air Force Base; Estevan Regional Aerodrome; Crosby Municipal Airport; Minot International Airport; Garrison; Sloulin Field International Airport; Mercer County Regional Airport; and Melita, Man.\nAll data relating to the Sun's position (e.g., sunrise and sunset) are computed using astronomical formulas from the book, Astronomical Tables of the Sun, Moon and Planets , by Jean Meeus.\nAll other weather data, including cloud cover, precipitation, wind speed and direction, and solar flux, come from NASA's MERRA-2 Modern-Era Retrospective Analysis . This reanalysis combines a variety of wide-area measurements in a state-of-the-art global meteorological model to reconstruct the hourly history of weather throughout the world on a 50-kilometer grid.\nLand Use data comes from the Global Land Cover SHARE database , published by the Food and Agriculture Organization of the United Nations.\nElevation data comes from the Shuttle Radar Topography Mission (SRTM) , published by NASA's Jet Propulsion Laboratory.\nNames, locations, and time zones of places and some airports come from the GeoNames Geographical Database .\nTime zones for aiports and weather stations are provided by AskGeo.com .\nMaps are © Esri, with data from National Geographic, Esri, DeLorme, NAVTEQ, UNEP-WCMC, USGS, NASA, ESA, METI, NRCAN, GEBCO, NOAA, and iPC.\nThe information on this site is provided as is, without any assurances as to its accuracy or suitability for any purpose. Weather data is prone to errors, outages, and other defects. We assume no responsibility for any decisions made on the basis of the content presented on this site.\nWe draw particular cautious attention to our reliance on the MERRA-2 model-based reconstructions for a number of important data series. While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\nWe further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:25a0e6ae-0c7c-4d11-9ffc-0892dd1afc77>","<urn:uuid:fb3697d6-ad3a-4b33-8595-5630b8660b49>"],"error":null}
{"question":"How do RUL estimation models help predict device failures, and what cybersecurity risks should be considered when implementing these predictive maintenance systems?","answer":"RUL (Remaining Useful Life) estimation models help predict device failures through three main approaches: similarity models that compare behavior patterns with historical data, degradation models that extrapolate past behavior to predict future conditions, and survival analysis that uses statistical methods to model time-to-event data. These models require historical run-to-failure data, known threshold values, or lifetime data. However, when implementing such networked predictive systems, cybersecurity vulnerabilities must be addressed. These include risks from unauthorized access, potential data/identity theft, and hacking. The systems need protection against malware introduction, especially with the growing trend of BYOD (Bring Your Own Device) in workplaces and cloud-based software usage. Organizations must implement features for detecting security compromises, ensure trusted content through authentication, and maintain systematic procedures for secure data transfer.","context":["Predictive Maintenance Toolbox™ includes some specialized models designed for computing RUL from different types of measured system data. These models are useful when you have historical data and information such as:\nRun-to-failure histories of machines similar to the one you want to diagnose\nA known threshold value of some condition indicator that indicates failure\nData about how much time or how much usage it took for similar machines to reach failure (lifetime)\nRUL estimation models provide methods for training the model using historical data and using it for performing prediction of the remaining useful life. The term lifetime here refers to the life of the machine defined in terms of whatever quantity you use to measure system life. Similarly time evolution can mean the evolution of a value with usage, distance traveled, number of cycles, or other quantity that describes lifetime.\nThe general workflow for using RUL estimation models is:\nChoose the best type of RUL estimation model for the data and system knowledge you have. Create and configure the corresponding model object.\nTrain the estimation model using the historical data you have. To do so, use the\nUsing test data of the same type as your historical data, estimate the RUL of the test\ncomponent. To do so, use the\ncommand. You can also use the test data recursively to update some model types, such as\ndegradation models, to help keep the predictions accurate. To do so, use the\nFor a basic example illustrating these steps, see Update RUL Prediction as Data Arrives.\nThere are three families of RUL estimation models. Choose which family and which model to use based on the data and system information you have available, as shown in the following illustration.\nSimilarity models base the RUL prediction of a test machine on known behavior of similar machines from a historical database. Such models compare a trend in test data or condition-indicator values to the same information extracted from other, similar systems.\nSimilarity models are useful when:\nYou have run-to-failure data from similar systems (components). Run-to-failure data is data that starts during healthy operation and ends when the machine is in a state close to failure or maintenance.\nThe run-to-failure data shows similar degradation behaviors. That is, the data changes in some characteristic way as the system degrades.\nThus you can use similarity models when you can obtain degradation profiles from your data ensemble. The degradation profiles represent the evolution of one or more condition indicators for each machine in the ensemble (each component), as the machine transitions from a healthy state to a faulty state.\nPredictive Maintenance Toolbox includes three types of similarity models. All three types estimate RUL by\ndetermining the similarity between the degradation history of a test data set and the\ndegradation history of data sets in the ensemble. For similarity models,\nestimates the RUL of the test component as the median life span of most similar components\nminus the current lifetime value of the test component. The three models differ in the ways\nthey define and quantify the notion of similarity.\nHashed-feature similarity model (\nhashSimilarityModel) — This model transforms historical degradation data\nfrom each member of your ensemble into fixed-size, condensed, information such as the\nmean, total power, maximum or minimum values, or other quantities.\nWhen you call\nfit on a\nhashSimilarityModel object, the software computes these\nhashed features and stores them in the similarity model. When\npredictRUL with data from a test component, the software\ncomputes the hashed features and compares the result to the values in the table of\nhistorical hashed features.\nThe hashed-feature similarity model is useful when you have large amounts of\ndegradation data, because it reduces the amount of data storage necessary for\nprediction. However, its accuracy depends on the accuracy of the hash function that the\nmodel uses. If you have identified good condition indicators in your data, you can use\nMethod property of the\nobject to specify the hash function to use those features.\nPairwise similarity model (\npairwiseSimilarityModel) — Pairwise similarity estimation determines RUL by\nfinding the components whose historical degradation paths are most correlated to that of\nthe test component. In other words, it computes the distance between different time\nseries, where distance is defined as correlation, dynamic time warping (\ndtw), or a custom metric that you provide. By taking into account the\ndegradation profile as it changes over time, pairwise similarity estimation can give\nbetter results than the hash similarity model.\nResidual similarity model (\nresidualSimilarityModel) — Residual-based estimation fits prior data to\nmodel such as an ARMA model or a model that is linear or exponential in usage time. It\nthen computes the residuals between data predicted from the ensemble models and the data\nfrom the test component. You can view the residual similarity model as a variation on\nthe pairwise similarity model, where the magnitudes of the residuals is the distance\nmetric. The residual similarity approach is useful when your knowledge of the system\nincludes a form for the degradation model.\nFor an example that uses a similarity model for RUL estimation, see Similarity-Based Remaining Useful Life Estimation.\nDegradation models extrapolate past behavior to predict the future condition. This type of RUL calculation fits a linear or exponential model to degradation profile of a condition indicator, given the degradation profiles in your ensemble. It then uses the degradation profile of the test component to statistically compute the remaining time until the indicator reaches some prescribed threshold. These models are most useful when there is a known value of your condition indicator that indicates failure. The two available degradation model types are:\nLinear degradation model (\nlinearDegradationModel) — Describes the degradation behavior as a linear\nstochastic process with an offset term. Linear degradation models are useful when your\nsystem does not experience cumulative degradation.\nExponential degradation model (\nexponentialDegradationModel — Describes the degradation behavior as an\nexponential stochastic process with an offset term. Exponential degradation models are\nuseful when the test component experiences cumulative degradation.\nAfter you create a degradation model object, initialize the model using historical data\nregarding the health of an ensemble of similar components, such as multiple machines\nmanufactured to the same specifications. To do so, use\nfit. You can\nthen predict the remaining useful life of similar components using\nDegradation models only work with a single condition indicator. However, you can use principal-component analysis or other fusion techniques to generate a fused condition indicator that incorporates information from more than one condition indicator. Whether you use a single indicator or a fused indicator, look for an indicator that shows a clear increasing or decreasing trend, so that the modeling and extrapolation are reliable.\nFor an example that takes this approach and estimates RUL using a degradation model, see Wind Turbine High-Speed Bearing Prognosis.\nSurvival analysis is a statistical method used to model time-to-event data. It is useful when you do not have complete run-to-failure histories, but instead have:\nOnly data about the life span of similar components. For example, you might know how\nmany miles each engine in your ensemble ran before needing maintenance, or how many\nhours of operation each machine in your ensemble ran before failure. In this case, you\nreliabilitySurvivalModel. Given the historical information on failure\ntimes of a fleet of similar components, this model estimates the probability\ndistribution of the failure times. The distribution is used to estimate the RUL of the\nBoth life spans and some other variable data (covariates)\nthat correlates with the RUL. Covariates, also called environmental\nvariables or explanatory variables, comprise\ninformation such as the component provider, regimes in which the component was used, or\nmanufacturing batch. In this case, use\ncovariateSurvivalModel. This model is a proportional hazard survival model\nwhich uses the life spans and covariates to compute the survival probability of a test","This paper is an exploration of what could be a subset of data integrity, with added dimensions -cybersecurity. Cybersecurity is a growing concern for all, whether you work in the legal, financial or consumer industries. It could affect you personally.\nCybersecurity is a recent concern for the medical products industries, a result of their increased reliance on networked electronic software, records and signatures. Initially there were regulations such as 21 CFR Part 11 in the U.S. and Annex 11 in Europe. But more must be done to ensure the integrity of CGMP documents / records. Cybersecurity is an issue that will only increase over time, as records become more electronic, and communications are more networked or accessible.\nAs a result, the US FDA has issued the following Guidances for Industry:\n- “Cybersecurity for Networked Medical Devices Containing Off-the-Shelf (OTS) Software Document”, issued on: January 14, 2005; and\n- “Content of Premarket Submissions for Management of Cybersecurity in Medical Devices”, issued on: October 2, 2014; and\n- “Postmarket Management of Cybersecurity in Medical Devices” -- Draft Guidance, issued on: January 22, 2016\nAs the titles above indicate, the focus is on medical devices. But the principles can and should be applied to a company’s computerized systems as well. In this article, we will also focus on devices, but will also draw attention to company computerized systems where appropriate, to flag related potential problems with an e-records based QMS, computer-facilitated production and/or test / lab equipment / systems.\nOf necessity, cybersecurity in the medical products industries is coming under increased regulatory review. Regulatory agencies leave the \"how\" of cybersecurity compliance up to the manufacturer, as long as the principles in the guidances are met in the resulting product and/or system. Related issues are primarily addressed by the CGMPs, specifically design control (21 CFR 820.30) for devices, and post-production by the CAPA system, among others.\nCybersecurity – Why?\nFDA and news media have emphasized the prevalence of cybersecurity issues, such as data / identity theft, and hacking which pose hazards to many activities and businesses / industries. Our, and the FDA’s and EU’s concern, is with medical products’ users.\nFurther adding to the problem is the industrial growth of BYOD – “Bring Your Own Device” (laptop, tablet, smart phone, or similar “smart” device) – in the workplace. This growing trend poses a problem to cybersecurity, including unintentional and intentional malware introduction.\nThe increasing use of cloud (Internet)-based software to accomplish CGMP tasks, store / retrieve data (data warehousing) and similar uses poses another two-fold concern. In many cases, the updates, upgrades, new revisions / releases, service packs, and similar are automatically uploaded to a company’s systems, which can pose:\n- Security risks, with the potential for introduction of compromised code, retrieval of confidential data, data integrity issues (discussed in our previous article), and similar; and\n- Render previous computer systems’ verification and validations worthless (no change control / “line drawn in the sand”).\nSuch issues are troubling because ensuring data integrity / cybersecurity is an important component of industry’s responsibility to ensure the safety, efficacy, and quality of medical products, the records documenting their manufacture and use, and consequently, of FDA’s ability to protect the public health.\nA growing number of medical devices are designed to be connected to computer networks, or facilitate the use of removable storage, such as flash or thumb drives. Such “networked” medical devices incorporate use software (off-the-shelf or custom) that is vulnerable to cybersecurity threats, such as viruses and worms, phishing, and similar unauthorized or unknown intrusions.\nSuch vulnerabilities pose a risk to the safe and effective operation of such medical devices. This mandates an ongoing maintenance effort throughout the product life cycle to assure an adequate degree of protection. FDA issued their Cybersecurity Guidance to clarify how existing regulations, including the Quality System (QS) Regulation, 21 CFR 820 (medical device CGMPs), 21 CFR Part 11, Electronic Records / Electronic Signatures, apply to such cybersecurity maintenance activities. It discusses the basic principles the FDA expects of companies producing products containing networked software / firmware. Though geared to medical devices, the principles apply to all regulated industries with networked and/or portable memory accessible software / firmware (e.g., flash / thumb drives, BYOD access) computerized systems – records maintenance, production and test equipment and similar.\nHowever, the Cybersecurity Guidance specifically targets devices that incorporate off-the-shelf (OTS) software. Therefore, it primarily applies to device manufacturers who incorporate OTS software in their medical devices. The QS Regulation, 21 CFR Part 820, applies to software maintenance actions. This is in addition to the other guidance documents the Agency has issued in the past on software. Companies using SOUP (software of unknown pedigree) in their products (or systems) should also be aware of and address similar concerns.\nThe FDA recommends that medical device manufacturers consider the “NIST Framework for Improving Critical Infrastructure Cybersecurity” framework core functions to guide their cybersecurity activities:\n- Respond; and\nIdentify and Protect\nMedical devices capable of connecting (wirelessly or hard-wired) to another device, to the Internet or other network, or to portable media (e.g. USB or CD) are more vulnerable to cybersecurity threats than devices that are not connected. However, never assume invulnerability, verify to challenge vulnerability, and document such verification.\nThe extent to which security controls are needed will depend on the following:\n- The device’s / systems intended use;\n- The presence and intent of its electronic data interfaces;\n- Its intended environment of use;\n- The type of cybersecurity vulnerabilities present;\n- The likelihood the vulnerability will be exploited (either intentionally or unintentionally); and\n- The probable risk of patient harm due to a cybersecurity breach.\nManufacturers should also carefully consider the balance between cybersecurity safeguards and the usability of the device in its intended environment of use. See also the newly revised standard IEC 62366-1 (2015) and -2 (2016), Usability Engineering, for such a process.\nEnsure Trusted Content\nRestrict software or firmware updates to authenticated code. One authentication method manufacturers may consider is code signature verification. Use systematic procedures for authorized users to download version-identifiable software and firmware from the manufacturer. Ensure capability of secure data transfer to and from the device, and when appropriate, use methods for encryption.\nDetect, Respond, Recover\nImplement features that allow for security compromises to be detected, recognized, logged, timed, and acted upon during normal use. Develop and provide information to the end user concerning appropriate actions to take upon detection of a cybersecurity event. Implement device features that protect critical functionality, even when the device’s cybersecurity has been compromised. Provide methods for retention and recovery of device configuration by an authenticated privileged user.\nVulnerabilities and Risk\nFDA states that a cybersecurity vulnerability exists whenever the software provides the opportunity for unauthorized access to the network or the medical device. Cybersecurity vulnerabilities open the door to unwanted software changes that may have an effect on the safety and effectiveness of the medical device. Failure to properly address these vulnerabilities could result in an adverse effect on public health. A major concern with OTS software is the need for timely software patches to correct newly discovered vulnerabilities in the software.\nThe FDA recommends that the manufacturer conduct a vulnerability analysis, both initially (premarket) and ongoing (post-market). The approach should appropriately address the following elements:\n- Identification of assets, threats, and vulnerabilities;\n- Assessment of the impact of threats and vulnerabilities on device functionality and end users / patients;\n- Assessment of the likelihood of a threat and of a vulnerability being exploited;\n- Determination of risk levels and suitable mitigation strategies; and\n- Assessment of residual risk and risk acceptance criteria.\nAs mentioned above, and especially software / firmware, initial design and subsequent continuous improvement activities must consider hazards / risk, per ISO 14971. Such risk is focused on the end user – patient and/or clinician. The hazard analysis should address both “normal\" as well as “fault” risks, i.e., not just failure mode risk, the most prevalent approach, but also risk posed by the proper function of the product. In such an analysis, the manufacturer must also consider cybersecurity as part of this regular hazard analysis. Exposure to the web, as in the networked devices focused on in this guidance, increases such cybersecurity risks to the patient / clinician. Hazard / risk analysis’ goal is risk mitigation, documented in the Risk Management File and Report (ISO 14971), as well as in the Design History File (21 CFR 820.30). Consider system boundaries and connections to the external environment. In all such analysis, software must be considered with its associated hardware.\nThe above is to be supplemented by on-going monitoring of actual and potential vulnerabilities, consistent with the Quality System Regulation (21 CFR part 820), including complaint handling, internal quality audits, CAPA (corrective and preventive action), software validation and risk analysis, and servicing. Such programs should emphasize addressing vulnerabilities which may permit the unauthorized access, modification, misuse or denial of use, or the unauthorized use of information that is stored, accessed, or transferred from a medical device to an external recipient, and may impact patient safety.\nThe Agency defines critical components of such a program to include:\n- Monitoring cybersecurity information sources for identification and detection of cybersecurity vulnerabilities and risk;\n- Understanding, assessing and detecting presence and impact of a vulnerability;\n- Establishing and communicating processes for vulnerability intake and handling;\n- Clearly defining essential clinical performance to develop mitigations that protect, respond and recover from the cybersecurity risk;\n- Adopting a coordinated vulnerability disclosure policy and practice; and\n- Deploying mitigations that address cybersecurity risk early and prior to exploitation.\nPostmarket cybersecurity information may originate from an array of sources including a company’s own CAPA / warranty / complaint system, independent security researchers, in-house testing, suppliers of software or hardware technology, health care facilities, and information sharing and analysis organizations. To manage postmarket cybersecurity risks for medical devices, a company should have a structured and systematic approach to risk management and quality management systems consistent with the CGMPs.\nPer the guidance, it is the device manufacturer who incorporates software in their medical device, that bears the primary responsibility for the continued safe and effective performance of the medical device / system, including the performance of the software that is part of the device / system. This is instead of the user or a contract programmer / software developer. FDA recommends that purchasers and users of medical devices who may be subject to a cybersecurity vulnerability contact the manufacturer with their concerns. Even when there are times when it is appropriate for the user to become involved, the user should not attempt to make changes without seeking the manufacturer’s advice and recommendations.\nSoftware patches that address cybersecurity vulnerabilities are addressed in general in the CGMPs. For example, the need to be vigilant and responsive to cybersecurity vulnerabilities is part of a company’s obligation under CAPA. The preamble to the QS Regulation explains that actions taken should “be appropriate to the magnitude of the problem and commensurate with the risks encountered”, i.e., “risk-based.” Design validation requires that devices conform to defined user needs and functional requirements, including an obligation to perform software validation and hazard / risk analysis, where appropriate.\nSoftware changes to address cybersecurity vulnerabilities are design changes, and thus must be verified / validated before approval and issuance. However, FDA premarket review is not usually required prior to implementation of a software patch to address a purely cybersecurity vulnerability for a previously cleared / approved device.\nIn general, FDA review is necessary when a change or modification could significantly affect the safety or effectiveness of the medical device (see also FDA Memorandum K 97-1, and their two recent draft guidances on changes to a device having a 510(k), one specifically addressing software). This is to allow an FDA review of any issue that affects a risk to health posed by the device, and should be the key determinant in making such a decision on the need for an new regulatory review.\nThus, it is possible, but unlikely, that a software patch will need a new 510(k) submission. And the more robust the V&V of the patch / change, the less likely the requirement for a new submission would be. As with all changes made to devices, a company must document the basis of its decisions in the Design History File (DHF) and/or related CGMP documentation.\nHowever, for medical devices approved under Premarket Approval Application (PMA) (21 CFR Part 814), a PMA supplement is required for a software patch, if the patch:\n- Results in a change to the approved indications for use; or\n- Is deemed by the manufacturer to have an adverse effect on the safety and effectiveness of the approved medical device.\nNote: The two points above are also “red flags” for changes to devices covered by a 510(k) that would probably also require a new 510(k).\nOtherwise, the company should report its decision to apply a software patch to its PMA device to the FDA in its annual reports.\nIn all cases, validation of software changes made to address cybersecurity vulnerabilities is required. All software design changes must be verified / regression tested / validated, including those computer software changes to address cybersecurity vulnerabilities. This would be in accordance with an established protocol, before approval and issuance. However, for most software changes intended to address cybersecurity vulnerabilities, analysis, inspection, and testing – documented -- should be adequate and clinical validation should not be necessary.\nThe manufacturer should always maintain formal business relationships with their OTS software vendors to ensure timely receipt of information concerning quality problems and recommended corrective and preventive actions – perhaps by means of the contract or a Quality Agreement. Because of the frequency of cybersecurity patches, the FDA’s Guidance recommends that the company develop a single cybersecurity maintenance plan to address compliance with the QS Regulation and the issues discussed in the guidance document.\nAlthough the guidance recommends that the medical device manufacturer to perform these software maintenance activities, there may be situations in which it is appropriate for the user facility, OTS vendor, or a third party to be involved. The software maintenance plan should provide a mechanism for the manufacturer to exercise overall responsibility while delegating specific tasks to such other parties. The vast majority of healthcare organizations will lack detailed design information and technical resources to assume primary maintenance responsibility for medical device software and will have to rely on the manufacturer to assume the primary maintenance responsibility.\nLifecycle considerations require that the expected growing knowledge of the science of the product and its manufacturing process / equipment translate into manufacturing improvements, and reduction in variation, consistency / homogeneity within lots, and consistency / homogeneity between lots, improved product quality, all reflected in subsequent V&V activities. See FDA’s Guidance on Process Validation.\nThe training of personnel in detecting data integrity issues is also required as part of a routine CGMP training program. Training personnel to detect data integrity issues is viewed as consistent with the personnel requirements discussed in the CGMPs.\nAs stated at the outset, Cybersecurity is a relatively new term for the CGMP community. As hacking of networked electronic systems has become more prevalent, cybersecurity requirements are also being added. Increasingly, manufacturers and users will have to demonstrate their ability to provide for cybersecurity to achieve full CGMP compliance when such vulnerabilities exist in their systems. This is the expectation and demand of all such systems / records stakeholders, not just regulatory agencies.\n# # #\nCAPA Corrective and Preventive Action (see 21 CFR 820.100)\nCDRH U.S. FDA’s Center for Devices and Radiological Health\nCGMPs Current Good Manufacturing Practices (for devices it is 21 CFR Par 820,\nQuality System Regulations)\nCFR The U.S. Code of Federal Regulation\nFDA The United States’ Food and Drug Administration\nISO International Standards Organization\nNIST U.S. National Institute of Science and Technology\nOTS Off-the-shelf [software]\nQA Quality Assurance\nQC Quality Control\nQMS Quality Management System\nQS Quality System\nR&D Research and Development\nSOP Standard Operating Procedure\nV&V Or V[T]&V; Verification [Testing] and Validation. Per the FDA, in computer science, validation refers to ensuring that software meets its specifications. However, this may not meet the definition of process validation as found in guidance for industry Process Validation: General Principles and Practices: “The collection and evaluation of data … which establishes scientific evidence that a process is capable of consistently delivering quality products.” See also ICH guidance for industry Q7A Good Manufacturing Practice Guide for Active Pharmaceutical Ingredients, which defines validation as providing assurance that a specific process, method, or system will consistently produce a result meeting predetermined acceptance criteria.\nJohn E. Lincoln, principal consultant, J. E. Lincoln and Associates LLC, assists companies in the design and implementation of complete 21 CFR 111, 210, 211, 820 and ISO 13485 quality management systems, fully CGMP-compliant, and which have passed FDA audits. He compiles 510(k) submissions, new and changed, product Risk Management Files / Reports per ISO 14971, Design History Files, Technical Files, and Design Dossiers. He assists companies in remediation / FDA responses, SOP writing, audits, validations, including software. His work is described in peer-reviewed technical articles and workshops, world wide. John has also managed pilot production, regulatory affairs, product development / design control projects. He has over 35 years of experience, 21 as a full time consultant, primarily with medical devices – working with start-ups to Fortune 100 companies. He is a graduate of UCLA. John can be reached at email@example.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a8d01b0a-02a2-4cc8-a593-e02901746f26>","<urn:uuid:56b59ff9-a261-414f-afa2-7c3a85a9fd21>"],"error":null}
{"question":"How are soldiers physically described in the opening of 'Dulce et Decorum Est'?","answer":"The soldiers are described as 'bent double, like old beggars under sacks' and as being 'knock-kneed, coughing like hags'. This portrayal presents them as exhausted and deteriorated, cursing as they walk through sludge, painting a picture that contrasts sharply with the glorified image of warriors.","context":["Page 4 dulce et decorum est analysis two students who are familiar with the main effects of world war i will be able to appreciate the soul-shocking contrast offered by wilfred owen in dulce et decorum est between the. Part 1 of the line by line analysis of wilfred owen's 'dulce et decorum est' created by p dunning. Themes in dulce et decorum est death is the overriding theme in dulce et decorum est , although never actually mentioned except in the latin word ‘mori’, which means ‘to die’ the soldier who is gassed is described as drowning, and the physical details and disfigurement of this process made overt. The overarching theme in wilfred owen’s poetry is ‘the pity of war’ (sorrow for another’s suffering) and it is evident that this sorrow is combined with a powerful message to his contemporary audience in ‘dulce et decorum est’has the effect of including the reader in an attempt to gain their sympathy for those on the battlefield.\nDulce et decorum est by wilfred owen bent double, like old beggars under sacks, knock-kneed, coughing like hags, we cursed through sludge, till on the haunting flares we turned our backs. Dulce et decorum est is without a doubt one of, if not the most, memorable and anthologized poems in owen's oeuvre its vibrant imagery and searing tone make it an unforgettable excoriation of wwi, and it has found its way into both literature and history courses as a paragon of textual representation of the horrors of the battlefield. Dulce et decorum est, hailed as the best poem of world war 1, is a skillfully crafted text which has been loved by all for its realistically gritty and gruesome representation of world war 1 and for its ironic quip at those who preach war as glorious. Focusing in particular on one moment in the first world war, when owen and his platoon are attacked with poison gas, ‘dulce et decorum est’ is a studied analysis of suffering and perhaps the most famous anti-war poem ever written.\nDulce et decorum est pro patria mori, which is a line taken from the latin odes of the roman poet horace, means it is sweet and proper to die for one's country in his poem, wilfred owen takes the opposite stance. “dulce et decorum est” (latin: it is sweet and honourable) depicts the horrific effects chlorine gas has on soldiers fighting in the tranches composition in order for you to better comprehend the poem, we will provide you with some points and ideas connected with. Dulce et decorum est is a really sad verse form about war in contrast to the rubric itself the poet owen who himself have experienced war describes the awful significances behind all the glorification people bask in. Dulce et decorum est», wilfred owen (1917, 1920) «dulce et decorum est» is a poem written by british poet wilfred owen, during world war one, in 1917 the translation of the latin title is: «it is sweet and proper.\nThe poem ‘dulce et decorum est’ was written by wilfred owen during world war one it’s a very anti-war poem and portrays an unseen version of war, the horrible part of it. War usually is a bloody series of battles between 2 or more factions usually it is between tribes or countries in dulce et decorum, wilfred owen describes war as being deadly, very bloody and disgusting where soldiers are innocently killed, ripped apart and treated like beggars without hope and they are very smelly. The poem we have been analysing in class, dulce et decorum est, was written by a man named wilfred owen wilfred owen was a soldier in the first world war and was born on the 18th of march 1893, and died on the 4th of november 1918, a week before the end of the first world war.\nDulce et decorum est creates a sharp and deeply ironic line between the civilians who prop up war efforts and the men who fight their battles as owen suggests, there's almost no way for either g. A hundred years on, 'dulce et decorum est' - somewhat like the last post - has moved, beyond literary history and cultural memory, into a structure of feeling in the english-speaking world for some, it is their first encounter with poetry. - analysis of dulce et decorum est by wilfred owen in the poem, dulce et decorum est written by wilfred owen, the speaker appears to be a soldier in the army, warning young people eager for war, “children ardent for some desperate glory,” that war is not what it seems. The meaning of ‘dulce et decorum est’ is –“it is sweet and honourable” this makes you think that it will be a poem encouraging war, but as soon as you start to read the poem you realise that it is the complete opposite. Dulce et decorum est: about the poem the poem dulce et decorum est is a prominent anti-war poem written by wilfred owen about the events surrounding the first world war owen served as a lieutenant in the war and felt the soldiers’ pain and the real truth behind war.\nEssay on dulce et decorum est exemplar 2015 dulce et decorum est analysis wilfred owen fought for his country in world war i at this time, the dominant ideology in britain was that it was an honour to fight for one’s country. This walk whitman quotes shares a similar theme with wilfred owens' poem, dulce et decorum est pro patria mori the ironically titled poem depicts the gruesome truth of engaging battle in war the poem is a far cry from the glorifying propaganda that a nation uses to lure young men into fighting for their nation, such as the soldier in the poem. Find and save ideas about wilfred owen on pinterest | see more ideas about dulce et decorum est, a level art themes and a level sketchbook. Dulce et decorum est summary supersummary, a modern alternative to sparknotes and cliffsnotes, offers high-quality study guides that feature detailed chapter summaries and analysis of major themes, characters, quotes, and essay topics.\nDulce et decorum est summary there was no draft in the first world war for british soldiers it was an entirely voluntary occupation, but the british needed soldiers to fight in the war therefore, through a well-tuned propaganda machine of posters and poems, the british war supporters pushed young and easily influenced youths into signing up. Dulce et decorum est, free study guides and book notes including comprehensive chapter analysis, complete summary analysis, author biography information, character profiles, theme analysis, metaphor analysis, and top ten quotes on classic literature. ‘dulce’ and ‘decorum’ are the two contentious, abstract nouns meaning ‘sweet’ and ‘honourable’, which he revisits in the final lines of the poem joined as they are by the similar sounds of ‘et’ and ‘est’, they set a pattern for the alliteration which follows."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bea69517-2d42-476c-b726-a8220227299f>"],"error":null}
{"question":"What are the environmental benefits of replacing cornfields with native perennial vegetation like switchgrass in the US Corn Belt?","answer":"Converting cornfields to native perennial vegetation like switchgrass offers multiple environmental benefits: it serves as wildlife habitat, reduces water pollution, and helps slow climate change by capturing carbon from the atmosphere. This is in contrast to current Corn Belt agriculture, which is ecologically disturbing and a major source of pollution into the Mississippi River and Gulf of Mexico.","context":["Native biofuels, food webs, and biodiversity\nThe US Corn Belt—a sea of corn and soy covering much of the Midwest—is one of the most intensively managed and ecologically disturbed regions of the country, and a primary source of pollution into the Mississippi River and the Gulf of Mexico. Most of the corn is not grown for human consumption, but is instead used to make ethanol for gasoline or sold to industrial feedlots.\nWe could resolve many of the ecological problems associated with Corn Belt agriculture by converting cornfields to native perennial vegetation, such as switchgrass fields or restored prairie. These native plants can be harvested to make ethanol, but unlike cornfields they also serve as wildlife habitat, reduce water pollution, and slow climate change by taking carbon from the atmosphere.\nUsing long-term landscape experiments managed by the Great Lakes Bioenergy Research Center at the Kellogg Biological Station in southwest Michigan, I compare the ecosystem services of native biofuels versus Corn Belt agriculture. Using a combination of insect surveys, sentinel prey experiments, and stable isotope analysis, I test whether native biofuels 1) contain more diverse and abundant communities of ants and other insects, 2) provide higher rates of natural pest control, and 3) extend food chain length and diversify food webs.\nDispersal, reproduction, and the evolution of life history strategies in ants\nAnts are one of earth’s predominant groups of land animals, but we usually witness only one part of their life cycle. We tend to view ants as colonies of wingless workers on the ground, but most species pass through a phase of life where they live alone, have wings, and fly through the atmosphere. These are the reproductive ants—the males and queens—that leave their birth nests to find mates and found new colonies.\nFlight is the deadliest part of the life cycle, and the coupling of reproduction with high mortality leads to strong flight-related selection. Dispersal, reproduction, and life history are thus fundamentally linked in ants. I study how these linkages lead to dispersal-reproduction tradeoffs, limit the spread of invasive ants, and drive the evolution of social parasitism and other alternate reproductive strategies.\nBiodiversity, indigenous lands, and conservation\nTraditional landscapes around the globe are rapidly being eroded by unrestrained consumption and globalization, causing both the erasure of native peoples and the loss of biodiversity. In most places, responsible biodiversity conservation goes hand in hand with respect and empowerment of indigenous cultures.\nThroughout my work I strive to link research with conservation, and increasingly work in concert with indigenous communities to protect landscapes. In Guyana’s South Rupununi Savanna, for example, I collaborated with NGOs and members of the Wapichan Community to survey biodiversity on traditional lands, document cultural and subsistence use of natural resources, and advocate for community empowerment and nature conservation.\nIn Indonesian Borneo, I likewise worked with a local planetary health NGO that merged social and environmental approaches to conservation. By providing healthcare, economic assistance, and education to Dayak and Malayu communities, recognizing their traditional ways of life, and empowering them to conduct reforestation and monitor deforestation on their lands, we worked to eliminate forest loss in protected areas. In the process we mapped historical forest degradation and recovery, documented native ant diversity of working landscapes, and found that even small community reforestation projects can have large and immediate benefits for orangutans and other wildlife."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:024334c6-1462-496d-94a6-966b8ac4ac85>"],"error":null}
{"question":"What role does fungi play in gut health, and what are the current regulatory challenges for microbiome treatments?","answer":"Fungi plays an essential role in gut health, working together with bacteria to create either a healthy microbiome or potentially harmful digestive biofilm. Dr. Ghannoum's research revealed that fungal organisms are as crucial to the microbiome as bacterial ones, leading to the term 'mycobiome' for fungal communities in the gut. Regarding regulatory challenges, the FDA is struggling to define guidance for microbiome therapies, with ongoing debate about whether to regulate them as pharmaceutical drugs or tissue transplants. The FDA currently allows manufacturers to use organ transplant protocols for C. Diff infection treatments, but other uses require IND status. These regulatory uncertainties affect how therapeutics companies structure their quality and regulatory operations.","context":["- This month, Cleveland, Ohio-based microbiome startup BIOHM raised a $7.5 million equity financing round.\n- Middleland Capital’s VTC Ventures led the oversubscribed round.\n- Additional investment came from Felton Group, JobsOhio Growth Capital Fund, Aztec Capital Management, and others.\n- New funds will enable further development of BIOHM’s discovery platform that turns microbiome data into potential solutions for food and beverage startups, consumers and the medical field.\nWhy it matters:\nBrian Mixer, managing director at Middleland Capital, says he’s seen a growing interest in the gut microbiome over the last several years. “We’ve seen overwhelming research that demonstrates the fundamental importance of the microbiome in human health,” he tells AFN.\nThe gut microbiome is the environment in and around our stomachs and intestines and is made up of trillions of microbes including bacteria, fungi, viruses, and protozoa. Some of these microbes are good for a person’s health, some are not.\nMuch of the work done on the microbiome so far has focused on bacteria. However, the role of fungi could be equally important. It is therefore a very overlooked component of health and wellness.\nBIOHM co-founder Dr. Mahmoud Ghannoum has studied fungi for more than 40 years. His work eventually led him to the conclusion that fungal organisms are as essential to the microbiome as bacterial ones. In 2010, Dr. Ghannoum and his team released research about fungal communities in the gut and coined the term “mycobiome” to describe these communities.\nThe research also explored the critical relationship between bacteria and fungi when it comes to gut health.\nWhile these two things can work together to create a healthy microbiome, they can also team up to create a digestive biofilm. This is a plaque-like lining that forms on a person’s intestines and hides bad germs in the gut. Because of this, BIOHM believes gut health solutions must address both bacterial and fungal balance.\n“There’s growing research that [fungi] are a very important piece of the puzzle,” says Mixer. “You can’t just look at the bacteria, but how they work together in promoting both health and disease. When it comes to understanding the role of fungi, BIOMH is the leader.”\nHow BIOHM works:\nBIOHM’s business includes both direct-to-consumer and business-to-business products and services.\nBIOHM FX is a probiotic supplement for consumers that specifically addresses the aforementioned digestive biofilm in the gut. Once that lining is destroyed, the product releases good bacteria and fungi to neutralize the bad ones that were hiding out in the biofilm.\nIn the B2B realm, BIOHM partners with other companies to make products and ingredients based on its extensive library of microbiome data. BIOHM says it manages one of the most comprehensive gut health datasets and has access to the second-largest collections of fungi strains, after the Centers for Disease Control and Prevention (CDC). This enables “predictable and low-cast developments,” according to Mixer.\nBIOHM also works with partners on bioinformatics, testing, and clinical trial support. The company is currently embarking on a clinical extension directly for medical practitioners.\nMixer says the B2B pipeline and discovery platform were of particular interest to Middleland Capital.\n“They’re turning AI into powerful insights and working with some of the largest global ingredient companies. That validates that there’s real value in their dataset.”\nThat validation is no small matter.\nControversy often surrounds microbiome startups. They’ve been accused of fraud, raided by the FBI, and slammed for outrageous pricing on tests. All of this rolls up into concerns from both consumers and scientists about the strength of the science backing many of these companies.\nMixer believes BIOHM stands apart in this respect. “Proven science is an anomaly. There is a lot of buzz in the space, but not too many that are clinically backed by science.”\n“They built this state-of-the-art microbiome innovation platform and have a really exciting, robust B2B pipeline,” he adds. “They’re working with global industry leaders to create next-gen products using BIOHM’s database. We saw that robust pipeline validating that this is proven science, which is unique in the sector.”\nBIOHM’s location was also attractive to Middleland and VTC: Cleveland, Ohio has some of the world’s leading academic medical centers, such as Case Western University. It’s also an underdeveloped ecosystem for technology, including foodtech and ingredient tech.\nBoth of these factors work in BIOHM’s favor, suggests Mixer. The company can leverage expertise from the universities and scientific community, and there’s a real, untapped opportunity in the area’s venture ecosystem.\n“It’s an exciting market when you look at it from an investor perspective,” he says of the microbiome space. “Research will continue to get better.”","Investments in new microbiome therapies as medicinal products continue to be on the rise and are attracting large Consumer Health giants such as Nestle Health Science, as well as Big Pharma. While the pipeline continues to grow, the FDA and other regulatory bodies are struggling to define guidance and enforce safety for this growing therapeutic area. Concerns regarding the safety of microbiome therapies have caused the FDA to pause a number of clinical trials. In June 2019, two patients became severely ill after receiving a transplant from the same contaminated donor sample, resulting in one fatality. Microbiome therapies are designed to restore gut health for people with severe health issues, but faulty samples have demonstrated an increased risk of severely endangering patients.\nManufacturers are also having difficulties finding ways to scale-up production while maintaining sample safety. Furthermore, the FDA has yet to finalize its guidance for FMTs (fecal microbiota transplants). However, biotech companies can prepare for future legislative action and prepare to resume clinical trials by ensuring their screening and manufacturing methods are optimized for patient safety.\nDespite setbacks, the field of microbiome therapies has enormous potential to treat various diseases and generate revenue. The global market is expected to have a CAGR of 14 percent, with applications ranging from treatments for C. Diff infections, Crohn’s disease and IBS to diabetes and post-chemo treatments. Treating persistent C. Diff infections is the most common use of FMTs, with the therapy demonstrating an effectivness rate of 90 percent. Since these infections affect half a million Americans yearly and cost the US $5 billion in healthcare, reducing the disease burden has significant impacts.\nState of FDA Regulations\nWhether the FDA will choose to regulate FMTs as pharmaceutical drugs or tissue transplants will affect how therapeutics companies structure their quality and regulatory operations. A group of leaders in GI medicine petitioned the agency to regulate the therapy as a transplant, while others in the industry would rather see it treated like a pharmaceutical product to ensure that long-term monitoring and rigorous clinical trials are being conducted. Draft guidance released in 2016 by the FDA allows manufacturers to use organ transplant protocols for therapies aimed at C. Diff infections, since early research indicates the treatment is promising. However, any other uses of FMTs require applying for IND status.\nManufacturing Best Practices for Microbiome Therapies\nWithout current guidelines for the screening, preparation, and storage of samples, the biotech industry has the opportunity to be proactive and form their own best practices for manufacturing at a level of excellence. The more types of bacteria used for a single therapy, the more manufacturing processes companies will have to design. For example, Vendanta Biosciences had to create eight-to-twelve processes in order to target various bacterium according to the individual thicknesses of their walls.\nWhile not as expensive to make as cell therapies, microbiome therapies still cost more to manufacture than other traditional types of treatments. As clinical trials start moving into Phase II and III, biotech companies will need to decide if a CMO would be equipped to handle these kinds of orders or whether to invest in building their own facilities. CMOs often excel at creating sterile environments, which is ironically counter-intuitive to microbiome therapy production, which requires bacterial growth. By lowering oxygen levels to cater to FMT products, CMOs could be putting their other products at risk.\nScreening Protocol Best Practices\nDue to the enormous biodiversity of microorganisms found in each person’s GI system, the world of gut health is complex and only partially understood. Bacterial strain typing is continuously improving, and researchers are continuing to look for tools to identify bacterium in inexpensive and accurate ways. As research progresses, biotech companies will want to consider changing their screening metrics to reflect the most up-to-date understandings. By investing in metagenomics studies, manufacturers may be able to better understand potential risk factors to screen for.\nOne of the most important factors involved in patient safety is the institution of rigorous, evidence-based screening methods. Banks operated by organizations such as OpenBiome provide many of the samples that hospitals use to fight C. Diff infections. Since research proves that samples from “unknown” donors are just as effective as samples from biological relatives, screening for contaminates should take precedence.\nAccording to current protocols, donors undergo blood and stool tests before receiving clearance. In addition to basic-level tests, manufacturers should consider screening for viral infections to ensure their product is safe for immuno-compromised patients. Screening the donor again sixty days after they give a sample is another way to prevent the accidental use of a contaminated sample.\nLike with any new drug or therapy, manufacturers realize that they must manage patient risk factors with the potential benefits of a treatment. By staying up-to-date on the best screening methods and crafting specialized manufacturing processes, biotech companies have the best shot at meeting the FDA’s current standards and moving their microbiome therapies down the pipeline.\nSubscribe to Clarkston's Insights\nContributions by Sabrina Zirkle and Courtney Loughran."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:71bbfdb4-7bbc-49ad-9105-ca7a65e7f56e>","<urn:uuid:53e1b3d8-a3c0-4a5f-93ee-f287cdc42f2d>"],"error":null}
{"question":"Can classical physics completely explain all physical phenomena, or are there limitations where quantum physics becomes necessary?","answer":"Classical physics cannot completely explain all phenomena. While Newtonian physics works well for macroscopic phenomena and is still applied in areas involving force, motion, and gravitation at low speeds, it is inadequate to handle the microscopic domain of atoms and nuclei. Quantum Theory is currently accepted as the proper framework for explaining microscopic phenomena. This is supported by the correspondence principle, which states that classical physics can only be derived from quantum physics in the limit where quantum properties are hidden.","context":["1.2 SCOPE AND EXCITEMENT OF PHYSICS\nWe can get some idea of the scope of physics by looking at its various sub-disciplines. Basically, there are two domains of interest: macroscopic and microscopic. The macroscopic domain includes phenomena at the laboratory, terrestrial and astronomical scales. The microscopic domain includes atomic, molecular and nuclear phenomena. Classical Physics deals mainly with macroscopic phenomena and includes subjects like Mechanics. Electrodynamics, Optics and Thermodynamics. Mechanics founded on Newton’s laws of motion and the law of gravitation is concerned with the motion (or equilibrium) of particles, rigid and deformable bodies, and general systems of particles. The propulsion of a rocket by a jet of ejecting gases, propagation of water waves or sound waves in air, the equilibrium of a bent rod under a load, etc., are problems of mechanics. Electrodynamics deals with electric and magnetic phenomena associated with charged and magnetic bodies. Its basic laws were given by Coulomb, Oersted, Ampere and Faraday, and encapsulated by Maxwell in his famous set of equations. The motion of a current-carrying conductor in a magnetic field, the response of a circuit to an ac voltage (signal), the working of an antenna, the propagation of radio waves in the ionosphere, etc., are problems of electrodynamics. Optics deals with the phenomena involving light. The working of telescopes and microscopes, colours exhibited by thin films, etc., are topics in optics. Thermodynamics, in contrast to mechanics, does not deal with the motion of bodies as a whole. Rather, it deals with systems in macroscopic equilibrium and is concerned with changes in internal energy, temperature, entropy, etc., of the system through external work and transfer of heat. The efficiency of heat engines and refrigerators, the direction of a physical or chemical process, etc., are problems of interest in thermodynamics.\nThe microscopic domain of physics deals with the constitution and structure of matter at the minute scales of atoms and nuclei (and even lower scales of length) and their interaction with different probes such as electrons, photons and other elementary particles. Classical physics is inadequate to handle this domain and Quantum Theory is currently accepted as the proper framework for explaining microscopic phenomena. Overall, the edifice of physics is beautiful and imposing and you will appreciate it more as you pursue the subject.\nYou can now see that the scope of physics is truly vast. It covers a tremendous range of magnitude of physical quantities like length, mass, time, energy, etc. At one end, it studies phenomena at the very small scale of length (10-14 m or even less) involving electrons, protons, etc.; at the other end, it deals with astronomical phenomena at the scale of galaxies or even the entire universe whose extent is of the order of 1026 m. The two length scales differ by a factor of 1040 or even more. The range of time scales can be obtained by dividing the length scales by the speed of light : 1022 s to 1018s. The range of masses goes from, say, 10-30 kg (mass of an electron) to 1055kg (mass of known observable universe). Terrestrial phenomena lie somewhere in the middle of this range.\nPhysics is exciting in many ways. To some people the excitement comes from the elegance and universality of its basic theories, from the fact that a few basic concepts and laws can explain phenomena covering a large range of magnitude of physical quantities. To some others, the challenge in carrying out imaginative new experiments to unlock the secrets of nature, to verify or refute theories, is thrilling. Applied physics is equally demanding. Application and exploitation of physical laws to make useful devices is the most interesting and exciting part and requires great ingenuity and persistence of effort.\nWhat lies behind the phenomenal progress of physics In the last few centuries? Great progress usually accompanies changes In our basic perceptions. First. It was realised that the scientific progress, only qualitative thinking, though no doubt Important. Is not enough. Quantitative measurement Is central to the growth of science, especially physics, because the laws ol\" nature happen to be expressible In precise mathematical equations. The second most Important Insight was that the basic laws of physics are universal — the same laws apply In widely different contexts. Lastly, the strategy of approximation turned out to be very successful. Most observed phenomena In dally life are rather complicated manifestations of the basic laws. Scientists recognised the Importance of extracting the essential features of a phenomenon from Its less significant aspects. It is not practical to take Into account all the complex phenomenon In one go. A good strategy is to focus first on the essential features, discover the basic principles and then Introduce corrections to build a more refined theory of the phenomenon. For example, a stone and a feather dropped from the same height do not reach the ground at the same time. The reason Is that the essential aspect of the phenomenon, namely free fall under gravity. Is complicated by the presence of air resistance. To get the free fall under gravity. It Is better to create a situation wherein the air resistance Is negligible. We can. For example, let the stone and the leather fall through a long evacuated tube. In that case, the two objects will fall almost at the same rate, giving the basic law that acceleration due to gravity Is Independent of the mass of the object. With the basic law thus found, we can go back to the feather. Introduce corrections due to air resistance, modify the existing theory and try to build a more realistic theory ol objects falling to the earth under gravity.","What is the difference between Newtonian physics and quantum physics?\n1. Classical Newtonian mechanics deals with things that are larger – generally large enough to see, and quantum mechanics deals with things that are tiny – a nanometer or less, which is the size of atoms.\nDoes Newton’s laws apply in quantum physics?\nNewton was obliged to give his laws of motion as fundamental axioms. But today we know that the quantum world is fundamental, and Newton’s laws can be seen as consequences of fundamental quantum laws. This article traces this transition from fundamental quantum mechanics to derived classical mechanics.\nWhat is meant by quantum transition?\nIn physics, a quantum phase transition (QPT) is a phase transition between different quantum phases (phases of matter at zero temperature). The transition describes an abrupt change in the ground state of a many-body system due to its quantum fluctuations.\nCan classical physics be derived from quantum physics?\nClassical physics can be derived from quantum physics in the limit that the quantum properties are hidden. That fact is called the “correspondence principle.” Page 2 2. Quantum physics is the revolution that overthrew classical physics.\nIs Newtonian physics still valid?\nNewtonian physics continues to be applied in every area of science and technology where force, motion, and gravitation must be reckoned with. However, today’s physicists, unlike Newton, know that his laws do not work in all circumstances.\nIs Newtonian physics wrong?\nFor applications of mechanics at low speeds, Newtonian ideas are almost equal to reality. That is the reason we use Newtonian mechanics in practice at low speeds. On a conceptual level, Einstein did prove Newtonian ideas quite wrong in some cases, e.g. the relativity of simultaneity.\nDoes quantum physics disprove Newtonian physics?\nFirst of all, Quantum mechanics does NOT disprove Newtonian determinism. Newtonian determinism works wonderfully in the applications for which it was developed. However, it does not work for certain systems. That’s where you need Quantum mechanics to take over.\nDoes quantum physics contradict Newtonian physics?\nQuantum mechanics (QM) clearly violates Newton’s First Law of Motion (NFLM) in the quantum domain. In the process, a general argument is made that such a disparity may be found commonly for a wide variety of quantum predictions in the classical limit. The meaning of the classical limit is examined.\nIs the quantum realm?\nThe Quantum Realm is a dimension in the Multiverse only accessible through magical energy, mystical transportation using a Sling Ring, by tremendous subatomic shrinking caused by the Pym Particles, or a quantum bridge.\nHow are quantum mechanics and Newtonian mechanics related?\nThere was no explanation using using classical electromagnetism and newtonian mechanics 4) Interference effects seen in particles, like electrons, as if they were waves: individual electrons passing through slits showed an intensity pattern appropriate to waves not to newtonian particles\nWhat does it mean to talk about quantum phase transitions?\nTalking about quantum phase transitions means talking about transitions at T = 0: by tuning a non-temperature parameter like pressure, chemical composition or magnetic field, one could suppress e.g. some transition temperature like the Curie or Néel temperature to 0 K.\nWhere does the QPT occur in a quantum system?\nThe QPT occurs at the quantum critical point (QCP), where quantum fluctuations driving the transition diverge and become scale invariant in space and time. Although absolute zero is not physically realizable, characteristics of the transition can be detected in the system’s low-temperature behavior near the critical point.\nHow does the position of a particle affect Newtonian mechanics?\nIt’s not that the position of the particle won’t change the way that Newtonian mechanics predicts. It’s that particles don’t have well-defined positions in the first place. The uncertainty in position times the uncertainty in momentum must always be greater than a constant."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9c71c32f-eecc-4672-aa4f-a723d3e4a4e7>","<urn:uuid:c0de551d-d8ff-468c-85df-a45a71255f07>"],"error":null}
{"question":"What's the key difference between how atoms behave in solids versus gases?","answer":"In solids, atoms are held together by strong attractions and only vibrate in fixed positions, while in gases the atoms move freely in all directions with negligible forces between them. This is why gases can completely fill their containers while solids maintain a fixed shape.","context":["Kinetic Molecular Theory of Matter\nMatter is made up of particles which are in constant, random motion. It can be defined as anything which has mass or occupies space. Matter is classified by its state and type, of which there are three main types – solid, liquid and gas. These lead us to the Kinetic Molecular Theory of Matter.\nParticle Arrangement in Matter\n- Particles in the solid state are closely packed, in a regular arrangement, known sometimes as a lattice.\n- Particles in a liquid state are not as closely packed and are irregular in their arrangement.\n- In a gas, particles are separated.\nForces Between Particles\n- In the solid state, the forces are strong enough to keep the particles in a fixed position. Particles do, however, vibrate and rotate in their positions.\n- In the liquid state, there are weak forces which hold it together. The greater energy of the particles and the weaker forces allows for the disruption of the lattice and particles are, therefore able to slide past one another.\n- In the gaseous state, particles possess even higher energy levels and the forces which hold the gas together are negligible. This explains why gases are able to isolate themselves completely from one another and have no fixed size or shape.\nChanges in State\nChanges in state (also called phase transitions) involve heat energy being supplied to or removed from the substance. Increasing the amount of heat energy in a substance increases its kinetic energy since temperature is a measure of the amount of kinetic energy possessed by a substance. In a solid, heat energy causes the particles to vibrate at a greater rate until they possess sufficient energy to break away from their fixed position and become a liquid (known as melting). The temperature at which this occurs is called a substances melting point. As heat is removed from the liquid, the particles return to their closely-packed, fixed positions, this process is called freezing.\nWhen heat energy is supplied to a liquid, the particles also take on more heat energy which causes them to move around at a greater speed. Fast-moving particles at the surface of the liquid eventually have sufficient energy to escape from the liquid and move into the gaseous state. Here, these particles move rapidly, at a large distance from the other particles. This process is known as vaporisation. The point at which a substance moves from the liquid to the gas state is known as its boiling point. As heat is removed from the substance the particles move closer together once more, this process is called condensation.\nSome substances e.g. iodine crystals are able to change from a solid directly into a gas, without moving through the liquid state. This process is called sublimation. Deposition is the reverse of sublimation and occurs when a substance moves directly from a gas to a solid, omitting the liquid state. An example of this can be seen in sub-zero temperatures, where water vapor in the air changes directly into ice, without first becoming a liquid.\nSelect the best answer for each of the following questions.\n- Which of the following is an example of a molecule, but not a compound?\n- Which of the following is the correct definition for an atom?\n- The smallest building block of matter\n- The smallest building block of matter that retains the chemical properties of the element.\n- The smallest, indivisible building block of matter.\n- The purest type of matter.\nDecide whether the following statements are true or false.\n- All molecules are compounds, but not all compounds are molecules.\n- Compounds are pure substances.\nTheobromine, is a bitter alkaloid of the cacao plant found in chocolate. It has the chemical formula C7H8N4O2.\n- Calculate how many atoms are in two molecules of C7H8N4O2.\nPlasma – the 4th State of Matter\nPlasmas, like gases, have no fixed shape or volume. A gas can reach the plasma state when its atoms become ionized. This occurs when the atom loses some or all of the electrons leaving a positively charged nucleus. This process is known as ionization and explains why plasmas are able to conduct electricity since the electrons are free to move around. Recombination occurs when plasmas return to the gaseous state.\nNeon signs are an example of plasma. The electricity flows through the glass tube containing the gas, stripping the atoms of their electrons. The electricity promotes the electrons to a higher energy level. As the electron returned to its former energy level the excess energy is carried away as a photon, which we see as coloured light.\nVideo Lesson States of Matter\n- Decide if the following are true or false.\n- Solids possess a greater level of kinetic energy than gases.\n- The forces which hold a liquid together are weaker than those which hold a solid together.\n- Some solids exist in a regular structure known as a lattice.\n- Gases have a fixed shape when in a container.\n- Decide whether energy needs to added or removed for each of the following changes of state.\n- Decide which of the properties described below can be attributed to solid, liquid or gas. (Note some properties will be relevant for more than one state.)\n- Can be compressed\n- Requires large amounts of energy input in order to change state.\n- Particles are isolated\n- Fits the shape of the container\nSummarize the differences between solids, liquids and gases for the following:\n- Arrangement of particles\n- Shape (draw a diagram)\n- Shape of substance\n- Level of energy possessed by particles\n- Strength of forces involved\n- Processes involved to change state\nHere is your Free Content for this Lesson on Kinetic Molecular Theory of Matter!\nKinetic Molecular Theory of Matter - PDFs\n- 1-2 Additional Resources - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Bell Ringer SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Bell Ringer Teacher Edition - (MEMBERS ONLY)\n- 1-2 Guided Notes SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Guided Notes Teacher Edition - (MEMBERS ONLY)\n- 1-2 Homework Questions SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Homework Questions Teacher Edition - (MEMBERS ONLY)\n- 1-2 Lesson Plan - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Slide Show - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Vocabulary Doodle Notes SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Exit Quiz SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Exit Quiz Teacher Edition - (MEMBERS ONLY)\n- 1-2 Vocabulary Worksheet SE - Kinetic Molecular Theory of Matter (FREE)\n- 1-2 Vocabulary Worksheet Teacher Edition - (MEMBERS ONLY)\nKinetic Molecular Theory of Matter - Word Docs & PowerPoints\nTo gain access to our editable content Join the iTeachly Chemistry Teacher Community!\nHere you will find hundreds of lessons, a community of teachers for support, and materials that are always up to date with the latest standards.\nWant access to all of our Chemistry Lessons?\nSimply click the image below to GET ALL OF OUR LESSONS!\nDon't Forget to Pin this Lesson on the Structure and Properties of Matter - Kinetic Molecular Theory of Matter...\nThis lesson is from...","matter, material substance that constitutes the observable universe and, together with energy, forms the basis of all objective phenomena. … The three most familiar forms, or states, of matter are solid, liquid, and gas. Heating and cooling a substance may change it from one state to another.\nWhich statement best describes the definition of matter?\nMatter is anything that has mass and takes up space. Matter can be described in terms of physical properties and chemical properties. Physical properties and chemical properties of matter can change. Matter is composed of elements and compounds.\nWhich statement best describes the liquid state of matter?\nLiquid is the state in which the substance takes the shape of its container but has a definite volume.\nWhich statement best describes the atom in a solid?\nWhich statement best describes the atoms in a solid? They are held together by strong attractions.\nWhich is non example of matter?\nNon-matter includes the light from a torch, the heat from a fire, and the sound of a police siren. You cannot hold, taste, or smell these things. They are not types of matter, but forms of energy.\nWhat is the simplest form of matter?\nAn element is the simplest form of matter that has a unique set of properties. Examples of well-known elements include oxygen, iron, and gold (see the figure below). Elements cannot be broken down into a simpler substance. Likewise, one element cannot be chemically converted into a different element.\nWhich of the following statement describes the gas state of matter?\nGas is a state of matter that has no fixed shape and no fixed volume. Gases have lower density than other states of matter, such as solids and liquids. … The particles exert more force on the interior volume of the container. This force is called pressure.\nWhich statement best describes the properties of a liquid?\nA liquid is defined as having a definite volume, but not a definite shape.\nWhich of the following descriptions is used to describe the liquid state?\nwhich of the following descriptions is used to describe the liquid state? … Matter in this state has a definite shape and definite volume.\nWhich statement best describes the movement of atoms in a solid quizlet?\nWhich statement best describes the movement of atoms in a solid? They are able to slide past each other. They spread apart as far as possible.\nWhich statement best describes why a liquid needs a container when a solid does not?\nWhich best describes why a liquid needs a container when a solid does not? Solids fill their containers, and liquids have definite shapes. Solids have definite shapes, and liquids fill their containers. Solids particles will expand to fill their containers, and liquid particles will expand as well.\nWhich are examples of matter?\n- An apple.\n- A person.\n- A table.\n- A computer.\nWhich is a characteristic of atoms?\nThe single most important characteristic of an atom is its atomic number (usually denoted by the letter Z), which is defined as the number of units of positive charge (protons) in the nucleus. For example, if an atom has a Z of 6, it is carbon, while a Z of 92 corresponds to uranium.\nIs a rainbow matter?\nTime: Time can be measured, but it has no mass and occupies no volume. Rainbow: A rainbow is an optical phenomenon. … Gravity: You can feel its effects and it is associated with mass, yet it doesn’t consist of matter. Memories: Like emotions, these are non-matter.\nWhich of the following is an example of matter?\nAnything you can touch or taste is an example of matter. Matter has mass and occupies space.\nWhich term refers to the amount of matter in an object?\nmass: A measure of the amount of matter that an object contains. The mass of an object is made in comparison to the standard mass of 1 kilogram.\nWhich is the form of matter?\nAll things on Earth are made up of matter. Matter exists in three different forms. These forms are solid, liquid, and gas.\nWhich statement best describes the characteristics of particles of matter?\nCharacteristics of particles of matter\nAll matter is composed of very small particles which can exist independently. Particles of matter have spaces between them. Particles of matter are continuously moving. Particles of matter attract each other.\nWhat are the two classes of matter?\nMatter can be classified into several categories. Two broad categories are mixtures and pure substances. A pure substance has a constant composition. All specimens of a pure substance have exactly the same makeup and properties.\nWhich statement is true for all matter?\nExplanation: Matter means anything which occupies space and have mass is called matter. All matter is made up of elements, which have specific chemical and physical properties and cannot be broken down into other substances through ordinary chemical reactions.\nWhich of the following statements describe a solid?\nSolids are defined by the following characteristics: Definite shape (rigid) Definite volume. Particles vibrate around fixed axes.\nWhich answer best describes a solid dissolved in a liquid?\nWhen a solid dissolves the solid (solute) and the liquid (solvent) form a very close intimate mixture called a solution.\nWhich statement gives two characteristic of all liquids?\nWhich statement gives two characteristics of all liquids? They will change shape to fit a container and have definite volume. During which changes of state do atoms that cannot move past one another become free to move?\nWhich type of matter can be separated by physical means?\nMixtures can be separated into pure substances by physical methods. A pure substance is one with uniform and definite compositions. Pure substances can be divided into two groups – elements and compounds.\nWhich of the following are states of matter?\nThere are four natural states of matter: Solids, liquids, gases and plasma. The fifth state is the man-made Bose-Einstein condensates.\nWhich one of the following best describes a liquid?\nA liquid is the only state with a definite volume but no fixed shape. It takes the shape of the container it is poured in but it cannot be compressed.\nWhich statement best compares the motion of atoms in A and B?\nWhich statement best compares the motion of atoms in A and B? Atoms in A will move less than in B, because A is a solid and B is a gas.\nWhat are the states of matter quizlet?\nThere are three states of matter that are common on Earth. These are solids, liquids, and gases. All types of matter (substances) can exist in any one of the three states. Matter changes back and forth from one state to another as heat is added or removed.\nHow does matter relate to chemistry?\nIn more formal terms chemistry is the study of matter and the changes it can undergo. Chemists sometimes refer to matter as ‘stuff’, and indeed so it is. Matter is anything that has mass and occupies space. … The wonder of chemistry is that when these basic particles are combined, they make something new and unique.\nWhich statement describes a gas condensing into a liquid?\nThe change of state from a gas to a liquid is called condensation.\nWhich statement best describes the motion of the molecules of a solid object?\nWhich statement best describes the motion of the molecules of a solid object? They vibrate in place within a fixed volume.\nWhich of the following is the best definition of an atom?\nAn atom is a particle of matter that uniquely defines achemical element. An atom consists of a central nucleus that is usually surrounded by one or more electrons. … The nucleus is positively charged, and contains one or more relatively heavy particles known as protons and neutrons.\nAre atoms the largest unit of matter?\nThe basic unit of all matter is the atom. The atom is the smallest unit of matter that can’t be divided using any chemical means and the building block that has unique properties. … However, even the atom can be broken into smaller pieces, called quarks.\nWhich statement best describes the atoms in gas?\nWhich statement best describes the atoms in a gas? They move freely in all directions. Sample Response: The windsurfer, his board, and the air and water around him are all made of matter. That matter is made up of very small particles called atoms, which cannot be divided.\nWhich statement can be known about the behavior of atoms in each container?\nWhich statement can be known about the behavior of the atoms in each container? The atoms in the solid would be vibrating in position. Atoms in a stay in a fixed position and have no freedom to move. Density involves the amount of a material in a certain volume.\nWhich best compares the phases of matter to marbles in a tray?\nWhich best compares the phases of matter to marbles in a tray? A solid is like the tray being shaken slowly and all the marbles moving in their positions, a liquid is like the tray being shaken and the marbles moving around it, and a gas is like the tray being shaken hard and the marbles moving vigorously around it.\nWhat is matter definition with example?\nA common or traditional definition of matter is “anything that has mass and volume (occupies space)“. For example, a car would be said to be made of matter, as it has mass and volume (occupies space).\nWhat is matter define with example?\n(a) Matter can be defined as anything that occupies space and has a volume. It possesses mass. Examples – Air, Water, Soil and plastic. … The particles in matter attract one another.\nWhat is a matter give two examples?\nMatter is a substance that has inertia and occupies physical space. Examples :-solids, liquids, gases, plasma and Bose-Einstein condensates.\nIs fire a matter?\nIt turns out fire isn’t actually matter at all. Instead, it’s our sensory experience of a chemical reaction called combustion. In a way, fire is like the leaves changing color in fall, the smell of fruit as it ripens, or a firefly’s blinking light.\nIs water a matter?\nStates of Matter\nNot only is water the most common substance on earth, but it is also the only substance that commonly appears as a solid, a liquid, and a gas within the normal range of earth’s temperatures. This makes water a good model for discussing the solid, liquid, and gas states of matter.\nIs sound a matter?\nSound is a little different in that it is the propagation of a pressure wave through a medium of particles. Since it is a wave, it is not considered a form of matter."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c8d0dda1-c8bf-45b9-a61e-ae8eb2301611>","<urn:uuid:5aabca49-570d-4d11-8ce1-33766c7f5d39>"],"error":null}
{"question":"How much protein eat after workout?","answer":"You should consume a minimum of 20 grams of protein within 30 minutes after each workout to stimulate the leucine response and muscle protein synthesis. This recommendation applies to all athletes regardless of weight and gender. For elderly men, 40 grams of protein post-workout is essential for muscle synthesis. An inexpensive source of 20 grams of protein could be 20 ounces of chocolate milk and a banana.","context":["If you skim through a body building magazine, talk to someone in the gym, or conduct an online search for protein you will be inundated with information. Some if not most may be inaccurate but how do you decipher what is true? This article will outline protein sources to include the recommended quantity, timing and quality.\nFirst let’s discuss some background information on protein. Protein is the building blocks for the bones, muscles, cartilage and skin and is also essential for maintaining cellular integrity and function (1). Protein is made up twenty amino acids, nine of which are considered essential. Essential amino acids are ones the body cannot make so they must be acquired through food sources which can be found in animal food sources such as meat, fish, eggs and milk or vegetarian food sources such as grains, legumes, seeds and beans. Different types of protein have shown to provide a faster recovery response post workout that I’ll discuss later in the article.\nAccording to the Institute of Medicine Food and Nutrition Board, the recommended daily allowance (RDA) for protein for both men and women is 0.8 grams per kilogram body weight per day (1). The 2010 Dietary Guideline for Americans* state the daily recommended intake (DRI) for protein to be 10 to 35 percent of total calorie intake. In my opinion the RDA is excessively low when considering athletes and the DRI is too broad of a range especially if you are trying to dial in on your macronutrient needs, i.e. macronutrients are your carbohydrates, protein and fat. So how do you know how much you need? Digging deeper and coming to a more accurate response, in a 2009 Position Stand of the American Dietetic Association, Dietitians of Canada, and the American College of Sports Medicine: Nutrition and Athletic Performance, the recommended protein intake for athletes is 1.2 to 1.7 grams per kilogram body weight per day (2). The position stand notes the RDA and DRI do not take into account the specific needs of athletes which I completely agree with. The following is their recommendation for different types of athletes:\n- Endurance athletes: 1.2 to 1.4 grams per kilogram body weight per day\n- Strength Training: 1.2 to 1.7 grams per kilogram body weight per day\nFor example: A 165 pound (75 kilogram) endurance athlete would need 90 to 105 grams of protein per day OR the same weight athlete who was strength training would need 90 to 127 grams of protein per day. Both of these instances are well over the RDA of a calculated 60 grams per day (0.8 grams X 75 kilograms = 60 grams).\nQuantity and Timing of Protein Intake\nThis is all great information but what about specific timing of protein intake? I get this question a lot… how much post workout or how much during the day at each meal should I consume? I recommend a minimum of 20 grams of protein post workout. Research has shown that this amount illicits the leucine response and stimulates muscle protein synthesis (3). More specifically, protein intake for athletes includes (3):\n- Four equally spaced meals containing protein,\n- Three meals should be 0.25 to 0.3 grams per kilogram per body weight\n- A larger pre-sleep meal with protein intake at 0.6 grams per kilogram body weight\nFor example, let’s take a look at the 165 pound (75 kilogram) athlete. The protein intake would be to 18 to 22 grams at three meals with a larger intake of 45 grams of protein before bed. Total protein intake for the day would be 99 to 111 grams. This falls in the range of the 1.2 to 1.7 grams per kilogram per day in the example shown above. One minor exception, as stated above, I would recommend at least 20 grams of protein within 30 minutes after each workout because of the leucine response. For those who need more, the same leucine response has been found when consuming up to 40 grams of protein post workout. It has been shown that in elderly men, 40 grams of protein post workout is essential for muscle synthesis (3). The consumption of a minimum of 20 grams post workout recommendation would be across the board for any athlete, regardless of weight and gender.\nQuality of Protein Intake\nI’ve discussed timing and quantity of protein intake but now let’s take a look at quality. Not all protein is created equal especially for athletes and their recovery. Specifically whey protein produces a greater increase of muscle protein synthesis. Whey protein contains all 20 amino acids including the three branch chain amino acids leucine, isoleucine and valine which can be oxidized by muscled during exercise. The term “branch-chain” is the chemical structure of these amino acids. During recovery, due to the leucine content of whey, protein muscle synthesis is increased. Whey is more effective than soy which is more effective than casein protein sources (3). Lean body mass gains are seen with the ingestion of whey protein due to the leucine response. Whey is higher in leucine and is absorbed more quickly post workout. The ideal protein dose post workout to elicit the leucine response is 20 grams (3). The best whey protein source comes from diary. An inexpensive source of 20 grams of protein is 20 ounces of chocolate milk and a banana. While food sources of protein are best, protein powders can work but you have to be careful of the type of protein powder and what other \"junk\" is added to the product.\nI’ve seen many athletes who are trying to achieve a specific weight especially in endurance events. I myself know that I can run faster and with less pain if I am even 5 pounds lighter come race day. Based on personal experience I’ve cut back on food intake but lost lean muscle in the process. I came across a study looking at protein intake in women who are trying to lose weight. Research has found that increased protein intake, specifically through dairy during weight loss spares lean muscle mass while decreasing fat mass (4,5). When I work with athletes looking lose weight, I stick to the equally spaced protein at each meal and a bolus before bed as discussed earlier in the article.\nOverall, the main things to keep in mind with protein intake:\n- Consistent timing and quantity is important for athletes and in weight loss.\n- Post workout consumption of at least 20 grams of whey protein will stimulate a leucine response and protein muscle synthesis.\nWhile this article focuses on protein intake, athletes still need adequate carbohydrate intake for performance and recovery. I’ll look more into carbohydrate intake and research on timing and quantity in the near future.\n*The 2015 Dietary Guidelines Americans will be released this fall\n- Institute of Medicine, Food and Nutrition Board. (2005). Dietary Reference Intakes for Energy, Carbohydrate, Fiber, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids (Macronutrients). Retrieved June 15, 2015, from http://www.nap.edu/catalog/10490/dietary-reference-intakes-for-energy-carbohydrate-fiber-fat-fatty-acids-cholesterol-protein-and-amino-acids-macronutrients.\n- Position of the American Dietetic Association, Dietitians of Canada, and the American College of Sports Medicine: Nutrition and athletic performance. J Am Diet Assoc.2009; 109(3):509-527.\n- Phillips, Stuart. (2012). The Importance of Dietary Protein in Resistance Exercise-Induced Adaptation: All Proteins Are Not Created Equal. [PowerPoint slides]. Retrieved from http://scan-dpg.s3.amazonaws.com/resources/DOCS/webinars/2012_The_Importance_of_Dietary_Protein_in_Resistance_Exercise_Induced_Adaptation_webinar.pdf.\n- Josse AR, Atkinson SA, Tarnopolsky MA, Phillips SM. Diets higher in dairy foods and dietary protein support bone health during diet- and exercise-induced weight loss in overweight and obese premenopausal women. J Clin Endocrinol Metab. 2012 Jan;97(1):251-60. doi: 10.1210/jc.2011-2165. Epub 2011 Nov 2.\n- Josse AR, Atkinson SA, Tarnopolsky MA, Phillips SM. Increased consumption of dairy foods and protein during diet- and exercise-induced weight loss promotes fat mass loss and lean mass gain in overweight and obese premenopausal women. J Nutr. 2011 Sep;141(9):1626-34. doi: 10.3945/jn.111.141028. Epub 2011 Jul 20."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:1f8df907-7295-4ab2-a67a-165c5a725424>"],"error":null}
{"question":"What are the similarities and differences between Gandhi's non-violent approach to freedom struggle and O.R. Tambo's leadership of the ANC in terms of their views on armed resistance?","answer":"Gandhi and O.R. Tambo had fundamentally different approaches to resistance. Gandhi made non-violence the cornerstone of his political philosophy, declaring it the 'first article of his faith' and rejecting violence as a mode to assume political power. He believed violence bred violence and could never be a solution. In contrast, under O.R. Tambo's leadership, the ANC transitioned from non-violent resistance to armed struggle when they concluded that the non-violent path could not be sustained. This led to the creation of Umkhonto we Sizwe, the ANC's military wing, in 1961. However, both leaders maintained that armed/non-violent resistance was fundamentally political in nature. While Gandhi emphasized moral courage and shaming opponents into submission, the ANC under Tambo stressed that armed struggle should not be separated from its political context, rejecting pure militarism.","context":["In his trial speech made at Ahmadabad Sessions court in March 1922, Mahatma Gandhi\nput forward his philosophy with great eloquence, when he stated non-violence to be the 'first article of (his) faith' and the 'last article of (his) creed'. Non-violence had always been the founding principle of Gandhian spirituality, and his bedrock of his political philosophy. Gandhi's distrust of violence as a mode to assume political power and as a tool of revolution was ingrained in his world-view from the very early days of his political career.\nIt is impossible to look at Gandhi's political activism in isolation. Springing deeply from his belief in truth, Gandhi's political goals were ultimately specific correlatives of higher commitments to humanity and world peace. Non-violence preaches world peace and brotherhood, whereas political movements naturally revel in polemics of difference and anatagonism. Gandhi's greatness lies in bringing together these two apparently combative and incongruous ideas and putting them on a common platform, where they do not subtract, but support each other. Gandhi's significance in the world political scenario is two-fold. First, he retrieved non-violence as a powerful political tool, and secondly, he was the one of the chief promulgators of the theory that political goal is ultimately a manifestation of a higher spiritual and humanitarian goal, culminating in world peace. For Gandhi, the means were as important as the end, and there could be only one means - that of non-violence.\nThe Origins of Gandhi's Non-Violence Philosophy\nGandhi's secularism and openness to all kinds of theological and philosophical schools is well-known. It was through an assimilation of various concepts and philosophical tenets that Gandhi arrived at his own understanding of non-violence. Jainism and Buddhism were the most important influences that lay behind the foundation of Gandhi's non-violence theory. Both Jainism and Buddhism preached non-violence as the basic principle of existence. All other thoughts and actions propagated by these two religious schools were based on this base of non-violence. Gandhi was deeply influenced by his readings of these scriptures. The Acaranga Sutra of the Jains stated all life to be dear and precious, and Gandhi believed in it earnestly. The Bhagvadgita was another important influence, with its stress on non-attachment and selfless action. Christianity, along with its message of love and compassion, extended even to one's enemies, was another important influence on Gandhi's life. Bringing together all these theological schools, Gandhi was in search for a meaningful life, a life based on truth and honesty, a life that would boast of a moral courage to stand for the right and for justice, even at its own cost. It was this outlook that Gandhi employed as a tool to guide India's freedom struggle, which eventually succeeded to unite the length and breadth of the country like never before.\nGandhi's Use of Non-Violence in India's Freedom Struggle\nGandhi's championing of the cause of non-violence as the tool of India's freedom struggle was not without its share of criticism. That was, however, expected considering the fact that Gandhi entered the political scenario soon after the ascendancy of the extremists in the history of India's freedom struggle. Armed revolution was believed to be the only legitimate way to snatch political power from an oppressive regime. Gandhi's system of Satyagraha on the basis of non-violence and non-cooperation was largely unheard of, and generally distrusted. However, Gandhi's faith was strong. It was a faith based not on arms and antagonism, but on extreme moral courage that drew its strength from innate human truth and honesty. He applied his systems with success in South Africa and was convinced of its power. However, it was an uphill task for him to convince his countrymen. Gandhi slowly started to popularise the ideas in the ranks of the Indian National Congress, under proper guidance from his political mentor Gopal Krishna Gokhale. The Congress was suffering from a lack of national leadership following the arrest and execution of the extremist leaders like Tilak and Lala Lajpat Rai, and the protest to the insulting Rowlatt Act was an immediate necessity. Gandhi soon held the mantle and introduced his non-violence modes with great success in the non-cooperation movement. It was a new era in the history of Indian Freedom struggle. Though the movement ended on an abrupt note, yet its significance was immense.\nGandhi's Rationale for the Application of Non-Violence in Indian Freedom Struggle\nMost religions preached non-violence as a way to celebrate the miracle of life. Gandhi's concern was both based on theological as well as more pragmatic considerations. Gandhi in his trial speech accepted that Indian history is replete with tales and narratives of countless foreign invasions. However, he accused the British rule of being particularly despicable because they left the Indians more helpless and emasculated than any of its predecessors. India was in no position to get into an armed conflict with the British, having been robbed of all economic and moral strength. So, Gandhi had the option of reinvigorating a nation that has lost all confidence in its power and inner strength. After these practical considerations, Gandhi found that the only alternative was to fall back upon what was integral to India's cultural and historical psyche, the principle of non-violence. This non-violence was used in conjunction with the philosophy of non-attachment. Gandhi declared the two goals of his life to be ensuring India's freedom and to achieve it through non-violence. One without the other would be unacceptable and weakening. Violence, Gandhi believed, breded violence, and can never be a solution to India's problem. To shame the opponent into submission was a unique feature of Gandhi's political ideology, as were discussions and amicable arrivals at convenient conclusions. No person, for him was integrally good or bad, and he was cautious never to stoop into a visceral rhetoric of hatred, except against what was unacceptable to his spiritual ideology.\nOne of the greatest criticisms against non-violence was that it was demeaning and cowardly, forwarded particularly by freedom fighters like Savarkar. However, Gandhi believed just the opposite. He emphasized that the moral courage needed to uphold non-violence as a tool of protest was much greater than the one needed to strike back in a violent way. All through his life, he pleaded the Indians to exhibit the moral strength to refrain from resorting to violence, even at the face of all provocation. His disillusionment that followed the Chauri Chaura incident that led to his calling off the non-cooperation movement when at its zenith was an example of his lifelong and earnest commitment to the cause of non-violence.\nThe Legacy of Non-Violence\nNon-violence played a very important role in defining the course of Indian national movement, from the 1920s to the final achievement of the freedom. It formed the basis of the methods of Satyagraha that became closely associated with the Gandhian whirlwind in Indian politics. Gandhi understood economic profit to be the guiding force of the imperialist project and attacked the British government at where it hurt most, which was financial gain. Picketing, non-cooperation and organised resistance to British modes of oppression were the main modes of the non-violent political movements in India. It shaped the course of the Civil Disobedience Movement as well. Even at a later time, during the Quit India Movement, Gandhi's theory of non-violence held strong in the face of the new and radical waves in the world of Indian politics like communism and armed revolution. Even at the dawn of independence, as Nehru was getting ready to eloquently unleash his 'tryst with destiny', Gandhi was busy on the troubled roads of Bengal, preaching non-violence to mad rioters. It was probably pre-ordained that he had to lay down his life for holding on to his ideals.\nGandhi was truly a martyr for the cause of non-violence, who not only preached but practiced what he preached. His life was a glorious example of his thoughts, and thousands of Indians from all walks of life, from cities and villages alike, took encouragement and force from his simple life and unshaken faith in the innate goodness of the human soul. He wielded the weapon of love and understanding, and succeeded to upturn even the strongest of the martial nations with it. Gandhi has left the world richer with a renewed faith in the dictates of non-violence.\nLast Updated on 17/04/2013","For anyone to fully appreciate Oliver Reginald Tambo’s role in our socio-political make up, one needs to grasp the evolution of the ANC between 1912 and the 1940s when O.R. Tambo entered the political stage as the youth leader.\nIn order to appreciate this perspective we need to take a cursory look into the early South African Native National Congress (SANNC) - later the African National Congress (ANC). This was a movement formed by a leadership drawn from the aspirant members of the African proto-middle classes: lawyers, doctors, church ministers, landowners and traders.\nA glance at the leadership elected at the inaugural conference is a pointer. The first President, John Langalibalele Dube was an American trained educationist, church minister and newspaper editor. The Secretary General, Solomon Plaatjie was a newspaper editor and an author. The Treasurer General, Pixley ka Isaka Seme was a Columbia/Oxford and London trained lawyer.\nThe newly launched SANNC had to immediately deal with two draconian pieces of legislation, the discriminatory labour legislation of 1911 and the Natives Land Act of 1913. The latter Sol Plaatjie described as follows:\n“Awaking on Friday morning, June 20, 1913 the South African native found himself, not actually a slave, but a pariah in the land of his birth.”\nThe failure of the deputations and memoranda to Her Majesty resulted in a massive decline in the ANC during the period of the 1930s. The election of Dr Xuma in 1940, a period during in which the African Mineworkers Union and the ANC Youth League were formed, marked the beginning of a process of the renewal and radicalisation of the ANC. This is the period when OR Tambo comes to the fore.\nOliver Reginald Tambo, born into a modest peasant household in Mbizana travelled a long difficult journey from when he was part of the formation of the ANCYL in 1944. He received his education from two mission schools, Holy Cross in Flagstaff and St. Peter’s in Johannesburg where he matriculated - and later taught - in 1938. He was awarded a scholarship to the University College of Fort Hare where he graduated with a B.Sc degree in 1941. He remained in the College to do a teaching diploma but was expelled during the 1942 student strike. St Peter’s allowed him to take up the post of a mathematics and science teacher, where he remained for five years (1943-1947).\nOliver Reginald Tambo, together with his contemporaries, Lembede, Mbatha, Mda, Nelson Mandela and Walter Sisulu, became a founder member of the ANCYL. These young people who adopted the programme of action in the ANCYL, lobbied the ANC to adopt the programme of action in its 1949 national conference. This programme entailed, in the main, disobedience against the apartheid regime. Eight youth leaguers were elected to the NEC of the ANC in the 1949 conference, where Walter Sisulu was elected the Secretary General.\nWhen Walter Sisulu was served with a banning order in 1954 Comrade Oliver Tambo became the acting Secretary General. In 1958 he was elected Deputy President General alongside Duma Nokwe, who was elected the Secretary General. The ANC decided to launch the anti-pass campaign on 31 March 1960, only to be upstaged by the Pan Africanist Congress ten days earlier - leading to the Sharpeville massacre.\nThe banning of the liberation movement thereafter started a new chapter for Comrade Oliver Tambo. His task became that of mobilising the international solidarity and to establish ANC infrastructure in exile. This was to prove the words of the President-General, Chief Albert Luthuli, true when he said:\n“The quality of our Deputy President, Oliver Tambo’s speech makes me very happy - even if I and others in the leadership of the ANC were to die, there are young men like Oliver Tambo who are now ready to take responsibility for the ANC”\nThe banning of the liberation movement led the ANC to resolve that the non-violent path could not be sustained. (Thereby) created a military wing to prepare for the mode of struggle initially based on sabotage. (And) On 16 December 1961 Umkhonto we Sizwe, with Mandela as Commander-in-Chief, announced its existence with a manifesto and attacks on Government buildings in Johannesburg, Port Elizabeth and Durban.”\nOliver Reginald Tambo, became the architect of the ANC’s liberation army and porter of its international solidarity infrastructure, which made the ANC an integral part of the network of liberation movements in the African continent. Resultantly, the struggle for freedom was intensified in Tanzania, Algeria, Ethiopia, Guinea-Bissau, Mozambique, Angola, Namibia and many other parts of the continent.\nPropaganda, ensuring the movement keeps contact with the people, was an important part of the intensification of the struggle. An effective underground movement was equally important for the movement to increase the pool it would recruit from. Operating from Tanzania and later Zambia MK operatives had to go through a hostile terrain to be deployed in the battlefront.\nThe impatience among the trained cadres, keen to engage the enemy, led to the agreement to infiltrate through the then Southern Rhodesia in 1967, what is known as the Wankie and Spolilo encounter with the enemy. This was a bold and risky initiative, which further encouraged the desire among cadres to engage the enemy at home.\nThe rumblings in the camps that came in the aftermath of these expeditions led to the Morogoro Conference where delegates were allowed to be as critical as possible. Following the severe criticism that the leadership was subjected to the NEC resigned en masse to give delegates space to elect new leadership. The Morogoro Conference changed the ANC in two ways: first, it opened membership to all races and, also adopted a militant programme of action.\nIn order to fully understand the Strategy and Tactics one must read the political alliance report of Comrade OR (as he was affectionally known) to this conference, in which he set the agenda as follows:\n“The vital and central task of the African National Congress today is the intensification of armed struggle for the overthrow of the white fascist regime and the liberation of our motherland. Today armed struggle together with other forms of struggle constitute the weapons of the oppressed in our country against the oppressor. How can we intensify the revolution? What forms of organisation can ensure maximum mobilisation of resources at our disposal? What are the motive forces of our struggle and their potential? What strategy and tactics are to be employed?”\nThe Strategy and Tactics that emerged out of the Morogoro Conference characterised the international context as that of “transition to the socialist system, of the breakdown of the colonial system as a result of national liberation and socialist revolutions, and the fight for social and economic progress by the people of the whole world.”\nDescribing the relationship between the political and the military, the Strategy and Tactics explains that “when we talk of revolutionary armed struggle, we are talking of political struggle by means which include the use of military force even though once force as a tactic is introduced it has the most far-reaching consequences on every aspect of our activities. It is important to emphasise this because our movement must reject all manifestations of militarism, which separates armed people’s struggle from its political context.”\nUnder the leadership of O.R Tambo, this clarity kept the ANC intact in trying conditions. It assisted the ANC articulate our struggle and the four pillars of our revolution, the armed struggle, mass mobilisation, underground and international solidarity against apartheid. The momentum that took the struggle to heights in the 1980s was a combination of all these pillars and the coordination thereof.\nThe rent boycott and a series of strikes in 1987 drove the Afrikaner capital to Dakar to test the possibility of a negotiated settlement. It is the total impact of these pillars that forced the nationalist party into negotiating power away, between 1990 and 1994. Looked through biblically eyes, Oliver Reginald Tambo is like Moses who led the people to freedom; and Nelson Rolihlahla Mandela the Joshua who led the people into the Promised Land.\nThe question, however, is not what OR did and achieved, nor what his life and times are about. The question that must be confronted is whether we are upholding the ideals the ANC stood for under the leadership of the Comrade OR Tambo. An appropriate premise is a reminder of the core values of the ANC, namely, selflessness, honesty, respect for leadership, unity, humility, and discipline.\nThe ANC must remain the leader in building a non-racial and non-sexist society. It must remain a mass and a multi-class movement it has always been. The ANC has always led and must continue its leadership of society. A divided ANC loses the moral authority to lead and unite society. The integrity of individual leaders of the ANC determines the extent to which society can continue trusting our movement. No leader has a private life. We must continue engaging various structures and organisations in society, even when there are no elections.\nThe ANC must be the first to talk against corruption, real and perceived. We must support institutions set to fight corruption. The ANC must shed the perception of it being corrupt through taking a tough stance against corruption, irrespective of who is implicated or involved. We must instill and enforce discipline and enable members to appreciate that no revolution can succeed without discipline. Cadres of our movement must always appreciate that militancy without discipline is anarchy.\nWe must be guided by the following principles:\n* Discipline must not be used to settle political differences. However, in the same breath, ill-discipline must not be veiled as political differences.\n* Past errors must not be used to justify continued ill-discipline. Deviant behaviour in the run-up to Polokwane must not be elevated into a norm, but an anomaly that must be corrected.\n* Where transgression occurs the movement must act.\n* We must all work for unity and cohesion.\nThe 53rd National Conference of the ANC will, among other things, elect leadership. It is not about succession as there is heir to the throne. Leadership must be discussed and assessed by branches of the ANC, as opposed to narrow circles that are well resourced and tend to buy their access to power. We must liberate the branches of the ANC to appreciate their power and their rights. Our leadership must be assessed in terms of its performance as a collective, and as individual leaders. It should never be about regionalism and friendship.\nComrades must be made to understand that the ANC has culture and traditions and nominations are made at an appropriate time. By this we wish not ban discussions and assessment of leadership. Oliver Reginald Tambo was a thorough leader who paid attention to detail. As Comrade Thabo Mbeki noted:\n“OR was an intellectual in the best meaning of that word. He was a person of reason, a person of rational thought and rational action… a person who could deal with both the concrete and the abstract, the specific, the particular and the general; between tactics and strategy - that dialectical interaction...”\nAs we move towards the policy conference our branches must bear this in mind, be engaged and empowered to participate in policy debates. The aptly termed nationalisation debate is but one example. We must understand that this policy debate is beyond proponents and opponents. The NGC directed the NEC to look into a number of case studies and analyse how other countries handle the state ownership of mineral resources. The research team has completed eleven case studies and the two remaining are China and Malaysia. The discussion will be scientific and emotional.\nComrade OR invested a fortune in strengthening the alliance. He made us understand that the alliance is not just an agreement signed by leaders in boardrooms, however one forged in the trenches of struggle. The tripartite alliance is an alliance between the national liberation movement and the two working class formations.\nThese partners do not melt into an alliance and lose their class character and ideological outlook. They understand the National Democratic Revolution as the minimum programme. It is the preparedness to compromise that has made our unique alliance to work. We must continue working for the unity and cohesion of the alliance.\nBy: Gwede Mantashe, ANC Secretary General"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:4d6fdf79-652e-4d5c-a0fd-38e8925d9ff2>","<urn:uuid:65260807-e02f-4c61-92e9-832b6b3c3bb2>"],"error":null}
{"question":"What's the difference between national and Kentucky-specific teenage smokeless tobacco usage rates, and what cultural factors influence this in Kentucky?","answer":"The CDC found that 1.6% of high school students nationally use smokeless tobacco, while Kentucky's teenage smokeless tobacco use rate is significantly higher at 17% compared to the national average of 8%. This higher rate in Kentucky is influenced by cultural factors, particularly in rural areas where the habit is often passed down through generations, with young men being offered chewing tobacco by fathers or grandfathers while working on farms, which is considered an honor in these communities.","context":["21 Mar Facts and Statistics of the Top 4 Substances Abused by Teens\nToday marks the 13th anniversary of National Drug and Alcohol Facts Week, an observance launched by the National Institute on Drug Abuse in 2010. The purpose of this event is to inspire dialogue with youth about the science of drug use and addiction. Although it may seem difficult to start a conversation on this topic, we have compiled a list of facts and statistics on the top 4 substances abused by young people that you can use to kick-off a conversation with someone you may know.\nTobacco & Vaping\nMost long-term tobacco users begin using tobacco products during youth and young adulthood. As of 2022, the CDC has found that in American high schools 1.6% of students use smokeless tobacco, 2.0% use cigarettes, 2.8% use cigars, and 14.1% use e-cigarettes. Use of any of the products, including e-cigarettes, is unsafe. Use of smokeless tobacco, such as dips and pouches, increases the risk of oral and esophageal cancer, gum disease, and tooth decay. Cigar and cigarette use increase the risk of lung cancer, heart disease, stroke, and emphysema. And, although most consider it safer, not enough research has been done on vaping to establish risks of long-term use. All these products contain nicotine, which is extremely addictive and harms the developing brain. It can also increase future use of, and addiction to, other drugs.\nClick here for more info on smoking and youth\nAlthough underage drinking is extremely common in the United States, it can pose serious risk. A 2019 study by the CDC concluded that among high schoolers, in one month 29% drank alcohol, 14% binge drank, 5% drove after drinking, and 17% rode with a driver who had been drinking. Underage drinkers are more likely to experience disruption of normal growth/development, memory problems, misuse of other substances, alcohol poisoning, and changes in brain development that can have life-long effects. Underage drinking is also related to adult drinking, with those having drank in their youth continuing to drink excessively into adulthood and that children of binge drinkers are also more likely to drink in adolescents than those whose parents don’t drink.\nClick here for more info on underage drinking\nAlthough experimenting and taking risks in our youth can help foster independence and identity development, some risks that teens may take have the potential to have adverse effects on health and well-being. A 2019 study by the CDC found that 37% of high school students reported long-term use of marijuana and 22% reported use in the past month. Since the adolescent brain continues to develop until around age 25, use of marijuana can have negative effects on the developing brain, such as difficulty thinking/problem-solving, memory/learning problems, reduced coordination, reduced attention span, and increase risk of mental health issues.\nClick here for more info on marijuana use and teens\nPrescription & OTC Drugs\nAlthough often seen as “safer” than illegal substances, prescription and over-the-counter drugs are a faster growing drug problem than anything that can be found on the streets. The 3 main categories of prescription/OTCs are stimulants, opioids, and depressants with each posing different threats to the developing adolescent. Stimulants, like cocaine, may cause paranoia, irregular heartbeat, and dangerously high body temperatures. Opioids, like heroin, cause drowsiness, nausea, constipation, and can have dangerous effects on breathing and heartrate. Depressants can cause slurred speech, shallow breathing, fatigue, and possibly seizures upon withdrawal. Abuse of any of these drugs can fracture developing neural pathways and, since brains are becoming hardwired in adolescence, the pathways we support, like addiction, are the ones that stay around in adulthood.\nClick here for more info on prescription drug misuse and abuse impacting teens","The Surgeon General of the United States officially declared tobacco as harmful to human health in 1964, and despite declines in tobacco use among Americans in the decades since, there are still approximately 1,300 tobacco-related deaths each day in the United States. In Kentucky, smoking rates remain stubbornly high at around 25 percent compared with 15 percent in the nation overall.\nFor many health officials, the best approach to reducing these rates starts with educating children about the dangers of tobacco use. In the documentary Tobacco-Free Kentucky Kids, leaders in Kentucky’s teen smoking prevention field discuss successful public awareness campaigns that are ongoing around the commonwealth.\nThe program also examines teenagers’ use of smokeless tobacco and e-cigarettes, and interviews high school students who are using the TATU (Teens Against Tobacco Use) platform to educate both younger students and adults about smoke-free policies and tobacco cessation. This program is funded in part by a grant from the Foundation for a Healthy Kentucky.\n“We know that 90 percent of people who start smoking, start before the age of 18, before they’re really even legally able to buy cigarettes” says Audrey Darville, PhD, APRN, a tobacco treatment specialist with University of Kentucky HealthCare. “And, by the age of 25 or 26, nearly everyone has either decided not to smoke, and will not smoke going forward in their life, or become tobacco users.”\nKentucky ranks second behind West Virginia in the teenage smoking rate. Tobacco was one of Kentucky’s primary agricultural crops during the 19th and 20th centuries, and in many parts of the state the plant still exerts a hold on the economy and culture.\nBut more than ever before, younger generations of Kentuckians are turning away from tobacco use due to their understanding of the myriad health problems associated with smoking and chewing or dipping. The efforts to spread awareness of tobacco’s harmful effects are being led by both medical professionals and motivated teen advocates who have seen their elder relatives suffer from debilitating diseases after decades of using tobacco.\nTeen Advocates Making a Difference\n“My grandma has smoked my whole life,” says Katelyn McWhorter, a senior at Lincoln Co. High School. “So, I’ve seen the effects of smoking and how bad it can be and the effects it can cause. And so, I just really want to prevent other people from putting that upon their families and upon themselves, because I’ve seen how bad it can get.”\nMcWhorter and some of her fellow high school students in Lincoln County are members of a program called Teens Against Tobacco Use (TATU). The program was developed by the American Lung Association, the American Cancer Society, and the American Heart Association. TATU enlists local health department officials to first educate teens about tobacco addiction, and then supervise them as they lead meetings with students in middle and elementary schools.\nThe TATU model turns high school students into mentors for younger kids, and instructs the teens on the basics of tobacco use and how nicotine addiction works. TATU organizers in Lincoln County and other school districts feel that anti-tobacco messages to young children are more effective coming from their high school peers, who are often seen as role models.\n“It’s so important to teach them at a younger age,” says youth services coordinator at the school, Mindy Stevens. She notes that in the current media-saturated culture, children are being exposed to tobacco products at an early age compared with previous generations. “So we’re trying to reach them now at even kindergarten levels, before they make those decisions, because if we wait until they are in high school, it’s too late, they’ve already made those choices.”\nIn addition to the TATU counseling groups, high school students around the commonwealth are engaging in public relations campaigns targeted at their own peers and even at adults. The Casey Youth Coalition is one example. Motivated by persistent smoking in the high school bathrooms, the students at Casey County High School created a campaign called Photo Voice. For Photo Voice, the students took photographs of smoking’s toll on their high school campus and community, added captions, and then displayed their collection and also posted it online. The campaign has been well-received, and other school groups in Kentucky and from out of state have inquired about copying the format.\nJacob Steward, a sophomore at Bourbon County High School, recently gave a speech at the launch of Smoke-Free Tomorrow, an advocacy group featuring some of Kentucky’s prominent health care organizations. Steward is a member of a group called Students Making a Community Change (SMACC), and his speech in Frankfort drew rave reviews from health officials and politicians alike.\n“This is as much an educational opportunity as it is really a volunteer or service opportunity, in my opinion,” Steward says, “in improving the lives of Kentuckians as well as learning about the dangers and effects of tobacco and the risks that Big Tobacco poses to teenagers.”\nChanging Kentucky’s Tobacco Culture for the Next Generation\nRecent national television and online public service campaigns such as the “Real Cost” campaign from the Food and Drug Administration and the “Finish It” series of PSAs from the Truth Initiative have inspired health officials and teen groups to produce their own media spots. In western Kentucky, officials with the Barren River District Health Department started a Facebook campaign that promotes Kentucky’s Quit Line (1-800-QUIT-NOW, www.quitnowkentucky.org). Meanwhile, tech-savvy teens in the same area have created their own PSAs for local television. Their message: “We don’t have time for tobacco!”\nIncreasingly, advocates are directing their efforts to curb usage rates for both smokeless tobacco and the relatively new product of electronic cigarettes. Kentucky’s rate of teenage chewing tobacco use is 17 percent, compared with the national average of 8 percent. The habit is one that is often passed down through generations in the commonwealth.\n“Smokeless tobacco use invariably leads to what we call ‘poly-tobacco use,’ which is using more than one tobacco product,” says Donald Helme, PhD, associate professor in the Department of Communications at the University of Kentucky. Helme has recently conducted research into what “is a primary rural phenomenon.”\nSmokeless tobacco users are overwhelmingly male, Helme says, and from his many interviews he’s gained an understanding about how ingrained tobacco chewing is within communities. “These young men would be out working on the farm with their father or their grandfather, and he would offer a piece of chew. And they all described [the offer] as an honor.”\nElectronic cigarettes have become popular among teenagers during this decade, and studies into their health effects are just starting to be published. But according to Elizabeth Hoagland, policy analyst with the tobacco prevention program at the Kentucky Department of Public Health, officials already know enough about “vaping” to be very concerned about how it is being marketed to teenagers.\n“E-cigarettes do contain nicotine, which is an addictive product,” she says. “We know that teenagers who start using e-cigarettes are about 30 percent more likely to start smoking conventional cigarettes, so they could be an initiation product, they could be a starter product.”\nIn November, the new 2017 Kentucky Youth Behavior Risk Survey was released, and its data offers fresh, new evidence that the persistent and dedicated efforts by anti-tobacco advocates are working. The survey showed that among high school students in the commonwealth, cigarette and cigar use decreased from 23.4 percent in 2015 to 18.2 percent in 2017. Cigarette use among middle school students dropped from 22.5 percent in 2015 to 12.1 percent in 2017, and e-cigarette use among middle schoolers dropped from 12.1 to 3.9 percent.\n“Really, we are very encouraged,” Hoagland says. “I think what the numbers show is that tobacco control efforts really do work. I think these new lower tobacco use numbers really do reflect the impact of youth efforts in this state. We see time and again that youth are able to move policy and we know that these policies like smoke-free schools, smoke-free communities, they really do reduce youth tobacco use.\n“If we are able to really double down on these efforts, then we can envision a future that is tobacco-free.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4dc07f45-f32f-489f-84ac-b54ce91e51b7>","<urn:uuid:ef23803a-96b0-4166-b55b-7201a50f3515>"],"error":null}
{"question":"¿Cómo progresa el ADHD desde la niñez hasta la edad adulta, and what are the socioeconomic impacts on affected individuals?","answer":"About 30-50% of children with ADHD continue to have symptoms into adulthood. The diagnosis requires attention to early development and symptoms of inattention, distractibility, impulsivity and emotional lability. For adults, symptoms include hyperactivity, poor concentration, affective lability, hot temper, inability to complete tasks, disorganization, stress intolerance, and impulsivity. These symptoms can lead to various socioeconomic challenges, including trouble maintaining employment, difficulties managing finances, and potential legal issues due to poor impulse and emotion control. Additionally, ADHD can affect marriage relationships and create workplace problems that may require specific accommodations and management strategies.","context":["Diagnosing ADHD in Adults\nApproximately 50-percent of children with ADHD become ADHD adults. Find out about diagnosis and treatment of ADHD in adults.\nADHD or Attention Deficit Hyperactivity Disorder affects thirty to fifty percent of adults who had ADHD in childhood. Accurate diagnosis of ADHD in adults is challenging and requires attention to early development, and symptoms of inattention, distractibility, impulsivity and emotional lability.\nDiagnosis is further complicated by the overlap between the symptoms of adult ADHD and the symptoms of other common psychiatric conditions such as depression and substance abuse. While stimulants are a common treatment for adult patients with ADHD, antidepressants may also be effective.\nADHD receives considerable attention in both medical literature and the lay media. Historically, ADHD was considered to be primarily a childhood condition. However, recent data suggest that symptoms of ADHD continue into adulthood in up to fifty percent of persons with childhood ADHD.\nSince ADHD is such a well-known disorder, adults with both objective and subjective symptoms of poor concentration and inattention have got the probabilities for evaluation. While the symptoms of ADHD have been extended developmentally upward to adults, most of the information about the etiology, symptoms and treatment of this disorder comes from observations of, and studies in, children (Weiss, 2001).\nDiagnosis of Adult ADHD\nFor several reasons, family physicians may be uncomfortable evaluating and treating adult patients with symptoms of ADHD, particularly those without a previously established ADHD diagnosis. First, the criteria for ADHD are not objectively verifiable and require reliance on the patient's subjective report of symptoms. Second, the criteria for ADHD do not describe the subtle cognitive-behavioral symptoms that may affect adults more than children.\nThe family physician's role as diagnostician is further complicated by the high rates of self-diagnosis of ADHD in adults. Many of these persons are influenced by the popular press. Studies of self-referral suggest that only one third to one half of adults who believe they have ADHD actually meet formal diagnostic criteria.\nEven as family physicians are knowledgeable about childhood ADHD, there is a noticeable absence of guidelines for primary care evaluation and treatment of adults with symptoms of the disorder (Goldstein and Ellison, 2002).\nThe diagnostic criteria describe the disorder in three subtypes. The first is the predominantly hyperactive, the second is the predominantly inattentive, and the third is a mixed type with symptoms of the first and the second.\nSymptoms should be persistently present since age seven. While a longstanding symptom history is often difficult to elicit clearly in adults, it is a key feature of the disorder.\nThe following are the symptoms:\nInattention: where a person often fails to give close attention to details or makes careless mistakes, often has difficulty sustaining attention in tasks, often does not seem to listen when spoken to directly, or often does not follow through on instructions.\nTasks: Where a person often has difficulty organizing tasks and activities, often avoids, dislikes or is reluctant to engage in tasks that require sustained mental effort, often loses things necessary for tasks or activities, often easily gets distracted by extraneous stimuli, or is often forgetful in daily activities.\nHyperactivity: Where a person often fidgets with hands or feet or squirms in seat, often feels restless, often has difficulty engaging in leisure activities quietly, or often talks excessively.\nImpulsivity: Where a person often blurts out answers before questions have been completed, or often interrupts or intrudes on others.\nThere is a growing consensus that the central feature of ADHD is disinhibition. Patients are unable to stop themselves from immediately responding, and they have deficits in their capacity for monitoring their own behavior. Hyperactivity, while a common feature among children, is likely to be less overt in adults. Utah criteria may be called the imperative criteria for this. For adults, it is used like this: What is the childhood history consistent with ADHD? What are the adult symptoms? Does the adult have hyperactivity and poor concentration? Is there any affective lability or hot temper? Is there the inability to complete tasks and disorganization? Is there any stress intolerance, or impulsivity? (Wender, 1998)\nWender developed these ADHD criteria, known as the Utah criteria, which reflect the distinct features of the disorder in adults. The diagnosis of ADHD in an adult requires a longstanding history of ADHD symptoms, dating back to at least age seven. In the absence of treatment, such symptoms should have been consistently present without remission. In addition, hyperactivity and poor concentration should be present in adulthood, along with two of the five additional symptoms: affective lability; hot temper; inability to complete tasks and disorganization; stress intolerance; and impulsivity.\nThe Utah criteria include the emotional aspects of the syndrome. Affective lability is characterized by brief, intense affective outbursts ranging from euphoria to despair to anger, and is experienced by the ADHD adult as being out of control. Under conditions of increased emotional arousal from external demands, the patient becomes more disorganized and distractible.\nTreatment of Adult ADHD\nSome treatments for ADHD in adults are as follows:\nStimulants: Stimulants work by increasing both blood flow and the levels of Dopamine in the brain, especially the frontal lobes where the brain's Executive Functions take place. Stimulants will increase the brain's ability to inhibit itself. This allows the brain to focus on the right thing at the right time, and to be less distracted, and less impulsive. Stimulants increase the \"signal to noise ratio\" in the brain.\nAntidepressants: Antidepressants are considered a second choice for treatment of adults with ADHD. The older antidepressants, the tricyclics, are sometimes used because they, like the stimulants, affect norepinephrine and dopamine.\nOther Medications: Sympatholytics have also been used in the management of ADHD as well as the non-stimulant ADHD medication, Strattera.\nSelf-Management Strategies: Adults with ADHD benefit considerably from direct education about the disorder. They can use information about their deficits to develop compensatory strategies. Planning and organization can be improved by encouraging patients to make lists and use methodically written schedules.\nWender, Paul (1998). Attention-Deficit Hyperactivity Disorder in Adults . Oxford University Press.\nWeiss, Margaret (2001). Adhd in Adulthood: A Guide to Current Theory, Diagnosis, and Treatment . Johns Hopkins University Press.\nGoldstein, Sam; Ellison, Anne (2002). Clinicians' Guide to Adult ADHD: Assessment and Intervention. Academic Press.\nStaff, H. (2007, June 6). Diagnosing ADHD in Adults, HealthyPlace. Retrieved on 2020, October 21 from https://www.healthyplace.com/adhd/articles/diagnosing-adhd-in-adults","ADD-ADHD Medical Reference\nHow Do Socioeconomic Disadvantages Impact ADHD?\n- Vocational Rehab for Adult ADHD\nJust because you have adult ADHD doesn’t mean you can’t land a great job. Learn more about how this service can help.\n- Can ADHD Get Worse?\nLearn how aging, substance use, and environment can impact your disorder.\n- How ADHD May Lead to Trouble With the Law\nLearn why ADHD symptoms like poor impulse and emotion control can lead to legal issues.\n- Adult ADHD and PTSD: What’s the Link?\nResearchers are learning more about the connection between PTSD and ADHD. If you have one condition, you’re more likely to also have the other.\n- Adult ADHD and Childhood Trauma: Is There a Link?\nWe don't know what causes ADHD. But there’s a strong association between childhood trauma and adult ADHD. Here’s a deeper look at the connection.\n- How ADHD Can Affect Your Marriage\nADHD can cause a wide range of problems for couples. But there are ways to manage it together and build a deeper relationship.\n- Adult ADHD: What Are Functional Impairments?\nYour symptoms can lead to problems that affect you at home, work, and socially. Here’s what you need to know.\n- Adult ADHD and Internet Addiction\nIt’s possible to get hooked on the internet, especially if you have untreated ADHD. Here’s how to stop it from taking a major toll on your life.\n- ADHD and Your Menstrual Cycle\nIf you have ADHD and also have periods, each of these things can impact the symptoms of the other. Find out why and how you can manage these effects.\n- Managing Your Finances With ADHD\nManaging money can be a challenge for anyone, but having ADHD throws extra twists into the mix. Follow these tips to stay on top of your financial game.\n- Diagnosing ADHD and Autism in Adulthood\nThe symptoms of ADHD and autism often overlap, so it’s no surprise that some people aren’t diagnosed until adulthood. Here’s what to look for.\n- How to Manage an Employee With ADHD\nWorking with employees with ADHD? Learn ways to help them succeed at work.\n- Parenting When You Have ADHD\nLearn ways to overcome your challenges and thrive as a parent.\n- Adult ADHD: Statistics and Facts\nFind out who’s most likely to be diagnosed and treated with ADHD as an adult.\n- Tips to Organize Your Home With ADHD\nTry these strategies to tame clutter and keep your home in order.\n- ADHD and Menopause\nCan menopause make your ADHD symptoms worse?\n- Is Memory Loss ADHD, or Something Else?\nADHD can make you forgetful, but so can a lot of other things. Find out whether your memory loss is part of your ADHD or a sign of a more serious problem.\n- Adult ADHD and Burnout\nExhausted? Irritable? Failing at work? If you have ADHD, you may have burnout. Here’s how to break the ADHD burnout cycle.\n- Does ADHD Cause Antisocial Personality Disorder?\nCan ADHD cause you to develop antisocial personality disorder? Learn whether there’s a link between these conditions.\n- Common Workplace ADHD Problems and How to Fix Them\nADHD can create problems for adult employees. Learn ways to manage ADHD at work with practical tools and strategies, and when to ask for accommodations.\n- Amino Acids for ADHD\nSome studies have looked at amino acid levels and amino acid supplements in ADHD. Overall, the evidence that ADHD supplements will help with adult ADHD is lacking.\n- Adult ADHD in Black, Indigenous, and People of Color\nWhite people are diagnosed with ADHD much more often than people who are Black, Indigenous, or of color. Learn why and what can be done to bridge the gap.\n- What Are Noradrenergic Medicines for ADHD?\nNoradrenergic agents for ADHD are medicines that change levels of brain chemicals to improve attention, memory, and other ADHD symptoms.\n- Can Adult ADHD Go Into Remission?\nWhat’s known about adult ADHD remission and what it means for people with ADHD.\n- Tips to Improve Your Social Skills With ADHD\nADHD symptoms like inattention and hyperactivity might make it difficult to respond to social cues. Here are some tips to master your social skills.\n- What Does It Mean to ‘Mask’ Adult ADHD?\nWhen you “mask” adult ADHD, it means you hide your symptoms from other people. Here are the pros and cons of doing that.\n- ADHD: Medications, Alcohol, and Marijuana\nADHD can make you more likely to misuse alcohol and marijuana. Learn more about why, what the possible side effects are, and how to get help.\n- ADHD: Latest Research\nHere’s what some of the most recent research says about ADHD symptoms, diagnosis, treatment, and more.\n- What’s Going On With My Child’s Behavior?\nLearn how to tell whether it’s normal rowdy kid stuff or the signs of ADHD.\n- Why Can’t I Focus?\nCertain habits and health issues can shorten your attention span. Learn what to look out for and how to get back on track.\n- How to Manage ADHD Medication Rebound\nChildren who take ADHD medication may have periods where their symptoms begin to flare. Doctors call this a “medication rebound,” which happens when your child’s brain reacts to their medicine wearing off. Find out what the symptoms of medication rebound are, why it happens, and how to avoid it.\n- Living Well With Adult ADHD\nHealthy habits in your daily life can help you better manage the symptoms of ADHD.\n- The Costs of Adult ADHD\nADHD can come with high out-of-pocket costs, both direct costs of care and additional costs for therapy and coaching if you choose to get them.\n- ADHD in Older Adults\nThere’s little knowledge on how ADHD affects older adults. But getting an accurate diagnosis can improve your quality of life.\n- When Your ADHD Meds Stop Working\nADHD meds work for most people, but not everyone. Learn why they may stop working and what you should do if that happens.\n- Newly Diagnosed With Adult ADHD? Here’s What to Know\nA diagnosis of adult ADHD brings many questions. Learn what you can expect and how to get treatment and support.\n- ADHD in Women\nThere’s little knowledge on how ADHD affects adult women. Experts say undiagnosed and untreated ADHD in women can lower their quality of life.\n- ADHD: How to Be More Attentive\nLearn the many simple habits you can form to increase your focus when you have ADHD.\n- ADHD: How to Ask for the Support You Need\nIf you have ADHD, you may tend to try to manage things on your own, or maybe you know that you need help but don’t know where to start. Learn about how to talk with your loved ones about ADHD and where to find community support, both in-person and online.\n- Ritalin vs. Strattera for ADHD\nStrattera and Ritalin are medicines children and adults can take to manage their ADHD symptoms. They work in different ways and have different side effects.\n- Misdiagnosed ADHD: Conditions With Overlapping Symptoms\nSometimes ADHD symptoms are actually something else. Learn how these and symptoms of other conditions can overlap.\n- A Brief History of ADHD\nThe condition we now call ADHD was first identified in 1798 by Sir Alexander Crichton. Learn more about the history of ADHD and its treatments.\n- Adderall and Sleepiness\nAdderall is a stimulant that can help with symptoms of ADHD, but you may feel tired or even sleepy when you take it. Find out why and how you can manage this surprising side effect.\n- ADHD Facts and Statistics\nYou might know someone who has ADHD. But how much do you know about this most common childhood brain disorder? Find out how it affects adults, famous people who have it, and other facts and statistics.\n- Can EEG Diagnose ADHD?\nSome doctors use EEG to help diagnose ADHD in children. Called NEBA, here’s how it works and why it’s somewhat controversial.\n- Complementary and Alternative Treatments for ADHD\nAre there nonmedication options that can help ADHD symptoms? Find out what the science says.\n- Is ADHD Causing Your Sexual Problems?\nResearch suggests that about 40% of men and women with ADHD will have some sexual problems. But there’s a lot you can do to manage any troubles you have.\n- Home Delivery of ADHD Medication\nIt’s convenient and may even save you money. Could home delivery of your attention deficit hyperactivity disorder (ADHD) medication be right for you?\n- ADHD and Dyslexia: How to Tell Them Apart\nADHD and dyslexia are brain disorders with some common links. But they differ in key ways. Here’s how to tell them apart."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7ad58892-aefc-4533-859f-0a14927f9c9b>","<urn:uuid:aa4ccd6d-a6e2-4479-9da6-b8f0a43b5bee>"],"error":null}
{"question":"Why is studying Chlamydomonas cell division relevant to human health?","answer":"Studying Chlamydomonas cell division is relevant to human health because some of the same molecular mechanisms controlling cell division in Chlamydomonas are also found in humans. Additionally, understanding cell division decisions is important because cancer and other human diseases of cell proliferation occur when these decisions are made incorrectly. Cell size is often impacted by proliferative diseases, though the relationship between cell size and abnormal cell physiology is not fully understood.","context":["UD professor part of NIH grant to understand mechanisms controlling cell division\nAbhyudai Singh, an associate professor in electrical and computer engineering, biomedical engineering, and mathematical sciences at the University of Delaware, is part of a new three-year, $675,000 grant from the National Institutes of Health (NIH) to understand cell-size control in the unicellular green alga Chlamydomonas reinhardtii (Chlamydomonas). This is a joint project with the Donald Danforth Plant Science Center, one of the world’s largest independent plant science institutes.\nSingh, an expert in mathematical modeling, will work with James Umen, Ph.D., Joseph Varner Distinguished Investigator and member, Enterprise Rent-a-Car Institute for Renewable Fuels at the Danforth Center, who is an expert in algal cellular and molecular biology.\nThe grant will enable the Umen and Singh research groups to begin unraveling the “counting” mechanism that underlies Chlamydomonas cell division, in which a period of uninterrupted growth (by as much as twenty-fold in size) is followed by a series of rapid successive divisions to produce daughter cells that return to a uniform starting size. For this mechanism to operate properly, larger mother cells must “count out” more divisions than smaller mother cells; but the division system exhibits imperfections. For example, two different Chlamydomonas mother cells of the exact same size won’t always execute the exact same number of divisions; but the reasons for differing cell behaviors in the face of seemingly identical starting conditions are unknown.\n“Diverse organisms from bacteria to humans employ size control strategies to ensure that their cells do not become abnormally large (or small),” said Singh. “This grant facilitates a joint computational-experimental collaboration to uncover size control principles in a unique model system, the unicellular alga Chlamydomonas reinhardtii. For the first time, real-time cell size tracking of individual cells will be performed together with expression measurements of key size regulators. The overall goal is to combine these single-cell assays with mechanistic mathematical models to elucidate biomolecular circuits mediating size control in Chlamydomonas.”\nAlthough human cells don’t divide exactly the same way as Chlamydomonas, they also must make yes/no decisions about cell division that directly impact human health and disease. For example, cancer and other human diseases of cell proliferation occur when cell division decisions are made incorrectly. Cell size is often impacted by proliferative diseases, but it is not clear how cell size relates to abnormal cell physiology. In addition, some of the same molecular mechanisms controlling cell division in Chlamydomonas are also found in humans and in plants, so this research may yield insights into how cell division is controlled in more complex organisms where it is more difficult to study the impacts of noise on cellular decision making.\n“Noise or stochasticity in decision making permeates biology, but the sources of noise and how they interact with more deterministic aspects of cellular control systems are poorly understood,” said Umen. “This grant will enable us to model and better understand how noisy decision-making impacts cell division in Chlamydomonas, and may also provide insights into other areas of biology that are impacted by stochastic behavior.”\nThe grant will also support an educational component aimed at exposing students to topics at the interface of mathematics and systems biology through collaborative training of graduate students, summer workshops at the University of Delaware, and the National Science Foundation funded Research Experience for Undergraduates summer internship at the Danforth Center."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3c0bf16c-1a8d-4835-8dbb-da606fdb7d77>"],"error":null}
{"question":"Which one start earlier: P5 Process or Treaty of Pelindaba?","answer":"The P5 Process started earlier, being created in 2009, while the Treaty of Pelindaba opened for signature in 1996 and entered into force in 2009. Despite being signed earlier, the Treaty of Pelindaba took longer to come into effect, coincidentally reaching enforcement the same year the P5 Process was established.","context":["The once-every-five-years review conference of the 191-member Nuclear Non-Proliferation Treaty (NPT) will start in April in New York. One of the most divisive issues at the conference will be the lack of sufficient progress by Nuclear Weapons States (NWS) toward fulfilling their legal obligation on nuclear disarmament. Against the background of an increasingly intensive nuclear arms competition among the major powers, the collapsing of existing arms control treaties, and the simmering crises around North Korea and Iran, the five NWS, who are also the permanent members of the U.N. Security Council, must exercise leadership to safeguard a stable nuclear order.\nThe NWS has created the P5 Process in 2009 to discuss steps to implement their NPT obligations, and especially to promote disarmament through transparency and confidence-building measures. This mechanism is being under-used, but has potential to make greater contribution to arms control.\nNWS have generally argued that it is the responsibility of all countries to improve the international security environment so as to create necessary conditions for nuclear disarmament. But Non-Nuclear Weapons States (NNWS) also have a point in noting that the existence of nuclear weapons has continued to poison the international security environment. The 2017 deployment of a THAAD missile defense system in South Korea caused serious Chinese concern about this system’s potential capability to undermine China’s nuclear deterrent against the United States, and subsequently led to the most serious crisis in Beijing-Seoul bilateral relationship in decades. This example demonstrates how international struggles over nuclear issues can spill over into non-nuclear security domains and derail relations not only between NWS but also between NWS and NNWS. Therefore, in addition to an effort by all to improve the international security environment, NWS need to work simultaneously on reducing the salience of nuclear weapons in national security.\nAs a first step to do so and to address the potential humanitarian consequences of nuclear weapon use, NWS should discuss how they can align their nuclear policies with the law of armed conflict at P5 meetings. They should seek to apply the basic principles of discrimination, proportionality, and military necessity, in order to prevent excessive targeting policies from legitimizing oversized arsenals and escalatory employment strategies. NNWS, especially those under the nuclear umbrella of NWS, also have homework to do. They need to reexamine and readjust their defense strategies to ensure their national security is not dependent on the first use of nuclear weapons by their nuclear allies in conventional conflicts. By creating the conditions for the universal adoption of no first use policy, NNWS can help create a world with less nuclear risk.\nNWS deserve some credit for trying to reach out to the rest of the international community, including to host a P5 side event at the upcoming NPT review conference. NNWS should use these opportunities to drive home the point that, although short of complete disarmament, to continue scaling down existing nuclear arsenals is important. Countries like South Korea and Japan, despite their reliance on the U.S. extended nuclear deterrence, would benefit from global nuclear reductions and thus can play a special role in calling for the maintenance of deterrence and security with smaller arsenals.\nDue to the largely dysfunctional bilateral strategic security talks between NWS and the lack of multilateral arms control dialogues, the P5 Process has a unique responsibility to address the growing risk of nuclear arms race. The recently concluded P5 meeting in London committed the NWS to advancing the goal of ending the global production of fissile materials which are indispensable for building nuclear bombs. If the five NWS can take the lead by declaring a joint moratorium on fissile material production, that would impose a cap on their future potential to build up nuclear forces and thus serve as a concrete first step toward containing an arms race.\nNortheast Asia is an area of particular concern regarding nuclear stability. North Korea is leveraging the growing great power competition to advance its nuclear agenda. The five permanent members of the Security Council have a special responsibility to prevent their divergent geopolitical interests from obstructing an international united front against North Korea’s nuclear ambition. The P5 meetings can serve as a less formal and less political platform than the Security Council for the leading powers to coordinate policy. They should start substantive discussions on maintaining pressure on Pyongyang, building consensus on key elements of a denuclearization roadmap, and establishing conditions and mechanisms to reciprocate North Korean cooperation. As U.S.-North Korea bilateral talks stall, it is time for the major nuclear powers to collectively assert their leadership.","In Praise of the Non-Nuclear World\nDuring the 2019 UN General Assembly, speakers representing multiple state organizations opened a meeting of the UN First Committee, which has focused on the range of disarmament issues since the start of the United Nations. The first speaker represented the members of the Non-Aligned Movement (NAM)—a forum of 120 world states that are not formally aligned with or against any major power bloc—and following them were a range of speakers representing the nuclear-free zones of the world. A consistent message from these speakers was strong support for the Treaty on the Prohibition of Nuclear Weapons and encouragement of their memberships to sign and ratify this treaty as soon as possible.\nThis message prompted two questions:\nWhat exactly are nuclear-free zones?\nHow many countries are we talking about who want to outlaw the production, transportation, storage, stockpiling, and possible use of this classification of weaponry?\nThe establishment of Nuclear-Weapon-Free Zones (NWFZ) is “a regional approach to strengthen global nuclear non-proliferation and disarmament norms and consolidate international efforts towards peace and security.” To fully answer the first question however, we need some history: How did this concept of a zone without nuclear weapons begin? Following the end of World War II, Poland offered the first proposal: the Rapacki Plan, named after the Polish foreign minister who introduced the plan to the UN General Assembly in 1958.\nAccording to a history of Nuclear-Weapon-Free Zones, “The Rapacki Plan sought to initially keep nuclear weapons from being deployed in Poland, Czechoslovakia, West Germany, and East Germany, while reserving the right for other European countries to follow suit.” A September 2019 article from the Wilson Center’s blog on history and public policy describes that: “For Poland, the Rapacki Plan offered enormous benefits. It would remove U.S. tactical nuclear weapons stationed in West Germany and ensure that West Germany did not receive intermediate range nuclear weapons under potential North Atlantic Treaty Organization (NATO) sharing agreements… Overall, the West believed that the Rapacki Plan could destroy NATO and imperil the foundations of Western security. Western states officially rejected the Rapacki Plan by mid-1958.”\nAs noted in the history of Nuclear-Weapon-Free Zones cited above, “The Soviet Union, Sweden, Finland, Romania, and Bulgaria also floated similar proposals. All these early efforts, however, floundered amidst the U.S.-Soviet superpower conflict,” despite support by persons like Charles deGaulle, the Canadian government, and individuals in the Eisenhower government, “although the Rapacki Plan would serve as a model to the nuclear-weapon-free zones that were eventually set up in other regions of the globe.”\nAs time went on, the concept of a zone with no nuclear weapons gained acceptance. Article VII of the Nuclear Non-Proliferation Treaty (NPT) of 1970 states: “Nothing in this Treaty affects the right of any group of States to conclude regional treaties in order to assure the total absence of nuclear weapons in their respective territories.” The UN General Assembly reaffirmed that right in 1975 and outlined the criteria for such zones. However, within these nuclear-weapon-free zones, countries were permitted the use of nuclear energy for peaceful purposes.\nThe following treaties form the basis for the existing NWFZs and will help in addressing the second question:\nTreaty of Tlatelolco — Treaty for the Prohibition of Nuclear Weapons in Latin America and the Caribbean was the first such treaty to establish an NWFZ. It opened for signature on February 14, 1967 and entered into force October 23, 2002, with 21 ratifications in the first year and 33 countries participating by 2002. (The treaty specified that the full zone would not enter into force until it was ratified by all states within the zone. That did not occur until Cuba ratified the treaty in 2002. However, the treaty permitted individual states to waive that provision and declare themselves bound by the treaty, which many did beginning in 1968.)\nTreaty of Rarotonga — South Pacific Nuclear Free Zone Treaty opened for signature on August 6, 1985 and entered into force on December 11, 1986. Ten countries including Australia and New Zealand ratified the Treaty that first year and a total of 13 countries are now participating.\nTreaty of Bangkok — Treaty on the Southeast Asia Nuclear Weapon-Free Zone opened for signature on December 15, 1995 and entered into force on March 27, 1997. All 10 nations signed this treaty in 1995 and soon thereafter ratified it.\nTreaty of Pelindaba — African Nuclear-Weapon-Free Zone Treaty opened for signature on April 11, 1996 and entered into force on July 15, 2009. Fifty nations out of 52 signed onto this treaty within two years, with enough ratifications to validate the Treaty by 2009.\nTreaty on a Nuclear-Weapon-Free Zone in Central Asia — opened for signature on September 8, 2006 and entered into force on March 21, 2009. Five states signed on in the first year and in less than three years all had ratified the Treaty.\nThat makes 108 states involved in the nuclear free zones so far. Some of these overlap with the 120 members of the non-aligned movement. But this shows that a substantial portion of this planet is composed of countries that have already indicated their non-nuclear status and their willingness to maintain it.\nEfforts began and have continually been undertaken, without success, to form a Middle East Nuclear-Free Zone. Members at the UN First Committee meeting in October 2019 supported an upcoming conference to discuss creating a Middle East Weapons of Mass Destruction Free Zone (ME WMDFZ). This November conference is funded by the European Union and is considered critical for the 2020 Review Conference of the UN Nuclear Non-Proliferation Treaty. There are obvious obstacles to creating the Middle East Nuclear-Free Zone: the open secret of Israel’s nuclear arsenal, and the nuclear weapon aspirations—and possible achievements—of Syria, Turkey, Saudi Arabia, and Egypt.\nNevertheless, the objectives for the current undertaking, to be carried out over three years, are as follows:\nTo fill an important research gap related to how the issue of the ME WMDFZ evolved, including lessons for current and future prospects;\nTo build analytic capacity to support new thinking on regional security issues in the zone including drawing on lessons from the establishment of other nuclear free zones;\nTo collect ideas and develop new proposals on how to move forward on this issue;\nAnd to foster inclusive dialogue among experts and policymakers on regional issues and the zone, which in turn could contribute to ongoing multilateral processes.\nAs far as enforcement of NWFZs, the International Atomic Energy Agency has the authority to ensure that no country within the nuclear free zone has any illegal weapons. All nations within the zone must agree to inspections under comprehensive safeguards administered by the Agency.\nGenerally the duration of entry into a zone is unlimited, however if a country wishes to withdraw, a 12-month notice is required. The nuclear weapon states are encouraged to sign a protocol, specifying their respect of the terms of the NWFZ treaty. However, some countries have refused to sign such a protocol and instead have indicated that they will use nuclear weapons against a non-nuclear country in the cases of special circumstances, like their use of chemical or biological weapons against the nuclear country.\nThe United Nations has recommendations and guidelines for Nuclear-Weapon-Free Zones which can be found near the bottom of this webpage that also describes NWFZ treaties pertaining to the Antarctic, Outer Space, the Moon, and Seabeds.\nThere are many aspects to helping our planet disarm meaningfully. The countries indicated in this blog show how these nations as a whole are deliberately making a concerted effort to create a nuclear free planet and that alone needs to be applauded and the trend encouraged. It is the dream that the supporters of the Treaty on the Prohibition of Nuclear Weapons envision"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2a6650d6-71d9-42a0-b0f5-401dc8d8892e>","<urn:uuid:21254449-7c54-48f2-ab12-196f2793fb6d>"],"error":null}
{"question":"What were the strategies and outcomes of the first large-scale operations conducted by Germany in Poland (September 1939) versus those of Italy's invasion of Egypt (September 1940)?","answer":"Germany's invasion of Poland in September 1939 employed Blitzkrieg tactics with massive Luftwaffe strikes destroying vital communications, while Panzer and motorized divisions made deep penetrations into Polish defenses. By September 7, German forces had reached the Narev river and captured Cracow, showing rapid advancement. In contrast, Italy's invasion of Egypt in September 1940 was far less successful. Despite having 80,000 men in the Italian Tenth Army against Britain's 36,000-strong Western Desert Force, the Italian advance stopped after just sixty miles at Sidi Barrani where they set up fortified camps. This limited advance was disappointing for British Lieutenant-General Richard O'Connor, who had hoped to annihilate them if they moved further. The contrasting outcomes highlighted the different levels of military effectiveness between German and Italian forces.","context":["With the fall of France and Italy’s entry into the war on 10 June 1940, British expectations that Italy would ally itself with Nazi Germany were realized. New theatres of operations became a definite concern for the British as territorial possessions, dominions and protectorates came under threat from a new enemy which saw British forces and territories overseas as its primary targets. The Italian Duce, Benito Mussolini, fearful that the war might end without his Fascist empire becoming a reality, hoped to occupy and annex French and British colonies and Egypt, which was a British protectorate. This would give him control of the Suez Canal and considerable influence on world trade, allowing Italy to dominate the central and eastern Mediterranean.\nTheir nation’s entry into the war was both surprising and unwelcome to many Italians. The most senior commanders knew just how ill-prepared the country was. The reaction of many was one of surprise and despondency – indeed, as Tenente Paolo Colacicchi of the Granatieri di Sardegna recalled:\nIn fact, of no enthusiasm at all. Marshal Balbo, who was the governor of Libya and one of the four Fascists – the Quadrumvirs of Italy – was apparently playing billiards when the news came through Italy had declared war and he was so angry that he picked up the billiard balls and smashed all the glasses in this billiard room. He was absolutely furious because he knew the position there and he had a lot of friends in Egypt among the British.\nItalo Balbo was dead less than a month later, killed when his plane was shot down by Italian anti-aircraft fire whilst over Tobruk harbour, but Maresciallo d’Italia Pietro Badoglio, of the Italian Comando Supremo, continued to counsel against Mussolini’s annexationist ambitions. There was little military ardour amongst the Italian troops in Tripolitania and Cyrenaica either, as Paolo Colacicchi confirmed:\nI was commanding a platoon then in a machine-gun battalion on the Tunisian front and I was told to call my men and tell them we were at war now with France (this was a few days before France gave up) and Britain. And the main reaction amongst my men was ‘What about our mail? Aren’t we going to hear from home anymore?’ Which is symptomatic, I think, of the type of man we had there. These were not all young; they were not even good troops. There were some recruits, but there were very few and they were tired and wanted to be home. They were thinking of home, they were thinking of leave and they were thinking of their fields left unattended. They certainly had no imperial or aggressive dream about them. This was one of the problems.\nDespite this lack of will amongst many of his subjects, the Duce was not to be denied. In August 1940, Italian forces occupied the British protectorate known as British Somaliland, and Mussolini turned his attention to Egypt, defended by a small, but relatively modern (by comparison with its opponent), Western Desert Force. This consisted of 7th Armoured and 4th Indian Divisions with elements of 6th Division from Palestine. Commanded by Lieutenant-General Richard O’Connor, it comprised approximately 36,000 men. On 9 September 1940, Maresciallo d’Italia Rodolfo Graziani, who had replaced Balbo as commander of Italy’s North African forces, was finally persuaded to invade Egypt with the X Armata (Italian Tenth Army) of 80,000 men. After an advance of sixty miles, the Italian force (chiefly composed of unmotorized infantry formations) stopped at Sidi Barrani and set up fortified camps. This was disappointing for O’Connor who had plans to annihilate them if they moved on Mersa Matruh.3 The Commander-in-Chief Middle East, Lieutenant-General Sir Archibald Wavell, resisted pressure from Prime Minister Winston Churchill to launch an immediate counter-attack. Instead, he and O’Connor began planning an unconventional all-arms surprise attack on the Italian camps.\nThe plan depended on co-operation from the aircraft of Air Headquarters Western Desert and especially from Air Commodore Raymond Collishaw’s No. 202 Group. Despite his aircraft chiefly being obsolete Gloster Gladiators and increasingly obsolescent Bristol Blenheims operating with no radar and an unreliable signals network, Collishaw’s guiding principle was that obtaining and retaining air superiority were essential before any other task, even close support for troops in an advance or retreat, could be attempted with any reasonable hope of success. Nevertheless, he and O’Connor forged excellent relations. Chiefly, aircraft were to ensure that the initial advance of almost seventy miles by O’Connor’s force was not detected and reported by Italian reconnaissance planes.\nAlthough O’Connor was not a tank officer and had never worked with large armoured forces, this was not a barrier to his successful use of the tanks Although O’Connor was not a tank officer and had never worked with large armoured forces, this was not a barrier to his successful use of the tanks available to him. There was considerable experience in his command of military exercises in desert conditions, which the British had acknowledged for many years as ideal for armoured warfare. In the inter-war years several formations trained there and, in 1938, Major-General Percy Hobart had trained the ‘Mobile Division’ (7th Armoured Division’s predecessor) in modern armoured warfare theory. Although Hobart was no longer in command by late 1940, 7th Armoured’s training put it in good stead for the role envisaged by O’Connor, as O’Connor himself explained:\nThe ‘Infantry’ tanks from their name were there to assist the infantry’s advance and help them in every possible way and they were obviously used for that. The 7th Armoured Division with its much larger radius of action could be used in a way – especially in this fine desert country – for getting behind and cutting off troops – in fact in the way strategic cavalry used to be used.\nO’Connor’s unsophisticated approach to the operation was based rather more on common sense than military theory:\nIt’s quite true I had read most of [Basil] Liddell Hart’s ideas in his books but at that time the ordinary officer of my ‘height’ in the army didn’t really have any great reason for adopting his point of view. We had our own regulations, our own instructions and I don’t think that I considered very greatly Liddell Hart’s any more than I considered our own Field Service Regulations. In our very small operation, I can’t think I said to myself, ‘Now, what would Liddell Hart have done?’\nOn 9 December 1940, after a long and difficult approach march, shielded by the light reconnaissance units of 7th Armoured and Collishaw’s aircraft, O’Connor’s infantry with Matilda heavy tanks in close co-operation attacked and routed the Italians. Within two days, 38,000 prisoners, seventy-three tanks and 237 guns had been captured.6 Soon Bardia and Tobruk had fallen. The Italian forces fell back into Libya but were harried all the way and eventually outflanked and trapped. This culminated in a further heavy Italian defeat on 5 February 1941 at Beda Fomm and the surrender of X Armata with the loss of 130,000 prisoners, 400 tanks and 1,200 guns during the campaign. British losses were 1,744 killed, wounded or missing. O’Connor, having reached El Agheila, was for pushing on into Tripolitania in the hope of completely driving the Italians from Libya, despite being at the end of considerably extended and, therefore, attenuated supply lines. Wavell prevented him from doing so.\nThe campaign was a masterpiece of all-arms co-operation based on established principles and was possible because of the quality of the highly trained forces at O’Connor’s command. In this regard it harked back in many ways to the later battles on the Western Front in 1918. It was the high water mark of British military operations in the Western Desert for over eighteen months but, for many, it was the radix malorum of all subsequent failings in those operations. This was through the inappropriate application of its lessons, through the slavish adherence of first 7th Armoured and then other armoured formations to an erroneous tactical doctrine, and because it gave the misleading impression that all Italian formations could be easily overcome in battle.\nThis critique presupposes that circumstances would have allowed a different approach. They did not. Just as the British Expeditionary Force (BEF) in the Great War had struggled to inculcate ‘the lessons of the fighting’ in its forces whilst engaged in almost continuous conflict on the Western Front, with few opportunities for meaningful tactical training schemes incorporating, for example, tanks and artillery, so circumstances dictated possibilities for the British Eighth Army (as Western Desert Force became known on expansion to a two-corps organization in September 1941). There was simply no time to review the lessons of the offensive at the level of detail required before events intervened. The strategic situation in the Mediterranean and Africa required Wavell to dispatch a large part of O’Connor’s command to Greece. The units that replaced them were newly formed and inexperienced (especially in desert warfare). There was little continuity of learning and few opportunities for training. There were also fundamental problems with the structure and organization of British formations which were not addressed and which were to present particular problems.\nLieutenant-Colonel J.C. ‘Jock’ Campbell VC\nThe British Army was heavily outnumbered by the Italians, so General Archibald Wavell formulated a plan with his senior commanders to retain the initiative by harassing the enemy using mobile all-arms flying columns. Campbell’s brilliant command of one of these columns led to them being given the generic name “Jock columns” (although it is unclear if the idea originated with Campbell or not).\nBritish armoured divisions were too heavy on armour and too light on infantry and lacked sufficient artillery (with no self-propelled guns). In tactical terms, this encouraged them to focus on tank-versus-tank operations in which there was no co-operation with the other arms – something which O’Connor believed was a result of the influence of Liddell Hart’s theories on commanders of armoured units. As a consequence, ad hoc formations of all arms except tanks were formed. These ‘Jock Columns’ – named after their inventor Lieutenant-Colonel J.C. ‘Jock’ Campbell VC of 4th Royal Horse Artillery (RHA) – were typically made up of a battery of 25-pounder field guns, a motorized ‘motor’ infantry company, an armoured car troop, a troop of 2-pounder anti-tank guns and a section of 40mm Bofors anti-aircraft guns, plus ancillary arms such as medical personnel and signallers. Until July 1942, these seemingly aggressive formations were actually responsible for dissipating the artillery strength of the British in the desert and impeded effective co-operation between the infantry and armour. This tactical schism was exploited repeatedly by the new, and extremely skilful, tactician who arrived soon after the defeat at Beda Fomm to lead their opponent’s forces.","|Feldgrau.com - research on the German armed forces 1918-1945|\nWorld War II Axis Military History Day-by-Day:\nSeptember 1, 1939: Three German Armeegruppen begin the invasion of Poland at 4:45am CET, offically launching the world into what would be the most destructive and deadly war in all of history. Massive strikes by the Luftwaffe destroy vital communications and assembly areas, decimating the Polish air force on the ground. Panzer and motorized divisions make deep penetrations into the Polish defenses, using tactics soon to be known as the Blitzkrieg. Offically, the first shots of the war are fired from the 280mm deck guns of the vintage First World War Battleship Schleswig-Holstein. The ship had survived the ravages of WWI and entered service in the Kriegsmarine in 1935, serving initially as a cadet training ship. Under the guise of honoring the anniversary of the Battle of Tannenburg, the German Battleship, complete with a hidden cargo of Marine Assualt troops, was allowed by the Poles to anchor directly off the strategic island of Westerplatte, located at the mouth of the Vistula River in Danzig. At 4:47am, permission was given to the ship to open fire on the island, a strategic point on the Baltic Coast needed to support the troops advancing to the south. Shortly after 4:47am, the ship opened up its massive main guns, firing at near-pointblank range and zero elevation. Needless-to-say, the shells literally pounded the small island, but although the ships guns devestated the target, they inflicted minimal casualties on the Poles stationed within. When the Marinesturmkompanie hidden within the Battleship disembarked and launched its main assault on the island, it was repulsed after taking heavy casualties. Another assault was launched later in the morning, again by the Marinesturmkompanie, after more shelling from the Schleswig-Holstein, but this assault also ending in heavy German casualties. The Westerplatte would prove impossible to take on the first day of WWII.\nSeptember 1, 1942: Units of 1.Panzerarmee (von Kleist) form a bridgehead across the Terek river at Mozdok in the Caucasus.\nSeptember 1, 1944: The US Third Army (Patton) occupies Verdun. Start of an attack to capture the strategic port city of Brest which the Germans have turned into a fortress.\nSeptember 2, 1939: Failure of a last-minute effort by Mussolini to find a peaceful solution of the German-Polish conflict. German troops capture the Jablunka pass in the Tatra mountains. Fighting continued for the strategic island of Westerplatte at the mouth of the Vistula River. A massive attack was launched by 60 Stuka divebombers of the II and III Stukageschwader Immelmann directed at crushing the island garrison. The air assault was not directly followed up by a German attack from the ground, and the Poles were able to reorganize their defense.\nSeptember 2, 1944: The remnants of German forces surrounded in the Kishinev pocket surrender to the Red Army. Finland breaks off diplomatic relations with Germany and demands the withdrawal of all German forces on Finnish soil.\nSeptember 3, 1939: THE OFFICAL BEGINNING OF WWII: After the expiration of their ultimatum at 11:00am CET which called for the withdrawal of all German forces from Poland, Britain and France declare war on Germany. The Kriegsmarine begins its campaign against British merchant shipping with 17 U-boats.\nSeptember 3, 1940: Hitler sets the date of the start of Operation Sea Lion, the German invasion of England, for September 21.\nSeptember 3, 1942: Continuing round-the-clock air attacks by Luftflotte 4 (von Richthofen) against Stalingrad. Units of 6.Armee (von Paulus) penetrate the inner city after having joined up with forward elements of 4.Panzer-Armee (Hoth) advancing from the south.\nSeptember 3, 1943: The new Italian government under Marshal Badoglio signs a ceasefire with the Allies. Troops of the British 8th Army cross the Strait of Messina and land on the Italian mainland without encountering any opposition.\nSeptember 3, 1944: Field Marshall von Rundstedt assumes command of the German armies in the West. US forces advancing from the south capture Lyon. British troops occupy Brussels.\nSeptember 4, 1939: German 3.Armee (von Kuechler) and 4.Armee (von Kluge) join in the Corridor and reestablish the land connection between East Prussia and the Reich that was severed in 1919 as a result of the Versailles Treaty.\nSeptember 4, 1941: The US destroyer Greer is attacked by a German U-boat while tracking and harrassing it.\nSeptember 4, 1944: The Finnish Army ceases hostilities against the Soviet Union. British forces occupy Antwerp.\nSeptember 5, 1939: Under the relentless pressure by the Wehrmacht, the Polish Army withdraws behind the Vistula. First British air raids on German soil against Wilhelmshaven and Cuxhaven, with negligible results (10 Wellingtons shot down by German fighters). The United States declares its neutrality in this war.\nSeptember 5, 1940: An irate Hitler orders a new offensive by the Luftwaffe against Britain with no regard for the civilian population, with London as the primary target, after the RAF for the first time makes several night raids on Berlin, causing many civilian casualties.\nSeptember 5, 1944: Failure of a German-Hungarian counterattack in the area of Klausenburg in Rumania. The Soviet Union declares war on Bulgaria and invades the country.\nSeptember 6, 1939: German troops advancing in Poland occupy the formerly German industrial area of Upper Silesia. In the West, French forces begin a limited offensive toward Saarbruecken. South Africa declares war on Germany.\nSeptember 6, 1940: King Carol of Rumania cedes the throne to his son, Michael, and appoints Marshal Antonescu head of state.\nSeptember 6, 1942: 4.Gebirgsdivision of 17.Armee (Ruoff) captures the Black Sea port of Novorossisk. Heavy house-to-house fighting continues in the center of Stalingrad.\nSeptember 6, 1943: The Red Army succeeds in separating Heeresgruppe Mitte (von Kluge) from Heeresgruppe Sued (von Manstein).\nSeptember 6, 1944: Soviet troops occupy Turnu-Severin on the Danube in Rumania and advance to the Yugoslav border. In Belgium, Ghent and Liege fall to British and American forces.\nSeptember 7, 1939: Mobile spearheads of the Heer reach the Narev river. Cracow surrenders, while 10.Armee (von Reichenau) advances closer to Warsaw.\nSeptember 7, 1940: The Luftwaffe begins the Blitz by launching a series of heavy night raids against the London docks, causing widespread fires and destruction.\nSeptember 7, 1941: The offensive by 20.Gebirgsarmee (Dietl) in northern Finland to capture the vital Lend-Lease port of Murmansk comes to a halt. Mobile units of 6.Armee (von Paulus) achieve a breakthrough at Konotop in the Ukraine.\nSeptember 7, 1943: 17. Armee begins the evacuation of the Kuban bridgehead across the Strait of Kerch to the Crimea.\nSeptember 7, 1944: Rumania, now allied with the Soviet Union, declares war on Hungary whose forces are still fighting on the German side.\nSeptember 8, 1939: Polish defenders of the Westerplatte at Danzig surrender after a week of continuous bombardment. The Polish government leaves Warsaw for Lublin, while its forces encircled at Radom face a hopeless situation.\nSeptember 8, 1941: Leningrad is now completely surrounded after German troops close the land bridge at Schluesselburg.\nSeptember 8, 1943: German reserves are rushed to Italy in the wake of the ceasefire between the Badoglio government and the Allies.\nSeptember 8, 1944: The first supersonic V-2 rockets are launched against London and other cities from mobile bases in Holland. Bulgaria declares war on Germany.\nSeptember 9, 1939: 4.Armee (von Kluge) captures Lodz and Radom on the road to Warsaw.\nSeptember 9, 1943: All Italian forces within the German-controlled areas of Italy, southern France, Yugoslavia, Albania and Greece are disarmed without opposition and made prisoners of war. The US Fifth Army (Clark) carries out a landing at Salerno in southern Italy. The Italian fleet sails to Malta where it surrenders to the Royal Navy. Formation of an anti-Badoglio, German-allied Italian government, the Republican Fascist Government of Salo, in northern Italy. Iran, under pressure from the Allies who occupy the country, declares war on Germany.\nSeptember 9, 1944: General de Gaulle forms a provisional French government that includes Communists.\nSeptember 10, 1939: In Poland, German troops achieve a breakthrough at Kutno and Sandomir and reach the Vistula. Canada declares war on Germany.\nSeptember 10, 1941: British convoy SC42 (64 ships) is attacked by a wolfpack of German U-boats off Greenland. They sink 18 ships, with the loss of two submarines, U-207 and U-510.\nSeptember 10, 1942: Failure of Soviet forces attacking from besieged Leningrad to break the German lines.\nSeptember 10, 1943: Soviet marines supported by naval units of the Red Fleet recapture the Black Sea port of Novorossisk.\nSeptember 10, 1944: US First Army (Hodges) occupies Luxemburg.\nSeptember 11, 1942: Heavy RAF raid on Duesseldorf.\nSeptember 11, 1943: British 8th Army occupies Brindisi in southern Italy. German officers captured by the Red Army form an anti-Hitler association, the \"Bund deutscher Offiziere\".\nSeptember 11, 1944: British troops advancing in Belgium cross the Dutch border, while spearheads of the US Third Army (Patton) reach the German border at Trier on the Moselle river.\nSeptember 12, 1939: Beginning of the battle in the Vistula bend near Kutno, the last major engagement of the Polish campaign.\nSeptember 12, 1941: German forces in the Kremenchug bridgehead across the Dnepr in the Ukraine advance north to aid in the encirclement of Kiev.\nSeptember 12, 1943: Mussolini, held prisoner by the Badoglio government on the Gran Sasso, is rescued by German paratroopers who land in gliders on top of the mountain. SS major Otto Skorzeny, the leader of the mission, becomes an instant celebrity in Germany.\nSeptember 12, 1944: Start of a German-Hungarian counter-offensive toward Arad and Temesvar in Hungary. German troops evacuate Rhodes and other Greek islands in the eastern Mediterranean.\nSeptember 13, 1939: Polish troops trapped in the Radom pocket surrender (60,000 prisoners).\nSeptember 13, 1940: Italilian forces in Cyrenaica under Marshal Graziani advance toward Sidi Barrani in Egypt.\nSeptember 13, 1943: Beginning of a German counterattack against the US Fifth Army's (Clark) bridgehead at Salerno.\nSeptember 13, 1944: The Red Army captures the Warsaw suburb of Praha on the east bank of the Vistula.\nSeptember 14, 1941: Heeresgruppe Mitte completes the encirclement of two Soviet armies at Kiev. In North Africa, British naval forces fail in their attempt to achieve a landing at Tobruk.\nSeptember 14, 1942: At Stalingrad, counterattacks by the Soviet 62nd Army (Chuikov) fail to stop LI. Armeekorps's (von Seydlitz) advance toward the Stalingrad inner city and the Central Station.\nSeptember 14, 1943: British troops occupy the Greek island of Leros in the Mediterranean.\nSeptember 14, 1944: In Estonia and Latvia, the Red Army begins an offensive against Heeresgruppe Nord which is forced to fall back to defensive positions around Riga.\nSeptember 15, 1939: Gdynia is captured by German forces. Polish breakout attempts from the Kutno pocket are unsuccessful.\nSeptember 15, 1940: Climax of the Luftwaffe daylight raids against the London docks (later designated Battle of Britain Day). British fighter aircraft destroy almost one quarter (57) of the attacking German bomber force.\nSeptember 15, 1941: The US Navy begins to take over the convoying of British ships as far as Iceland, seen as an unneutral act by the German government.\nSeptember 15, 1942: Fierce fighting between German and Soviet forces for possession of Mamayev Kurgan, the strategic hill overlooking Stalingrad.\nSeptember 15, 1944: On the Northern front, the Soviets achieve a breakthrough at Narva. The US First Army (Hodges) occupies Nancy.\nSeptember 16, 1943: German counterattacks against the US bridgehead at Salerno are halted.\nSeptember 16, 1944: Conclusion of the Quebec meeting between Roosevelt and Churchill who sign off on the Morgenthau Plan for the treatment of postwar Germany. In response, Dr. Goebbels exhorts all Germans to resist with the utmost fanaticism.\nSeptember 17, 1939: In Poland, Kutno and Brest-Litovsk are captured by German troops. The Red Army invades eastern Poland. The Polish government seeks asylum in Rumania and is interned.\nSeptember 17, 1941: The US allocates $100,000,000 to the Soviet Union for the purchase of war materials.\nSeptember 17, 1943: The Red Army recaptures Brjansk. In Italy, the British 8th Army joins forces with US troops in the Salerno bridgehead.\nSeptember 17, 1944: Start of Operation Market-Garden: British airborne forces land at Arnhem and Nijmegen in Holland to capture the vital Rhine bridges.\nSeptember 18, 1939: U-29 (Kapitanleutnant Schuhart) sinks the British aircraft carrier Courageous.\nSeptember 18, 1941: Units of Heeresgruppe Süd capture Poltava in the Ukraine.\nSeptember 19, 1939: Conclusion of the battle in the Vistula bend, with the Wehrmacht taking 170,000 prisoners.\nSeptember 19, 1941: German troops of Heeresgruppe Mitte capture Kiev.\nSeptember 19, 1944: British paratroop forces dropped at Arnhem encounter unexpected heavy German resistance.\nSeptember 20, 1939: German troops in eastern Poland withdraw to the line agreed upon in the German-Soviet treaty of August 26, 1939. The Red Army moves in behind them to occupy the formerly Russian territory.\nSeptember 20, 1943: Heeresgruppe Süd begins its withdrawal to the Melitopol-Zaporoshe line. The British 8th Army occupies Bari in southern Italy. German troops evacuate the island of Sardinia.\nSeptember 21, 1939: The remaining parts of the Polish Southern Army surrender at Zamosz and Tomaszov (60,000 prisoners).\nSeptember 21, 1942: RAF raids on Munich and the Saar valley.\nSeptember 21, 1943: The Red Army forces a crossing of the Dnepr at Dnepropetrovsk, breaking into the German defensive lines of the Panther-Stellung.\nSeptember 21, 1944: In Italy, the British 8th Army captures Rimini. German forces of Heeresgruppe E evacuate the Peloponnes peninsula in Greece.\nSeptember 22, 1939: Polish forces fighting the invading Red Army surrender at Lvov (217,000 prisoners). The NKVD begins rounding up thousands of Polish officers and deporting them to Russia where they will be liquidated a year later in the forest of Katyn near Smolensk.\nSeptember 22, 1942: At Stalingrad, units of the 6th Army (von Paulus) anf 4th Panzerarmee (Hoth) split the Soviet 62nd Army (Yeremenko) in two and capture nearly the entire southern part of the city, including the huge grain elevator defended by Soviet marines.\nSeptember 22, 1944: The Red Army recaptures Reval in Estonia. In the West, German troops holding out in the port city of Boulogne finally surrender to British forces.\nSeptember 23, 1942: At Stalingrad, Soviet counterattacks to dislodge German advance units near the Volga landing stage are unsuccessful. In North Africa, Field Marshal Rommel takes a medical leave and hands over command of the Afrikakorps to General von Thoma. Wendell Willkie, 1940 Republican presidential candidate, confers with Stalin and calls for a second front at the earliest possible moment.\nSeptember 23, 1943: The Red Army recaptures Poltava in the Ukraine.\nSeptember 24, 1941: Heeresgruppe Süd begins an offensive against the vital land bridge to the Crimea at Perekop.\nSeptember 24, 1942: General Halder is forced to resign as chief of staff of OKH by Hitler and is replaced by General Zeitzler. In the Caucasus, units of Heeresgruppe A (List) launch an attack against the Black Sea port of Tuapse.\nSeptember 24, 1944: British naval units begin operations against German-occupied islands in the Aegean Sea in the eastern Mediterranean.\nSeptember 25, 1940: An attack by Free French forces landed by ships of the Royal Navy against the west African port of Dakar is repulsed by Vichy troops.\nSeptember 25, 1941: Hitler orders all attacks by Heeresgruppe Nord (von Leeb) on Leningrad stopped; the city is to be besieged and starved-out, and after its eventual surrender, levelled to the ground.\nSeptember 25, 1943: Soviet forces succeed in establishing a bridgehead across the Dnepr at Dnepropetrovsk.\nSeptember 26, 1941: The Free French government in London under General de Gaulle signs an alliance with the Soviet Union.\nSeptember 26, 1944: In Holland, Operation Market-Garden ends in failure, with heavy losses to the British airborne forces engaged. At Arnhem, 6,450 survivors surrender and are taken prisoner.\nSeptember 27, 1939: Warsaw, besieged for more than two weeks, surrenders after continuous air and artillery bombardments.\nSeptember 27, 1942: At Stalingrad, units of 6.Armee succeed in capturing most of the strategic Mamayev Kurgan hill, and penetrating the heavily defended Red October and Barricades housing estates.\nSeptember 27, 1943: General withdrawal of all German forces in the Ukraine to positions on the west bank of the Dnepr river.\nSeptember 27, 1944: German forces of Heeresgruppe E evacuate western Greece.\nSeptember 28, 1939: German and Soviet troops meet at Brest-Litovsk, and together stage a military review. An agreement is signed delineating their common border lines in eastern Poland.\nSeptember 28, 1944: Start of a Soviet offensive from western Bulgaria and Rumania toward Belgrade.\nSeptember 29, 1939: Polish forces defending the fortress of Modlin surrender (35,000 prisoners).\nSeptember 29, 1941: The attacks by Heeresgruppe Süd (von Rundstedt) to force an entry into the Crimea are halted.\nSeptember 30, 1941: Armored forces of Heeresgruppe Mitte (von Bock) launch an attack to capture Orel.\nSeptember 30, 1942: Top-scoring Luftwaffe ace and Diamonds winner Hans-Joachim Marseille (158 British aircraft) of 3./JG 27 is killed by an accident in North Africa."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:b699f398-ff1d-4a11-b3df-9e35d6e55b6e>","<urn:uuid:1922d76b-99b2-4b6e-af4a-76a71851c711>"],"error":null}
{"question":"How did the Red Army Faction (RAF) evolve from student protests to violent extremism in Germany during the late 1960s?","answer":"The RAF emerged from anti-authoritarian and anti-capitalist student movements of the late 1960s. Several factors contributed to their turn to violence: police brutality, an increase in rightwing journalism, and rightwing violence against student protesters. The young educated Germans were particularly sensitive to potential state fascist tendencies given their history. While their initial protestations were legitimate and their anger understandable, they eventually abandoned political debate in favor of violent action.","context":["Nobody could accuse contemporary German cinema of shying away from the past. Films like The Downfall, The Lives of Others and now The Baader Meinhof Complex have all explored very dark chapters of the country’s history, ensuring that the events depicted will be preserved as a constant reminder for future generations. In the case of The Baader Meinhof Complex, directed by Uli Edel (Christiane F., Last Exit To Brooklyn), it is the creation and the terrorist actions of the radical and militant leftist group the Red Army Faction (RAF) from 1967-1977 that is under scrutiny. The RAF had its foundations in the anti-authoritarian and anti-capitalist student movements that were happening worldwide in the late 1960s and The Baader Meinhof Complex carefully reveals the conditions under which that rebellious sentiment led to violent action. The young generation of educated Germans knew all too well what could happen if state fascists tendencies were left unchecked and police brutality, an increase of rightwing journalism and rightwing violence against student protesters were all ingredients in turning their outrage into extremism.\nThe various characters who eventually form the RAF initially come across as very sympathetic. Their protestations are legitimate and their anger understandable. What is so disturbing about The Baader Meinhof Complex is how these characters soon become terrifying as they turn their backs on political debate and resort to violence. Producer/writer Bernd Eichinger states that this transition is why The Baader Meinhof Complex focuses on the RAF’s actions rather than the theories behind those actions. The obvious film to compare The Baader Meinhof Complex to is The Battle of Algiers, a film that has rightly been long regarded as the definitive film about urban terrorism. The Baader Meinhof Complex is a much slicker film but it shares The Battle of Algiers’s episodic narrative, lack of protagonist and refusal to take sides or make moral judgements. The filmmakers have also gone into painstaking detail to recreate the real events by filming at the original locations and even matching the number of shots fired in each scene to the number of bullets recorded by the police during the original incidents.\nAll the cast are excellent including Moritz Bleibtreu (Run Lola Run, The Experiment) as the young, almost childish Badder and Martina Gedeck (The Lives of Others) as the “bourgeois” journalist Meinhof. Bruno Ganz is also terrific as Horst Herold, the head of the German police force who realises that understanding the terrorists and changing the conditions that have led to their disillusionment is not sympathising with them, but the only way to stop them and prevent others from repeating their actions. His measured approach makes him the voice of reason in all the madness. The Baader Meinhof Complex is gripping cinema that will keep you on the edge of your seat. It is fascinating, exciting, terrifying and sombre. Its attempts to stick closely to the source material, Stefan Aust’s definitive 1985 book of the same title, means that not all narrative strands are explored as satisfactorily as you may expect but that’s a small price to pay for such authenticity."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a631d763-9b43-4917-a43a-b8bf13a649f3>"],"error":null}
{"question":"How do GSM networks enable monitoring in both fire alarm systems and gas distribution facilities?","answer":"In both fire alarm and gas distribution monitoring systems, GSM networks play a crucial role in transmitting alerts. For fire alarms, a black box antenna transmits fire and fault signals to the Alarm Receiving Centre via the GSM network, with signal strength displayed on digital units (requiring a strength of 12 or over). Similarly in gas distribution facilities, data is transmitted through wireless GSM networks using GPRS channels, where a communicator with an external antenna sends information about gas parameters and emergency conditions to an enterprise server. Both systems rely on GSM technology for remote monitoring and quick response to emergency situations.","context":["Fire Alarm Monitoring\nA fire alarm can be either “Stand-Alone” or Monitored.\nA Stand-Alone system will alert the occupants locally within the building using audible and visual indicators.\nA monitored system can also alert the occupants locally, but will automatically send a signal to a remote building usually called an Alarm Receiving Centre (ARC) informing the personnel in the ARC that the fire alarm has activated. The ARC will then following the predefined instructions given in the event of receiving a signal; this may well be to call the Fire & Rescue Service and the building keyholders.\nHaving the fire alarm monitored has the advantage that the Fire and Rescue Service are automatically summoned at the first instant and without having to rely on anyone making the call. The Early calling of the Fire & Rescue service can be vital when saving lives or reducing damage to property.\nShould a P-type category fire alarm be installed to protect the property and assets, then it will have to be monitored.\nThe most well-known brand of monitoring is BT Redcare. However, there are many Alarm Receiving Centres known as ARC’s all over the country monitoring not only fire alarms but intruders, Building Management systems, Fridges, or any equipment that needs to be monitored and if activated someone alerted.\nIn the video, we show how a fire alarm is interfaced to a Redcare Secure unit and how the fire alarm company or the client can access and control their monitoring via a web portal.\nTo give you all some idea as to how the Redcare Secure operates, we have installed a unit into our office at Fire Systems Ltd. As we venture inside the fire alarm panel you will observe the comms type cable in the connection block to the right-hand side.\nIn this example, we will be monitoring a “Fault” and “Fire” condition from the system.\nThe unit is wired into the “Fault” relay and the “Fire” relay as shown.\nThe blue wire is going to Pin 1 and will provide a “Fire” signal.\nThe Yellow wire is going to Pin 2 and will provide a “Fault” signal.\nThe red and black wires are providing the power to the unit.\nThe unit can handle up to 7 different channels.\nThe black box to the left of the unit is the antenna, or sometimes referred to as the Ariel. The reason for the antenna is to transmit the fire and fault signal to the Alarm Receiving Centre via the GSM network.\nThe two green 8 way segment display units, provides useful information such as the\n- GSM signal strength. Which should be 12 or over, our unit is showing a value of 19\n- Alarm Signals – In our case it will be 1 or 2, representing the fire signal on channel 1 or the fault signal on channel 2.\nThe red and black cables provide the power to the unit, with the blue wire on pin 1, providing a fire signal and the blue wire on pin 2, providing a fault signal.\nThe black box to the left is the antenna to transmit the alarms signals using the GSM network. The digital display provides GSM signal strength and Alarm signal information.\nRight, let’s carrying out some testing and view the results. As you can see from my mobile phone the date is Friday 17th April at 11:42.\nThe first test will be to generate a fault by removing the 230v supply. You will see on the digital display the fault has been received. Showing an alarm on channel 02.\nThe next test will be another fault test and is being recorded at 11:44. The engineer will remove the battery backup supply that will again generate a fire alarm fault. The fault has been accepted. The battery has been reinstated and the panel reset.\nThe following test at 11:45 was another fire alarm fault generated by the removal of a smoke detector.\nThis time the following test will be a fire alarm activation and not a fault. The time recorded for this activation is 11:46.\nYou will not observe the number on the digital display is now changed to 01.\nThe following test will be recorded at 11:46, and will be a “fire” alarm activation and not a “fault”.\nYou will now observe that on the digital display the Alarm indication is now followed by a 01, as opposed to a 02. 01 is the number for a fire alarm activation.\nThe fire alarm system has gone back into a fire alarm condition as there is still smoke detector gas within the chamber of the detector. This 2nd alarm will also be recorded in the event log.\nTouch Portal – Fire Alarm Monitoring\nThe “Touch” portal is accessible via the web, allowing you to view the site history from your desktop or mobile device. At arriving at the portal, you enter you’re your security details and this will take you to the “Account Statistics Overview” page.\nWe then click the top option “Total Systems Monitored” to view a list of all sites currently active and being monitored.\nThe site we are looking for is our office “Fire Systems Ltd”, we click on this, and this takes us to a further list of options. We then select, the “Events History” and this will reveal an event history log.\nIn relation to the test we carried out, our first test was on Friday the17th at 11:42, this was the removal of the 230v mains supply. As you can see it is listed as a “Fire Fault” under a Zone ID of 2. If you remember all faults will be logged as channel 2. Once the fire alarm panel is reset, this is also logged as a “Restored PA Zone” and the time is recorded.\nIf we continue with the next fault, which was the removal of the backup battery; as you can see the fault was logged again as a “Fire Fault” channel 2 at 11:44 and restored at 11:44.\nThe next fault was the removal of the smoke detector, and this was recorded at 11:45 and restored at 11:45. All of these fault signals came in on Zone ID 2, which is channel 2.\nOur first fire alarm activation is recorded at 11:46, and is listed as a “Fire Alarm” with a Zone ID as channel 1. The system is restored at 11:46.\nFinally, we have another fire activation, this was due to the detector reactivating the system, as the sensing chamber still has an element of test gas within it. The reactivation was also recorded at 11:46 and restored at 11:47.\nAs you can see from the event log, all alarms have been accounted for and match the times recorded.\nFire Systems Ltd have all their connections controlled using this portal called “Touch”.","THE \"VEGA-TSG\" AUTOMATED TELEMETRY SYSTEM COMPLEX\nThe main task of the system for telemetry of gas distribution facilities is monitoring technological parameters: inlet and outlet gas pressure, filter pressure drops, activation of safety fittings, etc. Emergency signals should be generated with minimum delay to ensure continuous facility status monitoring and prompt response in event of failures.\nEquipping gas distribution networks with telemetry systems enables receiving valid information online from a big number of territorially remote facilities, and improving substantially the safety and operational reliability of the gas supply system by enhancing real-time control and prevention of emergencies.\nThe facilities being modernised:\nGas control units (GCU) and gas control stations (GCS) are one of the most critical components of the gas distribution system because they have a direct inpact on safe and economical combustion of gas fuel. Incidentally, the latter factor, with account of the current gas situation in Ukraine, is the one that conditions the need to use advanced automated monitoring and metering instrumentation. Its functioning shall ensure maximum savings along with improving the reliability and safety of operating the gas distribution system.\nAll GCU and GCS (with few exceptions) are built to standard projects. Therefore, the transition to a new automatics system should be, on the one hand, integrated, and on the other hand, a standard one to use the same equipment with minimum setup to the features of a specific facility.\nWith this in view, JV RADMIRTECH has started developing and executing the project “Automated complex for the VEGA-TSG telemetry system”.\nThe main task was making a reliable system for controlling the values of gas parameters, signalling and transmission of data to the central dispatcher station over a wireless communication channel. The latter condition was obligatory because the GCU are territorially remote and using wire communication lines is challenging.\nStructural diagram of the VEGA-TSG telemetry system:\nThe system comprises the following:\n- Equipment located in an explosion-hazardous area:\n- Absolute (excess) pressure transducers (up to 5 pcs.),\n- Pressure difference transducers (up to 5 pcs.),\n- Temperature transducers (up to 3 pcs.),\n- Door opening sensors,\n- Safety valve detectors (blow-off and shutoff, up to 5 pcs.),\n- Gas pollution sensor,\n- Control unit (CU),\n- Discrete signals controller (DSC),\n- Equipment in the explosion-proof area\n- Telemetry system communication module (installed in the mounting box):\n- cable with a galvanic decoupler (up to 15 m),\n- communicator with an external antenna,\n- gas pollution detector power controller and DSC (with a built-in galvanic decoupler),\n- power supply with switching devices,\n- storage battery.\nThe control unit (CU) of the VEGA-TSG telemetry system is intended for collecting information about gas parameters over a digital channel from analog absolute (excess) pressure transducers, pressure difference transducers and temperature transducers. The CU also receives information through the discrete signals controller (DSC) from sensors detecting door “open-closed” conditions in GCU rooms, blow-off and shutoff valve detectors, and room gas pollution sensors. The built-in keyboard panel is used to display information on the CU screen on the status of each analog and discrete detector, read archived information, and view the status of the built-in power battery, current time, date, and other data. The CU keyboard is also used for entering the operator authentication password.\nA cable with a galvanic decoupler is used for providing power supply to the CU and for transmitting received information to the communicator for subsequent data transmission through wireless networks to the enterprise server.\nThe dedicated enterprise server processes the data received from GCU (GCS) in the packet GPRS channel over the GSM standard digital cellular radio communication network, and enters it to the database.\nThe Web portal on-line “Vega–WEB” service is used to access industrial process information from GCU (GCS) and compile reports on the side of the enterprise dispatcher station.\nSpecial software “nVEGA_G” installed on a PC is used for adjustment and set-up of components in the VEGA-TSG telemetry system, and for reading data at the facility.\nSoftware “Set_RTV” is used for set-up of the communicator built-in GPRS modem.\nThe VEGA-TSG telemetry system components are powered from an a.c. mains 220 V through a power supply controller and the communicator. The power supply circuits are decoupled. In the event of an external power supply failure, the system is powered from a storage battery capable of supplying the system up to 6 months without recharging. The storage battery is part of the delivery set.\nBasic system functions:\n- Polling controlled units on request or to a specified schedule (by the dispatcher);\n- Displaying all controlled units on the PC monitor with their visualisation on Google maps;\n- For the selected controlled unit, displaying on the PC monitor the current readings of all analog and discrete detectors, and real-time, per hour and daily parameter values in graphical or tabular form;\n- Online display on the PC monitor of information about occurrence of incidents (off-range readings of detector settings, unauthorised opening of doors, emergency actuation of valves, excess methane concentration in a room, etc.);\n- Archiving and printing all received data;\n- Display of current system parameters: time on CU and built-in GPRS modem, GSM signal level, storage battery and mains voltage levels, information about the life of built-in CU and GPRS modem batteries, etc.;\n- System setup at the facility or remotely using “nVEGA_G” software."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:bac0c2ad-1270-45f5-b5a4-5f7893f9f34a>","<urn:uuid:931c97d4-914e-41f0-90e1-c062d3d4a6c0>"],"error":null}
{"question":"How can attackers use Windows access token manipulation to escalate privileges, and what are the preventive measures?","answer":"Access token manipulation in Windows involves fooling the system about process ownership through three methods: token theft, token creation, and token impersonation. While access tokens cannot be disabled in Windows since attackers need administrative access first, prevention focuses on: applying the least-privilege principle for administrative rights, regularly reviewing and revoking unnecessary admin accounts, and monitoring privileged accounts for anomalous behavior.","context":["Privilege escalation is a type of network attack used to gain unauthorized access to systems within a security perimeter.\nAttackers start by finding weak points in an organization’s defenses and gaining access to a system. In many cases that first point of penetration will not grant attackers with the level of access or data they need. They will then attempt privilege escalation to gain more permissions or obtain access to additional, more sensitive systems.\nIn some cases, attackers attempting privilege escalation find the “doors are wide open” – inadequate security controls, or failure to follow the principle of least privilege, with users having more privileges than they actually need. In other cases, attackers exploit software vulnerabilities, or use specific techniques to overcome an operating system’s permissions mechanism.\nWe’ll cover the five most common privilege escalation attack vectors, and show specific examples of privilege escalation techniques attackers use to compromise Windows and Linux systems.\nIn this article:\nThere are two types of privilege escalation:\nFor attackers, privilege escalation is a means to an end. It allows them to gain access to an environment, persist and deepen their access, and perform more severe malicious activity. For example, privilege escalation can transform a simple malware infection into a catastrophic data breach\nPrivilege escalations allow attackers to open up new attack vectors on a target system. For example, it can involve:\nWhen security teams suspect privilege escalation it is important to perform an in-depth investigation. Signs of privilege escalation include malware on sensitive systems, suspicious logins, and unusual network communications.\nAny privilege escalation incident must be dealt with as a severe security incident and, depending on the organization’s compliance obligations, might have to be reported to authorities.\nPrivilege escalation attacks typically involve the exploitation of vulnerabilities such as software bugs, misconfigurations, and incorrect access controls.\nEvery account that interacts with a system has some privileges. Standard users typically have limited access to system databases, sensitive files, or other resources. In some cases, users have excessive access to sensitive resources, and may not even be aware of it, because they do not try to gain access beyond their entitlements. In other cases, attackers can manipulate weaknesses of the system to increase privileges.\nBy taking over a low-level user account and either abusing excessive privileges, or increasing privileges, a malicious attacker has an entry point to a sensitive system. Attackers might dwell in a system for some time, performing reconnaissance and waiting for an opportunity to deepen their access. Eventually, they will find a way to escalate privileges to a higher level than the account that was initially compromised.\nDepending on their goal, attackers can continue horizontally to take control of additional systems, or escalate privileges vertically, to gain admin and root control, until they have access to the entire full environment.\nHere are the most important attack vectors used by attackers to perform privilege escalation.\nSingle factor authentication leaves the door wide open to attackers planning on performing privilege escalation. If attackers obtain a privileged user’s account name – even without the password – it is a matter of time before they obtain the password. Once they obtain a working password, they can move laterally through the environment undetected.\nEven if the attacker is detected and the organization resets the password or reimages the affected system, the attacker may have a way to retain a persistent presence – for example, via a compromised mobile phone or rootkit malware on a device. This makes it important to thoroughly eradicate the threat and continuously monitor for anomalies.\nHere are common ways attackers can gain access to credentials:\nAttackers can perform privilege escalation by exploiting vulnerabilities in the design, implementation, or configuration of multiple systems – including communication protocols, communication transports, operating systems, browsers, web applications, cloud systems, and network infrastructure.\nThe level of risk depends on the nature of the vulnerability and how critical is the system in which the vulnerability is discovered. Only a small fraction of vulnerabilities allow vertical privilege escalation. However, any vulnerability that can allow an attacker to change privileges should be treated with high severity.\nSee the following sections for examples of vulnerabilities that can lead to privilege escalation on Windows and Linux.\nPrivilege escalation very commonly results from misconfiguration, such as failure to configure authentication for a sensitive system, mistakes in firewall configuration, or open ports.\nHere are a few examples of security misconfigurations that can lead to privilege escalation:\nAttackers can use many types of malware, including trojans, spyware, worms, and ransomware, to gain a hold on an environment and perform privilege escalation. Malware can be deployed by exploiting a vulnerability, can be packaged with legitimate applications, via malicious links or downloads combined with social engineering, or via weaknesses in the supply chain.\nMalware typically runs as an operating system process, and has the permissions of the user account from which it was executed. So there are two directions for exploitation:\nHere are common examples of malware that can be used for privilege escalation:\nSocial engineering is used in almost all cyber attacks. It relies on manipulating people into violating security procedures and divulging sensitive or personal information. It is a very common technique used by attackers to gain unauthorized access and escalate privileges.\nSocial engineering is highly effective because it circumvents security controls by preying on human weaknesses and emotions. Attackers realize that it is much easier to trick or manipulate a privileged user than break into a well-defended security system.\nHere are common types of social engineering attacks and how they are used for privilege escalation:\nThere are many privilege escalation methods in Windows operating systems. Here is a brief review of three common methods and how you can prevent them.\nWindows uses access tokens to determine the owners of running processes. When a process tries to perform a task that requires privileges, the system checks who owns the process and to see if they have sufficient permissions. Access token manipulation involves fooling the system into believing that the running process belongs to someone other than the user who started the process, granting the process the permissions of the other user.\nThere are three ways to achieve access token manipulation:\nIn this method, an adversary has a username and password, but the user is not logged\nThere is no way to disable access tokens in Windows. However, to perform this technique an attacker must already have administrative-level access. The best way to prevent the attack is to assign administrative rights in line with the least-privilege principle, regularly review administrative accounts and revoke them if access is no longer needed. Also, monitor privileged accounts for any sign of anomalous behavior.\nThe Windows user account control (UAC) mechanism creates a distinction between regular users and administrators. It limits all applications to standard user permissions unless specifically authorized by an administrator, to prevent malware from compromising the operating system. However, if UAC protection is not at the highest level, some Windows programs can escalate privileges, or execute COM objects with administrative privileges.\nReview IT systems and ensure UAC protection is set to the highest level, or if this is not possible, apply other security measures. Regularly review which accounts are a local administrator group on sensitive systems and remove regular users who should not have administrative rights.\nAttackers can perform “DLL preloading”. This involves planting a malicious DLL with the same name as a legitimate DLL, in a location which is searched by the system before the legitimate DLL. Often this will be the current working directory, or in some cases attackers may remotely set the working directory to an external file volume. The system finds the DLL in the working folder, thinking it is the legitimate DLL, and executes it.\nThere are several other ways to achieve DLL search order hijacking:\nHere are several ways to prevent a DLL search order hijack:\nIn Linux systems, attackers use a process called “enumeration” to identify weaknesses that may allow privilege escalation. Enumeration involves:\nAttackers use automated tools to perform enumeration on Linux systems. You should also use the same tools to pre-empt an attack, by scanning your own system, identifying weaknesses, and addressing them.\nBelow are two specific techniques for escalating privilege on Linux and how to mitigate them.\nFrom time to time, vulnerabilities are discovered in the Linux kernel. Attackers can exploit these vulnerabilities to gain root access to a Linux system, and once the system is infected with the exploit, there is no way to defend against it.\nAttackers go through the following steps:\nFollow security reports and promptly install Linux updates and patches. Restrict or remove programs that enable file transfers, such as FTP, SCP, or curl, or restrict them to specific users or IPs. This can prevent transfer of an exploit onto a target device. Remove or restrict access to compilers, such as GCC, to prevent exploits from executing. You should also limit which folders are writable or executable.\nSUDO is a Linux program that lets users run programs with the security privileges of another user. Older versions would run as the superuser (SU) by default. Attackers can try to compromise a user who has SUDO access to a system, and if successful, they gain root privileges.\nA common scenario is administrators granting access to some users to perform supposedly harmless SUDO commands, such as ‘find’. However, the ‘find’ command container parameters that enable command execution, and so if attackers compromise that user’s account, they can execute commands with root privileges.\nNever give SUDO rights to the programming language compiler, interpreter or editors, including vi, more, less, nmap, perl, ruby, python, gdb. Do not give sudo rights to any program that enables running a shell. And severely limit SUDO access using the least-privilege principle.\nIn this article, we were only able to cover a few common privilege escalation attacks. For more attacks and additional details on how to mitigate and detect each attack, refer to MITRE ATT&CK privilege escalation tactics.\nCynet 360 is a holistic security solution that can help with three important aspects of privilege escalation—network security, endpoint security, and behavioral analytics.\n1. Network Analytics\nNetwork analytics is essential to detect and prevent initial penetration and privilege escalation on your network.\nThe challenge—sophisticated attackers target an organization’s weak points. Following an initial endpoint compromise, the attacker looks to expand their reach and gain privileges and access to other resources in your environment. Their ultimate aim is to access your sensitive data and to transfer it to their premises. Key parts of these attack vectors can only be discovered via generated anomalous network traffic.\nThe solution— Cynet Network Analytics continuously monitors network traffic to trace and prevent malicious activity that is otherwise invisible, such as credential theft and data exfiltration.\n2. Endpoint Protection and EDR\nUnauthorized access to endpoints is a common entry point in a privilege escalation attack.\nThe challenge—attackers with strong motivation will eventually bypass the prevention measures on the endpoint. They will use several tools to work undetected until they achieve their desired outcome.\nThe solution— Cynet EDR continuously monitors the endpoints, so defenders can detect the active malicious presence, immediately understand its impact and scope, and respond.\n3. User and Event Behavioral Analytics\nBehavioral analytics can help you detect anomalous activity on organizational systems or user accounts, which may indicate intrusion attempts or privilege escalation. It is also especially important to detect privilege escalation conducted by malicious insiders.\nThe challenge—attackers with clear objectives in mind, or those with insider privileges, might bypass detection, succeed in compromising user accounts and use them for data access and lateral movement.\nCynet User Behavior Analysis monitors and profiles user activity continuously, to establish a legitimate behavioral baseline and detect anomalous activity that suggests compromise of user accounts or privilege escalation."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:58c2cfb8-0194-44d8-b1f1-fe17d87418db>"],"error":null}
{"question":"What are los síntomas of diarrhea in cats, and how does inflammatory bowel disease affect their appetite?","answer":"Diarrhea symptoms in cats can include increased thirst due to fluid loss, flatulence, abdominal pain, blood/fat/mucus in the stool, weight loss, increased urgency to defecate, and defecating outside the litter tray. The type of diarrhea can be categorized as small intestine diarrhea (increased volume, 2-3 times normal frequency) or large intestine diarrhea (normal to decreased volume, mucus and blood present, more than 5 times normal frequency). Regarding appetite in inflammatory bowel disease (IBD), cats can show varying patterns - they may experience a lack of appetite (anorexia), but interestingly, some cats with IBD may actually show an increased appetite while still losing weight.","context":["|What is diarrhea? Causes Symptoms Diagnosis Treatment Home care|\nAt a glance\nAbout: Diarrhea is the passage of loose, watery stools. It can be acute (sudden onset) or chronic (lasting longer than two weeks). It can originate from the small or large intestine.\nDiagnosis: Baseline tests to evaluate the overall health of your cat, fecal tests, diagnostic imaging and additional tests depending on the veterinarian’s index of suspicion.\nTreatment: A bland diet to rest the gastrointestinal tract, supportive care such as fluids and nutritional support and specific treatment depending on the underlying cause.\nDiarrhea is an intestinal disturbance characterised by the rapid movement of abnormally loose or watery stools (feces). It is not a disease in itself, but a symptom of an underlying disease or disorder.\nDiarrhea can affect the small intestine, the large intestine or both. It may be acute (sudden onset), chronic (over a long period of time) or intermittent (come and go). There may be an increase in the number of bowel movements, an increased amount of feces or watery feces. Feces may also be yellow and frothy in appearance, be mixed with blood (dysentery) and/or mucus.\n- Acute diarrhea has a rapid onset and lasts less than 2 – 3 weeks.\n- Chronic diarrhea lasts longer than 2 – 3 weeks.  Blood and or mucous may or may not be present in the feces.\n- Food allergy.\n- Food intolerance: Such as lactose in milk.\n- Dietary indiscretion: Eating something he shouldn’t have, such as garbage).\n- A sudden change in diet.\n- Bacterial infection: Salmonellosis, campylobacter, E. Coli, tuberculosis, tularaemia.\n- Viral infection: FIV, FeLV, panleukopenia, rotavirus, pseudorabies.\n- Protozoa infection: Giardia or cryptosporidium.\n- Parasitic infection: Liver flukes, parasitic worms such as roundworm and hookworm.\n- Histoplasmosis (disseminated): A rare fungal infection which usually affects the lungs, in some cases the infection can spread. throughout the body causing a number of symptoms including diarrhea.\n- Colitis: Inflammation of the colon.\n- Inflammatory bowel disease: A group of conditions in which different types of inflammatory cells invade the intestines.\n- Pancreatitis: Inflammation of the pancreas.\n- Hyperthyroidism: A benign hormone-secreting tumour of the thyroid gland.\n- Kidney disease: Chronic which is slow and progressive and seen most often in middle-aged to senior cats or acute which is sudden onset.\n- Liver disease: Which may be due to ingestion of toxins, congenital disorders, infection or inflammation.\n- Complications of diabetes.\n- Blockage: Hairball or foreign object.\n- Certain medications: Antibiotics, chemotherapy, antihistamines, steroids.\n- Exocrine pancreatic insufficiency: Failure of the pancreas to secrete appropriate levels of pancreatic enzymes which are necessary for the digestion of food.\n- Heinz body anemia: A type of anemia (reduced number of red blood cells) characterised by the presence of Heinz bodies on the red blood cells which leads to their destruction by the cat’s own immune system.\n- Poisoning: Zinc, Ibuprofen, uremic, antifreeze or other.\n- Insect/spider bites or sting.\n- Anaphylaxis: An acute allergic reaction.\n- Malabsorption: An inability of the digestive system to absorb nutrients as it should, there are several underlying causes of this including inflammatory bowel disease, exocrine pancreatic insufficiency, blockage, bacterial overgrowth, parasites, certain cancers.\nSymptoms can vary depending on the underlying cause and if the small or large intestine are involved.\n- Small intestine diarrhea: Volume is increased, frequency 2-3 times normal, no mucus, urgency may be normal to mildly increased.  Cats with chronic small intestinal diarrhea lose weight and body condition as they are not absorbing nutrients.\n- Large intestine diarrhea: Volume is normal to decreased, mucus and blood may be present, the urgency is increased and frequency is more than 5 times normal. \nOther symptoms may also occur depending on the underlying cause of diarrhea and may include:\n- Anorexia (loss of appetite)\n- Increased thirst, due to fluid loss\n- Flatulence (farting)\n- Abdominal pain\n- Blood, fat or mucus in the stool\n- Weight loss\n- Increased urgency to defecate\n- Defecating outside the litter tray\nDoes a cat with diarrhea need to see a veterinarian?\nMild cases of diarrhea, lasting 24 hours or less, where your cat seems to be otherwise well can be watched carefully at home. As a precaution, take away your cat’s food for 12-24 hours to see if the problem resolves. Leave water out at this time. See a veterinarian if the following symptoms are present:\n- Diarrhea which contains blood\n- If he appears to be in pain, such as hunched over or a tucked up belly\n- Kittens under 12 months of age, or senior cats\n- Pale gums\nIdentifying a cat with diarrhea in a multi-cat household:\nWhen there’s more than one cat in the home, it can be difficult to identify the cat with diarrhea, unless you watch them in the litter tray. An easy way to determine which cat has diarrhea is to use a box of crayons (Crayola claim theirs are non-toxic to humans, so I would give this brand a try if you can). Use a pencil sharpener or cheese grater, to make some crayon shavings, each cat has his own colour. Add 1 teaspoon of shavings to each cat’s food. Obviously, this means that the cats must be fed separately and any uneaten food removed. The shavings will pass harmlessly through the cat and out via the feces.\nYour veterinarian will perform a physical examination of your cat and ask you some questions to determine if the diarrhea is acute or chronic? If there have been any changes to your cat’s diet, possible exposure to toxins, or other symptoms you may have noticed.\nThe type and colour of diarrhea, along with accompanying symptoms as well as your cat’s age can help your veterinarian narrow down a cause. For example, a kitten is more likely to have eaten something he shouldn’t, a senior cat is at greater risk of hyperthyroidism or cancer.\nTests will vary depending on other symptoms your cat is displaying and may include the following:\n- Complete blood count: A blood test which counts the different cells in the blood which may reveal anemia (low red blood cells) and/or increased white blood cell count (infection or inflammation).\n- Biochemical profile: Performed on the clear/fluid portion of the blood the biochemical profile measures liver, kidney and pancreatic function as well as electrolytes and enzymes.\n- Urinalysis: A urine test to evaluate kidney function and level of hydration.\n- Multiple fecal examination tests: Fecal flotation, cytology, smear, zinc sulfate, culture, and sensitivity to determine if the cause is parasitic, bacterial, or protozoal.\n- FIV and FeLV blood tests.\n- Thyroid test: To evaluate for hyperthyroidism in middle-aged to older cats.\n- X-Rays: To check for blockage, foreign body or tumour and assess the internal organs.\n- Ultrasound: To evaluate for cancer or intestinal blockage and assess the internal organs.\n- Endoscopy and biopsy: An endoscope is a thin flexible tube with a light and camera which enables your veterinarian to see the small intestines and stomach. A tissue sample (biopsy) can be taken at this time.\n- Colonoscopy and biopsy: Similar to the endoscopy, a thin flexible tube is passed into the rectum and colon to view the structures and take a biopsy if necessary.\nTreatment naturally depends on the cause of diarrhea. Your veterinarian may recommend that you withhold food if your cat seems otherwise fit and well, water should still be provided. Food may be re-introduced after a day, this will usually be bland to rest the gastrointestinal tract.\n- Gastrointestinal blockage: Surgery to remove the blockage or laxatives to help it pass.\n- Colitis: Eliminate the cause where possible, such as switching your cat to a highly digestible diet.\n- Diet: Avoid switching types of food suddenly. If an allergy is a cause, gradually replacing his diet to a hypoallergenic type. Lactose intolerant cats should not have dairy.\n- Exocrine pancreatic insufficiency: Replacing enzymes with a pancreatic enzyme extract, feeding a highly digestible diet, antibiotics if necessary.\n- Hyperthyroidism: Surgical removal of the tumour, radioactive iodine treatment to kill the tumour or a prescription diet which is low in iodene.\n- Infection: Antibiotics for bacterial infection, anti-parasitic medications for worms and protozoan infection, supportive care for viral infections.\n- Inflammatory bowel disease: Switching to a highly digestible diet, corticosteroids, immunosuppressive drugs.\n- Kidney disease: Dietary changes to reduce protein and phosphorous binders.\n- Liver disease: Supportive care such as nutritional care and IV fluids, surgery to treat portosystemic shunt.\n- Neoplasia (lymphoma, carcinoma, and others): Surgery to remove the tumour if possible and/or chemotherapy to shrink it.\n- Pancreatitis: Supportive care such as painkillers, anti-nausea medications, and antibiotics.\n- Poisoning is usually treated by removing the toxin (where possible) by inducing vomiting or pumping the stomach, supportive care is commonly necessary.\n- Heinz body anemia: Treating the underlying cause, blood transfusions may be necessary for severely anemic cats as well as fluid therapy and supportive care.\n- Anaphylaxis: This is a medical emergency. Administration of epinephrine (adrenaline) will be necessary, which counteracts the body’s immune response. Supportive care will be necessary while your cat recovers.\n- Diabetes complications: Treating the underlying cause and managing diabetes correctly with diet, proper insulin administration and regular monitoring of your cat’s blood glucose levels.\n- Histoplasmosis: Mild cases may not require treatment, a severe infection will require anti-fungal antibiotics as well as supportive care.\n- Malabsorption: Finding and treating the underlying cause is necessary such as pancreatic enzyme supplements for cats with exocrine pancreatic deficiency. Supportive care may also be necessary.\n- Intestinal lymphoma: The mainstay of treatment for lymphoma in cats is chemotherapy, a combination of drugs will be used. Chemotherapy is well tolerated by cats although they may experience a few side effects such as anorexia or lethargy. Lymphomas tend to respond well to chemotherapy but this treatment is not curative, it can, however, give your cat some extra time.\nYour veterinarian may recommend your cat be fed Hills I/D while he recuperates, designed for gastrointestinal disorders, this food is highly digestible and low fat. Other supportive care may include IV fluids to treat dehydration and anti-diarrhea medications.\nOther supportive care may include IV fluids to treat dehydration, nutritional support, and anti-diarrhea medications.\nIf the diarrhea is mild and your cat otherwise seems well, you may wish to try treating it at home for a day or so, if symptoms persist or you notice other symptoms, seek veterinary attention. Never give anti-diarrheal medications to your cat unless your veterinarian has told you it is safe to do so as many human medications are extremely toxic to cats. Diet:\n- Feeding a bland diet of chicken and rice (mix 1 cup cooked chicken breast with 1/2 cup cooked brown rice) or baby food (make sure it contains no onion or garlic, which is toxic to cats) for several days can help give the unsettled digestive tract a chance to rest and recover.\nLactobacillus milk or plain yoghurt:\n- Lactobacillus, a type of friendly bacteria residing in the intestinal tract of mammals (including cats), these bacteria protect the body against harmful bacteria which can be of help. This can be of help to cats with diarrhea, especially if they have been or are on a course of antibiotics which don’t discriminate against good and bad bacteria. Giving lactobacillus milk can replace lost helpful bacteria.\nPumpkin: Either cooked (steamed or boiled) or canned (not the pie filler type) may be of help in relieving diarrhea. Add 1 tablespoon to food, or plain, if your cat will eat it.","Inflammatory bowel disease (IBD), also known as diet or food responsive diarrhea, food intolerance or food allergy, antibiotic responsive diarrhea, small intestinal bacterial overgrowth, protein losing enteropathy, and lymphangectasia, is a catch-all term used to describe a syndrome of chronic stomach and intestinal disorders as the result of inflammatory in the gastrointestinal (GI) mucosa.\nMore recently the term chronic enteropathy has been used. Although the exact cause is poorly understood, many believe genetic factors, interactions of dietary antigens and microflora in the intestine with the local immune immune system may affect the lining of GI tract.\nIBD is categorized on the cellular level by the type of inflammation present. This based on the result of biopsies. Categories of IBD include lymphoplasmacytic (most common), eosinophilic, and granulomatous.\nSymptoms Canine and Feline Inflammatory Bowel Disease (IBD)\nClinical signs if Inflammatory Bowel Disease in dogs and cats may include:\n- Lack of appetite (anorexia)\n- Weight loss\n- In cats, an increased appetite with weight loss has been reported\nDiagnosis of Canine and Feline Inflammatory Bowel Disease (IBD)\nDiagnosis of IBD in dogs and cats will be based on a combination of history, clinical signs and test results. Your veterinarian will ask you lots of questions about your dog or cat including information about the diet, medications and any possible exposure to parasites.\nThere are a variety of approaches to the diagnosis of IBD. Depending on the severity of your pet’s sign, your veterinarian may recommend these options prior to additional diagnostic tests.\n- Food trials. In stable patients with a normal albumin protein levels in the blood and without weight loss, a food trial using a hypoallergenic diet (novel or hydrolyzed protein diets) may be recommended. Clinical studies report that 40 – 60% of dogs and cats with chronic enteropathies respond to an elimination or hydrolyzed diet, which supports the value of a food trial in the treatment of IBD.\n- Empirical deworming with a dewormer medication such as fenbendazole and a course antibiotics such as with metronidazole or tylosin may be considered before pursuing further diagnostics.\nVarious diagnostics tests are considered when evaluating patients with IBD include:\n- Routine bloodwork including a Complete Blood Count (CBC) and biochemical profile (chemistry profile)\n- Fecal examination to look for parasites\n- Cobalamin (Vitamin B12 levels) and folate (Vitamin B9) levels\n- Serum Trypsin-Like Immunoreactivity (TLI) to evaluate for pancreatic disease\n- Pancreatic Lipase Immunoreactivity (PLI)cto evaluate for pancreatic disease\n- Fecal alpha-protease inhibitor to evaluate for intestinal protein loss\n- Abdominal ultrasound\n- Histopathology is the gold standard for the diagnosis of IBD. Endoscopy is the most practical and least invasive method of obtaining biopsies for histopathology. Full thickness biopsies by laparotomy or laparoscopy are useful in differentiating IBD from lymphosarcoma.\n- Recently, introduction of new diagnostic tests, immunohistochemistry, flow cytometry, T-cell clonality assay and PARR (PCR antigen receptor rearrangement) which can be performed on endoscopic biopsy are also helpful in differentiating IBD from the cancer lymphosarcoma. Standards for describing and grading endoscopic biopsies for IBD have recently been developed by the World Small Animal Veterinary Association Gastrointestinal Standardization Group.\nYour veterinarian will want to evaluate for obstructive, metabolic, infectious, and neoplastic disease of the gastrointestinal system when evaluating patients with chronic vomiting, diarrhea, and weight loss.\nTreatment of Canine and Feline Inflammatory Bowel Disease (IBD)\nTherapy for IBD in dogs and cats may include:\n- Corticosteroids remain the cornerstone of treatment of dogs and cats with IBD. The dosage and duration of therapy is based on severity of clinical signs, the type and severity of inflammation, clinical response, and drug tolerance. The dose is tapered based on response over 6-12 weeks. Budesonide is an orally administered corticosteroid with high topical activity in the gut with low systemic activity.\n- Dietary therapy. In dogs with a protein losing enteropathy and lymphangectasia, a low fat gastrointestinal diet is recommended.\n- Cobalamin (Vitamin B12) is important in many metabolic processes and low levels may result in a delayed or lack of response to appropriate therapy. Cobalamin is supplemented once a week for 6 weeks then on an as needed basis.\n- When a poor response to corticosteroids, elimination diet, and antibiotics is seen, additional immunosuppressive drugs such as azathioprine, cyclosporine, chlorambucil, and sulfasalazine (colitis) are indicated.\nPrognosis with Canine and Feline Inflammatory Bowel Disease (IBD)\nSome dogs and cats with IBD require either dietary management or medical therapy throughout their lives. Although IBD cannot be cured, the goal of treatment is to control the clinical signs with the lowest dose of medications possible. If reoccurrence of clinical signs is seen, re-institution or adjustments of medical therapy may be needed. Only a small number of dogs and cats with IBD are non-responsive to therapy."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:081aceea-d443-4542-bd73-0a6ba0397d3c>","<urn:uuid:5e38f1ca-3345-4f58-9e03-e9753556776c>"],"error":null}
{"question":"What role do beneficial insects play in both school gardens and compost systems, and how can their presence be encouraged?","answer":"Beneficial insects play vital roles in both settings. In school gardens, less than 1% of insects are harmful to crops, with most being beneficial - like ladybugs catching pests and insects that fertilize flowers. These can be encouraged by growing flowering plants and including weed patches. In compost systems, insects and worms are essential decomposers - particularly compost worms (Tiger worms, Indian Blues and Red Wrigglers) which can eat their body weight daily and produce valuable fertilizing castings. To support these beneficial organisms, both systems avoid toxic chemicals and insecticides, maintain proper moisture levels, and provide appropriate organic materials for them to process.","context":["There is plenty to do before starting on the garden itself. Here are four actions for raising environmental awareness. It is best to undertake them early:\nMapping the school grounds and the garden site encourages observation and helps with later planning.\nLaunching a “greening project” enhances the school grounds and draws attention to them.\nLooking at the existing garden terrain and wildlife raises children’s awareness of the ecosystem.\nStarting a compost heap reinforces understanding of soil and recycling, and prepares the soil in good time for the coming gardening season.\nA. MAKING MAPS\n1. Mapping the grounds\nStart by getting pupils to make sketch-maps of the whole school grounds - whatever they can manage for their age. Young children can do impressions, older lines pupils can make measurements and attempt realism (perhaps as part of their maths class). Make a sketch map yourself as well. A good map or picture sharpens the attention of those who make it, and will help in appeals, talks, explanations or grant applications. It can be presented to children, parents, sponsors, the garden group, the local authorities. It gives a basis for discussion about what needs doing and how much it will cost. It also boosts morale to have “before” pictures which can be contrasted with “after” pictures at the end of the year’s work.\nThe map should show all the main features of the grounds - for example, school buildings and facilities, trees and bushes, flowerpots, pathways, main roads, gateways, rubbish pits or bins, water supply, power lines. Label everything: this is a good learning exercise for pupils and helps outsiders to understand the map. Have children select which maps to display, and put copies in the Garden File.\nThe garden site If you have a choice of positions for the garden, mark possible sites for the garden, and use the map to discuss and decide where it should go. Ideally the garden should be:\nWill all these be possible? If there is a choice, open up the debate on where to put the garden. Consult children, school staff, parents and expert gardeners.\n2. Mapping and describing the garden site\nBefore starting major work, it’s also a good idea to make a full description of the proposed garden site, with a photo, drawing or sketch-map (students can usually do most of these), as suggested for the grounds as a whole in Part 4. The map should show the points of the compass and be labelled with appropriate information, as below.\nTwo garden sites\n|Notes on garden site: Goats eat plants. Fairly fertile soil, but never been cultivated: very hard. Needs plenty of organic matter. Plenty of water but has to be carried uphill.|\nMain work required: Clear ground of stones - keep stones for walls. Plough soil and dig in green manure. Bring water from river (handpump and pipe?). Fence in against goats.\n|Notes: Clay soil needs aerating/drainage/ humus. Just enough water for drinking and hand-washing - need extra for the garden.|\nMain work required: Keep bushes for afternoon shade. Find alternative water supply (harvest from roof?). Cultivate and enrich soil.\nThe map should show:\nPupils who have done the lessons Starting with soil and Soil quality (Part 3) will be able to add notes on the soil.\nB. SCHOOL GREENING\nWhile the Garden Team is thinking about sites and amenities, it can take the lead in encouraging the school to look at the school grounds as a whole. Improving the grounds need not take a lot of effort, and it can generate environmental awareness, improve amenities, raise morale, involve the community and bring the school some good publicity. A positive approach to the whole school environment also creates a favourable atmosphere for developing the school garden itself.\n|Green Belt school|\n|Gangadhar Bidyaniketan is a secondary school surrounded by paddy fields in eastern India. During the rainy season … students and teachers had to walk two kilometres in mud and water to reach the school. In the summer there was no water at all and children had to carry water bottles to school. There was not a single tree in the area. The school decided to make changes. Each student agreed to plant and care for one tree. Choosing the locations carefully, they created a green belt around the school. With the help of a local NGO the villagers built an approach road and a water tank on the school campus. The school now has plenty of drinking water and can use the surplus water for growing vegetables and flowers.… The land that was barren and saline is now green and colourful.|\nSuggest to the school and the parent-teacher association that they think about priority projects for the school environment. The questions below can be discussed with pupils, school staff and parents, using the map of the school grounds for reference, and followed up with the lesson Ideas for the school grounds (see the end of this chapter).\nDoes the school environment look good? How could it improve (trees, entrance, flowers, bushes, hedges, grass, divided areas, outdoor art, clearing or hiding rubbish)?\nDoes it have all the necessary facilities for health and healthy eating (e.g. toilets, washbasins, rubbish pits, cooking facilities, drinking water, tables to eat at, proper paths, seats, rain shelters, a stand/stall for serving or selling garden produce)?\nIs it a good place to play, relax, talk and study? What does it need (seats, shade, eating areas, study areas, a courtyard, swings, a performance area, trees to climb, a play house, a ball wall, a tree house, a courtyard for meetings or performances)?\nDoes it have room for wildlife (e.g. a wildlife habitat, a pond, a bird house/bird table, a woodlot)?\nWhich of these needs are high priority? Which can be tackled by pupils?\nSome possible “greening” projects are:\nGreening the grounds\nNote: Start the school thinking about improving its environment. But leave ambitious projects for the school grounds to others. A garden is quite enough to be getting on with!\nC. TAKING THE ECO-VIEW\nBefore starting major works in the garden itself, remember that your activities are going to interrupt an existing pattern of life. Nature’s garden is already established and working. Before you change this existing world by creating your own garden, get children to look closely at it (see lesson outlines Ecological audit, Garden citizens, Insects and others at the end of this chapter).\nThis will introduce pupils to the idea of ecosystems and interdependent systems of living things, and will help them to understand organic approaches to gardening. They will learn the valuable habit of making observations of insects, plants and earth, which can build up later into regular garden patrols. The results of these inspections can be added to the Garden File.\nD. STARTING COMPOST HEAPS\nIf you plan to use compost in your gardening, you will need to prepare your first compost heaps early (see Compost in Horticultural Notes and the two outline lessons on compost at the end of this chapter). Preparing compost reinforces children’s understanding of soil and the natural cycle of vegetation, introduces the idea of waste recycling and can begin to involve parents and families in making contributions to the garden. Decisions to make are whether you will have one big heap or several small ones, where you will put them, what you will use for compost and whether families can help.\n|SUGGESTIONS FOR ACTION|\nMake and display a map of the school grounds.\nOpen a debate on where the garden should go (if there is a choice).\nMap the garden site.\nSuggest that the school discuss priority needs and wants for the school grounds with school staff, the Garden Group, children and parents.\nBefore establishing the garden, encourage children to study the existing ecosystem.\nWork with pupils to start compost heaps.\nOutputs: Maps of school grounds and garden site\n|TIPS AND IDEAS|\n|IN THE CLASSROOM|\n|ENTRY POINTS These lessons aim to raise children’s environmental awareness. They look closely at the existing ecosystem and the role of insects, introduce children to composting and the idea of waste recycling and raise ideas for improving the school grounds.|\n1. Ecological audit Looking at nature’s garden.\nObjectives Pupils become aware of the existing ecosystem, its diversity and interdependence and recognize that it is a habitat for many forms of life.\nActivities In the classroom, students imagine approaching the garden site from different points of view: (1) Flying. They are flying slowly over the site - what do they see? What kind of terrain? (2) Landing. They land near a particular plant. What is it? What does it live on? What lives on it? What does it produce? Does it give shelter? (3) Creeping. They “shrink” to beetle size. What is around them? What is going on? What lives here? What passes by? What food is there? (4) Burrowing. They burrow into the soil like worms. How does it feel? Who lives here? Who eats what?\nPupils then go into the garden, repeat the exercise, and report their observations.\n(Adapted from Kiefer & Kemple, 1998)\n2. Garden citizens Agriculture depends on insects.\nObjectives Pupils recognize that most garden life is friendly, and start the habit of observing insects and other garden creatures.\nActivities Pupils find and observe garden creatures and in class describe what they have seen. The teacher adds live examples or pictures of common “garden citizens”. Pupils say how they feel about each, and why. Class groups take on the roles of insects, other animals, plants and soil, and say how they link to other groups (e.g. We’re birds; we eat insects). The insects then “die” and the class discusses what would happen if there were no insects (e.g. hungry birds, no fruit, poor soil). Discuss how to have friendly insects in the garden (e.g. by growing flowering plants, including a weed patch, and not using insecticides). Follow with a Bug Hunt or study a clutch of insect eggs hatching on leaves in the classroom.\n3. Insects and others Less than 1 percent of insects are dangerous to crops and many are beneficial.\nObjectives Pupils identify particularly beneficial insects and common harmful pests\nActivities Using real specimens or pictures, pupils identify the most common garden creatures, say what they know about them and speculate which are helpful, harmless or harmful. The teacher presents two “garden enemies” (e.g. slugs, aphids) and discusses what they do (chew or suck leaves or roots) and how we can see this (holes in leaves, plants wilting); then two “garden friends” (e.g. earthworms, ladybugs) which fertilize flowers, catch pests, turn garden waste into nutrients and open up the soil. Follow with a garden walkabout to spot garden friends and enemies or the signs of them; make a “Garden friends” poster or a Bug Book based on observations.\n(See Beneficial garden creatures, Pests, in the appendix Horticultural Notes).\n4. Compost Do this lesson in the garden before starting the compost heap.\nObjectives Pupils learn to recognize compost and appreciate its value.\nActivities The teacher introduces compost as plants’ favourite food and distributes handfuls to small groups. Pupils look, smell, feel, squeeze and say what they observe (brown, crumbly, damp, earthy, light). The teacher demonstrates planting a “happy plant”, showing how compost is added at various stages for various reasons. At the end, pupils chorus the answers to questions:\nIs this a happy plant? (Yes!) What makes it grow? (COMPOST!)\nWhat keeps the soil airy? (COMPOST!)\nWhat gives it food? (COMPOST!)\nWhat keeps it damp? (COMPOST!).\nThe teacher reads out a list of compost ingredients and pupils undertake to bring some from home for the compost heap.\n(see Compost in Horticultural Notes appendix.)\n5. Cooking compost This lesson prepares for compost making.\nObjectives Pupils appreciate the value of compost, know how to make it and start to use it.\nActivities Pupils recall the virtues of compost (gives nutrients; makes soil roomy and airy for roots to breathe and bacteria to work; holds water but also lets it run through; is natural and cheap). The teacher says making compost is like cooking: you need food, heat, air, water and a pot. S/he demonstrates by making a little compost in a bucket, talking through the process by asking questions about what to do next and why (see Making compost in Horticultural Notes). The class monitors the experimental compost, which will be ready in about two weeks. Fix a date for making the real compost heap, and ask pupils to bring contributions.\n6. Ideas for the school grounds A practical lesson in environmental awareness.\nObjectives Pupils make practical proposals for improving the school grounds and initiate action.\nActivities The teacher presents several ideas to the class (see Section D above), with pictures or sketches if possible. Older students add their own ideas. The class goes into the grounds to size up the possibilities (older students work in groups, one for each idea, and report back). For each idea pupils consider relevant questions, e.g. Where will it be? How big? What will it be made of? The class makes the final decision and suggests the first practical steps to take and who is to take them.","Compost bins and worm farms\nYarra residents can further reduce greenhouse gas emissions and the amount of waste that goes to landfill by using worm farms and compost bins.\nMore than half of household waste is made up of food and garden scraps. Using a worm farm or compost bin helps the environment by significantly reducing the amount of waste that is thrown away.\nOrdering and paying for your worm farm or compost bin\nYou will need to order and pay for your worm farm or compost bin before picking it up or before it is delivered to you.\nThis can be done at Richmond Town Hall (333 Bridge Road, Richmond), Collingwood Town Hall (140 Hodle Street, Collingwood) or The Connie Benn Centre (160 Brunswick Street, Fitzroy).\nMake sure to bring proof of residency, such as a driver’s licence or utilities bill.\nPick up or delivery\nDelivery of worm farms and compost bins is free and takes place on Fridays.\nIf you are not home on Fridays, you will need to provide Council with details of where best to leave your new worm farm or compost bin. Alternatively, they can be picked up at Council’s Depot, corner of Roseneath and Gray Street Clifton Hill, Monday to Friday 8.30am to 2.30pm.\nYou will need to bring with you your receipt of payment.\nHUNGRY BIN WORM FARM and COMPOST BIN\nDimensions: About the size of a small wheelie bin. Click on the link above to read more about this product.\nCost - $270.00\n210L BMW WORM FARM\nL 600mm X W 600mm X H 980mm\nCost - $48\nRELN WORM FARM\nL 570mm X W 395mm X H 640mm\nCost - $79\nWhat is worm farming?\nWorm farming is when you feed fruit and vegetable scraps to compost worms. There are three common types of compost worms; Tiger worms, Indian Blues and Red Wrigglers. These worms eat their body weight in a single day and can double in population every two to three months. The worms produce castings that make a great fertiliser for gardens and indoor plants. Keeping a worm farm requires minimal maintenance and is ideal for people living in flats or in houses with small backyards.\nHow do I worm farm?\nYou can purchase a variety of different worm farms from nurseries and hardware stores or you can make your own. Once you have your worm farm and compost worms, all you need to do is add your fruit and vegetable scraps each week. Add a small amount of food in the first week and increase this amount gradually over six months. Make sure to chop up the food first and include a variety of fruits and vegetables.\nCompost worms will also eat coffee grinds, paper, leaves and even damp cardboard. Do not add onions, garlic and chilli or acidic food such as oranges or lemons. Avoid meat and dairy foods or materials contaminated with toxic chemicals such as sawdust from treated wood. If uneaten food remains in the farm, you will know you have overfed the worms. Place a few layers of newspaper on top of the food to keep the moisture in your worm farm. Pour some water on the newspaper every few days during summer to prevent the worm farm from drying out.\nFor more information about worm farms visit Sustainability Victoria’s website\n220L COMPOST BIN\nW 770mm X H 780mm\nCost - $33.90\nWhat is composting?\nComposting is when household food scraps and garden waste is broken down to create a dark soil. This soil is nutrient-rich and provides an excellent fertiliser for your garden.\nHow do I compost?\nChoose a shady spot in your garden. Turn your compost pile regularly, breaking up clumps of food waste or adding twigs and newspapers to increase air spaces. Keep it moist, but not wet. Ensure you feed it the right food - almost anything organic can be composted, including fruit and vegetable. scraps, egg shells, tea leaves and bags, ground coffee, grass clippings, garden clippings, cut flowers and old potting mix, animal and human hair, vacuum cleaner dust and newspapers.\nMeat and dairy products are not recommended as they don’t break down easily, can rot and make your compost smell and will attract vermin. Animal manures, metals, plastics, glass and magazines are not recommended either. Healthy compost needs a balanced diet of nitrogen and carbon. Waste high in nitrogen includes food scraps and grass clippings and waste high in carbon includes paper, sawdust, straw and leaves.\nBuilding your compost\nThere are a number of methods for building your compost and each will take a different amount of time:\nLayering: add 10cm layers of vegetable and fruit scraps, grass clippings, leaves and shredded newspaper, covering each layer with a thin layer of soil and a small amount of fertiliser. Your compost will be ready in three to six months, but will take less time if it is turned regularly.\nAll together: add saved kitchen and garden waste to your compost at once and turn several times a week. This generates a lot of heat, making your compost ready in three to six weeks.\nCompost Worms: Layer your compost as usual but add compost worms. The worms will turn the heap for you making your compost ready in approximately three months. Many nurseries and hardware stores sell compost worms.\nYou will know your compost is ready to use when it is dark and crumbly. Always remember to wear gloves when handling the compost and adding the soil to your garden.\nFor more information about compost visit Sustainability Victoria’s website\nBack to top\nYarra City Council"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:fb493141-e77a-4d32-8437-08d44bf29d82>","<urn:uuid:dfe5e889-d513-4ca4-aaee-cb74fc841e46>"],"error":null}
{"question":"Which format provides better sound quality and compression: FLAC or MP3?","answer":"FLAC provides better sound quality than MP3 since it is a lossless compression format that does not discard any information, while MP3 is a lossy format that discards information and degrades sound quality. FLAC compresses files to nearly half the size of uncompressed formats without any loss in sound quality, and can provide resolution up to 32-bit, 96kHz (better than CD quality). Additionally, MP3 files deteriorate further each time they are edited and saved, while FLAC files can be compressed/uncompressed multiple times without any loss in quality.","context":["Praat for Beginners:\nTutorial: Sound file formats\n- Sound files can be very large, depending on time and sampling rate (e.g. speech recordings need at least 20000Hz sampling to capture [s t f], hifi recordings need higher sampling rates than 40000Hz).\n- One minute of CD stereo quality (44100Hz sampling and 16-bit resolution) is around 10MB.\n- One minute of mono telephone quality (8000Hz sampling, 16-bit) is just 1MB.\n- The large size of sound recordings means they occupy a lot of disk space, and makes it time-consuming or impractical to send them round the web, especially by e-mail).\n- Consequently ways have been devised to compress sound files to smaller sizes.\n- There are two kinds of compression, lossless and lossy.\n- Lossless compression does not discard any information, and is reversible, recovering all the original sound data when decompressed. Lossless compression is useful for speech analysis because it preserves the original recording intact.\n- Lossy compression discards information that can never be restored and inevitably means poorer sound quality when decompressed. Worse still, every time a lossy-compressed file is edited and saved anew there is further deterioration. Lossy compression is usually achieved by degrading the lower frequencies and the higher frequencies, which usually renders the recording useless for speech research.\n- Note that converting a lossy-compressed sound file to a lossless sound file, say MP3 to WAV, does not improve or restore the existing degradation of the MP3, the poorer quality MP3 file simply becomes a poorer quality WAV file. The advantage of conversion is that there is no additional degradation each time you work on your file and save it again.\n- If you are nevertheless tempted to use lossy mini recorders:\n- Remember that field expeditions are costly; cheap lossy recorders are not true savings.\n- Some mini recorders will also do uncompressed recordings; if so, ignore the lossy settings.\n- Make comparative tests before you do fieldwork, to make sure that the specific speech characteristics you intend to measure have not been degraded.\n- Log all lossy-compressed recordings and subsequent converted sound files as having been compressed at some time, so that no-one will think they are full quality. Better still, include and keep the method in the filenames you choose. A filename like dagestaniMP3.wav will always remind you how the degraded recording was originally acquired.\n2. Lossless sound formats\nPraat handles the following uncompressed and lossless-compressed sound file formats\n- AIFF: For Macintosh.\n- AIFC: A variant of the AIFF file format for Macintosh that is capable of compression. Praat reads and saves uncompressed AIFC files, but does not support compressed AIFC files.\n- Binary: a standard format for sound files, independent of computer system (if nothing else works, this could be a way of exporting data to other programs on other systems)\n- FLAC: An open source lossless compression format.\n- Kay: The format used by the Computer Speech Laboratory (Kay Corporation)\n- NeXT/Sun: A format used by NeXT computers and Sun computers\n- NIST: A format used by theTIMIT data base (American English, produced by Texas Instruments and Massachussetts Institute of Technology)\n- WAV: A format used by MS Windows\nYour choice of format is mainly determined by what is recognised by your computer and its sound system, by any other computer systems you intend to move between, and by any external program you intend to use your data with.\nAs long as you are only using Praat you are free to use at least AIFF, Binary, NeXT, NIST, or WAV, since the program can read them all independently of any computer system you may happen to move between.\n3. Lossy formats\n- Atrac: a proprietary sound data compression used in mini recorders and mini players (exclusive to some Sony products). Praat does not handle the Atrac format, so these files will need to be converted to one of the lossless formats listed above. For Atrac, you probably need the software supplied with the device. More recently, Sony have developed a lossless variant of Atrac, but this is not supported by Praat either.\n- MP3: is a patented lossy method, widely used in mini recorders and mini players. Praat opens MP3 files but does not save them. You might find conversion to other sound formats is available in some commercial or shareware sound programs (or your computer sound system might also come with software that converts MP3, check your manual).\n- MP3: See also MP3 and recording.","- Is FLAC the best quality?\n- Is FLAC or WAV better?\n- How do I get the best sound quality?\n- Does converting WAV to FLAC lose quality?\n- What is the best audio format for ripping cds?\n- Is FLAC better than CD?\n- Is 320kbs CD quality?\n- Is WAV good quality?\n- Does FLAC really sound better?\n- What is the highest audio quality?\n- How many GB is a FLAC song?\n- Is FLAC 16 or 24 bit?\n- Is FLAC better than 320 Kbps?\n- How many kbps is FLAC?\n- What lossless format is best?\n- What FLAC level is best?\n- How many kbps is CD quality?\n- Are FLAC files worth it?\nIs FLAC the best quality?\nA lossless file, the FLAC (Free Lossless Audio Codec) is compressed to nearly half the size of an uncompressed WAV or AIFF of equivalent sample rate, but there should be no “loss” in terms of how it sounds.\nFLAC files can also provide a resolution of up to 32-bit, 96kHz, so better than CD-quality..\nIs FLAC or WAV better?\nFLAC is a compressed lossless audio stream, WAV is uncompressed lossless audio stream. FLAC is like ZIP in audio world, you can compress/uncompress the data multiple times without any loss. … FLAC is superior to LPCM (WAV) in almost all aspects, but FLAC’s support is very limited.\nHow do I get the best sound quality?\nTurn on High-Quality Streaming on Android From the home page, tap on your avatar in the top-right corner. Next, select the “Settings” button. Choose the “Audio Quality On Mobile Network” or “Audio Quality On Wi-Fi” option. Finally, select the streaming quality option that you want to use on your Android handset.\nDoes converting WAV to FLAC lose quality?\nSave for bugs in your encoder software (if there are any), you will not lose any audio data going back and forth between FLAC compression and raw WAV. You won’t lose any data. … You’ll definitely save on space at no cost to quality but you may find yourself converting from FLAC back to WAV to work with the files.\nWhat is the best audio format for ripping cds?\nLossless files are recommended as the best format to rip CD:WAV,FLAC,AIFF/AIF,ALAC,WMA lossless.\nIs FLAC better than CD?\nWhile FLAC files are up to six times larger than an MP3, they are half the size of a CD, and can have the same boost in audio quality. Furthermore, FLAC is not just restricted to 16-bit (CD quality), and you can buy files up to 24-bit/192kHz for another potential boost in performance.\nIs 320kbs CD quality?\nBy way of comparison, a 128 kbps AAC file is said to be roughly equal to a 160-192 kbps MP3 file, with a 192 kbps MP3 being (as my test subjectively confirms) ‘near’ CD quality. … However, at higher rates (such as 320 kbps), there is little audible difference.\nIs WAV good quality?\nDespite being an older format, the WAV file has several major advantages when it comes to professional, high fidelity recording applications. It is an accurate, lossless format – in a nutshell, this means that the format reproduced the recording accurately without losing audio quality due to the format itself.\nDoes FLAC really sound better?\nYes and no. The thing is that, yes, there is a very clear difference in the sound when one listens to FLAC files. … That’s why many people claim to hear no difference between FLAC and MP3 and it is obvious that they choose MP3 over FLAC – in addition to same quality the size is smaller! But they are so deeply mistaken.\nWhat is the highest audio quality?\nThe highest quality MP3 has a bitrate of 320kbps, whereas a 24-bit/192kHz file has a data rate of 9216kbps. Music CDs are 1411kbps. The hi-res 24-bit/96kHz or 24-bit/192kHz files should, therefore, more closely replicate the sound quality the musicians and engineers were working with in the studio.\nHow many GB is a FLAC song?\nTHat’s very helpful! An easier way to look at it is to consider it as roughly 3 FLAC albums per GB.\nIs FLAC 16 or 24 bit?\nFLAC: Studio master sound quality (24-bit) and CD-quality (16-bit) … Unlike MP3, which throws some content away to reduce file sizes, FLAC is lossless, and works like a computer zip file. It’s uncompressed ‘on the fly’ as you play the music, and delivers exactly the same data present before the file was compressed.\nIs FLAC better than 320 Kbps?\nIn a nutshell, my opinion on the matter is that, for DJing, there is no difference in playing a flac or an mp3. In FACT, I find that there is really not much difference between a 192kbps mp3 and a 320kbps one either. Little system, big system, well-tuned system, it doesn’t matter.\nHow many kbps is FLAC?\n36,864 kbpsThe max bit rate of FLAC is probably around 36,864 kbps. This is a sample rate of 192 KHz, 24-bits per sample and 8 channels (the absolute max that FLAC will encode). The max size of a FLAC file is limited to around 4 GB and the max uncompressed file the encoder will take is also around 4 GB.\nWhat lossless format is best?\nFLAC: The Free Lossless Audio Codec (FLAC) is the most popular lossless format, making it a good choice if you want to store your music in lossless. Unlike WAV and AIFF, it’s been compressed, so it takes up a lot less space.\nWhat FLAC level is best?\nLevel 5The default compression level for FLAC is Level 5, and this is where an optimal balance between file size reduction and playback performance is achieved. To read more about the FLAC compression codec, click here.\nHow many kbps is CD quality?\nThere is no best bitrate, only the right bitrate. Audio CD bitrate is always 1,411 kilobits per second (Kbps). The MP3 format can range from around 96 to 320Kbps, and streaming services like Spotify range from around 96 to 160Kbps.\nAre FLAC files worth it?\nFLAC is worth it because you can convert it to other lossy formats as appropriate, without the issues of lossy -> lossy conversions. … In my system with my ears flac makes a big difference over 128 or 320. Though 320 can be tolerable. Hi rez can be even better but really depends on the master quality."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d2f7cba9-a137-446c-8e26-60645473759c>","<urn:uuid:847eb19a-6f9c-4fc4-8c52-1ac225797b9d>"],"error":null}
{"question":"How long can salmonella bacteria survive in a kitchen sink versus on leftover food at room temperature?","answer":"Salmonella can survive up to 32 hours in a kitchen sink. In comparison, when it comes to food left at room temperature, there is significant risk as food should not be left out for more than 2 hours at room temperature, or only 1 hour if the temperature is over 90 degrees, as lukewarm temperatures create ideal breeding conditions for bacteria. Salmonella bacteria specifically can survive on food surfaces for up to four hours before they're no longer infectious, though some strains like Salmonella enteritidis can survive for up to four days in amounts sufficient to cause illness.","context":["Note: The Pregistry website includes expert reports on more than 2000 medications, 300 diseases, and 150 common exposures during pregnancy and lactation. For the topic Food Poisoning, go here. These expert reports are free of charge and can be saved and shared.\nGetting a case of food poisoning is never a good thing. But it can be especially serious to one while you are pregnant.\nFood poisoning is also called foodborne illness, because you got it when you ate some food or drink that was contaminated with a bacteria, virus, or parasite. Symptoms of food poisoning include vomiting, diarrhea, fever, aches and pains, and weakness.\nBecause the vomiting and diarrhea may last a while and be very severe, you can become dehydrated, which in turn causes weakness and dizziness. In most cases, the symptoms of food poisoning go away in a day or so, but some cases can last for several days.\nMost cases of food poisoning show symptoms within a few hours of eating the contaminated food, but some types may take days before you start feeling ill. Not all “stomach bugs” are due to contaminated food, however. You can catch a virus or bacteria that causes vomiting and diarrhea from something that you’ve touched, or even from shaking someone’s hand.\nOne type of food poisoning, listeriosis, is especially dangerous for pregnant women because the bacteria that causes it can cause serious infections in your baby. The Listeria bacteria that cause listeriosis can survive refrigeration and freezing, which means that contaminated foods that are not cooked before being eaten may cause an illness even if they have been kept cold. These include foods like cold cuts, salami products, and soft cheeses like brie and queso fresco. Cheeses and dairy products that have been pasteurized are usually safe. Your doctor may have told you to avoid these foods while you are pregnant.\nRaw fruits and vegetables can be contaminated by bacteria such as E. coli. To be safe, wash any fruits or vegetables that you will eat raw before you eat it or cut it up. This is especially true for melons.\nHere are the best ways to help prevent food poisoning:\nKeep it clean. Wash your hands, your utensils, and any surface that you prepare food on. Wash your hands with soap and water for a full 20 seconds before and after you handle food. Wash counters and tables where you prepare food with a product that says it kills bacteria on the label or use a solution of one tablespoon of household bleach to one gallon of water.\nKeep it separated. Bacteria from raw meat and raw poultry can be spread from one surface to another and contaminate other foods, which is called cross contamination. Keep separate cutting boards for meats and vegetables so that you don’t cross contaminate. You might use cutting boards of different colors so that you don’t mix them up. Clean up any spills of juices from raw meats immediately with soap and water.\nCook it right. Make sure that you heat foods to a temperature high enough to kill viruses, bacteria, and any parasites. Don’t guess. Use a food thermometer. Beef, pork, lamb, and veal should be cooked to at least an internal temperature of 145 degrees. Ground meat should be cooked to 160 degrees. Chicken and turkey should be cooked to 165 degrees.\nCool it right. Refrigerate all leftover foods promptly. Food that are lukewarm are an ideal breeding ground for bacteria, so don’t let foods cool on the counter. Make sure your refrigerator keeps food at 40 degrees or lower. Your freezer should keep food below 0 degrees. Put food products in the refrigerator as soon as you get them home from the store.\nServe it right. Remember to keep hot foods hot and cold foods cold. If you are serving food at a buffet, use a chafing dish or slow cooker to keep foods hot. Use a dish or bowl nested into a bowl of ice to keep foods cold or put out smaller amounts of cold food and replenish the dish often. Food should not be left out for more than two hours at room temperature, and only for one hour if the temperature is over 90 degrees.\nBuy it right. Never buy foods that are past their “sell by” or “use by” dates. Foods that are past their expiration date may have become contaminated. If a can of food is bulging, throw it out.\nIf you have the symptoms of food poisoning for more than a day, call your doctor. If you are vomiting and/or having diarrhea, drink plenty of fluids.","Salmonella are destroyed at cooking temperatures above 150 degrees F. The major causes of salmonellosis are contamination of cooked foods and insufficient cooking. Contamination of cooked foods occurs from contact with surfaces or utensils that were not properly washed after use with raw products.\nCan Salmonella be killed by cooking?\nDoes cooking kill salmonella? Thorough cooking can kill salmonella. But when health officials warn people not to eat potentially contaminated food, or when a food is recalled because of salmonella risk, that means don’t eat that food, cooked or not, rinsed or not. The stakes are too high.\nHow long do you have to cook to kill Salmonella?\nThese bacteria reproduce very slowly, if at all, below 40 F and above 140 F. But note that the temperatures at which bacteria are killed vary according to the microbe. For example, salmonella is killed by heating it to 131 F for one hour, 140 F for a half-hour, or by heating it to 167 F for 10 minutes.\nCan Salmonella survive boiling?\nBoiling does kill any bacteria active at the time, including E. coli and salmonella. … And the spores can survive boiling temperatures. After a food is cooked and its temperature drops below 130 degrees, these spores germinate and begin to grow, multiply and produce toxins.\nCan Salmonella survive frying?\n“Any process in which the whites or yolks are insufficiently cooked — yielding whites or yolks that are still liquid — provides the potential for Salmonella to survive. … For instance, he said, eggs fried sunny-side-up or over-easy are only partially cooked and still can harbor bacteria.\nWhat disinfectant kills Salmonella?\nBleach-based cleaners kill bacteria in the most germ-contaminated sites, including sponges, dishcloths, kitchen and bathroom sinks and the kitchen sink drain area. Use bleach-based spray or a solution of bleach and water on cutting boards after every use to kill harmful bacteria like E. coli and Salmonella.\nCan you get food poisoning from frozen food?\nFrozen and raw produce may also carry germs that can cause foodborne illness. It is important to handle produce properly to prevent the spread of germs to your food and kitchen.\nDoes dish soap kill salmonella?\n“Soap is not a sanitizer. It’s not intended to kill microorganisms,” Claudia Narvaez, food safety specialist and professor at the University of Manitoba, explained to CTVNews.ca. “It will kill some bacteria, but not the ones that are more resistant to environmental conditions, like salmonella or E. coli.”\nDoes cooking kill salmonella in onion?\nWhat if onions are cooked? Cooking an onion will kill the salmonella bacteria, Warriner said. The real risk is that the bacteria could be on the outside of the onion, which could spread to kitchen surfaces and other ingredients when it’s chopped, he added.\nCan you kill salmonella in the microwave?\nCan microwaving or re-heating these foods kill the bacteria? If properly and thoroughly reheated, yes. That said, we know heat doesn’t help kill salmonella — it helps breed it — so when microwaving, you must be sure everything is re-heated to the same, proper internal temperature.\nHow long does Salmonella live on food?\nMost Salmonella bacteria live on dry surfaces for up to four hours before they’re no longer infectious. But Salmonella’s survival rate also depends on its species. A 2003 study found that Salmonella enteritidis can survive for four days in high enough amounts to still lead to illness.\nHow long does Salmonella live in sink?\nFoodborne-illness causing bacteria can remain on surfaces for a very long time. Campylobacter can survive in your kitchen for up to 4 hours, and Salmonella can last for up to 32 hours (and both can be found on raw poultry).\nHow quick is Salmonella?\nWhat Is Salmonella? Symptoms of Salmonella usually appear within six hours to six days after eating food (or touching an animal) contaminated with the bacteria and include. Nausea, vomiting, fever and diarrhea are all hallmark symptoms."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:26640b88-c878-46b8-a5ad-56888164e578>","<urn:uuid:a52e87cc-a1b3-4e09-8673-b087b4f987e3>"],"error":null}
{"question":"What are the unique flavor characteristics of blonde roast coffee, and how does it contribute to environmental sustainability through waste reduction?","answer":"Blonde roast coffee exhibits a light-bodied mouthfeel with subtle fruity and floral notes, and is roasted at 370°F just before the first crack stage. It features hints of toasted sweet malt and chocolate, with a smooth and clean finish. The beans maintain high acidity content and showcase the original varietal characteristics of the coffee. As for sustainability, blonde roast coffee, like all coffee production, generates waste that can be repurposed. The used grounds can be transformed into various sustainable products including biofuels, methane storage material, and compost. For instance, companies like Bio-bean in London convert used coffee grounds into biodiesel and biomass pellets, while researchers have discovered ways to use spent grounds for capturing greenhouse gases.","context":["As coffee enthusiasts, we are familiar with the classic trio of light, medium, and dark roast coffees, each offering a distinct flavor profile and aroma. However, the world of coffee roasting goes beyond these well-known categories, as other roast levels are gaining attention within the coffee community. Among these intriguing alternatives is the blonde roast coffee, a level just before the first crack, where the beans display unique characteristics that set them apart from their darker counterparts. In this article, we will delve into the fascinating world of blonde roast coffee, exploring its distinct flavor profile, roasting process, and how it compares to medium and dark roast varieties.\nWhat is Blonde Roast Coffee?\nHowever, the roasting spectrum is not confined to these three categories, as other roast levels outside the scope are getting attention within the coffee community. Some of these include white, gold, and blonde roast, existing outside the light roast end of traditional roast profiles.\n- White roast coffee is the mildest level, with an off-white color and a notably acidic taste profile.\n- Gold roast coffee are beans roasted at 345°F, resulting in a golden brown color with an aroma similar to toasted bread.\n- Blonde roast occurs at 370°F, at the edge of the first crack, which happens at 385°F. The beans begin to shed their chaff, and the sucrose starts to caramelize at this stage, resulting in a light-bodied roast with defined acidity and sweet undertones of caramel and honey.\nThe Coffee Roast Spectrum\nRoasting levels are a palette for roasters to craft their creations. Each roast level impacts the flavor, aroma, and body of the coffee to provide a distinct experience.\nWhile coffee roasting is a meticulous process involving the application of heat to green coffee beans to transform their appearance and taste, the roasting technique determines the outcome of blonde roast coffee.\nThe roasting spectrum includes various roast levels, generally classified into light, medium, and dark.\n- Light roasts have the shortest roasting time at the lowest temperature. The coffee on this level has a bright and vibrant flavor profile, highlighting the bean's varietal characteristics.\n- Medium roast profiles lie in the middle of the spectrum, characterized by a balanced flavor profile. They maintain the bright and acidic taste notes of light roasts and sweet flavors of caramelized sugars with the mouth feel of dark roasts. While it highlights some of the beans' origin attributes, it slightly caramelizes the coffee to produce a more rounded flavor profile. Examples: City & Full City Roast Coffee.\n- Dark roasts are at the end of the roasting profile, resulting in a shiny dark brown to almost black color, bold and more robust flavors with low acidity, and emphasized toasty and bitter notes. Examples: Italian & French Roast Coffee.\nFlavor Profile of Blonde Roast Coffee\nThe blonde roast is the level just before the first crack, making the beans have notably different characteristics from beans that have attained the first crack stage. Firstly, the beans are hard as the roasting time is too short to crack and soften them. Moreover, taste notes we typically associate with coffee are more subtle as the bean's sucrose is just beginning to caramelize.\nThe coffee also exhibits fruity and floral notes depending on its origin, with a light-bodied mouthfeel with a smooth and clean finish. They say coffee has over 800 flavor components that we experience depending on the aging, roasting, and brewing process. However, light roasts like the blonde style bring the bean's varietal characteristics, such as terroir, to the forefront. But, they also have a high acidity content, which may be a health concern for some sensitive stomachs.\n- It is an inviting and approachable blend\n- It has unique hints of toasted sweet malt and chocolate\n- Its flavor is enhanced when mixed with milk\n- The blend is inspired by Latin American coffee farmers\n- Offers complex flavors\n- Smooth and light taste\n- 100% Arabica coffee beans\n- Single Origin Colombian\n- Comes in a large 2.2 pound bag\n- Offers a unique and delectable taste\n- Very easy and efficient to prepare\n- Each tin can produce up to 40 cups\n- Ethically-sourced Arabica beans\n- Environmentally-friendly with recyclable tins\n- Rich and wholesome flavour profile\n- Inspired by Latin American coffee farming tradition\n- Mixes well with milk to bring out notes of sweet malt and chocolate\n- Very approachable coffee blend\n- Convenient to use with Keurig Brewers\n- Versatile for various sweetener and creamer additions.\n- Inspired by Starbucks coffee culture.\n- Perfect for enjoying at home or on-the-go.\n- High-quality Starbucks coffee for home use\n- Made with 100% recyclable aluminum capsules\n- Guaranteed coffee freshness\n- Creates a mellow, soft, and balanced espresso\n- Enhances the sweet side of milk in espresso drinks\n- Pleasant, light and crisp flavor\n- Ideal for daily brewing\n- Beans are whole for maximum freshness and taste\n- Notes of toasted sweet malt and milk chocolate\n- Mixes well with milk\n- Inspired by traditional Latin American coffees\n- Ideal for all coffee lovers\n- Easy to prepare\n- Bright and complex flavor profile.\n- Balanced notes of citrus and chocolate.\n- Suitable for any time of day.\n- Unique essences in every sip.\n- Demonstrates care and quality.\n- Sweet blend\n- Lightly roasted\n- Higher in acidity\n- Dynamic flavor nuances\n5 Coffee Drinks Ideas for Blonde Roast Coffee\nBlonde roast coffee's delicate and bright flavors make it a versatile base for a variety of delightful coffee drinks. For those who appreciate the pure essence of the bean's origin, a simple cup of brewed blonde roast coffee highlights its subtle fruity and floral notes, offering a refreshing and clean taste experience. Here are a few ideas for coffee drinks to try out Blonde Roast Coffee:\n- Brewed Blonde Roast: Enjoy the pure essence of the bean's origin with a simple cup of brewed blonde roast coffee, highlighting its subtle fruity and floral notes for a refreshing and clean taste experience.\n- Blonde Latte: Indulge in a classic and creamy treat by blending steamed milk with the light-bodied coffee, creating a harmonious balance of flavors.\n- Blonde Roast Iced Coffee: Stay refreshed on warm days with this icy and invigorating option, maintaining the coffee's vibrant qualities even when chilled.\n- Blonde Caramel Macchiato: Add an extra touch of sweetness to your coffee with a Blonde Caramel Macchiato, combining blonde roast coffee with velvety steamed milk and a drizzle of caramel syrup for a luxurious and indulgent sip.\n- Flavored Coffee Drinks: The clean finish and lack of lingering aftertaste of blonde roast coffee make it an excellent base for various flavored coffee drinks, allowing added ingredients to shine without overwhelming the coffee's natural charm.\nBlonde Roast vs Darker Roast\nBlonde roast coffee beans undergo a comparably shorter roasting time at lower temperatures. It results in beans with a light brown color and a dry surface, while medium roasts have a medium brown color with a slightly shiny surface. The bean's natural sugars are caramelized in the medium roast, giving a broader range of flavors. On the other hand, the blonde roast has an acidic taste with a clean finish and light-bodied mouthfeel.\nDark roasts are coffee beans roasted for a long duration at high temperatures, creating a richly flavored coffee. The beans are dark brown to almost black and usually have a shiny surface as the extended roasting releases coffee oils. Furthermore, the prolonged roasting process reduces the chlorogenic acid levels in the beans, lowering the acidity level.\nIn comparison, blonde roasts are light-bodied with bright acidity and subtle fruity and floral flavors, easily discernible on the palate. It has a clean and crisp aftertaste, while dark roasts are notably bitter with smoky notes.\nBlonde roast coffee is an excellent example of the delicate world of coffee roasting. This roast profile helps preserve the bean's original flavors, allowing its unique characteristics to take center stage. For coffee lovers, the fascination for blonde roast coffee lies in its refreshing and bright flavors, clean finish, and lack of lingering aftertaste, often associated with darker roasts. Moreover, blonde roast coffee offers health benefits due to its high antioxidant and caffeine content. So, whether you are a seasoned or a casual coffee consumer, we recommend trying the bright and mellow taste of the blonde roast to experience a unique coffee adventure.","Global coffee production creates in excess of 23 million tons of waste per year, according to sustainability researcher Gunter Pauli, from the pulp of fresh coffee cherries through the packaging that brings the roasted beans to your favorite barista. At the consumer end of the supply chain, used coffee grounds are the most visible example of this waste, the bit we dump in the bin after making each fresh brew.\nMost coffee lovers don’t give much thought to their spent grounds, but these black, sodden remains of pulverized coffee seeds have inspired the freshly caffeinated imaginations of scientists, entrepreneurs and social innovators from Melbourne to London to Seoul, sprouting into ideas of real consequence for the coffee industry.\nSustainability and the Economy\nAcross the world, companies, researchers and engaged consumers are pioneering the next era of environmental sustainability. Enabled by policy changes and innovative technologies, a new paradigm known as the circular economy is emerging, one that holds the promise of reshaping the global economy and transforming our relationship with the natural world — the coffee tree very much included. The aim of the circular economy is to close the loop of our industrial system, reducing resource consumption and environmental pollution by transforming waste into input material for the next stage of production.\n“Sustainability has evolved significantly from the late 1990s to now,” says Nina Goodrich, executive director of GreenBlue, a nonprofit based in Charlottesville, Virginia, dedicated to the sustainable use of materials. “The most difficult aspect of sustainability has been integrating this into your business strategy, and the circular economy provides a better framework to help companies do that.”\nThe circular economy traces its origins back to the 1970s, but it is only in the past few years that the concept has taken off, driven in large part by the work of the U.K.-based Ellen MacArthur Foundation. As Goodrich explains it, a circular economy is based on the recognition that “we have to move away from what folks call our linear economy of take-make-waste and envision a next life for what currently is our waste. … It’s about how we build and make things so that one person’s waste becomes another person’s input materials.”\nOver the past several decades, nongovernmental organizations have gradually succeeded in persuading businesses to adopt a variety of sustainable practices that ultimately transformed the specialty coffee industry. In 2012, for example, 40 percent of global coffee production was produced in compliance with a voluntary sustainability standard, up from 15 percent in 2008, according to the State of Sustainability Initiatives, an organization that reports on global sustainability projects.\nAt the same time, the way for-profit businesses view sustainability has shifted. What began as a somewhat radical concept has evolved into a set of pragmatic managerial practices aligned with corporate objectives, something businesses of all sizes have, for the most part, embraced.\nTransitioning to a circular economy will be a bigger challenge, but just as the global specialty coffee sector pioneered the adoption of sustainable production practices, it now stands poised to lead the food sector in closing the loop of production to reduce resource costs and environmental damage. This concern for the environmental impact of coffee production isn’t just altruism or public spirit. Climate change and accompanying public concern present a real threat to the viability of the coffee industry, and consumers are increasingly concerned about the environmental and social impacts of the coffee they consume.\nClearly these challenges are multi-faceted and won’t be resolved by any single movement, no matter how expansive, but adopting a circular economic business model, and thus reducing waste and alleviating the environmental impacts of non-sustainably sourced input materials, could make a significant impact. Fortunately, scientists, entrepreneurs and coffee lovers around the world are creating innovative ways to move the industry forward.\nReduce, Reuse, Recycle\nShane Genziuk is founder of Ground to Ground, a social enterprise based in Melbourne, Australia, that educates coffee lovers about the myriad wonders of used coffee grounds and connects them to cafes that bag used grounds for easy pickup. He estimates he’s signed up nearly 1,000 cafes, working in partnership with a liates in London and Austin, Texas.\nGenziuk — who has a full-time job and a young family, and is completing a Ph.D. in business administration — runs Ground to Ground on passion and copious levels of caffeine. He is acutely aware of the waste that goes into making his favorite drink, and he wants to make a difference by leading a movement for social change. When you educate consumers about the environmental impact of coffee production, he says, they typically recognize their own contributions to the global problem and, in turn, are motivated to become part of the solution.\nGround to Ground is, at its heart, an effort to encourage greater recycling and reduce waste. Genziuk distributes educational material on his website and in person to cafes on the numerous and diverse uses for coffee grounds, such as compost, de-icing material, hair dye and skin exfoliator, among others. Participating cafes agree to bag used grounds and offer them to customers free of charge, helping to divert organic waste from the land fill while sparking greater awareness about sustainability and the impact of individual consumption on the environment.\nMoving Beyond Recycling\nAccording to Zhu Dajian, director of the Institute of Governance for Sustainable Development at Tongji University in Shanghai, initiatives like Ground to Ground are important, but insufficient as a response to global environmental challenges. What sets the circular economy apart is the imperative not just to reduce impact, but to create new value while doing so. Only in this way can the shift away from the linear economy be realized.\nDajian helped to create a circular economy strategy for the Chinese central government, which is struggling to manage the environmental damage produced by its rapid industrialization. The circular economy, he says, is “a new economic model that will reduce environmental impact and at the same time provide new jobs.”\nOne of the key ideas of the circular economy draws its inspiration from industrial ecology, which studies the flow of material and energy through industrial systems in a manner analogous to the natural ecosystem. Just as nature wastes nothing, recycling nutrients in a closed loop, so proponents of the circular economy aim to make use of all waste as input for further value creation, doing away with the idea of unwanted byproducts.\nIn recent years, there has been a surge of companies using coffee waste — both used grounds and discarded coffee cherry pulp — to create new products, including paper, our, 3D printer lament, charcoal, textiles and numerous others. One of the most widely touted examples is Bio-bean, a London firm that has attracted considerable media and investor attention for its plan to collect waste coffee grounds from the city’s cafes and convert them into biofuels. The concept is based on research from the University of Nevada, which analyzed used grounds for oil content and found they contained on average about 10 to 15 percent oil by weight.\nBio-bean is the first company in the world to industrialize this process, and recently opened a 20,000-square-foot factory in north London capable of processing 50,000 metric tons (about 55,000 tons) of used coffee grounds per year — about one-tenth the waste grounds from all the coffee consumed in the U.K. The factory is designed to turn waste coffee into biodiesel, barbecue coals and biomass pellets. The company is even exploring the possibility of selling these pellets back to coffee shops to be used to roast coffee or boil water, which would create a true circular economy, with waste becoming the input power for the production activities that created it.\nBiofuel produced from used coffee grounds is referred to as “second generation,” meaning it isn’t made from crops that could otherwise be used as food. In a world of food scarcity, where the production of fuel from corn and sugar cane has led to skyrocketing food prices for the world’s poorest, second generation biofuels are particularly promising.\nThe Challenges of Waste Collection\nBio-bean recently partnered with Network Rail, which owns and manages the U.K.’s railway network, to collect the coffee waste generated by its six biggest stations. These sorts of partnerships will be crucial as companies strive to create products from used coffee grounds, because unlike other agricultural commodities, where processing and waste production takes place in a central location, coffee grounds are used and disposed of in thousands of locations dispersed across a wide area.\nGenziuk says this has been one of the largest barriers to his own enterprise, and he’s not sure the economics can be resolved. For him, that’s not necessarily a deal-breaker.\n“Maybe the idea isn’t to get rich out of it,” he says, “it’s just to do the right thing by society.”\nOf course, the circular economy isn’t being promoted by organizations such as the World Economic Forum strictly because of its potential to do right by society; it is also a massive economic opportunity. A 2014 report by the global management consulting firm McKinsey & Company suggested the adoption of circular economic principles could lead to savings in materials costs exceeding $1 trillion a year by 2025.\nFor its part, bio-bean is doing its best to prove Genziuk wrong. In addition to partnering with Network Rail, the company has partnered with recycling company First Mile to run a coffee ground collection service, sending a van to pick up grounds from cafes in central London. The cafes give bio-bean the grounds free of charge, as they would otherwise have to pay disposal fees (which are based on weight) to have the grounds carted to land fills. To date, around 100 London cafes have signed up to have their coffee waste collected, saving them money and providing bio-bean with free input material for its production process.\nWhether or not this sort of collection method is scalable remains to be seen. London is one of the most densely populated cities in the developed world, making this method of collection relatively economical. With coffee being produced in thousands of different pots and machines throughout a city, the dream of putting all coffee waste to good use still faces enormous obstacles.\nChanging public policy could help. Genziuk notes that cafes in Melbourne pay for waste collection by the bin rather than by weight.\n“If they throw out 100 kilos, they pay the same as if they throw out 5 kilos,” he says. “In the U.K. it’s different. They pay by weight, and that influences behavior. Our system is geared up for volume of waste, and there’s no disincentive for generating waste, so they tend to have wasteful practices.”\nDajian agrees that governments can play a role in changing economic behavior and encouraging the adoption of circular economic practices.\n“If raw materials are cheaper than secondary materials, I think it’s not possible to make the linear economy into a closed loop,” he says. He advocates a tax on raw materials and subsidies for pilot projects and businesses that are putting circular economic models into practice and looking to scale.\nCoffee Grounds and Greenhouse Gasses\nBio-bean’s innovation has been to extract the value from the leftover oil and combustible material in used coffee grounds. Meanwhile, researchers in South Korea have found value in the absorbency that allows grounds to retain oil in the first place. In a recent paper in the scientific journal Nanotechnology, Christian Kemp and his fellow researchers at South Korea’s Ulsan National Institute of Science and Technology show that heating, or activating, used coffee grounds using potassium hydroxide (i.e., lye) creates a new material with high surface area and the ability to store large amounts of methane.\nMethane is a powerful greenhouse gas, 84 times more potent as a warming agent than carbon dioxide. When returned to room temperature, the material created from the used grounds is stable and retains methane, meaning it could be used for long-term storage of greenhouse gases produced in industrial processes.\nKemp is a South African physical chemist with a focus in material science. His primary interest is in environmentally friendly materials with catalytic or gas absorption properties. Scientists around the world in this field are using sophisticated production methods to create high-tech materials that can absorb and store the gases responsible for heating our atmosphere — including carbon dioxide and methane. These novel compounds have demonstrated breakthroughs in storing gas, but the starting materials are enormously expensive, which could limit their widespread adoption.\nKemp was working on one such project when he was forced to rethink his approach. Unsure of how to move forward, he did what many of us do: He sat down for a cup of coffee with his colleagues. The coffee worked its magic, and the scientists landed on the idea to try using the coffee grounds as their input material.\nThey combined the grounds (Kirkland brand, 100 percent Colombian coffee, dark roast, fine ground) with lye, then activated the mixture at 800 degrees C (1,472 degrees F) for an hour in a furnace. Turns out, that’s all it takes to make coffee capable of capturing the gas. The process is simple, Kemp says, and the cost for input material is next to nothing.\nResearchers have used other organic materials, including corn cobs and cocoa husks, to do something similar, but, Kemp says, “coffee is just so good at absorbing the moisture and absorbing the lye that the process was easier to do compared with other materials.”\nKemp also sees the material as a potential medium for storing methane in natural-gas-powered vehicles, which produce far fewer greenhouse gases than traditional vehicles. In cars currently running on natural gas, the gas is contained in a large metal tank in the trunk, a dangerous and heavy storage system. This limits the appeal, at least in the United States, for buses and heavy trucks.\nThe activated coffee material has the potential to remove these limitations. Because it stores methane at such low pressure, it doesn’t need to be shaped into a conventional round metal tube to prevent the gas from exploding. It can be molded into any shape desired, even eliminating the need for a gas tank.\n“We can build the gas storage into the contours of the car,” says Kemp. “Because it’s malleable, it doesn’t need to fit to strict dimensions.”\nThe findings already have attracted attention from companies interested in the potential application in lithium ion batteries, the technology that powers laptops and smartphones. Just as the nano-scale pores in the activated coffee grounds capture methane atoms, the material also can be used to capture lithium ions.\nFor Kemp, it’s all in keeping with his “one Earth” philosophy.\n“We only have one Earth,” he says, “so we should at least try our best to recycle what we have, and use it in an appropriate manner and not just waste as much as possible.”\nWhen it comes to coffee, Dajian notes that only a small fraction of the material used and produced by the industry ends up in the cup; the remainder, from the skin of the coffee cherry at origin to the grounds left after brewing, ends up as waste. Reducing this massive output of waste, and ultimately achieving a true circular economy, will take decades of work and the efforts of many — from entrepreneurs and scientists to coffee roasters and coffee lovers — but the tide does seem to be turning."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1f7c43e0-081c-4140-8ed3-45c672795141>","<urn:uuid:7ea21c07-5f07-463a-9504-ecba9089bc4f>"],"error":null}
{"question":"How do drawing pens differ from regular pens when inking artwork?","answer":"Drawing pens use archival ink that is waterproof and fade-resistant, producing consistent, fluid, and graceful lines that stay put on the paper. Unlike regular pens, they are smudge-proof and ideal for inking pencil sketches. They come in various tip sizes, from very thin (0.05 mm) to thick, allowing artists to create different line weights for details and shadowing. However, it's important to note that even with smudge-proof ink, results may vary depending on eraser quality and pressure applied.","context":["|Pens||Pen Refills & Inks||Pencils||Erasers Etc||Pencil Cases & Bags||Paper||Craft||Office||Blog||New Arrivals|\nHave you ever looked at a perfect hand-inked drawing and wondered, how are the lines so clean?\nTo some extent, you must have steady hands -- like a surgeon or a pianist. And you can always modify things digitally, with programs like Photoshop and GIMP. But the first and most basic component is always the quality of the instrument you hold in your hand. Just as you wouldn’t use an Xacto knife to perform surgery, you wouldn’t use a ballpoint pen to ink a pencil sketch. Instead, artists use drawing pens with archival (meaning waterproof and fade-resistant) ink because they produce consistent, fluid, and graceful lines that stay put on the paper.\nGraphic drawing pens are also highly recommended by academics and writers of all kinds: researchers in the field, journalists, Asian language students, and Youth Group pastors alike find the precision and quality of drawing pens to be quite useful. Although they’re made to satisfy the rigorous demands of professional artists, they require no special care or maintenance.\nDrawing pens give you that extra bit of oomph to get through class, particularly if that class has anything to do with Asian Kanji characters. The thinner tip sizes, like 0.05 mm, 0.1 mm, and 0.2 mm, typically won’t bleed or feather. Japanese and Chinese students regularly write characters with ten strokes or more, and the most complex Kanji character tops out at a whopping twenty-nine strokes. There are more strokes in this single character than there are letters in the English alphabet! Appropriately, this Kanji means “depression” or “gloom” in Japanese... emotions you can hopefully avoid with the help of a smudge-proof, no-bleed drawing pen.\nAnother niche that drawing pens appeal to is the bible study set. Ink quality is very important when you’re working on a long-term project, especially when that long-term project has thin pages. Since most personal bibles are made to stay open at a certain page, like textbooks, fade-resistant archival ink is a must. That way, you don't have to worry about losing your work.\nStaedtler Pigment Liner Marker Pens\nSakura Pigma Micron Drawing Pens\nDrawing pens with medium-to-big tip sizes are excellent for journal writing, particularly if you’re on the road. If you’re looking to document a lengthy bike trip down the coast, or to jot down quick notes and impressions of a music festival, it might be worth it to invest in archival quality drawing pens. Since drawing pens are waterproof, they’re ideal for outdoors adventures. However, keep in mind that most drawing pens are not immune to the effects of changing air pressure, and may leak on airplane flights. If you want to be absolutely sure your pen won’t leak or break while traveling by air, a Fisher Space Pen is your best bet.\nOhto Graphic Liner Needle Point Drawing Pens\nUni Pin Pens - Pigment Ink\nWhen you’re inking a pencil sketch, smudge-proof ink is an absolute necessity. After meticulously outlining your illustration in three different line widths, the last thing you want is to have the ink smudge while you’re erasing the pencil lines underneath. Although most drawing pens are relatively smudge-proof, your results may vary depending on: the quality of the eraser, how much pressure you’re applying, etc. Erase with caution! For some instant inspiration, check out the insanely detailed doodlings of Ester Wilson (featured in this month's Artist Highlight).\nPilot Drawing Pens\nMarvy LePen Technical Drawing Pens\nWith comic and manga illustration, you generally want thin lines for facial detail, a thick brush tip for filling dark, shadowed areas, and a few sizes in between for everything else. Brush pens like the Pentel Pocket Brush Pen and Tombow Fudenosuke Brush Pen can create lines of many varying sizes and are renowned for their versatility, but do not mix well with markers or watercolors. Only use them if you’re planning to color your artwork digitally, or you’re not coloring your artwork at all.\nIf you want to throw markers (like Copic Markers) or a watercolor wash over the line drawing, you definitely want a pen with waterproof ink. All archival inks are waterproof, but not all waterproof inks are archival. Even if the ink is waterproof, give it some time to dry before applying the color layer!\nKuretake Zig Cartoonist Mangaka Outline Pens\nDeleter Neopiko Line 2 Pens\nArchival inks are manufactured specifically to resist fading, and are usually non-toxic and acid-free (or “pH-neutral”). Archival pigment inks tend to be more expensive than other inks, but are probably the best, most enduring inks that you can use for creative projects. When it comes to comic/manga artists and animators, archival ink pens are the industry standard. If you’re serious about journaling, invest in something like the Rhodia Webnotebook, which has the archival-quality paper to match.\nFelt tip (also known as “fibre tip”) pens have a nice and fluid feel, and the larger tip sizes are ideal for writers that like bold lines. They’re also useful for smoothing out shaky or spidery handwriting. Sakura Pigma Micron Drawing Pens are felt tip drawing pens at their best -- they’re crisp, dark, and great for carefully inking sketches. However, since felt tip pens are porous, they can get crushed by a hurried hand bearing down with too much pressure.\nHeavy writers should try a drawing pen with a metal tip, like the Ohto Graphic Liner Drawing Pen. The archival ink is quite capable of rising to any occasion, and the metal tip can withstand an absent-minded writer. In general, the Ohto Graphic Liners are perfect for accompanying travelers, journalists, grad students, and researchers in the field. Use the 0.05 mm pen for intricate outlines in a map, or data points in a graph. Save the 1.0 mm powerhouse for quickly shading dark areas of a chart, or adding emphasis to a particular topic.\nAlthough graphic drawing pens can be used for taking notes, journaling, outlining sketches, and comic / manga illustration, the line doesn’t stop there. They’re frequently used for greeting cards, wedding invitations, scrapbooking, and all sorts of miscellaneous crafts. Typographic sketches are often outlined with a black drawing pen, then digitized from there. Drawing pens can also be used in conjunction with blendable alcohol-based markers for concept art, fashion sketches, and book illustrations.\nThere’s very little that a good drawing pen can’t accomplish, given a little patience and a steady hand. Check out all the options in our Drawing Pens Selection Guide, and look out for new comic papers, pens, inks, and nibs in our Comic / Manga Pens section!\nWhat do you use your drawing pens for?\ncomments powered by Disqus"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ecff64ba-4992-4a4c-99a9-54e19dac8344>"],"error":null}
{"question":"What are the key housing affordability challenges in Ireland's rental market, and how does UCD's procurement policy address sustainability in its purchasing decisions?","answer":"In Ireland's rental market, affordability is a major challenge, particularly affecting middle-income earners who don't qualify for social housing but struggle to afford private market rents. The report recommends scaling the government's Cost Rental Scheme and reviewing the Housing Assistance Payment Scheme. As for UCD, its sustainable procurement policy requires considering environmental, social, and economic factors in purchasing decisions, including evaluating products' materials, origin, manufacturing, disposal, and encouraging the 'reduce, reuse, recycle' hierarchy while prioritizing sustainable products and services.","context":["Rental sector report identifies challenges and opportunities for a sustainable future\nIn partnership with Irish Institutional Property (IIP) and KPMG Future Analytics, Clúid Housing today announces the publication of Towards a Sustainable Rental Sector in Ireland: Understanding the Key Challenges & Opportunities. The report was launched by Minister for Housing, Local Government and Heritage, Darragh O’Brien TD and focuses on improving the sustainability and attractiveness of the rental sector in Ireland.\nThe research was compiled by KPMG Future Analytics on behalf of Clúid Housing and IIP as part of Clúid’s annual Adrian Norridge Research Bursary. The report offers an analysis of the rental sector and examines the many layers that need to be considered to ensure the sector offers a stable, secure and sustainable option for both tenants and landlords.\nThe key findings from today’s report launch show that between 2006 and 2016, owner occupation in Ireland fell from 74.6% to 67.6%. During the same period, private rental grew from 9.9% to 18.2%. Lack of supply, affordability, migration, increasing student populations, increasing precarity in the labour market, and increased demand for residential choice and flexibility have all contributed to the growth of the rental sector in recent years. The report also found that nearly a third of all households now occupy rental accommodation.\nAlong with this growth, the landscape of the sector has also changed. 18.2% of all households rent privately, while 9.5% rent from either a social landlord, local authority or non-profit body. The typical profile of the private renter is younger, and renting is the majority tenure for those under 35 years of age.\nThe report found a growing trend toward apartment living. Apartment stock has grown by 44% – from 140,000 units in 2006 to 201,000 units in 2016. 72% of apartments nationally are within the rental sector. Meanwhile, the research also determined that while large-scale institutional landlords (excluding AHB’s) only make up 4.6% of tenancies, their activities are pronounced at a local level.\nIn drawing together this profile of the sector, the research makes some key recommendations including:\n• Scaling of the government’s Cost Rental Scheme;\n• The important role of AHBs in the rollout of the Cost Rental Scheme;\n• The incentivisation of smaller scale landlords;\n• Achieving a more certain regulatory environment;\n• A review of the Housing Assistance Payment (HAP) Scheme;\n• Enhanced delivery of apartment stock;\n• Recognising new and emerging housing demand drivers;\n• Addressing viability challenges.\nThe report highlights the fact that a significant cohort of people who are middle-income earners and do not qualify for social housing are struggling to afford to rent in the private market. Thus, the report recommends that the affordability of renting is a key issue to be addressed in the sector.\nNational planning framework\nAs well as examining national trends, the report examines nine ‘study areas’ covering five major cities (Dublin, Cork, Galway, Limerick and Waterford), and four towns (Athlone, Castlebar, Dundalk and Letterkenny). Whilst highlighting urban centres serves to reflect the goals of the national planning framework, comparing towns and cities also provides an overview of the differences between regional and local contexts.\nThe research was conducted through online surveys and interviews and includes a comprehensive analysis of key data sources. Whilst the sample sizes of the surveys do not claim to be fully representative of the whole sector, they provide valuable insight into the trends, influences and issues that have been identified elsewhere in the report.\nThe research identifies the contribution of different providers within the sector, including non-profit organisations like Clúid and local authorities, large institutional landlords like those represented by IIP, and smaller landlords. It looks at the interface between them and how they each contribute to the wider landscape and evolving needs of the market. The research also examines demographic, socio-economic and spatial factors affecting supply and demand within the sector.\nSupply and affordability were identified as the dominant challenges facing the sector. A number of other challenges were also identified. These include:\n• Security of tenure;\n• Quality of rental stock;\n• Construction and land costs;\n• Uncertainty in the regulatory environment;\n• Sustainability and effectiveness of rental support mechanisms;\n• Tax incentives.\nCommenting on today’s report launch, Minister for Housing, Local Government and Heritage, Darragh O’Brien TD, said: “I’m very pleased to be launching this report today. It’s a comprehensive analysis of how the rental sector has developed in Ireland and brings together the many facets that influence the market, including the crucial role of both social and private landlords. There’s no doubt the sector has grown in recent years and moving forward, we need to make sure it develops as a viable rental option as it’s a key aspect of how we meet our current and future housing demands.\nWe’re already moving forward with some of the recommendations made in the research including developing cost rental schemes and in the coming months I will bring in a new comprehensive bill for renters to the Oireachtas which will deal with security of tenure and sustainable rent levels. This report will certainly add to our understanding of the challenges faced in the rental market.”\nSpeaking at the launch of the report, Fiona Cormican, Clúid Housing’s New Business Director, commented: “The rental sector in Ireland is a growing market and this research demonstrates the essential role in meeting the housing needs of our population now and into the future. To meet the challenge of ensuring the sector’s growth is sustainable and relevant, we must understand the many factors that have influenced its development so far and the policy responses that are required to meet demand into the future.”\n“In order for the rental sector to thrive, it is essential that it offers a stable, secure and sustainable option for both tenants and landlords. We cannot continue regarding renting as a mere transitory sector but promote renting as a viable housing choice. This report will help us to understand the challenges facing the sector and therefore we hope will contribute positively to its future development.” she continued.\nPat Farrell, CEO of Irish Institutional Property added: “Institutional investors represent a small but growing source of supply, are here for the long term and therefore have a keen interest in the development of a sustainable rental sector that meets the needs of all stakeholders. We welcome the opportunity in partnership with Clúid to support this research which we trust will contribute to a reasoned and evidence-based discussion on how the sector evolves to meet demand and renters’ need for a stable and secure supply of good quality residential rental accommodation that meets their changing needs over time.”\nNotes to the Editor:\nCommunications Manager – Clúid Housing\n+353 87 113 5411\nAbout Clúid Housing\nEstablished in 1994, Clúid Housing is an independent, not-for-profit charity. Clúid is one of the largest approved housing bodies (AHBs) in Ireland with over 8,300 properties in management. Clúid leads the way in delivering social housing solutions to those on local authority housing lists. Our team of over 250 professional employees are committed to providing quality housing and services that enable people to create homes and thriving communities. For more information visit www.cluid.ie\nAbout Irish Institutional Property\nIIP is the voice of institutionally financed investors with significant international backing in the Irish real estate market. The mission of IIP is to promote the development of a sustainable world-class real estate sector in Ireland that benefits members, the economy, communities and wider society. IIP members manage approximately €14bn of Irish property.","Sustainable development and climate change are two of the greatest challenges of our time and require attention and action from individuals, organisations and governments. UCD recognises the important role it has to play as a leader in these global challenges, not only in research and teaching but also in the operation and development of the UCD estate. The University seeks the attainment of a sustainable, healthy and living campus and as such endeavours to manage the campus in a way that considers energy and water usage, waste management, sustainable commuting and biodiversity in all of its activities where relevant.\n“University College Dublin’s vision is to be an exemplar institution in energy management. We shall meet or exceed legislation and best practice relating to all aspects of energy use.\"\nUCD achieved the 2020 targets 1 year early!\nUCD is playing its part in the global effort against climate change through the efficient usage of energy and increasing the amount of energy sourced from low-carbon or renewable generation.\nAs a result of these efforts, UCD managed to reach the 2020 targets one year early. The targets required a 33% improvement in energy efficiency with UCD achieving 24.1% based on calendar year 2019.\nAs a result of Covid-19, and reduced activity on campus, consumption was unusually low in 2020. However, the UCD Energy Team, operating within Estate Services took this as an opportunity to review building base-loads and ensure that the maximum level of energy savings were achieved during this period while maintaining essential services.\nThe UCD Energy Unit manages the UCD Energy Management System (EnMs), which has been developed to meet ISO50001 standard – the international standard in energy management.\nHowever, the UCD community, which is the ultimate user of this energy also has an important role to play by “switching off” all non-essential equipment, reporting maintenance issues such as lights remaining on at night and closing windows each evening.\nNew targets for the University include a reduction carbon emission by 50% and a further improvement in energy efficiency to 50% by 2030. These are ambitious targets and will require the continued participation of students, staff, and student residents.\n“UCD Estate Services supporting the Green Campus Committee in its goal to achieve the An Taisce Green Flag for UCD”\nIn 2015 a group of students and staff set up a Green Campus Committee...\nIn 2015 a group of students and staff set up a Green Campus Committee. The group, which is chaired by the VP for Campus Development, Professor Michael Monaghan, aim is to achieve the An Taisce Green Flag for UCD.\nThe Green-Campus Programme (based on the successful Green-Schools Programme) provides a means to foster environmental awareness in a third level institution in a way that links to everyday activities and study, and ties in with the operational requirements of a complex multi-use facility.\nThe Green Campus Committee registered with An Taisce in early 2016 and aims to achieve the Green Flag by the end of 2019.\n“To continue to reduce the use of potable water and find effective, efficient and sustainable ways to source the University's water needs.”\nA new strategy for a growing campus...\nWith the knowledge of continued expansion due to UCD’s intensive capital building programme and with ageing water infrastructure in some areas of the campus, a strategic plan was formulated aimed at carrying out a major renewal and upgrade programme over a 5 year duration starting in 2010/2011.\nThe over-riding aim of this strategy was to conserve water, improve control, reliability, firefighting ability, water quality and monitoring of the existing water main network and to enhance surface water protection.\n“Minimising the impact of UCD generated waste on the environment and increasing recycling rate”\nLess than 10% of UCD's waste ends up in landfill.\nUCD produces approx. 2,000 tonnes of Mixed Municipal Wastes per annum. Through processes used by its waste management service providers, less than 10% of this waste ends up in landfill.\nEstate Services is working to achieve higher direct recycling rates, alterations to the management of the external component of the waste management process has taken place over the last 18 months and systems are now in place to manage the transport and bulk collection of different waste streams.\n“To protect and enhance the UCD natural environment in a way that fosters biodiversity while providing a recreational amenity for the UCD community”\nThe 335 acre UCD Belfield Campus is an invaluable asset...\nThe 335 acre UCD Belfield Campus is an invaluable asset, providing a woodland and parkland backdrop to Ireland’s largest University. The campus features almost 40 acres of woodland with approximately 50,000 tress, natural watercourses and lakes, parkland and wildflower meadows, all of which support a rich mixture of flora and fauna.\nThe woodland is made accessible by a series of interconnected woodland pathways, ensuring this unique resources can be enjoyed by the UCD Community.\nThe University Sustainable Procurement Policy supports, and is supported by, the University's Sustainable Development Goals. This policy applies to all members of staff at UCD who are involved in purchasing goods and services.\nThe purpose of this policy is to support the University's overarching Strategic Plan and to ensure that staff involved in the procurement of goods, services and works throughout the University consider appropriate environmental, social and economic factors in their purchasing decisions.\nSustainable Procurement – Definition\nSustainable Procurement is an approach to sourcing fit-for-purpose products and services that takes into account the economic, environmental and social impacts of a supply network.\nProcurement, Contracts & Buying Function\nAlso known as ‘green purchasing’, sustainable procurement addresses environmental and social factors, as well as the total costs associated with each purchase. It means looking at what products are made of, where they come from, how they are made, and how they will be disposed of. It is also about evaluating whether a purchase needs to be made at all, e.g., consider renting/ leasing, loaning or sharing existing equipment with various schools/departments on campus and with other institutions.\nSustainable Procurement should consider the environmental, social and economic consequences of: design; nonrenewable material use; manufacture and production methods; logistics; service delivery; use; operation; maintenance; reuse; recycling options and disposal of all goods and services purchased by the University. Suppliers' capabilities to address these consequences throughout the supply network should also be taken into consideration.\nUCD has a long-standing commitment to leadership in the area of sustainability. As part of that commitment, the Procurement, Contracts and Buying Office has incorporated the principles of sustainable purchasing into many of its contracts. Consistent with the University’s sustainability goals, UCD shall require all tendered suppliers to incorporate sustainable practices and sustainable elements in the goods and services they provide. UCD will promote environmentally responsible procurement by identifying sustainable products and services, practices, processes and procedures.\nUCD’s objectives for Sustainable Procurement are to:\n- comply with all relevant legislative and regulatory requirements;\n- reduce the demand for products and services by promoting the waste hierarchy of ‘reduce, reuse, recycle’, thereby cutting down on waste and promoting re-use and recycling;\n- source sustainable products and services where reuse or repurposing are unavailable;\n- prioritise environmental, economic and social repercussions when purchasing goods or services;\n- include sustainability criteria when evaluating tenders from potential suppliers;\n- ensure the efficient management of existing resources;\n- promote compliance with UCD’s sustainable procurement guidelines when considering the purchaseof goods or services; and\n- encourage all staff commissioning procurement of products or services to consider sustainability issues that their purchase may raise.\n“As a community, we have to take responsibility for the impact of commuting, which is why we have prepared our first ever UCD Travel Plan.”\nWe’re on the road to a healthier and more sustainable community...\nAs a Smarter Travel Campus Partner in conjunction with the National Transportation Authority (NTA), our ambition is to be recognised as an exemplar in sustainable transportation.\nThe UCD Travel Plan is key to achieving more sustainable travel patterns associated with the University. It is also referenced as a specific objective of the Dun Laoghaire Rathdown County Council (DLRCoCo) County Development Plan 2016-2022. The implementation of this Travel Plan will be funded by revenues raised from permit parking on our campuses."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:df51bc2f-b77c-4adc-bc6f-7cfba7f3f02d>","<urn:uuid:37f3e453-efc5-4cae-8347-c68d76729a7b>"],"error":null}
{"question":"How do the ethical oversight responsibilities compare between charity boards and the IDEA Foundation board when it comes to organizational culture?","answer":"Both boards have significant oversight responsibilities for organizational culture, but with some distinct approaches. Charity boards must ensure they properly discharge duties without rubber-stamping while avoiding over-involvement in operations, and need to monitor how inappropriate behaviors are recorded and dealt with. The IDEA Foundation board has explicit responsibility for implementing and monitoring ethical standards across the foundation, its partners and projects, with specific focus on meeting UN Global Compact principles and core values around leadership, ethical standards and continuous improvement. Both types of boards must ensure organizational culture aligns with their institution's purpose and values.","context":["The Financial Reporting Council’s new UK Corporate Governance Code [PDF] was published earlier this week. Reading it I think there are matters that resonate for charities.\nThere is obvious common ground between the roles of the board in all sectors, but charity boards are invariably made up entirely of non-executives who are not usually involved with management. There are also different perspectives of ownership, contribution and management. For example, trustees of a charity do not own what they work for; individuals guide, support and influence with the key aim of acting in the best interests of the organisation overall, rather than a constituency or grouping. Importantly, this should not fetter those who work for the organisation or those for whom it is working, nor should it deny the flexibility that is needed if activities and services are to adapt to changing needs.\nSection 177 of the Charities Act 2011 defines charity trustees as \"the persons having the general control and management of the administration of the charity\". This definition sometimes causes confusion because of the concept of it involving management and administration. It is important to recognise that what is intended is that trustees need not actually carry out the management and administration, but be responsible for them. Boards are also required to make the appropriate level of challenge and make decisions on key issues.\nWith most large non-profits, the board has had the foresight to appoint professional management to manage the organisation. This sometimes causes a dilemma. A charity trustee’s position stands in contrast to that of the non-executive director of a company in the private sector, who shares their legal responsibilities with their executive colleagues on a unitary board. In some ways, the greater the competence and professionalism of management, the greater the challenge in understanding the issues of governance and management. Trustees must properly discharge their duties without being seen to simply rubber stamp, but at the same time they should not become over-involved in operational management to the detriment of their critical perspective.\nI have carried out many governance reviews and getting the balance right can be challenging. The correct balance will be achieved only when individuals or teams have a clear understanding of responsibilities, the authority necessary to fulfil these responsibilities and the accountability for the consequences of what they have done or failed to do. This accountability is required not just from management but also from boards. A key question is how the board and management are held to account for ensuring that they foster and indeed enforce the right culture and behaviour.\nThere is much more to it than ensuring compliance with standard policies and procedures, and the FRC’s updated corporate governance code now places much emphasis on culture and behaviour. It explains that \"the board should assess and monitor culture. Where it is not satisfied that policy, practices or behaviour throughout the business are aligned with the company’s purpose, values and strategy, it should seek assurance that management has taken corrective action.\" The charity governance code espouses similar requirements.\nAt a Crowe charity breakfast briefing called \"Culture and Behaviour Trumps Policies and Procedures\" there was widespread agreement that board members often wrongly take for granted that the right culture and behaviour permeate through the organisation. Boards need to take time to ensure that organisational culture or values are discussed as part of the formal board agenda. Ethical dilemmas need to be discussed and the decisions should be carefully reviewed. Sometimes this needs to be a theoretical discussion in anticipation of what might go wrong – for example, the risk that organisational or personal performance indicators can lead to or incentivise inappropriate behaviours. An important question for boards is how incidents of inappropriate behaviours or unwanted culture are recorded, monitored and dealt with. The position in the past was that proper accountability and transparency led to unwanted publicity that should be avoided. This approach is no longer seen as appropriate and can lead to damaging consequences.\nThis article first appeared in Third Sector.","Ethical guidelines for IDEA Foundation\nEthical guidelines and values - IDEA FoundationBackground and definitions\nA code of ethics is a set of principles of conduct within an organization that guide decision making and behavior. The purpose of the code is to provide members and other interested persons with guidelines for making ethical choices in the conduct of their work. Professional integrity is the cornerstone of credibility in a working community. Member of an organization adopt a code of ethics to share a dedication to ethical behavior and adopt this code to declare the organization's principles and standards of practice.\nViews on ethical considerations have changed greatly in recent years and are still evolving. Idea Foundation will work hard to meet our own and our stakeholders expectations to be at the forefront of this development. Establishing and implementing ethical standards and values for Idea will therefor be an ongoing and dynamic process, constantly under revision and improvement. The clear responsibility for this process lies with the board of directors. The board also holds the ultimate responsibility for implementation and monitoring of activities conducted by the Foundation it self, its partners, stakeholders and projects, supported by the Foundation.\nIdea Foundation is a non-profit organization and should not be compared with a traditional investor, maximizing his returns. Our ambition is never the less to influence and contribute to development of sustainable growth and prosperity through out all the projects and geographical areas where we are involved. To be able to successfully reach our long term ambitions The board of directors with Idea Foundation have set ethical standards, that as a minimum must meet the principles of ethical investment management, including endorsement of the UN principles for responsible investment (UN-PRI).\nThe UN Global Compact asks companies to embrace, support and enact, within their sphere of influence, a set of core values in the areas of human rights, labor standards, the environment, and anti-corruption:\nPrinciple 1: Businesses should support and respect the protection of internationally proclaimed human rights; and\nPrinciple 2: make sure that they are not complicit in human rights abuses.\nPrinciple 3: Businesses should uphold the freedom of association and the effective recognition of the right to collective bargaining;\nPrinciple 4: the elimination of all forms of forced and compulsory labor;\nPrinciple 5: the effective abolition of child labor; and\nPrinciple 6: the elimination of discrimination in respect of employment and occupation.\nPrinciple 7: Businesses should support a precautionary approach to environmental challenges;\nPrinciple 8: undertake initiatives to promote greater environmental responsibility; and\nPrinciple 9: encourage the development and diffusion of environmentally friendly technologies.\nPrinciple 10: Businesses should work against corruption in all its forms, including extortion and bribery.\nIn addition to the The Ten Principles of the UN Global Compact, the Idea Foundation has established a set of core values to be followed in all aspects of our behavior and in our relationships internally and externally. Idea Foundation has a clear ambition to take an active stand in implementing the same core values in all our projects and businesses established and developed on the basis of support from Idea Foundation.\nWe want to be a leader in every aspect of our involvement and our business. In the development of our team leadership skills at every level; in our management performance; in the way we design, build, and support our products and services; and in our financial reporting.\nWe are committed to practice the highest ethical standards, and by honoring our commitments. We will take personal responsibility for our actions and treat everyone fairly, with trust and respect.\nWe will strive for continuous quality improvement in all that we do, so that we will rank among the best in customer, employee, and community satisfaction.\nPeople working together\nOur strength and our competitive advantage is and always will be – people. We will continually learn, and share, ideas and knowledge. We will encourage cooperative efforts at every level and across all activities in our company.\nGood corporate citizenship\nWe will provide a safe workplace and protect the environment. We will promote the health and well-being of people and their families. We will work with local communities to support education and other worthy causes.\nFundamental equal rights and respect for all individuals Equal rights offering same opportunities to all, despite, race, religion, sex.\nThe Foundation will support business ideas and projects that can document a long term sustainable development.\nCorporate Social Responsibility\nNo matter how small a business is it can have a strong belief in social responsibility.\nThe ethical guidelines for the Idea Foundation are promoted through the following four measures:\nExercise of power to support proposed and established projects\nExclusion of companies and projects\nObservation of companies and projects\nThe role of the board and the investment committee is to provide evaluation on whether or not investment and support in specified companies and projects is inconsistent with the established ethical guidelines and The UN Global Compact. This work shall be based on the following principles:\nHonesty and Integrity: We act with honesty and integrity.\nProfessional Behavior: We operate within the letter and the spirit of applicable laws.\nCompetence: We apply appropriate skills and capabilities to every\ndecision we make\nObjectivity: We are objective in forming our professional opinion\nConfidentiality: We respect the confidentiality of information.\nFair Business Practices: We are committed to fair business practices.\nResponsibility to Society: We recognize and respect the impact we have on the world\nRespect and Fair Treatment:We treat all our colleagues with respect, courtesy and\nAccountability and Decision-Making: We lead by example, using our Core Values as"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:ce153572-ab9b-4e97-932a-0110522224e8>","<urn:uuid:10159761-6257-4e01-b061-4205ae03e2e1>"],"error":null}
{"question":"What are the economic advantages and leadership challenges of implementing a democratic, community-based system like Marinaleda's?","answer":"The economic advantages include maintaining steady local employment despite national unemployment rates of 26%, providing affordable housing through cooperative building programs (€15 monthly mortgages), and operating facilities at a loss to ensure accessibility (like €3 summer pool access). However, the leadership challenges include potential delays in decision-making, as democratic processes require extensive consultation and consensus-building. As seen in traditional democratic leadership settings, this can lead to procrastination when urgent decisions are needed, and leaders may struggle when quick, authoritative decisions are required during crises. Additionally, there's the challenge of managing expectations when multiple solutions are proposed but only one can be implemented.","context":["In the south of Spain, the street is the collective living room. Vibrant sidewalk cafés are interspersed between configurations of two to five lawn chairs where neighbours come together to chat over the day’s events late into the night. In mid-June the weather peaks well over 100 degrees Fahrenheit and the smells of fresh seafood waft from kitchens and restaurants as the seasonably late dining hour approaches.\nThe scene is archetypally Spanish, particularly for the Andalusian region to the country’s south, where life is lived more in public than in private, when given half a chance.\nSpecifically, this imagery above describes the town of Marinaleda, which would be indistinguishable from its local counterparts in the Sierra Sur mountain range, were it not for a few tell-tale signs. Maybe it’s the street names (Ernesto Che Guevara, Solidarity and Salvador Allende Plaza, to name a few); maybe it’s the graffiti (hand-drawn hammers and sickles sit happily alongside circle As, oblivious to the differences the two ideologies have shared, even in the country’s recent past); maybe it’s the two-story-high portrait of Che emblazoned on the outer wall of the sports stadium.\nMarinaleda has been called Spain’s “communist utopia,” though the local variation bears little resemblance to the Soviet model. Classifications aside, this is a town whose social fabric has been woven from very different economic threads than the rest of the country since the fall of the Franco dictatorship in the mid 1970s. A cooperatively owned olive oil factory, houses built by and for the community, and a famous looting of a large-scale supermarket, led by the town’s charismatic mayor, in which proceeds were donated to food banks, are amongst the steps that have helped position Marinaleda as a beacon of hope.\nRather than rely exclusively on cash to get things done, Marinaleños have put their collective blood, sweat, and tears into creating a range of alternative systems in their corner of the world.\nAs the Spanish economy continues its post-2008 nosedive, unemployment sits at 26 percent nationally, according to the International Business Times, while over half of young people can’t find work. Meanwhile, Marinaleda boasts a modest but steady local employment picture, in which most people have at least some work and those that don’t have a strong safety net to fall back on.\nBut beyond its cash economy, Marinaleda has a currency rarely found beyond small-scale activist groups or indigenous communities fighting destructive development projects: the currency of direct action. Rather than rely exclusively on cash to get things done, Marinaleños have put their collective blood, sweat, and tears into creating a range of alternative systems in their corner of the world.\nWhen money hasn’t been readily available — the lack of it is probably the only consistent feature since the community set out on its current path — Marinaleños have turned to one another to do what needs doing. At times that has meant collectively occupying land owned by the Andalusian aristocracy and putting it to work for the town. At others it has simply meant sharing the burden of litter collection.\nWhile still operating with some degree of central authority, the local council has devolved power into the hands of those it serves. General assemblies are convened on a regular basis so that townspeople can be involved in decisions that affect their lives. The assemblies also create spaces where people can come together to organize what the community needs through collective action.\n“The best thing they have here in Marinaleda, and you can’t find this in other places, is the [general] assembly,” says long-term civil servant for the Marinaleda council, Manuel Gutierrez Daneri. “The assembly is a place for people to discuss problems and to find the solutions,” he continues, pointing out that even minor crimes are collectively addressed via the assembly, as the town has no police or judicial system since the last local cop retired.\nIn his time as mayor, Juan Manuel Sánchez Gordillo has managed to leverage considerable financial support from the state government, a feat which Gutierrez Daneri attributes to the town’s track record for direct action. “If you go ahead with all of the people behind you, that is very powerful,” he says.\nEven minor crimes are collectively addressed via the assembly, as the town has no police or judicial system since the last local cop retired.\nAs a result, the small town boasts extensive sports facilities and a beautifully maintained botanical garden, as well as a range of more basic necessities. “For a little village like this, with no more than 2,700 people, we have a lot of facilities,” says Gutierrez Daneri.\nBritish ex-pat Chris Burke has lived in Marinaleda for several years, and he explains that access to the public swimming pool only costs €3 for the entire summer. Burke recounts Mayor Sánchez Gordillo saying to him, “The whole idea of the place being somewhere good to live is that anyone can afford to enjoy themselves.” Burke says he believes that projects operated at a financial loss are essential to utopia.\nFrom occupation to cooperation\nIn 1979, Sánchez Gordillo was first elected as the town’s mayor. He led an extensive campaign to change Marinaleda’s course, which began with hunger strikes and occupying underutilized land.\nManuel Martin Fernandez has been involved in “la lucha” (the fight) since the beginning. He explains how through the general assembly process the community decided something had to be done to stem the flow of migration from the town. They began a weeks-long occupation of a nearby reservoir to convince the regional government to allocate them enough water to irrigate a tract of land.\nAfter this proved successful, they then went on to occupy 1,200 hectares of the newly irrigated land, which at that time was owned by an aristocratic family. In 1991, the plot of land was officially expropriated and turned over for local use. “It took 12 years to obtain the land,” Martin Fernandez explains, calling their victory “a conquest.”\nToday, extensive fields of olives, artichokes, beans, and peppers form the backbone of the local cash economy. The land is collectively managed by the cooperative El Humoso and a canning facility has been set up on the edge of town. “Our aim was not to create profits but jobs,” Sánchez Gordillo told British author Dan Hancox, explaining why the town chose to prioritize labour-intensive crops to create more employment for local people.\nLike most agricultural employment, whether in the fields or the factory, work in Marinaleda is both seasonal and varied from year to year. But unlike many small agricultural towns, Marinaleda shares the work among those who need it.\nDolores Valderrama Martin has lived in Marinaleda her entire life and she has worked at the Humoso canning factory for the past 14 years. From the upstairs office she explains that if 200 people are looking for work, but they only need 40 workers, they will bring everyone together.\n“We gather all of these people who are directly affected,” she says. “We make groups of 30 to 40 people and each group works for two days.”\nWhile the cooperative is formed of nine separate entities, Valderrama Martin says they collectively decide on important issues like the allocation of work. They may even take the issue to a general assembly for wider input from the town. But, she cautions, “When there is no work they are unemployed, like anywhere else.”\nMost of the town decries the relative lack of work, but the wider social security net built on the principles of direct action and mutual aid have meant that unlike other parts of the country, two months’ wages can go a long way to keeping you afloat for the year. At the core of this is the town’s approach to housing, which offers one of the clearest examples of how collective effort can fill the void left by a stagnant cash economy.\nThe houses that community built\nWhen many young people think about making their first foray into the housing market, money is inevitably the biggest obstacle. State of the economy aside, a down payment is always a sizeable sum, even in relatively tame markets, and is increasingly unattainable for what has been described as “the jilted generation”.\nWhile capitalism frames our relationships as a series of self-interested economic transactions, Marinaleda relies on a model of mutual aid.\nHousing has been partly removed from the free market in Marinaleda using a combination of state housing subsidies for building materials, free labour for construction, and land given by the town. Photo: Naturalezartificial AMA ETSAS 0809. Creative Commons BY-NC-SA.\nBut through a maverick decision spearheaded by Mayor Sánchez Gordillo, housing has been partly removed from the free market in Marinaleda using a combination of state housing subsidies for building materials, free labour for construction, and land given by the town. Community members come together with architectural plans provided by the council to build a block of houses, with no sense in advance of which home will belong to which family.\nThe houses — some 350 units in total, with 20 new builds underway at the time of our visit — become part of a housing cooperative. Needless to say, when citizens are only left paying €15 per month for mortgages, this has a massive impact on work requirements.\nThe direct action economy\nWhile capitalism frames our relationships as a series of self-interested economic transactions, Marinaleda relies on a model of mutual aid, as locals work together to meet shared needs, with far less money circulating. While it can be easy to forget, money is simply a way of facilitating action, which creates an incentive for people to do tasks that they otherwise may not have any interest in doing.\nDirect action, on the other hand, is rooted in common interests and explores the practicalities of what needs doing, based on who is there to do it. Direct action eliminates the consumer-provider divide, making cash an unnecessary intermediary in getting things done, as those who want something done, and those doing it become one-in-the-same.\nWhile Marinaleda has its flaws, it reminds us that alternative economic models are not only possible, they already exist. A striking piece of graffiti on Marinaleda’s main road depicts a dream-catcher, super-imposed with a hammer and sickle. The accompanying message implores us, ‘Catch your dreams — utopia is possible’.\nThis article first appeared on Contributoria.","What is Democratic Leadership?\nThe democratic leadership style is essentially a mode of leadership that is found in participative management and human resources theory. This article will seek to explore the essence of democratic leadership and will, among other things, outline the advantages of democratic leadership, the disadvantages of democratic leadership, the function of the democratic leadership council and provide democratic leadership. The definition of democratic leadership from an organizational standpoint involves the redistribution of authority and power between managers and employees to provide employee involvement in the process of decision making.\nThe Democratic Leadership Style in Action\nBefore going any further, it is important to point out that there are conceptual differences between authority and leadership. The former is formalized power that is conferred on a person to engage in particular activities sanctioned by an institution or individual who has the power to be authoritative. For example, in a case where an individual is elected to a public office, the citizens of the state effectively conferred on that individual the power of representation. In an organizational structure, certain powers are conferred on the CEO of the company by a board of directors to implement board policies. However, leadership can be considered as the power possessed by an individual, who influences the action and belief of others. Formal authority may or may not be possessed by a leader and an individual who has authority may or may not occupy a leadership position. In essence, leadership is a behavior and not an office or position held by an individual. Democratic principles are used to manage the democratic leadership approach. These principles include deliberation, inclusiveness, equal participation and self-determination. The democratic leadership style is characterized by the three features outlined below:\nDelegation of Responsibility\nA democratic leader will delegate responsibility among members of his or her team to facilitate member participation in making decisions.\nEmpowerment of Group Members\nIt is incumbent on leaders to empower their team members in order that the members will be properly equipped to accomplish their tasks. Included in the process of empowerment is the provision of the education and training required for the completion of delegated tasks.\nAiding the Process of Group Decision Making\nA significant role played by the democratic leader is ensuring that democratic deliberation takes place when group decisions are being made. This indicates that the leader is supposed to serve as a mediator and facilitator between members of the group, make sure that the members are psychologically well and there is a respectful environment at all times.\nSimilar to every other style of leadership, there are advantages and disadvantages of democratic leadership. Some of these benefits and drawbacks are outlined below:\nAdvantages of Democratic Leadership\nThe techniques used in democratic leadership play a significant role in creating job satisfaction, since a sense of autonomy, control and participation is fostered within the democratic leadership style. Greater participation from employees during the decision-making process could also result in more creative solutions and greater innovation to address problems and serve the organization better. Below are some specific advantages of democratic leadership:\nSolution for Complex Problems\nDemocratic leaders are typically excellent at solving complex issues. They have the ability to work collaboratively, using a consensus of opinions to get things done the right way. The democratic leader often thinks innovatively and encourages others to do the same, so that solutions to complex and strategic problems can be found.\nGood Business Fit\nSolutions that are democratically derived generally last for the longest period of time. The democratic process ensures that the solution is reviewed on a continuous basis. Additionally, engaging team members will enable leaders to maintain effective processes that fit the business well. Democratic leaders ensure that team members work well with other individuals, so that they are suitable to function in large corporate environments in which co-operation and communication are crucial.\nStrong Teams are Built by Democratic Leaders\nTeam members under democratic leadership tend to be supportive and strong. Honesty flourishes and more collective working is done because the opinions of everyone are taken into consideration. Democratic leaders are usually popular within the organization.\nFoster Creative Environments\nDemocratic leaders effectively foster creative environments since they encourage the input and innovation of team members. Creative designers succeed under democratic leadership because of the support and nurture that is embodied in this leadership approach.\nDisadvantages of Democratic Leadership\nCan Appear Uncertain\nDemocratic leadership can sometimes come across as being indecisive. In certain situations, especially during a crisis, leaders must be very directive and democratic leaders do not function well in an authoritarian role. In the midst of a crisis, no time is usually available to address everyone concerned.\nThe consultation process could result in procrastination. If an organization has a project or issue that is urgent, democratic leaders cannot usually work to the timescales required. It takes time to talk to a number of individuals and collect a variety of opinions and many democratic leaders find it difficult to cut corners.\nDemocrat leaders sometimes build an environment in which individuals expect for their idea to be implemented. Where there are a variety of solutions in a particular project, only one can be implemented and democratic leaders will have to invest time in apologizing and smoothing things over with individuals whose ideas were not used.\nDemocratic Leadership Examples\nDemocratic leadership style examples include Dwight D, Eisenhower, who was a military leader faced with the challenge of getting the Alliance forces to be in agreement on a common line of attack. He put in hard labor to ensure that there was a united front, so that a common understanding could be reached. This is considered one of his greatest accomplishments as a leader.\nIn the majority of corporate cultures, the democratic leadership style is generally used. There are good results garnered from democratic leaders, which is evident in the level of employee satisfaction that takes place in such an environment. However, the democratic leadership approach will not always get the job done. A democratic leader will have to learn how to put his or her foot down and come to a decision in certain situations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:92271146-0f23-4893-95c4-ae81f4dbcde5>","<urn:uuid:6ef3e9ee-cdaf-42aa-a982-95f09af572b7>"],"error":null}
{"question":"In what ways do traditional Cherokee and Chinese wedding ceremonies incorporate food and symbolism?","answer":"Both Cherokee and Chinese wedding ceremonies incorporate significant food elements and symbolism. In Cherokee weddings, the couple traditionally exchanges food - the groom gives a ham of venison to symbolize his intention to provide meat for the household, while the bride gives corn to symbolize her willingness to be a good housewife. The couple also drinks together from a special Cherokee Wedding Vase with two openings. In Chinese weddings, food plays a central role during the reception, which typically features a twelve-course meal including elaborate dishes like lobster, shark fin soup, and traditional roasted pig. They also incorporate games involving food, such as 'cherry bobbing' where couples compete to eat suspended cherries. Both cultures use specific colors symbolically - Cherokee ceremonies use blue blankets that are later replaced with a white one to symbolize new life together, while Chinese traditions emphasize red decorations, table covers, and bridal attire as symbols of joy, prosperity, and love.","context":["Cherokee Wedding Ceremony\nThe Cherokee wedding ceremony is a very beautiful event,\nwhether it is the traditional or ancient ceremony, or a modern variation.\nThe original ceremony differed from clan to clan and community to\ncommunity, but basically used the same ritual elements.\nclanship is matrilineal in Cherokee society, it is forbidden to marry\nwithin one's own clan. Because the woman holds the family clan, she is\nrepresented at the ceremony by both her mother (or clan mother) and her\noldest brother. The brother stands with her as he gives his vow to take\nthe responsibility of teaching the children in spiritual and religious\nmatters, as that is the traditional role of the 'uncle' (e-du-tsi).\nIn ancient times, the matrimonial couple would meet at\nthe center of the townhouse, and the groom gave the bride a ham of\nvenison while she gave an ear of corn to him, then the wedding party\ndanced and feasted for hours on end. In those early days, venison\nsymbolized his intention to keep meat in the household and her corn\nsymbolized her willingness to be a good Cherokee housewife. The groom is\naccompanied by his mother.\nAfter the sacred setting for the ceremony has been\nblessed for seven consecutive days, it is time for the ceremony. The\nbride and groom approach the sacred fire, and are blessed by the priest\nand/or priestess. All participants in the wedding, including the guests,\nare also blessed. Songs are sung in Cherokee, and those conducting the\nceremony bless the couple. Both the bride and groom are covered in a\nblue blanket. At a specified moment in the ceremony, the priest or\npriestess removes each blue blanket, and covers the couple together with\none white blanket, indicating the beginning of their new life together.\nof exchanging rings, in the old times the couple exchanged food. The\ngroom brought ham of venison, or some other meat, to indicate his\nintention to provide for the household. The bride provided corn, or bean\nbread to symbolize her willingness to care for and provide nourishment\nfor her household.\nThis is interesting when noting that when a baby is\nborn, the traditional question is, “Is it a bow or a sifter?” Even at\nbirth, the male is associated with hunting and providing, and the female\nwith nourishing and giving life. The gifts of meat and corn also honor\nthe fact that, traditionally, Cherokee men hunted for the household,\nwhile women tended the farms. It also reflects the roles of Kanati\n(first man) and Selu (first woman).\nThe couple drank together from a Cherokee Wedding Vase.\nThe vessel held one drink, but had two openings for the couple to drink\nfrom at the same time. Following the ceremony, the town, community or\nclans provided a wedding feast, and the dancing and celebrating often\ncontinued all through the night and into the next morning.\nToday, some Cherokee traditionalists still observe\nportions of these wedding rituals. The vows of today's ceremony reflect\nthe Cherokee culture and belief system, but are in other ways similar to\nwedding ceremonies of other cultures and denominations. Today's dress\ncan be in a tear dress and ribbon shirt, a wedding gown, or normal\nattire worn at a Ceremonial Ground.\nBecause the Cherokee Nation is a sovereign governing\nbody, the United Red River\nCherokee Nation has its own marriage law, and Cherokee couples are\nallowed to marry under this law instead of the State marriage laws.\nCherokee couple is not required to obtain a marriage license;\nhowever, for legal purposes, it is highly recommended that the couple\nobtain a marriage\ncertificate through First Nation Church. (Couples that are not\nmembers of the tribe may also\nobtain a marriage\ncertificate through First Nation Church for a slightly higher fee.)\nIn addition, the person conducting the ceremony\nmust be licensed by First Nation Church in\norder to legally do so, whether or not he or she is a member of the\nAfter the couple or their religious leader (minister or\nelder) contacts the First Nation Church, the church administrator will prepare a\nmarriage certificate. This document shows that the couple were indeed\nmarried in a ceremony by a religious or spiritual leader licensed to do\nso. The certificate is forwarded to the\nUnited Red River Cherokee\nNation after all parties have signed the document, after which it is\nfiled in the official records.","The purpose of a Chinese wedding is to join and two families together. A traditional Chinese marriage is best described as a transfer of personnel (the bride) from one group to another. This transition was not arranged by the bride and groom but rather by their respective families, who take on the “match marker” role.\nTRADITIONAL CHINESE WEDDIGN PROPOSAL\nUnlike proposals that happen in many other cultures where the man proposes to the bride, and she then accepts or respectfully declines, a different route is taking when the engagement of two individuals in the Chinese culture are about to get wed. The proposal begins with a “matchmaker” or go between, who tries to find a man the perfect wife. The go between would consult and negotiate with the parents of the likely bride and groom, not the potential bride and groom themselves.\nWhen the go-between finds a likely spouse, the potential groom’s parents if they agree will send the bride’s family gifts for the bride with the go between, and the go between would state to the bride’s parents how the groom’s parents feel. If the proposal goes well and the parents accept the brides gifts, the go-between would obtain the date and hour of the girl’s birth recorded ona formal document. The handing over of the girl’s birth information is seen as consent by the parents.\nIf the bride’s parents hand over their daughter’s birth information, The groom’s family would also place his document on the ancestral altar for three days. If no altercations between the parents or a loss of property took place within that time, the parents would give the information to an astrological expert to confirm that the young woman and their son would make a good match. If the boy’s family found the horoscope to be favorable, the grooms parents would then give the go between to give to the bride’s parents and the bride’s parents would do the same thing.\nFinally the families get to meet face-to-face for the first time, the families start off by evaluating each other in terms of appearance, education, character, and social position. If both were satisfied they would proceed to the betrothal, the final step before the wedding.\nThe betrothal is the bargaining the parents conduct so that they can arrive as the amount of money that they feel will be a sufficient amount to gift the entire bride’s family. The bride’s parents must then pick from a list of potential wedding dates outlined by the groom’s family.\nCHINESE TRADITIOJNS IN MODERN DAY SOCIETY\nToday, having go betweens arrange marriages are very rare in western societies, however may still be practiced in China. Today, couples are more free to pick whom they want to marry and spend the rest of their life with, without having the parents make the ultimate decision for them. In today’s society, instead of he conducting the traditional betrothal between the parents as previously mentioned above, bridesmaids may put on their own love of test for the groom. The groom can either gift each bridesmaid something special wrapped in red, (a very important color in Chinese tradition) or by performing specific things dictated to the groom by the bridesmaids for instance, doing headstands or cartwheels. This test goes to show that the groom will do anything for the love of his life.\nHaving a tea ceremony is another tradition found in the traditional Chinese wedding. The tea ceremony is done to show respect from the bride and groom to their parents and elderly respect, and may take place prior or immediately following the ceremony. The bride and groom may kneel before their sets of parents as a gesture of respect while asking for permission to get married. At this time, the bride and groom are provided with gifts such as gold necklaces and bracelets, or red envelopes with money enclosed.\nTHE CHINESE WEDDING BANQUET\nChinese weddings receptions are usually very grand and detailed. The reception may consist of a twelve course sit down meal of Chinese inspired creations such as lobster, shark fin soup and the traditional roasted pig. Chinese weddings are also very game oriented. One example may be the “cherry bobbing,” where the couple tries to eat a suspended cherry before the other one.\nIt is also very traditional for a Chinese bride to change outfits up to four times during her ceremony. Two of the primary dresses however consist of the one worn at the ceremony and the one worn at the reception. The tradition dress worn at the ceremony is usually the color red, for red in the Chinese culture is a symbolic one symbolizing joy, prosperity and love. At the reception, the bride can stick to a traditional white dress.\nA lot of couples try to incorporate more of the Chinese tradition by decorating the hall by having things like red table covers, Chinese lanterns or even Chinese symbols on napkins."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3cba94da-94e2-4a8e-9a45-c0cd58811117>","<urn:uuid:91fb7ac4-c410-4da6-a0f5-ec184fe99c0d>"],"error":null}
{"question":"How does the binding mechanism of nanoparticles affect polymer brush collapse, and what role does this process play in nuclear protein transport?","answer":"The binding mechanism involves nanoparticles (like karyopherins/kaps) binding to polymer chains (like nucleoporins/nups), which causes the polymer brush to collapse when the binding strength is sufficiently high. As the concentration of nanoparticles increases, the brush height suddenly decreases due to these binding interactions. This process is crucial for nuclear protein transport, as importins (a type of karyopherin) bind to proteins with nuclear localization signals and transport them through the nuclear pore complex, which is lined with nucleoporin proteins. The importins use special binding sites to interact with phenylalanine-rich sequences on the nuclear pore proteins, enabling them to move cargo through the pore.","context":["Presentation on theme: \"Michael Opferman (Univ. of Pittsburgh) Rob Coalson (Dept. of Chemistry, Univ. of Pittsburgh) David Jasnow (Dept. of Physics & Astronomy, Univ. of Pgh.)\"— Presentation transcript:\nMichael Opferman (Univ. of Pittsburgh) Rob Coalson (Dept. of Chemistry, Univ. of Pittsburgh) David Jasnow (Dept. of Physics & Astronomy, Univ. of Pgh.) Anton Zilman (Los Alamos National Lab, University of Toronto)\nNPC is a structure in the nuclear envelope which allows transport of material in and out of the nucleus (e.g. mRNA) Walls of the NPC are lined with natively unfolded proteins called nucleoporins (“nups”) Nups bind to transport receptors, typically karyopherins (“kaps”) What role does binding play in transport? Picture from: B. Fahrenkrog and U. Aebi, Nat. Rev. Mol. Cell Biol. 4, 757 (2003). Adapted from Cryo-electron tomography\no Nup (polymer) filaments grafted onto a nanodot collapse in the presence of (nanoparticle) receptors... From: Lim et al., Science 318, 640 (2007)\n1. Use a simple statistical mechanical model (lattice gas mean field theory = MFT ** ) to understand the Lim experiment Count states Minimize free energy 2. Use coarse-grained multi-particle Langevin Dynamics simulations to verify the theory and add more detail ** a la S. Alexander [J. de Phys., 1977. 38 : p. 977-981] and P. de Gennes [Macromol., 1980. 13 : p. 1069-1075] = “AdG”\nBrush Solution h Blue = Nup (Monomer) Red = Kap (Nanoparticle) First, consider a gas of nanoparticles (“solution”) in contact with a gas of monomers mixed with nanoparticles (“brush”). Note : v=(nanoparticle volume)/(monomer volume) = 1 here\nBlue = Nup (Monomer) Red = Kap (Nanoparticle) How many ways are there to arrange N S nanoparticles on M S lattice sites? Use binomial coefficient:\nBlue = Nup (Monomer) Red = Kap (Nanoparticle) How many ways are there to arrange N B nanoparticles and N monomers on M B lattice sites? Use multinomial coefficient:\nBlue = Nup (Monomer) Red = Kap (Nanoparticle) But these are monomers of a polymer chain, not a gas. They should have stretching entropy, not translational entropy! So replace the unphysical term. h\nBlue = Nup (Monomer) Red = Kap (Nanoparticle) Finally, make nanoparticles “bind” to polymers by adding an “enthalpic” term to the free energy. Number of binding interactions will be (invoking “random mixing”): (Number of nanoparticles) x (Average number of monomers neighboring each nanoparticle) So free energy from binding interactions will be And the Total Free Energy will be:\nThe solution and brush can exchange nanoparticles and volume. This means that the chemical potential of nanoparticles, and the osmotic pressure must be equal in the two regions at equilibrium. Equivalently, we can minimize a “Grand Potential” Note: Here [ = bulk nanoparticle concentration ] Minimizing this function over: (1) the number of nanoparticles in the brush and (2) the volume of the brush for fixed concentration in the solution determines the equilibrium state of the solution/brush system.\nHere’s what it looks like for a given, sufficiently large binding strength ( χ large and negative) as you sweep through the solution concentration (C 0 ) Double Minimum structure – Phase Transition! Brush height suddenly collapses due to a small increase in C 0\nSmall binding strength: No phase transition. Large binding strength: Discontinuity!\nLangevin Dynamics Overdamped regime, Implicit solvent, Coarse-grained Lennard-Jones Repulsion between all particles Lennard-Jones Attraction to represent binding FENE springs to connect polymer strands Polymers grafted in a square array to the “floor” Periodic boundary conditions on “walls”\nWhite = Polymer Beads (Nups) Red = Transport Receptors (Kaps) Top: Reservoir of Red particles Bottom: Hard wall to which polymers are grafted Sides: Periodic boundary conditions Solution Brush Grafting Sites h C 0 = (# of red) (volume)\nVertical Drop: “Phase Transition!” M. Opferman, R.D. Coalson, D. Jasnow and A. Zilman, http://arxiv.org/abs/1110.6419http://arxiv.org/abs/1110.6419, 2011 and Phys. Rev. E 86, 031806 (2012)\nIncreasing C 0 Homogeneous Extended Homogeneous Collapsed Collapsing\nIncreasing C 0 Homogeneous Extended Homogeneous Collapsed Inhomogeneous\nBrush N B red N blue M sites M/v supersites Solution M S /v supersites h Blue = Nup (Monomer) Red = Kap (Nanoparticle) When nanoparticles are larger than monomers, place the larger particles first so that the number of available “super-lattice” sites is easily calculated.\nv>1 shares many qualitative similarities with the v=1 case, including the decrease in brush height when more nanoparticles are bound and the phase transition between an extended and collapsed state when the binding strength is sufficiently high. v=10v=1\nComparison of MFT vs. BD simulations for v=10. Note: BD simulations for v=10 were performed with spherical nanoparticles having spherically homogeneous attraction nup (polymer) monomers.\nMilner-Witten-Cates (MWC) / Zhulina Mean Field Theory of a Plane-Grafted Polymer Brush: Here: z =distance from grafting plane = monomer (polymer bead) density (volume fraction) = function derived from the brush free energy function above ( sans polymer chain stretching energy term) A,B = positive constants dependent on polymer chain length and grafting density A better level of theory is provided by …\nA A - Bz 2 Illustration of MWC theory inversion procedure : at every distance z from the grafting surface, there is a unique value of monomer density Ψ :\nLangevin simulation data vs. MWC theory for v=1,20,100. ** Overall, the agreement between Langevin simulations and MWC is quite reasonable (good?) over the entire range v=1-100. (Quantitative agreement degrades as v increases, but all qualitative features are faithfully reproduced.) A few conclusions: 1)No true “phase transition” (discontinuity in h vs. c ) even for v=1. 2)The collapse transition is sharper for smaller v. V=1 V=20 V=100 ** MGO, RDC, DJ and AZ, Langmuir, in press.\nSpatial distribution of monomers, ψ (z), and nanoparticles, Φ (z), for v=1, a=4: Comparison of Langevin simulations to MWC theory. Increasing nanoparticle concentration, c → Simulations MWC theory Red = Φ Blue = ψ extended statecollapse regimecollapsed state\nNew results from Lim et al. ** on a nup-based brush grafted to a flat surface with attractive kap proteins in solution: ** Schoch, R.L., L.E. Kapinos, and R.Y. Lim, PNAS 2012. 109 : p. 16911–16916. Δ d = change in brush height from its value when there are no nanoparticles (here, “kaps”) in solution, and ρ kap β 1 is the number of nanoparticles inside the brush per unit surface area. [N.B.: ρ kap β 1 increases monotonically with bulk nanoparticle concentration, which is indicated in parentheses in the figure.]\nPotential Nanotechnology Application: Tunable nano-valves (for separations applications): Our variation on this theme: Control via nanoparticle concentration Control via solution pH: Iwata, H., I. Hirata, and Y. Ikada, Macromol., 1998. 31: p. 3671-3678. Control via temperature: Yameen, B., M. Ali, R. Neumann, W. Ensinger, W. Knoll, and O. Azzaroni, Small, 2009. 5 : p. 1287-1291.\nWe developed a simple theory capable of explaining the collapse of a polymer brush when exposed to binding particles Depending on the binding strength, collapse may be quite sharp over a small nanoparticle concentration range. Next steps: I) Add more realism. E.g.: discrete binding sites on the (large) nanoparticles, cylindrical geometry, range of polymer grafting densities and nanoparticle sizes. II) Applications to both biology (NPCs) and materials science (controlling the morphology of a polymer brush) are envisaged. $$: NSF","Importins deliver proteins into the nucleus through the nuclear pore complex\nInside your cells, the process of protein synthesis is separated into two compartments. The first half of the job, when DNA is transcribed into RNA, is performed in the nucleus. The second half is then performed outside the nucleus, when ribosomes translate the RNA to construct proteins in the cytoplasm. This separation requires a continuous traffic of molecules: new RNA molecules must be transported out of the nucleus and nuclear proteins, such as newly-synthesized histones or polymerases, must be transported back into the nucleus. Huge tube-shaped nuclear pores act as the highway connecting the nucleus and the cytoplasm, and importins and exportins (collectively known as karyopherins) ferry molecules back and forth through the pore.\nImportins transport thousands of different proteins into the nucleus to perform the many tasks of storing, reading, and repairing the genome. However, it would be far too costly to design a special importin to carry each one inside. Instead, many nuclear proteins are built with a special tag—a short sequence called the nuclear localization signal—that tells the transport machinery to carry the protein into the nucleus. Importins recognize this signal, bind to the protein, and transport it through the nuclear pore.\nThe Import Business\nThe importin complex shown here (built from three separate PDB entries) transports proteins with nuclear localization signals. At the top in blue is importin-beta (PDB entry 1qgk\n). It is the engine of the complex, recognizing nuclear pores and moving through them. It wraps around the end of importin-alpha, shown at the center in green (PDB entry 1ee5\n). Importin-alpha is an adapter molecule that connects importin-beta with the cargo. In this picture, the cargo is nucleoplasmin, shown at the bottom in yellow (PDB entry 1k5j\n), a chaperone protein that is important in nucleosome assembly. Notice how the extended nuclear localization signal of this cargo is gripped by importin-alpha.\nOnce the importin-beta/importin-alpha/cargo complex gets inside the nucleus, the cargo must be released and the importins must be recycled back to the cytoplasm. The Ran protein, shown here in light red, is responsible for releasing the cargo. It binds to importin-beta and causes a significant change in shape, leading to the release of importin-alpha and the cargo. Then, the complex of importin-beta and Ran (shown on the left, PDB entry 2bku)\ntravels back through the pore. Outside, a GTP molecule in Ran (shown in bright red) is cleaved and the Ran dissociates, leaving importin-beta ready to carry the next cargo protein inside. Importin-alpha can't get back to the cytoplasm by itself, so it gets some help from CAS (a nuclear export factor), shown here on the right in purple (PDB entry 1wa5\n). CAS is similar to importin-beta, but moves through the nuclear pore in the opposite direction. It binds to importin-alpha and Ran and carries them out of the nucleus. Then, a similar cleavage of the GTP in Ran releases importin-alpha for another round of transport.\nExploring the Structure\nThe actual mechanisms that importins use to pull molecules through the nuclear pore are still a subject of active debate, but PDB entry 2bpt\ngives some hints of how it might be done. The thousands of proteins that make up the nuclear pore are covered with special amino acid sequences that are flexible and that contain many phenylalanines. One side of importin-beta binds to these special sequences. The structure shown here includes the full importin-beta (in rainbow-colored cylinders) and a few short pieces from the nuclear pore proteins (shown in spheres at the bottom, with the phenylalanine amino acids in red). Notice that the phenylalanines bind in pockets on the outer surface of the importin. Importin-beta may jump from site to site through the nuclear pore, guided by these special sequences.\nAs you take a look at these structures yourself, take a moment to explore the unusual fold of the chain. Importin-beta is folded like a spring, which then wraps into a big spiral that traps its binding partners inside.\n- M. Stewart (2006) Structural basis of the nuclear protein import cycle. Biochemical Society Transactions 34, 701-704.\n- E. Conti, C. W. Muller and M. Stewart (2006) Karyopherin flexibility in nucleocytoplasmic transport. Current Opinion in Structural Biology 16, 237-244.\n- D. S. Goldfarb, A. H. Corbett, D. A. Mason, M. T. Harreman and S. A. Adam (2004) Importin alpha: a multipurpose nuclear-transport receptor. Trends in Cell Biology 14, 505-514.\nJanuary 2007, David Goodsell"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ea2509ee-da0f-4f9f-bc56-f5efd147189b>","<urn:uuid:e03bb7ee-e669-4fa6-8149-1331156cb3fd>"],"error":null}
{"question":"What are the specific steps involved in creating and verifying digital signatures in blockchain transactions?","answer":"Digital signatures in blockchain work through a public/private key system. First, users generate a public/private key pair - the public key is known to everyone while the private key is kept secret. To sign a transaction, users input their message and secret key into a 'sign function' which produces a unique digital signature. To verify the signature, others use a 'verify function' that takes the message, signature, and sender's public key to confirm validity. The signature changes completely if the message is altered, and the private key cannot be deduced from the public key. Each transaction also needs a unique identifier to prevent duplicate transactions.","context":["Banking without Banks: Blockchain Simply Explained\nBitcoin and Blockchain\nBitcoin is a digital decentralized currency that has transformed finance. Because it’s decentralized there’s no government that issues bitcoins, and there’re no banks that manage accounts.\nBitcoin is often used for investing, but it’s life changing for migrant workers.\nThere are over 250 million international migrants who transfer money to their family back in their home country. This is known as remittance. The problem with remittance is that fees can be as high as 10%, and it takes weeks for the money to reach its destination. Bitcoin allows migrant workers to send money to their families with almost no fees, with transfers taking less than an hour.\nRemittance is just one possible application of a Blockchain (the tech underlying Bitcoin). To understand what else is possible with Blockchain this article will explain how it works by showing you how you’d design your own decentralized currency.\nLedgers: Keeping Track of Who Owes Who\nWhy is making a decentralized currency so hard?\nLet’s take a look at a basic example of currency without banks or governments, and some of the problems with it.\nIf you and your friends exchanged money frequently, like covering the dinner bill for each other, it would be inconvenient to exchange cash all the time. So you might keep track of who owes who on a ledger (a list of financial transactions).\nThe ledger could be on a website that anybody can see and add to.\nIf Bezos paid $14 for Elon’s free-trade coffee, then either of them could add “Elon pays Bezos $14” to the ledger.\nAt the end of the month, you’d all pay cash to the people you owe.\nOur ledger rules so far:\n- Anybody can add a transaction\n- Anybody can view the ledger\n- Settle up with cash at the end of the month\nDigital Signatures: Controlling Your Own Money\nThere’s a big problem with our ledger protocol.\n“Anybody can add a line” means that Elon could add “Bezos pays Elon $420” even if Bezos never agreed to.\nTo fix this we can say that whoever is sending money has to add their digital signature to the transaction.\nAt first glance it might seem like digital signatures wouldn’t work. Couldn’t people just copy your signature?\nThis is where the first bit of cryptography comes in. Everybody would generate a public / private key pair. Like the name suggests, everybody knows your public key, but only you know your private key.\nHandwritten signatures look the same no matter what document you’re signing. Digital signatures are a lot stronger. They’re a long string of 0s and 1s, and changing the message slightly will completely change the signature.\nWithout diving into the math, signing and verifying transactions is pretty simple. All you need to know is what functions are.\nA function is an operation that takes in an input (a sad cat) does something to it (pets it) and then has an output (a happy cat).\nTo sign a message you give a “sign function” your message and your secret key, and it’ll output a digital signature. Since the signature depends on your secret key, only you can create that signature. And because the signature also depends on the message, nobody can copy your signature and forge it on another message.\nTo verify a message you give a “verify function” a message, the signature on the message, and the public key of the person who signed it. This function will then indicate whether or not the signature is valid.\nAn important point to note is that although the public key is used to verify if a signature came from its associated private key, there’s no way to deduce the private key from its public key.\nOur new ledger protocol:\n- Anybody can add a line with a transaction\n- Anybody can view the ledger\n- Settle up with cash at the end of the month\n- Only signed transactions are valid\nTimestamps: Preventing Duplicate Transactions\nThere’s a small problem with how our ledger and signature system currently works. Since digital signatures only depend on the message and secret key, signature message pairs can be duplicated.\nIf Bill put on the ledger “Bill owes Elon $420” and signed it, Elon could copy the entire transaction, the message AND the signature, to get more money.\nThe fix to this is pretty simple. Each transaction has an identifier that increases by one each transaction. The 42nd transaction would then have the identifier “42” attached to the beginning of the message.\nNow, if somebody tries to copy a transaction to get more money, the signature will be invalid, because you can’t repeat the identifier “42”, and changing it to “43” will completely change the signature.\nCryptocurrencies: No Overspending!\n“Settle up with cash at the end of the month”\nWhat about if somebody doesn’t settle up at the end of the month, and disappears forever?\nTo prevent somebody from taking advantage of this, we can make a rule that people can buy “Ledger Coin” (our own cryptocurrency) with cash, that all transactions are made in Ledger Coin, and that overspending isn’t allowed.\nNow, “settling up” is done automatically, since all transactions are made in Ledger Coin.\nAt any point in time, you can turn your Ledger Coin into cash, but you can only make transactions with Ledger Coin that you have.\nWait! How can ledger coin have any actual value?\nLedger coin and any other currency has value because people think they do. If people are willing to give cash or services in exchange for your ledger coin then it’s valuable.\nRight now the ledger is on a website. If the website goes down or gets hacked, nobody will be able to trade currency. If the website owner changes the rules, then everybody could suffer.\nCentralized systems only need one point of failure to be compromised. Decentralizing the ledger solves this problem, by giving everyone a copy of the ledger. Everybody has a copy of the ledger, and is in charge of keeping their own copy up to date.\nNow, when Elon wants to pay Bezos $10 for his window, he’ll broadcast the message “Elon pays Bezos $10” to everyone.\nThere’s a really big problem with this approach. When you see the message “Elon pays You $5” how can you be sure that everybody else got this message, and that you’ll be able to use these Ledger Coins with others? Maybe Elon only sent that message to you, and nobody else.\nWe need a protocol that tells how to accept or reject transactions and in what order, to ensure that your copy of the ledger will be the same as everybody else’s in the world.\nThe solution? Trust the ledger with the most computational work put into it.\nTo do this, all we need to use is something called a cryptographic hash function and some clever tricks.\nA cryptographic hash function takes in a message, does some crazy math to it, and outputs a hash (a string of 0s and 1s) with a fixed length.\nThe most commonly used hash function, SHA256, will turn any message into a string of bits 256 characters long. The output is called a “hash” and it looks random, but it’s not. Every time you give the message “Blockchain” to SHA256 it will output the same hash.\nChanging the message going into SHA256 will change the hash in a completely unpredictable way. Importantly, when given a hash there’s no way to figure out what message generated it. If you wanted to create a hash that started with five 0s, you’d have to randomly guess and check messages.\nProof of Work\nThe reason why SHA256 is useful is because it can be used to prove that a large amount of computational work went into a specific list of transactions.\nImagine someone shows you a list of transactions and says “I found a special number, that when you put it at the end of this transaction, and apply SHA256 to the whole thing, the hash starts with 30 zeros.”\nFor a random message, the odds that the hash will start with 30 zeros is 1/2³⁰, which is about one in a billion.\nBecause the hash of a message after applying SHA256 is essentially random, the only way to find a special number that gives a hash with 30 zeros is by guessing and checking. That means the person had to go through about a billion different numbers to find a special one.\nTo verify that the number is special, you just have to run SHA256 on the list of transactions with the special number at the end, and check that the hash starts with 30 zeroes.\nSHA256 allows us to verify that somebody put in a lot of computational work for a specific message, without having to the same work again.\nThis is called “Proof of Work”.\nBlock + Chain\nLet’s go back to our decentralized ledger.\nThere might be a lot of potential ledgers being broadcasted. Some might be from honest people, and others fraudulent.\nAs a rule, we’re only going to trust the ledger with the most computational work put into it. To do this we’ll organize a ledger into blocks. Each block has a list of transactions and a proof of work that makes the hash of the entire block start with a large amount of zeroes.\nJust like how a transaction needs a signature to be valid, a block needs a proof of work to be valid.\nTo form a chain of blocks, the header of each block will contain the hash of the previous block. Now if a criminal tried to change any transaction in a past block, its hash would change, which will make its proof of work invalid and will also change the header of the next block and its hash. This would make all the following blocks have invalid proof of works.\nProof of work makes it computationally infeasible to modify past blocks, making them effectively unalterable. This is known as immutability and is why blockchains are secure for transacting money and are censorship-resistant.\nSince each of the blocks are linked tightly to each other, we call this type of ledger a blockchain.\nIn practice, there’s only a subset of users who compute proof of works. These people are known as miners.\nMiners listen for transactions, bundle them into a block, and then start computing a proof of work for their block. If they find a valid proof of work before anyone else they’ll broadcast their block to the entire world. If somebody else creates a valid block before they do, they’ll update their ledger and get started on the next block in the chain.\nComputing proof of works for blocks takes a lot of computing power and energy. To incentivise miners, there are block rewards. The creator of any block is rewarded with some Ledger Coin for their work.\nThis is the only way that new ledger coins are introduced into the blockchain network. There’s no central government that mints new coins. New coins come into being from miners who create blocks.\nWe now have a full outline for how a decentralized currency system works!\n- Anyone can submit a transaction to be put in a block\n- Transactions are signed so users can only spend their own money\n- Users can only spend money that they actually have\n- Proof of work shows that computational work has been put into a block\n- Blocks are chained together so that past transactions cannot be modified\n- Miners are users who bundle transactions into blocks and find valid proof of works. They’re rewarded with cryptocurrency when they “mine” a block\n- Honest users will always trust the longest chain of blocks\nIf you’d like to play around with Blockchain concepts like hashing or mining, this website has great interactive demos.\nBlockchain and Beyond\nBlockchains aren’t only limited to Bitcoin. Ethereum is another technology that uses blockchain to allow users to do almost any computation in a decentralized and immutable manner. It allows for decentralized computing, file storage, investing and more."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:62d1f13f-c79e-40dc-b434-4f250fd0b100>"],"error":null}
{"question":"What are the relative impacts of deforestation on wildlife populations versus food production systems globally?","answer":"Deforestation has led to a dramatic 68% decrease in population sizes of mammals, birds, amphibians, reptiles, and fish in less than 50 years, while simultaneously requiring massive resource allocation for food production - with one third of all land mass and three quarters of all freshwater now dedicated to producing food. The deforestation is creating a vicious cycle where forest loss is damaging soil quality and changing weather patterns are reducing crop yields, while also increasing human-animal contact and raising pandemic risks. This shows both wildlife populations and agricultural systems are being negatively impacted by ongoing deforestation.","context":["Clearer public-private pathways are putting us on the right track\nThe end of 2020 marks a crossroads that is both deeply worrying and quite exciting. Deforestation and conversion of natural ecosystems continue unabated, with 3.8 million hectares of tropical primary forest lost in 2019—a 2.8% increase from the previous year. Wildfires rage from the Amazon to the Arctic, and the recent Living Planet Report released by World Wildlife Fund (WWF) shows an average 68% decrease in population sizes of mammals, birds, amphibians, reptiles, and fish in less than 50 years. Deforestation is enabling more human-animal contact and raising the chances of new pandemics spilling over to humans; it also continues to be a primary driver of climate change, creating a vicious cycle.\nConcurrently, conservation and restoration of forests, and nature more broadly, have been elevated on the global agenda. Forests are included in the Paris Agreement and in the land-based carbon targets of many countries’ Nationally Determined Contributions. And, building from the experiences of REDD+, governments at the national and subnational level in many key commodity-producing regions are translating these ambitions into action by providing leadership in place-based, multistakeholder efforts to address deforestation and conversion. Increasingly, governments and the business community are engaging with each other. Ghana, for instance, has created action plans with cocoa buyers to address deforestation. In Indonesia, the National Action Plan for Sustainable Palm Oil is providing a structure around which palm oil companies can coordinate their forest protection and restoration efforts in line with government strategies.\nMeanwhile, the business case for far more expansive action to protect nature has become obvious. Nature loss is no longer just an issue of reputational risk—it threatens the future of commodity supply and the jobs linked with raw material production. Forest loss is damaging soil quality while changing weather patterns are reducing yields of planted crops. The World Economic Forum’s Global Risks Report 2020 ranked biodiversity loss and ecosystem collapse among the top five threats the global economy will face in the next 10 years.\nThe private sector has taken notice and begun to evolve its sustainability ambitions to match the scale of the challenge confronting nature. Over the last decade, much of the business community committed to eliminate deforestation from its commodity sourcing, and the Accountability Framework initiative (AFi) and Collaboration for Forests and Agriculture (CFA) have created best-practice guidance to support implementation. More recently, leading companies have begun to align with government and other actors in producing regions on strategies that go beyond individual supply chains to address underlying drivers of nature loss. Several important platforms like the Consumer Goods Forum and Tropical Forest Alliance (TFA) are helping to mainstream expectations that the public and private sectors should collaborate through landscape and jurisdictional initiatives.\nThis trend is encouraging, but the overall number of companies engaging in these scaled efforts remains low. Major barriers have included uncertainty around the business case for multistakeholder collaboration and a dearth of clear examples to follow. There is also misalignment between the sort of actions civil society is asking companies to undertake and the sustainability practices for which companies are currently recognized and rewarded.\nBut barriers to engagement are coming down. Over the past year, a group of organizations has developed a suite of new tools and guidance to enable broader company engagement in production geographies where they are invested or exposed. Whereas AFi and CFA provide the key guidance for implementing deforestation/conversion-free commitments within supply chains, these new tools elaborate complementary guidance for addressing systemic drivers. They pull from concrete examples to help companies understand and navigate through their options to engage. And thoughtful collaborations have positioned these tools to be reasonably aligned, thus avoiding the pitfalls of conflicting guidance from civil society.\n- A new paper from the United Nations Development Program, Value Beyond Value Chains, clarifies why and broadly how companies can engage in landscape and jurisdictional initiatives. It explains the business case for collaborating beyond value chains at landscape, subnational, and national levels to help create the enabling conditions for sustainable production and provides broad schemas to help companies think about how to engage in multistakeholder initiatives in producer countries.\n- A complementary paper from Proforest, Engaging with Landscape Initiatives, fleshes out the how and adds guidance on where to engage. It walks through steps companies should take when thinking about how to engage in landscapes, describing elements of the engagement process like building trust, planning and implementing interventions, and monitoring of progress. And it helps companies understand their supply base, how to prioritize landscapes for engagement, and decide which initiatives they might work with.\n- A resource that Walmart recently launched provides more granularity on the question of where to engage, providing maps that show the jurisdictions where companies are likeliest to source key deforestation-risk commodities and the deforestation risk of these jurisdictions.\n- Building on these tools, TFA released a set of corporate guidance and a dynamic web-based tool developed by WWF and Proforest that goes the next step in describing what specific actions companies can implement to advance landscape and jurisdictional initiatives. It provides concrete interventions companies can take, offers real-world examples where companies are already doing so, and proposes guidance on how to execute.\n- Each of the previous tools informs corporate action. New guidance from ISEAL Alliance on Verification of Jurisdictional Claims lays out the parameters for assuring progress at the landscape/jurisdictional scale and for making credible claims about contributions toward that progress. It walks through practical steps to ensure the integrity of landscape-level performance data and how progress is communicated, and it explores the types of claims companies can make depending on the ways they engage.\nWhile time is running out to reverse global ecosystem loss, we’re finally at a point where governments and companies are beginning to mobilize at the scale required to meet our conservation imperatives. Thanks to this new guidance, the pathways for corporate action are clearer than ever. Now, with these tools in hand, the moment has come for public-private partners to accelerate their joint efforts—for the future of the natural world and generations to come.","Global animal, bird and fish populations have plummeted more than two thirds in less than 50 years because of rampant over-consumption, experts said on Thursday in a stark warning to save nature in order to save ourselves.\nHuman activity has severely degraded three quarters of all land and 40 per cent of Earth's oceans, and our quickening destruction of nature is likely to have untold consequences on our health and livelihoods.\nThe Living Planet Index, which tracks more than 4,000 species of vertebrates, warned that increasing deforestation and agricultural expansion were the key drivers behind a 68 per cent average decline in populations between 1970 and 2016.\nIt said relentless natural habitat loss increased the risk of future pandemics as humans expand their presence into ever closer contact with wild animals.\n2020's Living Planet Report, a collaboration between WWF International and the Zoological Society of London, is the 13th edition of the biennial publication tracking wildlife populations around the world.\n\"It's an accelerating decrease that we've been monitoring for 30 years and it continues to go in the wrong direction,\" said WWF International director general Marco Lambertini.\n\"In 2016 we documented a 60 per cent decline, now we have a 70 per cent decline.\n\"All this is in a blink of an eye compared to the millions of years that many species have been living on the planet,\" he said.\nIn the last half decade there has seen unprecedented economic growth underpinned by an explosion in global consumption of natural resources.\nWhereas until 1970, humanity's ecological footprint was smaller than the Earth's capacity to regenerate resources, the WWF now calculates we are over-using the planet's capacity by more than half.\nWhile aided by factors such as invasive species and pollution, the biggest single driver of species lost is land-use changes: usually industry converting forests or grasslands into farms.\nThis takes an immense toll on wild species, which lose their homes.\nBut it also requires unsustainable levels of resources to uphold: one third of all land mass and three quarters of all freshwater are now dedicated to producing food.\nThe picture is equally dire in the ocean, where 75 per cent of fish stocks are overexploited.\nAnd while wildlife is declining rapidly, species are disappearing faster in some places than others.\nThe index showed that the tropical regions of Central and South America had recorded a 94 per cent fall in species since 1970.\n\"It is staggering. It is ultimately an indicator of our impact on the natural world,\" said Mr Lambertini.\nThe Living Planet update comes alongside a study co-authored by more than 40 NGOs and academic institutions, which lays out ways of arresting and reversing the losses human consumption has inflicted.\nThe research, published in the journal Nature, suggests that reducing food waste and favouring healthier and more environment-friendly diets could help to \"bend the curve\" of degradation.\nCoupled with radical conservation efforts, these measures could avert more than two thirds of future biodiversity loss, the authors suggested.\n\"We need to act now. Rates of biodiversity recovery are typically much slower than those of recent biodiversity loss,\" said lead study author David Leclere, research scholar at the International Institute of Applied System Analysis.\n\"This implies that any delay in action will allow further biodiversity losses that might take decades to restore.\"\nMr Leclere also warned of \"irreversible\" losses to biodiversity, such as when a species goes extinct.\nMr Lambertini said that, like public discourse on climate change, societies are increasingly concerned about the links between the health of the planet and human well-being.\n\"From being sad about losing nature, people are beginning to actually get worried,\" he said.\n\"We still have a moral duty to co-exist with life on the planet, but there's now this new element of impact on our society, our economy and, of course, our health.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2fb0fe0c-d6be-45ee-925f-8f548ccafb15>","<urn:uuid:8f97e239-71cf-4d18-b118-e96a382bb79e>"],"error":null}
{"question":"How does the New Zealand Pavilion at Shanghai Expo 2010 incorporate Maori creation mythology into its architectural design?","answer":"The pavilion incorporates the Maori creation myth through its design elements: a hovering lightweight canopy represents the sky father (Ranginui), while the folded surface below represents the earth mother (Papatuanuku). This design references the Maori creation story where Tane separated his parents by pushing his father Ranginui up to become the sky while his mother Papatuanuku remained as Earth, bringing light into being. The architectural elements physically embody this mythology through their spatial relationship.","context":["Shanghai Expo 2010 New Zealand Pavilion, Warren and Mahoney Architects, Chinese Design\nShanghai Expo New Zealand Pavilion\nArchitecture Information China – design by Warren and Mahoney\n9 Aug 2010\nNew Zealand Pavilion at Shanghai Expo 2010\nArchitect: Warren and Mahoney\nThe New Zealand Pavilion for the 2010 World Expo in Shanghai was commissioned through a limited design competition by the New Zealand Government. The brief called for a challenging design ambition in that it had to be both the physical representation of New Zealand’s culture and identity; as well as a symbol of the relationship between two nations.\nArchitects are typically engaged with built form that is an active part of an evolving urban fabric – buildings and structures that develop a ‘personality’ in the public’s eye over many years. Working in an Expo context in a foreign land requires an architectural approach which is much more immediate. With minutes rather than years to form a relationship with a building, a different approach and visual language was required.\nAmongst the architectural ‘white noise’ of the expo context, the design ambition for the New Zealand Pavilion is to move beyond architecture as object – to provide an integrated experience which combines seamlessly with the architectural form and exhibition content.\nThe architectural expression of the pavilion takes the form of a folded, inclined surface – a literal representation of the New Zealand landscape, transported to Shanghai. The folded surface of the Pavilion references the proximity and connection between New Zealand cities and the landscape, while promoting the idea that our urban and rural environments are balanced and sustainable.\nThis surface is combined with a hovering lightweight canopy ‘sky surface’ and sculptural supports in a manner that forms a contemporary expression of the Maori myth of creation:\n“The Maori creation story begins with a description of darkness and nothingness – Ranginui, the sky father, and Papatuanuku, the earth mother, prevented light from reaching the world because of their close embrace, and their offspring lived in a world of darkness between the bodies of their parents. It is said that some of the sons decided that their situation could be remedied only if they separated their parents, so that Ranginui would be pushed up to become the sky and Papatuanuku remain as their Earth. They set about their task. Tane it was who finally rendered them apart by resting his shoulders upon Papatuanuku and thrusting his legs upwards and pushing Ranginui to the sky. By this separation the world of light, of existence, came into being”\nSpatially, the pavilion is composed of three key sequential experiences: a welcome and arrival space; an internal exhibition space; and an external rooftop garden. Visitors are welcomed in the arrival space by Maori carvers and performers, then travelling by ramp through the internal exhibition before exiting onto the top of the sloped building and descending through the New Zealand landscape – from the mountains to the sea. In this manner, the exhibition content is delivered outside, within, and on top of the folded surface of the pavilion, eliminating any division between architecture and experience; or container and content.\nThe New Zealand Pavilion is about authentic experiences – real welcome, real hospitality, real performance. Authenticity is a commodity in short supply in an expo context and a manner in which New Zealand experiences and products are often positioned on the world stage. A key part of this authenticity is the concept of welcome. The New Zealand Pavilion is open and approachable and reaches out into the adjacent plaza space – the invitation to enter and engage therefore being tangible. It is the antithesis of the closed sculptural form which an audience approaches and then enters, and is appropriate to a young, multi-cultural Pacific country which prides itself on its openness, warmth, and transparency.\nMATERIALITY & TECTONIC\nNew Zealand’s urban environments are highly connected to the landscape, and its people remain intensely aware of this relationship. The desire for connection permeates much of our lives, from understanding the origin of the water we drink, to the food we eat and the fabrics we wear. The Pavilion is made with a similar sensibility. The connections and materials are celebrated, and their origin and nature are expressed. There is a conscious move away from the slick and the sanitised, towards the tactile and authentic qualities of natural, sustainable materials. Within an expo context, which is very much focussed on technology, the resulting impression is one of calmness, with a sense of brooding, primordial mystery. The underlying message is one of honesty and openness – a response that has a depth beyond the sales pitch, and which attempts to forge a real connection with its Chinese hosts.\nShanghai Expo New Zealand Pavilion images / information from FD\nLocation: Shanghai, China\nArchitecture in Shanghai\nShanghai Architecture Designs – chronological list\nWarren and Mahoney project : Christchurch Airport, New Zealand\nArchitecture Walking Tours by e-architect\nShanghai Architecture – Expo Pavilion Selection\nDanish Pavilion, Expo 2010\nShanghai Expo 2010 Danish Pavilion\nBritish Pavilion Shanghai Expo 2010\nShanghai Expo British Pavilion\nSpanish Pavilion Shanghai Expo 2010\nMiralles Tagliabue EMBT\nShanghai Expo 2010 Spanish Pavilion\nComments / photos for the Shanghai Expo New Zealand Pavilion 2010 Architecture page welcome"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2d318066-337b-450a-bddc-57cb33be1abf>"],"error":null}
{"question":"What is special about the stone paths called 'cat steps' in Shrogg's Park, such as their design or purpose?","answer":"The 'cat steps' were stone-setted paths that incorporated a distinctive raised run of setts at frequent intervals. This configuration was specifically designed to aid grip on the stone paths.","context":["- Heritage Category:\n- Park and Garden\n- List Entry Number:\n- Date first listed:\nThe above map is for quick reference purposes only and may not be to scale. For a copy of the full scale map, please see the attached PDF - 1001557.pdf\nThe PDF will be generated from our live systems and may take a few minutes to download depending on how busy our servers are. We apologise for this delay.\nThis copy shows the entry on 28-Jul-2021 at 14:29:04.\nThe building or site itself may lie within the boundary of more than one authority.\n- Calderdale (Metropolitan Authority)\n- Non Civil Parish\n- National Grid Reference:\n- SE 08234 26130\nA public park laid out in the late 1870s and opened in 1881.\nOn 22nd November 1872, Colonel Edward Akroyd, a leading business man and benefactor, promised to build Shrogg's Park in Halifax. The site was a rough, irregular piece of waste land, entirely barren at one end and at the other, thickly covered with dwarf oak scrub, from which the Park gained its name, and pieces of rock. His plans almost came to nothing when the proposed Midland Railway scheme intended to build a line through part of what is now the Park. During a committee meeting of the House of Commons, Akroyd gave evidence of the measures he had undertaken so far and of his intention to provide a place for cricket, bowls, croquet, archery and other games. The railway scheme was turned down and work on the Park commenced. From 24th June 1873, the freehold for the land was leased from Captain Henry Savile of Rufford Abbey, Nottinghamshire for 999 years at a rent of £1. At a cost of over £10,000, work began on building the boundary wall, levelling ground and forming drives and footpaths. On 25th June 1879, Colonel Akroyd handed the partially completed park over to the Halifax Corporation for the benefit of the town. A list of conditions were attached to the generous gift:- the area was to be used only as a public promenade and recreation ground, and kept open during the whole year, including Sundays; that no charge be made for admission; that music and games be allowed under certain regulations; and that the Corporation spend a minimum of £1500 completing the Park and set aside an annual maintenance budget of at least £100 to keep the Park in a proper condition.\nThe final layout of the Park featured over 60,000 trees and shrubs; a fountain basin; a handsome, ornamental drinking fountain; striking bedding out displays and a shelter pavilion. In April 1881, Captain Savile gifted two plots of land in Shrogg's Road to improve the entrance to the Park. Later that year, on the 23rd June, the Park was officially opened to the public. It remains (2001) in public ownership today. DESCRIPTION\nLOCATION, AREA, BOUNDARIES, LANDFORM, SETTING Shrogg's Park is located c 2km north-west of Halifax town centre. It is situated on high promontory overlooking the Shroggs Mills and the Hebble Brook valley to the west. To the north and east the park lies against streets of late C19 stone villas whilst to the west and south the land falls to the industrialised valley. The park is c 10ha and comprises a mixture of landscape elements. The centre of the park is level and is laid out to accommodate a lake, floral displays, sports field, bowling greens, playground and depot. To the west and south-west the land falls steeply to the Shrogg's Road. The slopes are covered with broad leaf scrub and in the north-west of the site, rock outcrops form a striking landscape feature and viewing platform to the south and west. The land to the north and east falls more gently to St. George's Road and Lee Mount Road.\nA dressed stone wall c 1.70m high contains the park. Along St.George's Road and to the west and east of the Lee Mount Road entrance, the wall is reduced to a curb to accommodate iron railings, which were removed in the mid C20 as part of the war effort. Along the north of the park against the Lee Mount School the boundary is marked by a steel security fence c 2.25m high.\nENTRANCES AND APPROACHES The principal entrance, and accompanying lodge, is at the junction of Lee Mount Road and St. George's Road in the east of the park. Three stone gate piers, c 3m high mark the vehicular and pedestrian entrance although the original gates are gone and are replaced with a steel security barrier of late C20 design and construction. To the west and east of the gate piers the wall comprises a low stone curb on which iron railings were located. These were removed in the mid C20 and have not been replaced (2001). A second vehicular and pedestrian entrance is situated at the extreme north of the park at the junction between Lee Mount Road and Wheatley Road. The three stone gate piers are in place although they are not complete as two of them have lost their stone balls that cap the pier.\nIn addition to the two main entrances there are three pedestrian entrances. One is situated on Lee mount Road c 120m west of the Lodge Entrance. Original stone gate piers remain, however the steel gate is from the late C20. Restoration work and repointing to the stone wall to the west and east of the entrance is inappropriate and not of conservation standard. A second entrance is c 150m south of the Lodge Entrance on St. George's Road. The gate and gate piers are missing. Access to the south of the park from Shrogg¿s road is via a flight of stone steps which punctuate the boundary wall. Two stone gate piers, c 2m high, mark the entrance and although the gate is missing there is 0.4m run of original cast iron railings on a stone curb still in situ on either side of the piers. Immediately beyond the entrance a 3.5 m high stone wall retains the steep west bank of the park. Access to the north-west and south-east paths along the bank is provided by two flights of steps which run along the face of the retaining wall. The steps lead to the paths that are surfaced with stone setts, which incorporate a distinctive raised run of setts at frequent intervals. This configuration is known locally as cat steps and was designed to aid grip on the stone paths.\nPRINCIPAL BUILDING The principal building is a stone and slate lodge situated c 20m south of the entrance gates at the junction of St. George's Road and Lee Mount Road. It is still used as accommodation and is in a good state of repair.\nGARDENS AND PLEASURE GROUNDS The five entrances are linked to a perimeter path that encompasses the flatter areas of the park on the top of the hill. Set within the path are the recreational and games facilities including two bowling greens, multi purpose sports areas and two football fields. Between the path and the north boundary wall shrubberies covered banks\nThe principal park features are located in the north-west of the site. Located c 150m south-east of the Wheatley Road entrance a stone edged fountain basin is placed in the centre of gently raised grassed banks. The fountain no longer contains water and is now (2001) turfed. A raised bank for floral displays is situated c15m to the west of the fountain basin however the bank also now (2001) completely turfed . Approximately 25m to the south-east of the basin is a small stone edged serpentine lake c 70m long and c 14m wide. A gothic style, stone drinking fountain set on four steps is situated on the perimeter walk c 60m south-east of the fountain basin.\nShrogg's Road to the west is served by two historical paths which are paved with stone setts with rockery stone edges. A third path, which ran underneath the rock outcrops in the north-west of the park has fallen into disuse. C20 paths were built along the western slope.\nHalifax Antiquarian Society Transactions,1948, p98 The New Shrogg's Park, The Halifax Guardian, 4 June 1881 Shrogg's Park, Colonels fight against railway inroad, The Halifax Guardian, 30 January 1950\nMaps OS 25\" to 1 mile: 2nd edition published 1907 2nd edition published 1919 3rd edition published 1922 3rd edition published 1933\nIllustrations Early C20 postcards of Shrogg's Park, Halifax, Halifax Central Library\nArchival items Halifax County Borough Minutes, 1880-1883, Halifax Central Library\nDescription written: July 2001 Amended: August 2001 Register Inspector: PV Edited: October 2001\nThe contents of this record have been generated from a legacy data system.\n- Legacy System number:\n- Legacy System:\n- Parks and Gardens\nThis garden or other land is registered under the Historic Buildings and Ancient Monuments Act 1953 within the Register of Historic Parks and Gardens by Historic England for its special historic interest.\nEnd of official listing"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:09d9f0af-cf53-43c3-9f59-ca5ede30b30e>"],"error":null}
{"question":"How do NECAP1-related seizures compare to West Nile virus neuroinvasive disease in terms of severity and outcomes? Please include details about onset timing and manifestations.","answer":"NECAP1-related seizures begin in early infancy (around 3 months of age) with intractable seizures including generalized tonic-clonic and clonic types, preceded by hypotonia and profound developmental delay. These seizures are part of a genetic condition and can be severe enough to present as Ohtahara syndrome with burst suppression pattern on EEG. In contrast, West Nile virus neuroinvasive disease occurs in less than 1% of infected people and can cause encephalitis, meningitis, and tremors/convulsions. While WNV neuroinvasive disease can be severe, most people recover completely. WNV symptoms develop within 3-14 days after exposure (up to 3 weeks in immunocompromised individuals). Death from WNV is very rare, while NECAP1-related conditions have been reported in multiple cases with severe developmental impacts.","context":["Intellectual disabilityGene: NECAP1 Amber List (moderate evidence)\nComment on list classification: New gene added by external expert and reviewed by curation team. NECAP1 has been given an amber gene rating based on the evidence provided by Konstantinos Varvagiannis. As patients from the cases described in PMID: 24399846, 30525121 are from Saudi Arabia and have the same variant, these are counted as a single piece of evidence. Therefore, there are not enough evidence to promote this gene to a green rating at this stage.\nCreated: 20 Feb 2019, 4:14 p.m.\nGreen List (high evidence)\nBiallelic pathogenic variants in NECAP1 cause ?Epileptic encephalopathy, early infantile, 21 (MIM 615833).\nPMID: 24399846 (Alazami et al. 2014) report on 6 individuals from an multigenerational family from Saudi Arabia with biallelic NECAP1 nonsense variant. The common phenotype consisted of hypotonia, profound global developmental delay preceding the onset of intractable seizures (fragmented multifocal clonic and tonic) in early infancy. Initial workup excluded metabolic causes.\n4 of these individuals were born to first cousins once removed, while 2 additional affected subjects from the broader pedigree were born to seemingly unrelated parents from the same region. All affected individuals shared a single autozygous 4.78-Mb interval on chromosome 12p. Linkage analysis confirmed involvement of this locus (LOD score : 5.0447). Exome sequencing demonstrated homozygosity for a nonsense variant (NM_015509.3:c.142C>T - p.R48*). mRNA levels in lymphoblast cell lines from affected subjects were significantly reduced when compared to controls, probably due to NMD.\nNecap1 was shown to be strongly expressed in the developing (E14.5) mouse brain and spinal cord, upon immunohistochemical analysis (part of the current study).\nNECAP1 has been previously shown to have a functional role in Clathrin-mediated encocytocis (CME), a process which plays a critical role at the site of synapsis (in synaptic vesicle recycling).\nPMID: 30525121 (Alsahli et al. 2018) report on a 41-month-old girl with hypotonia, profound global developmental delay and onset of seizures at the age of 3 months (generalized tonic and clonic / flexor hemispasms). Initial workup was negative for an eventual metabolic origin. The girl was born to consanguineous Saudi parents and was found to harbor the p.R48* variant in the homozygous state, following trio-WES.\nPMID: 30626896 (Mizuguchi et al. 2019) report on a 16-month-old boy, born to consanguineous parents from Malaysia. This individual presented with axial hypotonia and profound developmental delay and developed generalized tonic-clonic and clonic seizures at the (corrected) age of 3 months. EEG demonstrated a burst suppression pattern and a clinical diagnosis of Ohtahara syndrome was retained. Metabolic workup was normal.\nHomozygosity for a splice-site NECAP1 variant (NM_015509.3:c.301+1G>A) was demonstrated following exome sequencing. The variant was shown to result in inclusion of a 44-bp intron, resulting in frameshift and introduction of a premature termination codon (p.Gly101Aspfs*45). The level of abnormal transcript was 2-fold increased in lymphoblast cells trated with cycloheximide when compared to cells treated with DMSO, suggesting involvement of NMD.\nAs also in PMID: 30525121, the present study suggests similarities with the DNM1-related phenotype (Epileptic encephalopathy, early infantile, 31 - #616346 - DNM1 is rated green in the ID panel) as DNM1 also participates in vesicle recycling. The authors of the present study also note that mutations in CLTC (encoding clathrin heavy chain) cause hypotonia with DD/ID with or without epilepsy (Mental retardation, autosomal dominant 56 - #617854 - CLTC is rated green in the ID panel).\nNECAP1 is not associated with any phenotype in G2P.\nThis gene is included in gene panels for ID offered by some diagnostic laboratories.\nAs a result this gene can be considered for inclusion in this panel as green (or amber).\nCreated: 3 Feb 2019, 8 p.m.\nMode of inheritance\nBIALLELIC, autosomal or pseudoautosomal\n?Epileptic encephalopathy, early infantile 21, 615833\nVariants in this GENE are reported as part of current diagnostic practice\nGene: necap1 has been classified as Amber List (Moderate Evidence).\ngene: NECAP1 was added gene: NECAP1 was added to Intellectual disability. Sources: Literature Mode of inheritance for gene: NECAP1 was set to BIALLELIC, autosomal or pseudoautosomal Publications for gene: NECAP1 were set to 24399846; 30525121; 30626896 Phenotypes for gene: NECAP1 were set to ?Epileptic encephalopathy, early infantile 21, 615833 Penetrance for gene: NECAP1 were set to Complete Review for gene: NECAP1 was set to GREEN gene: NECAP1 was marked as current diagnostic\nIf promoting or demoting a gene, please provide comments to justify a decision to move it.\nGenes included in a Genomics England gene panel for a rare disease category (green list) should fit the criteria A-E outlined below.\nThese guidelines were developed as a combination of the ClinGen DEFINITIVE evidence for a causal role of the gene in the disease(a), and the Developmental Disorder Genotype-Phenotype (DDG2P) CONFIRMED DD Gene evidence level(b) (please see the original references provided below for full details). These help provide a guideline for expert reviewers when assessing whether a gene should be on the green or the red list of a panel.\nA. There are plausible disease-causing mutations(i) within, affecting or encompassing an interpretable functional region(ii) of this gene identified in multiple (>3) unrelated cases/families with the phenotype(iii).\nB. There are plausible disease-causing mutations(i) within, affecting or encompassing cis-regulatory elements convincingly affecting the expression of a single gene identified in multiple (>3) unrelated cases/families with the phenotype(iii).\nC. As definitions A or B but in 2 or 3 unrelated cases/families with the phenotype, with the addition of convincing bioinformatic or functional evidence of causation e.g. known inborn error of metabolism with mutation in orthologous gene which is known to have the relevant deficient enzymatic activity in other species; existence of an animal model which recapitulates the human phenotype.\nD. Evidence indicates that disease-causing mutations follow a Mendelian pattern of causation appropriate for reporting in a diagnostic setting(iv).\nE. No convincing evidence exists or has emerged that contradicts the role of the gene in the specified phenotype.\n(i)Plausible disease-causing mutations: Recurrent de novo mutations convincingly affecting gene function. Rare, fully-penetrant mutations - relevant genotype never, or very rarely, seen in controls. (ii) Interpretable functional region: ORF in protein coding genes miRNA stem or loop. (iii) Phenotype: the rare disease category, as described in the eligibility statement. (iv) Intermediate penetrance genes should not be included.\nIt’s assumed that loss-of-function variants in this gene can cause the disease/phenotype unless an exception to this rule is known. We would like to collect information regarding exceptions. An example exception is the PCSK9 gene, where loss-of-function variants are not relevant for a hypercholesterolemia phenotype as they are associated with increased LDL-cholesterol uptake via LDLR (PMID: 25911073).\nIf a curated set of known-pathogenic variants is available for this gene-phenotype, please contact us at [email protected]\nWe classify loss-of-function variants as those with the following Sequence Ontology (SO) terms:\nTerm descriptions can be found on the PanelApp homepage and Ensembl.\nIf you are submitting this evaluation on behalf of a clinical laboratory please indicate whether you report variants in this gene as part of your current diagnostic practice by checking the box\nStandardised terms were used to represent the gene-disease mode of inheritance, and were mapped to commonly used terms from the different sources. Below each of the terms is described, along with the equivalent commonly-used terms.\nA variant on one allele of this gene can cause the disease, and imprinting has not been implicated.\nA variant on the paternally-inherited allele of this gene can cause the disease, if the alternate allele is imprinted (function muted).\nA variant on the maternally-inherited allele of this gene can cause the disease, if the alternate allele is imprinted (function muted).\nA variant on one allele of this gene can cause the disease. This is the default used for autosomal dominant mode of inheritance where no knowledge of the imprinting status of the gene required to cause the disease is known. Mapped to the following commonly used terms from different sources: autosomal dominant, dominant, AD, DOMINANT.\nA variant on both alleles of this gene is required to cause the disease. Mapped to the following commonly used terms from different sources: autosomal recessive, recessive, AR, RECESSIVE.\nThe disease can be caused by a variant on one or both alleles of this gene. Mapped to the following commonly used terms from different sources: autosomal recessive or autosomal dominant, recessive or dominant, AR/AD, AD/AR, DOMINANT/RECESSIVE, RECESSIVE/DOMINANT.\nA variant on one allele of this gene can cause the disease, however a variant on both alleles of this gene can result in a more severe form of the disease/phenotype.\nA variant in this gene can cause the disease in males as they have one X-chromosome allele, whereas a variant on both X-chromosome alleles is required to cause the disease in females. Mapped to the following commonly used term from different sources: X-linked recessive.\nA variant in this gene can cause the disease in males as they have one X-chromosome allele. A variant on one allele of this gene may also cause the disease in females, though the disease/phenotype may be less severe and may have a later-onset than is seen in males. X-linked inactivation and mosaicism in different tissues complicate whether a female presents with the disease, and can change over their lifetime. This term is the default setting used for X-linked genes, where it is not known definitately whether females require a variant on each allele of this gene in order to be affected. Mapped to the following commonly used terms from different sources: X-linked dominant, x-linked, X-LINKED, X-linked.\nThe gene is in the mitochondrial genome and variants within this can cause this disease, maternally inherited. Mapped to the following commonly used term from different sources: Mitochondrial.\nMapped to the following commonly used terms from different sources: Unknown, NA, information not provided.\nFor example, if the mode of inheritance is digenic, please indicate this in the comments and which other gene is involved.","Mosquitoes and Disease - Frequently Asked Questions\n1. What do I need to know about mosquitoes?\nWhile mosquitoes usually are considered a nuisance pest, occasionally they can transmit infections that can cause illness and even death in people and some animals. Mosquitoes are small flying insects that feed on human and animal blood or plant juices. There are about 70 species of mosquitoes in New York State. Only female mosquitoes bite to get a blood meal for their growing eggs. Mosquitoes usually become infected from feeding on infected birds.\n2. Do all mosquitoes spread disease?\nNo. Most mosquitoes do not spread disease. While there are about 70 different species of mosquitoes in New York State, only certain species transmit disease.\n3. Where do mosquitoes live and breed?\nMosquitoes lay their eggs in moist areas, such as standing water. The eggs become larvae that remain in the water until they mature into adults and fly off. Weeds, tall grass and shrubbery provide an outdoor home for adult mosquitoes. They also can enter houses, apartments and buildings through unscreened windows and doors. Many mosquitoes will breed in any container that holds water, such as flowerpots, wading pools or discarded tires.\n4. When are mosquitoes most active?\nSome mosquitoes are active between dusk and dawn, when the air is calm. However, others will feed at any time of day. Mosquitoes prefer a warm, moist environment. They are active from early summer until late fall in New York State. In southern states that have a warm year-round climate, mosquitoes that can transmit diseases are active year round. New Yorkers should take measures to protect themselves from mosquito bites whenever mosquitoes are active.\n5. What diseases can mosquitoes transmit to people?\nMosquitoes can spread a number of diseases to people, but two viruses that some mosquito species in New York State can transmit are eastern equine encephalitis (EEE, \"triple E\") and West Nile virus (WNV).\nEEE is spread only from infected mosquitoes. Infected mosquitoes also are the primary way people become infected with WNV, although a few cases of WNV have been transmitted by blood transfusion, organ transplantation, from a pregnant woman to her infant and through breastfeeding. Neither virus is spread from person-to-person, from humans to animals or from animals other than infected mosquitoes to people.\n6. What animals can be infected with eastern equine encephalitis (EEE) or West Nile Virus (WNV)?\nA number of animals can be infected with these mosquito-borne infections, but horses are the most susceptible to serious illness and death from EEE and WNV. EEE occasionally occurs in livestock, deer, dogs, reptiles, amphibians and captive birds such as the ring-necked pheasant, emu, ostriches, quail and ducks. WNV has been identified in at least 326 bird species and in horses, dogs and cows.\n7. Are there vaccines for EEE or WNV for people?\nNo, there are no human vaccines for EEE or WNV. The best way to prevent eastern equine encephalitis is to limit your exposure to mosquitoes and mosquito bites.\n8. Are there vaccines for EEE or WNV for my pets?\nThere are no EEE or WNV vaccines for dogs or cats. There are vaccines for EEE and WNV for horses and other equines (donkeys and mules). These vaccines should never be given to humans or other animals.\n9. What is the treatment for EEE and WNV infections?\nThere is no specific treatment for EEE or WNV. Antibiotics are not effective against viruses. Patients' symptoms are treated and those with severe illnesses receive supportive therapy, which may include hospitalization, respiratory support, IV fluids and prevention of other infections.\n10. How common is eastern equine encephalitis (EEE) in people?\nAbout 5-10 EEE cases of EEE are reported in the U.S. each year. New York State has reported five human cases: one each in 1971, 1983, 2009, 2010 and 2011. All were fatal. The risk of getting EEE is highest from late July through September.\n11. Who is at risk of becoming infected with EEE?\nPeople of all ages are at risk for infection with EEE virus (EEEV). However, people 50 of age and younger than 15 years of age are at greatest risk for developing serious disease.\n12. When do symptoms of EEE infection appear?\nMost people bitten by an infected mosquito will not develop any symptoms. Of those who do, symptoms usually appear 4-10 days after the bite of an infected mosquito.\n13. What are the symptoms of EEE?\nCases of EEE begin with the sudden onset of headache, high fever, chills and vomiting. The illness may then progress into disorientation, seizures, inflammation of the brain (encephalitis) or coma. Many people (about 3 out of 10) who develop EEE die, and many of those who survive have mild to severe brain damage for life.\n14. How common is West Nile virus (WNV) in people?\nSince 2000, 490 cases of West Nile virus have been reported in New York State. Of these 37 people died (less than 1% of reported cases).\n15. Who is at risk of being infected with WNV?\nAnyone who is exposed to infected mosquitoes and is bitten is at risk of WNV. However, people over 50 years of age are at the greatest risk of developing severe WNV disease (called neuroinvasive disease).\n16. When do symptoms of WNV infection appear?\nMost people bitten by an infected mosquito will not develop any symptoms. People who become ill from WNV usually develop symptoms within 3-14 days after exposure, although people with weakened immune systems may take up to 3 weeks to develop symptoms.\n17. What are the symptoms of WNV?\nMost people (about 8 out of 10) infected with the West Nile virus do not have any signs or symptoms.\nAbout 20% of the people who become infected with WNV develop West Nile fever (WNF), also called non-neuroinvasive disease. People with WNF typically have mild symptoms, including a sudden onset of fever, headache, stomachache, body aches and occasionally a skin rash and swollen lymph glands. Symptoms usually improve after several days, but people may feel tired, weak and generally unwell for weeks.\nAbout 1 in 150 people infected with WNV (less than 1%) develop severe neuroinvasive disease that affects the central nervous system. Symptoms of neuroinvasive disease can include sudden onset of headache, high fever, neck stiffness, muscle weakness, altered mental status, tremors, convulsions, paralysis, inflammation of the brain (encephalitis) or of the membranes of the brain and spinal cord (meningitis) or coma. Death from WNV is very rare, but can occur. Most people recover completely from even severe WNV neuroinvasive disease.\n18. How can I protect my family and myself from mosquito-borne infections?\nAlthough your chances of being infected with a disease through a mosquito bite are small, there are simple steps you can take to reduce your risk of being bitten. Reduce the mosquito population around your home and property, reduce or eliminate all standing water:\n- Dispose of tin cans, plastic containers, ceramic pots or similar water-holding containers.\n- Dispose of used tires, which are a significant mosquito-breeding site. Call your local landfill or Department of Public Works to find out how to dispose of used tires properly.\n- Drill holes in the bottoms of outdoor recycling bins so they can drain freely.\n- Clean clogged roof gutters and make sure they drain properly.\n- Remove leaf debris from yards and gardens.\n- Drain temporary pools of water or fill with dirt.\n- Turn over wading pools and wheelbarrows when not in use.\n- Change the water in birdbaths twice weekly.\n- Clean vegetation and debris from edges of ponds.\n- Clean and chlorinate swimming pools, outdoor saunas, hot tubs and other water features, such as fountains and garden ponds.\n- Drain water from pool covers.\n- Use landscaping to eliminate standing water that collects on your property.\n19. Should we stay indoors when mosquitoes are out?\nIt is not necessary to stay indoors. However, try to reduce your risk of being bitten by mosquitoes. In addition to reducing standing water in your yard, take the following steps:\n- Make sure all windows and doors have screens, and that screens are free of rips, tears and holes.\n- Cover your skin as completely as possible. Wear shoes and socks, long pants and a long-sleeved shirt when outdoors for long periods or when mosquitoes are most active.\n- Use mosquito repellent. Always apply according to label directions. Information on choosing and safely using insect repellents is on the U.S. Environmental Protection Agency (EPA) website at http://cfpub.epa.gov/oppref/insect/.\n- Cover baby carriers and beds with mosquito netting when outdoors.\n- Stay indoors at sunrise, sunset and early in the evening when mosquitoes are most active.\n20. What is my community doing to control mosquitoes?\nLocal communities, in consultation with the NYSDOH, may implement various control measures, for example, spraying, based on geographic location and assessed level of risk. However, the risks and benefits of control methods must be carefully considered prior to taking such measures."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4ee29418-4181-4820-9242-72ff556a5c12>","<urn:uuid:2bb2cc8b-8b91-4552-9d1d-1a4609da192b>"],"error":null}
{"question":"What are the main research areas covered in recent developments of maritime transportation?","answer":"The main research areas include Ship Hydrodynamics, Marine Structures, Ship Design, Shipyard Technology, Ship Machinery, Maritime Transportation, and Safety and Reliability. Additional areas covered are environment, Renewable Energy, Wave and Wind Modelling, Coastal Engineering, Fisheries and Legal Maritime Aspects.","context":["By Carlos Guedes Soares, Fernando López Peña\nDevelopments in Maritime Transportation and Exploitation of Sea Resources covers fresh advancements in maritime transportation and exploitation of sea assets, encompassing ocean and coastal components. The booklet brings jointly a variety of papers reflecting basic components of contemporary learn and improvement within the fields of:\n– send Hydrodynamics\n– Marine Structures\n– send Design\n– Shipyard Technology\n– send Machinery\n– Maritime Transportation, and\n– protection and Reliability\nIssues corresponding to the surroundings, Renewable strength, Wave and Wind Modelling, Coastal Engineering, Fisheries and felony Maritime features also are addressed. Developments in Maritime Transportation and Exploitation of Sea assets is meant for lecturers and pros eager about the advance of marine transportation and the exploitation of sea resources.\nRead or Download Developments in Maritime Transportation and Exploitation of Sea Resources: IMAM 2013 PDF\nBest civil engineering books\nWorld-renowned publication publishers seeing that 1913, CRC Press is proud to introduce its latest imprint and suggestion CRCnetBASE: a dynamic, increasing database for the twenty first century. CRCnetBASE is designed for the worldwide specialist, technical, and clinical groups. greater than easily handbooks on CD-ROM, our items reside, evolving records increasing as wisdom expands, starting to be as know-how advances, and achieving to fulfill your info wishes immediately.\nThis booklet information the peculiarities of the necessities for refractories designed for aluminium metallurgical approach: relief, solid apartment, and anode construction. the writer describes standards particular to the houses and constitution of refractory fabrics that differentiate it from the refractories for ferrous metallurgy and different refractories.\nHighlights the Economies and building Industries of the Asia-Pacific sector Spon’s Asia-Pacific development expenses guide: 5th version presents overarching development fee info for sixteen nations: • Brunei • Cambodia • China • Hong Kong • India • Indonesia • Japan • Malaysia • Myanmar • Philippines • Singapore • South Korea • Sri Lanka • Taiwan • Thailand • Vietnam presents an advent to every nation and its development undefined It incorporates a nearby assessment of the development within the Asia-Pacific zone in addition to vast and macroeconomic information on key nationwide signs, building output signs, and normal building price info.\nEven supposing various fastenings assembles are put in on a daily basis, knowing within the engineering neighborhood in their behaviour is proscribed, and there's no as a rule permitted layout strategy. layout of fastenings in concrete over comes this. This layout advisor is predicated on a security idea utilizing partial protection elements taken from the CEB/FIB version Code 1990, and it covers all loading events and failure types.\n- Nonconventional Concrete Technologies: Renewal of the Highway Infrastructure (Compass Series)\n- Karl Terzaghi : the engineer as artist\n- Practical Railway Engineering\n- Structural Dynamics: Theory and Computation\nAdditional info for Developments in Maritime Transportation and Exploitation of Sea Resources: IMAM 2013\nThe resulting wake fields are compared not only in terms of the different velocity values but also (and mainly) in terms of Ship propeller performances have always increased during time, with the introduction of more accurate design and analysis procedures, better experimental testing techniques both in model and full scale. ) stringent requirements in terms of pressure pulses and radiated noise. In particular, radiated noise problems, which were usually linked to naval ship operating requirements, are likely to acquire more and more importance also for merchant ships, due to the increased attention to environmental issues, which in future may lead to more stringent limits (André et al.\nSoc. , Ser. A, 350, 1–26. , 1984, An Introduction to Theoretical and Computational Aerodynamics, J. Wiley & Sons. , 2011, Application of a BEM time stepping algorithm in understanding complex unsteady propulsion hydrodynamic phenomena, Ocean Engineering, 38, 699–711. , 2012, Biomimetic propulsion under random heaving conditions, using active pitch control, Journal of Fluids & Structures, (in press). doi. 004. , 2003, Forces on oscillating foils for propulsion and maneuvering, Jour. Fluids & Structures, 17, 163–183.\nIndb 20 9/5/2013 4:55:28 PM ACKNOWLEDGEMENTS Bugalski, T. , 2011. Numerical Simulation of the Self-Propulsion Model Tests. Second International Symposium on Marine Propulsors, Hamburg, Germany, June 2011. Cd-Adapco, 2012. 011, 2012. H. , 2002. , 2002, Springer Verlag, Berlin, Heidelberg. , 2010. A Workshop on Numerical Ship Hydrodynamics. Department of Shipping and Marine Technology, Chalmers University of Technology, Gothenburg. Sasajima, H. & Tanaka, I. 1966. On the estimation of wakes of ships.\nDevelopments in Maritime Transportation and Exploitation of Sea Resources: IMAM 2013 by Carlos Guedes Soares, Fernando López Peña"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:df248aba-0969-4b38-bf76-71d8228906bb>"],"error":null}
{"question":"What solutions are being developed to make both supply chain systems and cryptocurrency mining more environmentally sustainable?","answer":"For supply chain systems, solutions focus on data analytics to optimize operations and reduce waste, including using descriptive analytics to find realistic solutions and taking minimalist approaches that yield better results while avoiding overcomplicated solutions. For cryptocurrency mining, several sustainability initiatives are being developed: using renewable energy sources like solar and wind power, developing energy-efficient mining equipment like ASICs, implementing proof-of-stake consensus mechanisms instead of energy-intensive proof-of-work systems, and using carbon offsetting by investing in renewable energy projects. Both industries recognize the importance of sustainability for long-term success, though they approach it through different methods based on their specific operational requirements.","context":["The power of AI for supply chain efficiency\nArtificial Intelligence (AI) offers huge potential for supply chains, helping to reduce disruption and improve efficiency. So far, however, there has been a lack of practical examples and case studies demonstrating how data-driven approaches can be implemented effectively. Research from the IfM’s Manufacturing Analytics research group, led by Dr Alexandra Brintrup, has been addressing this gap by running pilot studies and highlighting the opportunities and potential pitfalls. Alexandra explains what her research reveals and the lessons for organisations…\nSupply chain analytics isn’t a new concept – in fact the manufacturing sector has long been an enthusiastic adopter of data-driven techniques. So apart from a fancy new name, what is really new?\nA combination of three emerging developments is changing the game: The first is computational power, enabling us to do calculations real time; secondly, powerful new algorithms can automate analysis and decision-making; and crucially, new and previously untapped sources of data are emerging.\n‘Supply chain analytics’ is an umbrella term, referring to a multitude of capabilities. There is no single solution that works for every organisation – it depends on the nature of the supply chain, the organisational strategy and priorities, and the information that is available. And not every capability is appropriate for every organisation – they should not be viewed as a to-do list! Instead, firms can pick and choose which capabilities to develop to suit particular supply chain functions and business needs.\nHowever, there are some common questions that organisations can address in order to understand how supply chain analytics and AI can best be deployed in their own context.\nHow much data and from what source?\nWhere does your data come from? Traditionally in supply chains we’ve had enterprise resource planning (ERP) systems taking structured sources of data (which are mostly manually populated but also drawn from some automated processes). Now we have data from a larger array of sources: smart products that provide data with status updates on their use, location, and condition, through sensors and IoT connectivity; GPS location tracking data; and even unconventional sources such as social media.\nMuch of this data can be obtained in real time, and can be gathered from beyond your immediate organisational boundaries – whether from supply chain partners, external organisations or customers.\nHow will the data provide valuable information?\nData volume can be overwhelming, so understanding and focusing on what will genuinely add value is essential. How will you use the data to create a better understanding of what is going on?\nIt’s also important to consider how data from different sources can be integrated to provide a dynamic overview. This can, for example, enable predictive analytics to reduce disruption.\nHow does the data improve decision-making?\nImproved data availability offers the potential for much greater awareness of systems. But what will you do with the new-found awareness - what kinds of decisions will the data be used to improve?\nCan you identify ways to optimise current processes, or about how to redesign systems in the future? Is the focus primarily on streamlining day-today operations, or on tactical areas, or strategic decisions? And can data be used collectively across the supply chain to improve efficiency?\nHow does this support automation or semi-automation of tasks?\nData analytics can allow hidden patterns and trends in the data to be uncovered and acted upon, in order to improve supply chain operations. The automation or semi-automation of mundane operational tasks, identified through the data, can have a transformative impact on optimisation in the supply chain.\nSupply chain analytics in practice: Real-world examples\nOur Manufacturing Analytics research team has conducted several studies on supply chain analytics with partners from the automotive, and aerospace industries as well as FMCG and other sectors.\nThe goal was to map the supply chain structure, understand how disruptions may cascade and impact this structure, and then use this knowledge to inject resilience after predicting hidden dependencies and supplier deliveries (see Figure 1).\nFigure 1: Identifying and reducing disruption in the supply chain through data analytics\nThese studies reveal five key lessons for using supply chain data analytics:\n1. Don’t underestimate the power of descriptive analytics to lead to more realistic solutions\nThe very first lesson is about not underestimating the potential benefits that can be gained from exploratory, descriptive analytics before moving onto solution finding. The insights we gained from exploratory studies provided greater depth than we expected. For example, the hub-spoke structure that emerges in supplier-manufacturer connections and the density of connections told us that there is a higher than expected likelihood of tier one suppliers connecting to one another, unbeknownst to the original equipment manufacturer (OEM).\nBy mapping the supply chain data, we were able to spot patterns, and predict what failures were likely to occur, and therefore take much better precautions. In one of the studies we worked with a large FMCG corporation, and we able to spot where inventory could be injected to provide a buffer against likely stock shortages. In fact, we found that some types of network structures require much less inventory to ride out the same level of disruption - so we could map out what level of inventory is needed for a particular type of network structure to reduce disruption without unnecessary stockpiling.\n2. Don’t go crazy! A minimalist approach may yield the best results\nThe second lesson is about staying focused and avoiding overcomplicated solutions. For example, one company we worked with wanted to find out which of their suppliers were supplying to each other – as this is a problem for reliability of supply. Where were there hidden dependencies amongst suppliers? Initially we tried to identify this with sophisticated methods: using time series data, and trying to tease it all out with deep neural nets, recursive nets, and other techniques… but nothing worked.\nThen we went to back to basics, and asked ‘What is likely to connect suppliers to each other?’ If they produce these products, maybe their models are compatible, so they supply the same OEMs. This worked. Getting back to fundamental patterns was key.\n3. One solution doesn’t fit all\nOften data across supply chains can quickly become too complicated. In our studies, variables such as suppliers, parts, delivery times, volumes, locations, routes, level of confidence – all added up to a very large data set that became too complex. We found that it was essential to divide the problem into more digestible chunks, considering one or two elements at a time to produce more meaningful and useable information.\n4. Domain knowledge is golden\nIt is crucial to work with the operational team to decipher patterns in data – for example in a supplier disruption prediction project, what we initially thought was noise in the data turned out to be new product configurations, which helped us understand how the system may stabilize over time.\n5. Strive to create traceability, accountability and buy-in\nThe fifth and final lesson? It’s about trust. AI is wonderful, a life-long passion of mine, but it’s clear that not everyone trusts AI. And they are right to ask these important questions. When it goes wrong, who will be accountable? Can we place mechanisms to make it transparent? These are big questions without straightforward answers, at least for now.\nIn our research, we worked with an aerospace company to create a self-organizing system using what we call ‘software agents’ (essentially what drives Alexa and Siri) to automate spare parts procurement. The system would take data from sensors, analyse them to understand what part is expiring or deteriorating by when, then find the best supplier to schedule aircraft maintenance depending on when and where it is flying. This is a complicated problem, and the analytics could provide optimal solutions. It could even negotiate with suppliers and run auctions.\nBut the questions asked by the real people involved were pertinent: Exactly how do you arrive at these solutions? What if our people want to negotiate with suppliers themselves? Are you automating me? So due to the valid questions raised about trust, the solution that had been developed was patented but will take a lot longer to be implemented.\nThe key lesson perhaps is that we need more research to build transparency of algorithms and understand how and when they should be used.\nWe are currently working with several companies to understand how these concepts can be applied to a variety of sectors. Please contact Alexandra Brintrup: [email@example.com] to find out more.","As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:235d8953-209d-4045-92bc-e0cfb8b79d7d>","<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>"],"error":null}
{"question":"What safety features have manufacturers incorporated into modern salt spreaders to prevent maintenance-related accidents?","answer":"Manufacturers have implemented several safety features in spreaders to prevent maintenance-related accidents. These include an auto-reverse feature on the material feed system to automatically clear material jams without manual intervention, electronic controllers that detect when the spinner is removed and prevent the spreader from starting until it's reinstalled, and protective shields like the top screen that keeps oversized material out while preventing anyone from entering the unit. These features are designed to protect operators from coming into contact with moving parts like the auger and to prevent injury during maintenance operations.","context":["The job of snow and ice management professionals is to create safe driving and walking conditions for others. However, in the process it can be easy for them to slip up on their own safety precautions. Take salt and sand spreaders, for instance. So much focus is placed on material types and application rates that people sometimes overlook the proper techniques for installing, operating and maintaining the equipment safely.\nA wide range of outcomes can result from not following the safety recommendations provided by the manufacturer. Oftentimes, no damage is done by handling spreaders incorrectly. But, on the other hand, the consequences can get much worse, such as personal injury, property damage, costly downtime and equipment repairs. To help prevent any of these setbacks, here are several helpful hints for staying safe while spreading.\nMaterial Spreader Installation\nThe first rule of spreader safety is to ensure the equipment is installed securely. Of course, manufacturer recommendations may vary slightly, so it’s important to always read the owner’s manual for specific instructions. Nonetheless, there are a few tips that are consistent among various makes and models of V-box spreaders.\nA key step is to bolt the spreader to the truck chassis using the mounting holes provided in the spreader frame. This is necessary to make sure the spreader stays firmly in place.\nNext, the operator should run ratchet straps from the front corners of the spreader to the rear tie-downs of the truck. Then, he should run ratchet straps from the rear corners of the spreader to the front tie-downs of the truck. These straps provide extra security in case the mounting bolts fail.\nThe final step is installing the stop brackets provided with the spreader. These brackets go in-between the spreader and the truck cab, and they help prevent the spreader from sliding forward and hitting the cab in the event of a quick stop.\nUnfortunately, many people assume that ratchet straps are good enough for spreader installation, and they skip the more difficult steps of bolting the spreader to the frame and installing stop brackets. However, when only the straps are used, the potential for an accident increases greatly and, in fact, some operators have had their spreaders fall off the back of the truck as a result.\nLoading A Deicing Spreader\nAfter a spreader is securely installed in the truck bed, it’s important to pay attention to how the spreader is loaded with de-icing material. Level the load from front to back to distribute the weight evenly. Some spreaders offer a cab-forward design to help shift the weight properly, but this feature isn’t available from all manufacturers.\nMost snow and ice professionals want to load the spreader as full as possible — oftentimes past the brim — in order to reduce the number of times they need to refill the hopper. Although this may seem like the most efficient way to work, it often surpasses the payload capacity of the truck.\nExceeding the truck’s gross vehicle weight rating (GVWR) causes multiple issues. Not only can it produce unnecessary wear and tear on the vehicle, but it also creates unsafe driving conditions. The tires, suspension, brakes and other vital truck components aren’t designed for overweight operation, so they may not function properly under a heavy load. This may cause an accident, blown tire or other expensive breakdown. Additionally, being overweight may result in a costly DOT fine, which can quickly wipe away the profits made from a snow event.\nFinding the GVWR of a vehicle is simple. Just look for a label in the doorframe, under the hood of the truck, or in the owner’s manual. After tracking down this number, keep in mind that the weight of the vehicle, accessories, fuel and passengers must be subtracted from the GVWR before determining how much extra weight the truck can carry.\nFurthermore, after the hopper is filled, be sure to cover the load with a tarp. This feature increases safety by keeping material from flying out of the hopper. Not to mention, it’s the law in many states.\nEven if the hopper isn’t overloaded with salt or sand, it still affects how the truck will handle. Take a one-cubic-yard spreader, for instance. At full capacity it can weigh more than 1.5 tons. This amount of weight significantly increases the braking distance of the truck, so the driver must allow extra time to stop. Furthermore, the driver must also make slower turns, since the truck carries more forward momentum when cornering. If an inexperienced operator isn’t made aware of these factors before getting behind the wheel, he may learn these lessons the hard way.\nDuring operation, spreader safety is fairly straightforward. The number one rule is to not turn the spreader on while pedestrians are in the area. Of course, it can be difficult to spot all pedestrians from the truck’s cab, since V-box spreaders block the view out the rear-facing window. Therefore, the operator must learn how to efficiently use the side-view mirrors, and he should also turn on the spreader’s work light to provide extra visibility in dark and stormy conditions.\nMaintaining Your Ice Control Equipment\nThe next set of safety tips applies to servicing spreaders. In a perfect world, all maintenance would be performed during an off day in the shop, but that isn’t always feasible. Sometimes a spreader needs to be fixed at the worst possible time — during a storm — when snow and ice professionals are rushing to get back on the job. But no matter what day or time maintenance must be performed, it’s important to have patience, use common sense and not cut corners in order to correct issues.\nSalt & Sand Spreaders Manufactured for Safety\nFortunately, spreader manufacturers have tried to prevent maintenance-related accidents by incorporating a variety of safety features into their designs. For instance, some spreaders have an auto-reverse feature on the material feed system to automatically clear material jams, so the operator doesn’t have to unclog the unit by hand.\nIf, however, the material feed system does require servicing, the operator typically has to remove the spinner assembly to correct the issue. But, the electronic controller on some spreaders can detect when the spinner is removed, and it won’t allow the spreader to start back up until the spinner is re-installed. This helps prevent the operator from coming into contact with moving parts of the spreader, such as the auger. Manufacturers have also designed various protective shields in an attempt to keep operators free from harm. An example of this is the top screen. Not only does it keep oversized material out of the hopper, but it also acts as a safety feature to help prevent anyone from entering the unit.\nAlthough the top screen and other protective shields are bolted to the spreader, manufacturers can’t stop operators from removing them. Consequently, people may take out some bolts or remove a shield completely in an attempt to provide easy access to the hopper or specific components. However, this is never a good idea, because all shields are installed for a purpose — to protect both the spreader and operator from injury and damage. If a shield must be removed to clean out salt around the motor, transmission or other components, the spreader must be completely shut off before doing so, and the shields should be reinstalled immediately according to manufacturer recommendations.\nGiven the potential for personal injury or machine damage, intensive repairs should be reserved for trained service technicians. Therefore, if an inexperienced operator runs into a problem in the field and the fix isn’t obvious, he should call into the office for instructions on how to proceed. If nobody at the shop is familiar with how to correct the problem, then it is time to call the local servicing dealer or the spreader manufacturer itself.\nSimply put, reading the owner’s manual and using a little common sense will go a long ways. And, in the end, following safe practices is in everybody’s best interest. Not only does it help protect the operator, but it also increases the safety of surrounding drivers and pedestrians…which is what snow and ice professionals are meant to do in the first place."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c49849b8-a2ac-4a9b-9069-2a4f9ac7e2ea>"],"error":null}
{"question":"What's the main difference between how stamp duty rates are determined for joint male-female owners versus how property completion dates are set?","answer":"Stamp duty rates for joint male-female owners are mathematically determined as an average of the individual rates (for example, 5% for joint ownership versus 6% for men and 4% for women in Delhi), while property completion dates are set through mutual agreement and compromise between the buyer and seller during the exchange of contracts process. The completion date becomes legally binding once agreed upon, whereas the stamp duty rate is predetermined based on the ownership structure and location.","context":["Buying your first property can be a daunting experience, from arranging viewings, to sorting a mortgage, making an offer and using a solicitor, there are lots of stages to go through. We've put together a simple guide below to help you on this journey.\nThe first thing you need to do is decide how much you can afford. You will need to look at how much money you have available yourself and how much you can borrow. There are a number of different financial institutions, which offer loans to people buying a property, for example, building societies and banks. At nest we have an adviser who can help guide you through the process, offering a certificate of a mortgage in principle to help you get your offer accepted\nBefore finally deciding how much to spend on a property, you need to be sure you will have enough money to pay for all the additional costs. These may include:-\n• survey fees\n• valuation fees\n• Stamp Duty Land Tax\n• land registry fee\n• local authority searches\n• fees, if any, charged by the mortgage lender or someone who arranges the mortgage, for example, a mortgage broker\n• removal expenses\nany final bills, for example, gas and electricity, from your present home which will have to be paid when you move.\nMaking An Offer\nOnce you have decided on the property, we will guide you through the process. We will need your financial adviser details or if you are a cash buyer, we need proof of finance.\nOnce an offer has been agreed, we will need your solicitors details. nest have solicitors we use on a daily basis who we have a strong working relationship with, who like us are committed to getting your offer to go through completion.\nThe Legal Bit\nYour sale becomes legally binding on both sides when contracts have been exchanged through your solicitor. On exchange a completion date is set by mutual agreement, sometimes with a bit of compromise. This is also legally binding.\nAlthough it is impossible to give a precise idea of how long the legal work involved in buying a property, our job is to ensure this is pushed forward in a timescale that all parties involved agree with.\nNext you will instruct your conveyancer to draw up a contract which will eventually be signed by you and the seller. However, before the contract can be signed, your solicitor or licensed conveyancer must make sure that there are no problems with the ownership of the property, rights of way, access, or future developments in the area that might affect the property. This is called ‘making enquiries and searches’. The solicitor or licensed conveyancer makes the enquiries and searches as follows:-\n• local searches. These are enquiries made to the local authority (or in Northern Ireland, the appropriate government department) about any matters which affect the property which involve the local authority, such as whether there is a compulsory purchase order on the property. Local searches also include questions about any proposed changes or development in the area that might affect the property such as roads, housing, shops. During the local search, the local Land Charges Register (Registry of Deeds in Northern Ireland) is also checked. This gives information about any matter which affects the property such as tree preservation orders, if it is a listed building or in a conservation area; and\n• enquiries made to the seller by the solicitor or, in England and Wales, a licensed conveyancer. These are a set of standard questions about the property, boundaries, neighbour disputes and fixtures and fittings that will remain in the property. There may also be additional questions that the solicitor or licensed conveyancer thinks are necessary, such as the transferability of guarantees for any work done on the house, for example, a damp proof course; and\n• from the Land Registry.\nArranging to pay the deposit\nWhilst the solicitor or, in England and Wales, a licensed conveyancer is making the enquiries, you should sort out how you will pay the deposit that has to be made when the contracts are exchanged. This deposit is often 10% of the price of the home but it can vary.\nIf you are also selling a house, it is usually possible to put the deposit on the property being sold towards the deposit on the property you are buying.\nInsuring the property\nYou should make sure that buildings insurance is arranged from the date of exchange, because once contracts have been exchanged you are responsible for the property.\nExchange of contracts\nThe final contract between you and the seller is prepared when:-\n• the solicitor (or licensed conveyancer) and you are satisfied with the final outcome of all the enquiries\n• any surveyor’s report has been received and any necessary action taken\n• the formal mortgage offer has been received\n• arrangements about the payment of the 10% deposit have been made\n• the date of completion has been agreed.\nYou and the seller each have a copy of the final contract which you must sign. These signed contracts are then exchanged. At exchange of contracts both you and the seller are legally bound by the contract and the sale of the house has to go ahead. If you drop out, you are likely to lose your deposit.\nYou should make arrangements for the supply of gas, electricity and telephone service and make sure that the seller is arranging for final meter readings to be made.\nNest will get a call to allow us to release keys. This is the moment you have been waiting for. The keys are released from the bottom of the chain upwards so a bit of patience and compromise is needed.\n• the mortgage lender releases the money\n• the deeds to the property are handed over to your solicitor or licensed conveyancer\n• the seller must hand over the keys and leave the property by an agreed time.","What Is Stamp Duty And Registration Charges?\nThe ache for home lives in all of us, the safe place where we can go as we are and not be questioned. Buying a house is not only a financial but an emotional investment as well and it is most probably one of the biggest financial decisions you as an individual can make in your entire life and once you decide to buy a house, you have to work on lot of factors like identifying the property, working on the down payment, applying for the loan, signing the sale agreement etc and one of the most important and final step while buying a house is the possession and registration of your property which means the physical transfer of the property in your name.\nAfter you have bought a property the seller has to give you a document stating that the mentioned property is being transferred to you and you have to register your name in the local municipal records against the property in picture and the tax which you pay on property transactions is called stamp duty. Normally stamp duty varies from state to state, value of property and type of property if it is old or new.\nStamp duty is payable under Section 3 of the Indian Stamp Act 1899 and is similar to income tax i.e. stamp duty must be paid in full and on time and can attract penalty if not paid well in time. Stamp duty is a proper legal document and is considered as evidence in courts, provided it is properly stamped. Section 80C covers stamp duty and registration charges and the maximum deduction one can claim is Rs 1.5 lakhs and if your expenses have gone beyond the given threshold amount, the extra amount will not be eligible for deductions under this section.\nStamp duty and registration charges are considered to be one of the heftiest charges borne by an individual while buying a house and to get it transferred in your name wherein stamp duty could be as high as 8% of the property value and registration charge, also known as processing fee, could roughly be approx 1% of the property value so when combined, total of both becomes approx 8-10% of the property value which could be quite a hefty amount. However, that’s where Government steps in and has provided relief in respect of payment of stamp duty and registration charge and the amount and charges paid under stamp duty and registration charges can be claimed as deduction under Section 80C of the Income Tax Act 1961 under below mentioned conditions:\n- Only individuals and Hindu Undivided Family (HUF) can claim the deduction of Stamp Duty and Registration Charges.\n- You need to have the house possession for a time period of at least 5 years from the purchase or registration date, whichever is later. You need to make sure that you claim the deduction before you start living in the same because you won’t be able to claim the expenses if you have already occupied the house property either completely or partially. The house or the property has to be new.\n- Also, you cannot claim the deductions if the property is under construction.\n- You can claim the deductions only in the year of actual payment.\n- Deductions can be claimed only in the year of actual payment.\n- The property or the house must be in the name of the assessee and the expenses of stamp duty and registration charge must be borne by the same and not by any other person.\n- Payments done for commercial property is not applicable for deduction under this section.\n- If there is more than one owner of the property, then you will be able to claim the deductions under this section as joint owner up to Rs 1.5 lakhs each.\n- Any other expenses such as service tax paid for the transfer of the property can also be claimed as deduction under this section i.e. Section 80C.\n- Stamp duty or the registration charge for which you are seeking the deduction has to be from previous financial year only i.e. in the FY 2016-17 you can claim deduction for the stamp duty and registration charge paid in the FY 2015-16.\nUnder Section 80C, following expenses are not eligible for deductions such as:\n- Admission fees\n- Initial deposit to become a shareholder\n- Cost involved in any addition or alteration or renovation or repair of the house which s carried out after the completion certificate has been issued.\nAdding Your Spouse As Co-Owner Of Property\nBuying a house together has many financial benefits other than the obvious emotional reasons which go hand in hand in making the decision. If experts have to be believed then it always makes more sense to add your spouse as co-owner of the property because it will not only help you in extending your loan eligibility but also helps in availing various tax benefits such as interest and loan repayment. Also, succession of such properties is much easier and smoother than the single owned properties.\nApart from the above mentioned benefits, one of the major benefit one gets from a co-owned property is of stamp duty and registration charges. So if you have made your wife as the first owner of the property, you have saved yourself lot of money towards paying stamp duty and registration charges.\nIn many states such as New Delhi, stamp duty charges for registration of a property is much lower for women than what it is for men. Stamp duty fees for a man is 6% for a man whereas it is 4% for a woman and thus if you have bought a house under co-ownership you will have to pay stamp duty of 5% only against what you have paid had you bought in your name i.e. 6%. States like Haryana which are constantly working towards women empowerment have special stamp duty fees for women from urban areas and rural areas which is 6% and 4% respectively against stamp duty of 8% and 6% for man from urban and rural areas respectively.\nAlso having a property jointly with your spouse has a positive effect on loan eligibility as well as on the loan amount because while giving the loan, the financial institutions considers various factors like your income, credit score, your payment record etc and in case you have low income or have a low credit score or payment record, involvement of your spouse as a loan applicant is nothing but the blessing because in this case your spouse income or credit score will be taken into consideration.\nFor example, if your yearly income of Rs 10 lakhs makes you eligible for a loan up to RS 50 lakhs, however if your spouse also earns Rs 10 lakhs per annum, your chances to get loan up to Rs 1 crore increases considerably. Besides having a positive effect on the loan eligibility and loan amount, having your wife as a co-applicant gets you a concessional interest rate at various financial institutions and it is always advisable for both the joint owners to have equal contribution while buying the property because it will help them in availing capital gain benefits.\nHow Is Stamp Duty And Registration Charges Paid?\nStamp duty and registration charges is not a due based deduction but a payment based deduction which means that you can claim the deductions if there is actual payment of these expenses and also there should be no due. Also, the stamp duty percentage depends on several factors such as:\nOne can pay the stamp duty and registration charges through three ways i.e. through non-judicial stamp paper, by using the e-stamping method or by franking method. If you are using non-judicial stamp paper method, complete details of the agreement is mentioned in non-judicial stamp paper and is registered within four months at the sub-registrar’s office duly signed by the executants. In case of franking method, the agreement is printed on plain paper and submitted to an authorized bank which processes the same through franking method. There is third option which is available in not all but few states where you can make the online payment of stamp duty and registration charges through RTGS/NEFT and the stamp duty certificate can be downloaded later for the registration process."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:da30186a-a553-422c-a2a8-065b83076856>","<urn:uuid:457a77ae-a2e9-4d52-90ec-4177ea13fc31>"],"error":null}
{"question":"What technical components make up GPS satellite signals, and what challenges exist in achieving optimal interoperability between different GNSS systems' time references?","answer":"GPS satellite signals consist of three main components: L-band carrier waves, ranging codes modulated on the carrier waves, and the Navigation Message. These signals are derived from highly stable atomic clocks generating a fundamental frequency of 10.23MHz. Regarding interoperability between different GNSS systems' time references, there are ongoing discussions about whether each system should cross-reference other systems' time (using GGTO-type messages) or compare itself to a common international GNSS ensemble time. The precision of these time offsets and their preferred method of transmission (satellite messages, Internet messages, or internal calculations) are key considerations for achieving optimal interoperability.","context":["It’s an interesting and perhaps unanswerable question, whether governments ever truly listen to the voice of the people, and act accordingly. That premise gets another test in a GNSS setting, in Hawaii next month. An April 26 full-day session of the International Committee on GNSS (ICG) Interoperability Workshop will give those who design and build GNSS receivers a forum to offer their best advice to signal providers — BeiDou, Galileo, GLONASS, GPS, QZSS, and IRNSS are mentioned, as well as unspecified others — about how to achieve optimum interoperability benefits for their customers. “ Providers who have not finally decided on new signals will greatly appreciate your recommendations,” according to session organizer Tom Stansell, acting under the auspices of the ICG Working Group on GNSS Compatibility and Interoperability (WG-A).\nThe questions that candidate speakers are asked to address (given further on in this column) read like a primer on modern signal design, and suggest the complexity of issues dealt with at this very high technical level. The depth and level of detail at which the session organizers seek input reveal — what? That such issues are truly not yet determined by U.S., Chinese, Russian, and European system operators? This is impossible to know, outside government circles, and could easily be doubted by cynical minds. Yet the organization of such a workshop hints that at least some, if not many, such delicate matters remain in flux and under discussion.\nThe organizers seek three speakers each in four main topic areas. They emphasize that they are trying to involve only those who design GNSS receivers, not users, service providers, or product integrators.\nHigh-Precision Code is for products with sub-decimeter accuracy that use wide area correction signals such as from OmniSTAR or StarFire.\nHigh-Precision Phase is for products with sub-cm accuracy that use terrestrial correction signals to resolve carrier phase ambiguities.\nMedium Precision is for products with sub-50 cm accuracy, which often are single-frequency receivers using local correction signals.\nConsumer Applications are for chipsets embedded in consumer products.\nIf you want to participate, in person, by Internet, or by recorded PowerPoint presentation, you can contact the workshop organizers via GPS World magazine, by emailing firstname.lastname@example.org. Please indicate the topic that best fits your presentation. If you are not selected to speak, you are welcome to submit a paper or presentation that will be given to each signal provider.\nSpecific questions that candidate speakers are asked to address include:\nSupported applications: What types of applications do your receivers (or receiver designs) support?\nIncrease in noise floor: Do you see a threat to GNSS receivers due to many more GNSS signals centered at 1575.42 MHz?\nWhether you see a threat or not, do you prefer all new CDMA signals at L1 to be centered at 1575.42 MHz, or have some of them elsewhere, e.g., at 1602 MHz?\nGiven that most GNSS providers plan to eventually transmit a modernized signal at 1575.42 MHz, what is your long-term perspective on whether you will continue to use C/A? Why and How?\nCDMA and FDMA: Once there are a large number of good CDMA signals, will there be continuing commercial interest in FDMA signals? Why or why not?\nCompatibility: Do you prefer signals in different L1 frequency bands for interference mitigation rather than at one center frequency for interoperability? Why?\nWhat to do about misbehaving signals: If a satellite’s signals do not meet quality standards, should they:\n- Be set unhealthy?\n- Transmit with a nonstandard code?\n- Transmit with reduced signal power (reduce interference)?\n- Be switched off?\n- What combination of the above?\nTo assure only good signals, should GNSS providers agree on minimum international signal quality standards and agree to provide only signals meeting the standard?\nE5a and E5b: Given that L5/E5a will be transmitted by most GNSS providers, do you intend to use the E5b signal? If so, for what purpose?\nFrequency steps: For your applications, are small satellite frequency steps (Δf) a problem?\nIf so, what interval between frequency steps and what Δf magnitude would be excessive?\nInteroperable use: Assuming signal quality is acceptable from every provider, would you limit the number signals used by provider or by other criteria? What criteria?\nIs having more signals inherently better or do you think there should be a limit?\nWill the marketplace force you to make use of every available signal?\nFor best interoperability, how important is a common center frequency? How important is a common signal spectrum?\nAnother common open-service signal: Will you provide tri-lane capability in the future? Why?\nIf so, do you prefer a common middle frequency or the combined use of L2 (1227.6), B3 (1268.52), and E6 (1278.75) if B3 and E6 open access is available?\nWould you prefer a common open signal in S Band? In C Band? Why?\nPrecision code measurements: Does a wider satellite transmitter bandwidth help with multipath mitigation?\nWhat minimum transmitter bandwidth would you recommend for future GNSS signals in order to achieve optimum code precision measurements?\nAdded GNSS or SBAS messages: Would you recommend GNSS or SBAS services provide interoperability parameters:\n- System clock offsets\n- Geodesy offsets\n- ARAIM parameters\nShould they be provided by other means so as not to compromise TTFF or other navigation capabilities?\nSignal coherence: For your applications and for each signal, what amount of drift between code and carrier over what time frame would be excessive?\nFor your applications and for two or more signals in different frequency bands, e.g., L1 and L5 (when scaled properly), what amount of relative drift in code and carrier between the signals would be excessive?\nSpectrum protection: Should the international community strive to protect all GNSS signal bands from terrestrial signal interference?\nSystem geodesy: Do the current differences (~10 cm) in geodesy pose a problem for your users? Why or why not?\nIf geodesy differences are a problem, what is the preferred method of compensation:\n- Published values (e.g., on websites)\n- Satellite messages\nSystem time: Do you want each system to cross-reference the other’s time (e.g., with a GGTO type of message) or compare itself to a common international GNSS ensemble time? To what precision?\nWill your future receivers calculate a time offset between systems based on signal measurements or use only external time offset data?\nWhat is the preferred method of receiving time offsets: Satellite messages, Internet messages, or internally calculated?\nFurther information and background on the April 26 session can be downloaded at http://www.mediafire.com/?cegvqb9l8ya1c.\nThe timed agenda for the April 26 meeting follows, showing both Hawaii Standard Time (HST) and Coordinated Universal Time (UTC), provided because some presenters will speak from remote locations using GoToMeeting over the Internet. Another option for presenters is to provide a PowerPoint file with embedded audio.\nHST/UTC Topic Presenter\n9:00/19:00 Welcome and Introduction Working Group Co-Chairs\n9:25/19:25 Welcome and Introduction Xiaochun Lu\n9:35/19:35 Framing the Presentations Tom Stansell\n9:50/19:50 Certified Avionics TBD\n10:15/20:15 High Precision Code #1 TBD\n10:5520:55 High Precision Code #2 TBD\n11:20/21:20 High Precision Code #3 TBD\n11:45/21:45 High Precision Phase #1 TBD\n12:10/22:10 High Precision Phase #2 TBD\n13:35/23:35 High Precision Phase #3 TBD\n14:00/0:00 Medium Precision (~GIS) #1 TBD\n14:25/0:25 Medium Precision (~GIS) #2 TBD\n14:50/0:50 Medium Precision (~GIS) #3 TBD\n15:30/1:30 Consumer Applications #1 TBD\n15:55/1:55 Consumer Applications #2 TBD\n16:20/2:20 Consumer Applications #3 TBD\n16:45/2:45 Summary Tom Stansell\n16:55/2:55 Summary Xiaochun Lu\n17:05/3:05 Conclusion Working Group Co-Chairs","GPS SATELLITE SIGNAL STRUCTURE\nEach GPS satellite transmits a unique navigational signal centred on two L-band frequencies of the electromagnetic spectrum: L1 at 1575.42MHz and L2 at 1227.60MHz. At these microwave frequencies the signals are highly directional and hence are easily blocked, as well as reflected, by solid objects and water surfaces. However, clouds are easily penetrated, but the signals can be blocked by dense or wet foliage. The satellite signals basically consists of (see Figure 1 below):\nThe two L-band carrier waves.\nThe ranging codes modulated on the carrier waves.\nThe Navigation Message.\nFigure 1. GPS satellite signal components.\nAs the name implies, the carrier waves provide the means by which the ranging codes and Navigation Message is transmitted to earth (and hence to the user). The primary function of the ranging codes is to permit the signal transit time (from satellite to receiver) to be determined. (This quantity is also sometimes referred to in the navigation literature as the time-of-arrival -- TOA.) The transit time when multiplied by the speed of electromagnetic radiation (= 299792458m/s in a vacuum) gives the receiver-satellite range. The Navigation Message is modulated on both carrier frequencies and contains the satellite ephemeris, satellite clock parameters, and other pertinent information such as general system status messages and an ionospheric delay model, necessary for real-time navigation to be performed. Each of these signal components are described below.\nAll signal components are derived from the output of a highly stable atomic clock (Figure 2 below). In the operational (Block II/IIA) GPS system each satellite is equipped with two cesium and two rubidium atomic clocks. (The Block IIF satellites may be equipped with a space-qualified hydrogen maser.) The clocks generate a pure sine wave at a frequency f0 = 10.23MHz, with a stability of the order of 1 part in 1013 over one day. This is referred to as the fundamental frequency.\nFigure 2. GPS signal component frequencies.\nMultiplying the fundamental frequency f0 by integer factors\nyields the two microwave L-band carrier waves L1 and L2 respectively (above\ntwo figures). The frequency of the two waves are obtained as follows:\nfL1 = f0 x 154 = 1575.42MHz\nequivalent wavelength: L1 = c / fL1 19cm\nfL2 = f0 x 120 = 1227.60MHz\nequivalent wavelength: L2 = c / fL2 24cm\nThese are righthand circularly polarised radio frequency waves capable of transmission through the atmosphere over great distances, but they contain no information. All satellites broadcast the same frequencies (though the received frequencies are slightly different because of the Doppler shift). In order to give the carriers information they must be modified, or modulated, in some way. In the Global Positioning System there are two distinct codes used to modulate the L-band carriers, namely the ranging codes and the Navigation Message.\nThe L1 carrier was designed to be modulated with two codes, one intended for civilian use and the other reserved for the military, whereas the L2 carrier is modulated only with the military code. Both carriers also contain the Navigation Message.\nTwo ranging codes are used:\nThe C/A code, the \"clear/access\" or \"coarse/acquisition\" code (sometimes also referred to as the \"S code\").\nThe P code, the \"private\" or \"precise\" code, which under Anti-Spoofing (AS) is replaced by the \"Y\" code.\nThe C/A and P (or Y) codes can be considered as the measuring rods -- they provide the means by which a GPS receiver can measure one-way distances to the satellites. Both codes have the characteristics of random noise, but are in fact binary codes generated by mathematical algorithms and are therefore referred to as \"pseudo-random-noise\" ( or PRN) codes.\nFigure 3 below illustrates the C/A code generation procedure based on \"Gold Codes\". Tapped Feedback Shift Registers are used to generate a sequence of \"0\"s and \"1\"s at the clock rate of 1.023 MHz. At each clock pulse the bits in the registers are shifted to the right where the contents of the rightmost register is read as output. A new value in the leftmost register is created by the modulo-2 addition (or binary sum) of the contents of a specified group of registers. In the case of the C/A code two 10-bit TFSRs are used, each generating a Gold Code: (1) the G1 (represented here as the polynomial: 1 + X3 + X10), and (2) the G2 (represented here as the polynomial: 1 + X2 + X3 + X6 + X8 + X9 + X10). The output of the G1 TFSR (rightmost register) is modulo-2 added to the register contents of the G2. Different combinations of the outputs of the registers of G2 (or \"taps\" from the register) when added to the output of the G1 code lead to different PRN codes. There are 36 unique codes that can be generated in such a straightforward manner. Figure 3 below also shows the first three PRN taps: PRN1 taps the contents of register 2 and 6, and adds it to the output of the G1 TFSR, PRN2 taps the contents of register 3 and 7, PRN3 taps the contents of register 4 and 8, and so on.\nFigure 3. Generating PRN codes using two Gold Code registers.\nTo measure one-way range, a knowledge of the codes is required by the GPS receiver's computer. Hence, knowing which PRN code is being transmitted by a satellite means that a receiver can generate a local replica of the same code sequence. These PRN codes possess a very important attribute: a given C/A (or P or Y) code will correlate with an exact replica of itself only when the two codes are aligned. Furthermore, without knowledge of the ranging code sequence, the Navigation Message cannot be recovered.\nThe C/A codes are 1023 \"chip\" long binary sequences, which are generated at a rate of 1.023 million chips per second, that is at a frequency of 1.023MHz (see Figure 2 above). Hence the entire C/A code sequence repeats every millisecond. The \"wavelength\" of the code (length of the chip) is approximately 300m, and the total sequence is therefore about 300km long. Each GPS satellite is assigned a unique C/A code (see Table -- section 2.2.2).\nThe P code is a far more complex binary sequence, being approximately 266.4 days long with a chipping rate at the fundamental frequency f0 = 10.23MHz. It is generated in an analogous manner to the C/A code, using two TFSRs. The \"wavelength\" of this code (length of the P code chip) is approximately 30m, ten times the resolution of the C/A code (Figure 4 below). Instead of assigning each satellite a unique code of its own, as is the case with the C/A code, the P code is allocated such that each satellite transmits a one week portion of the 266.4 day long sequence (restarting on Saturday midnight.\nFigure 4. Examples of the C/A and P code chip sequences.\nUnder Anti-Spoofing the P code is encrypted through the modulation of a further secret code -- the \"W code\". The sum, referred to as the \"Y code\", is then modulated in the normal way onto the L1 and L2 carrier waves. The same P (or Y) code is modulated on both carrier waves, and any difference in signal transit time of the same PRN sequence is due to the retardation of the two L-band signals by a different amount as they travel through the ionosphere. (The effect of the ionosphere on signal propagation is essentially a function of signal frequency) The effect of the ionosphere is to retard the PRN sequence, and to advance the carrier phase. An approximate ionospheric delay model is provided within the Navigation Message.\nIn order for a GPS navigator to derive real-time position (and to make the task of the GPS surveyor easier when he comes to reduce his data), a Navigation Message is transmitted on both L-band frequencies, containing the following information (section 3.3.1):\nPredicted satellite ephemerides.\nPredicted satellite clock correction model coefficients.\nGPS system status information.\nThe GPS system ionospheric model.\nThe Control Segment (via the Upload Stations) uplinks this information into each satellite for subsequent transmission to all users on a regular (nominally daily) basis. The satellite message is in a binary form, like the ranging codes, but the sequence is not random. The message is transmitted at a rate of one bit (\"0\" or \"1\", as in a computer) every 20 repetitions of the C/A code. This corresponds to a rate of 50bps (bits per second). The entire message length is 1500 bits.\nWHY IS THE SIGNAL SO COMPLICATED?\nHIGH ACCURACY POSITIONING\nMILITARY AND CIVILIAN USERS\nBack To Chapter 3 Contents / Next Topic\n© Chris Rizos, SNAP-UNSW, 1999"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:000ae8e6-0750-4ef8-9bf7-71af446c6f32>","<urn:uuid:d14fe64f-131d-48af-8d98-1df851237a26>"],"error":null}
{"question":"How do the timelines differ between an uncontested divorce with an attorney and a litigated divorce case?","answer":"An uncontested divorce with an attorney can be finalized in 4-6 weeks, while a litigated divorce case is much longer, typically taking 8-12 months to complete. A litigated case involves multiple phases including filing the petition, temporary arrangements (1-2 months), discovery phase (5-7 months), settlement phase (1-2 months), trial preparation (1-2 months), and potentially a trial phase lasting 1-2 days to 2 weeks. In contrast, an uncontested divorce is simpler, involving mainly the settlement agreement, parenting plan (if children are involved), and court processing.","context":["HOW TO GET A DIVORCE\nOPTION 1 – DIY Divorce\nHow to get a divorce and the correct documents. For instance you will have to go to the court that would have jurisdiction, you as the plaintiff are ordinarily resident.\nThis means the court in which area you reside. If you entered into a settlement agreement, you merely ask for a decree of Divorce, Therefore, incorporating the settlement agreement.\nYou are entitled:\n- If you can prove to a court that you and your spouse can no longer live together and there is no chance of resolving your differences.\n- If one of the spouses is mentally ill or continuously unconscious\nYou do not have to get your spouse’s permission to get a divorce. If your spouse is not willing, you can get a divorce granted without his or her consent.\nIn special circumstances you may get your marriage annulled. An annulment differs from a divorce in that it not only dissolves the marriage but also wipes it off the record.\nLegal separation does not exist in South Africa even if you are no longer living with your husband and not divorced. According to the law, you are still married.\nYou can proceed in either the Regional Court of the Magistrate Court having jurisdiction in your area or in the High Court.\nTo start the process you need to have a summons served. A divorce summons must be served personally on the defendant by the sheriff of the court.\nThere are two types.\nThe uncontested divorce is the best and most cost effective for all parties concerned. It can be finalised within 4 weeks. If a divorce is contested it may take between 2 – 3 years, but most contested divorces do settle long before they go on trial.\nA civil marriage and customary marriage need to be dissolved by a court.\nNote: A default divorce is similar to an uncontested/unopposed divorce. This works when your spouse does not respond at all to the divorce.\n- Your spouse will receive a summons with a date to respond.\n- If they do not respond, you can apply at the High Court, to add it to the roll.\n- The court will decide on your behalf and end your marriage.\nThere is two options available for you:\n- You can use the service of an attorney, he or she will draft all the court documentation for you and will do all the administration for you.\n- DIY divorce get all the court documentation and do all the administration yourself.\nDo-it-yourself divorces are concluded without the help of an attorney, and are thus far cheaper option. Divorcing without an attorney can be achieved in two ways:\n- Your local magistrate’s court can provide you with the necessary forms and give you guidance on how to conclude your own divorce without legal representation.\nDoing your own divorce might be an option when your divorce:\n- is uncontested;\n- is not complicated;\n- you have been married for a short period of time;\n- you don’t have substantial assets to divide;\n- there are no disputes regarding any children; and/or\n- you are prepared to do all the admin yourself.\nProceed with DIY divorce\nYour Divorce does not have to be an expensive process.\nOPTION 2 – Attorney Assisted Divorce\nEasy process to follow. Therefore we wil do all the work for you. Hence, you only have to appear in court.\nAn uncontested dovorce can be finalized in four to six weeks depending on the court role. Therefore, it is not necessary for both parties to appear together in court.\nThe process of an uncontested divorce is relatively simple. As already mentioned, the parties usually enter into a settlement agreement and parenting plan (when children are involved) prior to the divorce. Once the settlement agreement and parenting plan are signed by both parties and witnesses, the divorce process can commence.\nUsually, the settlement agreement and parenting plan will be attached to a summons and a particulars-of-claim document. The plaintiff then issues the summons and annexures at court.\nThe court registrar will open a file, stamp the documents and allocate a case number. The documents will then be handed back to the plaintiff and the plaintiff will deliver two sets of these documents to the sheriff in the area where the defendant resides or work. The sheriff will then serve the documents personally on the defendant and issue a return of service proving that the documents were served.\nAttorney Assisted Divorce\nEasy online form to complete, we will do all the work for you. Divorce can be completed in 6 - 8 weeks.\nOPTION 3 – International Divorce\nEdictal Citation is used. Where the Defendant lives in another country. Therefore the Plaintiff must use a process called Edictal Citation. Hence, the Plaintiff must first approach the High Court or the Regional court by the way of an Edictal Citation Application.\nThis affords permission to a Plaintiff to serve the Divorce documents on a spouse in a foreign country. Therefore, personal service is required.\nFirstly, it is important that the Court will require that the Plaintiff is domiciled in the Court’s jurisdictional area on the date that your Divorce Proceedings commence.\nSecondly that you are usually resident in that area.\nFurther that you have been living there for not less than a period on one year immediately prior to that date.\nA Summons must be served on the Defendant in person. Therefore, the Court needs to be satisfied that service will be done properly. Above all, by an official of the Court in a foreign country.\nYou can download form and forward to us or you can complete form online to proceed with International Divorce.","The Divorce Timeline below is a general summary based on King County Family Law Case Schedules for a litigation case. If you have a case pending in any county you must check your own Case Schedule and court rules.\nGeneral Phases of the Litigation Process\n1) Filing of Petition for Dissolution of Marriage\nRestraining Orders may be sought along with the filing of the Petition for Dissolution. This event causes the Timeline to begin.\n2) Temporary Arrangements/ Temporary Orders phase.\nEither by agreement or through a court hearing your finances must be organized throughout the divorce process so that everyone can live and pay their bills, including attorney’s fees and the residences of the children must be organized. If a court hearing is required the person who files the Motion for Temporary Orders must give the other person 14 days’ notice. There must be a response filed by the responding party and a reply filed by the person who initiated the hearing. This phase of organizing how finances and the children will be handled during the divorce process can take 1 to 2 months from the date the Petition for Dissolution is filed.\n3) Discovery phase.\nDuring this phase of the process you are developing your case and finding out information from your spouse or partner such as their earnings history, list of assets and liabilities and values of each, monthly expenses. Financial experts are hired and do their work during this phase. Many issues to do with parenting after the divorce are explored during this time if parenting is not agreed upon. Mental health evaluations, court parenting experts and others are involved during this time if necessary. This phase takes 5-7 months.\n4) Settlement phase.\nAfter you have sufficient information to come to an agreement you analyze all of the information, prepare mediation materials and attend mediation to try to resolve the financial and parenting issues. In a family law litigation case this is usually a one day, 8 hour mediation. This is not the same as using mediation throughout your divorce to resolve the issues. See our page The Four Kinds of Divorce for a description of using mediation throughout the process, not just a one day mediation. This phase takes 1-2 months in litigation cases.\n5) Trial Preparation phase.\nIf you have not settled your case at mediation you must organize the information you are going to present to the judge, organize all documents you collected during the discovery phase and prepare your testimony and the testimony of other witnesses who will come to the trial. This phase takes 1-2 months.\n6) Trial phase.\nOn the date set forth on your Case Schedule you will appear at court prepared to address the judge and explain to the judge what you want the judge to order and why. Trials generally last 1 or 2 days to 2 weeks, if there are complex financial and parenting issues.\nForms and Court Deadlines After Filing the Petition for Dissolution\nA divorce is started by filing a Petition for Dissolution of Marriage. There are other forms that must be filed at the same time. When you file a Petition for Dissolution of Marriage you will get a Case Schedule that gives you all of the forms that have to be filed and deadlines you must meet.\nThe other party must be notified that you have filed the Petition for Dissolution. This is done by having a third person hand the papers to him or her, called “serving” the other person. Or, you can give the papers to your spouse or partner and they can sign an Acceptance of Service which is filed with the court letting the court know that the person has received the paperwork.\nThe other party must file a Response within 20 days of being served or signing an Acceptance of Service. In the Response the person tells the court if they agree with the statements in the Petition for Dissolution and they ask for whatever relief they want the court to provide them.\nAll parents who have children under the age of 18 must attend a Parenting Seminar within 90 days after the Petition for Dissolution is filed.\nA form called Confirmation of Issues must be filed around 90 days after the Petition for Dissolution is filed. This informs the court that the case is on track procedurally and there are no problems with necessary court forms. This form also informs the court if there is pending mediation and if there are special issues involved with the children. If a Confirmation of Issues is not filed then the parties must appear before the judge at a Status Conference to discuss the matters above.\nBeginning about 8 months after the Petition for Dissolution is filed you must begin to disclose the names of all witnesses you intend to call at trial and what they will testify about.\nIf you need to change your trial date there is a deadline for doing that which is about 2 months before your trial date.\nYou can gather information from the other person (Discovery phase above) until 6 weeks before the trial date. After that, the Discovery Cutoff Date, you cannot formally ask the other person or third parties to provide information to you.\nYou must engage in mediation one month before your trial date.\nIf you have not settled your case informally or in mediation then you have to appear before the trial judge, testify about your case and bring witnesses to testify about important financial and parenting issues. This is the trial that finalizes all issues in your divorce and ends your marriage.\nMeditation, Cooperative and Collaborative Law Case Timelines\nIn cases using mediation, cooperative law and collaborative law the parties go through most of the phases described above but in a much less formal manner and sometimes in reverse order to what is shown above. For example, often the Petition for Dissolution of Marriage is not filed until after the parties have reached agreement on most issues.\nTo resolve any case, information must be collected. Information forms the basis for agreements on finances and parenting issues. To resolve financial issues you must have income information for both spouses, what each person’s monthly living expenses will be, what assets exist to divide and what the debts are. In the family law litigation process this is done in a formal way through what is called “discovery”. In litigation cases this is done through a series of legal procedures and documents prepared by attorneys. In cases using mediation, cooperative law or collaborative law this information is exchanged voluntarily and informally in meetings with the mediator or attorneys.\nTo resolve issues concerning parenting of children sometimes evaluations by trained mental health professionals is helpful. In litigation cases, again, this is a formal process and the recommendations of a Parenting Evaluator or Guardian ad Litem are often very persuasive to the court if the court is making a decision about parenting issues. In cases using mediation, cooperative law or collaborative law input of a mental health professional is informal and not usually written. The professional often comes to meetings to offer suggestions for resolving parenting issues.\nAfter all of the information is gathered, people using mediation, cooperative law and collaborative law work to reach agreements that satisfy both people’s needs. No agreement is perfect but the agreement needs to be acceptable to both people. It is not always possible to reach agreements through mediation, cooperative law or collaborative law processes. When agreement is not possible then parties may try other informal processes or they may decide the only way they can resolve the issues is through litigation.\nConsult with a professional and organized Seattle divorce attorney at Integrative Family Law\nDivorce proceedings can seem overwhelming, but having an experienced, professional, and organized divorce lawyer on your side will help. Call us at 206.859.6800.\nNothing on this page is intended as legal advice for an individual.\nCall us now to learn more about how we can assist in your specific situation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:275e46c1-b9a9-4c07-bc61-f38c745e40e2>","<urn:uuid:4149fa85-05bc-4b70-9066-9229053e3ac0>"],"error":null}