{"question":"Between Pisgah Crater in Mojave Desert and western Grand Canyon's basalt flows, which volcanic formation is younger?","answer":"The basalt flows in western Grand Canyon are younger. The document shows that Pisgah Crater is approximately 22,000 years old (~22 Ka), while the Grand Canyon lava flows are dated between 725,000 and 75,000 years ago, with the youngest flows occurring between 150,000-75,000 years ago.","context":["November 12, 2018\nLet’s be real, we all love fieldwork. It feels great to take a break from the repetitive days of grinding in the office and go outside to get your hands dirty.\nIn mid-October I was fortunate to travel to the Mojave Desert and participate in the Mojave Broadband Seismic Experiment. The project seeks to understand how strain is distributed across the Eastern California Shear Zone and how the lithosphere evolved since the Laramide Orogeny. Led by PIs Thorsten Becker, Whitney Behr, and Vera Schulte-Pelkum, the experiment is comprised of 19 broadband seismometers deployed 2 km apart on a single 40 km-long line. The instruments were deployed in Spring 2018 and are set to record seismic events from all over the world for two years. Robert Porritt (UTIG Postdoctoral Fellow working with Thorsten) recruited Kelly Olsen (PhD ‘21), Simone Puel (PhD ‘22), and me (PhD ‘21) to help service the seismometers, which consists of downloading data, making sure everything is working properly, and installing protection to the GPS and sensor cables.\nOverview maps showing location of the seismic instruments in Mojave Desert with regional faults.\nOur basecamp was the California Inn in the thriving metropolis of Barstow, CA. The seismic line was located a 1-hour drive east of Barstow near a “town” called Ludlow, which only had a gas station, a Dairy Queen, a small diner, and a Motel 6. Rob described the motel in Ludlow as the “sketchiest thing I’ve ever seen”, so we were all pretty happy to exchange a slightly longer drive for tasty restaurants and comfort in Barstow. Every day we split into two teams with the goal of servicing three stations per team. We quickly learned this is easier said than done in the Mojave Desert! The area is extremely desolate – no roads, no trails, and no signs of life, anywhere. To get close to some stations we drove up dry river channels and through desert shrubs (sorry rental car company). Other stations were located on Mojave National Preserve, so we just had to pull over on I-40 and crawl under barbed wire fences.\nTypical Mojave Desert landscape view from instrument K.\nThe Mojave landscape is breathtaking. The red dirt and basalt colluvium made it feel like we were doing fieldwork on Mars. I am very thankful that it wasn’t summertime when temperatures are commonly 125°F. The geology was astonishing, and I was thrilled to see complex folds and volcanic flows, which was a nice break from the carbonates in Austin. Volcanic rocks dominate the desert – basalt, andesite, and rhyolite flows, lapilli, ignimbrites, and pumices. We also saw mountains of granite, schist, and gneiss, some volcanic breccias and arkose sandstones. Seeing cool rocks was a real treat for me and brought me back to my undergrad days as a geologist. Every desert shrub tried to kill us as we scrambled across this beautiful landscape; we all bled a small sacrifice to the Mojave.\nFolds in schists, gneiss, and volcanic lapilli? (top right).\nSweet contact between orthogneiss and biotite schist.\nAt each station, we used a Bluetooth app to connect with the instrument and receive information, such as its current status, temperature, GPS clock, and the amount of data recorded. We also used the app to make sure the three sensor components were properly working and responding to ground motion (yes, we jumped on the ground like little kids). While downloading data onto our laptop, we installed a blue “fire hose” type material around the sensor cable and then buried it for protection against inquisitive desert critters. Lastly, to secure the GPS and solar panel cables, we wrapped them with $0.25 foam pool noodles (Kelly’s brilliant idea). A quick check to make sure everything was working fine, and we were off to the next station.\n(Top left): A content seismometer. (Top right): Analog to digital converter stores data. (Bottom left): Rob digging up the seismometer to change the sensor cable. (Bottom right): Installing the blue fire hose to protect the sensor cable.\nBeautiful views at stations K (top left), S (bottom left), B (top right), and J (bottom right – check out the killer folds in the background!).\nAll in all, our mission was a success, collecting quality seismic data from 18 of 19 stations for analyses back in Austin. Two stations had a severed GPS cable, one had a chewed sensor cable, and one was dead from losing connection with its solar panels. We brought spare cables to replace the damaged ones, and the new fire hose and pool noodle protection should ensure the stations happily record seismic arrivals until the next service run.\nChilling on some coarse granite at station S (Budweiser Springs).\nAfter completing service on all 19 stations, we explored lava tubes at Pisgah Crater, a young (~22 Ka) cinder-cone volcano. I was surprised to find a road that allowed us to drive up to the rim of the crater! Watching the sunset on fresh basalt flows was a great way to end our time in the Mojave Desert. Before we flew back to Austin, we also spent a few hours relaxing at Santa Monica and Venice Beach. After hiking over 75 km through rough desert terrain for six days in tennis shoes, I was happy to dip my feet in the cold Pacific #SendItForScience.\nHanging out on top of Pisgah Crater. Photo credit: Simone.\nHook ‘Em Horns inside a young lava tube.\nRelaxing in Venice Beach to celebrate after fieldwork. Photo credit: Simone.\nThis experience taught me that sometimes the best fieldwork does not have to be associated with your personal research. Even though I may not get involved with the analyses of these data, it was an extremely rewarding experience to learn about broadband seismic instruments and the geology of the Mojave Desert region, and to grow relationships with friends and colleagues.","Highlights are provided below. Representatives of the media may obtain complimentary copies of articles by contacting Ann Cairns at email@example.com. Please discuss articles of interest with the authors before publishing stories on their work, and please make reference to GEOSPHERE in articles published. Contact Ann Cairns for additional information or other assistance.\nNon-media requests for articles may be directed to GSA Sales and Service, .\nLarge-magnitude Miocene extension of the Eocene Caetano caldera, Shoshone and Toiyabe Ranges, Nevada\nJoseph P. Colgan et al., U.S. Geological Survey, Menlo Park, California 94025, USA.\nKeywords: Basin and Range Province, Miocene, extension tectonics, calderas.\nThe late Eocene Caetano caldera formed about 34 million years ago during eruption of the Caetano Tuff (described in a companion paper by John and others in this issue of Geosphere). Remnants of the caldera are presently exposed in a series of north-trending, east-tilted (about 40 degrees), fault-bounded blocks that crop out across 40 km (east-west) of the Shoshone and Toiyabe Ranges in north-central Nevada. Restored geologic cross-sections indicate that the caldera was originally about 12-18 km (north-south) by 20 km (east-west) and has therefore been stretched to about twice its original width. The authors interpret Miocene sedimentary rocks exposed between these fault blocks to represent material shed from rising mountain ranges into adjacent basins while the faults were moving, indicating that deformation began about 16 million years ago and continued until 10-12 million years ago. These older basins and ranges were broken up by younger, locally active faults that formed the modern basins and ranges seen in north-central Nevada today. It is likely that Miocene faulting and tilting was not confined to the former caldera, but also affected surrounding Paleozoic rocks-and, potentially, large, nearby Carlin-type gold deposits.\nHistory of Quaternary volcanism and lava dams in western Grand Canyon based on lidar analysis, 40Ar/39Ar dating, and field studies: Implications for flow stratigraphy, timing of volcanic events, and lava dams\nRyan Crow et al., Department of Earth and Planetary Sciences, University of New Mexico, Albuquerque, New Mexico 87131, USA.\nKeywords: Grand Canyon region, Uinkaret, basalt flows, lava dams, volcanic history.\nJohn Wesley Powell wrote in 1895: “...what a conflict of water and fire there must have been [in western Grand Canyon]! Just imagine a river of molten rock running down over a river of melted snow.” Over 110 years later, a synthesis of new and existing dates on these lava flows shows that many are significantly younger than initially thought and all are less than 725 thousand years old. The geochronology data indicates four major episodes when lava flows either erupted into the canyon or flowed over the rim into it: 725-475 thousand years ago (ka), 400-275 ka, 225-150 ka, and 150-75 ka. These flows formed lava dams in western Grand Canyon that had dramatic impact on the Colorado River. This paper presents light detection and ranging (lidar) data to establish the elevations of the tops and bottoms of basalt flow remnants along the river corridor. These data show the original extent of now-dissected intra-canyon flows and aid in correlation of flow remnants. From 725 to 475 ka, volcanism built a high edifice within Grand Canyon in the area of the Toroweap fault, with dike-cored cinder cones on both rims and within the canyon itself. These large-volume eruptions helped drive the far-traveled basalt flows which flowed down-canyon over 120 km. A second episode of volcanism, from 400 to 275 ka, built a 215-m-high dam along the Hurricane fault, about 15 km downstream. The ca. 200 and 100 ka flows (previously mapped as Gray Ledge) were smaller flows and lava cascades that entered the canyon from the north rim between the Toroweap and Hurricane faults. The combined results suggest a new model for the spatial and temporal distribution of volcanism in Grand Canyon in which composite lava dams and edifices were generally leaky in proximal areas. Available data suggest that the demise of volcanic edifices may have involved either large outburst-flood events or normal fluvial deposition at times when the river was established on top of basalt flows. These data highlight complex interactions of volcanism and fluvial processes in this classic locality.\nReelfoot rift and its impact on Quaternary deformation in the central Mississippi River valley\nRyan Csontos et al., University of Memphis, Ground Water Institute 300 Engineering, Memphis, TN 38125, USA.\nKeywords: Reelfoot rift, Mississippi embayment, New Madrid seismic zone, Mississippi River alluvium, geomorphology.\nThis article presents research into the Reelfoot rift within the Mississippi embayment of the central United States. The project is timely and relevant to a wide audience in light of the seismic risk and increased interest in petroleum potential within the Mississippi embayment. The northern end of the rift near the town of New Madrid, Missouri, was the site of the great 1811-1812 New Madrid earthquakes, and it remains the most seismically active area east of the Rocky Mountains. This research utilizes existing and new data sets which better define the structure and stratigraphy within the Reelfoot rift and presents them as a 3-D model for interpretation and visualization. This allows the generation of a unique picture and new understanding of the central Mississippi River Valley. Quaternary reactivation of Precambrian basement faults is of wide international interest in developing a better understanding of rift systems and their earthquake threat. Within the Mississippi embayment it is important to understand the source of the current and historic seismicity. Delineation of basement structures within the Reelfoot rift is also of interest to the petroleum industry, because the authors have mapped basement blocks that may help define petroleum targets. This work substantially improves the interpretation of the geologic history, formation, and development of the Reelfoot rift and its impact on Quaternary deformation.\nAutomated extraction of data from text using an XML parser: An earth science example using fossil descriptions\nGordon B. Curry et al., Digital Geosciences Laboratory, Dept of Geographical and Earth Sciences, University of Glasgow, Gregory Building, Lilybank Gardens, Glasgow G12 8QQ, Scotland, UK.\nThis paper describes a method of automating the computerization (digitization) of important sections of earth science information that is currently only available in printed text. At the present time, full digitizing of printed information requires manual entry of data into a database, which is slow, unrewarding, and likely to introduce mistakes. This paper demonstrates a method of automatically digitizing descriptions of fossil species, which are an immense source of information on the history of life on the planet. These descriptions are written in a very regular way, to such an extent that they can be read by computers using new software (a parser) that creates markers, or tags, around segments of text. Once tagged in this way, the information can be analyzed much more thoroughly that was previously possible. The digitized information is also much more complete than was previously available, as the entire species description is tagged, including all the features of the fossil, its stratigraphic distribution, and geographic location.\nLidar mapping of faults in Houston, Texas, USA\nRichard M. Engelkemeir and Shuhab D. Khan, Department of Geosciences, University of Houston, Houston, Texas 77204-5007, USA.\nKeywords: lidar, Houston, faults, subsidence\nThis paper uses LIDAR for mapping active surface faults in Houston, Texas. These faults result in damages to houses, pipelines, roads and other constructions. Accurate mapping of their locations therefore aids in hazard mitigation.\nAsh-flow tuffs and paleovalleys in northeastern Nevada: Implications for Eocene paleogeography and extension in the Sevier hinterland, northern Great Basin\nChristopher D. Henry, Nevada Bureau of Mines and Geology, University of Nevada, Reno, Nevada 89557.\nKeywords: paleogeography, extension, Eocene, ash-flow tuff, Nevada\nThe distribution of distinctive volcanic and sedimentary rocks indicates that northeastern Nevada 40 million years ago was a high plateau, possibly 4 kilometers (13,000 feet) high, incised with deep valleys. Rivers in the valleys on opposite sites of a “paleo-continental divide” that ran approximately north-south slightly west of Elko, Nevada, drained either westward to the Pacific Ocean or eastward to large basins in Utah. The high plateau probably resulted from thickening of the crust following a long period of folding that ended about 60 million years ago. The plateau was probably similar to parts of the modern Andes Mountains of South America or Tibet.\nMagmatic and tectonic evolution of the Caetano Caldera, north-central Nevada: A tilted, mid-Tertiary eruptive center and source of the Caetano Tuff\nDavid A. John et al., U.S. Geological Survey, 345 Middlefield Road, Menlo Park, California 94025, USA.\nKeywords: calderas, ash-flow tuff, magma resurgence, Basin and Range Province, extensional tectonics\nThe Caetano caldera in north-central Nevada formed during a supervolcano eruption of greater than 1100 cubic kilometers (greater than 270 cubic miles) of Caetano Tuff about 33.8 million years ago. The caldera formed by when the roof of the magma chamber collapsed as the Caetano Tuff erupted from the chamber. Collapse left an ovoid depression, a caldera, about 20 kilometers long by 12 to 18 kilometers wide and up to 1 kilometer deep. Due to younger faulting and tilting (see companion paper by Colgan and others in the same issue of Geosphere), the caldera was broken into several blocks that expose caldera features from the former surface to a depth of more than 5 kilometers. This extraordinary three-dimensional view allows a far more detailed analysis of caldera formation and evolution than is available for almost any other caldera in the world. Reconstruction of the Caetano caldera also constrains nearby Carlin-type gold deposits, presently the largest producing gold deposits in the United States, to have formed at depths of less than or equal to 1 kilometer.\nOutcrop fracture characterization using terrestrial laser scanners: Deep-water Jackfork sandstone at Big Rock Quarry, Arkansas\nMariana I. Olariu et al.; John F. Ferguson, corresponding author, Geosciences Department, University of Texas at Dallas, P.O. Box 830688, Mail Station FO21, Richardson, Texas 75083-0688, USA.\nKeywords: cluster analysis, fractures, turbidite, outcrop, lidar, laser scanner, laser.\nA new way of doing geology is being invented that is sometimes called \"cyber geology.\" This paper is a pioneering example of this new approach to geologic mapping. Laser scanners and GPS are used to produce high-resolution (approaching one sample per square centimeter) maps of the outcrop of the Jackfork sandstone in the Big Rock Quarry near Little Rock, Arkansas. A three-dimensional virtual model of the outcrop is produced on a computer from this very large (millions of samples) data set. A processing scheme has been implemented that automatically identifies distinct surfaces and their orientations from the three-dimensional point cloud of sample points. These surfaces can be interpreted in terms of fractures and bedding planes in the Jackfork sandstone. The large number of virtual strikes and dip measurements permit a statistical characterization of the fracture orientations as well as an analysis of the special variability of the fractures in different locations within the quarry.\nA prominent geophysical feature along the northern Nevada rift and its geologic implications, north-central Nevada\nDavid A. Ponce and J.M.G. Glen, MS989, U.S. Geological Survey, 345 Middlefield Rd, Menlo Park, CA 94025, USA.\nKeywords: gravity and magnetic anomalies, northern Nevada rift, epithermal gold deposits, Battle Mountain-Eureka mineral trend, Basin and Range, Nevada.\nThe origin and character of a prominent large-scale geophysical feature in north-central Nevada is considered. This crustal-scale fault is coincident with the western margin of the northern Nevada rift—a mid-Miocene rift that includes mafic dike swarms and associated volcanic rocks and is partly coincident with the central part of the Battle Mountain-Eureka mineral trend. Geophysical evidence suggests that the northern Nevada rift partly followed this pre-existing feature in north-central Nevada. If the crustal fault along the northern Nevada rift and a previously inferred crustal fault along the central part of the Battle Mountain-Eureka mineral trend are in fact the same feature, deposits at a greater distance from the crustal feature associated with the northern Nevada rift at Battle Mountain could be explained by post-emplacement tectonic events. In any case, these large-scale crustal features are important to understanding the metallogeny, tectonics, magmatism, and water resources of the Great Basin.\nIdentification of quartz and carbonate minerals across northern Nevada using ASTER thermal infrared emissivity data—Implications for geologic mapping and mineral resource investigations in well-studied and frontier areas\nBarnaby W. Rockwell and Albert H. Hofstra, U.S. Geological Survey, Box 25046, MS 973, Denver Federal Center, Denver, Colorado 80225, USA.\nKeywords: remote sensing, ASTER, Thermal infrared, quartz, carbonate\nOutcrops and detritus composed of quartz and carbonate (calcite and dolomite) were identified and mapped across a 400 km by 400 km area of northern Nevada, USA, using thermal infrared data collected by the ASTER sensor aboard the EOS Terra satellite platform. This paper shows that such data can be used to generate accurate and cost-effective maps of these minerals at regional to local scales. Well-mapped rock types include: thick sequences of quartz sandstone (or quartzite) and conglomerate, bedded radiolarian chert, rhyolite, and diatomite as well as thick sequences of dolomite, limestone, and marble. Alluvial fan surfaces, sand dunes, and beach deposits composed of quartz and/or carbonate are prominent map features. Also detected were small hot spring silica sinter and travertine deposits in geothermal areas and quartz deposited from ancient hydrothermal systems that formed large deposits of gold and silver.\nGondwanan/peri-Gondwanan origin for the Uchee terrane, Alabama and Georgia: Carolina zone or Suwannee terrane(?) and its suture with Grenvillian basement of the Pine Mountain window\nMark G. Steltenpohl et al., Department of Geology and Geography, Auburn University, Auburn, Alabama 36849, USA.\nKeywords: Uchee terrane, Carolina zone, Gondwana, peri-Gondwana, southern Appalachians\nIn 1964, an exploratory petroleum well was drilled through one mile of Gulf Coastal Plain sediments in southeastern Alabama. The retrieved core included Silurian to Devonian sedimentary rocks that surprisingly contain fossils provincial to proto-Africa (i.e., Gondwanan), rather than proto-North America (Laurentian), setting off a scientific debate about how these rocks got there. At the same time, discoveries made in studies of the ocean floor were being formulated into the sea-floor spreading mechanism that would explain why and how continents drift around the surface of Earth, the missing link that had escaped Alfred Wegner’s conceptualization of continental drift. The Suwannee terrane, as the Alabama rocks became known, figured prominently in J. Tuzo Wilson’s 1966 article in Nature (“Did the Atlantic close and reopen?”), which led Kevin Burke to later coin the term “Wilson Cycle.” This orphaned block of Gondwanan crust is now known to extend in the subsurface beneath southern Alabama, Georgia, and South Carolina. A collage of deformed and metamorphosed fragments of ancient volcanic island arcs, known collectively as Carolinia (i.e., Carolina Superterrane), form the most eastern exposures of the Appalachian orogen and also project to depth beneath the Atlantic coastal plain. These arc terranes formed in an ancient ocean peripheral to western Gondwanaland prior to its climactic Appalachian collision with Laurentia that consolidated the supercontinent Pangaea. Due to the lack of fossils and reliable isotopic dates within these terranes, the nature and timing of their docking with Laurentia are controversial; hence, one of the most significant events in Appalachian history is also one of the least understood. Steltenpohl and others report new isotopic and structural information on the poorly known Uchee terrane in Alabama that bears on the problem. The Uchee terrane occupies a particularly critical tectonic position, being sandwiched between Laurentian continental basement exposed in the Pine Mountain window and Gondwanan crust of the overlying, albeit buried, Suwannee terrane. Steltenpohl and coauthors confirm that the Uchee is an “exotic” peri-Gondwanan arc terrane -- not part of Laurentia as was previously believed -- and thus provide a new puzzle piece that helps to constrain models for plate tectonic development of the Pangaean suture.\nLate Cenozoic paleogeographic evolution of northeastern Nevada: Evidence from the sedimentary basins\nAlan R. Wallace, et al., U.S. Geological Survey, MS 176, Mackay School of Earth Sciences and Engineering, University of Nevada, Reno, Reno, Nevada 89557, USA.\nKeywords: sedimentary basins, tectonics, geomorphology, Nevada, Miocene, Pliocene, gold, Humboldt River.\nNew geologic and dating studies along a 200-km-long transect across northeastern Nevada has led a better understanding of how the landscape has changed over the past 20 million years (m.y.). The studies focused on four sedimentary basins along the transect (Chimney, Ivanhoe, Carlin, and Elko, from west to east) because they record events, such as the formation of mountains and erosion by streams, that produced the landscape changes over time. The landscape was a subdued, eroding upland until about 16 m.y. ago, when volcanic eruptions in the western half of the transect and faulting in the eastern half dammed streams and formed the sedimentary basins. These basins gradually filled up and partially buried adjacent highlands. The dams for the western three basins failed after about 2 m.y., and the streams integrated into the beginnings of the Humboldt River drainage system. The Elko basin remained isolated until about 10 m.y., at which point it began to drain to the west into the Humboldt system. The river system flowed westward into northwestern Nevada, where active faulting was forming deepening valleys. As a result, the basin sediments in the upstream areas have been eroding and carried to the new downstream basins. This erosion has re-exposed the pre-basin highlands, so some modern ranges are fairly old rather than young as previously thought. Similarly, major mineral deposits in the upstream areas gradually have been exposed, weathered, or eroded, whereas those in downstream areas have been covered up. This concept is very important for the evaluation of and exploration for mineral deposits in the region."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:9868de5e-d77d-4450-af9d-7ef813b2d488>","<urn:uuid:f501d403-2943-405d-b986-f522dae3a3c6>"],"error":null}
{"question":"How has Myanmar's gemstone industry evolved over time, and what safety concerns do local communities face from modern mining operations?","answer":"Myanmar's gemstone industry has transformed significantly over time. During the Bagan Dynasty (1044-1287 CE), rubies were exclusively worn by royalty and used in Buddhist ceremonies. After independence and nationalization in 1962, the industry remained relatively stagnant until 1988, when the government adopted a free market policy and allowed private enterprise and foreign investment. The Ministry of Mines established the Myanma Gems Enterprise, leading to increased licenses and production. However, modern mining operations have created severe safety hazards for local communities. Residents in areas like Hpakant face constant dangers from landslides caused by mining waste disposal. Houses near jade mines become unstable during rains, with foundations cracking and ground becoming unsteady. Despite resident complaints, mining companies have been slow to address these issues, and government control over these companies has been limited due to their political connections and financial influence.","context":["GLEAM OF GEMS DOTS MYANMAR’S HISTORY\nMYANMAR has long been known as the best source for lustrous rubies in the world, gemstones whose beauty is rivaled only by the emeralds produced by the mines of Colombia.\nDuring the Bagan (Pagan) Dynasty (1044 to 1287 CE) rubies were worn by Myanmar royalty. Some of the royal rubies were so valuable that a Chinese emperor is said to have offered a city in his own country in exchange for one of the prized gemstones.\nRubies were used in ceremonies and to adorn royal regalia, and the choicest items mined were reserved for the court. Some were sold to India and the Middle East, but many of the finest rubies and other gemstones were dedicated to the Buddhist religion.\nMyanmar people follow Theravada Buddhism, which preaches the virtues of humility and living a simple life without ostentation. The gems were therefore not used for personal adornment but were encased in the htarpanar-taik, or relic chambers of pagodas and stupas. The search for these riches was one reason why more than 1000 pagodas were desecrated and destroyed by British troops at the end of the Third Anglo- Burmese War.\nEuropean traders first visited Myanmar around 1400 CE with the pr4nary aim of engaging in the spice trade. But some early travelers -such as Nicola di Conti, Ludovico di Varthema, Hieronimo de Santo Stephano and Caesar Fredericke -re- ported on the profusion and quality of rubies and other gemstones worn by Myanmar royalty, and this aroused the interest of the West.\nBy the 17th century Jean-Bapiste Tavernier, a trader inl precious stones, had sold Myanmar rubies to King Louis XIV and Cardinal Mazarin. Napoleon Bonaparte himself is said to have possessed a Mogok ruby.\nDuring the reign of King Pindale (1648-1661) a ruby of surpassing quality was discovered by a villager named Nga Mauk. This was presented to the king and became the finest gem in his possession. The stone weighed 80 carats when cut and became known as the Nga Mauk Ruby.\nAt the end of the Third Anglo-Burmese War, the recently deposed King Thibaw was persuaded to entrust the crown jewels and the Nga Mauk ruby to a Colonel Sladen for safekeeping.\nLater, when Thibaw asked for the return of the ruby, he was told that Sladen had returned to England. The British authorities finally told Thibaw that Sladen had died in 1910 and that there was no record of his handing over any ruby of quality to the government.\nMany Myanmar believe to this day that Thibaw was given the runaround and was the victim of deceit in high places. No trace of the Nga Mauk ruby has surfaced since.\nAfter the British annexed Myanmar, international interest grew in the ruby mines at Mogok, known to be the richest in the world. There was fierce competition to acquire mining concessions. In 1889 a company called Burma Ruby Mines Ltd won a lease to work the mines. However, due to their reliance on heavy equipment and machinery the venture failed and the company went into voluntary liquidation in 1934.\nAnother company, Ruby Mines Ltd, took over. When the Japanese invaded Myanmar the managing director and staff fled to India.\nMyanmar regained independence after World War II. Not much was accomplished in gem mining and the gem industry was nationalised in 1962.\nWhen the State Law and Order Restoration Council took over the reins of government in 1988 it repealed the old laws, adopted a free market policy and threw open the doors to private enterprise and direct foreign investment.\nThe Ministry of Mines set up a new agency called the Myanma Gems Enterprise to oversee the changeover. Under the enterprise the gemstone industry was liberalised, joint venture agreements were signed between the government and ethnic groups inhabiting the gem-bearing areas, and private companies were allowed to import machinery and equipment without paying customs duty.\nThese measures led to an increase in the number of local gem companies. In 1995224 new licenses were issued, boosting the exploration and production of gemstones and heavily fractured Mong Hsu rough rubies.\nMong Hsu located in Shan State, about 150 miles east and slightly to the south of Mogok. The mine there was worked in the 19th century, but since the rubies obtained were usually opaque and could not be easily faceted, work in the area was largely abandoned.\nThe discovery by the Thais that Mong Hsu rough rubies, when subjected to intense heat, take on the colour of Mogok rubies changed all that. Soon monstrous quantities of rough rubies from the region were being sold at gem auctions. In March 2002, more than five million carats of Mong Hsu rough rubies were purchased, while sales of genuine Mogok rubies languished.\nTai gemstone “cook ers” are constantly experimenting with heat treatments to enhance the quality of rough stones. They have already achieved considerable success and flooded the market with all kinds of heat-treated stones.\nThe Myanmar government has taken pains to assure potential buyers that all the rubies and sapphires sold at the Myanma Gems Enterprise auctions and Union of Myanmar Economic Holdings Ltd auctions are natural and untreated, and that the Mong Hsu rough rubies offered for sale are untreated unless otherwise stated.\nThe question of provenance or place of origin has lately come to the fore with regard to rubies. Myanmar rubies are the finest in the world, against which all others are measured, and to be able to say that a particular stone comes from Myanmar enhances its value by 10 to 20 per cent over those of similar quality from other sources.\nFormerly there was no surefire method of proving provenance, the method being chancy and based on anecdotal evidence. However, a new technique using D N A fingerprinting has been developed.\nThe water in which emeralds, rubies, sapphires and other precious gems were crystallised millions of years ago varied widely from area to area in the presence and quantity of certain minerals. The DNA process takes a small sample of the surface of the stone, vaporizes it and measures the oxygen isotope ratio, which can be used determine with certainly from which mine a given gemstone came.\nAnother heartening development is that many geologists now believe that the Mogok Stone Tract may be larger than formerly believed, being 10 to 25 miles wide and extending from Putao in Kachin State in the far north to Moattama in Mon State nearly 1200 kilometres of the south.","Following a deadly mudslide earlier this week at a mine in the Hpakant jade mining region of northern Myanmar’s Kachin state, local residents on Friday expressed concern about future landslides triggered by the collapse of mounds of cast-off earth and water and the possible destruction of their homes.\nFifty-four miners were buried along with 40 mining machines, including bulldozers and backhoes, when a lake of cast-off earth and water collapsed, triggering the slide late Monday night, sources told RFA’s Myanmar Service in an earlier report.\nAs of Wednesday, recovery teams had pulled four bodies from the sludge left by the mudslide, Agence France-Presse reported.\nA Hpakant resident named Nwe said she and others who reside in the area are concerned that their homes will collapse in landslides during the monsoon season, which runs from late May through October.\n“I am really scared when it rains heavily,” she said. “I’m concerned for my kids. As an adult, I could escape from the disaster, but it would not be the case for the children. I am concerned my house will be washed away if the heavy rains cause further landslides.”\nHer home used to sit on smooth land, but landslides from the dumping of mining tailings in the area have caused its foundation to slide, Nwe said.\n“You can see the land condition now,” she said. “I can no longer push up the building’s columns by placing stones against them.”\nHpakant resident Tin Hla told RFA that the ground beneath houses near jade mines becomes unsteady whenever it rains.\n“The houses become shaky when it rains at night,” she said. “We are too scared to sleep at home since the ground beneath the house is already cracking. We tried propping up the house from below to keep it stable, but the more we prop it up, the more the land falls.”\n“We don’t want to do the propping up anymore,” she added. “I want those who are responsible to repair the ground.”\nSince early February, houses in the area have become wobbly and now appear as if they are about to collapse from jade mining companies disposing slag, residents said.\n“Earlier, when these land plots were allotted by the authorities, these landslides didn’t occur,” said Hpakant local Su Hlaing. “Before the earthmovers disposed the mining waste around here, there were no landslides. There were no problems for one or two years before the dumping began.”\n“[But] the land began to collapse after the trucks started dumping their waste, because of the heavy weight of slag heaps pressing down the land beneath them,” she said. “Since the slag heaps have piled up slowly, they are now higher than the village itself, and one of them collapsed.”\nThe dumping of tailings caused a landslide in 2017 that flattened several homes in Seik Mu village tract in Hpakant's Ward No. 6, locals said.\n‘Happening all the time’\nResidents said they have demanded that two companies stop dumping mining waste near their homes and repair damage caused by landslides before the arrival of the monsoon season.\nDaung Say, administrator of Ward No. 6 where the residents live, said local authorities have asked the mining companies to repair homes.\n“We have asked the companies to repair houses damaged by the landslides,” he said. “They said we have to prop up the houses using longer columns to prevent a collapse.”\nHpakant authorities requested that the companies not wait until the monsoon season arrives to do the work, Daung Say said.\n“If the houses totally collapse, the companies will be responsible for it,” he said.\nRFA was unable to reach the mining companies for comment.\nResidents also said they submitted a formal compliant with the township administration office, but have not yet received a response.\n“We have instructed the township administrators to have people evacuate houses that have an immediate risk of collapse,” said Da Shi Lar Sai, Kachin state’s minster of natural resources and environmental conservation.\n“This is happening all the time,” he said. “Not just now.”\nGovernment has fallen short\nMeanwhile, a Myanmar official said the government has fallen short when it comes to taking action against mining companies that violate industry regulations in Hpakant.\nMyo Nyunt, spokesman for the ruling National League for Democracy (NLD) party, said the Myanmar government is responsible for those who lost their lives in and those who are suffering from the latest disaster.\n“But we have a difficult time controlling these companies because most of them working in the Hpakant region have connections to the previous government, and they have a lot of money,” he said.\n“When we try to control these companies, we can’t do as much as we want,” he said. “But when we have this kind of tragedy, we have to do something to ensure accountability for the people who lost their lives,” he said.\nThe government has been planning to amend the law the regulates the granting of permission to small-scale miners to work in jade mines, Myo Nyunt said.\nThan Zaw Oo, general manager of Myanmar Gems Enterprise, which organizes emporiums and special jade and gems sales, said many accidents have occurred in Hpakant because of shortcomings in the operations of jade companies themselves and in government departments responsible for controlling the companies.\n“We have the Gemstone Law, but it does not cover enough,” he said. “The Ministry [of Mines] might have its own plan [to control the companies], but we need to follow the ministry’s instructions.”\nAfter the law was passed in December 2018, London-based international anti-corruption NGO Global Witness pointed to its failure to address licensing criteria and its absence of clear guidelines and strong enforcement mechanisms to prevent companies with histories of illegal activities and irresponsible mining practices from obtaining new mining licenses.\nMyanmar attorney Thein Thyan Oo said mine workers do not listen to officials who warn them about the dangers of slag heap collapses.\n“Authorities would stop the daily workers from searching for jade in the damp soil, but the workers did not listen to them because they needed money,” he said.\n“If we have to blame something, then we must blame the poverty,” he said. “They will work in the damp soil anyway, even though they know they can die.”\nAuthorities must work on plans for developing businesses to fight poverty. Meanwhile, they need to do something to protect the workers.”\nHpakant, which lies about 400 miles (640 kilometers) north of Myanmar’s capital Naypyidaw, is the center of country’s jade mining industry and produces some of the highest-quality jade in the world.\nMuch of the gem output is exported or smuggled to neighboring China, where demand for the precious stone is high.\nReported by Elizabeth Jangma and Khin Khin Ei for RFA’s Myanmar Service. Translated by Ye Kaung Myint Maung and Khet Mar. Written in English by Roseanne Gerin."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:166a165e-89f9-4d2e-a994-8d7b54fcd215>","<urn:uuid:d6ed0da8-35d7-4606-8266-ad3b65705246>"],"error":null}
{"question":"Which complications are shared between the MB&F Legacy Machine No. 1 and the Patek Philippe Ref. 5207, and what are their unique features?","answer":"Both timepieces share refined finishing and sophisticated mechanics, but their complications differ significantly. The MB&F Legacy Machine No. 1 features a floating balance wheel visible from the dial side and a second time zone display adjustable to the minute and second. The Patek Philippe Ref. 5207 is more complicated, featuring a minute repeater, instantaneous perpetual calendar with aperture displays for day, date, month, and leap-year cycle, plus a tourbillon escapement (visible only through the case back), moon phase display, and day/night indication.","context":["The only thing about the Legacy Machine No. 1 that concerns Maximilian Büsser, really, is the fact that it’s vintage-looking, just when the wave of vintage-looking watches is at its peak. What if people think that he’s jumping on the bandwagon? Or that he’s catering to Asian markets, with their reputed penchant for classically styled timepieces? For Büsser, who has never made any bones about going his own way, regardless of market clamor, this is a troublesome concept.\nThen again, there’s rarely any basis for MB&F to be accused of copycat intentions, or pandering to consumers. The truth of it is that Büsser first sketched out the LM1 in late 2007, recognizable from the very first with its hovering balance wheel and two round dials. As any observer of the industry will probably tell you, the heritage-seeking, back-to-fundamentals watch aesthetic only really took hold a couple of years after that.\nWhich is not to say that the Legacy Machine No. 1 is by any means a watchmaking anachronism from the 19th century, Lazarus-ed into being by MB&F for a triumphant comeback tour. Because you see the immediate cues — the glossy white dials and Roman numerals, the 14mm screwed balance beating at a languid 2.5Hz, the cambered dial crystal, the large jewels set in polished gold chatons — juxtaposed with the floating balance wheel, the hyper-real refinement of the finish, and the double-headed Grendizer battle axe on the winding crown. If the Legacy Machine No. 1 is an anachronism, it’s one that was made in a Donnie Darko universe tinged with the eternal return of Nietzschean metaphysics, poised somewhere high up on the asymptote to the axis of perfection.\nMy first encounter of the LM1 was of its reverse, as Max Büsser presented it, saving the jaw-dropping obverse for last in his unerring instinct for dramatic flair. The movement, finished to Kari Voutilainen’s flawless standards, was so captivating as to completely obscure the absence of the escapement. As a matter of fact, when Voutilainen was asked about the most difficult part of designing this watch, his answer was that it was designing a movement that could be beautiful even without the emotive charge of a visible regulating organ. “He ended up designing something so beautiful that you forget there’s no balance wheel,” Büsser said.\nThere is, of course, a balance wheel. It’s simply — as if anything about executing this timepiece could be described as “simple” — on the other side of the baseplate. The balance wheel, hairspring, anchor and escape wheel are extruded through the baseplate, with the escape pinion left in its conventional position as point of contact with the fourth wheel. The result, as seen through the dial-side crystal, is the almost magical-seeming balance wheel that hangs suspended from a cantilevered double-arched bridge like an extraterrestrial visitor from the pages of an H. G. Wells novel. The rest of the assortment cleaves close to the dial, the low-lying giveaway to this horological conjuration.\nThe twin white lacquer subdials are ostensibly for the display of two separate time zones. Overshadowing their functional aspect, however, is their symbolic significance of continuing with the double-dial aesthetic that has defined every other timepiece from the brand. In a sense, the Legacy Machine No. 1 is the recipient of a bequest of design and philosophy from the Horological Machines, which is ironic, given that the LM1 is meant to embody Büsser’s creative force as filtered through an era in which none of the Horological Machines could have existed. As a prequel — and the LM1 is, essentially, a prequel to the HMs, being made after but set a full century before them — it comes dangerously close to the Platonic ideal. George Lucas ought to have taken some valuable pointers from the MB&F playbook. Imagine the possibilities. Jar Jar Binks might never have been foisted upon\nthis unhappy world.\nWith the Legacy Machine No. 1 being what it is and coming from where it does, its second time zone is not merely adjustable to the hour, as you would get with the majority of GMT timepieces on the market. It’s not even adjustable to the half- or quarter-hour, as some finer examples of such watches are. The second time zone on the LM1 is adjustable to the minute and second, and runs off the same regulator as the other dial indication (Theoretically, albeit impractically, it’s adjustable down to a fifth of a second, as the balance oscillates at 2.5Hz.) With such flexibility in the second time display, we now leave the prosaic realms of second time zones behind. After all, “you can also use it as a stopwatch,” says Büsser. Or it could very well represent time in a parallel world, in which civil time always runs 10 minutes slower than it does in this one; a world in which I wouldn’t have missed this morning’s train. There is a theory about the non-linearity of time, which posits that the past, present and future are all happening at once, and that the only reason human beings are unable to experience this is because we are restricted by our three-dimensional nature. If the human race ever became aware of time as scalar instead of vector, possibly the only watch we’d ever wear would be the Legacy Machine No. 1.\nThe peerless finish of the movement is thanks to the expertise and stratospheric standards of independent watchmaker Kari Voutilinen, whose name is engraved on the bridge supporting the center wheel, along with that of Jean-François Mojon, who designed the movement\nAs if it didn’t have enough up its sleeves already, the LM1 also has a world’s first as a trump card — its vertical power reserve indication is, according to Büsser, unique among all wristwatches. The gear train which transmits information about the state of wind of the mainspring is coupled to a differential which transforms its horizontal motion to a perpendicular plane. The resultant action is displayed via the verticalized arc described by a steel lever, and the power reserve indication read off the flame-blued tip of the lever against a curved scale which projects off the sunray-brushed dial. The main issue with this, as it is in most things watchmaking-related, is one of maintaining the appropriate level of energy. The lever — indeed the entire power reserve mechanism — must be sturdy enough to overcome gravitational force at various positions, and that takes considerably more power than the standard power reserve displays of most other timepieces, which consist of a relatively simple gear train coupled to a slender hand. It’s no small matter, as any energy-guzzling complication eventually affects the amplitude of the balance, which is crucial to the fundamental timekeeping aspect of a timepiece. With the vulnerability inherent in the LM1’s floating balance wheel, one might think the development team would have left well enough alone, instead of messing around with (arguably unnecessary) vertical power reserves. Of course, this doesn’t seem to have daunted Jean-François Mojon, the man behind the LM1 movement, who is also responsible for such acts of horological audacity as the Harry Winston Opus X and de Grisogono’s Meccanico. The power reserve arches off the dial with effortless aplomb, literally and metaphorically meeting the magnificent balance wheel halfway. This is how Büsser is able, during presentations, to point out how harmoniously the different elements of the dial work together, how the power reserve display resembles a marine sextant, an oil rig — to be honest it could resemble any damn thing he likes, because it’s when things work beautifully (as this does) that you can start building worlds both real and imaginary upon them.\nWhen talking about the origins of the Legacy Machine No. 1, Büsser frequently brings up the 19th-century World’s Fairs — one of the few large-scale events in a time before mass media that “allowed people to dream and experience awe”. Countries would come to the World’s Fair and take up pavilions and exhibit their latest inventions and technological advancements to the admiring hordes; the first locomotives would have been presented there. To paraphrase Miranda, the wide-eyed protagonist from Shakespeare’s tale of exploration and discovery, The Tempest, “O brave new world, that has such creatures in it!”\nAnd so, with the Legacy Machine No. 1 in our hands, say we.","Presented at Basel world 2008 in the category “Exceptional Watches”, the Ref. 5207 Grand Complication watch from Patek Philippe incorporates minute repeater, instantaneous perpetual calendar and tourbillon complications. This timepiece elevates the perpetual calendar to a new level of technical ingenuity. Its instantaneous perpetual calendar with aperture displays for the day, date, month, and leap-year cycle. Additionally, the Ref. 5207 features a moon phase display and a day/night indication.\nThe most conspicuous facet of Patek Philippe’s new, highly complicated masterpiece is its understated look. At first sight, it projects the classically sleek, round form of a Calatrava with a balanced, easily legible dial. It takes a closer look to see the slide for the minute repeater, and the tourbillon, as always in Patek Philippe watches, is visible only through the case back.\nInstantaneous Perpetual calendar\nThis timepiece incorporates a spectacular arrangement of perpetual calendar functions on its dial to enable the minimalistic and clear display of indications. While, all indications starting from day, date, month, day/night, moon-phase and leap year are always visible on the dial, one special function of the perpetual calendar mechanism, the instantaneous switching of all calendar displays, can only be seen at midnight. This extremely elaborate, additional complication comprises 212 components.\nDepending on the design of the mechanism, it takes mechanical calendar watches anywhere from 20 minutes to several hours to switch the displays. The greatest amount of energy is expended around midnight on February 28, when the date ring has to be moved forward by no fewer than four days to switch to the first of March. It is precisely this problem that preoccupied Patek Philippe’s designers and master watchmakers for a long time. They spent five years developing a mechanism that could instantaneously and simultaneously switch the day, date, month, and leap-year aperture displays at midnight. The heart of their invention is a mechanism composed of levers and program cams for which two patent applications have been filed.\nThe entire instantaneous perpetual calendar mechanism was designed as a module that can be integrated into an existing movement. In the case of the new Ref. 5207, the host movement is the coveted caliber R TO 27 PS, which, together with the instantaneous perpetual calendar, becomes the caliber R TO 27 PS QI movement: R stands for répétition minutes (minute repeater), TO for tourbillon, 27 for a diameter of 27 mm, PS for petite seconde (seconds subdial) and QI for quantième perpétuel instantané (instantaneous perpetual calendar).\nIn the new caliber, a 425- millimeter long mainspring coiled up in a barrel with an inside diameter of 9.18 millimeters must reliably execute all functions of the watch for approximately 48 hours – including the simultaneous and instantaneous advance of all calendar indications at midnight.\nOne of the challenges in this particular instance is that switching does not involve advancing dainty, featherweight hands. In this watch, the displays are aperture types with disks that assure excellent legibility but whose mass is several orders of magnitude greater than that of hands. Nonetheless, the specifications for the new instantaneous perpetual calendar stipulated that all displays must switch dependably, even with a residual power reserve of merely 2.5 hours. To address this difficult task, Patek Philippe developed a calendar mechanism for which two patent applications were filed (published European patent application No. EP 1734419 A1 and Swiss patent application No. 01080/07).\nThe first patent-pending innovation relates to the activation of the calendar indications with a large yoke. It is controlled by a four-toothed rack connected to a month lever that samples the annual program cam. This cam is connected to further switching cams via articulated arms, and controls the concurrent, instantaneous switching of the date, day, month, and leap-year disks. These processes explain the complex shape of this large yoke. It consists of fifteen individual parts, some of which are movable. Annual programming relies on a months cam with a variable February planetary cam that changes its profile every four years.\nThe second patent application involves the daily tour de force of calendar switching. At the end of months with 31 days, the date ring only needs to be advanced by one day. If a month has 30 days, the 31st day on the date ring must be skipped. In leap years, the ring must skip two days when switching from February 29 to March 1. And in normal years, the instantaneous advance motion must skip three days to jump from February 28 to March 1. This gradually increases the path and the angular deflection of the large yoke while narrowing the angle between the large yoke and the axis in which the driving spring acts, thus gradually weakening the spring’s resultant force.\nThis declining force cannot be offset simply by choosing a stronger spring in the first place because at the end of long months, this would increase the risk of the date ring bouncing forward to the 2nd or 3rd day of the month instead of stopping at the 1st. Thus, the engineers had to find a solution for advancing the date ring with the same controlled force regardless of the number of days in any given month. Patek Philippe invented a system with two equally strong springs that act in different directions.\nIn long months, the large yoke is connected only to the first spring because of its steep angle. Its energy is sufficient to advance the date ring (and the other calendar displays) by one or two days in an instantaneous and controlled manner. But the more the yoke is deflected, the weaker the force of the first spring becomes. When a certain angle is exceeded, the large yoke comes into contact with the second spring whose force vector deviates by about 45° from that of the first spring:the weaker the force exerted on the large yoke by the first spring, the stronger the effect of the second spring. This assures that even at the end of February, the calendar displays are advanced with a controlled amount of energy. This is the second innovation for which a patent is pending(Swiss patent application No. 01080/07). am. At this time, the large yoke is completely quiescent, so the amplitude of the balance wheel remains unaffected by movements of the calendar mechanism.\nThe Minute Repeater\nThanks to more than 160 years of experience, Patek Philippe has evolved the art of making exceptional minute repeater wristwatches to an unprecedented degree of perfection. When the owner of a Ref. 5207 activates the slide and listens to the watch as it first counts the hours on a low-pitched gong, then indicates the quarter hours with double strikes on both gongs, and finally tallies the number of minutes that have elapsed since the last quarter on the high-pitched gong.\nIt took many years of research to develop the best steel alloy and optimize the shape and attachment of the gongs in cooperation with the internationally respected Swiss Federal Institute of Technology in Lausanne. It may not be surprising that the manufacture’s president, Philippe Stern, repeatedly listens to the sound of each of these rare watches before deciding whether the exceptional masterpiece is ready for delivery or must return to the workshops for acoustic tweaking. The sound, which for Philippe Stern constitutes the acoustic signature of Patek Philippe, has been recorded and digitized in an anechoic chamber to preserve it as a reference for all time.\nThe third highlight of the Patek Philippe Grand Complication Ref. 5207 is the tourbillon escapement that keeps the rate of the watch extremely steady even in vertical positions. It has a steel cage that rotates about its own axis once a minute. It consists of 69 individual parts and weighs a scant 0.3 grams. All steel parts are separately angled, satin-finished and ground or polished by hand.\nLike all Patek Philippe wristwatches with tourbillons, the Ref. 5207 also comes with a certificate issued by Switzerland’s official chronometer testing agency, C.O.S.C. (Contrôle Officiel Suisse des Chronomètres).\nAfter it has passed the C.O.S.C. tests, each chronometer is submitted to Philippe Stern who personally decides whether it deserves the inscription “Patek Philippe Tourbillon” or whether the entire procedure needs to be repeated. Since the in-house specifications at Patek Philippe are at least twice as demanding as those of the official testing agency, it is not uncommon for a movement to be precision adjusted a second time and then again sent to the C.O.S.C. The serial number of the movement is mentioned on the agency’s certificate for positive identification purposes. The same serial number is also inscribed on the dial of the tourbillon wristwatch, but otherwise, nothing else suggests that the timepiece embodies this horological distinction.\nThe tourbillon cage is lubricated with oil that can react to the ultraviolet radiation in sunlight and could age prematurely if exposed to it. For this reason, Patek Philippe deliberately does not display the tourbillon through the dial. However, the solid-platinum case back of the watch can be replaced with an included sapphire-crystal back.\nThe 41-millimeter, classic Calatrava-style platinum case of the Ref. 5207 is made using the traditional cold-forming technique. It is crafted in-house with high-tonnage presses. Its flanks and the minute-repeater slide are hand-engraved, and like all Patek Philippe platinum cases, it features a discreetly set Top Wesselton diamond between the lugs at 6 o’clock.\nThe “Honey Gold”-hued 18K gold dial beneath the slightly domed crystal reveals an unusual arrangement of displays for perpetual calendar functions. The apertures in mirror-polished frames show the day, date, and month along an arc between 10 and 2 o’clock. The date display with its mirror-polished white-gold frame is particularly prominent. The seconds subdial at 6 o’clock has an aperture for the moon-phase display, a poetic yet highly precise indication that deviates from the true lunation by only one day in 122 years. To its left is a small aperture for the day/night indication. The aperture on the opposite side shows the leap-year cycle with Roman numerals I to IV.\nThe Ref. 5207 is among Patek Philippe’s most complicated wristwatches; only very few of these watches will be crafted each year. Because of this limited availability, the Ref. 5207 will be sold exclusively in the Patek Philippe Geneva Salons during the introductory phase.\nModel: Grand Complication Ref. 5207 in platinum with minute repeater, instantaneous perpetual calendar in apertures, tourbillon, and moon-phase display\nCaliber R TO 27 PS QI\nManually wound mechanical movement, tourbillon, minute repeater, instantaneous perpetual calendar. Apertures for day of week, date, month, leap-year cycle, day/night indicator, moonphase display, and seconds subdial.\nOverall diameter: 32 mm.\nHeight: 9.33 mm\nNumber of parts: 548\nNumber of jewels: 35\nPower reserve: max. 48 hours\nFrequency: 21,600 semi-oscillations/hour (3 Hz)\nTourbillon: Tourbillon cage in steel, 69 parts, 0.3 grams 1 rpm. Tourbillon on one axis with balance and fourth wheel\nBalance spring: Breguet\nBalance spring stud: Movable\n– Pulled out: to set the time\n– Pushed in: to wind the watch\nExclusive certificate issued jointly by the C.O.S.C (Contrôle Officiel Suisse des Chronomètres) and the Geneva Seal. Hallmark: Geneva Seal\nWith hands: Center hours and minutes & Seconds at 6 o’clock\n– Day of week between 10 and 11 o’clock in polished white-gold frame\n– Date at 12 o’clock in a polished white-gold frame\n– Month between 1 and 2 o’clock in a polished white-gold frame\n– Moon phase at 6 o’clock\n– Day/night indicator between 7 and 8 o’clock\n– Leap year cycle between 4 and 5 o’clock\nCorrector push pieces\n– Day between 11 and 12 o’clock\n– Date between 6 and 7 o’clock\n– Month between 12 and 1 o’clock\n– Moon phase between 5 and 6 o’clock\nStylus in ebony and white gold included. Minute repeater with small strike (hours) on the 1st gong, minutes on the 2nd gong, and quarter hours on both gongs.\n950 platinum, delivered with solid-platinum back and interchangeable sapphire-crystal back; diamond with approx. 0.02 ct. between the lugs at 6 o’clock\nDimensions: Diameter: 41 mm\nHeight: 16.25 mm\nWidth between lugs: 20 mm\nSlide: In the side of the case to start the minute repeater\n18K gold, Honey Gold hue (New exclusive Patek Philippe dial color)\n11 hour markers in 18K white gold\nDauphine hands in 18K white gold for hours and minutes\nBaton seconds hand with counterweight, 18K white gold\nMinute scale on periphery of dial\nShiny brown alligator with rectangular scales, hand-stitched, 116- mm prong buckle in platinum"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f5023174-f05d-4c62-981a-f0b61db915b0>","<urn:uuid:991bccef-3e1d-4b0d-b6c5-fa19862a326c>"],"error":null}
{"question":"How do software frameworks and cross-platform mobile applications compare in terms of code reusability?","answer":"Software frameworks in general allow developers to avoid duplicate and redundant code through pre-built functionalities, while specifically in cross-platform mobile development, frameworks like Xamarin enable sharing up to 75% of the code across different platforms. The code reusability in general frameworks helps in establishing better programming practices and consistent code development, while in cross-platform mobile applications, it specifically allows writing the core application logic once and sharing it across iOS, Android, and Windows platforms, significantly reducing development time and resources.","context":["What is a Framework? [Definition] Types of Frameworks\nTable of Contents\nAs a programmer, you don’t need to start from scratch when you have tools designed to help you with your projects. Frameworks are software that are developed and used by developers to build applications.\nWhat is a Framework?\nSince they are often built, tested, and optimized by several experienced software engineers and programmers, software frameworks are versatile, robust, and efficient.\nUsing a software framework to develop applications lets you focus on the high-level functionality of the application. This is because any low-level functionality is taken care of by the framework itself.\nWhy do we use Frameworks?\nDeveloping software is a complex process. It necessitates a plethora of tasks, including coding, designing, and testing. For only the coding part, programmers had to take care of the syntax, declarations, garbage collection, statements, exceptions, and more.\nSoftware frameworks make life easier for developers by allowing them to take control of the entire software development process, or most of it, from a single platform.\nAdvantages of using a software framework:\n- Assists in establishing better programming practices and fitting use of design patterns\n- Code is more secure\n- Duplicate and redundant code can be avoided\n- Helps consistent developing code with fewer bugs\n- Makes it easier to work on sophisticated technologies\n- One could create their software framework or contribute to open-source frameworks. Hence, there is a continuous improvement in the functionality\n- Several code segments and functionalities are pre-built and pre-tested. This makes applications more reliable\n- Testing and debugging the code is a lot easier and can be done even by developers who do not own the code\n- The time required to develop an application is reduced significantly\nWhat goes in a Framework?\nWhen you install a software framework, the first thing that you need to take care of is the system requirements. Once a framework is installed and configured, it creates a directory structure.\nFor example, fig. (i) illustrates the directory structure of the Laravel Framework. Each of these folders could have additional directories. Directories can further have files, classes, test routines, templates, and more.\nDifference between a Library and a Framework\nSome may assume that a software framework is a collection of libraries just as libraries are a collection of precompiled routines. However, this is not true as not all software frameworks use or depend on libraries.\nThe difference between a library and a framework is that the latter calls the code. Opposite to this, the code calls the software library. Let us understand this with an example:\ncurl is a library in PHP. When you use one of the curl functions, the PHP code calls that particular function in the curl library. Your code is the caller, and the library code is the callee.\nWhen you use a PHP framework, such as Laravel, the relationship gets inverted, and so the software framework calls the application code written in the framework. This is technically known as Inversion of Control (IoC).\nProgramming language vs Frameworks\nA programming language tells the computer what it should do. Every programming language features a syntax and a particular set of rules, which need to be followed every time the code is written.\nA software framework is built on top of a programming language. For example,\nRails, also known as Ruby on Rails, is a web framework built on top of the Ruby programming language.\nTypes of Frameworks\nAs a developer, you should be on the lookout for frameworks that best suit your needs. Whether it is working on a website, data science, database management, or mobile applications, software frameworks exist for all genres of software programming.\nThere are many types of software frameworks to make it easier for developing applications for a wide range of application development domains. Let us dive into some of the software frameworks that are in vogue today:\nWeb Application Frameworks\nAngular is a typescript-based, open-source JS framework that makes it easy to build applications on the web. Angular solves application development challenges by combining declarative templates, dependency injection, end-to-end tooling, and much more.\nAngular empowers developers to build applications that live on the web, mobile, and desktop.\nSome popular websites developed using AngularJS are:\nDjango is a free and open-source web application framework written in Python. Built by a team of experienced developers, Django takes care of web development so that developers can focus on writing applications without reinventing the wheel.\nLarge organizations actively use Django in its development. Some popular websites developed using Django are:\nLaravel is a PHP-based web application framework with an expressive, elegant syntax. The open-source framework and follows a model-view-controller design pattern that is robust and easy to understand.\nAccording to Google Trends, Laravel has secured the position for the most powerful PHP framework, which offers a standardized and feature-packed platform for high-performing PHP web application development.\nSome popular websites developed using Laravel are:\n- Neighborhood Lender\n- World Walking\n1. Apache Spark\nApache Spark is a unified analytics engine for large-scale data processing. You can write applications quickly in Java, Scala, Python, R, and SQL using the Apache Spark.\nOver 3,000 companies are using Apache Spark, including top players like:\nPyTorch is an open-source machine learning framework that accelerates the process from research and prototyping to production deployment.\nPrimarily developed by Facebook’s AI research group, PyTorch can be used with Python as well as C++. PyTorch is used for Computer Vision and Natural Language Processing (NLP). Some popular websites developed using PyTorch are:\nTensorFlow is an end-to-end open-source framework for machine learning (ML). It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers dive in ML, and developers quickly build and deploy ML-powered applications.\nThree typical applications for TensorFlow are-\n- Convolutional Neural Networks (CNN) for image recognition and processing.\n- Large-scale linear models for data analysis and simple behavioral predictions.\n- Sequence-to-Sequence (Seq2Seq) models for human language-related features.\nCheck out these best data science tutorials and courses.\nMobile Development Frameworks\nIonic is a free, open-source mobile UI toolkit for developing high-quality, cross-platform native applications for Android, iOS, and the Web—all from a single codebase.\nIonic is a development platform for the entire application lifecycle that allows teams to build better and faster applications. Some of the popular applications developed using Ionic are:\n- McDonald's Türkiye\nXamarin is a free, open-source application development platform for building Android, iOS applications with .NET, and C#. Xamarin is part of the .NET platform that has an active community of over 60,000 contributors from more than 3,700 companies.\nSome of the popular applications developed using Xamarin are:\n- Alaska airlines customer applications\n- CA Mobile for mobile banking\n- Novarum DX, a medical app\nFlutter is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase. It has an expressive and flexible UI and gives a native performance on iOS and Android platforms.\nSome of the popular applications developed using Flutter are:\n- Alibaba (eCommerce)\n- Google Ads(utility)\nA Word of Advice Before Starting with Software Frameworks\nIt is a good idea to learn and develop coding skills by learning the nuances of a programming language before using frameworks for application development. If not, you might miss out on a valuable experience with the underlying technology that exists in a framework.\nIf you’re not already an expert programmer, then it is essential to understand the code which powers the framework. This knowledge would make it easier when you run into complex challenges and would make you an overall skilled developer.\nMany front-end developers contribute to open-source frameworks to support the entire community of developers. For example, developers at Google build AngularJS and Polymer, both of which are freely available to all front-end developers.\nMany developers support the front-end community by contributing to open-source libraries as well.\nThe bottom line suggestion for programmers who want to use software frameworks for application development is to learn a new framework or a programming language based on the need for the applications that are to be developed.\nFurther, analyze areas such as front-end, back-end, cloud management, and mobile technology, the potential market share of the technology, sustainability, and more, and understand the technology features before deciding to sign up with one.","A well-developed mobile application is an important pillar that supports the growth of businesses. It can help you engage your current customers, open doors to new ones and even provide you with an opportunity to exploit a completely new market.\nCross platform apps are those that can run on multiple platforms. The rise in competition and the trend of Bring Your Own Device to work has done nothing but fuel the demand of this category of apps. Cross platform apps are also great for startups entering the market.\nOnce the call has been made to develop a cross platform app, the next question that arises is about the framework to use to build the app. There are a lot of options in the market that could do well.But our vote has been set on Xamarin. It is considered one of the best framework for cross platform mobile app development by a majority of agencies. It has been used widely since its launch in 2011.\nLet’s have a brief look at what Xamarin is:\nWhat is Xamarin?\nXamarin is a framework created using C# that is popularly used to create cross platform mobile apps. Xamarin works on a single language i.e. C# and offers a runtime that works on three major mobile platform (Android, iOS and Windows). This allows the mobile app to look and feel like a native one on the respective platforms.\nXamarin was founded by Miguel de Icaza, Nat Friedman and Joseph Hill in May 2011. These were the same engineers that had also created Mon, Mono for Android and MonoTouch. In February 2016, Microsoft acquired Xamarin and has been a Microsoft owned company ever since.\nXamarin was built to provide a very unique function i.e. the ability to make cross platform apps easily whilst providing the apps a native feel and look depending on the platform. The core logic is written in C# while having the flexibility to design native user interface specifically for each platform.\nUnder the UI layer of code which is specific to each platform, lies the shared application logic. This needs to be written just once and it works across all platforms similarly. Developers can share as much as 75% of the code across platforms depending on the UI.\nAdvantages of Xamarin for Cross Platform App Developmen\nFew reasons why Xamarin stands ahead of all other frameworks in cross platform app development are as follows:\n1. Native Interface: The most preferred solution for the development of any application is to be able to provide a native interface to its users. This where Xamarin’s unique approach gains leverage. The comprehensive Integrated Development Environment of Xamarin provides a way for each C# code to be able to access device-specific features and provide a native UI and performance.\n2. Faster Development: Writing more code takes more time and money. But Xamarin allows us to code application logic once and then share it across the iOS, Android and Windows platform. With up to 75% of the code required to write only once, a lot of time and resources can be saved and the app can be launched more quickly.\n3. Perfect Quality Assurance: Any app needs to be tested under various circumstances in order to make sure that the app functions as desired in all of them. The Xamarin Test Cloud provides testers this capability. Testers can perform their tests across a number of real devices of different models and makes to ensure that the app is perfect.\n4. Fewer Bugs: C# is a simple, general purpose, type safe and object oriented language. When you Hire a C# developer to write your code, the type safety will help him/her prevent typing errors that could result in flawed program behavior. Another reason for this is the fact that since most of the code is shared, the code is not exceptionally long for all platforms. Lesser code also leads to a less chance of bugs.\n5. Cost Saving: Although it seems a little obvious, however, the immense cost saving is a crucial advantage of using Xamarin that cannot be left out. Making use of Xamarin help you drastically cut costs on manpower and resources.\nFirstly, since only a developer skilled in C# is required for developing apps on Xamarin, you save on developer costs.\nSecondly, since majority of the app logic needs to be coded only once, the development lifecycle is reduced significantly and hence you save on development costs.\nThirdly, the Xamarin Test cloud is an effective and inexpensive option for testing your app. It also provides a lot more real devices (up to 1000) to test on than the others which provides a more practical result. Hence, you can save a lot on app testing.\nLastly, since your app is cross platform, you can launch it on all platforms at once and gain monetary value of your app from a much larger market quickly rather than having a small audience of just one platform. Hence, you save on product launch costs.\nThese are just a few of the benefits of employing Xamarin for your app project.\nIt is important to appeal users from different platforms to ensure complete satisfaction across different devices. The last thing you want is to develop an app for a particular platform just to realize that you need to target another platform. It is abundantly clear that multiple platforms are here to stay and you need a strategy to deliver applications in a high demand scenario. So make sure to make a wise choice."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:1fd1528b-2443-46fc-93d7-ffb7f4a3e0c2>","<urn:uuid:bb59525a-0c77-4865-8566-15e6a5b39c1d>"],"error":null}
{"question":"Hi! Me doing phd in games design - ATMSG model and virtual ethnography both good for analysis?","answer":"Both ATMSG and virtual ethnography are valuable analytical tools but serve different purposes. ATMSG is specifically designed for analyzing educational serious games' structure and how gaming/pedagogical elements connect to reach educational goals. It's most useful when evaluating game prototypes or doing detailed analysis of learning patterns. Virtual ethnography, on the other hand, focuses on studying online gaming behaviors and cultures in their natural context through participant observation in virtual spaces. It provides in-depth insights into player interactions and behavior but may sacrifice some critical distance due to researcher immersion. While both are analytical tools, ATMSG is more focused on game design elements while virtual ethnography examines player behavior and cultural aspects.","context":["Here I describe a model for analyzing serious games based on activity theory. The model can also be used for serious game design as a tool to evaluate prototypes. This post is a short summary of an academic article published in Computers & Education (link in the text).\nAn important part of my PhD research has been a theoretical one: I have been trying to understand the internal structure of serious games, particularly how gaming and pedagogical elements are connected to each other in a game to reach its educational goals. This objective supports my overall goal of building a framework to help serious games designers and developers in a very practical way, ultimately making it easier and cheaper to produce serious games.\nRecently, I and my colleagues published an article in the journal Computers & Education, titled An activity theory-based model for serious games analysis and conceptual design, in which we describe a model for serious games analysis and conceptual design. The article is also available in this website, in the Research & publications page (author’s version).\nWhat is this ATMSG model about?\nThe main idea of the ATMSG model is to support a systematic and detailed representation of educational serious games, depicting how the combination of serious games elements supports pedagogical goals.\nAs the name indicates, the model uses activity theory as underlying theoretical framework. Activity theory is a line of research initiated in the 1920s and 1930s by a group of Russian psychologists. It studies different forms of human practices and development processes, providing a model of humans in their social and organizational context 1.\nActivity theory provides us with a way to reason about the relationships between serious games components and the educational goals of the game, considering the game as part of a complex system that involves at least three activities: the gaming activity, the learning activity and the instructional activity.\nThe figure below illustrates the model.\nThe hierarchical structure of the activity also gives us the ability to change the focus of the analysis to different levels of detail. In ATMSG, each activity is broken down into a sequence of actions mediated by tools with specific goals. Each of these items is called “serious games components”, that is, the “concrete” pieces of a serious game that compose the game play over time, e.g. characters, tokens, tips, help messages, etc.\nA unified taxonomy of serious games components\nWe used the ATMSG model described above to reorganize existing taxonomies of learning, instruction, games and serious games (such as the game Ontology Project, Learning mechanics — Game Mechanics (LM-GM), GOM II, Bloom’s taxonomy, Gagné’s Nine Events of Instruction, ARCS Model of Motivational Design), into a unified vocabulary. The taxonomy is organized in a tree structure in which items are classified according to the activity to which they belong, and, within the activity, categorized as actions, tools or goals. It aims to aid in the identification and classification of components according to their characteristics and roles in the game.\nSerious games analysis with ATMSG\nWe defined a four-step process to guide users in analyzing existing serious games or prototypes. These steps take the user from a high-level understanding of the activities to the concrete components that implement those activities. The user identifies game components with the help of the taxonomy of serious game components.\n- In the first step, the user describes the main activities involved in the activity system and identifies their subjects and corresponding motives.\n- Then, to help in the identification of the components of the serious game, the user produces a diagram that represents the game sequence in a rough timeline.\n- Subsequently, the user identifies components related to each node of the game sequence. Each event in the game is decomposed into its actions, tools and goals.\n- Finally, the user groups each set of actions, tools and goals. For each of those blocks, the user analyzes how these components go together in the game and explains how the usage of such components and characteristics support the achievement of the entertainment and/or pedagogical goals of the game.\nYou can see an example here: an ATMSG analysis of the game DragonBox (PDF, 343 kb).\nWill ATMSG be useful in my project?\nThe ATMSG model can be a little bit complex and it has a steep learning curve to users with little or no experience with games 2. For this reason, it is recommended as a tool for the analysis of existing serious games when a thorough understanding of the characteristics of the game is needed, for example when adapting games for use in specific learning settings, or when detailing the analysis to identify and catalog learning patterns.\nIt can also be a helpful tool when evaluating game prototypes during the design process, providing a way for the design team to understand how well their game ideas can support the desired learning objectives before any code is written.\nInterested in this topic? Don’t hesitate to contact me!\nM. B. Carvalho, F. Bellotti, R. Berta, A. De Gloria, C. Islas Sedano, J. Baalsrud Hauge, J. Hu, and M. Rauterberg, “An activity theory-based model for serious games analysis and conceptual design”, Computers & Education, Volume 87, September 2015, Pages 166-181, ISSN 0360-1315, http://dx.doi.org/10.1016/j.compedu.2015.03.023.\nYou can find the author’s version of this paper in my Research & publications page.\n1. The best book I have read with a good and understandable explanation of the theory was “Acting with Technology”, by Victor Kaptelinin and Bonnie A. Nardi, particularly Chapter 3.","Posted by drmarkgriffiths\nIn offline situations, many social science researchers have employed ethnographic methods as a means of understanding and describing different culture and behaviours. However, ethnographic methods can also be used online for studying various types of excessive behaviour. For instance, I argued in a 2010 paper in the International Journal of Mental Health and Addiction that by being online, gaming researchers have the capacity to become a part of the phenomenon that is being studied. Recently, I along with a number of my research colleagues at Nottingham Trent University, published a paper in Studia Psychologia that online forums are providing a new and innovative methodology for data collection in the social sciences.\nFor those who don’t know, ethnography focuses on accounting for the actions and intentions of the studied social agents, and outlining how such behaviour is rationalized and understood by the wider group. Traditionally derived from anthropology, ethnography aims at studying people and their behaviours and cultures within their socio-cultural contexts. Behaviours and communications are engulfed with meaning by being situated within the field site. What is needed on behalf of the researcher is what Dr. Clifford Geertz described as the production of a “thick description” of what takes place in these field sites to discern the latter’s contextualized meaning.\nWhen it comes to virtual (i.e., online) ethnography, it is important to notice that while in-person ethnography is constrained by the laws of the physical world where the researcher needs to interact with the participants, online ethnography or as Dr. Robert Kozinets calls it in his 2010 book about online ethnography – “netnography” – can be done in a more unobtrusive way without the need to interact with the participants. Lurking is a possibility that Dr. Kozinets describes as opening a “window into naturally occurring behaviour” without the interference of the researcher.\nVirtual ethnography takes the idea of participant observation a step further by challenging the notion of a geographically bound and relatively stagnant field site by replacing it with the virtual sphere that has no set boundaries. In this respect, virtual ethnography is what Dr. Christine Hine says “ethnography in, of and through the virtual”. The Internet is used as place, topic and means of research. It is an important qualitative online methodology and has been used in a variety of different research endeavours, including the studies of people’s explorations of multi-layered identities on the Internet, different levels of online experience, and playing Massively Multiplayer Online Role-Playing Games such as Everquest.\nThere are a number of principles that underlie virtual ethnography. Ethnographers must immerse themselves in the (virtual) field site in order to gain an in depth insight into why and how interactions take place and what they mean within the context of the respective virtual sphere. However, the relationship of the latter’s agents to their real (embodied) lives cannot be disregarded. The online space is a space ‘in between’ that is connected to the world outside of the Internet. Therefore, virtual ethnography is but a partial study and cannot deliver a full account of what it sets out to study. It can never be a holistic approach. Nevertheless, this is aided by the researchers who must be reflexive about what they experience, and about the method they use. The involvement of the researcher and the researcher’s interpretation of the actions and communications that occur within the virtual field site are integral to this type of research. In addition to this, the technology of the Internet itself is essential because it provides the tools for, the objects and the context of analysis.\nGiven the wide variety of advantages of and important insights virtual ethnography can offer for the researcher, potential disadvantages also need to be taken into consideration. The researchers’ active participation in the field site offers them the possibility of in-depth insights that would not be possible without their involvement. However, at the same time, they might lose their critical distance towards the object of their study. Sacrificing some critical distance therefore is a trade-off for in the collection of invaluable and profound data. As Dr. Christine Hine notes, these data are necessarily biased by the researchers’ experience and perception of online interactions in the respective realm that, due to their immersion, is knowledgeable and familiar.\nDr. Adrian Parke and I applied online ethnography to the study of poker skill development within online poker forums, and published our findings in a 2011 issue of the International Journal of Cyber Behavior, Psychology and Learning. Our study was a virtual ethnographical research design looking at how poker gamblers utilized computer-mediated communication (CMC) to develop their poker skill and profitability, and to examine the factors associated with problem gambling. The study was a six-month participant observational analysis of two independent online poker forums. Dr. Parke participated in poker gambling during the entire study period and used strategies proposed from forum members to develop poker ability. This approach provided an insider’s perspective into how skill development through CMC affects poker gambling behaviour.\nWe generated forum discussions regarding specific behavioural concepts and cognitive processes based on accumulative analysis of emergent data from the online poker players. Forum interaction was observed, monitored and analyzed through traditional content analytic methods. Membership and participation in such online community forums provided poker players the opportunity to benefit from the consequences of reporting gambling experience and acquiring both poker gambling structural knowledge and skill.\nOne of the key advantages of data collection via online forums is that it can provide a detailed record of events that can be revisited after the event itself has finished. Furthermore, screen captures can be taken and used as examples or related back to the data collected – something that has been used in the gaming studies field (and outlined in a 2007 paper I published with Dr. Richard Wood in the International Journal of Mental Health and Addiction). Study findings can be posted on bulletin boards and participants have the opportunity to comment on their accuracy or comment on any other observations that they may have. This also helps to empower the participant and can prevent misrepresentation.\nGiven the increase in the number of hours we now spend online every day, carrying out research online is going to become an ever more popular (and useful).\nDr Mark Griffiths, Professor of Gambling Studies, International Gaming Research Unit, Nottingham Trent University, Nottingham, UK\nGeertz, C. (1973). The interpretation of cultures: Selected essays. London: Fontana Press.\nGriffiths, M.D. (2010). The use of online methodologies in data collection for gambling and gaming addictions. International Journal of Mental Health and Addiction, 8, 8-20.\nGriffiths, M.D., Lewis, A., Ortiz de Gortari, A.B. & Kuss, D.J. (2016). Online forums and blogs: A new and innovative methodology for data collection. Studia Psychologica, in press.\nHine, C. (Ed.). (2005). Virtual methods. Issues in social research on the Internet. Oxford, UK: Berg.\nHine, C. (2000). Virtual ethnography. London: Sage.\nKozinets, R.V. (2010). Netnography. Doing ethnographic Research Online. Sage: London.\nParke, A., & Griffiths, M.D. (2011). Poker gambling virtual communities: The use of Computer-Mediated Communication to develop cognitive poker gambling skills. International Journal of Cyber Behavior, Psychology and Learning, 1(2), 31-44.\nWood, R.T.A. & Griffiths, M.D. (2007). Online data collection from gamblers: Methodological issues. International Journal of Mental Health and Addiction, 5, 151-163.\nWood, R.T.A., Griffiths, M.D. & Eatough, V. (2004). Online data collection from videogame players: Methodological issues. CyberPsychology and Behavior, 7, 511-518.\nPosted in Addiction, Case Studies, Computer games, Cyberpsychology, Gambling, Gambling addiction, Gender differences, I.T., Online addictions, Online gambling, Online gaming, Psychology, Social Networking, Technological addiction, Technology, Video game addiction, Video games\nTags: Computer-mediated communication, Cyberpsychology, Ethnographic methods, Ethnography, Everquest, Massively Multiplayer Online Role Playing Games, MMORPGs, Netnography, Non-participation, Online ethnography, Online gambling, Online gaming, Participant observation, Poker, Poker skill, Research netiquette, Virtual ethnography"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:e94f1bad-afa6-4911-b2b2-eb5919a5101f>","<urn:uuid:f5744603-185a-4e85-93e9-0c4ad805b044>"],"error":null}
{"question":"How do the elevation characteristics compare between Bachelor Loop Road in Creede and the caves of South Dakota in terms of their tourist accessibility?","answer":"Bachelor Loop Road in Creede and the South Dakota caves have different elevation-related challenges for tourists. The Bachelor Loop Road starts at 8,907 feet and reaches 10,452 feet at its highest point, featuring steep switchbacks and gravel roads that can be challenging for visitors. Some sections are so steep that winter closures may occur. In contrast, the South Dakota caves' accessibility challenges were addressed through infrastructure improvements. Wind Cave's early tours required tourists to crawl through narrow passages, but these were later widened and fitted with wooden staircases. Similarly, Jewel Cave's initially restrictive low-ceiling passages were improved with concrete walkways, stairs, and an elevator installation to make the cave system more accessible to visitors.","context":["Length: 7.25 Miles Round Trip\nDifficulty: Moderate to Difficult\nFrom Alamosa, drive US 160 west for 47 miles to South Fork. Take a right on scenic Colorado Hwy 149 to Creede for 21 miles. Take a right in Creede on Main street, which will take you through town, up the hill into a steep walled canyon. Pass the Mining Museum and other buildings, take the Bachelor Loop Road until it forks. Numerous interpretive signs for reading about the mining history are along the road before officially entering the Historic Mining district. Depending on weather, the road may be closed in the winter, but could be cross country skied.\nThere are numerous places to park vehicles near the signs. Free parking. Pets are allowed. East Willow Creek road is the right fork. It receives much less traffic than Bachelor Gulch Road and is the preferred hiking/running/mountain biking route for the loop.\nBeginning elevation is 8,907 feet. CR 502 tops out at 10,452 feet before beginning descent back toward Creede.\nThis one lane gravel road travels past old mines and ruins of Colorado’s amazing mining history. Creede’s mines produced more silver than any other mining towns in Colorado. The steep walled canyons of the Bachelor Gulch road are not so pronounced on the East Willow Creek side as you will pass mines on the side of the hill as you get toward Phoenix Park, the site of an ore mill reclamation project. An interpretive sign is there. From that point, the road begins an ascent up through steep switchbacks toward the high point through pines and aspens. The last two miles back to parking lot are a steep gravel road past extensive mine ruins. Attempt to absorb the history.\nBoth available in the town of Creede. Water along trail with proper filter or treatments.\nEast Willow Creek Road is an easy gravel road for hiking and mountain biking in a narrow valley. This is a gradual ascent along the creek until Phoenix Park when the road leaves the valley and switchbacks of the mountain face. Good views of higher peaks emerge through the forest and then back south toward Creede will put you on CR 503 to Bachelor Gulch Road. The descent is steep through high walled cliffs and past massive old mining processing buildings. It is uniquely scenic.\nPleasures and Perils\nIt is the best place in Colorado to see historical mining activity, buildings and structures all in one place. The vertical geology of these intersecting canyons are like nothing else in Colorado. Dust kicked up by passing vehicles can make it unpleasant to breathe. Have a handkerchief to cover your nose. Old mines are for viewing only. They are dangerous especially for curious children. Be aware of 4 wheelers and vehicles on these roads. Tight canyons and this part of the San Juans can get big thunderstorms. Know the weather of the day.\nCreede, though far off the beaten path in Colorado, has as colorful of a mining history as Cripple Creek, Leadville and other mining towns, along with a cast of characters to complete the amazing historical past. It is well worth the trip!\nCreede is now mostly a summer tourist town with a Repertory Theatre, restaurants and shops. Its the perfect day trip for all sorts of activities. Grab a brochure and read up on the town!","Our family loves visiting caves – we’ve been to several across the United States! When we visit South Dakota back in 2004 we were excited to visit not one, but two caves.\nWind Cave National Park\nEstablished in 1903, Wind Cave National Park is located 11 miles north of Hot Springs, South Dakota and was the first cave to be designated as a national park. Wind Cave has the sixth longest cave system in the world with almost 140 miles of explored cave passageways, on the average four new miles of cave passageways are being discovered each year. Wind Cave is known for a rare calcite formation known as boxwork.\nThe Lakota Native Americans that lived in the Black Hills of South Dakota had known for centuries about the unique cave that blew air out of a hole in the ground. They consider the site scared and an old legend explains that they believe it was the place where they first emerged from the underworld where they lived before the creation of the world. In 1881, the first documented “discovery” of the cave was by two brothers, Tom and Jesse Bingham. The story is that they heard the wind blowing out of a 10×14 inch hole in the ground, and when Tom looked into the hole, the wind was blowing with such a powerful force that his hat blew off his head. When Jesse returned a few days later to show some friends, he looked into the hole, found that the wind had changed directions and his hat was blown into the cave.\nThis unusual phenomenon of a cave that “breathes” is created by the atmospheric equalizing pressure of the air both in and outside the cave. Rapid weather changes accompanied by rapid barometric changes are common in this area of South Dakota. When the air pressure inside the cave is higher than the outside, the air flows out. When the air pressure outside the cave is lower than the outside, the air flows in. This phenomenon in smaller caves with several large openings will go unnoticed, but Wind Cave is a large cave with very few openings and that is why the “breathing” of the cave is so obvious. It is possible on the day the Bingham brothers were at the site, a storm was approaching and the atmospheric pressure would have been dropping fast outside the cave causing the cave’s higher air pressure to rush out of the cave opening creating the wind, hence the name of Wind Cave.\nAs previously mentioned, Wind Cave National Park is known for a rare calcite formation known as boxwork. Boxwork is an uncommon mineral formation formed by erosion rather than water evaporation. As the walls of the cave begin to erode, the most resistant veins form thin blades of calcite that emerge from cave walls or ceilings intersecting at various angles which form honeycomb or box-like patterns. 95% of the boxwood formations in the world are found in Wind Cave.\nIn 1890, the South Dakota Mining Company took control of the Wind Cave site hoping to find valuable minerals, after a brief time the mining proved unsuccessful and no substantial and profitable minerals were found. Then, a local family, the McDonalds, began to develop the cave for tourism. In 1892 the cave was opened for visitors, the tour fee was $1 which was a very significant amount for that time. The guides would take the tourist down into the cave and explore by candlelight but these early tours were very physically demanding and the tourists sometimes had to crawl through very narrow passages. Eventually the cave passages were widened, wooden staircases were added and a hotel was built near the entrance to the cave.\nIn 1903, President Theodore Roosevelt signed a bill creating the Wind Cave National Park. The surrounding area above Wind Cave proved to be an excellent prairie habitat and in 1912 a national game preserve was established and fourteen bison, 21 elk and 13 pronghorn sheep were transferred from other areas of the country. The interest in the cave and the wildlife attracted an increasing number of visitors to the park and in the 1930s the Civilian Conservation Corps (CCC) made major improvements to the cave system by adding concrete walkways, stairs and an elevator, roads and other building structures were also built in the area.\nToday the Wind Cave National Park’s mission is to preserve and protect the natural resources of the 33,851 acre park. In the process of exploring additional areas of the cave system, park management has concluded that the cave is not an isolated environment. What happens above on the land in the surrounding area can greatly influence the cave and the way it continues to form, an example is that if the topography of the land is altered even slightly it might change the flow of water through the cave which will change the cave formations.\nThe Wind Cave Visitor Center is opened year round, except for Thanksgiving, Christmas and New Year’s Day. During the summer months, the visitor hours are expanded. All cave tours are ranger guided and there are several different types of tours that explore various parts of Wind Cave. Tickets are available at the visitor center and during the summer months there can be long lines and wait times, to avoid this inconvenience arrive early in the day to purchase tickets. Time can be spent waiting for a tour to start by a visit to the Visitor Center, there are many exhibits concerning the process of the cave formations, discovery and exploration of the cave and an 18-minute movie, “Wind Cave: One Park, Two Worlds” which is shown several times throughout the day.\nFor additional information regarding hours, available tours and prices please see the Wind Cave National Park page at the National Park Service website at www.nps.gov\nJewel Cave National Monument\nJewel Cave National Monument is located 13 miles west of Custer, South Dakota and it is currently the third longest cave system in the world with over 166 miles of explored cave passageway. Jewel Cave was formed when the limestone in the cave was gradually dissolved by water, the water served to enlarge the cracks that were formed when the Black Hills were formed approximately 60 million years ago. When the water that created the cave drained, the calcite formations started to form on the walls and ceilings of the cave, such as stalactites, stalagmites, flowstone and frostwork.\nIn 1900, Frank and Albert Michaud filed a mining claim and the brothers found a very small cave entrance, the hole was too small to get through and was enlarged several dynamite charges. When they finally entered the cave, cave passages with very low ceilings were covered with beautiful calcite crystal formations that sparkled like jewel by their lantern lights, hence the name of Jewel Cave.\nWhen the calcite crystals proved to have little commercial value, it became apparent to the mining company that the cave was a great natural wonder and they turned their attention to creating to creating a business venture that would profit instead by making it into a tourist attraction. Over the next ten years, a wooden trail was constructed inside the cave and a hotel was built nearby. Unfortunately, few people visited this area of the Black Hills of South Dakota. (remember this was long before Mount Rushmore was built)\nEventually the hotel closed and they Michaud family sold the claim to the federal government for $750. Meanwhile, a local organization was working to having the cave protected from further development and in 1908 President Theodore Roosevelt signed a proclamation naming the Jewel Cave National Monument. In 1933, the National Park Service began administering the monument in 1933 and park rangers from the nearby Wind Cave would lead cave tours during the summer months. In 1935, the Civilian Conservation Corps (CCC) made major improvements to the cave system by adding concrete walkways and stairs, improved roads and a public campground were also built in the area.\nAs of 1959, less than 2 miles of cave passageway were discovered. Then, in 1961, a geologist named Dwight Deal hired a husband and wife team, Herb and Jan Conn, with the specific purpose of exploring and mapping out new sections of the cave. With the discovery of the “Scenic Area” of the cave and an additional 15 miles of cave passageways mapped, the National Park Service became very interested in developing more tour routes for public and the original boundaries of Jewel Cave were expanded. As a result, more cave passageways were explored, the cave tour routes was extended and additional ones were added. Further trail improvements were made to the existing walkways, new ones were built, an elevator was installed and a visitor center was built. Exploration continues today and more information has been discovered regarding cave formations and efforts are being made to preserve and protect this natural wonder.\nJewel Cave is open year round, expect for Thanksgiving, Christmas and New Year’s Day. The National Park Service offers three different ranger-guided tours: the scenic tour, the historic tour and a special spelunking tour through an undeveloped section of the cave. Tickets are available at the visitor center and during the summer months there can be long lines and wait times, to avoid this inconvenience arrive early in the day to purchase tickets.\nFor additional information regarding hours, available tours and prices please see the Jewel Cave National Monument page at the National Park Service website at www.nps.gov"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:53604ec9-bf55-471e-9b86-7fa022d3c036>","<urn:uuid:0493f5bd-1bea-47b7-b5bc-2d8b7c7fe652>"],"error":null}
{"question":"What's the resting/rising time comparison between cannoli and croissant dough before shaping?","answer":"Cannoli dough requires a minimum 1-hour rest in the fridge. In contrast, croissant dough needs multiple rising periods: first a 3-hour rise until tripled in size, then another 1.5-hour rise until doubled in size, plus additional cooling periods in the freezer during the folding process.","context":["Ingredients for approximately 20 Cannoli: For the “shells”:250 g plain flour80 ml water30 g butter15 g icing sugar10 g unsweetened cocoa powder½ glass Passitoa pinch of salt --------------- For the filling:\n400 g sheep’s milk ricotta150 g icing sugar½ tsp cinnamon100 g bittersweet chocolate or 100 g candied fruit (orange peel, squash or according to taste)\nor half chocolate and half candied peel, or nothing, just the ricotta cream --------------- You will also need: an egg white (to brush over the shells), and plenty of extra virgin olive oil for frying them.\nCannoli are a traditional sweet delicacy and, in days gone by, were only prepared during Carnival season; however they are so good that they’ve become the best known and appreciated Sicilian sweet….. all year round! They have an outer shell of crispy, fried pastry, filled with a cream of ricotta (usually made with sheep’s milk) to which pieces of chocolate and/or candied orange peel (or squash) can be added, according to the recipe. To prepare them you need the right kind of \"cannelli\" (metal cylinders), or you can cut up “disposable” aluminium baking trays and make them yourself, or you can use bamboo canes, and cut them into pieces the right size.\nBefore you start preparing the “shells” – called “scorcie” in Sicilian – put the ricotta in a strainer resting on a bowl, to drain it of its liquid, and put it in the fridge. In another large bowl, sift together the flour, salt, cocoa powder and icing sugar. Add the butter (at room temperature) and the Passito; start stirring and, very slowly, pour in the water, mixing until it has all been absorbed. The dough must be soft and elastic, but compact (slightly harder than the consistency of bread dough). Knead it for a minimum of 5 minutes on your work surface, until it’s smooth and elastic; wrap it in cling film and leave it to rest in the fridge for at least an hour.\nTake the drained ricotta out of the fridge and press it through a sieve into a bowl, then add the icing sugar and cinnamon. Mix the ingredients delicately, without overdoing it, add the pieces of chocolate and/or candied peel (or, if you prefer, nothing); seal the ricotta cream in a container and put it in the fridge.\nTake the dough for the shells and roll it out into a 1-2 mm thick sheet (you can use a rolling pin, or put it through a pasta machine). Cut the sheet into squares measuring 9-10 cm and then wrap them round the cylinders; brush the edges with egg white so they stick together.\nHeat the oil in a small saucepan, or a fairly small, deep frying pan; when it’s nice and hot (try dropping a piece of bread into it to see if the temperature is right) fry the shells, a few at a time so as not to lower the temperature of the oil. They must turn golden brown, not burn! Remove them with a perforated spoon and lay them on a sheet of absorbent paper to get rid of any excess oil.\nWhen they have completely cooled down, remove the cylinders. Fill the shells liberally with the ricotta cream, using a spatula or knife, but only do so when you’re ready to serve them so that they don’t lose their crispiness. If you like you can decorate the ends of the Cannoli with candied orange peel or chocolate, a sprinkling of icing sugar on top and… you’ll sink your teeth into a sweet delicacy that takes a lot of work to make, it’s true, but tastes divine!","How to Make Croissants--homemade & delicious! (recipe w/pictures)\nLong-time HubBuddy jimmythejock asked me how I made some croissants, the pictures of which I had posted on Facebook. I can not claim the recipe—that goes to a Canadian baker named Sarah living in Poland—but I can attest, from personal experience, that this recipe and technique do indeed yield some pretty amazing croissants: flaky, buttery, delicate on the outside, and fluffy on the inside. Just take a look at the picture to the right, and tell me your stomach isn't growling!\nThe recipe is definitely time-consuming, but it is not very difficult, if you follow the instructions below carefully. I learned that each of the steps really exists for a reason. Yes, there are a lot of steps involved, but none of them are particularly difficult. You'll thank me for taking the time for each step when you bring out some golden brown croissants for your loved ones to scarf down.\nOther step-by-step baking guides\n- Make Bakery-Style Bagels the Easy Way, with Picture by Picture Directions\nHubBuddy K9keystrokes took pictures and posted this gorgeous and detailed recipe for making homemade bagels. Yes, you can make them at home and they are awesome!\n- How to Make Macarons (French Macaroons) - Step-by-Step Guide with Pictures\nThese delicate French treats are actually NOT impossible to make. I share the tricks that have helped me make them time and time again.\nFor 12 medium-sized croissants (smaller than the huge ones you typically see in cafes):\n- 1 3/4 cup all-purpose flour\n- 2 tbsp oil (I used olive, but something not terribly strong-tasting)\n- 1/2 cup milk (or any \"milk\" - I used rice milk and it turned out fine)\n- 2 tsp sugar\n- 1/2 packet active dry yeast (or 1 1/4 tsp)\n- 1 1/2 tsp salt\n- 1 stick (1/2 cup) unsalted butter\n- 1 egg\nClick on each picture for full-sizeClick thumbnail to view full-size\nMake the dough\nCroissant dough is a yeasted dough, meaning you have to let it rise a couple of times.\n- Put the yeast, 1 tsp of the sugar, and 3 tbsp of warm (105F/40C or cooler) water in a small cup. Mix together and set aside.\n- Put the flour in a large bowl.\n- Warm up the milk in a small bowl or cup, either using the microwave or on your stovetop. It just needs to be warm, not hot.\n- Add the salt to the milk and dissolve.\n- Add the oil to the flour and mix well.\n- Check the yeast mixture. It should have started foaming up a bit. Add it to the flour & oil mixture.\n- Add the milk & salt.\n- Mix it all together with a spoon or spatula until you have dough that holds together.\n- Take the dough out of the bowl, and begin kneading it. Knead it with the heels of your hands until the dough is uniform and sticky. Then, slap the dough hard against your counter about 8-10 times, until the dough is not sticky and has softened.\n- Clean out your bowl, and place the dough back in the bowl, Cover the dough loosely with plastic wrap, and then cover the bowl with another piece of plastic wrap.\n- Set in a warm place (about 75F/24C) for about 3 hours, until it triples in size.\n----3 hours later----\n- Pull out the dough and place it on your counter. Either using a rolling pin or your hands, roll/press it out into a square or rectangle, about 8-10 inches (20-30 cm) per side.\n- Fold the square into thirds, like you're folding a letter to put in an envelope.\n- Place this folded dough back in the bowl, cover it loosely, cover the bowl, and allow it to rise for another 1 1/2 hours or so, or until it has doubled in size.\n----1 1/2 hours later----\n- Remove the dough from the bowl, place on a cold plate, cover with plastic wrap, and place in the freezer (only for about 10-15 minutes, max).\nIncorporate the butter and start folding\n- Place the butter on your counter, and beat it with a rolling pin (yes, smack it). The goal is to beat it into a \"sheet\" of cold, but pliable, butter. It doesn't have to be a perfect sheet at all, just without huge lumps. Make sure it doesn't get so warm that it melts - it should stay a solid. (Pro tip! Place the stick of butter between two sheets of wax paper, and use your rolling pin to spread it out into a thin layer of cold butter. Avoids a lot of mess!)\n- Take the dough out of the freezer, and using your rolling pin, roll the folded-into-thirds dough into a square of similar size as last time.\n- Spread the butter into the top two-thirds of the dough square. Then fold the top third of the dough (with the butter) down, and then the bottom third up (just as you did before, but this time, you're encasing the butter in it). Ideally, both the butter and the dough should be the same temperature (cold but not freezing) and same texture (firm but pliable). Pinch the sides so the butter is full encased inside the dough.\n- Turn this \"folded letter with butter in it\" around 90 degrees. Flour your counter and the dough a little, and then using your rolling pin, very carefully roll out the dough into another square. You have to do this carefully to avoid liquefying the butter and having it squirt out! Take it slow and make sure your countertop is not warm!\n- Fold this square into thirds again, and, if the butter is still clearly firm inside the dough, roll it out again. If the butter has softened, cover up the \"folded letter\" and place it in the freezer for about 20 minutes so it firms up again.\n- Repeat this process 2 more times, cooling the dough in the freezer if you need to, to make sure the butter doesn't melt. After putting the butter in the dough, you should fold it into thirds and roll it out into a square four times. Naturally, the colder your kitchen and countertop, the less time you'll need to do this, since you'll need to toss your dough in the freezer less often.\nFinal rollout, cutting, and twisting\n- Refrigerate your dough a final time (30 minutes in the freezer, or 2 hours in the refrigerator).\n- Roll it out into a larger rectangle: 10x15 inches (25x40 cm)\n- Cut it into 3 rectangles of approx 5x10 inches (12x25 cm) each.\n- Place 2 of the rectangles on a plate and into the refrigerator.\n- Flour your counter, and roll the 5x10 rectangle into a larger rectangle of about 8x15 inches (20x40cm)\n- Cut that rectangle into half, and then each half into two triangles.\n- Roll each triangle into an isosceles triangle (two long equidistant sides, and a short third side)\n- Starting on the short side, roll up your dough triangle until you hit the tip.\n- Curl into a crescent shape.\nFinal rise, egg wash, bake & cool\n- Allow your shaped crescents to rise for 1-3 hours (or overnight in the refrigerator).\n- Preheat your oven to 475F (245C) - yes, pretty hot!\n- Place your crescents onto a greased cookie sheet or parchment paper.\n- Beat the egg with 1 tbsp of water.\n- Brush your crescents with the egg wash.\n- Bake for about 15 minutes, until the croissants have taken on a nice golden color.\n- Allow to cool on a rack for about 10 minutes.\n- ENJOY! I personally love them plain, but some like them with a pat of butter, some jam/jelly, or even Nutella.\nNutritional information on croissants\nMore by this Author\nMaking yogurt at home is easy, and does not require any special equipment. I've made yogurt dozens of times using this method, which I describe step-by-step with photos. Save money and have fun making your own yogurt!\nHow do you make bread with einkorn flour, and how does it taste? I've found a tried-and-true recipe that makes the most delicious, malty, nutty bread I've ever had.\nUpdated for 2017: Price per square foot for 20 prefab (modular) home companies' products, including Blu Homes, Proto Homes, ideabox (Ikea), weeHouses, and many others."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:12c36c60-e864-48e6-b80c-bcdf385bd43d>","<urn:uuid:b0d90572-f11f-4992-811f-b2d11e306bf2>"],"error":null}
{"question":"How do the reheating methods and safety guidelines compare between leftover rotisserie chicken and leftover salmon?","answer":"For rotisserie chicken, you can reheat it either in a 400 degree F oven for 15-20 minutes, or in the microwave at 75% power for three to five minutes, covered lightly with plastic wrap until heated through. For salmon, it must be reheated to an internal temperature of 145°F (63°C) to kill any bacteria, and should be heated slowly and evenly to avoid overcooking or drying out. This can be done using a microwave, oven, or stovetop. The reheating process for both meats should be done carefully to maintain their quality and ensure food safety.","context":["Meat Grinders Guide: How to Carve a Rotisserie Chicken\nTable of Contents\nA great pick up at your local grocery store to make your life a little easier in the kitchen is the purchase of a rotisserie chicken. It is very convenient, tasty and there are a lot of ways you can implement this protein into any kind of chicken recipe.\nA rotisserie chicken can be your best friend but handle it the wrong way and… this chicken can be your worst nightmare and become a hot mess. It is ideal to understand how to break down and remove all the tender and juicy meat from this wonderful chicken. This guide is here to break down how to carve a chicken teach you knife skills and throw some easy recipes to whip quickly on a stressful weeknight!\nHow to Properly Carve a Store-Bought Rotisserie Chicken\nLooking to extract the most meat out of your recently purchased rotisserie chicken? This easy step-by-step guide will assist you on receiving the most white meat as possible and will teach you how to carve a chicken like professional chef!! Make sure you start this process with a sharp knife and a carving fork to help you out.\n- Step 1 – On a large cutting board, set the chicken breast side-up and start by cutting any extra skin on the chicken body that you do not want to consume.\n- Step 2 – Pull a leg away from the chicken and cut the leg completely off. Repeat this process on the other leg. Then cut through the hip joint of both legs and separate the drumstick portion from the thigh.\n- Step 3 – It is time to remove the wings of the chicken. Pull each wing from the chicken and cut them off. Cut the portion of the wing where it is meeting the breast and slice each side off completely.\n- Step 4 – Make sure you are using your carving fork to help you slice the breast meat of the chicken. With the large carving knife make a horizontal cut in the breast portion of the chicken and cut down to the breastbone (right about the wing area). You want to move from the outside of the breast to the inside. You want to do this for both sides of the chicken body.\n- Step 5 – For extra flavors; top some seasoning to the white meat you have just sliced off for a nice touch. Either some rosemary or oregano will do just nicely!\nBest Rotisserie Chicken Ideas\nThe thing that is so great about a rotisserie chicken is the possibilities for roast chicken recipes that can come out of this bird. With all the chicken pieces that you’ll have, they are really easy to whip up for yourself or the family and they can make you look like a superstar in the kitchen. Here are some low-fat rotisserie chicken recipes that you will love! You can also add some veggies on the grill to your meat or make a salad, put everything on a serving platter and your friends will definitely think of you as the best home cook! Roasted chicken is extremely versatile, so take advantage.\nSummer Fresh Chicken Cob Salad Ingredients:\n- Eight slices of bacon\n- Two Tablespoons of extra-virgin olive oil\n- Two Tablespoons of salt and pepper\n- Two Tablespoons of balsamic vinegar\n- Two to three pounds of rotisserie chicken\n- Two heads of lettuce (chopped)\n- Four ounces of blue cheese crumbles\n- One pint of cherry tomatoes (sliced)\n- One avocado (chopped)\n- One red onion (chopped)\n- Four eggs (hardboiled and chopped)\n- Two to Three pounds of rotisserie chicken\n- Container of medium salsa\n- Container of sour cream\n- Package of monetary jack cheese\n- One avocado (chopped)\n- Two tomatoes (chopped)\n- Head of lettuce (chopped)\n- Taco shells\nDirections: This is really simple… ready? It is to make your own tacos. Serve all the ingredients in a presentable manner and have everyone make their own masterpiece. It is always a good time!\nFrequently Asked Questions\nHow do you reheat a rotisserie chicken in the microwave?\nYou can reheat a store-bought rotisserie chicken in a 400 degree F oven for 15 to 20 minutes, or you can reheat it in the microwave. Whether it’s whole or carved, you should place the chicken on a microwaveable dish and cover it lightly with plastic wrap. Heat it on 75% power for three to five minutes, until it’s heated through.\nHow do you cut up a rotisserie chicken?\nAs we’ve showed in this guide, the best way to carve a rotisserie chicken is to remove the legs first before separating the drumstick from the thigh. Then, you can pull off the wings before you slice off the breasts.\nWhat meals can you make with leftover rotisserie chicken?\nThe possibilities are endless once you’ve carved a rotisserie chicken. You can make chicken pot pie, fried rice, tacos, sandwiches, soups, quesadillas, casseroles, and more. In addition to using the chicken in cooked dishes, you can also eat the cold chicken as chicken salad.\nHow long does a store-bought rotisserie chicken last?\nThe best way to extend the shelf-life of a store-bought rotisserie chicken is to remove it from its original package. You can wrap it in heavy-duty aluminum foil or plastic wrap, or you can carve it and store it in an airtight container. When stored properly, it should last three to four days in the refrigerator.\nCan you freeze a whole cooked rotisserie chicken?\nIt’s best to carve the chicken before freezing it, as it will be easier to thaw and it will take less space in the freezer. Simply follow the instructions in this guide to carve your chicken and freeze the leftovers in an airtight bag for up to four months. We’d also recommend saving the carcass in the freezer as it makes excellent homemade chicken stock.","If you’re a salmon fan, you may wonder how long it lasts in the fridge. Whether you have leftovers from dinner or want to store fresh salmon, it’s essential to know how to properly keep it to ensure it stays fresh and safe to eat.\nAccording to the USDA, the good news is that Fresh salmon can last in the fridge for up to 2 days. However, Raw salmon has a shorter shelf life and will only stay fresh for about 2-3 days in your fridge or until the best-by date on the package. To extend the freshness of your salmon, it’s essential to store it properly in an airtight container and keep it at the right temperature.\nHow Long Does Fresh Salmon Last in the Fridge?\nFactors that Affect Salmon Freshness\nWhen it comes to fresh salmon, its shelf life depends on several factors, including the quality of the fish, the temperature at which it is stored, and how it is handled. Fresh salmon that is properly stored can last for up to 4 days in the fridge. However, if it is not stored correctly, it can spoil quickly, leading to foodborne illness.\nThe Shelf Life of Fresh Salmon\nThe shelf life of fresh salmon can be affected by several factors. One of the most critical factors is temperature. Fresh salmon should be stored in the fridge at a temperature of 40°F or below. Anything above this temperature can cause the fish to spoil quickly. It is also important to store fresh salmon in an airtight container or wrapped tightly in plastic wrap to prevent air exposure.\nAnother critical factor affecting fresh salmon’s shelf life is how it is handled. If fresh salmon is mishandled during processing or transportation, it can lead to bacterial growth and spoilage. It is best to purchase fresh salmon from a reputable source and handle it carefully to ensure its freshness.\nIn summary, fresh salmon can last up to 4 days in the fridge if stored correctly. To ensure the most extended possible shelf life, store fresh salmon at a temperature of 40°F or below in an airtight container or wrapped tightly in plastic wrap. Proper handling and purchasing from a reputable source can also help extend the shelf life of fresh salmon.\nHow to Store Fresh Salmon in the Fridge\nThe Best Way to Store Fresh Salmon\nWhen it comes to storing fresh salmon in the fridge, there are a few things you need to keep in mind to ensure that it stays new and safe to eat. The best way to store fresh salmon is to wrap it tightly in plastic or aluminum foil to prevent air from getting in. This will help keep the fish moist and prevent drying out.\nThe Importance of Proper Handling\nProper handling is also crucial when storing fresh salmon in the fridge. Always handle the fish gently to avoid damaging the delicate flesh. Keep the salmon in the coldest part of the fridge, usually at the back, to help maintain its freshness.\nThe Benefits of Airtight Containers\nAirtight containers are another great way to store fresh salmon in the fridge. These containers help to keep the fish fresh by preventing air from getting in and causing it to dry out. Airtight containers also help to prevent the salmon from absorbing any unwanted odors from other foods in the fridge. Storing fresh salmon in the fridge is a great way to keep it fresh and safe. By following these tips and using suitable storage methods, you can help maintain the freshness and texture of the fish while ensuring its safety.\nWhy Proper Salmon Storage is Important\nWhen it comes to storing salmon, proper storage is crucial to ensure its freshness and prevent foodborne illness. Here are the reasons why adequate salmon storage is essential:\nThe Dangers of Improper Storage\nImproper storage of salmon can lead to the growth of bacteria that can cause food poisoning. Scombroid poisoning is one of the most common types of food poisoning associated with salmon. This type of food poisoning is caused by the buildup of histamine in the flesh of the fish due to improper storage or handling.\nConsuming salmon that has not been stored properly can result in symptoms such as itching, headaches, nausea, and vomiting. In severe cases, it can even lead to hospitalization.\nBy following proper storage guidelines, you can ensure that your salmon stays fresh and safe to eat.\nHow to Tell if Salmon Is Bad?\nSalmon is a delicious and healthy fish that can be enjoyed in many ways. However, it can go well quickly if careful, and eating spoiled salmon can lead to food poisoning. Here are some signs to look out for to determine if your salmon has gone bad.\nSigns of Spoiled Salmon\nThe first thing to look for is the color. Fresh salmon should be pink or orange, but if it’s gray or brown, it’s a sign that it has gone bad. Another sign of spoiled salmon is a strong, unpleasant odor. Fresh salmon should have a mild, ocean-like smell, but it’s no longer safe to eat if it smells stinky or like ammonia.\nIf you notice any mold on the salmon, it’s time to discard it. Mold can cause various health problems, and it’s not something you want to mess around with.\nWhen to Discard Leftover Salmon\nKnowing when to throw it away is essential if you have leftover salmon. According to the USDA, raw salmon is good for up to two days in the refrigerator, while cooked salmon should be consumed within three to four days. If you need to figure out how long your salmon has been in the fridge, it’s better to err on caution and throw it out.\nHow to Safely Reheat Salmon\nIf you’re planning to reheat leftover salmon, it’s essential to do so safely. Make sure to heat it to an internal temperature of 145°F (63°C) to kill any bacteria that may have grown. You can use a microwave, oven, or stovetop to reheat your salmon, but do so slowly and evenly to avoid overcooking or drying it out.\nBy watching for these signs and following proper storage and reheating guidelines, you can enjoy delicious and safe salmon for days to come.\nCan You Freeze Salmon?\nIf you have a lot of salmon on your hands and need help with what to do with it, freezing is a great option. However, you should know the pros and cons of freezing salmon before you do it.\nThe Pros and Cons of Freezing Salmon\nOne of the main advantages of freezing salmon is that it allows you to store it for longer. If you have a lot of salmon, you won’t be able to eat it immediately; freezing it can help prevent it from going bad.\nHowever, there are some downsides to freezing salmon as well. One of the biggest concerns is the texture of the meat. When you freeze salmon, ice crystals can form inside the fish, damaging the texture and making it less enjoyable to eat. Also, freezing salmon can affect its health benefits, as some nutrients can be lost during freezing.\nHow to Freeze Salmon\nIf you decide to freeze your salmon, there are a few things you should keep in mind. First, ensure the salmon is as fresh as possible before freezing it. This will help preserve the texture and flavor of the fish.\nNext, wrap the salmon tightly in plastic or aluminum foil to prevent air from getting in. You can also place the salmon in a sealable plastic bag to help prevent freezer burn.\nWhen ready to use the frozen salmon, you must thaw it first. The best way to do this is to place it in the refrigerator and let it thaw slowly overnight. This will help preserve the texture of the fish and prevent it from becoming mushy.\nOverall, freezing salmon can be a great way to store it for later use. Just be aware of the potential downsides and take steps to minimize any adverse effects on the texture and health benefits of the fish.\nIf you’re wondering how long salmon last in the fridge, the answer is up to 3 days after it has been cooked. However, the freshness of the fish before cooking is a crucial factor in determining how long it will last. If the fish is fresh and in good condition before cooking, it can last up to 3 days in the fridge.\nRaw salmon, on the other hand, should be eaten within 3 days of thawing. If you have smoked salmon, it can last up to 2 weeks in the fridge. But it’s important to remember that eating spoiled salmon can cause serious health issues like food poisoning, scombroid poisoning, and ciguatera.\nTo ensure that your salmon lasts as long as possible, it’s essential to store it properly. Keep it sealed and refrigerated between 32°F and 40°F (0°C-4°C) to maintain its freshness. If you don’t plan to eat it within a few days, it’s best to freeze it.\nRemember, consuming spoiled fish can be dangerous, so it’s always better to err on the side of caution. If you need clarification on whether your salmon is still fresh, it’s best to throw it out."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:bbc7556c-4123-4214-b9ca-61af7516f34d>","<urn:uuid:0fc8f217-d4df-463c-a606-62c4370b4919>"],"error":null}
{"question":"I've noticed both Chinese and Russian influence in Central Asian history. How do the Tang Dynasty's control of Northern Kyrgyzstan in 744 AD compare to the Russian Empire's rule starting in 1867?","answer":"The Tang Dynasty's control of Northern Kyrgyzstan in 744 AD was relatively brief, lasting only until 751 AD when they were defeated by Muslim armies at the Battle of Talas and forced to retreat east of the Tian Shan mountains. In contrast, Russian rule was much longer and more transformative, lasting from 1867 to 1918 as part of Russian Turkestan. The Russian period brought significant changes including the arrival of railroads, Russian settlers, and conflicts over land and water resources. This led to the Basmachi revolt in 1916 and forced many Uzbeks, Kazakhs, and Kyrgyz to flee to China. The Russian influence continued well into the Soviet period, whereas the Tang Dynasty's influence was effectively ended by their defeat at Talas.","context":["history of Kyrgyzstan\nThe history of Kyrgyzstan extends from the Soviet Union to an independent country, on the Silk Road to the ancient Petrovsk. Kyrgyzstan has long been a historically important place in Central Asia, at the crossroads of trade routes and empires. Located between the Chinese, Persian, Arab, Indian, Turkish, and Russian empires, the land that forms Kyrgyzstan today has changed the history of many people, religions, cultures and travelers.\nThousands of years have been settled in Central Asia and the surrounding areas of Tian Shan, as the discovery of petroglyphs and archeology can be confirmed. Osh in southern Kyrgyzstan is one of the oldest settlements in Central Asia and has been known since ancient times. Some of the early settlers were nomadic infidels who practiced an ancient monotheistic religion, Tigrinism, centered around nature. Until the Battle of Tulsa, in 751 CE, between the Chinese Tang Dynasty and the Arab Abbasid Caliphate, Central Asia was largely Buddhist territory, though other religions and cultures were famous because of travelers on the Silk Road. The battle of Talus is a turning point, from which Islam began to become the dominant religion and influence in the region. The Karkhanis were one of the earliest Muslim kings, and they mixed many of the old Turkish elements with Islam. The factories were in power from the 9th to the 11th centuries, during which they built the Burana Tower (all that remains of their capital, Balsagne) and the tombs in Uzhgan.\nWith the Mongol conquest of Asia in the 13th century, today’s ethnic Kyrgyz people migrated from the Siberian River Yancy to their present home of Tian Shan. Tian Shan ruled the Mongols for several hundred years in various forms, subjected to climaxes, orates, and dzingers, depending on who was killed recently.\nIn the 18th century, King Raj reached its largest size in China, and Orites became an important state. With the rise of Kokand in the early 1700s, Kyrgyzstan came to power under the Khanate. During these past centuries in Kyrgyzstan’s history, the region played an important role in keeping the travelers crossing Silk Road Asia. Visitors can still see Tash Rabat, a stone caravanserai (like hotels) in Noreen Province, starting in the 15th century. There are also many land impacts that enter the languages and cultures (and even DNA) of the people living in the area, possibly from traders and travelers. And although Manas’s epic is known for being very ancient, the events it presents are similar to those of the 16th and 17th centuries.\nCentral Asia was right in the middle of the great game of royal expansion in the 1800s, played between Russia from the northeast and Britain from the south. At that time, Kokand’s mines were very weak, and therefore smaller regional rulers had significantly more power. When Aleksema Ditka, who ruled Alai (now in southern Kyrgyzstan), was killed in the Palace revolt, his wife, Strong Kermanjan, became the latest leader in 1862. As the Russians got closer and closer, Kermanjan Dutka requested a peaceful transfer, and in 1867, the Alai region was annexed by the Russian Empire. A 2014 film depicts her life story (as well as the amazing scenes in which she lived), which is now considered an important part of Kyrgyz history.\nFrom 1867 to 1918, Kyrgyzstan was part of Russian Turkestan, which was the Governor General’s Ship in the Russian Empire. Turkey has been a colonial outpost for many years, separated from the capital in St. Petersburg, but by the end of the 20th century, the arrival of railroads brought more Russian settlers, who had limited land and water resources. Stress. This resulted in a Basmaki coup in 1916, followed by severe retaliation on the Russian language. Many Uzbeks, Kazakhs, and Kyrgyz fled across the border to China, both after the Rebellion in 1916 and the forced arrival of the Communist Party in 1918.\nAfter the establishment of the Soviet Union in 1917, Turkey is divided into oblasts by approximately las race. Since many people were nomads, and many people identified more than their religion, city, or profession, it was difficult to draw borders, and many populations ended up out of their average nation (that’s why here There is a large population of Uzbeks in southern Kyrgyzstan today). Kara Kyrgyz Autonomous Oblast was founded in 1924, replaced only in 1926 by the Kyrgyz Autonomous Socialist Republic. Both of these entities were part of the Russian Socialist Federation of the Soviet Republic. In 1936, the Kyrgyz Soviet Socialist Republic was established, now ruled by the Kyrgyz branch of the Communist Party in Freon, the capital of Bishkek. One of the most notable figures coming from Soviet Kyrgyzstan was Genghis Imatmatov, a famous politician, diplomat, and author.\nOn August 31, 1991, the Republic of Kyrgyzstan declared independence from the Soviet Union. Since 1990, President Asker Akyev has become the president of the new Republic, holding office until the Tulip Revolution in 2005. Protests in 2010 were unstable until Bakiyev was ousted from power, which replaced Akayev. In Osh, tensions erupted between ethnic Kyrgyz and Uzbeks, and recalled similar riots in the 1990s. Rosa Otunbayeva became interim president in April 2010, and she became one of the few female leaders in the Muslim-majority country. After Almazbek Atambayev took office in 2011, he also became the first Kyrgyz leader to peacefully hand over power. Since then, Kyrgyzstan has been relatively stable, even hosting two World Nomad games, even in 2014 and 2016.","The Chinese general Chang Chien founded the Silk Road in 138 B.C when he traveled to the Fergana Valley. Soon after Chang Chien%u2019s expedition, the Chinese Empire began officially trading with city states in Fergana. The cities of Osh, Uzgen, and Jul in Kyrgyzstan were at the center of this prominent trading route, making them important commerce centers. Pictured above is an ancient caravansary in Kyrgyzstan on the Silk Road. This caravansary served as a hotel or inn for travelers from the west.\nThe Huns Take Control of Kyrgyzstan\nPrior to this point, many different nomadic empires had occupied the area of what is now modern day Kyrgyzstan. There is evidence that nomads occupied the area as early as 3000 B.C. The Usun ruled part of Kyrgyzstan until the Huns (or Hsiung-nu) took over in the 300s. The Huns, a group of nomadic clans from Mongolia, ruled much of Central Asia at this time. They were ruled by a fierce and aggressive leader named Attila, whom is pictured above.\n1st Jan, 0751\nThe Beginning of Islamic Influence\nIn 744 A.D. the Tang dynasty from China controlled Northern Kyrgyzstan. The Tang dynasty would have conquered more of Kyrgyzstan, but they were stopped by Muslim armies. In 751 A.D., a huge battle ensued between the Chinese army and the Muslim army. At the Talas River, the Chinese were defeated and retreated to east of the Tien Shan mountains. This was one of the most monumental battles in Kyrgyz history because it marked the beginning of Islamic ways in Kyrgyzstan.\n1st Jan, 0900\nThe Kyrgyzs Migrate to Kyrgyzstan\nAround 900 A.D., the nomadic Kyrgyz people began to migrate to what is now Kyrgyzstan from the Yenisey River in Siberia. Specifically, the places they moved into Kyrgyzstan are the Chu, Fergana, and Talas valleys. They dwelled in a portable tent-like structure called a yurt. It is believed that the origins of the Kyrgyz people are closely tied to the Kazkah, Karakalpak, and Turkic people. The migration was a gradual process, taking two centuries to complete itself.\n1st Jan, 1219\nGenghis Khan Invades Eastern Asia\nIn 1219, Genghis Khan, sparked by an attack on the Mongols, began to conquer Central Asia. Genghis Khan, the leader of the Mongols, gained a reputation as being a merciless and ruthless leader. He executed any who would resist, including the Kyrgyzs. However, their resistance was futile so in the effort to sustain their existence, they surrendered. After Genghis Khan%u2019s death in 1227, his sons took over and continued to rule for 150 years.\nThe Khans of Kokand Take Control\nSoon after the departure of the Kalymyks in the early 1700%u2019s, an Uzbek tribe called the Ming built a palace in Kokand, in the Fergana Valley. By 1762, they expanded into southern Kazakhstan, eastern Uzbekistan, and most of Tajikistan and Kyrgyzstan. Despite the fact that the Kyrgyz tribes frequently resisted, they did not have the technique or tools to escape the Kokand regime. The Kyrgyz often were assisted and protected by the Russians, who in the 1800%u2019s would take over most of Central Asia.\nRussian Occupancy of Central Asia\nIn 1876, due to Russian intervention, the Khanate of Kokand was abolished. Russian armies had slowly been taking control of Kyrgyz cities, but now that the Khans were no more, there was complete Russian occupation. Though the Russian invasion was more welcome than the Khanate of Kokand%u2019s invasion, the urbanization that came disrupted the nomadic ways of the Kyrgyz people. Private land ownership affected transhumance (transferring livestock), and brought ethnic diversity.\nKSSR Is Established\nIn 1924, 7 years after the February Revolution which ended the tsar%u2019s rule, Kyrgyzstan was assigned the Kara-Kyrgyz Autonomous Oblast. In 1936, this was elevated to the Kyrgyz Soviet Socialist Republic. This was the first time that the Kyrgyz people had their own specific piece of land, creating a nation. This change brought some benefits to %u201CKyrgyzya%u201D, such as improved literacy rates. However,many problems came, like when the USSR collected all farm land, destroying traditional nomadic ways.\nKyrgyzstan Declares Independence\nShortly after tensions between Uzbeks and Kyrgyz began to unravel, the Soviet Union collapsed. During a coup against Mikhail Gorbachev, on August 31, 1991, Kyrgyzstan declared independence. Askar Akayev was elected (running unopposed) as the new leader for Kyrgyzstan. Akayev was the only leader in Central Asia not in the Communist Party. Many new things came to Kyrgyzstan, such as independent media and new political parties. Problems came as well, one being a depleating economy.\nAskar Akayev: The Falling Out\nAskar Akayev%u2019s true intentions were beginning to emerge. In 2000, he had barred former Vice President Kulov from the election, and had him jailed under very vague charges. Akayev consistently resented and was rude towards reporters who questioned his actions, he was very reluctant to relinquish his power. This combined with voting irregularities sparked protests beginning February 2005. In March, Akayev fled the country and in April resigned, having President Kurmanbek Bakiyev take his place."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:8896c6f1-60de-4976-97db-ebfe8b4888e4>","<urn:uuid:b9787132-1251-49a7-88bc-9e77758288bd>"],"error":null}
{"question":"Are monitor gamma and color contrast the same thing? Need to know for my web design class","answer":"No, gamma and color contrast are different display properties. Gamma refers to the relationship between a pixel's numerical value and its brightness, defining how smoothly black transitions to white on a display. It's measured in standards like 1.8 or 2.2 and affects whether black appears truly black rather than gray. In contrast, color contrast refers to the difference in perceived luminance between two colors, measured in ratios from 1:1 (white on white) to 21:2 (black on white). Color contrast is particularly important for accessibility and is regulated by standards like WCAG to ensure text is readable for users with visual impairments.","context":["Monitor calibration refers to the accuracy of the different properties of a display. The more accurate an image is to its true colors, the better. This is why the people working with professional arts, designing, video, and photography-editing use expert tools, both software and hardware, to calibrate their monitors and achieve the best possible results.\nThe Windows operating system comes with its own integrated display calibration tool known as “Display Color Calibration,” using which an end-user can calibrate all of their monitors to obtain colors as accurate as the real ones, and even have the same color output across the different display devices.\nHowever, in this post, we are going to discuss several online tools that you can use to calibrate the different settings of your display. These tools can be beneficial for both Windows and non-Windows users since they offer more precise images and context to adjust the various monitor settings.\nTable of contents\n- Monitor Display Calibration Settings\n- How to Calibrate a Monitor\n- Best Tools to Calibrate Your Monitor\n- Challenge Your Color Perception\n- Closing Words\nBefore we discuss our top picks for the tools to use to calibrate your monitors, let us have a look at how to calibrate them, and what settings to adjust.\nMonitor Display Calibration Settings\nThere are a few things that you will need to adjust on your display to get the most accurate results. Some of these settings can be adjusted from the Windows Settings app, while almost all of them can be adjusted from your monitor’s built-in menu using the buttons. We have discussed the “how” part in the section below.\nLet us have look at what settings need adjustment.\nBy definition, gamma is the relationship between the numerical value of a pixel (between 0 and 255) and the pixel’s brightness. The correspondence between this relationship defines how smoothly black transitions to white on a display device.\nThere are different standards of gamma, such as 1.8, 2.2, etc. This number defines the curvature of the transition from black to white. We say “curvature” because the numerical values of the pixel do not define the brightness of the pixel linearly.\nThe table below will help you understand the different gamma standards:\nIn layman’s terms, adjusting the gamma correctly would show black as true black (and not gray) and white as true white.\nBrightness is referred to as the intensity of light coming through each pixel. Adjusting the brightness correctly would display the true red, green, and blue colors, especially when seeing grayscale images and text.\nWhen talking about a display’s contrast, it means color contrast. A monitor’s contrast ratio indicates the depth of the darker colors, like blacks. The higher the contrast, the greater the depth of the color.\nAs an example, the human eyes perceive white text on black (and vice versa) as the best reading setting. This is because the contrast of the two colors complement one another and are greatly distinguished.\nA display has 3 primary colors: Red, green, and blue. These 3 colors make up all of the other colors on each pixel on your screen. The right balance between these colors would only display the true colors of an image on your screen.\nIt is not always necessary that the same numerical value of these colors will show true colors, as each display has a different setting. Therefore, adjusting them to obtain a result as close to the real colors is important to get the best accuracy.\nThese 4 are the major components of optimizing your monitor display settings, apart from the screen resolution and scaling. However, a monitor’s internal settings may offer more control, such as adjusting the display’s sharpness. If yours does, we suggest that you should also adjust those, but only after adjusting the major settings discussed above.\nLet us now see how to adjust your monitor’s settings.\nHow to Calibrate a Monitor\nThe Windows Settings app offers some display calibration, but those are very limited and not ideal to perform a real calibration. From the Settings app, you can only adjust the screen brightness and some other display settings, but not the gamma, color balance, or contrast ratio.\nHowever, the Windows OS does come with its display calibration tool.\nDisplay Calibration Tool\nThe Display Color Calibration utility lets you calibrate the display settings we discussed above that you can do manually with the help of images inside the tool. It offers a complete guide on how to adjust these settings to obtain an ideal calibration.\nUsing this tool alone might not be sufficient for professionals who want the best of the best display calibrations. Therefore, you can use this tool to adjust the display settings alongside the online tools listed below. This way, you can make the adjustments using Display Color Calibration while monitoring live results using the colors and images in those online tools.\nTo open Display Color Calibration, type in dccw in the Run Command box.\nWe also have a dedicated guide on how to calibrate your monitor using the Display Color Calibration tool.\nThe other option to calibrate your monitor is with the help of the On-Screen Display (OSD) menu. This is the menu you can bring up on your screen using the buttons imposed on the monitor itself.\nDepending on how advanced your monitor is, it gives you several options to adjust, including gamma, color balance, brightness, sharpness, etc.\nTogether with the OSD menu and the online tools listed below, you can calibrate your display to obtain the best accurate results possible.\nBest Tools to Calibrate Your Monitor\nWe have listed down these online tools in the order that offer the best calibration options to the least. However, all of these tools offer images and context that will help you calibrate your display accurately.\nLagom LCD Monitor Test Pages\nThe Lagom monitor test pages are a compilation of 13 different pages with comprehensive images that will assist you in adjusting the different display settings of your monitor. It has a separate page for adjusting the following list settings:\n- Display resolution\n- Clock and phase for VGA\n- Black levels\n- White saturation\n- Black and white gradient shifting\n- Color inversion\n- Monitor response time\n- Viewing angle\n- Contrast ratio\n- Subpixel layout\nOpen each of these pages and adjust the corresponding display setting from the Display Color Calibration tool in Windows or the OSD menu. If confused, read the instructions given on each page to obtain the optimal calibration results.\nMoreover, the creator of this website has also given a downloadable ZIP file that will download all the images which you can then carry with you when shopping for a display device.\nAll these things make Lagom test pages our number one pick.\nPhotoScientia Gamma Assessment\nThe PhotoScientia Gamma Assessment tool is a gamma adjustment tool only. It provides images according to the different standards, and then you can adjust your device’s gamma to those standards.\nTo adjust your gamma, we suggest that you use this tool as this website is fully devoted to gamma. Adjust the gamma setting using the OSD menu or the Display Calibration Tool until all the squares match up with their backgrounds as closely as possible.\nOnline Monitor Test\nThe Online Monitor Test offers a series of different color calibration pages. It has a different page for different kinds of tests, like one for grayscale colors and gradients, another for different colored gradients, and so on.\nIt has a total of 16 pages to test and adjust different color-related monitor settings. It can also be used to determine blacklight uniformity across the entire display, as well as individual primary colors, and their smoothness in transition in gradient pages.\nPhoto Friday is a basic single-page website that offers different images on the same screen to adjust your monitor’s brightness and contrast. This tool is used mostly by photographers to obtain real results after editing their images.\nHowever, Photo Friday cannot be used to calibrate your monitor as you cannot use it to balance the colors, nor can you adjust your gamma settings.\nW4ZT is an all-in-one monitor calibration tool that can be used to adjust your screen’s color balance, contrast, gamma, and brightness. However, the images and content on this website are limited compared to the tools we discussed earlier.\nSimply scroll down the page and use the different images to adjust your monitor’s settings accordingly. You will also find on-screen instructions on how the images should appear after calibrating your display.\nChallenge Your Color Perception\nBy now you should be able to calibrate all your monitors perfectly. However, one thing that you also may have noticed is that all of these calibration settings depend on your ability to perceive the different colors and contrasts. The calibration will hence only be as good as your ability to identify minor color variations.\nHence, if in doubt, we recommend that you take a quick test to check your color-identifying abilities.\nThe X-Rite Color Challenge is a color-hue test where you need to adjust the color boxes in the correct gradient.\nThe color boxes on both ends of the rows are fixed – move the boxes in the middle to attain the perfect color gradient. When you are done, click Score my test to get the results.\nThis test will help you judge your color-identifying abilities.\nIf you score low in the X-Rite Color Challenge, then we suggest that you ask someone else to calibrate your devices for you.\nIf you are a professional who needs picture-perfect monitor calibration, then we suggest that you opt for calibrating hardware which includes cameras and other sensors that automatically calibrate your monitor. These include hardware from Datacolor, Calibrite, and Wacom. Of course, this option will cost you.","Color Contrast Checker\nThe tool tests the contrast ratio of background and text colors for accessibility. You can use it to visualize different color combinations for your website design that are in compliance with Web Content Accessibility Guidelines (WCAG) and international legislation based on it like the EU Web Accessibility Directive, the Americans with Disabilities Act (ADA), or the Accessibility for Ontarians With Disabilities Act (AODA).\nColor Contrast Tool Guide\nHow to Interpret the Color Contrast Ratios\nWCAG 2.1 Level AA\nLarge text - minimum contrast ratio of 3:1.\nWCAG 2.1 Level AAA\nLarge text: minimum contrast ratio of 4.5:1\nPlease note that incidental text such as images that are purely decorative or part of an inactive user interface component, and logotypes, such as parts of a logo or brand name, have no minimum contrast requirement.\nWhy Use a Color Contrast Checker?\nUse Monsido’s web color contrast checker to quickly check color combinations, and ensuring that all your branded content assets and design elements are accessible to everyone. It can also be used to test color contrast with other legislation, e.g. as an ADA contrast checker.\nNeed additional help?\nSee how we can help you make your website more accessible. Book a free web accessibility scan today.\nFrequently Asked Questions\nColor is such a vital element of web design as it is used to convey personality, attract attention, symbolize action, and indicate importance. As color carries a lot of significance in both being visually pleasing as well as conveying meaning, users must be able to perceive the content of different colors on a webpage. Color accessibility is important as it helps users with visual impairments like low vision or color blindness to properly distinguish content elements and read/view them effectively.\nA color contrast checker is a tool that measures the difference in perceived luminance between two colors to ensure that is perceivable to users with visual impairments or insensitivity.\nThe difference is measured on a ratio scale that starts at 1:1 (for white on white, to 21:2 (black on white). According to the WCAG level AA, the minimum contrast ratio that colors can have for the visual presentation of text and images of text is 4.5:1.\nWCAG 2.1 Level AA requires the visual presentation of text and images of text to have a contrast ratio of at least 4.5:1, except for large text, which should have a minimum contrast ratio of 3:1.\nWCAG 2.1 Level AAA requires a contrast ratio of at least 7:1 for normal text and 4.5:1 for large text (14 point and bold or larger, or 18 point or larger).\nImages must pass the WCAG contrast requirements. Images that contain text must ensure that the contrast between the image background and the text is sufficient, especially if the images are of low quality and if the image needs to be enlarged in any way. Images of text must have a minimum contrast ratio of 4.5:1.\nFor images that do not contain text, but still convey meaning, the image components must still have sufficient contrast to ensure that the overall image is perceivable. WCAG 2.1 level AA specifies that graphical objects and author-customized interface components such as icons, charts, and graphs, buttons, form controls, and focus indicators and outlines, have a contrast ratio of at least 3:1."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:7e778c4b-8ccc-4479-982e-dbc4eb2ac171>","<urn:uuid:829f0ada-f08f-4987-835c-90a206d14cd8>"],"error":null}
{"question":"As a fertility specialist, I need to understand both the diagnostic procedures and treatment options for tubal blockages. What are the main diagnostic tests available, and how do they differ from surgical interventions for treating blocked tubes?","answer":"The main diagnostic tests for tubal blockages include laparoscopic chromotubation (using methylene blue dye to check tube patency), hysterosalpingogram (HSG), falloscopy (catheter-based examination inside tubes), and sonohysterography (ultrasound imaging with fluid injection). For treatment, surgical interventions include laparoscopy to cut away scar tissue, recanalization using tiny wires, salpingectomy to remove or seal blocked tubes, tubal reanastomosis to connect healthy sections, fimbrioplasty to reshape tube ends, and tubal cannulation using catheters. Both HSG and laparoscopic chromotubation can serve therapeutic purposes by sometimes opening blocked tubes during the diagnostic process. If tubes are blocked beyond repair, IVF is generally recommended as the primary treatment option.","context":["Hysteroscopy is a diagnostic test that makes use of a thin telescope-like hysterescope to view and operate upon the endometrial cavity. Carbon dioxide is filled into the cavity to aid this process. While often hysteroscopy can be done as an outpatient procedure, some women may need local anesthesia. In some cases, hysterescopy is done along with a resectoscope. But this procedure destroys the uterine lining and is not a viable alternative for women who wish to have children. A laparoscope may be used to view the uterine exteriors.\nDiagnostic hysteroscopy involves observation of the endometrial cavity for any abnormalities. This procedure is often used in cases where there has been abnormal uterine bleeding or repeated miscarriage. Diagnostic hysteroscopy may also be used to confirm the results of HSG. Hysterescopy may be used to check for causes of heavy or irregular menstrual cycle or fit IUD.\nOperative hysteroscopy involves use of hysteroscope to remove polyps, cut adhesions or treat fibroids and septums. This can be used as an alternative to open abdominal surgery. This involves use of operative hysteroscope that allows the physician to insert operating tools. In rare cases, hysterescopy may lead to infection and heavy bleeding or injury to the cervix or uterus.\nOviduct blockage is the blockage in one or both microscopic fallopian tubes that allow a woman's egg to pass from her ovaries to her uterus. The blockage is an impediment for the egg to migrate, implant and begin pregnancy. The result is infertility. In rare cases, even if pregnancy occurs it can be dangerous, if not treated immediately.\nDiagnosing oviduct blockage\nLaparoscopic chromotubation: Usually done after sedating with a mild anesthetic, laparoscopy involves making a very small incision in the belly button and near the pubic bone area; a small camera helps view the tubes. Laparoscopy helps detect endometriosis, adhesions, and ovarian cysts and also check the tubes.\nChromotubation involves infusing diluted methylene blue dye solution into the uterine and tubal lumen. If the tubes are not blocked, the dye should come out of the ends of the tubes into the peritoneal (abdomen) cavity. This test is considered the most reliable way to determine oviduct blockages.\nBoth laparoscopic chromotubation and Hysterosalpingogram can sometimes open a blocked tube; hence these procedures can be both diagnostic and therapeutic. The rate of fertility is likely to improve after the procedure.\nFalloscopy: The newest form of endoscopic examination, falloscopy is helpful to look inside the fallopian tubes. Using a catheter-based system, a flexible tube is inserted through the vagina and cervix which threads through one of the fallopian tube. Very similar to hysterosalpingogram, it allows viewing of tubal walls and checks if it is healthy and also detects obstruction, if any.\nSonohysterography: This is a non-invasive procedure wherein fluid is injected through the cervix into the uterus, and ultrasound imaging is used to determine if any abnormality is present. The procedure is extremely helpful in detecting underlying cause of many problems such as abnormal uterine bleeding, infertility and repeated miscarriage. Like the other diagnostic procedures, sonohysterography is done when the woman is not having her menstrual cycle.\nOviduct blockage treatment\nThe goal of treating oviduct blockages is to unblock fallopian tubes and increase the chances for a successful pregnancy. Surgical procedures are the primary treatment option to open blocked oviducts. If needed, doctors will use more than one procedure to treat oviduct blockage. Take a look at the various surgical and non-surgical options available today to treat this particular condition.\nLaparoscopy: To treat oviduct blockages, laparoscopy is widely preferred. It involves inserting the scope into the abdomen and cutting away scar tissue which blocks the tubes and is a result of infection and/or endometriosis, primary causes for oviduct blockages. It helps in unblocking the oviducts and allows the eggs and sperm to meet and facilitates the egg to become fertilized.\nRecanalization: A tiny wire is inserted into the tube to remove the blockage.\nSalpingectomy: It involves removing a blocked fallopian tube or sealing it in order to maximize the functioning of the second, unblocked tube.\nTubal Reanastomosis: This is also a laparoscopic procedure in which small incisions are made through the abdomen. The blocked portion of the oviducts or fallopian tube is cut away and the healthy sections of the tube are connected. It is followed by a procedure called Salpingostomy to create a new opening in the tube close to the ovary.\nFimbrioplasty: This plastic surgery that facilitates to reshape ends of fallopian tubes closed off by scar tissue or some other blockage.\nTubal Cannulation: Is a non-surgical option which involves clearing blockages with the use of a catheter, or Cannula that is inserted through the uterus into the fallopian tube.\nThe placenta usually separates from the uterus after the birth of the baby. Placental abruption is a condition where the placenta separates from the uterine wall during the pregnancy. Placental abruption is a serious condition and can put the baby at risk. This is a medical emergency. Hypertension can sometimes lead to placental abruption. Women who have blood-clotting disorders may experience placental abruption. Women suffering from diabetes or abusing drugs are also at higher risk for placental abruption. Those women who have had multiple pregnancies are also at higher risk for placental abruption.\nA pregnant woman suffering from placental abruption is likely to have abdominal and back pain. There might be rapid uterine contractions. There is tenderness in the abdomen. Uterine bleeding is often noticed. Placental abruption can occur anytime after the 20th week of pregnancy.\nAn ultrasound can help in locating any possible blood clot behind the placenta. It can also help in checking for any signs of fetal distress. Fetal monitoring is essential since the placenta supplies nutrients to the growing fetus. There may be decreased fetal movements. The extent of placental abruption decides the course of treatment to be followed. Partial placental abruption needs adequate bed rest and close monitoring. In cases of total placental abruption, delivery of the infant is undertaken. This is either with vaginal delivery or cesarian section. But there is a risk of premature birth and fetal death. The newborn baby could suffer brain damage due to low levels of oxygen in the blood.Tags: #Hysteroscopy #Oviduct Blockage #Placental Abruption\nEnter your health or medical queries in our Artificial Intelligence powered Application here. Our Natural Language Navigational engine knows that words form only the outer superficial layer. The real meaning of the words are deduced from the collection of words, their proximity to each other and the context.\nDiseases, Symptoms, Tests and Treatment arranged in alphabetical order:\nBibliography / Reference\nCollection of Pages - Last revised Date: September 24, 2022","Tubal disease at a glance:\n- Tubal disease — when a fallopian tube becomes blocked or damage — is a common cause of female infertility.\n- Tubal disease has a variety of causes including endometriosis, infection or surgical damage to the tube.\n- It is diagnosed and corrected in several ways, although IVF, which bypasses the fallopian tubes, offers the best chance of achieving pregnancy.\nWhat is tubal disease?\nTubal disease is a common cause of female infertility. A woman’s fallopian tubes may become blocked from endometriosis, prior pelvic surgery or infection, or unexplained reasons.\nTreatment of tubal factor infertility depends on the cause, whether or not the condition is correctable, and the goals of the patient.\nA hysterosalpingogram, or HSG, is commonly used to screen for tubal abnormalities. If abnormalities are detected on the HSG or if the findings are inconclusive, laparoscopy (surgery) may be used to definitively diagnose and sometimes treat tubal disease.\nIf the tubes are blocked or damaged beyond repair, IVF is generally the method required to achieve pregnancy. In fact, if blocked tubes are the only abnormality discovered during an infertility investigation, then the couple has a very good prognosis for pregnancy with IVF.\nTypes of tubal disease & treatments\nHydrosalpinx is a condition in which the fallopian tubes are blocked at the end where they meet the ovary. If both tubes are blocked this way, it is nearly impossible to get pregnant without assistance. In addition to an HSG, an ultrasound may be used to diagnose this problem. View a surgical photograph of hydrosalpinx here.\nTreatment of hydrosalpinx depends on the goals of the patient. Laparoscopic surgery can be used to create a new opening at the end of the tube, in a procedure called neosalpingoscopy.\nThe success of this procedure depends on the severity of the dilation, surrounding scar tissue and whether the tube is otherwise normal.\nDespite our best efforts, many of these patients will ultimately require IVF if they wish to conceive, as dilated and scarred tubes will often become re-occluded even after they are surgically opened.\nUnfortunately, most women with hydrosalpinx cannot proceed directly to IVF. Since the fluid in the tubes cannot drain out the far end, it flows back into the uterus. This fluid is toxic to embryos and alters the uterine lining to make it less hospitable to an embryo.\nSome studies demonstrate that hydrosalpinx reduces IVF success rates by nearly 50%. Therefore, surgery is generally recommended to open or remove the tubes before starting IVF. This is thought to restore normal IVF success rates.\nProximal tubal occlusion\nProximal tubal occlusion is blockage of the fallopian tubes where they connect to the uterus. Mucus plugs, fibroids, endometriosis, scarring or inflammation can cause this type of tubal disease.\nProximal occlusion is usually diagnosed by HSG. However, many women diagnosed with proximal tubal occlusion on HSG actually have normal tubes after further investigation via laparoscopy.\nWhile the patient is under anesthesia, blue dye is injected into the uterus under higher pressure than can be obtained during an HSG. If the tube still demonstrates proximal occlusion, a surgical procedure called hysteroscopic canulation can then be performed to attempt repair.\nSalpingitis isthmica nodosa (SIN)\nSalpingitis isthmica nodosa, or SIN, is one type of proximal tubal disease that is not easily correctible and deserves special mention. The cause of SIN has not been well established, but is associated with endometriosis and may be related to prior inflammation in the tube.\nHSG images of SIN typically show “cauliflower” lesions, which are diverticula (out-pouching) of the tubes. Tubes affected by SIN are typically thick and tough when evaluated by laparoscopy.\nWomen with SIN are at increased risk for infertility and ectopic pregnancy (pregnancy in the fallopian tube). IVF is often recommended for individuals with SIN.\nTubal ligation or sterilization\nIf the tubes are blocked due to a prior sterilization procedure, the patient has two choices: tubal ligation reversal or IVF. Tubal reversal surgery is usually not paid for by insurance.\nThe best candidates for tubal reversal surgery are women under 35 years old with normal ovarian reserve testing and a partner with a normal sperm count. Sterilization by one of the following procedures improves the probability of successful surgical repair:\n- Filshie clip\n- Hulka clip\n- Fallope Ring\n- Pomeroy occlusion\nWomen with the ends of the tubes removed (fimbriectomy), less than 6 cm of tube remaining, or whose tubes were burned (coagulated) are less likely to succeed with tubal reversal surgery. We recommend getting a copy of your operative report and bringing it to your initial appointment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:10969c6b-fd4a-4128-b17d-b071e7651181>","<urn:uuid:ec427348-eb81-4a47-a2ce-a9e6bd04f1b5>"],"error":null}
{"question":"How does a wide aperture affect bokeh compared to depth of field in landscape shots?","answer":"A wide aperture (like f/2) creates a shallow depth of field with blurry backgrounds, which produces the bokeh effect where out-of-focus areas appear smooth and aesthetically pleasing. In contrast, for landscape photography, photographers typically use narrow apertures (like f/11 or f/16) to achieve greater depth of field, keeping both foreground and background sharp and in focus. The aperture choice directly impacts how much of the scene appears sharp versus blurred.","context":["Aperture is one of the most important styling tools in photography, so it is important to learn and understand how this camera setting can change your photographs to create a desired effect. Aperture is also one of the three components of what is known as the ‘Exposure Triangle’, where aperture, shutter speed and ISO work together to create an exposure of the photograph.\n- Explore these Photography Techniques\n- Discover What is Aperture? An Introduction to Aperture in Photography\n- Learn all about Aperture and F-Stop in Landscape Photography for Beginners\nIn the simplest of terms, exposure for photographers refers to how an image is recorded by the camera sensor and how much light is captured. If you know how to control or adjust these elements, then taking a well exposed photograph will not be a problem for you. In this article, we will focus on understanding the differences between using a wide and narrow aperture, as well as how that choice will alter your photographs.\nWhat is Aperture?\nThe literal meaning of aperture in optics is ‘a hole through which light travels’. In photography, this term relates to the opening of a lens.\nWhen you press your shutter to take a photo, the shutter mechanism opens to let the light in. Light then hits the sensor before the shutter mechanism closes again. The aperture that you set impacts the size of that hole. The larger the opening, the more light that will get in. Similarly, the smaller the opening, the less light that will be let in. It functions in the same way our eyes do. If we look at a bright scene, the iris in our eyes shrinks, however, if we enter a dark environment, it expands to let in as much light as possible.\nYou can see the aperture in the middle of this lens. Photo by: 'ShareGrid, Unsplash'.\nAperture in photography is also measured through using what is commonly known as an ‘f-stop’. This will appear on your camera as, for example, f/2 or f2, depending on which camera brand you use.\nThis number helps you to understand what kind of aperture you have set on your camera or lens. Each lens differs in terms of the aperture it has available. For example, some lenses are able to open up to f/1.2 while others might only open up to f/4. Most modern cameras will allow you to adjust the aperture by changing settings on the camera body, however, some lenses also have a physical aperture ring around their barrel.\nUnderstanding when you will benefit from using a wide or narrow aperture will help you achieve a result that is in line with your creative vision. Let’s look at the differences between using a wide and narrow aperture and what each one of them entails.\nA simple infographic that displays aperture range for easy understanding. Photo by: 'Anete Lusina'.\n- See also: How to Get Creative with a Fisheye Lens\nWhat is a Wide Aperture?\nIt might appear confusing and it still causes bewilderment amongst photographers at times, that using wide (or large) aperture actually means using a small number, such as f/2.\nThe confusion appears because we naturally assume that a ‘large aperture’ will be equal to a larger number.\nAn easier way to think about this is to think in fractions. A fraction of 1/2 is larger than 1/16. The same applies to aperture in photography, where f/2 will create a wider or larger aperture than f/16. So, when considering a wide aperture, we are referring to apertures that open as wide as f/1.2 – f/2.8. Photographers might also refer to choosing a wider aperture as ‘opening up’ the lens.\nWhen browsing for a lens, photographers generally pay attention to the maximum aperture the lens can open up to as it gives an indication of the faster shutter speeds available for the lens to be able to perform in low light scenarios. This is because a wide aperture has a larger opening hole, thus it lets more light in.\nHaving a lens with a wide aperture will give you more flexibility but it comes at a cost. While prime lenses have a set maximum aperture, some zoom lenses have a variable maximum aperture. This means that the maximum aperture will vary depending on the zoom range you are using.\n- See also: Beginner's Guide to Camera Settings\nDepth of Field\nAs we've already noted, aperture is a crucial styling tool in photography. The aperture allows you to control the depth of field in your images. This gives you more room to be creative because it determines the area that appears sharp in your photos.\nWhen you are using a wide aperture, such as f/2, you are creating a shallow depth of field, meaning, a shallow plane of your image is in focus. What this translates to in photographs is blurry backgrounds.\nWide aperture creates a blurry background. Photo by: 'Karl Chor, Unsplash'.\nIn what scenarios would we benefit from choosing a wide aperture? People, food, and product photographers will appreciate being able to use a lens that can open up wide. Being able to create a blurry background is beneficial when you want to isolate your subject, whether it is a person, a coffee cup or perhaps a single flower in a vase.\nPortrait photographers work in different styles, with some using only natural light and others adding an artificial light. If the portrait photographer works primarily with natural light, then they will favour a ‘fast’ lens with wide open apertures such as f/1.4. This is because wider apertures will make it easier to shoot in lower light scenarios. Choosing a wide aperture for portraits allows the photographer to focus the lens on the subject's eyes and let the background blur out. This can create striking portraits where the viewer's eyes are immediately led towards the subject's face.\nFocusing on the subject's eyes with a wide aperture, the foreground (hands) and the background become blurry which creates a striking portrait with the focus of the model's eyes. Photo by: 'Candice Picard, Unsplash'.\nPortrait photographers, as well as other social photographers, such as family, new-born or boudoir, often use a wide aperture to focus on the subject's details to create visual interest. This might be a strand of hair, a detail of an outfit for boudoir photographers, a couple's ring for wedding photographers, new-born baby’s hand for child photographers and so on.\nUsing a wide aperture and focusing on small details while letting the background become blurry combined with a variety of compositions and angles creates interesting and visually pleasing shots. You can also add details, such as a light piece of fabric, fairy lights or other items that can add interesting light or texture to the foreground of your shot. Using a wide aperture will blur it out while adding another level of interest to the frame.\nUsing a wide aperture creates a central focus of the image, which is intricate details of the bride's rings. Photo by: 'Chuttersnap, Unsplash'.\nSimilarly, food and product photographers will not only style their shots by arranging their scene but also by choosing a wide aperture to focus on a particular part of it. This can create an effective marketing shot where the main focus is on one product or a part of it, whilst the background becomes blurry.\nWide aperture helps food photographers style their shot by choosing what is in focus and what isn’t. Photo by: 'Claudia Crespo, Unsplash'.\nThe ability to isolate a subject by using a wide aperture not only helps to make the subject stand out but it can also help to blur out unwelcome backgrounds. Some photographers might find it beneficial to use a wide aperture when they are faced with a distracting or unpleasant background. This way, they are not only isolating the subject to make it stand out but also taking the attention away from the background, which may otherwise detract from the image.\nHowever, this may not always work in bright light conditions, such as in harsh sunlight where you might be required to lower your aperture in order to not overexpose your shot.\nWhat is a Narrow Aperture?\nWhen we refer to ‘a narrow aperture’, we mean choosing a larger number f-stop – such as, f/8 or f/16 – which creates a narrow (or small) opening to let the light through. As the opening is small, the camera needs to make up for the loss of light by slowing the shutter speed to achieve a well-exposed image. Photographers also use the term ‘stopping down’ when referring to selecting a narrow aperture, which is the opposite of ‘opening up’ when referring to choosing a wider aperture.\nAvailable light is one of the first things we evaluate when setting up a shot. A narrow aperture, such as f/16, lets in a lot less light compared to f/2.8, therefore we need to take that into consideration. So, in what situations might you find it handy to use a narrow aperture?\nAs a landscape photographer, you might want to create a soft motion-blur or long exposure effect in the sky or in running water. To do that, you will need to slow down your shutter speed which in return will require limiting the amount of light that reaches the sensor, otherwise you will overexpose your shot. Seeing as a narrow aperture allows you to do that, you can ‘stop down’ to a narrow aperture, such as, f/11 or f/16.\nNarrow aperture combined with slow shutter speed captures both sharp details of objects that are not moving and also creates a soft blurred effect in the sky and water. Photo by: 'Fabian Irsara, Unsplash'.\nSimilarly, architecture or street photographers can use a narrow aperture to create a similar effect in the city by shooting a building with softly blurred sky in the background. Narrow apertures may also be used to capture light trails on the road during evening or night time.\nUsing a narrow aperture in conjunction with a slower shutter speed can also make for creative portraits or other experimental shots, such as capturing the movement of people. Another creative way of using narrow aperture in landscape or architecture photography is through creating starburst effects to add an interesting detail to your scenic imagery.\nUsing a narrow aperture will blur backgrounds. Photo by: 'Miles Storey, Unsplash'.\nDepth of Field\nUsing a narrow aperture produces shots with an increased depth of field, which will allow you to have more of the scene in focus. This is crucial for landscape and architecture photographers to achieve an image with both the foreground and background as sharp as possible.\nUsing a narrow aperture means that you'll be able to get an entire building sharp and in focus. Photo by: 'Tobias Keller, Unsplash'.\nLook at any landscape shot and you will notice that the majority of photographers prefer using narrow apertures to showcase the scene they are capturing. The same applies to architecture photography, where capturing sharp details of the building and the surrounding environment is important.\nIf your goal is to showcase the whole scene in your landscape shots, you will want to use a narrow aperture to have all necessary parts of your image in focus. Photo by: 'Rolf Gelpke, Unsplash'.\nWhile you may want to get as much in focus as possible in your landscape shots, be mindful about your composition and focus. Choosing an interesting or appealing subject in the foreground, such as flowers, rocks, cracks in the ground, is generally a good starting point for deciding on where to place your focus and to set an appropriate narrow aperture.\nFor social photographers, using a narrow aperture will allow you to capture groups of people without losing focus on their faces. By ‘stopping down’, you will be able to increase the depth of field so that more of the image is in focus, which is useful for photographing groups of people who may not stand in the same plane of focus – for example, when people are standing behind one another. This is also beneficial for capturing larger scenes, such as events or gatherings, where the goal is to capture as much of the scene in focus as possible.\nIf you want the whole group in focus, you will have to ‘stop down’ to an appropriate narrow aperture that captures everyone in focus. Photo by: 'Tiago Rosado, Unsplash'.\nSome food or product photographers will also favour narrow apertures if they are required to capture a larger scene, such as a flat-lay of various products or dishes photographed from above. In this scenario, a narrow aperture is used to ensure that all of the necessary details or products or sharp. This is often done in conjunction with using artificial lights.\nTo summarise, it is important to understand that wide aperture pertains to a large opening but when measured in f-stops, it is displayed with a small number, such as, f/2. Equally, narrow aperture means a small opening, which is measured in f-stops with a large number, such as f/32.\nWide aperture is useful for low-light scenarios and to create a shallow depth of field, which produces beautiful blurry backgrounds. You can adjust the strength of the blur by adjusting your aperture. On the other hand, narrow aperture is widely used by landscape and architecture photographers to capture detail and sharpness in the whole scene. It can also be harnessed creatively for longer exposure shots to add movement to your images.\nThe more you shoot and practice, the easier it will become to choose an appropriate aperture for your chosen scene. It might appear complicated at first but the moment you start putting this information in practice, it will soon become second nature to you.\nAre you obsessed with shooting at wide apertures? How about always using a narrow aperture to get everything sharp and in-focus? After reading this article, will you consider experimenting a bit more with the aperture setting on your camera? Share your thoughts by leaving a comment below!\nLandscape Photography in the Lofoten Islands of Norway\nInterview with Erin Babnik\nThe Best Places to Photograph Puffins in Iceland\nUltimate Photography Guide to the Lofoten Islands of Norway\nOther interesting articles\nA Beginner's Guide to RAW vs JPEG in Landscape PhotographyOne of the most popular questions that we are often asked during our landscape photography tours in Iceland is whether it’s best to be shooting RAW versus JPEG. The answer though isn’t really all th...Read more\nLandscape Photography Composition Techniques | The S CurveTravelling to a place like Iceland is usually very high on the bucket lists of landscape photographers all around the world. It’s not hard to see why, as there are spectacular scenes to be captured...Read more\nHow and Why You Should Shoot Vertical Landscape PhotosIt's known as the great horizontal and vertical picture debate. For landscape and nature photography, should you be shooting horizontally or vertically? Check out these Landscape Photography Worksh...Read more","Bokeh, or as we say the quality of out of focus area in the photo is one of the factors we can use to get some creative results in our photos. Using shallow depth of field, you can emphasize on the subject by making the background out of focus. But did you ever try to experiment something different with the bokeh or the area in your photo which is out of focus?\nCreating custom bokeh shapes is one of the ways to enhance your photo and try something different using available light source in your background. You can create different bokeh shapes and that too at home without investing any money, as you would already be having the required supplies at your home.\nWhat all you need to make custom bokeh shapes?\nBokeh shapes such as heart shape, diamond shape, star shape, etc can be made using the following supplies:\n- Black Paper – as it would not reflect any light through the lens.\n- Pencil/Pen – to draw various shaped on the black paper\n- Scissors – to cut the black paper as per the shaped drawn\nHow to make custom bokeh shapes?\nGet started by cutting the piece of black paper is such a way that you get a square piece of paper which can fully cover your lens. Then pick up a pen/pencil and draw a shape that you wish to be the custom bokeh shape, make sure it is symmetrical and does not cover more that 30-40% area of the lens diameter. Then fold the paper into half so that you can easily cut symmetrical shape you have drawn using a pair of scissors.\nOnce you are done with it and have achieved the perfect custom bokeh shape on the piece of paper, all you have to do is place it in front of the lens at the time of clicking photo. Make sure that you position the paper in such a way that it is placed at the centre of the lens.\nCamera setting to click photos using custom bokeh shapes\nLet’s have a look at the camera settings you need to consider while clicking photos using custom bokeh shapes.\nManual mode/aperture priority:\nIt is required that you use your camera on either the manual mode or the aperture priority mode as you would need to control the aperture manually. If you use your camera on automatic mode, the camera exposure metering might change the aperture value thus affecting the bokeh effect.\nUse aperture wide open:\nAs you would want the bokeh to be smooth and visible easily, you would require to make the background out of focus by using wide open. This means that you would need a lens which can be used at an aperture value such as f/1.8 or wider such as the 50mm f/1.8 lens. You might not be able to achieve that good results if you the lens aperture opening at f/4 or smaller.\nDistance between the subject and the lights:\nIn order to achieve smooth bokeh effect and the make the background out of focus, you need to make sure that there some distance between the subject and the lighting the background. If you place them too close, you might not be able to achieve smooth bokeh effect. To learn more about factors affecting depth of field, click here.\nUse lens on manual focus:\nUsually, you would click photos using custom bokeh shaped during the evening/night time, and lighting conditions at the time of the day is low. Your lens might not be able to focus on the subject because of low lighting conditions and would keep hunting for the subject. So the best decision is to switch your lens to manual focus and focus on the subject manually.\nI hope that you find this article informative and you got to explore something new to do with your camera. Do share your custom bokeh links/photos in the comments section below."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:1f7f3b74-b514-40ca-8741-843296cefad1>","<urn:uuid:cccd3e34-945e-4637-a5d3-691466fac1b9>"],"error":null}
{"question":"What are the bone health risks associated with inhaled steroid use for asthma treatment?","answer":"Asthma patients who received 11 or more prescriptions for inhaled steroids within 12 months were 1.6 times more likely to have osteoporosis and 1.31 times more likely to have had a fracture compared to those not prescribed these drugs. Additionally, individuals on inhaled steroids were 20 percent more likely to sustain a fragility fracture if their doses exceeded 120 mg in a year.","context":["Are Doctors Doing Enough to Manage Bone Risks for People With Asthma Who Take Steroids?\nA new study highlights the need for stronger guidelines to manage the risk of osteoporosis and fractures in people with asthma who use steroid inhalers or pills.\nInhaled steroids (like fluticasone, beclomethasone, and budesonide) and oral steroids (like prednisone and methylprednisolone) are standard therapies for asthma management and flare-ups, particularly for people with severe asthma.\nThough doctors and health organizations, including the Asthma and Allergy Foundation of America and the American Lung Association, have long recognized that weakening bones and osteoporosis can be long-term risks associated with taking these medications, recommendations on managing this risk in patients with asthma are not well-established.\n“We believe that a lack of large-scale evidence on bone risk in asthma has so far meant that official guidance has neglected to cover it,” says Christos Chalitsios, a PhD student in respiratory medicine at the University of Nottingham School of Medicine in the United Kingdom.\nHe is the lead author of new research published online in October in the journal Thorax that Chalitsios says he and his colleagues hope will help in the development of more precise guidelines for managing bone health risks in patients on steroid medications (also sometimes referred to as corticosteroids).\nThe new data show a link between both cumulative dose and number of courses of inhaled or steroid pills and the risk of osteoporosis or fragility fractures.\n“Our study suggests risk and prevention of osteoporosis and fragility fractures should be addressed explicitly in future guideline updates,” Chalitsios says.\nData Shows Steroid Drugs Increase Bone Health Risks Among People With Asthma\nFor the new study, the researchers drew on anonymized health records of 69,074 patients with asthma living in the United Kingdom.\nThey specifically looked at 1,564 patients with osteoporosis; a control group of 3,313 patients without osteoporosis matched for age, gender, and the practice in which they were treated; 2,131 patients with fractures; and 4,421 patients without fractures, also matched for age, gender, and practice.\nChalitsios and his colleagues examined the impact of oral and inhaled steroids on osteoporosis and fracture in two smaller populations of people with asthma. By looking at these smaller cohorts, the researchers were able to control for other factors likely to affect bone health, including smoking, weight, and alcohol intake.\nBone Health Risks Linked to Oral Steroids\nIn comparing 992 people with osteoporosis to 2,607 without, the individuals taking two to three oral steroid prescriptions in a 12-month period were about 1.34 times (34 percent) more likely to have osteoporosis than those not taking any oral steroids.\nThe risk grew substantially for those who took nine or more prescriptions (adding up to cumulative doses of 2,500 milligrams [mg] or more of the drugs). These individuals were four times (400 percent) more likely to be diagnosed with osteoporosis compared with those who weren’t prescribed these oral drugs.\nThe scientists also looked at a subgroup of 1,663 who had fractures and compared them with 3,676 people who did not have fractures. Those on oral steroids were twice as likely to have had a fracture.\nBone Health Risks Linked to Inhaled Steroids\nInhaled steroid drug use also ratcheted up the risk of osteoporosis diagnosis compared with no steroid drug use, though it was not as heightened as for oral drugs.\nAsthma patients given 11 or more prescriptions for inhaled steroids within 12 months were 1.6 times (60 percent) more likely to have osteoporosis and 1.31 times (31 percent) more likely to have had a fracture compared with those not prescribed these drugs.\nIndividuals on inhaled steroids were 20 percent more likely to sustain a fragility fracture than those not taking inhaled steroids if their doses added up to more than 120 mg in a year.\nThe researchers noted in the study that patients with asthma and both osteoporosis and fracture were more likely to smoke, had more comorbid illness, and were of a lower socioeconomic status compared with the individuals who served as the controls.\nMany Are Not Taking Bone-Bolstering Supplements\nApproximately half of patients taking oral steroid drugs and fewer than half of those taking inhaled steroids were prescribed bisphosphonates in the year leading up to a diagnosis of osteoporosis or fracture, according to the new data. The scientists called the low percentage of bisphosphonate use (which could help prevent a bone problem) disappointing.\n“Bisphosphonates are the most effective bone protection therapy, and they are recommended for those people who are receiving long-term or high doses of [oral steroids],” says Chalitsios.\nIt’s important to point out that the researchers looked at existing records of patients’ steroid drug use and bone incidents. Because the data was observational, the analysis does not necessarily prove that the drug use did or didn’t cause the bone problems — only that they were related.\nThe study authors also highlight that inhalers can be difficult to use correctly, so actual doses taken may have been overestimated. The analysis also tracked prescriptions filled versus actual use, another way medication use may have been overestimated.\nAlbert Rizzo, MD, the chief medical officer of the American Lung Association and the chief of the pulmonary and critical care medicine section at Christiana Care Health System in Newark, Delaware, says it’s difficult to get a grasp on what an average amount of cumulative inhaled steroid is over a year’s time.\n“Patients are placed on varying doses, high to low — some take them once or twice daily, and compliance on that regimen is always an issue,” Dr. Rizzo says. “There is also the fact that not all inhaled steroids represent the same molecule but a range of molecules that have slight differences in potency.”\nIt’s important to note that people with asthma rely on inhaled steroids as a first-line emergency controller medicine when symptoms flare up. Those with more severe persistent asthma may take oral steroids to keep their condition under control. These drugs have an anti-inflammatory effect that can reduce inflammation and ease breathing.\nFor many asthma patients, these drugs can be lifesavers, and Chalitsios and his colleagues stress that patients should continue taking these medications as prescribed despite potential side effects. “Patients with asthma should not stop taking their inhalers. They save lives,” Chalitsios says.\nThe takeaway message here is that these risks should be better managed as part of standard asthma care, he says.\nData Are a Wake-Up Call to Pay More Attention to Bone Health for Patients on Steroids\nFor Rizzo, the article spotlights the fact that currently there are no set guidelines as to how often bone density tests should be done and when drugs like bisphosphonates should be taken.\n“I think an important aspect of that article is that it again raises awareness that osteoporosis and osteopenia [a condition of lower bone density] can be side effects of being on corticosteroids,” he says.\nInhaled steroids, which are delivered directly to the lungs, require smaller doses than oral steroids and tend to have fewer side effects, Rizzo says. But the new data suggest inhaled steroids can pose significant risks, too.\n“The research is also a wake-up call to say let's monitor the bone density a little more frequently in this population, and have some better criteria about when to use drugs like this in patients,” Rizzo says.\nThat said, Rizzo was not surprised by the results, because previous research has already suggested that even inhaled steroids may lead to an imbalance in the body that causes bone loss and deterioration of bone architecture, and osteoporosis and osteopenia are certainly recognized adverse events from steroids.\n“The point is we ought to be doing a better job in monitoring the situation with regard to bone resorption [a complex biological process that can result in shrinkage or loss of bone] and osteoporosis, especially in patients on long-term critical steroids,” he says.\nThe Bottom Line for People With Asthma: Continue Taking Prescribed Drugs — if You’re on a Steroid, Consider Talking to Your Doc About Bone Health\nThe message to clinicians prescribing these medications to people with asthma is to step down medication dose if symptoms and exacerbations are well managed. Doing so could make a difference in reducing the risk of these side effects.\nIn the research, investigators lay out guidance for clinicians by stratifying bone health risk by dose, number of prescriptions, and type of oral corticosteroid and inhaled corticosteroid.\nThe hope is that the data can help inform better bone protection guidelines for people with asthma, Chalitsios says."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9adb86c2-a0ca-4ba3-8e01-37090dc09cff>"],"error":null}
{"question":"How do the cooling mechanisms compare between the Pratt & Whitney R-985 Wasp Junior aircraft engine and the Toyan FS-S100GA RC engine? 🤔","answer":"Both engines use air cooling systems but implement them differently. The Pratt & Whitney R-985 Wasp Junior is a nine-cylinder, air-cooled radial engine where the cylinders are arranged in a circular pattern to maximize air exposure. The Toyan FS-S100GA, on the other hand, uses a 7-blade cooling fan specifically designed to keep the engine cool, adding realism and scale appearance while maintaining proper operating temperature. Both cooling systems are integral to their respective engines' operation, though they're scaled appropriately for their different applications - aircraft versus RC vehicles.","context":["The Pratt & Whitney R-985 Wasp Junior is a series of nine-cylinder, air-cooled, radial aircraft engines built by the Pratt & Whitney Aircraft Company from the 1930s to the 1950s. These engines have a displacement of 985 in; initial versions produced 300 hp (220 kW), while the most widely used versions produce 450 hp (340 kW).\nWasp Juniors have powered numerous smaller civil and military aircraft, including small transports, utility aircraft, trainers, agricultural aircraft, and helicopters. Over 39,000 of these engines were built, and many are still in service today.\nBegun in 1925 by former Wright Aeronautical employees as a spinoff from a machine tool company, Pratt & Whitney became one of the world’s largest manufacturers of aircraft engines, and the Wasp Jr. is one of the most successful reciprocating engines ever built. Pratt & Whitney introduced it as a complement to the highly successful Wasp and Hornet families of engines in 1930. The Wasp Jr. was essentially a Wasp of reduced dimensions. Pratt & Whitney and its licensees manufactured over 39,000 versions of the R-985 until 1953 for a wide variety of military and commercial aircraft, including light transports, trainers, sport aircraft, and helicopters.\nThe R-985-AN-14B powered the McDonnell XHJH-1 and XHJD-1 Whirlaway helicopters and the Avro Anson V trainer. This Wasp Jr.-aptly nicknamed “The Dancing Engine”- has been sectionalized and motorized to demonstrate the movements of its internal components.\nThe Wasp Jr. (pictured here from the Smithsonian Museum) was essentially a Wasp of reduced dimensions. Pratt & Whitney and its licensees manufactured over 39,000 versions of the #R985 until 1953 for a wide variety of military and commercial aircraft, including light transports, trainers, sport aircraft, and helicopters.\nThe Pratt & Whitney R-985 Wasp Junior was a 9 cylinder, single-row, air-cooled radial engine with horsepower ranging from 300 hp to 450 hp, depending on the model and configuration. It was used in a range of aircraft that included the Grumman Goose, Lockheed Model 10A and Beechcraft Model 18. Jacqueline Cochran used the Wasp Junior to set speed and altitude records in a specially built D17W Beechcraft Staggerwing.\nIn the mid-1930s, Pratt & Whitney produced five basic engines:\n- The single-row Wasp Junior.\n- The single-row Wasp.\n- The single-row Hornet.\n- The double-row Twin Wasp.\n- The double-row Twin Wasp Junior.\nThe Wasp Junior was a smaller version of the R-1340 Wasp designed to compete in the market for medium-sized aircraft engines. Development was completed in 1929 and the engine went into production in early 1930 at the new 400,000 sq./ft Pratt & Whitney plant that opened on January 1, 1930, in East Hartford.1 The original Wasp Junior was rated at 300 hp at 2,000 rpm and was similar in power and displacement to the J-6-9 Whirwind. A supercharged version developed 400 hp at 2,300 RPM at 4,000 ft. and by 1932 power was up to 420 hp with racing versions greater than that.2\nIn 1931, Pratt & Whitney developed a “hot spot” or heat exchanger on the oil regulator to improve engine performance. It was installed between the carburetor and the rear section of the engine. The regulator used the temperature drop of the gasoline evaporation to cool the oil and the same unit used the engine exhaust to heat the fuel/air mixture in cold weather.\nMore than 39,000 Wasp Juniors were produced from 1929 until the end of production in 1953.5It was considered the best engine in its class and many Wasp Junior engines are still flying today.\nSpecifications of the Pratt & Whitney R-985 Radial Engine\n|Pratt & Whitney R-985 Wasp Junior|\n|Configuration:||Single-row, air-cooled radial|\n|Horsepower:||400 hp (298 kW)|\n|Bore and Stroke:||5-3/16 in. (132 mm) x 5-3/16 in. (132 mm)|\n|Displacement:||985 cu. in. (16.14 liters)|\n|Weight:||640 lbs. (290 kg)|\nAircraft Powered by the Pratt & Whitney R-985\n- Airspeed Oxford (AS.46 Oxford V)\n- Air Tractor AT-300\n- Avro Anson (Mk V)\n- Barkley-Grow T8P-1\n- Beechcraft Model 18 and military derivatives\n- Beechcraft Staggerwing D17S, D17W, G17S\n- Bell XV-3\n- Bellanca 300-W\n- Berliner-Joyce OJ\n- Boeing-Stearman Model 75 (in aftermarket conversions)\n- Bratukhin G-3\n- CAC Winjeel\n- de Havilland Canada DHC-2 Beaver and L-20/U-6 military versions\n- Douglas C-26 Dolphin\n- Fleetwings BT-12\n- Gee Bee Model Z\n- Grumman G-164 Ag Cat (some models)\n- Grumman G-21 Goose\n- Howard DGA-11\n- Howard DGA-15P\n- Junkers F13 (Rimowa F13 replica)\n- Koolhoven F.K.51 (some models)\n- Lockheed Model 10-A Electra\n- Lockheed Model 12-A Electra Junior\n- Max Holste MH.1521 Broussard\n- McDonnell XHJH Whirlaway\n- Naval Aircraft Factory N3N (in aftermarket conversions)\n- North American BT-14\n- Seversky BT-8\n- Sikorsky H-5 helicopter (and S-51 civil version)\n- Sikorsky S-39 amphibian\n- Snow S-2B and S-2C\n- Spartan Executive 7W\n- Stinson Reliant SR-9F and SR-10F\n- Vought OS2U Kingfisher\n- Vultee BT-13 Valiant\n- Waco S3HD\n- Waco SRE Aristocrat\n- Weatherly 201 series\n- Weatherly 620\nWhat’s your favorite part of a the Pratt & Whitney R-985? Our is the sound, so here is a video of that round sound:\nThis #DHC2 comes to life with the #RoundSound of the #R985 we call music to our ears. This is @kenmoreair’s #aircraft with photo from @hangarnc #aviationphotography #aviationphoto #aviation #aviationlovers #aviationgeek #aviationdaily #AvNerd #AvGeek #AviationNerd #AviationGeek #PlaneSpotting #PlaneSpotter #aircraftmaintenance #aviationlove #aircraftenginemaintenance #instagramaviation #IGAviation #IGAircraft #avlove #pilot #privatepilot #radialengines #aircraft","Air cooled, GASOLINE powered Engine with basic accessories. This kit comes with an Air Cooled Gas Powered engine, and all basic accessories! Comes with an electric starting motor and hardware. This engine is ready to drop into your favorite RC vehicle!Big Engine Performance and characteristics, in a TINY package! This little engine is air cooled and comes with a 7 blade cooling fan designed to keep your engine cool! These engines are of the highest quality, which is why we've chosen to put our name behind them and bring them to the U. We are RC/small engine enthusiast too!\nToyan's motto is The most creative design of RC engines. Top notch engineering makes these engines not only functional but extremely powerful and dependable.\nThese engines would make an excellent choice for your next RC project, educational tool, or even desktop display. We have personally assembled/disassembled/run/tested these engines for quality and ease of use. All of our expectations have been surpassed! These engines are CNC machined for the highest precision possible. Made from high quality materials such as 6061-T6, stainless steels and brass.\nThe components have been anodized and/or treated to protect against the elements. We stock the full line of Toyan Engines and accessories.We are here to meet all of your needs & will do our best to accurately describe exactly what is and is not included with each kit we sell, in an effort to eliminate guess work. All of our listings will include full details on what is included/excluded so you can rest assured you will have what you need to get your Toyan Engine running! We realize that many enthusiasts already have their own auxiliary equipment for RC, such as ignition sources, batteries, etc.\nThis listing is for the FS-S100GA Engine and basic accessories only. You will need your own vehicle and starting equipment. Beware of listings that show pictures of items which may not be included! The marvel of the four stroke engine design has endless benefits. Ease of use will astound you with quick assembly, immediate start-up and extreme longevity.\nAside from the realistic and innovative design, these engines are the real deal. Honest to goodness four stroke engines complete with a cam shaft, valves, rockers, and the all important TORQUE. Not to mention the'BRAAAAP!!! Sound that excites experts and amateurs alike! Four stroke - single cylinder air cooled gasoline model engine.Highly efficient internal-combustion machine rated at. This amazing engine utilizes a tiny spark plug, HALL sensor and CDI ignition designed for four-stroke gas engines. Runs optimally on 92 octane premium unleaded gasoline. The gas must be premixed with oil per the manufacturers recommended ratio of 25:1 to 30:1 for proper lubrication of the lower end. The FS-S100GA comes complete with a starting motor and belt as well as a pre installed magnet/sensor on the flywheel for correct ignition timing. Also included is a HALL sensor bracket & mounting hardware. A HALL sensor is necessary so that the CDI ignition fires at exactly the right time to initiate combustion. Unlike a glow plug which remains hot/lit throughout the cycles, the gas engines spark plug only fires for a very brief and specific moment. This engine system requires 7.4v to both the starter and the CDI.\nThe high speed cooling fan adds realism/scale appearance while keeping the engine cool! In the interest of longevity and high performance, every engine is tested at the factory. 1 x TOYAN FS-S100GA AIR COOLED GAS Four Stroke Engine. 1 x Machined Aluminum Exhaust Pipe. 1 x Container of high temp grease.\n1 x Hall sensor brackets. 1 x Package of necessary hardware. Small and powerful air cooled 4 stroke performance! Ideal for RC cars, trucks, boats, airplanes and more!Engine Dimensions: 100.5 x 85.4 x 86.5mm. Product Dimensions: 10.5 x 8.54 x 8.65cm. Package Dimensions: 12 x 10 x 10cm. Packing: Cardboard Box, with quality foam inserts for protection. A four-stroke engine is an internal combustion engine that utilizes four distinct piston strokes (intake, compression, ignition, and exhaust) to complete one operating cycle.\nThe piston makes two complete passes in the cylinder to complete one operating cycle. Intake stroke: The piston moves downward towards the bottom, this increases the volume creating a vacuum to allow a fuel-air mixture to enter the chamber. Compression stroke: The intake valve is closed, and the piston moves up the chamber to the top.\nThis compresses the fuel-air mixture. The compressed fuel/air is ready for ignition to begin combustion.\nIgnition Stroke: The spark plug fires/fuel ignites, the heat released from combusting hydrocarbons increases the pressure which causes the gas to push down on the piston and create the power output. Exhaust stroke: As the piston reaches the bottom, the exhaust valve opens. The exhaust gas is forced out through the exhaust valve by the piston as it moves back upwards.FS-S100AS (Complete kit with test stand for desk top demonstration, includes all accessories). FS-S100A (Toyan 1 cylinder air cooled Nitro 4 stroke engine with basic accessories). FS-S100AC (Toyan 1 cylinder air cooled Nitro 4 stroke DIY assembly engine kit with basic accessories). FS-S100GA (Toyan 1 cylinder air cooled Gasoline 4 stroke engine with basic accessories). FS-S100WA Toyan 1 cylinder water cooled Nitro 4 stroke engine with.\nFS-S100WA1 Toyan 1 cylinder water cooled Nitro 4 stroke engine with. FS-S100WG (Toyan 1 cylinder water cooled Gasoline 4 stroke engine with external/electric water pump and basic accessories).\nFS-S100WG1 (Toyan 1 cylinder water cooled Gasoline 4 stroke engine with internal water pump and basic accessories). FS-V400A (Toyan V shaped 4 cylinder Nitro powered engine with basic accessories). FS-V400G (Toyan V shaped 4 cylinder Gasoline powered engine with basic accessories currently unavailable). Requires additional components to function. Glow plug heater, batteries, fuel tank, RC car, etc.Pictures represent EXACTLY what you will get. The item \"TOYAN RC Engine FS-S100GA 4 Stroke, Gas Powered, Air Cooled. Ships from the USA\" is in sale since Sunday, August 16, 2020. This item is in the category \"Toys & Hobbies\\Radio Control & Control Line\\RC Model Vehicle Parts & Accs\\Engine, Exhaust & Fuel Systems\\Gas/Nitro Engines\". The seller is \"mystic569\" and is located in Arvada, Colorado.\nThis item can be shipped to United States, Canada, United Kingdom, Denmark, Romania, Slovakia, Bulgaria, Czech republic, Finland, Hungary, Latvia, Lithuania, Malta, Estonia, Australia, Greece, Portugal, Cyprus, Slovenia, Japan, China, Sweden, South Korea, Indonesia, Taiwan, South africa, Thailand, Belgium, France, Hong Kong, Ireland, Netherlands, Poland, Spain, Italy, Germany, Austria, Bahamas, Israel, Mexico, New Zealand, Philippines, Singapore, Switzerland, Norway, Saudi arabia, United arab emirates, Qatar, Kuwait, Bahrain, Croatia, Malaysia, Brazil, Chile, Colombia, Costa rica, Panama, Trinidad and tobago, Guatemala, Honduras, Jamaica, Antigua and barbuda, Aruba, Belize, Dominica, Grenada, Saint kitts and nevis, Saint lucia, Montserrat, Turks and caicos islands, Barbados, Bangladesh, Bermuda, Brunei darussalam, Bolivia, Ecuador, Egypt, French guiana, Guernsey, Gibraltar, Guadeloupe, Iceland, Jersey, Jordan, Cambodia, Cayman islands, Liechtenstein, Sri lanka, Luxembourg, Monaco, Macao, Martinique, Maldives, Nicaragua, Oman, Peru, Pakistan, Paraguay, Reunion, Viet nam, Uruguay."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:7f143984-95c8-4928-bcb8-60f94c09c84e>","<urn:uuid:e91229d1-eb78-44c5-9700-908c46f8d8d0>"],"error":null}
{"question":"How much did Concorde tickets cost on average before flights ended?","answer":"Concorde tickets averaged $12,000 round trip before the transatlantic voyages ended.","context":["- Supersonic consumer flight could be back on the menu, with test flights in 2021.\n- The technically brilliant Concorde went out of business in 2003, with low demand and sky-high costs.\n- Startup Boom claims to be carbon neutral.\nAn American startup is set to begin testing a “new Concorde” supersonic jet next year. But what’s really changed since the Concorde discontinued service in 2003? And who, exactly, is clamoring for a new version?\nDenver-based Boom Supersonic says it will unveil XB-1, a 1:3 scale model of its planned supersonic jet, Overture, in October, with test flights to follow in 2021. Boom’s messaging of “bringing more people, places, and cultures” into physical reach with supersonic travel is a curious one in this particular moment in human history.\nSupersonic travel has only ever been the territory of the ultra-wealthy, to the point where the Concorde simply wasn’t profitable, even with tickets that averaged $12,000 round trip long before the transatlantic voyages ended.\nSmithsonian’s Bob van der Linden is now the Curator of Air Transportation and Special Purpose Aircraft. In 2004, he wrote in Smithsonian Air & Space that while the Concorde began as an ambitious global project, it didn’t stay that way:\n“Eventually, the tough airline marketplace forced Air France and British Airways to cut back their already limited service; routes from London and Paris to Washington, D.C., Rio de Janeiro, Caracas, Miami, Singapore, and other locations were cut, leaving only transatlantic service to New York. And even on most of these flights, Concordes flew only half full, with many of the passengers flying as guests of the airlines or as upgrades.”\nBy the end, the Concorde line was little more than a transatlantic shuttle service for corporate friends. And that’s sad, because the Concorde jet itself is a marvel of engineering, on a timeline that beggars disbelief: the first powered flight in 1903, the first experimental supersonic flight in 1947, and a full service supersonic airline that started flying in 1976. We’re certainly not in civilian rocket cars on the ground; hell, we can’t even persuade Americans to support public transit infrastructure.\nThere’s a broad context unawareness in hyping faster-than-ever international travel during the global COVID-19 (coronavirus) pandemic. But besides this poor timing, after the end of the Concorde in 2003, are we ready for the next generation of supersonic passenger flight?\nBoom claims its airliner is carbon-neutral, and its business plan has attracted major money. Or, at least, it did before all airlines began experiencing an extreme downturn. “Before the pandemic, Boom had garnered at least $6 billion worth of pre-orders for the aircraft, which has a price tag of $200 million, with buyers included Virgin Group and Japan Airlines, which invested $10 million in the company in 2017,” CNN reports.\nFor its carbon-neutral fuel, Boom has teamed with Prometheus, another startup that’s light on public-facing information. If Boom is right that consumer demand is at an “all time high” for supersonic international travel—supersonic flight is illegal over the U.S. and could not replace high-speed rail, for example, without legislative intervention—and it has indeed found a true carbon-neutral way to power flights that used to use a ton of fuel per seat, maybe the future really is in supersonic jets.\nBusinesspeople may very well line up with thousands of dollars apiece for everything they’re Zooming right now around the world. If not, well, a half-full flight is safer anyway."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:fc69e6ce-6f72-42ac-9bf3-dadeb322cf17>"],"error":null}
{"question":"I'm researching Latin American art styles - can you explain how Latin American artists maintained their cultural identity in their work while also embracing European geometric abstraction movements?","answer":"Latin American artists successfully combined their cultural roots with geometric abstraction by infusing traditional elements into modern styles. While they adopted European geometric principles like those of De Stijl and Constructivism, they maintained cultural identity through techniques like incorporating pre-Columbian pictographs, using earthy tones native to their land, and adding human touches through rough brushwork and imperfect grids. This approach is exemplified by artists like Torres-García, who developed 'Constructive Universalism' to reconcile geometric abstraction with regional cultural histories, combining the mathematical precision of European movements with symbols from ancient civilizations. Latin American artists consistently demonstrated their ability to be individual in style while complementing cultural diversity, creating art that bridged indigenous influences with multi-cultural mindsets.","context":["Latin Art is difficult to categorize in terms of a generalization of color, tone or style. The complexity and variance that each of the Latin Art movements contains are steeped in historical significance. The tumultuous timeline of Latin America tells the intriguing story of creativity influenced by society, government, and the need to embrace or standout amidst encroaching cultural movements, emboldening aspiring artists to keep at the ready the infallible paintbrush of nuance. Perhaps a by-product of resourceful, unique individuals caught in ever-changing social and economic environments, Latin Art stylizes resilience and versatility in art medium. Latin American Art combines cultural identity with unwavering ingenuity.\nBy its very definition Latin American art has and continues blazing trails across unwritten cultural boundaries. Defined by some as a compilation of influences from South America, Central America, the Caribbean, Mexico and artistic Latin Americans who have established themselves in other regions, Latin Art is rooted in indigenous cultures that had over time honed their creativity to survive.\nFrom the pre-Colombian Latin Art era, before 1492, to the sharp and visually cutting works near the start of the 20th century, the brilliant slashes of imagery in Latin Art offerings brings a startling dose of Surrealism, geometry and multi-cultural effervescence to the art world.\nDuring the Columbus era, when intense pressure by the reigning power prompted Latin Americans to encapsulate religious figures and produce time period stream line works, Latin America artists adapted focus but maintained strains of their colorful cultural influence. Social hierarchies throughout pre-Colombian and after continued to be highlighted in variances of Latin art styles.\nBoldly creating waves in the modern world and inspiring past and present artists enthralled in the Latin culture, a bevy of artistic gurus have surfaced over the last 100 years. In previous centuries as indigenous peoples, Latin Americans were known for their prominent sculpture work and embrace of the earthy tones so native to their land. Now in a different way but with similar impact, Latin artists ground their work in sharp contrasts from geometric shapes, forms of Cubism to purposeful strokes of vivid color. To describe defining characteristics of Latin art is to acknowledge Latin American artists' ability to incorporate the give and take between ancestral influence and multi-cultural mindsets.\nThe dynamic duo Diego Rivera and Frida Kahlo along with artists such as: David Alfaro Siqueiros, Wilfred Lam, Roberto Matta, and Rufina Tamayo have defined and defied art against the backdrop of more traditional predecessors--with evocative pieces.\nMuralist Diego and Surrealist Frida have produced many of the Latin artworks familiar to the current art scene. Their narrative works of Mexican culture challenge the great minds of professionals in the art field today. Many of the Latin Art works highlight or confront political and global aspects that fascinate the minds of many. Driving the desirability of their pieces, their purpose-filled creations continue to define important elements of the Latin Art movement.\nPainting with unchecked purpose, sculpting with surety and fearlessly incorporating a multi-cultural mindset barely surmise the list of characteristics that accompany Latin American artwork. Striving to be blatant, image rich and with a tenacity to recreate the tension that occurs when \"worlds collide\" (or integrate) Latin Art is a multilateral movement that stretches over centuries and historical boundaries. Latin American artists have showcased their talent to be individual in style and deed while complementing cultural diversity.","During his decades spent living in Europe, Uruguayan artist and theorist Joaquín Torres-García experimented with several different artistic modes, including making representational paintings (depicting recognizable images) in a Neoclassical style or with bright colors and bold brushstrokes. While living in Paris, he would also come into contact with European geometric abstraction (abstract art based on geometric forms) and created a group with other artists he encountered there. He transitioned from making small toys and representational paintings to making geometric abstractions. He eventually returned to Uruguay and helped influence artists in Argentina, Brazil, and later, Venezuela to make art in this new modern style.\nGeometric Abstraction rose to prominence in South America between the 1930s and the 1970s. Prior to this period, traditional representational art styles rooted in the traditions of academic painting were officially sanctioned and considered respectable in the region. This radical departure toward geometric abstraction was embraced by artists and state powers across Latin America as a way of culturally distancing themselves from the colonial past (c. 16th to early 19th century), while signifying their alignment with a new modern, economically independent future.\nJoaquín Torres-García in Paris\nLike many Latin American artists of his generation, who lived, worked, and studied in Paris between World War I and II, Torres-García came to Paris because it was seen as the artistic capital of the West. While living there in the 1920s and early 1930s, he met Theo van Doesburg and Piet Mondrian, the Dutch founders of the art movement De Stijl (also known as Neoplasticism). Together, they formed the artistic group “Circle and Square,” which promoted geometric abstraction in opposition to Surrealism, an art movement based on dream-like imagery and the subconscious that was well known in France at the time. Torres-García was inspired by Mondrian, but was critical of De Stijl’s strict austerity. He wanted to propose a more “human” approach to geometric abstraction that would engage his interests in Latin American Pre-Columbian and European ancient and Classical art.\nTorres-García’s Color Structure, made in Paris in 1930, demonstrates his interest in many of the same principles as the Neoplasticists, including the grid, a reduced palette of primary colors, and the use of the Golden Ratio. These qualities are also evident in Mondrian’s Composition with Red, Blue, and Yellow (made the same year as Torres-Garcia’s Color Structure), a non-objective painting divided asymmetrically by thick black lines into squares of various sizes filled with flat planes of white and primary colors, the largest one, a bright red. Torres-García was inspired by De Stijl’s emphasis on the grid and Constructivism’s geometry, as well as what he believed to be the “universalism” of nonobjective art—in other words, he believed that geometric abstraction, which does not depict recognizable figurative imagery, could be visually understood across all cultures. In Torres-Garcia’s Color Structure, we also see a grid composed of different sized of rectangles in blue, yellow, white, and red, arranged vertically and horizontally.\nTorres-García also adopted van Doesburg’s and Mondrian’s use of the Golden Ratio, an important concept to him, which he felt would help his art become integrated with natural and cosmic forces. But, unlike Mondrian, Torres-García emphasizes Color Structure’s imperfections: the grid is drawn freehand with wavy lines and the colors are muddy and include tonal variation and rough brushwork.\nConstructive Universalism in Uruguay\nWhen Torres-García returned home to Uruguay in 1934 (for financial reasons and the encouragement of friends), after more than forty years living abroad, he sought to bring geometric abstraction to Latin America as a way to reconcile this style with the region’s own cultural histories and artistic traditions. Dissatisfied with what he saw as a lack of emotion or humanity in Constructivism and De Stijl, he developed his own style called Universalismo Constructivo (Constructive Universalism), which sought to combine the “reason” of Constructivism’s and De Stijl’s geometry with the sources of abstract art found in the arts and crafts of ancient civilizations from around the world. He incorporated pictographs (simplified images or symbols) related to the cultures of various ancient civilizations (including Pre-Columbian cultures), into his images. He felt these pictographs communicated common ideas to people everywhere, making them “universal.”\nAs a cultural nomad who lived abroad in the U.S. and Europe for 43 years before returning to Uruguay, Torres-García was interested in the concept of “universalism” because he wanted to find visual elements that were shared by all cultures, underpinning his belief in the metaphysical wholeness of the universe. He also wanted to show how the geometric principles of pre-Columbian, Indigenous artistic styles actually anticipated later European geometric abstraction. His ideas paralleled psychologist Carl Jung, who believed that archetypal images could connect individuals to collective cultures and universal experiences. Torres-García was not relating to Jung directly, and for him, the metaphysical was much more important than the psychological. His search for the universal was not based on the psyche of the individual, but rather the universal collective.\nWe find an expression of these impulses in his painting Universal Art, a composition in earthy browns depicting many interconnected rectangular compartments filled with pictographs, including shapes, symbols, and recognizable images such as the sun, the moon, scales, fish, a heart, a house, boats, and people, that appear as if they are carved into wood or stone. Like his earlier works that borrow from Mondrian, Torres-García creates an asymmetrical grid based on the Golden Ratio with a reduced color palette, but his earth tones convey more warmth, and his rough, painterly strokes and shading appear more “crafty” and reveal a human touch.\nLater, Torres-García founded an arts and crafts workshop called the Taller Torres-García to disseminate his artistic theories to a younger generation of Uruguayan artists, whose style came to be known as the School of the South. Following Torres-García’s innovations in the 1930s and 1940s, artists across Latin America—especially in Argentina, Brazil, and Venezuela—began working in geometric abstraction, which would dominate the painting and sculpture in those countries until the 1960s and 1970s.\nAACI and Madí in Argentina\nIn 1944, a group of Argentine and Uruguayan artists based in Buenos Aires published the first and only issue of a magazine about abstract art titled Arturo, with texts and reproductions of artworks by Torres-García, Mondrian, and the Russian abstract artist Wassily Kandinsky. The magazine was intended to promote Concrete Art (a term first used by Theo van Doesburg to describe nonobjective geometric abstraction) in Argentina.\nTwo Argentine Concrete Art groups emerged from the magazine: the Association of Concrete-Invention (AACI) and the Madí Group. These developments took place during the first term of the presidency of the Argentine populist Juan Domingo Perón, whose administration officially sanctioned naturalistic, figurative art styles, which these Concrete art movements fought against.\nThe AACI was founded in 1945 by Argentine artist Tomás Maldonado and poet Edgar Bayley. The catalogue for the group’s first exhibition included their manifesto—a written statement declaring the group’s intentions, motives, and views—titled the “Inventionist Manifesto.” It called for artists to “invent” their own images, rather than trying to copy what they see, and justified Concrete art through Marxist political theories. \nEmbracing Mondrian’s and van Doesburg’s strict Concrete aesthetics while rejecting Torres-García’s emphasis on symbolism and a hand-made aesthetic, the AACI emphasized a rigorous, mathematical, even mechanical-looking approach to geometric abstraction with flat, planar colors. These principles are on display in Maldonado’s Development of a Triangle a composition on a white ground of intersecting straight and angular lines and a series of triangles, one yellow and another violet. \nIn 1946 the Madí Group was formed by artists Rhod Rothfuss, Carmelo Arden Quin, and Gyula Kosice.  Like the AAIC, Madí endorsed making nonobjective artworks with flat planes of bright color. But, unlike the AAIC, Madí art was more playful and experimented with three-dimensions and unusual materials such as Plexiglas and neon.\nMadí artists often rejected the traditional rectangular picture frame in favor of an irregularly-shaped canvas, known as a marco recortado (cutout frame). The cutout frame was first conceived by Rothfuss in an essay he wrote for Arturo, in which he argued that irregularly-shaped canvases would allow artworks to function like other objects in the world, rather than as windows framing views of another world. One example of the cutout frame is Rothfuss’s Three Red Circles, a bright yellow geometrically-shaped composition with shapes delineated with thick black lines, a blue rectangle at the top and on the side, and three small red circles on the left.\nConcrete and Neo-Concrete Art in Brazil\nIn 1951, Swiss Concrete artist and former Bauhaus student Max Bill won the grand prize for sculpture at the First São Paulo Biennial (a large international exhibition occurring every two years) in Brazil, for his metallic sculpture of flowing, intertwining ribbon-like forms, called Tripartite Unity. This sculpture would influence a young generation of Concrete artists in the country, including the Grupo Ruptura in São Paulo and the Grupo Frente in Rio de Janeiro. Concrete art captured the values of science and mathematical precision heralded in Brazil in the 1950s, a period of rapid industrial growth, modernization, and the development of a new capital in Brasília defined by architect Oscar Niemeyer’s futuristic International Style architecture.\nThe Grupo Ruptura\nThe Grupo Ruptura was formed in 1952 with an exhibition at the Museum of Modern Art of São Paulo. The group’s name meant “rupture,” and it sought to break with traditional art styles, namely naturalistic painting, which was seen as elitist (though this break was not specifically tied to political radicalism). Their works were characterized by flat colors and a reduced palette, geometry, and industrialized media like enamels and mechanical techniques like spray painting that would not reveal the hand of the artist. Their works also incorporated Gestalt psychological theory (a perceptual theory about how the brain forms a whole image from many component parts) by training the viewer’s eye on outlines as contours of a solid shape, as in Judith Lauand’s Virtual Space, a painting of a pinwheel-like shape of white and purple lines in a field of black. Lauand was also the only woman in the Grupo Ruptura.\nThe Grupo Frente and Neo-Concrete art\nThe Grupo Frente was founded by artist and teacher Ivan Serpa in Rio de Janeiro in 1954. Many of the artists involved were his former students at the Museum of Modern Art of Rio de Janeiro. They rejected the Grupo Ruptura’s strict adherence to purity, science, and math and promoted instead more creative intuition in geometric abstraction. This shift in the Grupo Frente’s brand of Concrete art eventually led to a new style altogether, known as Neo-Concrete art, pioneered by artists Lygia Clark, Lygia Pape, and later, Hélio Oiticica.\nThe principles of Neo-Concrete art were theorized by poet Ferreira Gullar in his “Neo-Concrete Manifesto” (1959), which called for more sensuality, freedom, and feeling in Concrete art. Like Torres-García who sought to infuse geometric abstraction with more emotion, the manifesto sought to distance the new movement from the dogmatic rationalism of European Concrete art styles like Neo-plasticism and Constructivism.\nGullar also developed the theory of what he termed the “non-object,” by which he meant an art object that would function as a mediator between the spectator and the physical experience of the object. This is exemplified by Lygia Clark’s Bicho (Critter), a metal sculpture made of moveable flaps intended to be manipulated and rearranged by the spectator-participant (or someone who observes and participates). As Clark’s Bicho suggests, Neo-Concrete art adapted Concrete art’s geometric shapes and transformed them into organic three-dimensional objects to be handled by spectators, or environments to be physically entered, which helped to break down boundaries between art and life in Brazilian art of the 1960s.\nOptical and Kinetic Art in Venezuela\nTwo other forms of geometric abstraction, known as Optical (or Op, a style of abstract art based on patterns and optical illusions) and Kinetic art (objects that have moving parts), became popular in Venezuela in the 1950s and 1960s. These styles were dominated by three artists in the country: Carlos Cruz-Diez, Alejandro Otero, and Jesús Soto. Their works included small and large-scale abstract sculptures and public artworks made of bright colors and industrial materials that moved, or appeared to move.\nA famous example are Soto’s “penetrables,” tubes of colored plastic suspended from the ceiling in box-like shapes, into which participants could enter and play. While Brazilian Neo-Concretists sought to move away from the purely optical toward the experiential and sensual, the Venezuelans still embraced the visual, but also created participatory and sensually experiential environments and objects to be interacted with, like Soto’s penetrables.\nThe Op and Kinetic works by these artists received corporate and government support from the Venezuelan regime of Colonel Marcos Pérez Jiménez’s dictatorship of the 1950s, as well as later democratically elected administrations in the 1960s and 1970s, and resulted in the commissions of several large-scale public artworks. The state and corporate leaders believed such works helped position Venezuela on the world stage as a modern, forward thinking, and technologically advanced country. This was important to them as they were growing their oil industry for international export. These works were used by corporations and the state as propaganda promoting Venezuela as an international modern center of industry, revealing how state-patronage and nationalist interests intervened in the so-called “avant-garde” art in the country during the period.\nAnother important artist in the history of Venezuelan abstraction was Gego (Gertrud Goldschmidt), a German emigrant, who developed her own approach to geometric abstraction, creating delicate sculptures and environments (sculptures that occupy entire gallery spaces) made of wire. Gego’s large Reticulárea (Reticular), created at the Museo de Bellas Artes in Caracas in 1969, comprises a sprawling wire grid that filled the gallery’s floor, walls, and ceiling, into which spectators could enter and walk around.\nContributions and Legacy\nLatin American geometric abstraction united international principles of modernist abstraction with local cultural traditions, and led to more participatory forms of art. It also served as an ideological tool for both Latin American artists and nation-states to signal a break with traditional art styles—associated with their colonial past—and to assert a new, modern, and often utopian industrialized future. Latin American geometric abstraction is probably most notable for the large number of women artists who were leaders in these movements and who achieved successful artistic careers in their lifetimes, something that was much less common with mid-twentieth century art movements in the U.S. and Europe. \nNotes: Their formulation of “Concrete-Invention” was rooted in their commitment to Marxist materialism and anti-idealist revolutionary and collective art.  The term “Madí” has various purported origins: it may have represented the combination of the first syllables (in Spanish) of the term “dialectic materialism,” it may have been an abbreviation of “Madrid,” or it could have been an acronym for “Carmelo Arden Quin.”  In addition to the women artists mentioned in this article, Judith Lauand, Lygia Pape, Lygia Clark, and Gego, were many others, including Lydi Prati, Mira Schendel, Fanny Sanín, María Freire, Amalia Nieto, and Mercedes Pardo.\nGabriel Pérez-Barreiro, “Concrete Art in Latin America.” Grove Art Online, March 26, 2018.\nDavid Fernando Cortés Saavedra, “Geometric Abstraction and Concrete Art in South America.” Routledge Encyclopedia of Modernism, September 5, 2016.\nAlexander Alberro, Abstraction in Reverse: The Reconfigured Spectator in Mid-Twentieth-Century Latin American Art (Chicago: University of Chicago Press, 2017).\nMónica Amor, Theories of the Nonobject: Argentina, Brazil, Venezuela, 1944–1969 (Oakland, CA: University of California Press, 2016).\nYve-Alain Bois, et al. Geometric Abstraction: Latin American Art from the Patricia Phelps de Cisneros Collection (New Haven, CT: Yale University Press, 2001).\nDan Cameron, Kinethesia: Latin American Kinetic Art, 1954–1969 (Palm Springs, CA: Palm Springs Art Museum, 2017).\nMaría Amalia García, Abstract Crossings: Cultural Exchange between Argentina and Brazil (Oakland, CA: University of California Press, 2019).\nAleca Le Blanc and Pia Gottschaller. Making Art Concrete: Works from Argentina and Brazil in the Twentieth Century in the Colección Patricia Phelps de Cisneros (Los Angeles: Getty Publications, 2017)."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9d2e95f9-e574-4dde-a386-dbe3ece964dd>","<urn:uuid:86e2e91b-fbaf-45c1-9fb3-0d6526737c20>"],"error":null}
{"question":"What guidance exists for proper flash pot placement during performances, and what electrical safety precautions are recommended for pyrotechnic wiring?","answer":"For flash pot placement, there should be a ten-foot radius around the flash pot that is free of performers. The operator must have clear visibility of the pot to monitor performer proximity. Flash pots should not be located in high-traffic areas to prevent accidents like kicking the pot against back curtains. Effects should be planned early, allowing at least three rehearsals with the effect in place during tech week. Regarding electrical safety, pyrotechnic wiring must be kept separate from lighting, audio, or other power sources, as even sound systems have enough power to light an electric match. The wiring should use heat and abuse resistant insulation like Teflon, and grounded metal conduit provides excellent physical protection while helping route unwanted power away from pyro devices. Cross-wires between ignition channels should be avoided, and systems should be isolated from ground to prevent ground faults. Regular testing or ground fault detection features are necessary to ensure system isolation.","context":["Flash Pot Basics\nThe season of Spring Recitals has arrived, and we at Theatre Effects have had our hands full these past few weeks with orders for all sorts of special effects. In addition to taking orders, we've also been answering lots of questions from people new to using flash pots, some of which I'd like to address this week. I imagine some of our more experienced readers groaning now, and getting ready to click on the next message in their Inbox. To them I say, \"Wait just a minute!\" Take a look through the topics discussed here, you may be surprised by what you didn't know!\nSo, what is a flash pot, anyway? Basically, a flash pot is a sturdy container that can hold a pyrotechnic powder, and is usually wired to fire an igniter. The powder can be standard flash powder, colored flash powder, smoke powder, or flame powder. An igniter is just a thin piece of wire (often Nichrome) that will spark when 110 volt current is run across it. Some flash pots are battery-powered and use a glow plug with flash cotton in place of an igniter. In these pots, the glow plug ignites the flash cotton which burns like a fuse into the powder, setting off the effect.\nTheatre Effects sells several different types of flash pots. Which of these is the right one for you depends on a few variables. First, if you plan on doing any loud report (aka concussion) effects, or if you want to produce flame columns, you'll need the Color Volcano (#UL01). This pot is sturdier than our other pots, and can handle the force produced by our Sonic Flash Powder. In addition, the extra-long barrel on the Volcano allows you to produce four to six-foot columns of flame when using our Flame Powder. While the Volcano is more expensive than many of our other pots, it's the only one that will do when creating a concussion effect.\nIf you're just looking to create a few small flash effects, and want to keep your effects budget as low as possible, the Standard Electric Flash Pot (#MU01) may be the answer. The standard electric flash pot uses bare nichrome wires for igniters and can handle most small effects. If, however, you want a somewhat sturdier pot that uses our more reliable Surefire Igniters, you might consider paying a little extra for the Ultra Flash Pot (#UL05).\nThe Color Volcano, Standard Electric and Ultra flash pots all have one thing in common. To create their effects, they require 110 volts of AC current. This is usually not a problem for performers working in a club or theatre. But what if you don't have access to AC power? What if you're performing outside, or the management at the venue refuses to allow you to plug pyro devices into their house current? It was for situations like these that Theatre Effects developed the Electronic Flash Pot.\nThe Electronic Flash Pot uses two AA-size batteries to fire the effect. Because there's no need to hook into an outlet, this flash pot can be set up and used almost anywhere. In addition to being a lifesaver to performers stuck without AC power, the Electronic Flash Pot also allows a performer to become creative with the placement of the flash pot. For example, a magician might want to hide a flash pot inside a prop that rolls on stage. With the electronic flash pot this is easy to achieve because the power is on-board, and the controller is a hard-wired footswitch that blends well with the stage floor.\nSpeaking of controllers, the other flash pots I mentioned will need some sort of switch to control the current to the pot. All of them come equipped with a standard electrical cord, but if you simply load the pot and plug it into the wall it will fire immediately, often tripping the circuit breaker as it does. Therefore, some sort of switch is needed to control power to the effect. I recommend the Ultra 4 Firing Panel or the Ultra 2 Footswitch, but performers who are confident with their electrical skills may decide to create their own. If so, bear in mind that a momentary switch (a switch that only allows current to pass while the switch is being held down) will help prevent accidentally tripping a breaker.\nChoosing the right powder for your flash pot is a somewhat easier process. Theatre Effects carries Flash Powder (in white, red and green), Smoke Powder (in a rainbow of colors), Flame Powder and Sonic Flash Powder. We also carry color additives for the Flame Powder, and Sparkle Additive for the Flash Powder. The powder you choose really depends on what look you want to create. A few basic hints\nFor an instantaneous effect (e.g. a magical appearance) you want to use flash powder. If you want to add a little \"magic\" to the effect, a very light dusting of Sparkle Additive will do the trick. I've found that as little as an eighth of a teaspoon will create all the sparks I need.\nFor a slower effect that produces thick clouds of colored smoke, choose smoke powder. Please make sure that your space has adequate ventilation, however, since you you don't want your audience coughing through the rest of your show! Smoke powder works best if you wrap it in a piece of flash paper or slow cotton to provide a longer lasting source of ignition.\nNow that you've chosen a pot and some powder, it's time to put it to use! For you beginners (and studious veterans) I have a few tips that will help ensure the success of your effect\nFirst, don't leave the special effects for the last minute. Too often, directors get caught up in the details of rehearsals and don't even consider adding effects until the show is \"just right.\" If you don't leave time to practice with a special effect, it will not look as good as it could, and it may even become a hazard on stage! As a rule of thumb, I try to have special effects ready for rehearsals by about a week before the show opens.\nIf you're using a flash pot with an off-stage controller, make sure that the operator can clearly see the flash pot from his position. If your operator can't see the pot, he can't tell if a performer is standing too close to the effect. In general, there should always be a ten-foot radius around the flash-pot that is free of performers. Be sure to give the operator final authority on whether or not to fire.\nPlan your effect as carefully as possible! Try to imagine everything that will be happening on stage when you want to use the effect, and then ask yourself whether anything could interfere with the effect, or pose a safety risk. When planning the position of the flash pots, make sure that they are not located in a high-traffic area -- the last thing you need is an actor kicking your flash pot up against the back curtain!\nFinally, as the hippie allegedly said to a man looking for Carnegie Hall, \"practice, man, practice!\" If you've ordered your effects in a timely manner, you should be able to have at least three rehearsals with the effect in place. These rehearsals are vital to work out any bugs in placement or powder loads, and to get the actors comfortable working with the effect. My rule of thumb here is that if an effect isn't in place at the beginning of \"tech week\", it doesn't go in the show.\nI've tried to offer a \"crash course\" in choosing and using flash pots here, but I know there are probably questions I've left unanswered. So, my final piece of advice is this, don't be afraid to ask for advice! As many a teacher has reminded me, \"the only stupid question is the one you didn't ask.\"\nTheatre Effects Customer Service Department\nTheatre Effects, 1810 Airport Exchange Blvd. #400, Erlanger, KY 41018\nPhone: 1-800-791-7646 or 513-772-7646 Fax: 513-772-3579","Fire When Ready – A Pyro-Electric Safety Primer\nBY Daniel Birket\nBirket Engineering, Inc., February 2002\nPyro-technicians are trained to handle pyrotechnic devices with great care in part because “you never know when it might go off.” The basis of this prudent uncertainty can be traced in part to a lack of information and sometimes to inappropriate techniques. This document attempts to illuminate some obscure areas of pyrotechnics and list some practices that can reduce the risk of unexpected ignition.\nTo Fire or Not to Fire…\nPyrotechnics are a much-anticipated component of shows at theme-parks, theatres, and on tour. When the “fireworks” occasionally fail to fire or work, it disappoints the audience, stirs up the management, and stresses-out the pyro crew. But any pyro-technician (who would like to continue to count to ten on his fingers) knows that the problem of devices that don’t ignite when you expect is nothing compared to the hazard of ones that do when you don’t. The lack of a pyrotechnic accent may detract from the artistic presentation, but a charge that explodes unexpectedly will shut down the entire show whether or not it harms anyone. This problem of unexpected ignition is the focus of this document.\nThere is a fairly widespread notion that it’s easy to make a system that will ignite pyro-electric devices. After all, you only need a medium-size battery to ignite an electric match (often called a “squib”). Anyone able to fix a flashlight can build a system that fires an electric match. This is true: it is easy to ignite pyro. In fact, it’s hard to make a system that won’t light pyro – until you want to – and that is the question.\nOhms and Amperes and Watts, Oh my!\nVoltage, current, resistance, and power determine whether or not an electric match will ignite and a good engineer will thoroughly analyze these parameters when designing a pyro-electric control system. But it’s not necessary to know Ohm’s Law to know how to avoid unexpected ignition – just a few rules of thumb.\nAn electric match will ignite if enough power is sent through it to heat it to its ignition temperature. A tiny amount of power won’t do it. When a pyro controller tests for continuity, it uses extremely low power to safely test for the presence of the match. All commercial pyro controllers take great care with the continuity check circuitry because it is an obvious area of risk – a little too much power during the test might ignite a match instead of just testing it.\nStopping Unexpected Power\nThere are two ways to avoid sending unwanted power through the electric match:\n- Block any unwanted power from reaching the match, and\n- Route any unwanted power away from the match.\nBlocking unwanted power involves placing insulators and shields between possible power sources and the match. Similarly, routing unwanted power away from the match involves putting conductors between the match and a safe place to dump the unwanted power. With a little thought, we can usually block the unwanted power sources from reaching the match. For the remaining sources, including those we don’t expect, we can try to route the unwanted power away to a safe place.\nThere are several kinds of unwanted electrical power that might find their way to an electric match. Most are fairly obvious and easily avoided, but some are quite devious.\nWhile everyone is used to wires guiding electricity along the circuits we want, circuits can and do form anywhere different voltages find a way to connect. Current can flow through catwalks, conduits, pipes, even damp stains on surfaces. Have you ever felt a tingle when touching some equipment? That electrical source might have been able to light an electric match.\nObviously, we don’t want the pyrotechnic wiring to come in contact with wiring for lighting, audio, or other power. There is more than enough power in a sound system to light an electric match. Cross-wires between ignition channels are another common cause of unexpected ignition. Cross-wires may occur where there are many channels of pyro or other wiring grouped together. A pyro controller that tests for channel cross-wires as it performs its continuity check will reduce this risk.\nOne unexpected, but very common, route for unwanted power is the ground – that is, the dirt under our feet. In almost any electrical power wiring system you’ll find “ground” wires that lead ultimately to the earth. Metal frame structures are often tied to the earth with special wiring. “Lightning rods” are wired to the ground with thick cables to dissipate the natural voltage of passing rain clouds. Practically every electrical system uses “ground” on one side of its circuits. This is a good safe practice for most electrical wiring, but bad news for a pyro-electric system.\nAlthough one connection to ground is harmless, it’s then easy for a stray wire strand, a fleck of metal, or even a drop of water to form a second connection somewhere else. A second connection may complete an electric circuit between an unwanted power source (perhaps another ignition channel) and the match through the ground. For this reason, good pyro control systems are isolated from ground to help prevent ground faults. Regular testing or a ground fault detection feature is necessary to insure that the system remains isolated from ground. A ground-fault detection circuit works like a “GFCI” (Ground Fault Circuit Interrupter) safety wall outlet to detect unexpected connections to the earth. (Note: Plugging a pyro controller into a GFCI outlet doesn’t work.)\nElectric power is able to “side-step” from one circuit into another even when there is no electrical conductor between. Wherever electricity flows there is a magnetic field that can cause current to flow in another circuit without ever touching. Power transformers use this principle. When it happens unintentionally, its called Electromagnetic Interference or EMI. If you have ever heard a 60-cycle “hum” in the audio system when the lighting system turns on you’ve seen this principle in action. The high-power lighting circuits can easily jump to unshielded audio wiring anywhere the wires are routed together, for example in a cable tray. High power circuits can have the same effect on unshielded pyro channel wiring.\nInsulate, Isolate, Shield, Protect, and Verify\nHere are some techniques for keeping these unexpected power sources out of the pyro-electric distribution system:\n- Grounded metal conduit: In a permanent installation, grounded metal conduit provides excellent physical protection for pyro channel wiring. Wiring is unlikely to be damaged inside the conduit and grounding the conduit helps to route unwanted power away from the pyro devices. Grounded metal conduit also shields the wire from EMI fields that may induce current in the pyro wiring. Note: If water collects in the conduit, it may eventually break through the wire’s insulation and produce a ground fault.\n- Heat and Abuse Resistant Insulation: Where pyro wiring is exposed or movable, the wire’s insulation should be able to stand up to burning fallout and rough handling. If the insulation melts or scrapes away, it’s easy to form an unwanted circuit. Teflon is one type of tough insulation.\n- Shielded wire: Shielded wire has a web of wire wrapped around the center conductors. When the shield (not the wire) is connected to ground at one end, it will block most Electromagnetic Interference. Note: Take care not to ground the pyro channel wiring by allowing it to touch the shield.\n- Twisted-pair wire: When a pair of wires is twisted together, they become less susceptible to EMI. This is good alternative to more expensive shielded wire.\n- NO “Zip Cord”: Un-shielded, un-twisted, “zip” or lamp cord generally has a soft thin insulation that is neither heat nor abuse resistant. It may be cheap, but it’s not suitable for permanent wiring. Zip cord wiring is a common factor in many cases of unexpected ignition. Remove any temporary zip cord with the spent pyro.\n- Isolation Transformers: If a pyro system uses a low-voltage AC firing current, isolation transformers between zones can help block ground faults and other unwanted circuits from forming. Note: this technique provides only limited protection and is easily compromised unless the isolation is tested regularly.\n- Isolated Firing Circuits: One excellent way to avoid unwanted circuits from forming between one pyro channel and another is to isolate every circuit from every other. Good pyro control systems use individual firing capacitors to handle each channel independently.\n- Verified Circuits: To insure that every pyro channel is isolated from ground and from every other pyro channel, use a pyro controller that verifies every circuit during the continuity check. A simple continuity check will insure that a match will fire when expected, but ground-fault and cross-wire checks are necessary to insure that no match fires unexpectedly.\nAfter the pyro system designer and the pyro-technician user have taken every reasonable precaution to prevent unwanted power from reaching the electric match, it can still happen. To deal with this problem, pyro control systems “shunt” each pyro channel. The “shunt” is an automatic and/or manual switch on each pyro channel that conducts unwanted power away from the electric match where it will do no harm. This works very well – as long as the channel wiring is heavy enough. If the wiring is too long or too thin, the unwanted power may prefer to flow through the electric match instead of the shunt. It depends on the wire length, but 14-gauge wire is usually heavy enough to insure that channel shunts can provide reasonable protection for the match.\nSome pyro systems use a manual shunt that protects the pyro-technician while loading the pyro devices. This is adequate for installations where no-one will go near the pyro after the shunt is removed, but inappropriate for a live-action show. Ideally, every pyro channel should be individually shunted up until the moment it is fired. With this style of controller, the entire cast and crew is protected from unexpected ignition at all times.\nThis document is presented as a service to the entertainment community for informational and promotional purposes only. It is not intended as engineering advice or opinion and is not guaranteed to be current, correct, or complete. Links to other web sites are not endorsements of those sites.\nYou are welcome to forward the link to this document to others interested in the safety of rides and shows or control systems in general. You’ll find similar safety articles in the Reading Room.\nAmusement-safety and Show-control are professional, spam-free discussion groups. They are hosted at http://groups.yahoo.com by and for industry professionals. Birket Engineering, Inc. does not operate either group."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:3fffe2a9-7271-4ab4-82d2-d3bb98e5b007>","<urn:uuid:f8b16713-5c6c-4599-8242-3ad2a8afec51>"],"error":null}
{"question":"I'm fascinated by astronomical observations. How do the methods for locating Planet Nine compare with traditional ways of determining Earth's position using the sun and stars?","answer":"Planet Nine's location is determined through indirect evidence like the strange orbital behaviors of Kuiper Belt objects and computer simulations, using advanced tools like the Subaru Telescope. In contrast, Earth's position can be determined through simple daily observations of the sun's path, shadow patterns, and star positions. For example, you can find your latitude by measuring the angle between the horizon and Polaris, while sunrise and sunset points reveal information about Earth's axial tilt and seasons. Both methods ultimately help us understand our cosmic position, though at very different scales.","context":["NASA has stated that it's quite likely that our solar system is home to a massive, distant ninth planet. Its existence could shed light on some burning questions about the cosmos.\nEarlier this month, NASA issued a press release stating that it’s likely that our solar system has a ninth planet—even if it’s proving difficult to find.\nThe planet could have a mass ten times that of Earth’s, and be situated twenty times as far from the sun as Neptune. It’s being referred to as “Planet Nine,” and while it’s very difficult to procure clear evidence of its existence, some scientists are absolutely convinced that it’s out there.\n“There are now five different lines of observational evidence pointing to the existence of Planet Nine,” said Konstantin Batygin, a planetary astrophysicist at the California Institute of Technology (Caltech), who is part of a team on the search for the planet.\n“If you were to remove this explanation and imagine Planet Nine does not exist, then you generate more problems than you solve. All of a sudden, you have five different puzzles, and you must come up with five different theories to explain them.”\nIn a 2016 paper, Batygin and co-author Mike Brown detailed six known objects in the Kuiper Belt that behave rather strangely. All of them have elliptical orbits pointing in the same direction, and all of those orbits are tilted the same way. Both of these traits serve as clues to the presence of Planet Nine.\nComputer simulations that took the hypothesized planet into account indicated that there should also be several other bodies with more extreme tilts from the solar plane, on the order of 90 degrees. Brown realized that astronomers are already aware of five such objects, meaning nature did fit with the simulations.\nPlanet Nine would also explain why the plane in which the other planets orbit is tilted about six degrees away from the sun’s equator: over time, Planet Nine’s distant gravity has made the entire solar system plane wobble away from center.\nFinally, there are the objects in the Kuiper belt that orbit in the opposite direction of everything else in our solar system.\n“No other model can explain the weirdness of these high-inclination orbits,” explained Batygin. “It turns out that Planet Nine provides a natural avenue for their generation. These things have been twisted out of the solar system plane with help from Planet Nine and then scattered inward by Neptune.”\nBased on the behavior of these distant objects, the astronomers believe the planet to be a Super-Earth, a massive rocky planet that is extremely common in the universe—but which our solar system, oddly, lacks. The planet could have coalesced out in the cold reaches of our system over millions of years, formed close to the sun and then been flung outward, or even been captured by the Sun from another system.\nBatygin and Brown are using the Subaru Telescope at Hawaii’s Mauna Kea Observatory to continue their search for Planet Nine. According to Batygin, this telescope is the best tool available to hunt down something dim and distant in the vast expanse of sky.\nHowever, Planet Nine isn’t the only explanation for the orbital behaviors observed. A recent survey of the outer solar system found over 800 trans-Neptunian objects. A random distribution of this matter could also potentially have the same effect on the tilt on the traits observed in various orbits—but the jury is still out.","Monday, September 21st, 2015 18:17 Lat: 0° 0’ 0” Long: 91° 37’ 61” W\nNo matter where you are, you stand in the center. Our perspective marks the focus of our unique personal sphere stretching from horizon to horizon and arcing above and below in all directions. Yet, in a world that is itself a sphere, there must be a place on its surface, which is more central than others regardless of where you stand. There must be a bisecting line that will split a sphere into two symmetrical bowls whose rims, on meeting, re-form an edge-less orb. On Earth, we call this line the equator. Today we crossed this line.\nThe distinctiveness of the equator has been recognized by the cultures who have lived along its contour well before we had developed world maps, globes, or satellites, and certainly before we came upon the concept of those other traveling spheres called planets. Yet, in the absence of all these tools–and through nothing more than careful observation of daily, seasonal and yearly cycles–we have and we can continue to discover the equator. Even more remarkable, through observation and reasoning, we can glimpse the nature of our planet and its place and motion in the cosmos.\nTo begin, consider the significance of the single observation that sunlight is not evenly spread across the Earth. At some times and in some locations we are flooded with light, while in others we are dealt nothing but darkness. There is a narrative to be read in this unequal distribution and in the patterns that separate light from dark. Pay attention and you can begin to see the world from a single point wherever you are. Shadows tell the story of the sun’s path, the points of sunrise and sunset identify the seasons, mindful stargazing reveals the distance separating you from the equator. You can experience all this in your own life if you look closely. Pay attention to the sun as it rises and falls along its daily journey and you’ll notice morning’s long shadows shorten until the sun reaches its noonday zenith then grow again as evening comes. Trace the shifting points of sunrise, zenith and sunset throughout the seasons and you’ll notice that the they all appear to migrate along the horizon like Canada geese— traveling further south in the winter, and north in the summer. Measure the angle between horizon and the North Star, Polaris, (or, if you are in the south, the Megallanic Cloud of the Southern Cross) and you will have determined your latitude.\nWeave all of these clues together and you can begin to see the underlying cause-and-effect relationships which reveal the secrets of Earth’s position and motion in space: The sun’s daily journey is a clue to planetary rotation. The position of sunrise zenith and sunset point to the tilt of the Earth’s axis and hint also to one’s latitude, which can be pinned down more fully by looking at those special points in space lying directly above the poles.\nWherever you are there is a compelling geographic story to be read within the elements of the everyday. Yet, there are special times (in special places) when you may experience the exceptional—if you take the time to look. Today was one of those times.\nAcross the Earth, the sun reaches its zenith (its highest point) at noon because the planet’s rotation has presented that locale—and all others of similar longitude—directly to the sun. The wandering nature of sunrise and sunset that you observe exists due to the Earth’s tilted axis, a twenty-three and a half degree bow that it makes to the sun or to space depending upon the season. Twice in the Earth’s yearly revolution around the sun its tilt is perfectly perpendicular to the sun’s rays and light is cast directly upon the equator. These are the autumnal and vernal equinoxes (also known as the first day of fall and the first day of spring respectively). If you are on the equator, and it is the equinox, and it is noon. This is what you get*:\nWhat is special—and peculiar—about this photo (besides how funny I look of course)? Look closely. Read the clues. Can you tell?\n*Minor caveat: the perfect alignment of date, time and position to allow me to be exactly on the equator at noon exactly at the equinox (this year, falling on September 23rd at 2:22am Galapagos time) was impossible given our journey. This picture was taken at noon (exactly) thirteen nautical miles south of the equator 37.5 hours prior to the precise equinox. With all this said, I’d say this is both incredibly close and surely close enough 🙂"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:61ab4d58-6d5d-40a2-bf46-96ce083d3ae2>","<urn:uuid:a411db71-1895-4f4d-a903-af437f6bce57>"],"error":null}
{"question":"What are the characteristics of waterproof electrical connectors for landscape lighting, and what protection methods exist for underground cable splices?","answer":"Waterproof electrical connectors for landscape lighting include waterproof caps that feature self-sealing flaps and grease to keep out moisture, similar to wire nuts used indoors but with additional weatherproofing. They can connect multiple wires together and come in different sizes to accommodate various wire gauges. There are also lug connectors with heat shrink tubes and junction boxes for multiple wire runs. For underground cable splice protection, proper methods include using corrosion inhibitors, ensuring thorough insulation, and installing concrete boxes (such as Christy Boxes) with concrete or traffic-rated lids to maintain accessibility. The Scotch Cast repair kit can provide maximum protection for electrical wiring repairs and moisture proofing for direct burial cables.","context":["What low voltage landscape lighting connectors will you need to get your lights to work. There are several different kinds of connectors to use. We will discover the way each type of connector can be used.\nWhy use Direct Burial Connectors\nIf you are just going to twist the wires together and wrap with electrical or duct tape, this is only going to be a temporary fix. The tape will not hold out the moisture and over time the ends of the wires will corrode and loose the ability to transfer the current needed to make your lights work.\nThe direct burial connectors are made to stay underground in the most severe weather conditions. They will withstand heat, cold, wet and invasion by insects or underground animals. These types of connectors will last for several years underground without any attention from the homeowner.\nTypes of Direct Burial connectors\nThe following will be a list of the types of connectors and when to use them. You can use a combination of these, you do not have to use just one kind of connector. Look at the uses of each kind and on your map that you laid out your wire runs, you will select the proper connector for your use. Use this link to see more about what wire size to use for your lighting.\nWaterproof caps are just like the wire nuts used inside your home when connecting two or more wires together. The major difference for the waterproof caps are the self sealing of the flaps and the grease used to keep out the moisture. These caps are made to connect several wires together, so that you can use them to split a run with three wires. Each cap will have the number of wires with each gauge that it can handle, as they will come in different sizes to fit your needs.\nLug connectors with heat shrink\nLug connectors are for straight line connections. It is a metal tube that the wires goes in with a screw on each end to secure the wire to. After the wire is secured the plastic tube goes over the tube and is heated to seal the connection.\nThe junction box will let you use a common box to run your wires from. This is good if you have an area to light that is some distance from the transformer and spreads out in several directions to have similar length runs out to your lights. This will look like a spider web if it is done correctly. One main wire runs from the transformer to the junction box and smaller gauge wires complete the runs to your lights.\nThese type of connectors are what you find in the kits. These are made only to connect the fixture to the line run. Do not use these to connect the run wires together as they are not made to carry the load that runs in a line run.\nStandard connectors will come in the old fashion push type or the newer screw type. The screw type is just heavier duty and easier if you have to change the fixture that is connected for some reason.\nWhen looking to set up your Low Voltage Lights you will have to make sure that you take the connectors into consideration in your planning. The proper connector from the beginning will make sure that your lights will last a long time with very little or no maintenance.\nPlease feel free to contact me with any questions or comments.","How to Splice Direct Buried Electric Cables\nIs there a way to cover spliced wires and lugs to prevent damage? Repairing Direct Burial Electrical Cables.\nSplicing Buried Electrical Cables\nElectrical Question: Is there a way to cover spliced wires and lugs to prevent damage?\n- I am working with a electrical service panel that feeds another service panel.\n- I have a 200 Amp panel with a 100 Amp breaker which has three #2 gauge wires running to a secondary electric service box.\n- The problem is that the #2 wires are direct buried cables.\n- When uncovering them next to the house I found that all three cables have been spliced in the same location.\n- My question is how can this be made safe? Is there some way to cover the bare spliced wires and lugs to prevent any kind of short since they are all three buried close to each other?\nThis home electrical repairs question came from: Ray, from Milton, Florida.\nSee more about Home Wiring for Florida\nThanks for your Electrical Repair question Ray.\nSplice Direct Buried Electric Cables\nApplication: Splicing Underground or Direct Buried Electrical Cables.\nSkill Level: Intermediate to Advanced – Best performed by a Licensed Electrical Contractor.\nTools Required: Basic Electricians Pouch Hand Tools and Voltage Tester Ground or Trenching Hand Tools.\nEstimated Time: Depends on the personal level experience and ability to work with tools, condition of the soil or environment for access to the cable or electrical wiring.\nPrecaution: Identify the affected power source, turn it OFF and then Tag it with a Note before performing any wire splicing or cable repairs.\nSpecial Materials: Depending on the final enclosure to protect the splices, Moisture proof or Water Resistant Materials including Properly sized wire connectors, anti-corrosion ointment, electrical tape, shrink tubing or other materials to perform an approved splice method.\nProtection for Buried Electrical Cable Splices\n- The direct burial splices must be made properly using a corrosion inhibitor and then well insulated.\n- The splices should be accessible as well which could be accomplished by installing a concrete box, such as a Christy Box with a concrete lid or a traffic rated lid if this is in a traffic area.\n- The concrete box may be placed over the splices after the splices have been made.\n- The buried cables should be checked to see if they are properly rated for Direct Burial and the cables should be buried a minimum of 24 inches deep.\n- Open splices such as this should be repaired right away to eliminate a potential shock hazard!\n- The circuit should be turned OFF until the condition has been resolved.\nMore about Repairing Direct Burial Electrical Cables\nThe Scotch Cast repair kit provides maximum protection for the electrical wiring repairs and moisture proofing for direct burial electrical cable. This kit is available in a wide variety of sizes for several applications. The 3M Scotch Cast Kit is available in a wide variety of sizes.\nTesters to Help Solve Electrical Problems\nElectrical Wire for the Home\nComplete listing of electrical wire types and parts used for home projects with electrical code information serves as selection guidelines.\nHouse Wiring Circuits and Circuit Breakers\nThis article looks at common 120 volt and 240 volt house wiring circuits and the circuit breakers that are installed identifying the types and amperage sizes used in most homes.\nBrowse other questions about this topic:\nElectrical Repair 3m scotch cast, ask an electrician, buried splices, cable splice, concrete box, direct burial cables, electrical question and answer, repair electric wire, UF Cable, use type cables\nOr Ask Dave a Question\nBe Careful and Be Safe - Never Work on Energized Circuits!\nConsult your Local Building Department about Permits and Inspections for all Electric Wiring Projects."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:4135a370-6b5b-4d2c-b800-93cc767d13d5>","<urn:uuid:6e15147c-819a-41cb-8843-93f8200c3237>"],"error":null}
{"question":"I am student learning about traditional arts. What natural materials are used in Dhokra and Thangka? Please list them!","answer":"For Dhokra art, the natural materials include: rice husk, mud, alluvial soil, coal, termite nest soil, beeswax, and non-ferrous metals like bronze and brass. For Thangka art, the natural materials include: white cotton cloth, white clay, natural mineral paints derived from gold, pearl, coral, lapis lazuli, weasel bristle brushes, and brocade for framing. Both art forms rely heavily on these specific natural materials for their traditional production processes.","context":["This article is part of The Chhattisgarh Story series (coming soon to YourStory)\nRemember the famous Mohenjodaro Dancing Girl statue from your middle school history books? While you may recall the figurine for its portrayal of confidence and swag, it is also known for being one of the earliest known examples of an intricate and primitive art form that’s in danger of becoming obscure today.\nTracing back its history to over 4,000 years, Dhokra is a folk art that involves the casting of non-ferrous metal like copper or bronze using the lost wax technique. It gets its name from its main practitioners, the Dhokra Damar tribes who are traditionally metalsmiths. Spread over the eastern part of India, including West Bengal, Orissa, Jharkhand, Dhokra tribals are also found in some parts of Chhattisgarh.\nWhat makes this art form unique is the fact that no two Dhokra artworks are alike, and every single sculpture is painstakingly crafted to be one of a kind. The artisans take inspiration from mythology, the environment and simple rituals, and showcase these through their works. This ancient tribal art is appreciated by art enthusiasts from around the globe.\nTime-consuming and intricate, the Dhokra art form involves making a basic clay model of an object and covering it with wax, before etching on finer details. Thereafter, another clay layer is added which acts as a mould into which hot metal is poured. The metal melts the beeswax and pushes it out to take on the shape of the object.\nIt is traditionally passed down from generation to generation and requires years of practice to gain expertise. Bhupendra Baghel of Kondagaon District, Chhattisgarh is one such master craftsman who has been crafting Dhokra artefacts for over 30 years. He learnt the art from his father who, in turn learnt it from his father and so on.\nBhupendra has won a number of awards for his craft, including the Madhya Pradesh state award in 2000 and a silver medal at a national level art fest in the following year. In 2016, one of his art works made it to The Museum of Anthropology and Anthology in Cambridge. He has been part of many national and international exhibitions to showcase Dhokra art.\nAccording to Bhupendra, the speciality of Kondagaon Dhokra is that their sculptures can be up to eight feet tall or even more, as against the standard ones that go up to four feet.\nThe artist starts with an initial base figure made of rice husk and mud, and after this, a layer of alluvial soil is added on top. The artiste then uses sandpaper or a piece of tile to give the figure its distinctive features. At every step, the artiste removes dust using fresh leaves, as even the smallest dust particle can create distortions during casting. A thread made of beeswax is wrapped around the object which is then covered with layers of alluvial soil, coal, termite nest soil and rice husk. These layers are dried one after another. We finally have a mould in which bronze, brass or other non-ferrous metals are put in a furnace and melted. The metal takes the place of the melted beeswax. It can take up to nine days to complete a three-foot high sculpture.\nBut life has not been easy for these Dhokra artisans. In recent years, the cost of raw materials like bronze and brass has been steadily rising. Also, rampant deforestation is leading to a paucity in the supply of other raw materials like beeswax and termite nest soil.\nWhile Dhokra artefacts are available on websites like Amazon, the demand for this unique art form is not as high as one would expect. “At times we don’t even get back the money that we spend on going to exhibitions,” laments Bhupendra.\nLittle wonder then that like with several other traditional Indian art forms, a number of artisans from the younger generations are opting out to choose jobs that offer them more secure incomes.\nOn being asked if his children are ready to follow his footsteps in pursuing this art form, Bhupendra says, “Of course they are interested. But when your work can’t fill your family’s stomachs, will you actually choose it over something that will?”\nThe district administration, for its part, is coming up with new ways to popularise and market this ancient art, and a special website which will give the Dhokra artisans an online platform to sell their artwork is in the offing. This is a significant step towards giving this valued folk art form the limelight it deserves, and comes as a ray of hope for artisans like Bhupendra who are willing to do whatever it takes to keep their art and tradition alive.","Thangka is a scroll painting created in Tibetan script on bright satin and hung as an offering on a religious scroll. Thangka is a unique type of art in Tibetan culture, the subject matter of which includes Tibetan history, politics, culture, and societal life. It can be said that it is a sort of encyclopedia of the Tibetan culture, as well as a precious intangible cultural heritage of the Chinese nation.\nThe production of thangka is a complicated process that necessitates a great attention to detail. It must be done in accordance with the teachings of the sutras, including the ritual that starts before the painting even begins, the manufacturing of the canvas, the drafts, and application of color, the finalization of the designs, the addition of gold and silver, the opening, the stitching, hanging, and the blessing of the work. Every step must be performed with the utmost care. If even one stroke is incorrect, all one’s efforts will be wasted.\n1. Canvas Preparation\nThe first thing that must be done in order to create a Thangka is to select a piece of flat, smooth, and thick white cotton cloth. The canvas must not be dirty, and must not be holed or perforated. The canvas should also be long and wide, with proportions suitable to the size of the cloth. Either side of the canvas can be used, but there must not be a crease or fold on the surface, or else this will potentially affect the design.\nThen, the cloth is carefully sewn onto four lengths of bamboo which are tightly strung to a large wooden frame. The artist then spreads a cost of glue over the whole canvas and leaves it to dry. He stirs up a mixture of white clay, water and glue in a clean pot to the consistency of thick cream. Blessed medicines or other sacred substances are added if available. The mixture is then strained through fine gauze to remove any impurities and applied evenly to the dry canvas. When this second coat has dried, the canvas is held up to the light; and the areas which have not been evenly coated are patched up with more of the clay mixture and again left to dry. This process is repeated 8-10 times until the entire canvas is evenly coated.\nThe canvas is then laid upon a smooth wooden board and a small area is moistened with water using a soft white cloth. Section by section, the artist vigorously rubs the canvas smooth with a piece of white marble, moistening it with water as he works. This takes about an hour. The entire canvas is then slowly are carefully stretched by tightening the strings tied to the frame and left to dry in indirect sun.\nOnce dry, the entire procedure is repeated for the other side of the canvas, stretching it after each moistening and leaving it to dry. When it has been thoroughly treated and dried, the canvas should be so tightly stretched that it makes a nice drum sound when tapped. This is the sign it is ready to be painted. The front of the canvas is then polished with a conch shell.\n2. The Secret of Colors\nWhen drafting the brightly colored thangka, two kinds of coloring are used. The first is conventional Chinese art paint, which fades over long periods of time. The second is mineral paint, which does not fade. Commonly used paint colors are black, red, white, blue, and rouge. The paint used is all natural ore derived from gold, pearl, coral, colored glaze, lapis lazuli, and so on. In Tibetan culture, gold and silver jewelry is an extremely important adornment.\nCarbon brushes are used to design the thangka, with big, medium-sized, and especially small writing brushes accompanied by paint cases and an easel. When producing a thangka, there are many small details one must mind. Many of the elements of the painting process require very careful attention, and some require the use of a very fine brush. Thus, many brushes used are made of very fine weasel bristle.\nTHANGKA PAINTING MAKING PROCESS\n1. Making the First Draft with Carbon Brushes\nTo sketch the figures in a thangka, the artist must be an expert in the measurements and proportions of Buddhas, Boddhisattvas and deities. There are thousands of different deities in Tibetan Buddhism. The artist will have to rely on a grid of exactly positioned lines to sketch the deities. The basic system of these coordinates is one vertical and two diagonal lines. The intersection of these three lines defines the centre of any thangka. In thangka with more than one figure, there will be additional circles and connecting lines, to contrast the main icon with the background figure. The grid system divides the painting into different parts with fixed proportions. If the artist wants to have a thangka twice the original size, he has to double the dimensions or distances between all the lines.\nThere is a definite, specific sequence to color application. In general, the thangka is painted from top to bottom. The first step is the sky. The dark green landscape and all the dark green areas are next. This is followed by light blue, then light green, red, orange, pink, brown, pale orange, yellow, pale yellow and finally white. When the whole series of base coat colors have been applied and allowed to dry, the thangka is scraped with a razor blade, held at an arched angle to the cloth, to smooth away any roughness in the paint. The dust is brushed off with a soft cloth or feather.\n3. Redrawing and Shading\nThe original detailed lines of the clouds and flowers which have been covered by paint are redrawn in pencil and traced over in black ink. The artist then shades them with a fine paintbrush.\nPainting the intricate details of the back and foreground landscape and brocade clothing designs follows the same sequence of color application as above. This takes 18 to 20 days to complete.\n5. Body Shading and Final Painting\nThe artist then shades in color to give shape to the figure’s body and face. The flowers are given a final shading, and all the minute background details such as fish, deer, birds, fruit and countless grass blades are painstakingly painted.\n6. Gold Application\nA considerable quantity of gold is used to highlight the painting and give it its final glorious touches.\n7. Opening the Eyes\nThis is the most important moment of a thangka artist’s work. Before painting the figure’s eyes, the artist bathes and makes offerings to the Buddha’s body, speech and mind.\nThe tailor will affix a brocade frame to the completed thangka.\nThis final step is what distinguishes Tibetan Buddhist practice from ordinary “idol worship.” The practitioner takes his or her newly completed thangka to a highly realized Buddhist master and makes offerings to request the master’s blessings. The master, endowed with the clear mind of enlightenment, is able to “bring alive” the image on the thangka by infusing it with energy and beseeching the deity to open its eyes and look upon all sentient beings. The thangka, having now been properly consecrated, is a receptacle of wisdom. It is ready to be hung and venerated as a genuine living embodiment of enlightened mind.\nIt is important to note that this final step is only necessary if the thangka artist himself is not acknowledged as a realized being. Over the centuries, many important Buddhist masters have intentionally taken rebirth as thangka painters, and if such an artist creates a thangka, the very mind of the artist naturally consecrates the image being painted. In such cases, there is no need to seek the services of a lama for an additional consecration."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:22cd6b99-d07f-40ef-ae11-22e615867e63>","<urn:uuid:d6bc4a50-20b5-4f90-aaa0-4fc5196831f8>"],"error":null}
{"question":"In what ways can music therapy help patients with neurological conditions, and what specific volume levels should be avoided to prevent hearing damage?","answer":"Music therapy has shown remarkable benefits for neurological conditions - it can organize cerebral function in Parkinson's and Alzheimer's patients, and has even helped nearly comatose patients become animated and engage in dialogue. Studies show it aids memory, triggers dopamine release for better cognition, and enhances brain function, particularly in those with musical training. Regarding safe volume levels, sounds should be kept at or below 70 decibels (comparable to normal conversation). Exposure to sounds at or above 85 decibels can cause hearing loss, and noise above 120 decibels can cause immediate harm. The average noise level should not exceed 85 decibels over an eight-hour period, and exposure to 100-decibel sounds should be limited to less than 15 minutes.","context":["Perhaps you recall watching record-breaking Olympian swimmer Michael Phelps listening to music via earphones before his races at the Athens, Beijing and London Olympics. The world’s most-decorated Olympic athlete listened to a mix of rap with a little techno to narrow his focus and pump him up pre-race. Scientific evidence supports the advantage that music has in improving blood oxygen capacity and performance and lifting feelings of fatigue in athletes. But what about music’s effect on the rest of us, especially older adults?\nScientific researchers and music therapy professionals across the globe are continuing to discover how listening to music positively affects cognitive, physical and emotional health, and social well-being.\nIn the Alive Inside film, educator and social worker Dan Cohen worked with renowned neurologist Dr. Oliver Saks to capture the transformation of nursing home patients who are given iPods with a playlist of music from their youth.\n“I regard music therapy as a tool of great power in many neurological disorders — Parkinson’s and Alzheimer’s — because of its unique capacity to organize or reorganize cerebral function when it has been damaged,” Dr. Sacks said.\nIn the film, nearly comatose patients become animated by the music and even engage in dialogue.\nFollowing are some ways music positively impacts the health of seniors and others:\n» Decreases anxiety and soothes pain. A 2011 report of cancer patients shows that musical interventions calm patients from operating rooms to family clinics. Dentists and doctors report that patients who listen to mellow music before, during and after surgery and medical procedures report less pain and anxiety and require less sedative medication.\n» Reduces stress. A comprehensive review of 400 research papers on music’s neurochemistry found that music reduces stress levels. Listening to and playing music significantly lowers the stress hormone cortisol, which also is known for weight gain.\n» Lowers blood pressure and boosts heart health. Listening to joyful music helps blood flow through blood vessels. In a University of Maryland Medical Center study, the diameter of blood vessels expanded by 26 percent when a person listened to happy music and constricted by 6 percent when a person listened to anxiety-triggering music.\n» Boosts the immune system. Music is shown to increase helpful antibodies, particularly immunoglobulin A, and supports cells that attack bacteria and germs invading the body.\n» Lifts mood and decreases depression. Both singing and listening to music increase endorphins, which boost feelings of happiness and pleasure.\n» Aids memory. Listening to music triggers the release of the brain’s neurotransmitter, dopamine, which aids cognition, voluntary movement, working memory and sleep. Stanford University researchers conducted brain scans that showed classical music helps the mind sharpen focus and sort information. Spontaneous singing along to music engages the brain’s prefrontal cortex that is responsible for creative thoughts.\n» Assists the aging brain. A 2011 study in the journal Neuropsychology reports that University of Kansas Medical Center researchers discovered people with the most musical training in their lives exhibited the best mental sharpness and scored higher on brain function tests.\n“People respond differently to music, so to garner its positive effects, they should listen to music they like,” said Tina Kreider, owner of Right at Home of Santa Barbara. “Whether it’s classical, country, ragtime or rock-‘n’-roll, music can help aging loved ones recover from illness or injury, improve their mood and recall past events. The benefits to the elderly are as numerous as the types of music!”","Statistics Show Nearly One Billion Young People at Risk for Hearing Loss\nWhether you blast music in your headphones or watch movies with spatial audio, you may want to reduce the volume. That’s because lowering the volume by a couple of notches could protect your ears and prevent hearing loss, according to a recent study published in the journal BMJ Global Health.1\nThe study found that more than 1 billion adolescents and young people worldwide could be at risk for developing permanent hearing loss from exposure to common and unsafe listening practices.\nThe World Health Organization (WHO) said more than 1.5 billion people worldwide currently live with hearing loss, and that number could reach over 2.5 billion by 2050.2\n“This supports the need to implement strategies to prevent hearing loss, which can occur by individuals making simple modifications to make their listening habits safe and through government and industry support in implementing policies focused on safe listening,” Lauren Dillard, AuD, PhD, lead author of the study and a WHO consultant, told Verywell in an email.\nDillard and her colleagues analyzed data from other studies published between 2000 and 2021 which examined personal listening devices and unsafe listening practices.\nThe analysis included 33 studies involving about 19,000 people between the ages of 12 to 34. About 24% of young people listened to music too loudly from their phones or music players, while 48% attended noisy venues that could impact their hearing. In addition, the researchers concluded that the number of young people at risk of hearing loss from exposure to these unsafe listening practices ranges between 0.67 to 1.35 billion.\nWhat Is Considered a Dangerous Noise Level?\nSound is measured in units called decibels (dB). The higher the decibel level, the louder the noise is.3 For example, normal conversations can range from 60 to 70 dB, whereas a motorcycle engine, sporting event, or concert can range from 95 to 110 dB.4\nThe National Institute on Deafness and Other Communication Disorders recommends keeping sounds at or below 70 dB. However, long or repeated exposure to sounds and noises at or above 85 dB could cause hearing loss.3 In addition, loud noise above 120 dB can cause immediate harm to your ears.4\nOver the course of an eight-hour day, the average noise level should not be more than 85 dB, per the National Institute of Occupational Safety and Health (NIOSH). Additionally, sounds that are 100 dB should not be listened to for more than 15 minutes.\nWhy Are Younger People at Higher Risk of Hearing Loss?\nYounger people are particularly vulnerable to hearing loss because of their frequent use of personal listening devices, such as smartphones, headphones, and speakers, according to Viral Tejani, AuD, PhD, an audiologist and assistant professor at Case Western Reserve University School of Medicine.\nAdolescents and young adults are also more likely to visit entertainment venues, nightclubs, concerts, movies, and sporting events. Engaging in these activities places them at risk of developing hearing loss early in life.\n“What can occur as a result of attending these events is a temporary threshold shift,” Tejani told Verywell in an email. “There is a temporary decline in hearing that recovers. However, this recovery isn’t really a true recovery.”\nTejani explained that sounds are funneled to the sensory cells—also called hair cells—in the hearing system. These hair cells are responsible for sending to our brain through complex pathways. However, “our hair cells are not always going to be resilient—repeated noise exposure will eventually compromise our hair cell function and lead to diminished hearing ability,” he said.\nLoud noise exposure can also cause tinnitus—a ringing, humming, crackling, or roaring in the ears. It can occur along with hearing loss.5\nWhat Can You Do to Prevent Hearing Loss?\nYou can still enjoy listening to music while protecting your hearing, experts say. Here are some recommendations:\n- Keep the volume of devices down (a rough estimate is below 60% of the maximum volume).\n- Try noise-canceling headphones to reduce background noise, so there is less of a need to increase the volume of the device to overcome the background noise.\n- Use disposable earplugs in loud music venues, concerts, or sporting events, and position yourself further away from the speakers.\n- Limit the amount of time you spend doing noisy activities. For example, listen to music from your device for less time per day and take breaks away from loud sounds when at an entertainment venue.\n- Monitor your listening levels through built-in safe listening features on your phones or consider sound level meter apps that can quickly measure sound levels in your environment.\n- Pay attention to warning signs of hearing loss, including ringing or buzzing in your ears and difficulties hearing high-pitched sounds or following conversations.\nIf you’re concerned about hearing loss or want to know ways to reduce your risk of hearing loss, have a conversation with your healthcare provider.\nArticle originally appeared on VeryWell"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e95dec08-30ba-4028-91f9-032ef7c93d86>","<urn:uuid:7bfc18f6-7853-46e1-bcb4-38dc1dfa0c21>"],"error":null}
{"question":"I'm researching transportation policy and would like to know: What evidence exists about the effectiveness of traditional road development for managing congestion, and how does this contrast with the infrastructure demands of the ongoing transition to electric vehicles?","answer":"Traditional road development shows limited effectiveness in managing congestion. The What Works Centre for Local Economic Growth's review of 2,300 evaluations found only 17 robust studies on roads' local economic impact, with mixed results. Congestion didn't always decrease with new roads, and traffic growth sometimes exceeded forecasts due to induced demand on minor roads. In contrast, the transition to electric vehicles presents clear infrastructure demands - by 2030, over 130 million EVs will require about 50 million AC charge points across homes, workplaces, and recreational places in China, the EU, and the US. This transition is being accelerated by government policies, such as Denmark's ban on gas-fueled vehicles by 2030 and California's 2035 deadline for gas-powered vehicle sales.","context":["Last month the government announced a £1bn-a-year plan to relieve congestion, including a bypass fund to take traffic around cities and towns with the worst jams.\nThe long-term plan is to improve productivity across the country by upgrading 3,800 miles of A-roads maintained by local authorities, who expect to receive a significant amount of funding to make that happen. It also aims to improve connectivity, responding to local growth priorities, and unlocking not new housing opportunities as well as economic potential.\nRoad schemes are broadly considered as a reliable way to take lorries out of town centres and make people’s journeys faster and easier. Unfortunately, however, there is little robust evaluation evidence on the impact they have on local economic development.\nThe What Works Centre for Local Economic Growth reviewed 2,300 evaluations of the local economic impact of transport projects, and found only 17 robust evaluations looking at the local economic impact of roads – and the findings on impacts are rather mixed.\nOn one hand, there is some evidence showing that bypasses took traffic out of the towns and city centres, with a mixed or negative effect on population depending on the nature of the project. For example, the evidence suggests that new highways can result in a population drop in city centres, but an increase in suburban areas. Congestion did not always decrease as a result of new roads, and traffic growth on new roads was sometimes higher than forecast. This is because of induced traffic on minor roads, which sometimes increased demand for newer roads.\nWhen it comes to jobs, there is some evidence to suggest that roads can positively impact local employment, but the majority of evaluations showed either no (or mixed effects) on employment. Of the six evaluations on local employment reviewed by the What Works Centre, two find a positive impact, three find no impact and one shows mixed effects (i.e. increasing, static and decreasing total employment in the areas around highway expansion). One study shows that improved accessibility increased employment and attracted new firms in the areas, but it’s hard to tell if it was a result of firm relocation outside of congested places.\nTo take a real world example, the introduction of a bypass around the Sussex town of Polegate means that people who would have once driven through the town now bypass it to Eastbourne for shopping trips. This may have contributed to economic decline for retailers and shop closures since the opening of the bypass. In the local economy, there is some evidence of the positive effect on wages and productivity, but the lack of quality evidence is worrying.\nThe local economic benefits of road infrastructure are therefore not as clear as they might seem in the government’s announcement. As such, local government should not only focus on how to take traffic out of cities and town centres, but also on how to tools to reduce congestion full-stop.\nImproving public transport, average occupancy rate, encouraging car-pooling, walking and cycling is the best way of taking cars off the road. Good city and transport planning can improve public transport by planning the city around its network and making it as easy as possible to catch a bus or train. For major cities, the use of technology and sophisticated traffic-signal timing that varies with traffic loads and the introduction of a congestion pricing is the most effective way of reducing congestion based on evidence.\nRoad investments are expensive schemes and have little measurable economic benefits. If local authorities do undertake these bypass projects, they should develop adequate evaluation of the schemes.","There is a surge in the number of Electric Vehicles (EVs) on the roads across many parts of the world. The regions with the most EVs are China, the United States, and the European Union. In these countries, the major […]\nThere is a surge in the number of Electric Vehicles (EVs) on the roads across many parts of the world. The regions with the most EVs are China, the United States, and the European Union. In these countries, the major charging points are in residential and commercial buildings. Upgrading these buildings’ electrical infrastructure will be crucial to ensure a constant supply of electric power needed for the growing number of electric vehicles.\nBy 2030, there could be over 130 million electric vehicles on the world’s roads. According to a top consulting firm, McKinsey & Company, expansion of charging points and capacity will require about $110-$180billion.These charging stations will be distributed across residential places, workplaces, and public spaces such as parking lots and malls. Urban planners, building developers, and electrical equipment suppliers need to come together to come up with buildings with great electrical infrastructure in their future projects. Existing buildings also need to be upgraded to provide adequate access to EV charging points as demand rises.\nEV sales are projected to Skyrocket in the next 5 years. For instance, in the EU, the number of EVs on the roads is expected to increase fourfold. In the US, the sales of EVs are expected to double. Generally, the three top players in the EV market will have over 50 million passenger vehicles and four million commercial vehicles by 2025.\nThe acceleration of EV sales has been influenced by the decreased car prices and the policy measures put in place by governments. For instance, Denmark imposed a ban on the sale of gas-fueled vehicles, effective by 2030. In the US, the state of California has given manufacturers up to 2035 to sell gas-powered vehicles. Charging point’s availability depends on the driving patterns and availability of infrastructure. In the US, most charging points are in residential places. This is because most EV users are the wealthy class who has their own homes. The homeowners don’t mind an electrical upgrade to accommodate electricity needs that come with owning an EV.\nLess wealthy people are embracing EVs. This calls for the installation of public charging stations in workplaces and apartment buildings, where most of these middle class live. In China and the UK, homeownership is not well established. This has resulted in a higher number of electric vehicles compared to the number of charging points, thereby increasing the demand.\nProperty owners need to add charging points on their buildings, both AC level 2 chargers and DC fast chargers. The latter might take longer to install as they require large batteries. EVs will need about 50 million AC charge points distributed across homes, workplaces, and recreational places in China, the EU, and the US. Fleet operators also need to upgrade their depot electricity infrastructure to meet the increasing demands as commercial EVs enter the market."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:87ace1e6-f0c8-4c12-85ad-050b53dfcc65>","<urn:uuid:37dadfe7-64bf-4b4f-a9b6-38b8d20984c8>"],"error":null}
{"question":"Why did people start using Carson Tops on custom cars in the 1930s?","answer":"The first Carson Top was built in 1935 by Carson Top Shop employee Glen Houser for a 1930 Ford Model A Convertible. These non-folding padded, smooth lined tops became popular because they were nicely done both inside and out, leading to renovation of seats and trim becoming a primary step in customization. The Carson Top Shop became very busy making these tops for countless customs and hot rods.","context":["- 1 Streamlining\n- 2 Customizing in the 1930s\n- 3 Metallic Enamels\n- 4 Dry Lakes Racing\n- 5 Hot Rods of the 1930s\n- 6 Custom Cars of the 1930s\n- 7 Sport Customs of the 1930s\n- 8 Nordic Special Racers of the 1930s\n- 9 Speed Shops of the 1930s\n- 10 Custom Body Shops of the 1930s\n- 11 Custom Car Builders of the 1930s\n- 12 Hot Rod and Custom Car Clubs of the 1930s\n- 13 Dry Lake Meets of the 1930s\n- 14 References\nStreamlining was generally seen as the most modern of styles in the 1930s. Being a popular expression of machine art, it signified a continuing belief in the beauty and function of the machine. Streamlining was also a sign of faith in science and technology and a sign of confidence in the idea of progress at the height of the Great Depression. Especial popular in America, streamlining was applied to everything from pencil sharpeners to automobiles. Influenced by the smooth designs of contemporary airplanes like the Douglas DC-3, streamlinging represented a shift in the machine aesthetic from an emphasis on machine parts like gears, screws, and pistons, to a focus on the skin of the machine.\nCustomizing in the 1930s\nDue to the great depression taking place, it wasn't common for a teenager to have his own car in the 1930s. If you were lucky enough to have a car, you took care of it, and didn't trade it off because the fashion had changed. One such lucky teenager was early custom pioneer Frank Kurtis. Frank, who was manager of Don Lee's Los Angeless Coachworks, owned a 1928 Ford Model A Roadster that he had bought new. The car was somewhat restyled, featuring a chopped top and windshield. When the 1930 Fords arrived at the showrooms, Kurtis quickly made deal for a set of fenders. The 1930 Ford fenders were longer and more graceful, and called for more car. Frank mounted the A-Bone's transverse front spring on a perch in front of the crossmember, race car style. With a longer wheelbase, Frank face lifted the old Model A with an extended the hood, home fabricated radiator shell and grille. He installed Cadillac headlights and 18-inch wheels. Franks's Model a Custom was quite sensational for its time. There was nothing \"hung on\" it, except for a spotlight, and a new trend in smoothing the lines of stock cars was emerging.\nGeorge DuVall was another early custom pioneer. A custom trend in the early 1930s was to build bizarre and unique grilles. The 1932 Ford radiator shell and grille became an immediate best-seller for transplanting to any car, as did the 1934 Ford grille which could be reworked into the popular heart shape. Working for Southern California Plating Company, George DuVall began creating custom bumpers and grilles, mostly for Depression-era Fords. At the time it was a relatively inexpensive way to give people luxury Cadillac style when a Cadillac was a luxury few people could afford. George DuVall is best known for designing the V'd and swept-back DuVall windshield. Completed in 1936, the Southern California Plating Company's 1935 Ford delivery truck was the first car that wore a DuVall Windshield. The first recognized instance of a DuVall Windshield on a hot rod dates back to the early 1940s, when Jimmy Summers installed a windshield on Jack Dorn's 1932 Ford roadster.\nThe smaller diameter, larger tired wheels, which would fit earlier models were also sought after by early customizers. Lowering the cars became more common, along with esoteric considerations as white leather upholstery and headlining. With the emergence of the flared fender came the fender skirts, followed immediately by the fender skirt lock. The \"nosed-over hood\", \"planed deck\" and interior release latches came on about the same time and grew in popularity.\nAs production cars put the radiator filler cap out of sight, everyone wanted an alligator-style hood that led to a solid front and dropped grille. By the mid-1930s, customized cars had started to pop up all over the United States. As the 1930s drew to an end, the move toward enclosure of all parts and other customizing methods were aimed at smoothing, cleaning and sharpening the stock lines. Work was often done on 1937 through 1939 Fords and Chevrolets. In the early days of customizing, it was mostly open cars that had been preferred. By the end of the 1930s, sedans and coupes were also more often customized, especially club coupes.\nFollowing the emerging streamlining trend, early customizers such as Jimmy Summers and Harry Westergard would lower the rooflines, smoothing contours, stretching and lowering the body on Fords and Mercurys into shapes closer to a teardrop. The cars were restyled to resemble coach-built European and American cars, sharing shapes with streamlined locomotives such as the Commodore Vanderbilt.\nAs chopped hot rods and customs became more regular on the streets of Southern California, the usual yardstick for determine ow much window area to leave was a giant malt or soda glass. If you could get a barely full malt glass through the window while parked at a drive-in, the top had been chopped right. With the new lowered steel tops came new sensitivity to interiors, and upholstery shops started to make good money from stitching up splashily colored materials. In 1935, the first non-folding padded, smooth lined \"Carson Top\" was built by Carson Top Shop employee Glen Houser. The top was built for a 1930 Ford Model A Convertible. The design's popularity quickly grew and the Carson Top Shop became very busy making Carson Tops for countless customs and hot rods. The Carson Tops were so nicely done inside and out, that the renovation of the seats and trim became the primary step in many expressions of individuality.\nAccording to Motor Life May 1955's story on the birth of customizing, one of the most popular custom touches in the 1930s was the inset license plate. This trick was supposedly first performed by Frank Kurtis on an Airflow DeSoto in 1936, and the stoy claims that this feature more than any other feature marked the \"California Car\" back then.\nBy the late 1930s, customized or restyled cars could be spotted anywhere in Southern California. Lowered, de-chromed, with smooth hood and deck, set-in plates and maybe a chopped top. The lines of a custom car created an impression of speed. It was the look of the future. The trend had also reached the east coast by now, and Bob Hart's 1932 Ford Roadster, who was built between 1938 and 1939 is one of the first known east coast customs. It looks like Bob got much of his inspiration from cars such as Southern California Plating's 1931 Ford Model A Delivery Truck.\nMetallic enamels became available in 1935. That was according to George Barris the medium used on most of the early customs. The colors were simple. Maroon was the most popular, but green and blue were also used.\nDry Lakes Racing\nHarry Reinhert began attending dry lake races in 1931. According to him, the first meets were small. \"There were anywhere from fifteen to twenty-five cars, thirty at the most, out there - a few motorcyclists too.\" Dry lake activities for amateurs were spontaneous and informal. Sometimes you could meet a handful of gearheads to talk to and race against, other times you might not find anyone there. If you found someone to race, you raced without rules.In 1931 one of the first known organized amateur speed trials was held at Muroc. It was sponsored by Gilmore Oil Company of Los Angeles. The driving force behind the event was George Wight, the owner of Bell Auto Parts. George realized that something had to be done to coordinate the haphazard dry lakes meets. He got Gilmore Oil Company to sponsor the speed trials if the hot rodders could come to an agreement regarding rules and regulations. Early in 1931 Wight sent letters to rodders in the area inviting them to an organizational meeting to be held in East Los Angeles. The rules that were made were fairly simple in the beginning. In the interest of fairness, classes were established according to engine type: Model T flatheads, Model T Rajos, Model T Frontenacs and Chevrolets, Model A flatheads, and Model A overhead valve conversions. Supercharged cars were not allowed to compete. The first organized meet was held March 25, and the second was held April 19, 1931. The speed trials were open to all cars within the designated class. A pace car would start up a dozen racers at a time. At around 50 mph the pace car would drop back and the racers would continue to accelerate to the finish line. No \"wildcat warmups\" were allowed, and any car that jumped the start would be given a penalty of a hundred foot handicap when the race was restarted. The need for safety was recognized, and cars returning from a speed run were limited to 40 mph. The cars ran as a group, and the two fastest cars from each class were eligible to run in the open competition from which a single winner would emerge. No records exist of the two Gilmore sponsored runs held in 1931, but it has been documented that Ike Trone won the Gilmore trophy for first place in the Main Event at the second met on April 19th. Ike ran a fenderless, stock bodies 1929 Ford Model A roadster with a Riley head.\nBefore the 1931 season ended, the Muroc Racing Association had been formed, complete with officers and a race program. It collected a one dollar entry fee which was used to meet the expenses. The same year the Purdy Brothers developed an electrical timer to clock the cars speeds. These two helped formalize the meets radically.\nThe quest for speed in the 1930s involved automobile companies as well as individuals. During the summer of 1932 Auburn made several speed runs at Muroc. A stock V-12 Auburn speedster covered the flying mile at 100 mph, and posted an average speed of 88.95 mph for a 500 mile run to set a new track record. In 1932 Ab Jenkins removed the fenders and windshield from a Pierce-Arrow V-12 roadster, and without factory sanction, drove around the ten mile diameter track at the Bonneville Salt Flats for 24 hours. He averaged at 112.93 mph, setting a new record, but it remained unofficial because of a dispute between driver, manufacturer and the AAA board. In 1933, Jenkins returned to the salt as a special consultant on high speed performance for Pierce Arrow. He encouraged the factory to bore the V-12 to 3 1/2\", giving it a displacement of 462 ci. He bought a new V-12 convertible coupe that was specially geared for top end. Driving it around the same ten mile track for 24 hours, Jenkins turned an average speed of 177.77 mph. This meet was sanctioned by AAA, and it recorded that he broke 66 American and 14 world stock car records. Jenkins, who later became mayor of Salt Lake City, wanted to promote the salt as a speed course superior to Daytona Beach, and used his own money to make a three reel film of his 1932-1933 speed runs. Jenkins sent copies to Sir Malcolm Campbell and Reid Railton, who were known as the British speed kings. In 1934 the British invaded the salt. Record attempts were made by Campbell, Railton, John Cobb and Captain G.E.T. Eyston. When Cobb set a world record of 352 mph, Eyston drove 357 mph hours later and broke it. Suddenly speed was in the air, and Bonneville had become the center of straight line record attempts.\nIn 1934 Ab Jenkins returned to the salt with a creation built on Pierce-Arrow V-12 chassis. The chassis was fit with a streamlined racing body and a large tail. The big British cars achieved speed through brute horsepower, using aircraft engines. Jenkins hopped up the V-12 by adding a homemade six-carburetor intake manifold. The heads were milled to increase compression. These modifications upped his horsepowers from 175 to 235, raising his average speed to 127.29 mpg. In 1935 he built a special supercharged Duesenberg, and in 1936 he built the famed Mormon Meteor. With the Mormon Meteor Jenkins brought back all the world record between 50 miles and 48 hours inclusive to the U.S. His speed for 24 hours average was 153.82 mph, and for 48 hours 148.64 mph. Jenkins continues to run the Mormon Meteor, and during the next few years he set more than 25 new records.\nHot Rods of the 1930s\nThe Glen Smith Special\nJack Engle's Ford Model T Roadster\nJulian Doty's 1923 Ford Model T Roadster\nPete Clark's 1925 Ford Model T Roadster\nJim Woods' Ford Model T Roadster\nAl Hawkins' 1927 Chevrolet Touring\nGlen Wall's 1928 Chevrolet Coupe\nDuke Hallock's 1928 Ford Model A Roadster\nIke Trone's 1929 Ford Model A Roadster\nKarl Orr's 1929 Ford Model A Roadster\nHarry Reinhart's 1930 Ford Model A Roadster\nJulian Doty's 1931 Ford Model A Roadster\nRobert Stack's 1931 Ford Model A Roadster\nBob Minor's 1932 Ford Roadster\nBob Westbrook's 1932 Ford Roadster\nDick Noble's 1932 Ford Roadster\nBob Minor's 1934 Ford Tudor\nCustom Cars of the 1930s\nFrank Kurtis' 1928 Ford Model A Roadster\nGeorge DuVall's 1929 Ford Model A Roadster\nFrank Kurtis' 1931 Ford Model A Roadster\nSouthern California Plating's 1931 Ford Model A Delivery Truck\nBob Hart's 1932 Ford Roadster\nGlen Wall's 1935 Ford Phaeton\nSouthern California Plating's 1935 Ford Phaeton\nTommy the Greek's 1936 Ford Phaeton\nRudy Makela's 1936 Pontiac\n1937 Kurtis Tommy Lee Special\nThe Phantom Corsair\nMarquis Hachisuka's 1937 Lincoln Zephyr\nThe Don Morris Special\nBill Hughes' Roadster\nLink Paola's 1940 Ford\nAlbrecht Goertz' 1940 Mercury Coupe\nSport Customs of the 1930s\nNordic Special Racers of the 1930s\nSpeed Shops of the 1930s\nCustom Body Shops of the 1930s\nCustom Car Builders of the 1930s\nHot Rod and Custom Car Clubs of the 1930s\nDry Lake Meets of the 1930s\nDid you enjoy this article?\nKustomrama is an encyclopedia dedicated to preserve, share and protect traditional hot rod and custom car history from all over the world.\n- Help us keep Kustomrama and history alive. For as little as 1 USD a month you can become a monthly supporter. Click here to learn more.\n- Subscribe to our free newsletter and receive regular updates and stories from Kustomrama.\n- Do you know someone who would enjoy this article? Click here to forward it.\nCan you help us make this article better?\nPlease get in touch with us at email@example.com if you have additional information or photos to share about 1930s.\nThis article was made possible by:\nSunTec Auto Glass - Auto Glass Services on Vintage and Classic Cars\nFinding a replacement windshield, back or side glass can be a difficult task when restoring your vintage or custom classic car. It doesn't have to be though now with auto glass specialist companies like www.suntecautoglass.com. They can source OEM or OEM-equivalent glass for older makes/models; which will ensure a proper fit every time. Check them out for more details!\nDo you want to see your company here? Click here for more info about how you can advertise your business on Kustomrama."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:b5680702-4127-42e0-b6fe-7f20bc80a30c>"],"error":null}
{"question":"How does cold housing affect health costs, and what environmental solutions could reduce these health impacts? 🤔","answer":"Cold housing causes at least 10,000 deaths annually and costs the NHS around £860 million in England for treating affected patients. People with respiratory and cardiovascular illnesses are particularly vulnerable to cold homes. Green infrastructure solutions, such as urban vegetation and trees, can help by reducing air pollution, particulate matter, and ground-level ozone that cause respiratory ailments. Additionally, green infrastructure helps regulate urban temperatures, reducing heating needs and associated energy costs while providing health benefits by encouraging outdoor physical activity.","context":["At least 10,000 people die each year due to a cold home3, while treatment costs the NHS around £860 million2 in England alone.\nThe scheme also helps identify “hard to reach” homes that are eligible for funding to improve insulation and energy efficiency.\nWarm Home Prescription is an innovative new service being piloted by Energy Systems Catapult and the NHS helping vulnerable people with both cold-sensitive health conditions and low incomes, to stay warm and well at home, and out of hospital – by paying their energy bills over winter.\nMillions of people in the UK have health conditions – such as respiratory and cardiovascular illnesses – that are made worse by living in a cold home. With over 10,000 people dying each year in England and Wales as a result of living in cold homes.\nThis pilot study aims to determine whether it is more cost-effective overall to help pay the heating costs of vulnerable people than it is to pay for their health care if they fall ill – saving the NHS money and reducing pressure on frontline staff.\nLast winter, Energy Systems Catapult successfully ran a pilot in Gloucestershire working with NHS social prescribers and a local energy charity, Severn Wye. Between December 2021 and March 2022, twenty-eight patients were “prescribed” warmth to reduce the risk that the cold would cause harm and hospitalisation.\nIn the same period, over 2,000 people5 with similar health conditions in Gloucestershire fell ill and were admitted to hospital for emergency treatment, costing the NHS over £6m5 and adding pressure to front line staff.\nThe Warm Home Prescription 2021/22 pilot study found the service was:\nQuick and easy to prescribe,\nImmediately impactful – meaning people didn’t have to choose between heating and eating,\nPositive for recipients – with people saying they felt warmer, healthier, less stressed about bills and less likely to visit GPs or hospital,\nIntegrated easily with other health and energy efficiency services, further improving health.\nGloucestershire were expanding the scheme during winter 2022/23, with up to 150 people across the county with cold-sensitive health conditions set to receive support – with energy bills paid between November 2022 and March 2023. The initiative is being funded through innovative use of the Government’s Housing Support Fund.\nEnergy Systems Catapult are testing a scaled up version of Warm Home Prescription this winter with around 1,000 people to be supported across the Tees Valley in England and Aberdeenshire in Scotland.\nVIDEO: Case study of Warm Home Prescription pilot project in Gloucestershire 2021-22\nDr Hein Le Roux, from the NHS in Gloucestershire, said: “People with conditions such as COPD, emphysema, or chronic bronchitis are at particular risk from complications associated with living in cold housing.\n“The Warm Home prescription allows us to be more proactive in supporting some of the most vulnerable people in our county. We want to stop people from becoming unwell and help them to stay healthy at home in housing that is safe and warm.”\nOwen Callender, Head of Affordable Warmth at Severn Wye Energy Agency, which handled the client facing aspects of the service, said: “Delivered alongside fuel poverty schemes that incorporate energy efficiency interventions, the impact of this project will be huge.\n“Working with social prescribers means we’re able to support clients we’ve never been in touch with before, reducing their bills and their fuel stress. At a time when the NHS is recovering from the impact of the pandemic, this project will have enormous social, environmental, and economic impacts, but most importantly it will allow us to improve the wellbeing of some of society’s most vulnerable people.”\nProfessor Sarah Scott, Executive Director, Adult Social Care, Communities and Wellbeing at Gloucestershire County Council, said: “With the increase in fuel costs and winter fast approaching, it’s important to look at the bigger picture and do what we can to ensure people with certain health conditions don’t have to experience the cold because they can’t afford to heat their home.\n“By coming together as One Gloucestershire health, care, and voluntary sector partners to pool our resources and address fuel poverty in a joined-up way, we can support some of our most vulnerable residents to stay warm and well this winter.”\nDr Rose Chard, Fair Futures programme lead at Energy Systems Catapult, said: “Living in cold homes puts millions with health conditions at risk of real harm. It costs the NHS around £860 million each year in England alone, and causes 10,000 deaths every winter. And it’s set to become an even bigger challenge this year as energy prices rise and household budgets fall.\n“There has to be a better solution to help the most vulnerable. If we buy the energy people need but can’t afford, they can keep warm at home and stay out of hospital. That would target support to where it’s needed, save money overall and take pressure off the health service. The scheme will also find homes we can insulate to cut running costs and emissions in future.”\nWarm Home Prescription also helped the local energy charity Severn Wye to find “hard to reach” homes that were eligible for funding to improve insulation and energy efficiency, cutting running costs and CO2 in the longer term (e.g. via ECO).\nGloucestershire Warm Home Prescription Scheme 2022/23:\nTo be eligible for the expanded Gloucestershire scheme, people must be diagnosed with chronic lung conditions such as emphysema, chronic bronchitis and bronchiectasis. They also must be either under 60 and in receipt of free NHS prescriptions, or over 60 and struggling to pay their heating bill.\nHealth and care teams, including social prescribers, district nurses and complex care teams, supported by GPs, will work together to identify eligible patients and prescribe them a warm home, with charity Severn Wye following up the referral to credit peoples’ energy accounts and arranging home energy upgrades where possible.\nThe service will prescribe a heating plan to keep homes at temperatures recommended by public health guidance, support people with further energy efficiency information and signpost to other services that could help.\nBefore the current energy crisis, around 13% of households in England6 struggled to pay their heating bills – even with the Government Energy Bills Support Scheme, it is estimated around 7 million homes or over 25% of households could be in fuel poverty by end of 2022.\nThe government spends around £2.55 billion each year trying to tackle fuel poverty4 (which includes ECO, Warm home Discount, Cold weather payments, Winter fuel payment). Only 16% of that annual spending reaches fuel poor households who are most in need4.\nMillions with health conditions (particularly respiratory/cardiovascular) are vulnerable to harm if they live in a cold home (this causes over 10,000 deaths per year in England and Wales3).\nTreatment for patients living in cold homes costs the NHS in England around £860 million each year2.\nIn Gloucestershire (the pilot area), hospitals spend £2.7 million per month on respiratory conditions alone5.\nIn the UK, GP consultations for respiratory illness in older people increase by as much as 19% for every degree the outdoor temperature drops below 5°C7.\n1 COPD, emphysema, chronic bronchitis or bronchiectasis\nCould keeping people warm and well at home reduce their need for health services? Pilot findings 2021-22\nTo download this file, we would be grateful if you could tell us a little about yourself.\nWe use this information for internal research purposes to help us better understand which energy sector stakeholders are interested in which areas of our work. We do not share your details with any third parties.","Green infrastructure is a cost-effective and resilient approach to our water infrastructure needs that provides many community benefits.\nWater Quality and Quantity\nWater Quality: Stormwater from urban areas delivers many pollutants to our streams, lakes, and beaches - including pathogens, nutrients, sediment, and heavy metals. In cities with combined sewer systems, high stormwater flows can also send untreated sewage into our waters. By retaining rainfall from small storms, green infrastructure reduces stormwater discharges. Lower discharge volumes translate into reduced combined sewer overflows and lower pollutant loads. Green infrastructure also treats stormwater that is not retained.\nFlooding: Conventional stormwater infrastructure quickly drains stormwater to rivers and streams, increasing peak flows and flood risk. Green infrastructure can mitigate flood risk by slowing and reducing stormwater discharges.\nWater supply: Rainwater harvesting and infiltration-based practices increase the efficiency of our water supply system. Water collected in rainwater harvesting systems can be used for outdoor irrigation and some indoor uses and can significantly reduce municipal water use. Water infiltrated into the soil can recharge groundwater, an important source of water in the United States.\nPrivate and Public Cost Savings: When stormwater management systems are based on green infrastructure rather than gray infrastructure, developers often experience lower capital costs. These savings derive from lower costs for site grading, paving, and landscaping, and smaller or eliminated piping and detention facilities. In cities with combined sewer systems, green infrastructure controls may cost less than conventional controls, and green-gray approaches can reduce public expenditures on stormwater infrastructure.\nGround Level Ozone: Ground level ozone or smog, is created when nitrogen oxides (NOx) and volatile organic compounds (VOCs) interact in the presence of heat and sunlight. Smog conditions are usually worst in the summer and can lead to respiratory health problems. Vegetation can reduce ground level ozone by reducing air temperatures, reducing power plant emissions associated with air conditioning, and removing air pollutants.\nParticulate Pollution: Particulate matter refers to the tiny bits of dust, chemicals, and metals suspended in the air we breathe. Because particulate matter is so small, it can enter into the lungs and cause serious health effects. Trees, parks, and other green infrastructure features can reduce particulate pollution by absorbing and filtering particulate matter.\nHealth Effects: Breathing ground level ozone and particulate pollution can cause respiratory ailments including chest pain, coughing, aggravation of asthma, and even premature death. In their triple bottom line study on the benefits of green infrastructure, the City of Philadelphia found that increased tree canopy would reduce ozone and particulate pollution levels enough to significantly reduce mortality, hospital admissions, and work loss days.\nEnergy and Climate Change\nUrban Heat Island: Urban heat islands form as cities replace natural land cover with dense concentrations of pavement, buildings, and other surfaces that absorb and retain heat. Trees, green roofs, and other green infrastructure features can cool urban areas by shading building surfaces, deflecting radiation from the sun, and releasing moisture into the atmosphere.\nEnergy Use: By reducing local temperatures and shading building surfaces, green infrastructure lessens the cooling and heating demand for buildings, reducing energy needs and decreasing emissions from power plants.\nClimate Change: As different parts of the country become drier, wetter, or hotter, green infrastructure can help communities adapt to climate change by increasing the capacity of drainage systems to handle large storms, increasing the resilience of water supply systems in times of drought, and mitigating the urban heat island effect. Urban vegetation can also mitigate climate change by reducing the levels of greenhouse gases in the atmosphere.\nWater/Energy Nexus: Treating and moving drinking water and wastewater takes a lot of energy. By reducing stormwater inflow into sewer systems, recharging aquifers, and conserving water, green infrastructure can significantly reduce energy use.\nHabitat and Wildlife\nHabitat Improvement: Vegetation in the urban environment provides habitat for birds, mammals, amphibians, reptiles, and insects. Even small patches of vegetation such as green roofs can provide habitat for a variety of insects and birds. By reducing erosion and sedimentation, green infrastructure also improves habitat in small streams and washes.\nHabitat Connectivity: Large scale green infrastructure, such as parks and urban forests, also help to facilitate wildlife movement and connect wildlife populations between habitats. Learn how Loxahatchee, Florida is protecting the local watershed and conserving native ecosystems through the Loxahatchee Regional Greenways System.\nGreen jobs: Green infrastructure can reduce a community’s infrastructure costs, promote economic growth, and create construction and maintenance jobs. As demand for green infrastructure skills increases, a range of new training and certification programs has emerged.\nHealth Benefits: More green space and parks encourages outdoor physical activity, reducing obesity and preventing associated chronic diseases such as heart disease, high blood pressure, stroke, Type II diabetes, arthritis, and certain kinds of cancer.\nRecreation space: Green infrastructure’s vegetation and trees can increase publicly available recreation areas, allowing urban communities to enjoy greenery without leaving the city. Additionally, green infrastructure’s vegetation and permeable pavements can reduce noise pollution by damping traffic, train, or plane noise.\nProperty values: By utilizing green infrastructure in construction and increasing vegetation and tree cover, green infrastructure can increase property values"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e21eb53c-32ba-4db1-88b4-33904db053d7>","<urn:uuid:9cb7dab0-7fea-445e-bbe3-af81dae72270>"],"error":null}
{"question":"How do modern flour milling techniques affect bread quality, and what testing methods are used to identify Celiac disease?","answer":"Modern flour milling uses a roller process that effectively breaks open starch granules and produces consistent quality flour, unlike stone grinding which creates dense textured loaves. The process involves cleaning the wheat, softening bran skins, and multiple grinding stages that produce various streams of flour that are mixed together. For Celiac disease testing, several methods are used: genetic testing for HLA DQ2 or DQ8 markers, a Celiac panel that tests for gliadin antibodies, tissue transglutaminase antibodies, and anti-endomysial antibodies. Additionally, both IgA and IgG immunoglobulins are tested, and some cases may require a small intestine biopsy to check for damage to the villi.","context":["Wheat is unique in being the only grain that contains gluten. Gluten is elastic and can hold bubbles of air, when you make bread you are trapping bubbles of air.\nWashing gluten to find out the quality\nTo do this we need a flour that has a high content of gluten. Gluten is made when protein is mixed with water. To see what gluten looks like take about 20 grams of dough and wash it under a slow running tap. Wash it by squeezing it and the starch will wash away from the dough.\nAfter a minute or two you will have a small ball of gluten left which looks like chewing gum.\nThe difference between chewing gum and gluten is that the gluten will try to regain its shape when stretched. This ability means that it can hold bubbles of air.\nTo make air to fill the bubbles in bread we use the starch in the flour. In the fermentation process the starch breaks down into sugar and the yeast feeds on it to create gas. If you grind the flour too lightly, you don't break enough starch granules to give food for the yeast to create gas. You can go too far and over grind the flour which then gives too much starch damage. This then lets the dough ferment too quickly and the dough collapses or goes very slack.\nTo make consistent quality flour we need to grind the flour to break open the starch granules, and use wheats with a high gluten content. Stone grinding flour doesn't break open enough starch, so loaves are small and have a dense texture. Most stoneground flour is firstly stoneground and then rollerground afterwards.\nThe Roller Milling Process\nFirstly we clean the wheat, taking out the straw, small seeds like cockle and vetch as well as oats and barley. Then we add water to soften the bran skins. This stops the bran from going to powder when we grind giving better bran flakes.\nWheat has an odd shape, it has a crease down the side of it. This means that we can not just polish off the bran, as we would be left with bran in the crease.\nWe split the wheat open using steel rollers that have small teeth in them. You can see the teeth at the top of the picture. This roll is going away to be re-cut. The stamp on the side says it was made on the 24/10/1939.\nThe top roll goes faster than the bottom. The wheat sticks to the bottom roll so that the faster top roll splits it open.\n1st Break (grind)\nAfter the first grind we have a mixture of bran, semolina and flour. It is then sieved. The bran goes off to be cut again as not all the endosperm has been released.\nSemolina (term of granularity, when it is small enough we call it flour)\nAt every grind we create semolina. It is sieved on a machine that has air flowing through it. The small particles of bran float off and the heavy semolina falls through the sieve. You can see the wheat germ in the picture. This gets flattened by the rolls and can be sieved out at this stage. The semolina then goes to smooth rolls to grind it into flour. Semolina is very hard so we gradually reduce the semolina into flour by grinding and sieving several times.\nEvery time we grind we produce flour. So at any one time we are producing more than 15 streams of flour. They are all mixed in together to make one flour. If we want wholemeal then all the bran and wheatgerm are added back as well.\nThe end result is a consistent quality flour. It can absorb water (slows down the staling of bread) and feed the yeast (to give more lift).","Celiac Disease and Thyroid Health\nPublished June 30 2014\nCeliac disease is an autoimmune condition that is triggered by gluten. This condition affects both adults and children, and while people with this condition can experience gastrointestinal symptoms such as bloating, gas, stomach pain, diarrhea or constipation, it’s common for others to experience extraintestinal symptoms. Some of these signs and symptoms include dermatitis herpetiformis, anemia, osteoporosis, weight loss, female infertility, neurological problems, and other health issues. As I’ll discuss in this article, there is a higher prevalence of Celiac disease in autoimmune thyroid conditions.\nLet’s begin by taking about why people develop Celiac disease. Just as is the case with most autoimmune conditions, Celiac disease is triggered by a combination of genetic and lifestyle factors. With regards to the genetics of Celiac Disease, more than 95% of patients with Celiac disease share the major histocompatibility complex II class human leukocyte antigen (HLA) DQ2 or DQ8 haplotype (1). As a result, if someone tests negative for these markers then there is a pretty good chance they don’t have Celiac disease, although not having these markers doesn’t completely rule this condition out, and of course there is the possibility that the person will have a non-autoimmune gluten sensitivity issue, which I’ll discuss later in this article.\nAdditional Testing For Celiac Disease\nIn addition to testing for the genetic markers of Celiac disease, one can also obtain a Celiac panel. Although a negative Celiac panel doesn’t always rule out Celiac Disease, obtain such a panel is usually a good place to start if someone suspects a gluten sensitivity issue. Some people get the gliadin antibodies tested, but keep in mind that it’s possible for these to be negative even if someone has Celiac Disease. If a problem with gluten is suspected you want to test the gliadin antibodies, the antibodies to tissue transglutaminase, and the anti-endomysial antibodies. It’s also a good idea to test both immunoglobulin A (IgA) and immunoglobulin G (IgG). The reason for this is because if one of these immunoglobulins happen to be depressed it can result in a false negative result.\nThe company Cyrex Labs has a comprehensive test for gluten called the Wheat/gluten proteome reactivity and autoimmunity panel. This measures one dozen of the markers associated with gluten, and tests the IgA and IgG for each one. Although it’s a great test, very rarely do I order it, as I usually just recommend for my patients to just avoid gluten. However, if someone really wants to know if they have an intolerance to gluten then this is a test to consider obtaining. This is a more comprehensive test than a Celiac panel, as in addition to measuring alpha gliadin antibodies it also measures additional antibodies against wheat proteins and peptides. In addition, whereas a Celiac panel will only measure transglutaminase-2 antibodies, the Cyrex Labs panel will also test for the antibodies against transglutaminase-3 and transglutaminase-6. This is important, as is it possible to test negative for the transglutaminase-2 antibodies, yet be positive for either the transglutaminase-3 or transglutaminase-6 antibodies.\nIf one of the antibodies come out positive then the doctor might want to perform a biopsy from the small intestine. The reason for this is because Celiac disease frequently shows damage to the villi of the small intestine. However, just because someone has a negative biopsy doesn’t conclude that they don’t have Celiac disease. If someone has Celiac disease yet has mild intestinal damage then it’s possible to have a negative biopsy. On the other hand, if a biopsy is performed and is positive, then a follow-up biopsy can be conducted at a later date to determine if the person responded well to a gluten free diet.\nSince Celiac disease can affect the absorption of some of the nutrients and can cause anemia, some other tests are important to obtain if one tests positive for this condition. This includes a complete blood count, a complete metabolic profile, an iron panel, as well as checking the vitamin B12 and vitamin D levels. Bone density can also be affected, and so a bone density scan might be a good idea in some cases.\nSo What Exactly Is Celiac Disease?\nI started out by mentioning that Celiac disease is an autoimmune condition that is triggered by gluten. But what actually happens from a physiological standpoint when someone has Celiac disease? In order to better understand the mechanisms behind Celiac disease, it’s probably a good idea to briefly define some of the common proteins and structures associated with this condition. I briefly mentioned gliadin earlier, as this is a protein of gluten. So when you measure the gliadin antibodies, these are the antibodies that your body produces against one of the gluten proteins. Earlier I mentioned how it’s possible for the gliadin antibodies to be negative, yet for someone to still have Celiac disease. The reverse is true as well, as it’s possible to have positive gliadin antibodies yet not have Celiac disease. I’ll elaborate on this shortly.\nTransglutaminase is a calcium-dependent enzyme that forms a complex with ingested gliadin and causes specific deamidation of gliadin (2), which in simple terms means that it modifies the protein structure of gliadin. It’s a complex process, but what’s important to understand is that autoantibodies against transglutaminase confirms that someone has Celiac disease, and thus will need to avoid gluten. So if you solely have gliadin antibodies then this isn’t considered an autoimmune process, and the reason for this is because gliadin is a protein of gluten, and isn’t part of your body. On the other hand, transglutaminase is an enzyme produced by your body, and so when there are antibodies against transglutaminase then this confirms the presence of autoimmunity. What about endomysium antibodies? The endomysium wraps around the muscle fibers, and so if you have positive anti-endomysial antibodies then this also confirms that you have an autoimmune component.\nSo without making this too difficult to understand, what essentially happens in someone with Celiac Disease is that someone consumes gluten, and the HLA-DQ2 and HLA-DQ8 antigen presenting cells I mentioned earlier present the gluten peptides to the T cells of our immune system, which results in immune system activation. In a case of “mistaken identity” the body will begin to produce antibodies against tissue transglutaminase, and anti-endomysium antibodies might also be present. When tissue transglutaminase-2 antibodies are present this will result in damage to the cells of the small intestine, and can lead to digestive symptoms, malabsorption, anemia, and some of the other problems I previously discussed. On the other hand, tissue transglutaminase-3 antibodies primarily affect the epidermis (the skin), whereas tissue transglutaminase-6 while affect the tissues of the nervous system.\nWhat Is Non-Autoimmune Gluten Sensitivity?\nNon-autoimmune gluten sensitivity, also known as Non-Celiac gluten sensitivity, is defined as a condition in which symptoms are triggered by gluten ingestion, in the absence of Celiac-specific antibodies and of classical Celiac villous atrophy, with variable Human Leukocyte Antigen (HLA) status and variable presence of first generation anti-gliadin antibodies (3). And so what this is saying is that this condition doesn’t necessarily cause damage to the villi, and there are no autoantibodies to tissue transglutaminase or anti-endomysial antibodies. However, in some cases there are positive gliadin antibodies. And the genetic markers will also be positive in some people.\nMany people with Non-Celiac gluten sensitivity will experience symptoms upon ingesting gluten. As I discussed in the beginning of this article, they might experience gastrointestinal symptoms, or they might experience extraintestinal symptoms, such as fatigue, brain fog, dermatitis, anemia, or muscle and joint pain. The pathophysiology of non-autoimmune gluten sensitivity is still controversial, and whereas an increase in intestinal permeability (a leaky gut) is common in people with Celiac disease who continue to consume gluten, it’s not known whether people who don’t have Celiac disease but are sensitive to gluten are at risk of developing a leaky gut.\nIs Eating A Small Amount of Gluten Okay?\nIf someone has Celiac disease then without question they need to strictly avoid gluten. Eating even a small amount can cause the development of the autoantibodies I discussed before, which in turn will lead to inflammation. I realize it’s not easy to completely avoid gluten, and what makes it even more challenging is that there are many hidden sources of gluten. Speaking of which, if you haven’t done so already I’d recommend reading my blog post entitled “Are You Really Gluten Free?”\nIf someone has a non-autoimmune gluten sensitivity problem should they still be strict when it comes to avoiding gluten? Well, if anyone has any type of gluten sensitivity I think it’s important to strictly avoid gluten. However, while someone with Celiac disease needs to avoid gluten on a permanent basis, there still is controversy over whether someone with a non-autoimmune gluten sensitivity should avoid gluten for the rest of their life. Some healthcare professionals will recommend for people with any type of gluten sensitivity to avoid gluten on a permanent basis. I’m still not sure about this, as while I think that everyone should minimize their consumption of gluten, even if they’re not sensitive to it, I’m not sure if everyone who has a gluten intolerance but doesn’t have Celiac disease needs to completely avoid gluten on a permanent basis. For example, if someone has developed a gluten sensitivity due to a leaky gut, upon healing the gut there is a chance that they might be able to tolerate gluten. But of course keep in mind that there is no good reason for eating gluten, and so even if someone can tolerate gluten it still is a good idea to minimize one’s consumption of it.\nCeliac Disease and Thyroid Autoimmunity\nWithout question there is a much higher prevalence of Celiac disease in people with autoimmune thyroid conditions. This is the case with both Hashimoto’s Thyroiditis and Graves’ Disease.\nCeliac Disease and Hashimoto’s Thyroiditis. One study involved 150 newly diagnosed patients with autoimmune thyroid disease, and the data suggested a significantly higher prevalence of Celiac disease in patients with autoimmune thyroid disease, in particular with Hashimoto’s Thyroiditis (4). A small study involving 27 adults with newly diagnosed Celiac disease showed that they had an increased risk of thyroid autoimmune conditions (5). This study showed that a gluten-free diet didn’t prevent the progression of the autoimmune thyroid condition during a follow-up of one year. So while some people with autoimmune thyroid conditions report a decrease in thyroid antibodies when they stop consuming gluten, this definitely isn’t the case with everyone. Another small study showed that a significant proportion of patients with Hashimoto’s Thyroiditis present signs of “potential” Celiac disease and of activated mucosal T cell immunity (6). Since Celiac disease seems to be more common in patients with hypothyroidism, a study looked to determine whether the absorption of levothyroxine was influenced by the presence of Celiac disease (7). The study showed that levothyroxine malabsorption likely occurs with hypothyroidism and untreated Celiac disease, and that absorption may improve after Celiac disease treatment. This is of course very important, as if someone with hypothyroidism is taking thyroid hormone and it doesn’t seem like it’s working well, it’s possible that they are having absorption problems which is responsible for this.\nCeliac disease and Graves’ Disease. One study looked to determine the prevalence of Celiac disease in people with Graves’ Disease. This involved 111 patients, and the study showed that the prevalence of Celiac disease in patients with Graves’ hyperthyroidism was 4.5% as compared with 0.9% in healthy controls (8). Another study involving 161 patients looked to screen for Celiac disease in patients with Graves’ Disease, and confirmed that patients with Graves’ disease are at a substantial risk of Celiac disease (9).\nCeliac Disease and Thyroid Autoimmunity in Children. Many children also have Celiac disease, and a few studies have shown a higher prevalence of thyroid autoimmunity in children with this condition (10) (11). This is very important, as if you have a child with Celiac disease, even if they aren’t experiencing any thyroid symptoms it probably would be a good idea to screen them for the presence of thyroid antibodies, especially anti-thyroglobulin antibodies and anti-thyroid peroxidase antibodies.\nSteps To Take After Being Diagnosed With Celiac Disease\nIf someone has been diagnosed with Celiac disease then they want to take the following steps:\n1. Avoid gluten…permanently. If someone has Celiac disease, eating even a small amount of gluten can result in the production of the autoantibodies mentioned earlier. As a result, if you have been diagnosed with Celiac disease you really do need to avoid eating gluten on a permanent basis.\n2. Consider avoiding other foods which cross react with gluten. This isn’t always necessary, although one does need to consider that eating other foods such as corn can cause problems in people with Celiac disease. Research has shown that the proteins from corn can cause a Celiac-like immune response due to similar or alternative pathogenic mechanisms to the proteins found in wheat (12). In other words, eating corn can cause a similar response as gluten. So if someone with Celiac disease completely eliminates gluten from their diet but still has overt symptoms, then they might be reacting to corn, or a different type of food.\n3. Conduct tests for anemia, nutrient deficiencies, and bone density. Because Celiac disease results in malabsorption of many nutrients it is a good idea to do some additional testing. For example, everyone with Celiac disease should receive a CBC and an iron panel. Serum testing for vitamin B12, folate, vitamin D, and magnesium also would be a good idea. Although the blood isn’t the best method of evaluating mild to moderate deficiencies of many nutrients, if someone has a severe nutrient deficiency this usually will show up on serum testing. Regardless of what the testing shows, everyone with Celiac disease should be on a good quality multivitamin with minerals. Due the malabsorption of nutrients people with Celiac disease might also have a decrease in bone density. As a result, in some cases having a bone density scan would be a good idea.\n4. Test for a leaky gut. If someone has Celiac disease then I would recommend either doing a test for intestinal permeability (a leaky gut), or just assuming that someone has a leaky gut and then doing things to help them repair the gut. Many people with Celiac disease who stop eating gluten don’t take measures to help repair the gut.\n5. Eat whole, healthy foods. This might seem obvious, but it is possible to be gluten free but still eat an unhealthy diet. And this what many people with Celiac disease do, as they avoid gluten, but eat a lot of “gluten free processed foods”. This includes gluten free cookies, gluten free cakes, gluten free potato chips, gluten free cereal, gluten free pasta, gluten free pizza, etc. I’m not saying that you can never eat these foods, but you of course want to eat mostly whole foods, and minimize the consumption of processed foods, even if they are gluten free.\nThere’s a lot more information relating to Celiac disease that I didn’t cover in this article. For example, there does seem to be a correlation between Celiac disease and irritable bowel syndrome (IBS). Other conditions such as autism and schizophrenia might also be related to gluten ingestion. If you visit pubmed.gov and do some research you will see that there is a correlation between Celiac disease and many chronic health conditions. And when this is the case, eliminating gluten usually will result in a dramatic improvement in the person’s health.\nIn summary, many people have Celiac disease, which is an autoimmune condition that is triggered by gluten. Celiac disease is more common in people with Graves’ Disease and Hashimoto’s Thyroiditis. Although a Celiac panel doesn’t always rule out Celiac disease, if someone is consuming gluten then it’s usually a good place to start. If someone has Celiac disease then they will need to avoid gluten on a permanent basis. They also will want to consider doing testing for anemia, nutrient deficiencies, and perhaps a bone density scan. Having a leaky gut is common for those people with Celiac disease, and so this is also something to consider. Finally, in addition to avoiding gluten, you ideally want to eat mostly whole, healthy foods, and minimize the consumption of processed gluten free foods."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c44190cf-b619-4b5e-a014-6e6ea56e07bc>","<urn:uuid:f9e8589a-4015-43f8-8118-c4859847c029>"],"error":null}
{"question":"What are the key differences between the approach to well-established invasive species like giant hogweed versus newly emerging threats in the EU's strategic planning?","answer":"The EU takes different approaches based on establishment status. Well-established species like giant hogweed are unlikely to make the priority 'Union List' because elimination costs would be too high. Instead, the legislation focuses on prevention and prioritizes species that either haven't yet occurred in the EU or are at an early invasion stage but pose major potential dangers. This preventative approach is considered more cost-effective than trying to combat already established invasive species.","context":["Brussels -- How many plants can be found in the Alps that are not native to that region? Which animals were deliberately or accidently introduced to the Danube? How big a threat will they become to local wildlife? EASIN, the European Alien Species Information Network, launched today by the European Commission's in-house science service, the Joint Research Centre (JRC), takes a first step towards answering these and other questions related to 16 000 alien species currently reported all over Europe. This information network – the first of its kind in Europe – is an important step to deal with the threat of alien species that become invasive. Invasive species present a serious threat to biodiversity and natural resources, with an economic impact estimated at around € 12 billion per year.\nEnvironment Commissioner Janez Potočnik said: 'Invasive alien species are causing growing problems for our natural resources, people's health and the economy. This threat arises from non-native species whose numbers are growing rapidly in an increasingly interconnected world. The EASIN network will help people in Europe get better information about where non-native species are, and how common they are – and that will support better policy making on this difficult issue.'\nAlien species – non-native organisms that become established in a new environment – are on the increase worldwide. Most of them do not present significant risks for their new environment. However, some of them adapt so successfully to the new environment that they become invasive – from being biological curiosities they become genuine threats to local ecosystems, crops and livestock, threatening our environmental and social wellbeing. Invasive alien species are the second leading cause of biodiversity loss, after habitat alteration.\nEASIN facilitates the mapping and classification of alien species by indexing reported data from over 40 online databases. Through dynamically updated web features, users can view and map the distribution of alien species in Europe and select them using criteria ranging from the environment in which they are found (terrestrial, marine or fresh water) and their biological classification through to the pathways of their introduction.\nAt the heart of EASIN is a catalogue that currently contains over 16 000 species. This inventory of all reported alien species in Europe was produced by compiling, checking and standardising the information available online and in scientific literature. Users of EASIN can explore and map geo-referenced information on alien species from the following online databases: the Global Biodiversity Information Facility (GBIF), the Global Invasive Species Information Network (GISIN) and the Regional Euro-Asian Biological Invasions Centre (REABIC). Further data providers will be included over the coming years. The EASIN web tools and services follow internationally recognised standards and protocols. They are free for use, while the data ownership remains with the source, which is properly cited and linked to in EASIN.\nCombating invasive alien species is one of the six key objectives of the EU 2020 Biodiversity Strategy, and the Commission is preparing specific proposals for strengthening legislation in this area.\nAlien species are present in almost every ecosystem type on Earth. In some cases they have become invasive, affecting native biota. They belong to all major taxonomic groups, including viruses, fungi, algae, mosses, ferns, higher plants, invertebrates, fish, amphibians, reptiles, birds and mammals. Invasive alien species can transform the structure and species composition of ecosystems by repressing or excluding native species, either directly by predation or competing with them for resources or indirectly by modifying habitats or changing the way nutrients are cycled through the system. The cost to human health includes the spread of disease as well as allergens; to the economy damage to agriculture and infrastructure; and to the environment the irretrievable loss of native species, damaging ecosystems and the biodiversity that underpins them.\nIt is estimated that 10-15 % of the alien species identified in the European environment have spread and cause environmental, economic and/or social damage. Species like Giant hogweed, signal crayfish, Zebra mussels and muskrats now impact human health, cause substantial damage to forestry, crops and fisheries, and congestion in waterways. Japanese knotweed for example inhibits the growth of other plants, outcompetes native plants, and seriously damages infrastructure, with huge economic implications. Studies have shown that in England, Scotland and Wales, this one plant alone causes € 205 million of damage each year.","The first Europe-wide legislation to combat alien species with far-reaching consequences\nA new regulation governing the control of invasive alien species became effective in all EU states on 1 January 2015. The European Union hopes that it will actively combat one of the greatest threats to biodiversity and thus the functioning of ecosystems. The new regulation has far-reaching implications for authorities as well as for trade in animals and plants. The legislation also incorporates the findings of the EU's DAISIE research project and thus the findings of scientists at the Helmholtz Centre for Environmental Research (UFZ).\nThe Oregon grape (Mahonia aquifolium) is from western North America and came to Europe as an ornamental shrub. Since then it spreads like invasion.\nPhoto: André Künzelmann/UFZ\nThere are around 12,000 alien species currently in the EU and its neighbouring European countries, of which an estimated 10 to 15 percent are considered to be invasive. That means that these invasive species displace native species, impair the functioning of ecosystems or cause economic damage.\nPoliticians are having to set priorities as there is now a very large number of invasive alien species. What is being termed a \"Union List\" of these prioritized species is thus to be produced by 2 January 2016 and will list species that pose transnational threats.\nClassification of the respective species as alien in at least three EU countries is the prerequisite for inclusion in this list. The EU DAISIE project undertook important preliminary work in this field. Between 2005 and 2008 researchers created a database, which now contains detailed information on 12,122 species and 2,440 experts on biological invasions in Europe. It also produced a list of the 100 most problematic species - including their occurrences and an assessment of the risk they pose.\nThe EU and its member states are now faced with the challenge of deciding which of them should be included in the Union List. However, it is not intended that this will stop at pure risk assessment but rather will also include the benefits of species and regional considerations.\n\"A species that could be a problem in Norway, for example, is not necessarily a problem in the south of Italy. It will not be easy to agree on a Europe-wide common denominator and yet still take into account regional interests when assessing the health, economic and ecological hazards,\" estimates the biologist Dr. Stefan Klotz of the Helmholtz Centre for Environmental Research. However, he regards the European regulation as being a major step in the right direction, as many countries are very small and, as is known, species do not stop at borders.\nConsequences extending to flower beds and home aquaria\nUnlike a regulation that only provides the framework and first has to be translated into national law, the new regulation has been in effect in all Member States since the start of the year. The direct legal implications will only become apparent when the Union List has been defined. \"The Union List is the key element of the regulation. Once you have the list, comprehensive bans on ownership and sale will apply to the listed species.\nThere will also be obligations on the part of the member states to enforce these bans and to take ongoing management and elimination measures where this is possible with reasonable effort and expenditure,\" states the environmental lawyer Prof. Wolfgang Köck of the Helmholtz Centre for Environmental Research, stressing the consequences that the legislation will have, above all, for commercial trade.\nThere is a special transitional rule for non-commercial owners: owners of companion animals shall be allowed to keep invasive alien species included on the union list until the end of the animals' natural life, if they make sure that reproduction or escape of these animals are not possible.\nThe involvement of science plays a key role\n\"The participation of the scientific community is important to provide an adequate knowledge base to address the problems raised by invasive alien species,\" acknowledges the new regulation explicitly, providing for the establishment of a scientific forum to involve researchers, among other things to produce and update what is being called a \"Union List\" for risk assessment and emergency measures. A first meeting on this subject was held in Seville, Spain, in January.\nAdvice from the scientific sector has already paid off: \"It was originally planned to limit the list of species to be actively combated to 50,\" reports Professor of Biology Ingolf Kühn from the Helmholtz Centre for Environmental Research. \"These plans were then dropped again after protests from the scientific sector. It simply does not make sense to impose an arbitrary number before the Member States have provided their data. Moreover, ecosystems can develop very dynamically and we therefore need to be able to respond flexibly to the latest developments in future. \"\nPrevention is cheaper than cure\nThe new legislation is not purely intended as a regulation to combat invasive species, but it also has a strong preventative character, as it is hard to combat species that have just become established or only to do so at great expense. Therefore only species, which do not yet occur in the EU or are at an early invasion stage but pose a major potential danger, should be given priority on the list. Experts therefore do not believe that the infamous giant hogweed (Heracleum mantegazzianum) or the allergy-inducing ragweed (Ambrosia artemisiifolia) will end up on the \"Union List\", as the cost of eliminating them would be too high.\n\"Promising candidates\" are to a greater extent the species included in an earlier nature conservation regulation and will therefore be explicitly named in the new regulation. These species include Pallas's squirrel (Callosciurus erythraeus) from Asia, the North American eastern grey squirrel (Sciurus carolinensis), fox squirrel (Sciurus niger), ruddy duck (Oxyura jamaicensis), American bullfrog (Rana catesbeiana), painted turtle (Chrysemys picta) and the red-eared slider (Trachemys scripta elegans).\nNature conservation authorities will be faced with a lot of work: within 18 months they have to determine the routes along which the reviled species are introduced into the EU, prepare action plans and establish a monitoring system. At the same time, it is hoped that this work will improve the exchange of information and establish a kind of \"early warning system\" so that unaffected regions can react in a timely manner.\nThe new EU regulation thus represents a core element of the EU strategy for the conservation of biodiversity passed in 2011. The regulation will have far-reaching implications in the coming years for authorities, retailers and even consumers, as it sets ambitious targets for solving the problems caused for nature by increasing globalisation.\nRegulation (EU) No 1143/2014 of the European Parliament and of the Council of 22 October 2014 on the prevention and management of the introduction and spread of invasive alien species\nHelmholtz Centre for Environmental Research (UFZ)\nDr. Stefan Klotz, Prof. Dr. Ingolf Kühn\nDepartment Community Ecology\nPhone: +49 (0)345-558-5302, -5311\nProf. Dr. Wolfgang Köck\nDepartment of Environmental and Planning Law\nPhone: +49 (0)341-235-1232\nTilo Arnhold, Susanne Hufe (UFZ press office)\nPhone: +49 (0)341-235-1635, -1630\nEU about Invasive Alien Species\nDatabase of the EU research project DAISIE (Delivering Alien Invasive Species Inventories for Europe):\nEuropean Alien Species Information Network\nresearch project INVASION - Evolutionary, ecological and social consequences of Biological Invasions\nGlobalisation burdens future generations with biological invasions (Press release from December 20, 2010)\nConsequences of being rich: wealth and population are key drivers of invasive species in Europe (Press release from June 16, 2010)\nEurope's flora is becoming impoverished (Press release from December, 11th 2009)\nEcologists Put Price Tag on Invasive Species (Press release from April 22nd, 2009)\nSignificant increase in alien plants in Europe (Press release September 17, 2008)\nVideo \"Mink invasion\" by UFZ\nIn the Helmholtz Centre for Environmental Research (UFZ), scientists conduct research into the causes and consequences of far-reaching environmental changes. Their areas of study cover water resources, biodiversity, the consequences of climate change and possible adaptation strategies, environmental technologies and biotechnologies, bio-energy, the effects of chemicals in the environment and the way they influence health, modelling and social-scientific issues. Its guiding principle: Our research contributes to the sustainable use of natural resources and helps to provide long-term protection for these vital assets in the face of global change. The UFZ employs more than 1,100 staff at its sites in Leipzig, Halle and Magdeburg. It is funded by the federal government, Saxony and Saxony-Anhalt. http://www.ufz.de/\nThe Helmholtz Association contributes to solving major and urgent issues in society, science and industry through scientific excellence in six research areas: Energy, earth and environment, health, key technologies, structure of matter as well as aviation, aerospace and transportation. The Helmholtz Association is the largest scientific organisation in Germany, with 35,000 employees in 18 research centres and an annual budget of around €3.8 billion. Its work is carried out in the tradition of the great natural scientist Hermann von Helmholtz (1821-1894). http://www.helmholtz.de/\nTilo Arnhold | UFZ News\nDispersal of Fish Eggs by Water Birds – Just a Myth?\n19.02.2018 | Universität Basel\nRemoving fossil fuel subsidies will not reduce CO2 emissions as much as hoped\n08.02.2018 | International Institute for Applied Systems Analysis (IIASA)\nA newly developed laser technology has enabled physicists in the Laboratory for Attosecond Physics (jointly run by LMU Munich and the Max Planck Institute of Quantum Optics) to generate attosecond bursts of high-energy photons of unprecedented intensity. This has made it possible to observe the interaction of multiple photons in a single such pulse with electrons in the inner orbital shell of an atom.\nIn order to observe the ultrafast electron motion in the inner shells of atoms with short light pulses, the pulses must not only be ultrashort, but very...\nA group of researchers led by Andrea Cavalleri at the Max Planck Institute for Structure and Dynamics of Matter (MPSD) in Hamburg has demonstrated a new method enabling precise measurements of the interatomic forces that hold crystalline solids together. The paper Probing the Interatomic Potential of Solids by Strong-Field Nonlinear Phononics, published online in Nature, explains how a terahertz-frequency laser pulse can drive very large deformations of the crystal.\nBy measuring the highly unusual atomic trajectories under extreme electromagnetic transients, the MPSD group could reconstruct how rigid the atomic bonds are...\nQuantum computers may one day solve algorithmic problems which even the biggest supercomputers today can’t manage. But how do you test a quantum computer to...\nFor the first time, a team of researchers at the Max-Planck Institute (MPI) for Polymer Research in Mainz, Germany, has succeeded in making an integrated circuit (IC) from just a monolayer of a semiconducting polymer via a bottom-up, self-assembly approach.\nIn the self-assembly process, the semiconducting polymer arranges itself into an ordered monolayer in a transistor. The transistors are binary switches used...\nBreakthrough provides a new concept of the design of molecular motors, sensors and electricity generators at nanoscale\nResearchers from the Institute of Organic Chemistry and Biochemistry of the CAS (IOCB Prague), Institute of Physics of the CAS (IP CAS) and Palacký University...\n15.02.2018 | Event News\n13.02.2018 | Event News\n12.02.2018 | Event News\n23.02.2018 | Physics and Astronomy\n23.02.2018 | Health and Medicine\n23.02.2018 | Physics and Astronomy"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:65546626-cfdc-4c3c-a389-35666509567c>","<urn:uuid:3ffd4540-dd45-4297-b00e-410970488af0>"],"error":null}
{"question":"What is the significance of the Blue Planet Prize in the environmental field, and how is it awarded?","answer":"The Blue Planet Prize is one of the world's premier environmental awards, considered among the most prestigious in the field of conservation. It is awarded annually by the Asahi Glass Foundation to two recipients - either individuals or organizations - who have made outstanding achievements in scientific research and its application in helping to solve global environmental problems. The prize is presented in Tokyo Kaikan, with notable attendees including Imperial family members, ambassadors, and Asahi Glass Foundation members.","context":["Ecological Footprint Work Receives International Recognition with Prestigious Blue Planet Prize\nJapan’s Prince Akishino and the Asahi Glass Foundation yesterday presented the Blue Planet Prize, one of the world’s premier environmental awards, to Global Footprint Network President Mathis Wackernagel and Dr. William E. Rees, co-creators of the Ecological Footprint, and Dr. Thomas Lovejoy, who was recognized for his work in advancing our understanding of biodiversity.\nTwo Blue Planet Prizes are awarded annually by the Asahi Glass Foundation to individuals or organizations that make outstanding achievements in scientific research and its application in helping to solve global environmental problems. The prize is considered one of the most prestigious in the field of conservation.\nBuilding on Rees’ earlier work on human carrying capacity, Drs. Wackernagel and Rees in the early 1990s developed the Ecological Footprint, the world’s premier resource accounting system, to track humanity’s demands on nature. Dr. Lovejoy was the first to clarify that human-caused habitat fragmentation damaged biodiversity and gave rise to environmental crises.\nThe prize was presented Wednesday in the Tokyo Kaikan, across from the Imperial Palace, with the Imperial Highness Prince and Princess Akishino, ambassadors and members of the Asahi Glass Foundation in attendance. The photo shows Drs. Wackernagel and Rees receiving the trophy from Asahi Glass Chairman Tetsuji Tanaka. Dr. Wackernagel has donated his $300,000 share of the prize money to Global Footprint Network to advance the Ecological Footprint work, and has invited supporters to help match the gift.\n“I am convinced more than ever that it is possible to turn our economies into stewards of our planet,” Dr. Wackernagel said in his acceptance speech. “We cannot continue forever to take more from the planet than the planet can give.”\nLike most nations, Japan is facing resource constraints, but is taking a greater leadership role. As an island nation relying on the importation of an increasing amount of biological resources and energy, it potentially puts itself at risk of supply disruption and/or price volatility. Japan is tackling these challenges head-on and looking closely at its Ecological Footprint.\n“We are part of a fabulous community of thousands of people who have contributed to dreaming with us and who are spreading the dream and building tools and projects in order to turn an abstract idea into practice,” Wackernagel said. “Japan, through its Ministry of Environment, was among the first three countries in the world to review and then apply the Footprint, and we appreciate and encourage Japan’s leadership in this domain. The trends are moving fast, and Japan’s firm and fast engagement is essential for making the world—and Japan—a stable place.”\nThe first Japan Ecological Footprint Report (2009) identified areas of ecological demand and offered policy recommendations to address them. It helped fuel calls to action at both the individual and government levels. In November, WWF-Japan, in collaboration with Global Footprint Network, will launch a second, updated Footprint report for Japan.\n“Japan’s leadership is essential. Japan has felt resource constraints, and has promoted climate change and biodiversity policy internationally,” Wackernagel said, referring to the nation’s leadership in climate change policy through the Kyoto Protocol and in biodiversity preservation through Nagoya Protocol. “We need more of it. In this context, Asahi Glass Foundation and its Blue Planet Prize are noble, courageous and deeply needed. I am utterly thankful for it.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:56343e4d-fc31-46da-adb6-e753aed218e3>"],"error":null}
{"question":"Hi! Could you tell me about how scientists study soil types and what mining does to them?","answer":"Scientists study soil types through detailed classification systems, examining factors like parent material, climate, vegetation, and topography to understand soil formation and properties. They use both national and international classification frameworks, such as Soil Taxonomy and FAO/UNESCO systems, to identify and categorize different soil types. Mining severely impacts these soil systems through multiple destructive processes - it contaminates soil with chemicals, causes erosion, removes topsoil, and destroys biodiversity. These mining activities can transform fertile soil into barren land that cannot sustain plant life, leading to the creation of lifeless wastelands and threatening local ecosystems.","context":["Many natural bodies, such as plants and animals, are discrete\nentities which can be classified and guidelines for their identification\nfollowed. Soils are much more difficult to identify and classify\nthan these discrete bodies for two main reasons: (i) soil is more\nor less a continuum covering the land surface of the earth, not\na set of discrete entities; and (ii) most of the soil is below\nground and therefore not readily visible. Soils grade into one\nanother across the landscape usually without sharp boundaries between\none type of soil and another. Soil surveyors who make maps of soils\nhave to use their skills in reading changes in the landscape coupled\nwith auger borings in the soil to identify the nature of the soil.\nThere are several ways of classifying a soil, from the simple\nto the complex. A soil type may be as simple as ‘a sandy\nsoil’ or ‘a clayey soil’ and this is often the\nperception of many land users, such as farmers or civil engineers,\nwho see it as material they have to deal with to achieve an end\nresult, such as the growing of a crop of wheat, or the building\na road. Simple classifications tend to be of local and restricted\nrelevance only. At the other end of the spectrum is the soil scientist\nwho needs to understand how soils have formed, which types occur\nwhere, and for what the different types of soil can be used. The\nsoil scientist seeks a much broader understanding, with the aim\nof underpinning the use and preservation of this important natural\nresource, and this has manifested itself in a number of detailed\nsoil classification systems worldwide.\nThere is an enormous diversity of soils across the world. This\nis hardly surprising given the fact that soil formation and soil\ntype are influenced by several key factors: the parent material,\nusually rock or sediment, but occasionally organic materials such\nas peat; climate, particularly temperature and rainfall; vegetation\nand other biota; topography/ relief; time; and, increasingly, the\ninfluence of humans. All these factors will have an influence on\nsoil development and hence soil type. The potential for different\ncombinations of these factors across the world is immense and hence\nit is not surprising that there are many thousands of different\ntypes of soil in the world, with different properties and potential.\nJust as Linnaeus produced a classification of plants that is in\nregular use today, over the centuries attempts have been made to\ndevelop classifications of soils on the basis of their properties,\ntheir genesis and in some cases their use potential. Since the\nlate 19th century there have been several national classifications\ndeveloped and used. Most countries now have a system of nomenclature\nfor their soil types. These classifications have grouped together\nsimilar soils and given names to the soils in the various classes.\nThe classifications are mostly hierarchical in the sense that they\nconsist of a number of levels, starting at the top with a number\nof major divisions, which can be subdivided into further subclasses\nand degrees of complexity.\nThere have also been several major attempts at a world classification\nof soils, each of which have differed in the emphasis they have\ngiven to different soil properties. Such classification systems\nenable soil types to be classified and identified within a national\nand international framework. The two most widely used international\nclassifications are those of Soil Taxonomy (developed by the US\nSoil Conservation Service) and based on soil properties that can\nbe objectively measured and observed and the FAO/UNESCO legend/classification\nwhich is along broadly similar lines but not as precisely defined.\nIt is regrettable that to date it has not been possible to produce\nan international classification that has been adopted worldwide\nbut soil science is a young science with an important future and\na unified classification system should eventually be developed.\nUntil that is available, soil types should be named using both\nnational nomenclature and have this linked to one or more of the\nExamples of some soil types from different parts of the world\nare given in the section on Soil Profiles.","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:238b659f-6954-475d-b2d6-68e43592fc23>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"Hey nature nerds! 🌿 What's the key difference in temperature patterns between temperate rainforests and tropical ones? Looking for a clear comparison!","answer":"Temperate rainforests experience significant seasonal temperature changes, varying from 80°F in summer to near freezing in winter. In contrast, tropical rainforests maintain relatively stable temperatures year-round, typically ranging between 72-93°F (22-34°C), with some equatorial forests varying only a few degrees throughout the year.","context":["They are found along the Pacific coast of North America from Alaska south through Canada to California. Though disappearing around the world they are also found in South America, Scandinavia, New Zealand, Ireland, and Scotland.\nWhen you think of a rainforest, do you picture monkeys, parrots, and huge, coiled snakes? Not all rainforests are tropical. The temperate rainforests on the west coast of North America are very different from the tropical rainforests of South America.\nTemperate rainforests have a long growing season, though unlike tropical rainforest they do have a change of seasons. Temperatures change from 80°F in summer down to near freezing in winter. The cooler temperatures mean that temperate rainforests have less diversity than tropical rain forests. (This means that there are fewer types of animals and plants found there.) This doesn’t mean though that things don’t grow there. These are among the most productive forests in the world.\nTemperate rainforests have lots of rainfall (sometimes up to 200 inches a year!) but also have cool, moist air in the form of ocean fog keeping things wet.\nTemperate rainforests have fertile soil. This is a result of all the dead materials rotting into the soil. Decomposing matter releases nutrients back into the soil and is good for growing thick stands of fast growing trees. Trees grow tall here – sometimes over 250 feet and their trunks can reach 15 feet across. Some of these trees can live for 500 years! Because of this, the temperate rainforests have been heavily timbered (trees are cut for lumber products).\nFor a high resolution PDF for printing, scroll to the bottom of the page.\nUnlike tropical rainforests where much of the animal life lives up in the trees, (some never coming to the forest floor at all), in the temperate rainforest, most animals live on or near the ground.\nCrossbills use their scissor-like beak to pry seeds from cones, while mule deer and red tree vole feed on young conifer needles. Porcupines and snowshoe hare feed on conifer bark in winter while black bears stuff themselves with berries in the fall. In addition to seeds, deer mice and Townsend’s chipmunk eat mushrooms that grow well in the wet environment. Other animals found in temperate rainforests include; raccoon, bobcat, mountain lion, mountain beaver, mink, Pacific giant salamander, shrews, pine sisken, and great horned owls.\nMost of the trees in the temperate rainforests of the Pacific Northwest are conifers. The dominant tree is the Douglas fir. It is one of the tallest trees and is considered one of the most important trees to the timber industry. Further south where heavy rains give way to ocean fogs, the great redwoods grow. Other trees that grow in these forests include western hemlock, Sitka spruce, western red cedar, noble fir, and Pacific silver fir. Smaller trees include big leaf maple, dogwood, and vine maple. The forests are shady and damp and shade-loving underbrush thrives here, including Pacific rhododendron, blackberries, salmonberries and thimbleberries, sword ferns, devil’s-club, redwood sorrel, mushrooms, moss and lichens.\nWhen you research information you must cite the reference. Citing for websites is different from citing from books, magazines and periodicals. The style of citing shown here is from the MLA Style Citations (Modern Language Association).\nWhen citing a WEBSITE the general format is as follows.\nAuthor Last Name, First Name(s). \"Title: Subtitle of Part of Web Page, if appropriate.\" Title: Subtitle: Section of Page if appropriate. Sponsoring/Publishing Agency, If Given. Additional significant descriptive information. Date of Electronic Publication or other Date, such as Last Updated. Day Month Year of access < URL >.\nAmsel, Sheri. \"Temperate Rainforests\" Exploring Nature Educational Resource ©2005-2019. November 19, 2019\n< http://www.exploringnature.org/db/view/Temperate-Rainforest-biome >","By Rhett Butler\n(learn more about rainforests)\nTropical rainforests across the world are highly diverse, but share several defining characteristics including climate, precipitation, canopy structure, complex symbiotic relationships, and diversity of species. Every rainforest does not necessarily conform to these characteristics and most tropical rainforests do not have clear boundaries, but may blend with adjoining mangrove forest, moist forest, swamp forest, montane forest, or tropical deciduous forest. Such is the case with the Amazon rainforest.\nGEOGRAPHY AND CLIMATE\nTropical rainforests lie in the \"tropics\", between the Tropic of Capricorn and Tropic of Cancer. In this region sunlight strikes Earth at roughly a 90-degree angle resulting in intense solar energy. This intensity is due to the consistent day length on the equator: 12 hours a day, 365 days per year. This consistent sunlight provides the essential energy necessary to power the forest via photosynthesis.\nBecause of the ample solar energy, tropical rainforests like the Amazon are usually warm year round with temperatures from in the 72-93°F (22-34°C) range, although forests at higher elevations, especially cloud forests, may be significantly cooler. The temperature may fluctuate during the year, but in some equatorial forests the average may vary as little as a few degrees throughout the year. Temperatures are generally moderated by cloud cover and high humidity.\nWhile the Amazon generally conforms to this pattern, parts of the basin periodically experience cold spells, when weather patterns bring cold air from Antarctica.\nclick to enlarge\nAn important characteristic of rainforests is apparent in their name. Rainforests lie in the intertropical convergence zone (ITCZ) where intense solar energy produces a convection zone of rising air that loses its moisture through frequent rainstorms. Rainforests are subject to heavy rainfall, at least 80 inches (2,000 mm) — and in some areas over 430 inches (10,920 mm) — of rain each year.\nIn equatorial regions, rainfall may be year round without apparent \"wet\" or \"dry\" seasons, although many forests do have seasonal rains. Even in seasonal forests, the period between rains is usually not long enough for the leaf litter to dry out completely. During the parts of the year when less rain falls, the consistent cloud cover is enough to keep the air moist and prevent plants from drying out. Some neotropical rainforests rarely go a month during the year without at least 6\" of rain. The stable climate, with evenly spread rainfall and warmth, allows most rainforest trees to be evergreen—keeping their leaves all year and never dropping all their leaves in any one season.\nThe Amazon rainforest does see some severe fluctuations in rainfall, especially in its southern extremes. These extremes are exacerbated in dry years and by deforestation, which reduces local transpiration. Some researchers warn that climate change could heighten the risk of severe droughts in the southern Amazon. Indeed, two of the worst droughts on record occurred in 2005 and 2010.\nMoisture from rainfall, constant cloud cover, and transpiration creates strong local humidity. Each canopy tree transpires some 200 gallons (760 liters) of water annually, translating to roughly 20,000 gallons (76,000 L) of water transpired into the atmosphere for every acre of canopy trees. Large rainforests (and their humidity) contribute to the formation of rain clouds, and generate as much as 75 percent of their own rain. By some estimates, the Amazon rainforest is responsible for creating as much as 50 percent of its own precipitation.\nRainforests are characterized by a vegetative structure consisting of several vertical layers including the overstory, canopy, understory, shrub layer, and ground level. The canopy refers to the dense ceiling of leaves and tree branches formed by closely spaced forest trees. The upper canopy is 100-130 feet above the forest floor, penetrated by scattered emergent trees, 130 feet or higher, that make up the level known as the overstory. Below the canopy ceiling are multiple leaf and branch levels known collectively as the understory. The lowest part of the understory, 5-20 feet (1.5-6 meters) above the floor, is known as the shrub layer, made up of shrubby plants and tree saplings.\nThe dense vegetation in the canopy effectively screens light from the forest floor, and in undisturbed Neotropical rainforest, there is little \"jungle-like\" ground growth. Ground vegetation in primary rainforest primarily consists of lianas, vines, and tree seedlings.\nAn important characteristic of the canopy system is the presence of plants known as epiphytes that grow on canopy trees. Epiphytes use the host tree only for support — they do not draw nutrients away from the host. Epiphytes have adapted well to the hotter and drier conditions of the canopy, developing various means to collect nutrients from their surroundings, the mechanisms for which are discussed in detail in the canopy section.\nAn additional plant type characteristic of the canopy system is the liana, which begins life as a shrub on the forest floor and makes its way up to the canopy by latching on to canopy trees. A related plant type, the hemiepiphyte, begins life in the canopy and grows long roots that eventually reach the forest floor. Once rooted, hemiepiphytes do not have to rely on capturing nutrients from their canopy surroundings, but can access nutrients from the forest floor. The strangler fig is one of the best-known hemiepiphytes.\nUnknown numbers of plants and animals reside in the canopy, the vast majority of which are specifically adapted to life in this leafy world. It is estimated that more than 70 percent of rainforest species reside in the canopy.\nINTERDEPENDENCE AND COMPLEX SYMBIOTIC RELATIONSHIPS\nInterdependence—whereby all species are to some extent be dependent on one another— is a key characteristic of the rainforest ecosystem. Biological interdependency takes many forms in the forest, from species relying on other species for pollination and seed dispersal to predator-prey relationships to symbiotic relationships.\nThese interdependent relationships have been developing for millions of years and form the basis for the ecosystem. Each species that disappears from the ecosystem may weaken the survival chances of another, while the loss of a keystone species—an organism that links many other species together, much like the keystone of an arch—could cause a significant disruption in the functioning of the entire system.\nFor example, Brazil nut trees (Bertholletia excelsa) are dependent on several animal species for their survival. These large canopy trees found in the Amazon rainforest rely on the agouti, a ground-dwelling rodent, for a key part of their life cycle. The agouti is the only animal with teeth strong enough to open their grapefruit-sized seed pods. While the agouti eats some of the Brazil nut's seeds, it also scatters the seeds across the forest by burying caches far away from the parent tree. These seeds then germinate and form the next generation of trees. For pollination, Brazil nut trees are dependent on Euglossine orchid bees. Without these large-bodied bees, Brazil nut reproduction is not possible. For this reason, there has been little success growing Brazil nut trees in plantations—they only appear to grow in primary rainforest.\nBrazil nut tree in Peru\nLife in the rainforests is competitive and countless species have developed complex symbiotic relationships with other species in order to survive. A symbiotic relationship is a relationship where both participant species benefit mutually. Symbiotic relationships appear to be the rule and not the exception in the rainforest. For example, ants have symbiotic relationships with countless rainforest species including plants, fungi, and other insects. One symbiotic relationship exists between ants and caterpillars. Certain caterpillar species produce sweet chemicals from \"dew patches\" on their backs, upon which a certain ant species will feed. In return, the ants vigorously protect the caterpillar and have even been observed carrying the caterpillar to the nest at night for safety. This relationship appears to be species specific in that only one caterpillar species will cater to a particular ant species.\nAll tropical rainforests are characterized by tremendous biological diversity. Section 3 concentrates on the diversity of the tropical rainforest.\n(NEXT Amazon rainforest wildlife)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4e5d0f6a-d4e2-4e0a-a35a-2eaf446886ae>","<urn:uuid:8b6664f2-e073-4bf0-b589-c2f78a3a1835>"],"error":null}
{"question":"Can either BCIS or DNA origami nanostructures be specifically targeted to certain cell types for therapeutic applications?","answer":"Yes, both systems show potential for cell-specific targeting. BCIS can potentially be engineered through modification of their tail pins to target specific cell types, which would be valuable for targeted drug delivery to diseased cells like cancer cells. Similarly, DNA origami structures can be functionalized by attaching specific molecules to their surfaces or within cavities, allowing them to be potentially targeted to specific cells. Both approaches are still in development stages but show promise for precise therapeutic applications.","context":["Für den Inhalt der Angaben zeichnet die Projektleitung verantwortlich.\nThis project is one of the five winners of the call 2016 «Microbials – Direct Use of Micro-Organisms».\nProject partners: University of Zürich; San Diego State University\nFörderbeitrag: CHF 352'000\nDauer: 01.2017 - 05.2020\nMicrobials, seit 2016\nAsst. Prof. Dr. Martin Pilhofer\nInstitute of Molecular Biology and Biophysics\n8093 Zürich (Schweiz)\n- email@example.com. ch\nThe administration of medication is more efficient if the respective drug can be delivered in a targeted way to the exact cells or tissues of the body on which the medication is intended to act on. The delivery of effectors to the cytoplasm of target cells is therefore an important problem in medicine (drug delivery) and biotechnology. Here, we will explore the use of bacterial contractile injection systems (BCIS) as a novel delivery vehicle. These BCIS are recently-discovered bacteriophage-like particles that inject their payload into target cells. Our recent efforts identified potential effectors and showed that some BCIS can accommodate large effector payloads. In an interdisciplinary approach, we will 1) investigate the mechanism of effector targeting/binding in the inner tube and 2) re-engineer the system to accommodate novel effectors of choice. The results will help to develop a bioinformatic tool, which can be used to design novel effectors for the translocation by BCIS in the future. In sum, the project could lead to the availability of re-engineered systems that are effective for antibiotic treatment or drug delivery.\nWas ist das Besondere an diesem Projekt?\nBased on their contractile, membrane-perforating properties, BCIS have been proposed for an application as novel antibacterial agents. The lack of knowledge on how considerable amounts of effectors can be loaded into a BCIS, however, has prevented to engineer them for the use for effector delivery to eukaryotic cells. This project will pave the way to load BCIS with many copies of an effector of choice.\nImpact on drug delivery strategies and biotechnological applications:\nThe groundwork of this project to understand effector binding to the BCIS will have major impact on the function of BCIS in general. The possibility to predict how a given effector has to be engineered in order for it to be delivered to the target presents a novel strategy for effector delivery. This is significant for drug delivery to organisms, as well as for biotechnological tools that can be used for research. The design of cell line-specific BCIS will be the next step in the future, enabled by the work proposed in this project. This vision has great potential, since it might allow targeted treatment of only diseased cells (e.g. cancer cells).\nWe identified the protein 615 as an effector molecule that is delivered by BCIS to cells of a tubeworm larva. We elucidated that the protein 615 is localized inside the inner tube of the BCIS. The protein 605 is likely a targeting factor that guides 615 to the inner tube. This way of loading a BCIS with an effector is novel and presents the possibility to accommodate a high payload per BCIS. Our results also discovered that the 615 N-terminus seems to play a role in guiding the effector to the lumen of the inner tube of the BCIS. We are now in the process to investigate how these findings can be exploited to load effectors of choice and translocate them into target cells. Further exciting results indicate that the so-called “tail pins” might play a role in the binding of the BCIS to target cells. Present and future work will explore whether the re-engineering of the tail pins will allow us to specifically target a certain cell type, which can be important for the efficient delivery of drugs. We envision that our results will pave the way for BCIS being broadly used as a novel vehicle for specific and efficient delivery of drugs.\nSophie A Howard, Alain Filloux, Imperial College London, United Kingdom Bacterial Protein Secretion: Looking inside an injection system\n*Ericson CF, *Eisenstein F, Medeiros JM, Malter KE, Calvalcanti G, Zeller RW, Newman DK, Pilhofer M, Shikuma NJ A contractile injection system stimulates tubeworm metamorphosis by translocating a proteinaceous effector\n*Shikuma NJ, *Antoshechkin I, Medeiros JM, Pilhofer M, Newman DK Stepwise Metamorphosis of the Tubeworm Hydroides elegans is Mediated by a Bacterial Inducer and MAPK Signaling\n*Shikuma NJ,*Pilhofer M, Weiss GL, Hadfield MG, Jensen GJ, Newman DK Marine tubeworm metamorphosis induced by arrays of bacterial phage tail-like structures\nAm Projekt beteiligte Personen\nLetzte Aktualisierung dieser Projektdarstellung 10.02.2021","First nanotechnological approach enables the design and replication of complex single-stranded DNA and RNA origami with potential for drug delivery and nanofabrication\nNanotechnologists are using DNA, the genetic material present in living organisms, as well as its multifunctional cousin RNA, as the raw material in efforts to build miniscule devices that could potentially function as drug delivery vehicles, tiny nanofactories for the production of pharmaceuticals and chemicals, or highly sensitive elements of electric and optical technologies.\nLike genetic DNA (and RNA) in nature, these engineered nanotechnological devices are also made up of strands that are comprised of the four bases known in shorthand as A, C, T, and G. Regions within those strands can spontaneously fold and bind to each other via short complementary base sequences in which As from one sequence specifically bind to Ts from another sequence, and Cs to Gs. Researchers at the Wyss Institute of Biologically Inspired Engineering and elsewhere have used these features to design self-assembling nanostructures such as scaffolded DNA origami and DNA bricks with ever-growing sizes and complexities that are becoming useful for diverse applications. However, the translation of these structures into medical and industrial applications is still challenging, partially because these multi-stranded systems are prone to local defects due to missing stands. In addition, they self-assemble from hundreds to thousands of individual DNA sequences that each need to be verified and tested for high-precision applications, and whose expensive synthesis often produces undesired side products.\nNow, a novel approach published in Science by a collaborative team of researchers from the Wyss Institute, Arizona State University, and Autodesk for the first time enables the design of complex single-stranded DNA and RNA origami that can autonomously fold into diverse, stable, user-defined structures. In contrast to the synthesis of multi-stranded nanostructures, these entirely new types of origami are folded from one single strand, which can be replicated in living cells, allowing their potential low-cost production at large scales and with high purities, opening entirely new opportunities for diverse applications such as drug delivery and nanofabrication.\nEarlier generations of larger-sized origami are composed of a central scaffold strand whose folding and stability requires more than two hundred short staple strands that bridge distant parts of the scaffold and fix them in space. \"In contrast to traditional scaffolded origamis, which are assembled from hundreds of components, our new approach allows us to reliably design and synthesize stable single-stranded and self-folding origami,\" said Wyss Institute Core Faculty member and corresponding author Peng Yin, Ph.D. \"Our fundamentally new approach relies on single-strand folding, rather than multi-component assembly, to produce large nanostructures. This, together with the ability to basically clone and multiply the single component strand in bacteria, presents a game-changing advance in DNA nanotechnology that greatly enhances single-stranded origami's potential for real-world applications.\" Yin is also co-lead of the Wyss Institute's Molecular Robotics Initiative and Professor of Systems Biology at Harvard Medical School (HMS).\nTo first enable the production of single-stranded and stable DNA-based origami with distinct folding patterns, the team had to overcome several challenges. In a large DNA strand that goes through a complex folding process, many sequences need to accurately pair up with sequences that are far away from each other. If this process does not happen in an orderly and precise fashion, the strand gets tangled and forms unspecific knots along the way, rendering it useless. \"To avoid this problem, we identified new design rules that we can use to cross DNA strands between different double-stranded regions and developed a web-based automated design tool that allows researchers to integrate many of these events into a folding path leading up to a large knot-free nanocomplex,\" said Dongran Han, Ph.D., the study's first author and a Postdoctoral Fellow on Yin's team.\nThe largest DNA origami structures created previously were assembled by synthesizing all their constituent sequences individually in vitro and by mixing them together. As a key feature of the new design process, the single-strandedness of the DNA origami allowed the researchers to introduce DNA sequences stably into E. coli bacteria to inexpensively and accurately replicate them with every cell division. \"This could greatly facilitate the development of single-stranded origami for high-precision nanotech like drug delivery vehicles, for example, as only a single easy-to-produce molecule needs to be validated and approved,\" said Han.\nFinally, the team also adapted single-stranded origami technology to RNA, which as a different nucleic acid material offers certain advantages including, for example, even higher production levels in bacteria, and usefulness for potential intra-cellular and therapeutic RNA applications. Translating the approach to RNA also scales up the size and complexity of synthetic RNA structures 10-fold compared to previous structures made from RNA.\nTheir proof-of-concept analysis also proved that protruding DNA loops can be precisely positioned and be used as handles for the attachment of functional proteins. In future developments, single-stranded origami could thus be potentially functionalized by attaching enzymes, fluorescent probes, metal particles, or drugs either to their surfaces or within cavities inside. This could effectively convert single-stranded origami into nanofactories, light-sensing and emitting optical devices, or drug delivery vehicles.\n\"This new advance by the Wyss Institute's Molecular Robotics Initiative transforms an exciting laboratory research methodology into a potentially transformative technology that can be manufactured at large scale by leveraging the biological machinery of living cells. This work opens a path by which DNA nanotechnology and origami approaches may be translated into products that meet real-world challenges,\" said Wyss Institute Founding Director Donald Ingber, M.D., Ph.D., who is also the Judah Folkman Professor of Vascular Biology at HMS and the Vascular Biology Program at Boston Children's Hospital, as well as Professor of Bioengineering at Harvard's John A. Paulson School of Engineering and Applied Sciences (SEAS).\n\"The results announced today establish DNA nanotechnology as a viable alternative approach for applications that have the potential to benefit all of us and the Nation as a whole,\" said Jim Kurose, Assistant Director of the National Science Foundation's (NSF) Directorate for Computer and Information Science and Engineering (CISE). \"We are delighted this work was supported by NSF's Expeditions in Computing program, which has, over the last decade funded large teams of researchers to pursue ambitious, fundamental research agendas that help define and shape the future of computer and information science and engineering, and impact our national competitiveness.\"\nBesides Yin and Han, the study includes corresponding authors Hao Yan, Ph.D., and Fei Zhang, Ph.D., Director and Assistant Professor at the Biodesign Center for Molecular Design and Biomimetics at Arizona State University, Tempe, respectively, and Byoungkwon An, Ph.D., Principle Research Scientist at Autodesk Research, San Francisco; Shuoxing Jiang, Ph.D., Xiaodong Qi, and Yan Liu, Ph.D., Assistant Professor from the Biodesign Institute; Cameron Myhrvold, Ph.D., Bei Wang, and Mingjie Dai, Ph.D., past and present members of Yin's team at the Wyss Institute; and Maxwell Bates, who worked with An. The study was funded by the Office of Naval Research, the Army Research Office, the National Science Foundation's Expeditions in Computing program, and the Wyss Institute for Biologically Inspired Engineering.\nWyss Institute for Biologically Inspired Engineering at Harvard University\nBenjamin Boettner, Benjamin.Boettner@wyss.harvard.edu, 1-617-432-8323\nWyss Institute for Biologically Inspired Engineering at Harvard University\nSeth Kroll, firstname.lastname@example.org, 1-617-432-7758\nThe Wyss Institute for Biologically Inspired Engineering at Harvard University uses Nature's design principles to develop bioinspired materials and devices that will transform medicine and create a more sustainable world. Wyss researchers are developing innovative new engineering solutions for healthcare, energy, architecture, robotics, and manufacturing that are translated into commercial products and therapies through collaborations with clinical investigators, corporate alliances, and formation of new startups. The Wyss Institute creates transformative technological breakthroughs by engaging in high risk research, and crosses disciplinary and institutional barriers, working as an alliance that includes Harvard's Schools of Medicine, Engineering, Arts & Sciences and Design, and in partnership with Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Boston Children's Hospital, Dana-Farber Cancer Institute, Massachusetts General Hospital, the University of Massachusetts Medical School, Spaulding Rehabilitation Hospital, Boston University, Tufts University, Charité - Universitätsmedizin Berlin, University of Zurich and Massachusetts Institute of Technology.\nHarvard Medical School has more than 11,000 faculty working in 10 academic departments located at the School's Boston campus or in hospital-based clinical departments at 15 Harvard-affiliated teaching hospitals and research institutes: Beth Israel Deaconess Medical Center, Boston Children's Hospital, Brigham and Women's Hospital, Cambridge Health Alliance, Dana-Farber Cancer Institute, Harvard Pilgrim Health Care Institute, Hebrew SeniorLife, Joslin Diabetes Center, Judge Baker Children's Center, Massachusetts Eye and Ear/Schepens Eye Research Institute, Massachusetts General Hospital, McLean Hospital, Mount Auburn Hospital, Spaulding Rehabilitation Network and VA Boston Healthcare System.\nBenjamin Boettner | EurekAlert!\nSwitch-in-a-cell electrifies life\n18.12.2018 | Rice University\nPlant biologists identify mechanism behind transition from insect to wind pollination\n18.12.2018 | University of Toronto\nResearchers from the University of Basel have reported a new method that allows the physical state of just a few atoms or molecules within a network to be controlled. It is based on the spontaneous self-organization of molecules into extensive networks with pores about one nanometer in size. In the journal ‘small’, the physicists reported on their investigations, which could be of particular importance for the development of new storage devices.\nAround the world, researchers are attempting to shrink data storage devices to achieve as large a storage capacity in as small a space as possible. In almost...\nThe more objects we make \"smart,\" from watches to entire buildings, the greater the need for these devices to store and retrieve massive amounts of data quickly without consuming too much power.\nMillions of new memory cells could be part of a computer chip and provide that speed and energy savings, thanks to the discovery of a previously unobserved...\nWhat if, instead of turning up the thermostat, you could warm up with high-tech, flexible patches sewn into your clothes - while significantly reducing your...\nA widely used diabetes medication combined with an antihypertensive drug specifically inhibits tumor growth – this was discovered by researchers from the University of Basel’s Biozentrum two years ago. In a follow-up study, recently published in “Cell Reports”, the scientists report that this drug cocktail induces cancer cell death by switching off their energy supply.\nThe widely used anti-diabetes drug metformin not only reduces blood sugar but also has an anti-cancer effect. However, the metformin dose commonly used in the...\nA research team from the University of Zurich has developed a new drone that can retract its propeller arms in flight and make itself small to fit through narrow gaps and holes. This is particularly useful when searching for victims of natural disasters.\nInspecting a damaged building after an earthquake or during a fire is exactly the kind of job that human rescuers would like drones to do for them. A flying...\n12.12.2018 | Event News\n10.12.2018 | Event News\n06.12.2018 | Event News\n18.12.2018 | Materials Sciences\n18.12.2018 | Physics and Astronomy\n18.12.2018 | Physics and Astronomy"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3ea4db52-90dc-4073-be20-a8c5f7f3786a>","<urn:uuid:502aed39-75be-48b4-a8b2-a78156158cba>"],"error":null}
{"question":"What strategic advantages did Britain have sobre la Luftwaffe during the Battle of Britain, and how did the RAF aircraft perform compared to German planes?","answer":"Britain had two key strategic advantages: First, they had invented and perfected radar technology, which allowed them to precisely track incoming Luftwaffe raids, including their strength and altitude. Second, while the German Me-109 fighter was fast and agile, the RAF's Spitfire proved to be a superior weapon. The Spitfire was well-matched against the Bf 109E in speed and agility, and its effectiveness was so renowned that when Luftwaffe Lt. Gen. Adolf Galland was asked what he needed to defeat the RAF, he famously replied that he wanted a squadron of Spitfires - much to Hermann Goering's displeasure. The Hurricane, while slightly larger and slower than the Spitfire, was also highly effective, particularly against bombers.","context":["The Battle For Free World Survival\nBy Tom Morrow\nArguably, the “Battle of Britain,” which took place in the summer and fall of 1940, saved the free world from Nazi tyranny and possible world domination – at least it slowed down Germany’s Adolf Hitler.\n“The Battle of Britain” didn’t win the six-year long World War II, but it brought Hitler’s menacing march across Europe to a standstill. Hitler didn’t conquer any more countries, thanks to the stubborn British people. That encounter should be more aptly named: “The Battle For Freedom.”\nAfter Hitler’s lighting success in capturing most of Western Europe using the German army’s “Blitzkreig” (lighting warfare) tactics, only Great Britain remained to be harnessed in the Nazi yolk of dictatorship. All that stood in the way of Nazi domination of England, Scotland, and Wales was the English Channel and the world’s most powerful flotilla: Britain’s Royal Navy and a beleaguered group of young pilots of the Royal Air Force.\nAt that time, German Luftwaffe (air force) was the most powerful in the world with more than 2,500 fighters and bombers; Britain’s RAF numbered less than 800 fighters and bombers. Probably around 600 usable aircraft was a more-realistic accounting. Hitler decided that bombing British airfields and demolishing the RAF would be a prelude to a Channel crossing and land invasion from the French coast some 25 miles away. Such a military maneuver hadn’t been tried in centuries.\nLuftwaffe leader and Hitler’s No. 2, Reichsmarshall Herrman Goring, underestimated the tenacity of the British people’s will to resist as well as the bravado of RAF pilots. In order to equalize the numbers, British pilots had to shoot down invading German planes on a four-to-one ratio.\nHitler was confidant his bombers could overwhelm the British with brute force, knocking out coastal defenses and shipping, eventually giving the Germans air control over the whole of southern England.\nWhen the initial mission failed to destroy the RAF, Hitler launched a night-time bombing campaign, or as the British called it, the “Blitz,” of London. So confident was Goering of his air force’s superiority, he bragged to Hitler that if any British bombs ever fell on Berlin, he mockingly told the Nazi leader he could call the rotund air commander, “Meyer,” (a Jewish name considered by the Nazis to be a supreme insult). When the first RAF bombs rained down on Berlin, no one knows what Hitler said to”Meyer” Goering, but it’s a good bet it wasn’t pleasant.\nHitler under estimated the stamina of the British people and the skill of RAF pilots and their aircraft. While the Me-109 fighter was a fast and agile aircraft, the RAF “Spitfire” was a superior weapon.\nThe Brits united under Prime Minister Winston Churchill’s leadership to defend their nation at all costs. They refused to give up, even as their cities were repeatedly bombed. The British had an advantage over the Germans: radar, which they invented and perfected. The Brits knew exactly when the Luftwaffe was coming, in what strength and at what altitude. RAF fighters, “Spitfires” and “Hurricanes,” kept German air raids at bay with a great cost to the RAF, but not as much as to the Nazis. British and German aircraft factories were turning out planes as fast as possible to keep up with the daily destruction. Still, when the Battle of Britain ended, the British had lost some 900 aircraft to the Germans’ 2,300 planes. The RAF had nearly equaled their needed “four-to-one” combat ratio.\nThough the United States had yet to enter the war, President Roosevelt persuaded Congress to approve the “Lend-Lease” agreement, which sent ships, planes, guns, ammo, and desperately-needed food and medical supplies to the beleaguered British people. Historians generally agree that if the British nation had not stopped the Nazi aggression, an attack on North America would have been imminent. That five-month air battle in the summer of 1940, might very well have preserved our democratic way of life instead of living under a dictatorship.\nChurchill paid tribute to those legendary Battle of Britain airmen by saying: “Never so many owed so much to so few.”\nA WWII Footnote: During the air battle with the British, Hermann Goering asked his top flight leader and fighter ace what he needed to defeat the RAF? Luftwaffe Lt. Gen. Adolf Galland replied: “Sir, give me a squadron of Spitfires.” Goring was not amused.\nSCAG SEZ: “Ya know, they say what you don’t know can’t hurt you, but it can make you look pretty damned stupid.” – Cecil Scaglione, Mature Life Features.","The 53rd WRS hurricane hunters operate ten Lockheed WC-130J aircraft, which fly directly into hurricanes, typically penetrating the hurricane’s eye several times per mission at altitudes between 500 feet (150 m) and 10,000 feet (3,000 m).\nSimilarly, Was the Spitfire better than the hurricane?\nThe Spitfire and Bf 109E were well-matched in speed and agility, and both were somewhat faster than the Hurricane. The slightly larger Hurricane was regarded as an easier aircraft to fly and was effective against Luftwaffe bombers.\nAdditionally, How much do Hurricane Hunter pilots make? It depends on the individual’s rank and years of experience, and whether or not he or she is a full-time Hurricane Hunter (an Air Reserve Technician, or ART) or a regular reservist. ART salaries are anywhere from $30,000 to $70,000 per year, whereas a reservist will make between $8,000 and $15,000 in a typical year. 5.\n- 1 What planes are used to fly into hurricanes?\n- 2 What are the names of the hurricane hunter planes?\n- 3 Did the Hurricane shoot down more planes than the Spitfire?\n- 4 Did the Spitfire or Hurricane won the Battle of Britain?\n- 5 Was the Spitfire the best plane in WW2?\n- 6 What do you need to be a hurricane hunter?\n- 7 Why do hurricane hunters use prop planes?\n- 8 How do hurricane hunter planes fly into hurricanes?\n- 9 How do they fly into hurricanes?\n- 10 Why does NOAA use prop planes?\n- 11 How many planes does NOAA have?\n- 12 Who shot down the most planes in ww2?\n- 13 Who shot down the most planes in the Battle of Britain?\n- 14 How many planes did the Spitfire shoot down in ww2?\n- 15 Did the Hawker Hurricane win the Battle of Britain?\n- 16 Who won the Battle of Britain?\n- 17 How did the Battle of Britain end?\n- 18 What was considered the best WW2 fighter plane?\n- 19 What was the best plane in World war 2?\n- 20 What was the most successful plane in World war 2?\nWhat planes are used to fly into hurricanes?\nG-IV Jet: Above and Around the Storm\nNOAA’s Gulfstream IV-SP (G-IV) which can fly high, fast and far with a range of 4,000 nautical miles and a cruising altitude of 45,000 ft., paints a detailed picture of weather systems in the upper atmosphere surrounding developing hurricanes.\nWhat are the names of the hurricane hunter planes?\nNOAA uses two Lockheed WP-3D Orion turboprops to fly through hurricanes and a Gulfstream IV-SP which flies around the upper fringes of storms to get a read on steering currents.\nDid the Hurricane shoot down more planes than the Spitfire?\nThe Hurricanes made an outsized contribution to the battle itself. More than half of the nearly 1,200 German aircraft shot down were by Hurricanes, but its impact has tended to fade into the background compared to the more graceful Spitfire. “The Spitfire had mystique about it,” Beaver says.\nDid the Spitfire or Hurricane won the Battle of Britain?\nThose who have previously studied the Battle of Britain have most often come the conclusion that, although the Spitfire was an integral part to the Royal Air Force’s defense of Britain, it was ultimately the workhorse Hawker Hurricane that won the battle.\nWas the Spitfire the best plane in WW2?\nThe Spitfire is most likely the most famous aircraft from the World War II era. It was the king of low-altitudes, it will be known forever as the plane that turned the tide in the Battle of Britain. … In the Battle of Britain, the Spitfire gained fame by having the highest victory-to-loss ratio among British aircraft.\nWhat do you need to be a hurricane hunter?\nAerial Hurricane Hunter Career\nRequires Commercial Pilot certificate. Includes charter pilots with similar certification, and air ambulance and air tour pilots.\nWhy do hurricane hunters use prop planes?\nAnswer: The turboprops are more tolerant of hail than the jets. Airplanes that penetrate thunderstorms have an increased chance of encountering hail. Additionally, the two types of airplanes used, the P-3 and the C-130, are especially rugged.\nHow do hurricane hunter planes fly into hurricanes?\nHurricane hunters fly through intense lightning during Tropical Storm Nestor in October 2019. … Of course, there are other times when the aircraft is surrounded by lightning and it might drop a thousand feet in the blink of an eye because that’s just what the mass of air surrounding it felt like doing.\nHow do they fly into hurricanes?\nThe NOAA uses specialized aircraft and instruments to fly into hurricanes and study storms’ movements for better tracking. … The data collected by the flight crew and weather crew on the plane is the only way to precisely locate the eye of the storm. It’s called fixing the eye.\nWhy does NOAA use prop planes?\nToday, hurricane hunters fly specially equipped planes that NOAA describes as “high-flying meteorological stations.” The data the planes and crew gather “help forecasters make accurate predictions during a hurricane and help hurricane researchers achieve a better understanding of storm processes, improving their …\nHow many planes does NOAA have?\nNOAA’s fleet of 10 manned aircraft is operated, managed and maintained by NOAA’s Aircraft Operations Center (AOC), part of NOAA’s Office of Marine and Aviation Operations.\nWho shot down the most planes in ww2?\nWhile serving in Germany’s Luftwaffe in World War II, Erich Hartmann flew more than 1,400 missions in the Messerschmitt Bf 109, enabling him to score an astonishing 352 kills.\nWho shot down the most planes in the Battle of Britain?\nIn just 42 days 303 Squadron shot down 126 German planes, becoming the most successful Fighter Command unit in the Battle of Britain. Nine of the Squadron’s pilots qualified as ‘aces’ for shooting down 5 or more enemy planes, including Sergeant Josef Frantisek, a Czech flying with the Poles who scored 17 downed planes.\nHow many planes did the Spitfire shoot down in ww2?\nThe Spitfire entered service with No. 19 Squadron at Duxford in August 1938. Production was slow at first, but by September 1940 it was in service with 18 RAF squadrons. Spitfires shot down a total of 529 enemy aircraft, for a loss of 230 of their own.\nDid the Hawker Hurricane win the Battle of Britain?\nFor the RAF aircraft which actually won the Battle of Britain was an older, larger, slower but still deadly fighter, the Hawker Hurricane. … Despite tributes such as this, the Hurricane never received the credit it deserved.\nWho won the Battle of Britain?\nIn the event, the battle was won by the Royal Air Force (RAF) Fighter Command, whose victory not only blocked the possibility of invasion but also created the conditions for Great Britain’s survival, for the extension of the war, and for the eventual defeat of Nazi Germany.\nHow did the Battle of Britain end?\nBy the end of October 1940, Hitler called off his planned invasion of Britain and the Battle of Britain ended. Both sides suffered enormous loss of life and aircraft. Still, Britain weakened the Luftwaffe and prevented Germany from achieving air superiority. It was the first major defeat of the war for Hitler.\nWhat was considered the best WW2 fighter plane?\nThe Focke-Wulf FW-190 was widely believed to be the best fighter aircraft of World War II. As the war went on the FW-190 was manufactured in no fewer than 40 different models. The appearance of the new aircraft over France in 1941 was a rude surprise to the Allied air forces.\nWhat was the best plane in World war 2?\nThese Were The 10 Best Planes Of WW2\n- 1 De Havilland Mosquito – Ultimate Multi-Role Aircraft.\n- 2 North American P51 Mustang – Best Allied Fighter. …\n- 3 Avro Lancaster – Best Heavy Bomber. …\n- 4 Supermarine Spitfire – Best British Fighter. …\n- 5 Boeing B29 Superfortress – Best Long-Range Bomber. …\n- 6 Focke-Wulf FW-190 – Best Fighter. …\nWhat was the most successful plane in World war 2?\nThe P-51 is widely regarded as the finest all-around piston-engined fighter of World War II to have been produced in significant numbers. Approximately 1,500 Merlin-powered Mustangs were used by the RAF for daylight duties over Europe, and the plane was produced under license in Australia toward the end of the war."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:5a6ed4fd-daf2-4286-9884-7de95d3e1833>","<urn:uuid:2539820c-ca19-4174-a300-02b9d7766931>"],"error":null}
{"question":"What is the relationship between dreams and reality according to modern philosophy, and how does scientific research explain the biological function of dreaming?","answer":"According to philosophical perspectives, dreams and waking reality are considered to be on equal phenomenal footing - the phenomenal world itself is seen as a dream from which one must awaken. This involves understanding that truth in dreams cannot be separated from appearance, as the authenticity of truth can only be gauged from its artificiality. From a scientific standpoint, while sleep has a clear biological function and adaptive value, dreams are viewed as side effects or 'free-riders' that are evolutionarily irrelevant. Dreams occur when brainstem activity during sleep generates a mixture of memories, images, thoughts, emotions, and desires, which the cerebral cortex then shapes into a coherent narrative.","context":["A Dream Interpreted within a Dream\nDreams have attracted the curiosity of humankind for millennia. In A Dream Interpreted Within a Dream, Elliot Wolfson guides the reader through contemporary philosophical and scientific models to the archaic wisdom that the dream state and waking reality are on an equal phenomenal footing--that the phenomenal world is the dream from which one must awaken by waking to the dream that one is merely dreaming that one is awake. By interpreting the dream within the dream, one ascertains the wakeful character of the dream and the dreamful character of wakefulness. Assuming that the manner in which the act of dreaming is interpreted may illuminate the way the interpreter comprehends human nature more generally, Wolfson draws on psychoanalysis, phenomenology, and neuroscience to elucidate the phenomenon of dreaming in a vast array of biblical, rabbinic, philosophical, and kabbalistic texts.\nTo understand the dream, Wolfson writes, it is necessary to embrace the paradox of the fictional truth--a truth whose authenticity can be gauged only from the standpoint of its artificiality. The dream, on this score, may be considered the semblance of the simulacrum, wherein truth is not opposed to deception because the appearance of truthfulness cannot be determined independently of the truthfulness of appearance.\nAbout the Author\nElliot R. Wolfson is Abraham Lieberman Professor of Hebrew and Judaic Studies at New York University. He is the author of Venturing Beyond: Law and Morality in Kabbalistic Mysticism, Luminal Darkness: Imaginal Gleanings From Zoharic Literature, and other books.\n\"For his fiftieth birthday Elliot Wolfson has offered himself a dream-book, dazzling and rich, poised against finitude's charge. What an extraordinary gift for readers of poetry and philosophy, for the psychoanalyst companioned by theory! Dropping into intractable regions of thought, Wolfson explores with enthralling precision the edges of asymmetry, prophesy, the alternate logic of dream archeology and often blinding illumination that such a venture implies.\"\nAvital Ronell, author of Fighting Theory and The Test Drive\n\"Over the last two decades, Elliot R. Wolfson has produced a most remarkable body of scholarship on the Kabbalah. Here, in his new book on the mind-bending paradoxes of the dreamscape and its actualizations in the acts and arts of interpretation, he applies his thought to that mysterious space of the imagination where the dreamer dreams and is in the process dreamt. Understanding, like his rabbinic and kabbalistic sources, the dream as a self-creating text that 'always follows the mouth,' that is, that requires acts of interpretation to become real, Wolfson employs a whole range of interpretive tools to reveal what is concealed within the dreamscape: quantum physics and neuroscience, hermeneutical theory, psychoanalysis, and literary theory, the 'mythologic' or coincidence of opposites, even the philosophy and transcendence of time within Mind as we have it in Buddhist and Hindu idealism. It is in this way that the dream becomes in Wolfson's vision a kind of spontaneous poetry or, as the Jewish sages had it, one-sixtieth of prophecy itself.\"\nJeffrey J. Kripal, author of Authors of the Impossible: The Sacred and the Paranormal\n\"The book's characteristic Wolfsonian achievement is to open to the non-philosopher the significance of philosophical thought without reducing its complexity. The lucidity of the exposition and the clarity and brilliance of Wolfson's thinking is what makes this happen. Equally characteristic is to show how earlier traditions have been engaged in asking these questions. Although much of the text draws on Wolfson's main expertise in Jewish mystical thought, the analysis extends far beyond the confines of that discipline, engaging questions both ontological and epistemological. A book for everyone interested in the phenomenon of dream consciousness.\"\nDaniel Boyarin, author of Socrates and the Fat Rabbis","Dreaming Souls: Sleep, Dreams, and the Evolution of the Conscious Mind: Sleep, Dreams, and the Evolution of the Conscious Mind\nGraduate studies at Western\nOUP USA (2001)\n|Abstract||What, if anything do dreams tell us about ourselves? What is the relationship between types of sleep and types of dreams? Does dreaming serve any purpose? Or are dreams simply meaningless mental noise--'unmusical fingers wandering over the piano keys'? With expertise in philosophy, psychology, and neuroscience, Owen Flanagan is uniquely qualified to answer those questions. And in Dreaming Souls he provides both an accessible survey of the latest research on sleep and dreams and a compelling new theory about the nature and function of dreaming. Flanagan argues that while sleep has a clear biological function and adaptive value, dreams are merely side effects, 'free-riders', irrelevant from an evolutionary point of view. But dreams are hardly unimportant. Indeed, Flanagan argues that dreams are self-expressive, the result of our need to find or create meaning, even when we are sleeping. Rejecting Freud's theory of manifest and latent content--of repressed wishes appearing in disguised form--Flanagan shows how brainstem activity during sleep generates a jumbled profusion of memories, images, thoughts, emotions, and desires, which the cerebral cortex then attempts to shape into a more or less coherent story. Such dream narratives range from the relatively mundane worries of non-REM sleep tot he fantastic confabulations of deep REM that resemble pyschotic episodes in their strangeness. But, however bizarre these narratives may be, they can shed light on our mental life, our well being, and our sense of self. Written with clarity, lively wit, and remarkable insight, Dreaming Souls offers a fascinating new way of apprehending one of the oldest mysteries of mental life.|\n|Keywords||No keywords specified (fix it)|\n|Categories||categorize this paper)|\n|External links||This entry has no external links. Add one.|\n|Through your library||Configure|\nSimilar books and articles\nOwen J. Flanagan (2000). Dreaming Souls: Sleep, Dreams, and the Evolution of the Conscious Mind. Oxford University Press.\nPiotr Markiewicz (2004). Sny - Nasze Stany Wyzwolone (O. Flanagan, \\\"Dreaming Souls. Sleep, Dreams and the Evolution of the Conscious Mind\\\"). Humanistyka I Przyrodoznawstwo 10.\nAlexander A. Borbély & Lutz Wittmann (2000). Sleep, Not Rem Sleep, is the Royal Road to Dreams. Behavioral and Brain Sciences 23 (6):911-912.\nJ. Allan Hobson, Edward F. Pace-Schott & Robert Stickgold (2003). Dreaming and the Brain: Toward a Cognitive Neuroscience of Conscious States. In Edward F. Pace-Schott, Mark Solms, Mark Blagrove & Stevan Harnad (eds.), Sleep and Dreaming: Scientific Advances and Reconsiderations. Cambridge University Press.\nMark Solms (2000). Forebrain Mechanisms of Dreaming Are Activated From a Variety of Sources. Behavioral and Brain Sciences 23 (6):1035-1040.\nJ. Allan Hobson, Edward F. Pace-Schott & Robert Stickgold (2000). Dreaming and the Brain: Toward a Cognitive Neuroscience of Conscious States. Behavioral And Brain Sciences 23 (6):793-842; 904-1018; 1083-1121.\nRobert Haskell (1985). Dreaming, Cognition, and Physical Illness: Part I. [REVIEW] Journal of Medical Humanities and Bioethics 6 (1):46-56.\nMilton Kramer (2000). Dreaming has Content and Meaning Not Just Form. Behavioral and Brain Sciences 23 (6):959-961.\nThomas Metzinger & Jennifer Michelle Windt (2007). Dreams. In D. Barrett & P. McNamara (eds.), The New Science of Dreaming. Praeger Publishers.\nAntti Revonsuo (2000). The Reinterpretation of Dreams: An Evolutionary Hypothesis of the Function of Dreaming. Behavioral and Brain Sciences 23 (6):877-901.\nCorrado Cavallero (2000). Rem Sleep = Dreaming: The Never-Ending Story. Behavioral and Brain Sciences 23 (6):916-917.\nMichael Schredl (2000). Dream Research: Integration of Physiological and Psychological Models. Behavioral and Brain Sciences 23 (6):1001-1003.\nOwen Flanagan (2000). Dreaming is Not an Adaptation. Behavioral and Brain Sciences 23 (6):936-939.\nRainer Schonhammer (2005). 'Typical Dreams' Reflections of Arousal. Journal of Consciousness Studies 12 (s 4-5):18-37.\nRamon Greenberg (2005). Old Wine (Most of It) in New Bottles: Where Are Dreams and What is the Memory? Behavioral and Brain Sciences 28 (1):72-73.\nSorry, there are not enough data points to plot this chart.\nAdded to index2012-01-31\nRecent downloads (6 months)0\nHow can I increase my downloads?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e15780b6-4091-4ef8-bdd1-111f7f1f5201>","<urn:uuid:d469df1e-10c8-41f0-9ce1-7eca3b5f7f2f>"],"error":null}
{"question":"When is amniocentesis typically performed for collecting amniotic fluid stem cells?","answer":"Amniocentesis is routinely conducted between 16-18 weeks of pregnancy. The collected amniotic fluid is used for prenatal genetic testing and as a source of amniotic fluid stem cells.","context":["In recent years, great interest has been devoted to finding alternative sources for human being stem cells which can be easily isolated, ideally without raising honest objections. cells to day, and emphasize the need to investigate its part, particularly in the context of cell tumorigenicity. . They designated these cells as induced pluripotent stem cells or iPS cells. Although iPS cells have clinical potential like a source of cells for regenerative medicine similar to Sera cells, transplanting differentiated cells derived from iPS cells into individuals remains a grave concern, as the genomic integrity of the cells as well as the safety of the individual continues to be an presssing issue . A second issue may be the low performance and gradual kinetics of iPS cell era in vitro . To get over these limitations, research workers started to search for alternative resources of stem cells. This undertaking gave rise to analyze in neuro-scientific perinatal stem cells. Perinatal stem cells could be produced from postembryonic tissue, such as the tissue sourced at the proper period of delivery, but NVP-AEW541 also comprise the proper period period in the 16th week of gestation through the neonatal period [4,5]. These tissue are the amniotic liquid, the placenta, placental membranes (amnion, chorion and Wharton jelly) and umbilical cable [6,7,8,9,10]. At the proper period of delivery, these tissue are discarded as natural waste usually. As these tissue are discarded in any case, harvesting stem cells from these resources is a straightforward and noninvasive way for obtaining stem cells that might be employed for therapy. Curiosity about perinatal stem cells was initiated, when Kaviani and co-workers reported in 2001 about the usage of these cells for tissues engineering as well as for the operative fix of congenital anomalies in the perinatal period . Not only is it available conveniently, perinatal stem cells could be isolated, extended, and differentiated in vitro [12,13,14,15,16,17]. Hence, it is anticipated that these cells can serve as a novel source and an alternative to human Sera cells for research and therapy. The amnion encloses the amniotic cavity containing the amniotic fluid, a protective and nutrient-containing liquid for the developing fetus . It is mainly composed of water, electrolytes, chemical substances, nutrients, and cells shed from the growing embryo [19,20]. Among the heterogeneous population of amniotic fluid cells, a class of multipotent cells, the amniotic fluid stem (AFS) cells have been identified. These cells share characteristics of ES and adult stem cells . Most interestingly, and in contrast to ES cells, the AFS cells are not tumorigenic when injected into immune-compromised animals [14,22]. This property makes these cells particularly attractive for clinicians and researchers in the field of regenerative medicine. A comparison between the main features of ES and AFS cells NVP-AEW541 is shown in Table 1. Table 1 Comparison between human embryonic stem (ES) cells and human amniotic fluid stem (AFS) cells. and to induce a pluripotent state, and then differentiated into functional cardiomyocytes using inhibitors of glycogen synthase kinase 3 (GSK3) and Wnt . Cells from the first trimester PLA2G12A that have been selected for the surface antigen c-kit NVP-AEW541 can furthermore be fully reprogrammed to pluripotency without transfecting ectopic factors when they are cultured on matrigel in cell culture medium that has been supplemented with the histone deacetylase inhibitor, valproic acid . The lack of tumorigenesis after transplantation is an interesting feature of AFS cells, although no information is available regarding the underlying mechanisms. Important clues could be gathered by investigating in AFS cells the activities and functions of crucial cell cycle regulators, like the tumor suppressor gene p53. p53 is one of the most well-known and most intensively investigated tumor suppressor proteins. A lot of work has already been done on looking into the part of p53 in Sera cells and additional adult stem cells and it’s been founded that aside from its traditional tumor suppressor function, p53 is reported to be engaged in controlling Cell Prolif also.eration, self-renewal, and differentiation of stem cells . 2. Phenotypic Characterization of Amniotic Liquid Stem Cells Amniocentesis is conducted between 16C18 weeks of pregnancy routinely. The gathered amniotic liquid can be used for prenatal hereditary testing so that as a way to obtain AFS cells. Different protocols and techniques have already been utilized to isolate AFS cells [8,30,31]. These isolation protocols could be recognized into we) a one-step tradition protocol; in this process the principal amniocyte tradition is remaining undisturbed for seven [32,33]."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:17b3480d-5655-495e-9231-f3c6a7b2bcd8>"],"error":null}
{"question":"What benefits do both no-till gardening and conservation tillage share in terms of soil erosion control?","answer":"Both no-till gardening and conservation tillage effectively reduce soil erosion through similar mechanisms. In no-till gardening, soil erosion is reduced because the soil is never left bare, with previous crops left on the field as mulch to hold soil in place. Similarly, in conservation tillage, farmers leave crop residue (such as corn stalks) in the field instead of tilling the entire field after harvest, which serves as a protective 'mulch' for the next season's crop. Both methods prevent rainwater from washing away topsoil, which is crucial given that half of the planet's topsoil has been lost in the last 150 years due to erosion.","context":["At The FruitGuys Community Fund we look to fund ecologically sustainable projects. One of the projects that we’ve been proud to support is the transition to no-till agriculture. No-till gardening meets our criteria for good water management, good soil and air quality management, and climate change mitigation. One other great thing about no-till agriculture is that it’s a feasible project for gardeners to tackle at home as well.\nWhat is No-Till Gardening?\nNo-till gardening is pretty much exactly what it sounds like. Farmers and gardeners using this method don’t till or turn their soil over. Instead they use other methods like broad forking, mulching, and cover cropping to prepare their gardens for planting. They then plant through a layer of existing plant material so the soil is never bare.\nBenefits of No-Till\n- Improves soil health.\nAdvances in science have taught us that there’s so much more to a productive garden than soil fertility. Soil is filled with microbes, fungi, and insects that help plants grow. Unfortunately when we turn soil over and disturb its natural layers many of these are killed.\n- Reduces erosion.\nNo-till gardening reduces soil erosion because the soil isn’t ever bare. When soil is left bare it’s easy for rain water to wash it away. Rather than tilling in a previous crop, it’s left on the field as a mulch and to help hold soil in place.\n- Reduces use of fossil fuels.\nGoing no-till means that fossil fuels aren’t used turning over the soil each year which is one of the most fuel intensive operations on a farm.\n- Improves soil structure.\nNo-till agriculture improves soil structure in a couple of ways. One of the key parts of creating good soil structure is the addition of organic matter which lightens soil, creates air spaces in the soil, and increases its capability to hold moisture. These are key to productive, healthy plants. Soil compaction through the use of heavy tractors and equipment is also avoided.\n- Reduces soil moisture loss.\nIn the same way that no-till gardening reduces erosion, it also reduces moisture loss. When the soil isn’t left bare less moisture will evaporate.\n- It can reduce the need for weeding.\nIf done properly no-till gardening can actually reduce the need for weeding. Cover crops and mulch or dead plant material block many weeds from sprouting up.\n- Improves crop yields.\nNo-till gardening has been found to increase crop yields. Improved soil and less moisture loss can create more productive gardens.\n- Reduces the amount of CO2 released into the atmosphere.\nUsing cover crops and keeping soils intact increases their ability to hold onto CO2.\nWhile no-till has proven to be a great method in the longterm there can be a few initial drawbacks.\n- For those with heavy wet spring soils tilling is often helpful in drying soil out to get crops in on time.\nIn a no-till garden the cover crop, mulch, or crop residue on the field helps hold moisture. While this is great during the height of summer it may not be desirable in early spring. However in the longterm no-till gardening can vastly improve soil structure which will help with drainage issues.\n- Depending on your situation there can be a few costs.\nAs you won’t be tilling each spring you’ll want to ensure your garden isn’t completely overtaken by weeds through the fall and winter. This means you either want to mulch your garden or better yet plant a cover crop. While the cost of cover crop seed or mulch isn’t enormous, especially for smaller gardens, it is something to consider.\nHow You Can Start a No-Till Garden\nIf you already have a garden one of the easiest ways you can transition to a no-till garden is to divide it into beds and pathways. You don’t have to build raised beds just designate areas. Not walking where you plant will reduce soil compaction and the need for tilling. Then whenever a crop is finished plant a cover crop or mulch the bed. When you’re ready to plant again you can cut your cover crop and use it as mulch. If you need to plant seed you can just rake or hoe the immediate planting area using leftover mulch and crop residue around your new plants to prevent weeds and keep them moist.\nIf you feel you need to till because you have heavy or compacted soils consider trying out a broad fork. With a broad fork you’re not turning over the soil you’re just lifting and loosening it. They’re easy to use and more affordable than a tiller.\nFor those who don’t already have an existing garden creating a no-till garden can seem like a daunting task. While some may opt to use a tiller to initially start a garden there is an easy way to go no-till from the very start. You can create a “lasagna garden.” First lay down a layer of brown cardboard, then a thick layer or hay or straw, then a thick layer of compost or soil. It’s an instant no-till garden. As the hay and cardboard rots they’ll provide more nutrients for your plants.\nYou don’t have to be a farmer to make a difference like The FruitGuys Community Fund grantees. Even for the backyard gardener going no-till can be a great project to help you create a more sustainable garden.","When most people think about soil and farming the first image that comes to mind is that of a farmer plowing his or her field. That rural image has become emblematic of agricultural cultivation and our shared connection to the land, soil and food that sustains us, but the reality is actually quite problematic.\nTillage, or the act of mechanically turning over and breaking up soil to prepare it for planting, has been an important part of farming practices for thousands of years, for good reason, but with unintended consequences both for farmland and the environment at large.\nThis past week we celebrated World Soil Day – a global awareness day the United Nations declared to celebrate the importance of soil as a vital contributor to human well-being. It’s also an opportunity to reflect on how GMOs have helped to improve soil health and the environment.\nHistorically, tillage practices have been used as a means for weed control; to help break up severely compacted soil; and to mix harvest residue, organic matter and nutrients evenly into the soil to prepare it for planting. All of these tillage aspects can provide benefits to crop production. However, tillage is increasingly recognized as a cause of soil damage that leaves soil vulnerable to erosion and contributes to other environmental ills, such as increased pollution and sedimentation in streams and rivers and loss of land to desertification.\nHow Does Tillage Impact The Environment?\nLet’s take a closer look. John Graham, a soil health specialist for the Natural Resources Conservation Service (NRCS) in Kentucky, explains that “tillage destroys and/or depletes the soil’s aggregate stability, structure, pore space, water holding capacity, infiltration, permeability, gaseous exchange and nutrient storage ability.” In other words, destroying soil severely reduces water infiltration – causing the soil’s capacity for water to be depleted. Instead of water going into the soil, it runs off, taking topsoil, chemicals and fertilizers with it. This leads to more vulnerable and less productive farmland and an environmental chain reaction. According to the World Wildlife Foundation, half of the topsoil on the planet has been lost in the last 150 years. The effects of soil erosion, which go beyond the loss of fertile land, have contributed to declines in fish and other species and worsened flooding and desertification.\nConservation Tillage Improves Soil Health, The Environment\nFor these reasons, movements toward more sustainable agriculture, including conservation tillage, are critical if we hope to meet the food production needs of a growing global population, while also reducing our impact on the environment.\nSo, How Does Conservation Tillage Work?\nConservation tillage allows farmers to till the soil less often. Here is a breakdown of the benefits:\n- Less tilling = less erosion. Instead of tilling an entire field after harvest, farmers can leave the crop’s residue (like corn stalks) in the field, and then plant seeds directly into that residue during the next planting season. This serves as a “mulch” for the next season’s crop, also protecting the soil.\n- Less tilling = more water retention. Utilizing crop’s residue drastically increases water infiltration and therefore retention. This also results in reduced runoff into water ways.\n- Less tilling = fewer greenhouse gas emissions. Tillage is a very fuel intensive operation and releases carbon dioxide. Reducing tillage reduces the amount of fuel used and greenhouse gas emissions into the atmosphere.\nWhat It Means For Farmers & The Environment\nThe ecological benefits of conservation tillage are clear, but what does it actually mean for farmers and how, after thousands of years as a mainstay of agricultural production, have we become less reliant on tilling?\nThe answer may come as a surprise. Advances in biotechnology, particularly the development of GMO crops, have been instrumental in facilitating conservation tillage and resulting improvements in soil health. The development of herbicide-tolerant GMO crops was a breakthrough. By allowing farmers a highly effective and reliable means to fight weeds by chemical means, GMOs have reduced reliance on tillage for weed control and made it easier and less risky for farmers to adopt more sustainable production strategies.\nTake soybeans for example. According to the Conservation Technology Information Center (CTIC), before the introduction of glyphosate-tolerant soybeans, approximately 27% of the nation’s full-season soybeans were no-tilled. Today, the latest CTIC surveys show that 39% are no-tilled, “a phenomenon that closely tracked the adoption of herbicide-tolerant soybeans.”\nCTIC further states that “biotechnology has played a significant role in influencing the shift of millions of acres of U.S. cropland to conservation tillage systems, which in turn has reduced topsoil loss, energy consumption, pesticide use, labor, water pollution and air pollution.” Conservation tillage means fewer trips across the fields, saving farmers time and money by lowering fuel and machinery maintenance costs, and reduced soil compaction.\nResearch by PG Economics published in 2013 in the journal GM Crops Food, also showed that “GM traits have contributed to a significant reduction in the environmental impact associated with insecticide and herbicide use on the areas devoted to GM crops.” Further, a 2014 meta-analysis of the impacts of GM crops published by Wilhelm Klümper and Matin Qaim found that on average, GM technology adoption has reduced chemical pesticide applications by 37%, increased crop yields by 22%, and increased farmer profits by 68%, offering proof that with GMOs what’s good for the environment can also be good for farmer’s bottom lines.\nAt GMO Answers, we believe that maintaining healthy soil is of critical importance, not only to farmers, but to the global community at large. We are proud of the positive impact GMO crops have made and continue to make by enabling more sustainable agricultural production and promoting soil health in ways that benefit farmers, the land they work and the environment as a whole."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3be22b9a-3dcb-41d8-92e0-fd2389cb754c>","<urn:uuid:b9e01065-ed05-4369-8bc1-7ff49603fd7b>"],"error":null}
{"question":"Need to understand severity levels - what are the progression stages from minor to life-threatening consequences of pet dental disease?","answer":"Pet dental disease progresses from minor to life-threatening in the following stages: It starts with relatively minor problems like bad breath, then advances to loss of teeth and bone. As bacteria spreads beneath the gum line, it can cause gum recession, inflammation, and tooth decay. In severe cases, it can lead to fractured jawbones and nasal problems. Ultimately, it can become life-threatening when the infection spreads through blood vessels to vital organs, potentially causing serious conditions like kidney failure, heart disease, and liver infections.","context":["The Importance of Dental Care\nPoor dental health is one of the most common problems facing dogs and cats. It is estimated that as many as 85% of dogs and cats have some form of periodontal disease by four years of age. The problems associated with dental disease can range from relatively minor problems such as bad breath, to loss of teath and bone, and even to life-threatening conditions such as kidney failure and heart disease.\nWe understand that good dental care is essential to maintaining your pet's overall health. This can only be accomplished through a regimen of regular preventive care, such as brushing, dental chews, or a dental diet, along with regular cleanings. Just as we humans need annual dental cleaning, it is important that our pets' mouths be regularly examined for hidden problems. Remember, they can't tell us when something hurts!\nFrequently Asked Questions About Pet Dentistry\nWhat is periodontal disease?\nPeriodontal disease is a term used to describe inflammation or infection of the tissues surrounding the tooth. Accumulation of tartar and calculus on the teeth causes gum recession around the base of the tooth. Infection soon follows and the gums recede further, exposing sensitive unprotected tooth root surfaces and the bony tooth sockets. Left untreated, the infection spreads deep into the tooth socket, destroying the bone. Ultimately, the tooth loosens and falls out. Advanced periodontal disease can lead to other serious health problems, including infections of the throat, heart, kidneys, and liver.\nWhat is involved in a routine dental cleaning?\nA routine dental cleaning involves a thorough dental examination, along with a dental scaling and polishing to remove the tartar and invisible plaque from all of the tooth surfaces. We will perform pre-anesthetic blood tests to ensure that kidney and liver function are satisfactory for anesthesia. Sometimes antibiotic treatment is started before the periodontal therapy is performed. Your pet's doctor will discuss the specific pre-dental recommendations for your pet.\nOnce your pet is anesthetized, tooth scaling will be performed using both traditional hand scalers and ultrasonic cleaning equipment to remove all traces of tartar, both above and below the gum line. The tartar below the gum line causes the most significant gum recession and it is extremely important that it is removed thoroughly. After scaling, full mouth x-rays are taken, which will help the doctor in determining the extent of any problems below the gum line. Next, the doctor will thoroughly examine the mouth for abnormalities such as periodontal pockets, fractures, or resorptive lesions. The teeth are then polished to remove microscopic scratches in order to help prevent subsequent plaque build-up. Special applications such as fluoride, antibiotic preparations and cleaning compounds may be indicated to decrease tooth sensitivity, strengthen enamel, treat bacterial infection and reduce future plaque accumulation.\nThe procedures that your pet may require will be discussed with you before scheduling the dental cleaning. Since it can be difficult to predict the extent of dental disease in advance of the procedure, it is imperative that we be able to reach you during the procedure to discuss any additional treatment that may be necessary.\nWhat if extractions are necessary?\nIf the doctor determines that one or more teeth need to be extracted, we will perform this after the cleaning. Additional pain medications will be given to your pet prior to the extractions, and the resulting pockets will be filled and sutured. Extractions are not something anyone looks forward to, but when indicated they can greatly improve your pet's quality of life.\nWhat about anesthetic safety and pain?\nPain management and patient safety are top priorities with us. Our dental procedures are performed under the same\nanesthetic and pain management protocols that we use for surgery. These are\ndesigned and proven to increase safety, improve recovery time, and minimize pain during and after the\nCan we do the dental procedure without the use of anesthesia?\nThe practice of veterinary dentistry without the use of anesthetics is not endorsed by the vast majority of veterinary practitioners, for a number of reasons. Most important, it is not possible to fully examine your pet's teeth without sedation, since 70% of the tooth is below the gum line. This is where most problems occur. It is also impossible to fully clean the teeth with the patient conscious, since they will not tolerate the use of instruments on the gums. Finally, non-anesthetic dentistry can be dangerous for your pet and our staff, since the patient has to be physically restrained during the procedure.","Periodontal disease is one of the most prevalent afflictions in human and veterinary medicine. Most pets have developed some degree of periodontal disease by the time they are three years old. Periodontal disease, or severe decay of the gums and teeth, is found in 80% of dogs over the age of 3, and in 70% of cats over 3. One reason for this is likely that less than 1% of pet owners regularly brush their pet’s teeth. If you happen to fall into that other 99%, we hope that the following article will offer some convincing motivation to begin brushing your pet’s teeth. If you have already tried brushing and found it difficult, skip to our next article on how to acclimate your pet to having her teeth cleaned.\nPeriodontal disease occurs in pets when bacteria is overabundant and spreads over an animal’s teeth and, in particular, underneath the gum line. As it multiplies, bacteria forms into a dangerous substance called plaque which sticks to the teeth and gums. When this plaque begins forming beneath the gum line, it erodes not only the gums, but the teeth themselves and the structures which keep teeth in place.\nWhen plaque is left untreated, teeth begin to decay and lose their structural integrity and the gum line can become inflamed and quite painful. It is not uncommon for patients with bad periodontal disease to experience bone decay in their jaw and in the part of the oral cavity which separates the mouth from the nose. In severe cases, this can lead to fractured jawbones and painful nasal problems. Even worse, the blood vessels connecting to the teeth also lead directly to the heart, kidney, and liver and can cause these organs to become infected themselves.\nPerhaps the most important facet of periodontal disease to understand is that the plaque underneath the gum line of an animal isn’t visible to the naked eye, and it is this plaque which can cause the most harm. So, even if your pet’s teeth look bright, shiny, and clean, they could very well still be experiencing dental disease.\nThe only reliable way to keep your pet safe from periodontal disease is a combination of regular brushing and regular veterinary dental cleanings. Every routine examination with a veterinarian should include a thorough examination of the teeth. However, as we’ve said, even a veterinarian can’t see beneath the gum line—the most important area to protect. For this reason, dental procedures which allow doctors to take X-rays are the only way to detect and diagnose problems. Full dental cleanings (known as COHATs—Comprehensive Oral Health Assessment and Treatment) require general anesthesia and are somewhat costly.\nIn order to minimize the need for frequent COHATs, daily brushing and good oral hygiene are crucial. The duration, complexity, and cost of these procedures can be greatly diminished by regular brushing. Without daily at-home brushing, periodontal disease can progress far enough in the year between annual dental cleanings to cause serious problems and necessitate tooth extractions. Each tooth that needs to be removed from a pet’s mouth significantly increases the time and cost of a dental procedure, so it is truly in everyone’s interest to practice assiduous brushing habits. Regular brushing will help remove bacteria and plaque from the teeth before it becomes tartar. Brushing also helps maintain a healthy and robust gum line that will be less prone to infection.\nHere’s some good news: teaching your pet to let you brush her teeth isn’t as difficult as it sounds. Given the right techniques, some patience, and good dental hygiene products, brushing your pet’s teeth can become an easy part of your daily routine. For a detailed walkthrough of good brushing technique, see our companion article on tooth brushing!\nMore and more locations are beginning to offer and advertise non-anesthetic dental cleanings. You can think of a good non-anesthetic cleaning as a tune-up for your pet in between full anesthetic cleanings. The key word here is good. When done well, non-anesthetic cleanings involve highly trained technicians working under the supervision of a veterinarian on dogs who have been pre-screened and deemed viable non-anesthetic candidates. These cleanings are more thorough than an at-home brushing and can help ensure that a pet’s teeth remain healthy between full dental cleanings. One of the chief criteria for being a suitable non-anesthetic patient is having had a full anesthetized dental procedure within the last year. Anesthesia is the only way to get an accurate picture of what is underneath a pet’s gum line, and you can seriously jeopardize your pet’s health if you believe that non-anesthetic cleanings are as good as full dental cleanings. Non-anesthetic dentals should never be used as a replacement for a fully anesthetized cleaning and should always be done under direct veterinary supervision. Procedures offered in pet stores and groomers without a veterinarian present are often ineffective and may even be dangerous for your pet.\nWe hope this article has made you think more seriously about your pet’s oral hygiene. Please feel free to contact us with any comments or feedback you have, and please see our companion article on how to brush your pet’s teeth!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:5aabd5b9-7e38-4153-a58e-ffdc4960f882>","<urn:uuid:b905ce41-9369-483f-977f-73130e3d1c44>"],"error":null}
{"question":"What winter sports facilities and activities are available at Bjelašnica Mountain near Sarajevo and Whiteface Lodge in Lake Placid?","answer":"Bjelašnica Mountain, located southwest of Sarajevo at 2,067 metres (6,782 feet) high, features a ski area with several runs and a large area above the tree line ideal for bald peak skiing. It offers accommodation for overnight stays and serves as a popular hiking destination in summer. Meanwhile, Whiteface Lodge in Lake Placid, designed by former Olympic luger Joe Barile, provides guests with access to various Olympic-themed activities including ice-skating, skiing, and bobsledding. The lodge also offers complementary amenities such as hot tubs, saunas, and steam rooms. Additionally, Lake Placid maintains Olympic facilities where athletes actively train in luge, skeleton, and bobsled at the Olympic Sports Complex.","context":["© All Rights Reserved AAY\nSarajevo is the largest city and capital of Bosnia and Herzegovina. It has a historic reputation as a city of religious diversity, with Islam, Catholicism and Orthodoxy all well represented throughout the centuries. Although the area of Sarajevo has been inhabited since prehistoric times, it wasn't until it was conquered by the Ottomans that it gained prominence. It was under Ottoman rule that the Old Town (Stari Grad) was constructed.\nThe city was burned to the ground by the Habsburg Monarchy in 1697 and never fully recovered. It remained an important administrative center for the Ottoman Empire until the late 19th century when the Austria-Hungarian Empire conquered Bosnia and Herzegovina.\nOn June 28, 1914, Archduke Franz Ferdinand of Austria and his wife Sophie, Duchess of Hohenberg, were assassinated in Sarajevo - an event that triggered World War I. Following the war, the Balkans were unified under the Kingdom of Yugoslavia. In 1984, at the peak of a boom period for the city, Sarajevo hosted the Winter Olympics.\nAs Yugoslavia was disintegrating in 1992, Sarejevo was surrounded by the Yugoslav National Army and several paramilitary formations. The city remained under siege until October 1995, with drastic consequences for the population and considerable destruction. After the Dayton Agreement was signed in 1995, the city underwent a great deal of reconstruction, with very little evidence of the war remaining.\nSarajevo is now a cosmopolitan city that delights its visitors with a friendly attitude.\nThe city of Sarajevo consists of four municipalities (Općina):\nBjelašnica Mountain is in the central part of the country and is directly southwest of Sarajevo. During the summer this mountain is a popular place to go hiking while in the winter it is a popular downhill skiing mountain. The Bjelašnica Ski Area has several nice runs and a large area above the tree line, which is ideal for fans of bald peak skiing. The mountain is 2,067 metres (6,782 feet) high and is a day trip from Sarajevo. There is a accommodation on the mountain for people thinking of spending the night.\nThis festival began in 1984 and has since grown to become one of the most anticipated events in Bosnia and Herzegovina. Often held in the month of February, art exhibits from different parts of the world compete for the highest title possible, the Sestoaprilska Negarda Sarajeva.\nOne of the most popular cultural events in Sarajevo, the month-long festival of Bascarsija Nights features some 40 to 50 different events that honor the rich culture of Bosnia. It is marked by different children’s programs, literary events, film showings, classical music, and even opera and ballet.\nThe Sarajevo Fim festival is held annually in August. Long and short regional films from a wide variety of genres are shown (some for free). It also serves as a huge venue for artists from all over the world to meet and share their passion for the arts.\nHeld in November, Jazzfest Sarajevo is the best place to enjoy Bosnia and Herzegovina’s rich music scene. The city’s café culture is at its liveliest.\nSarajevo has a continental climate with warm summers and cold winters. Temperatures in summer are well above 25 °C during the day, while winter nights can drop below -10 °C. Rain (and snow in winter) falls evenly throughout the year. June and September are good months, avoiding cold or hot weather and crowds as well.\nSarajevo International Airport (SJJ) is located a few kilometres to the southwest of the city. There are good connections within Europe offered by B&H Airlines to Amsterdam, Belgrade, Copenhagen, Frankfurt, Gothenburg, Istanbul, Stockholm, Vienna and Zürich, Austrian Airlines to Viena, Adria Airways to Ljubljana, Croatia Airlines to Zagreb, Germanwings to Cologne/Bonn, JAT Airways to Belgrade, Lufthansa to Munich, Malév Hungarian Airlines to Budapest, andTurkish Airlines to Istanbul. There are also seasonal flights to Antalya, Bodrum, Hurghada, Sharm el-Sheikh, Monastir, Jeddah, Medina (latter two both during Hajj).\nThere are no public transport links directly from the airport making taxis the most convenient option. Taxi fares are regulated and should adhere to the following rates: \nThe rate is increased by 30% during the night (10:00pm - 05:00am) and on Sundays and holidays.\nAn option to save some money is to catch a taxi to the nearby tram terminus at Ilidza and then catch a tram from there into the city.\nGRAS operates public transportation in Sarajevo. The centre of Sarajevo is served by a spinal tram network which makes a counter clockwise loop around the central district. This tram network opened in the mid-1870s and was the first in the Austro-Hungarian Empire. Tickets should be purchased in advance from kiosks labeled tisak on the street or from the driver, where they cost slightly more (around 1.80KM). Tickets should be validated upon boarding the vehicle and are valid for a one way trip only. Changing tram or bus means validating a new ticket. A day card valid for unlimited travel on all local public transport in Zone A is available for about 5KM. Please note that inspectors board public transport very frequently: if you can't reach the validator machine because the tram is too crowded, then don't board the tram.\nMost of central Sarajevo, including the old town area can easily be navigated on foot. It is about 15 minutes from the central train and bus stations to the old town.\nSarajevo has countless shops selling burek (meat pie, sold in layers by weight or by piece), ćevapi and pizza stores. Pita (burek, sirnica, krompirusa, tikvenica, zeljanica etc.) is a filo type pasty pie generally offered in several varieties - meat (meso), cheese (Bosnian cheese called \"young cheese\" similar to ricotta and never aged) (sirnica), cheese and spinach (zeljanica), pumpkin (tikvenica), and spicy potato (krompirusa). It is usually served with a traditional yogurt sauce which resembles sour cream. Most Cevapi places do not serve alcohol.\nSarajevo has vibrant night life with a plenty small thematic bars. Clubs are usually opened until early morning. Thursday, Friday and Saturday are hot days to hang out despite the rest of the week offers quite good night life.\n|Hotel Alem||Put mladih muslimana 16,||Hotel||-|\n|America Hotel||Himzarina 23, Visnjik, Sarajevo||Hotel||-|\n|Apartment Orient||Kujundziluk 6||apartment||-|\n|Apartmentselma||Ocaktanum 30 Sarajevo||apartment||-|\n|Hotel Astra Garni-Sarajevo||Kundurdziluk 2, Kundurdziluk 2,||Hotel||-|\n|Hotel Astra-Sarajevo||Zelenih beretki 9, Zelenih beretki 9,||Hotel||-|\n|Bed and Breakfast Divan||M. M. Baseskije 54||Guesthouse||-|\n|Belvedere Hotel - Sarajevo||Visnjik 2||Hotel||-|\n|Bosnia Hotel||Kulovica 9||hotel||-|\n|Camp site Hill||Hadzisabanovica 15||Campsite||-|\n|Casa Grande Hotel-Ilidza||Velika Aleja 2||Hotel||-|\n|Dardanija Hotel - Sarajevo||Radiceva broj 15||HOTEL||-|\n|Downtown||Ludviga Kube 11||HOSTEL||-|\n|Hostel Enjoy||Fra Ancela Zvizdovica 8||Hostel||89|\n|Grand Hotel - Sarajevo||Muhameda ef. Pandze 7||Hotel||-|\n|Haris Youth Hostel||Vratnik Mejdan 29||Hostel||89|\n|Hostel & Guest House Bistrik||Garaplina 14a 71000||HOSTEL||-|\n|Hostel & Guest House SA||HRVATIN 15||HOSTEL||89|\n|Hostel and Guest House Rose||Bakarevica 33 Budzak bb||Hostel||90|\n|Hostel City Center Sarajevo||Saliha Hadzihuseinovica MUVEKITA 2/3||Hostel||88|\n|Hostel Duka||Pehlivanua 43||Guesthouse||-|\n|Hostel Ferhadija||Ferhadija 21||HOSTEL||-|\n|Hostel Feri||Igman, Veliko Polje||hostel||-|\n|Hostel Hayat||Ocaktanum 5 Abdesthana 27||Hostel||83|\n|Hostel Ljubicica||Mula Mustafe Baseskije 65 Basèar||HOSTEL||81|\n|Hostel Magaza||Tahčića Sokak 7||HOSTEL||-|\n|Hostel Marko Polo||Logavina 6||Hostel||87|\n|Hostel Posillipo||Besarina èikma, No.5||Hostel||86|\n|Hostel Skend Sarajevo||Tekija 12||Hostel||-|\n|Hostel Mills Sarajevo||Mlini 1||Hostel||88|\n|Hotel Bosna-1||Butmirska cesta 8||hotel||-|\n|Hotel Ahar||Pijacna 82||hotel||-|\n|Hotel Delminium||Stup - Bare 16H||Hotel||-|\n|Hotel Garni Konak||Mula Mustafe Baseskije 54||hostel||-|\n|Hotel Hayat-Sarajevo||Abdesthana 27,||HOTEL||-|\n|Hotel Hecco||Medresa 1, Bosnia and Herzegovina Sarajevo||Hotel||-|\n|Hotel Hecco Deluxe-Sarajevo||Ferhadija 2||Hotel||-|\n|Hotel Hollywood||Mustafe Pintola 23||hotel||-|\n|Hotel Italia||Pofalici 7||hotel||-|\n|Hotel Old Town||Malio Curciluk 11||hotel||-|\n|Hotel Seraj||Safvet bega Basagica 12||Hotel||-|\n|Hotel Telal||Abdesthana 4||hotel||-|\n|Jezero Hotel - Sarajevo||Patriotske Lige bb||HOTEL||-|\n|Lion Hostel||Bravadziluk 30||Hostel||87|\n|Ljubičica Apartments||Mula Mustafe Baseskije 65||APARTMENT||-|\n|MD Apartments||Bistrik 84||Apartment||-|\n|Merak House||Besarina Cikma 5||Hostel||-|\n|Motel Latinski Most||Isa Bega Isakovica 1||Hotel||-|\n|Motel Liberty||Mula Mustafe Bašeskije 24||apartment||-|\n|Hotel Omega Ambasador-Sarajevo||Omera Stupca 19, Omera Stupca 19,||Hotel||-|\n|Ada Hotel - Sarajevo||Radenka Abazovica 2||Hotel||-|\n|Pansion Centar||Hadzisulejmanova 19 Sarajevo||Guesthouse||-|\n|Pansion Divan||Bravadziluk 38||GUESTHOUSE||-|\n|Pansion Harmony-Sarajevo||Vrbaska 26 G-Vraca||Guesthouse||-|\n|Pension Stary Grad||Bijelina Cikma 4||Guesthouse||-|\n|Hotel Park - Sarajevo||Vogosca||Hotel||-|\n|Plavi Zamak||Zvornicka 27||hostel||-|\n|Private Accomodation Gogo||Mehmed-pase Sokolovica 12/3||Apartment||-|\n|Residence Rooms||s.h.muvekita 1||Hostel||93|\n|Rimski Most Hotel-Ilidza||Blazujski drum bb (Ilidza)||HOTEL||-|\n|Roman Bridge Hotel||Blazujski drum bb||Hotel||-|\n|Hotel Saraj||Nevjestina 5||Hotel||-|\n|Sarajevo Rent||Josipa Stadlera 4||Apartment||-|\n|Star Hotel||Blazuj 44||hotel||-|\n|Sunce Hotel||Feriza Merzuka 76 Vogosca||Hotel||-|\n|Tower Hostel||Hadzisabanovica 15||hostel||91|\n|MIS Hostel||Nevjestina 7||Hostel||-|\n|Hostel Srce Sarajeva||S.H. Muvekita 2||HOSTEL||88|\n|Bascarsija Live||obala kulina bana 41 bjelave 115||APARTMENT||-|\n|Suljovic Hotel||Kurta Schorka 22||Hotel||-|\n|Banana City Hotel||Dzemala Bijedica bb||Hotel||-|\n|Apartments Sarajevo||Bistrik Medresa 15||Hostel||-|\n|Apartment Otoka||Zrtava Fasizma No.1||APARTMENT||-|\n|Hayat Apartments||Sarajevskih gazija 45||APARTMENT||-|\n|Hostel Alifakovac||Podcarina 1||Hostel||-|\n|Apartment Center||Kralja Tvrtka 15||APARTMENT||-|\n|Old Town Accommodation||Dzinina 9||GUESTHOUSE||91|\n|New Age Sarajevo||Muse Cazima Catica 18||HOSTEL||-|\n|Travellers Home Hostel||Cumurija 4||HOSTEL||94|\n|4U Hostel||Dzinina 12||HOSTEL||83|\n|Hostel Rooms Kesten||Sagrdzije 39||HOSTEL||85|\n|Guesthouse Bentbasa||Bentbasa 9||GUESTHOUSE||-|\n|Amra Apartment||Cemerlina 17||APARTMENT||-|\n|Apartment Bistrik||Garaplina 14a||APARTMENT||-|\n|Prenociste Sebilj||Bravadziluk bb||GUESTHOUSE||-|\n|Apartment Ado||Mali Curciluk||APARTMENT||-|\n|Guesthouse Zeyna||Toplik 8||GUESTHOUSE||-|\n|Hostel Kod Keme||Mali Curciluk 15||Hostel||-|\n|Apartmani Vila Selin||HASANA BIBERA 39||Apartment||-|\n|Hostel Kan||Brace Begic 35||Apartment||-|\n|Delminium Hotel - Sarajevo||Bojnicka 100||HOTEL||-|\n|Hotel Ada||Radenka Abazovica 2||Hotel||-|\n|Motel Mejdan - Sarajevo||Mustajpasin Mejdan 11||HOTEL||-|\n|Apartment Ida||Gradacacka 37||APARTMENT||-|\n|Apartment Malta||Zmaja od Bosne 63||APARTMENT||-|\n|Hostel Ami||Gorusa Br 2||HOSTEL||-|\n|Imzit - Ilidza||Cetvrte Viteske Brigade 1||HOTEL||-|\n|Imzit - Dobrnja||Lukavicka Cesta 121||HOTEL||-|\n|Apartmani||Sarajevskih gazija 12||APARTMENT||-|\n|Hostel Home Sweet Home||Saliha Hadzihuseinovica Muvekia 9||GUESTHOUSE||86|\n|Jugoslavija||Gajev trg 2||GUESTHOUSE||-|\n|Downtown Sarajevo Hostel||Marshal Tito 27||HOSTEL||-|\n|Hostel Ville||Safvet-bega Basagica 93||HOSTEL||-|\n|Sarajevo Old Town Apartments||Čeljigovići 48||Apartment||-|\n|Hotel Herc||Podcarina 1||HOTEL||-|\n|Vila Selin||Hasana Bibera 11||APARTMENT||-|\n|Emily Rooms & Apartments||Obala Kulina Bana 41||APARTMENT||-|\n|Sofa Rooms||Mlini 2a||APARTMENT||-|\n|Apartman Aladin||M.M.Baseskije br. 36||APARTMENT||-|\n|Hostel AE||Zelenih Beretki 22||HOSTEL||-|\n|Radon Plaza - Sarajevo||Dzemala Bijedica 185||HOTEL||-|\n|Apartments and Hostel Rooms Castanea||Kecina 21||APARTMENT||-|\n|Hostel CMöCity Magic||Hostel||-|\n|nocenje u sarajevu||marsala tita 54 sarajevo centar||Hostel||-|\n|Apartman AIDA||Travnička do br. 36||Hostel||-|\n|Markale Guest House||Guesthouse||89|\n|Markale||M. M. Baseskije 9||GUESTHOUSE||-|\n|Motel Boss||Safeta Zajke 183A||GUESTHOUSE||-|\n|Zemzem||Mula Mufe Baseskije 61||CAMPSITE||-|\n|Nocenje u Sarajevu||Marsala Tita 54||GUESTHOUSE||-|\n|Hostel Sarajevo Center||Cemalusa 2/2||HOSTEL||-|\n|Apartment Senija||Seada Sinanovica 5||APARTMENT||-|\n|Rooms in the heart of Sarajevo||Terezija 38||APARTMENT||-|\n|Hostel Jasmin Sarajevo||Kenana Demirovica 2||GUESTHOUSE||-|\n|Balkan Han Hostel||Dalmatinska 6||HOSTEL||91|\n|Hostel Balkan||Jelica Br. 2||HOSTEL||-|\nMost cities and major towns have at least one internet café. Wifi is becoming more and more popular as well, especially in cities like Sarajevo and Mostar. Don't rely on it though, as outside the main tourist areas, there might be few options.\nSee also: International Telephone Calls\nThe country calling code to Bosnia and Herzegovina is 387. To make an international call from Bosnia and Herzegovina, the code is 00.\nThere are three mobile phone networks in Bosnia and Herzegovina: HT ERONET (Mostar), GSMBiH (Sarajevo) and m:tel (Republika Srpska, Banja Luka). You can buy a prepaid SIM card from any network at any kiosk for 10 KM or less. Be careful with roaming charges on your smartphone, as mobile internet when abroad is still extremely expensive.\nBH Posta offers postal services in Bosnia and Herzegovina. Prices and services are very reasonable and reliable. Post offices are generally open from 8:00am to 4:00 or 5:00pm during weekdays and also on Saturday mornings. The main post offices in big cities like Sarajevo might keep longer hours. For sending packages you might consider a private courier like FedEx, TNT, UPS or DHL.\nAsk stojrma a question about Sarajevo\nOur name is Sarajevo Funky Tours and Visit Sarajevo. We are agency designed purely to promote Sarajevo to foreign and local visitors.\nUnder our own sources of quality materials about Sarajevo we can list:\nWe really think we can contribute very much to this article about Sarajevo on Travellerspoint.com because it really needs good update.\nSarajevo Funky Tours & Visit Sarajevo\nExcept where otherwise noted, content of this article is licensed under a Creative Commons Attribution-ShareAlike 3.0 License","Four Decades After a Miracle, Lake Placid Is Still America's Olympic Village\nThe Adirondack town has barely changed since the '80s — which is exactly why you should visit\nIn the nearly 100-year history of the Winter Olympiad, only three towns have hosted the Games multiple times.\nThe first? St. Moritz, Switzerland (1928 and 1946). The second? Innsbruck, Austria (1964 and 1976). And the third is Lake Placid, New York, which hosted the Games in 1932 and then again in 1980.\nThough it’s been 40 years since Lake Placid last brought the world’s winter athletes together in one place, the Adirondack town of fewer than 3,000 residents still carries the Olympic torch with pride (sometimes literally, like during an Opening Ceremony re-enactment that occurred on Valentine’s Day).\nAround town, evidence of Lake Placid’s link to the Winter Games is everywhere, from the selections that top the shelves in the bookstore to the centrally located Olympic Center that played host to “The Miracle on Ice” in 1980 and has welcomed hockey fans and players for tournaments ever since.\nAnd, of course, there are the Olympians themselves.\nLocals routinely cross paths with members of the Olympic team who live in Lake Placid or routinely travel to the area to train on the luge, skeleton and bobsled tracks at the Olympic Sports Complex. You might run into them while having cocktails at Top of the Park or getting a pint and a plate of comfort food at Big Slide Brewery. In the past, Olympians have also worked in town, some at Whiteface Lodge, a luxurious all-suite resort designed with timber and fieldstone that was conceived and built by former Olympic luger Joe Barile in 2005. In addition to giving all guests free access to hot tubs, saunas and steam rooms, Whiteface offers a variety of on- and off-site Olympic-themed activities including ice-skating, skiing and bobsledding.\n“A lot of the Olympians train here,” Jon Lundin, the director of communications for the Olympic Regional Development Authority (ORDA), tells InsideHook. “The slide-y sports and ski jumping all train here, figure skating to some degree, some of the hockey. And then we have the World Cup and the World Championships. We’re still a major international destination for competitors.”\nLundin, who notes local hotspot Lisa G’s is a favorite of many Olympians and athletes in training but also suggests elevated bistro Salt of the Earth, has lived in Lake Placid for most of his life and has seen how the town has changed since the ’80 Games. As it turns out, it sort of hasn’t.\n“The town really hasn’t changed that much,” Lundin says. “There were 2,600 year-round residents in 1980 during the Games and there are about 2,600 year-round residents now. Obviously, the cost of living has changed, the second-home purchasing has changed, but if you look at a picture of Main Street 40 years ago and compare it to a picture of Main Street today, it looks very similar. I think the biggest part of it is that there’s such an Olympic legacy here, that we’re still the destination for international sports to come and train and race, we feel as though it’s part of our responsibility to keep that legacy alive.”\nTo get a taste of that Olympic legacy, the most popular destination is the historic Olympic Center hockey rink on Main Street.\n“I work in the Olympic Center and every day I see somebody come in with their wife and their family,” Lundin says. “They talk about the ‘Miracle’ game and about where they were and who they were with and what they were wearing. That memory, even though it’s 40 years ago, is still at the top of people’s minds. It’s something that continues to resonate.”\nIn addition to the Olympic Center, another destination where people go to get a taste of the ’80 Games is the Olympic Jumping Complex, a venue where current athletes train that also lets visiting fans take a gondola up Whiteface Mountain followed by a glass elevator ride to the top of the property’s 120-meter ski jump tower.\nBetween the historic hockey rink, the jumping complex and the Olympians themselves, it’s hard not to feel a bit of the Olympic spirit in Lake Placid or see the pride the small Upstate New York community has in its ties to the Games.\n“If you’re an Olympic buff like I am, Lake Placid is to the Olympics as Cooperstown is to baseball,” Lundin says. “It’s a place to escape. Put away your cell phone, put away your iPad, put away the social media and enjoy the beauty that surrounds you. There is a simplicity to Lake Placid and to the Adirondacks that’s hard to describe unless you’re here. The Olympics and everything that we do is great, but take in the rest of it, take in the climbing, take in the snowshoeing or skiing and just enjoy this wilderness that we have. It’s truly something special and it really does rejuvenate you.”\nFor more travel news, tips and inspo, sign up for InsideHook's weekly travel newsletter, The Journey.\nSuggested for you"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:88d8d6cb-91e4-4c53-b7f0-b8e21b6830e6>","<urn:uuid:ea27a387-ee71-49cd-9838-fb58508f4d49>"],"error":null}
{"question":"Por favor, explique cómo la validación empírica de teorías económicas se manifiesta tanto en el estudio histórico del béisbol como en la integración actual de vehículos eléctricos a la red.","answer":"Economic theories have been empirically validated in both contexts. In baseball, Scully's 1974 monopsony theory predictions were dramatically confirmed when free agency led to player salaries increasing by a factor of four, aligning with his estimated 80% wage suppression. In the EV sector, theoretical predictions about VGI benefits are being validated through demonstrator projects, though these currently focus mainly on residential customers and fleets with private charging facilities. However, unlike baseball's comprehensive market test, VGI validation remains limited since business models for public charging stations still require more research and development.","context":["Gerald W. Scully: A Note of Appreciation\nAs Phil and others around the econ blogs have noted, Gerald Scully has passed away. He is known to us primarily as a pioneer in sports economics, but as John Goodman notes, he made significant contributions to a number of fields. His work in public economics is particularly insightful. Indeed, Google Scholar lists his 1988 JPE paper, “The Institutional Framework and Economic Development” at the top, with 406 citations, besting his 1974 AER classic, “Pay and Performance in Major League Baseball,” at 302. But it is the latter paper that is worth celebrating here. Although rudimentary in its method by today’s standard, it was an eye-opening analysis with a staggering conclusion: that major league baseball players of that era were paid a small fraction of their marginal revenue product, on the order of 1/5 or less.\nMy colleague Bill Dougan once told me that he regarded “Pay and Performance” as one of the best pieces of economic scholarship in the last quarter-century, something that I repeat to my students in sports economics classes to this day. Note that we are speaking of economic scholarship, and not just scholarship in the economics of sports. Scully’s 1974 paper is evidence that the study of economics in the context of sport can be important, and make a significant contribution to the discipline as a whole.\nWhat made “Pay and Performance” special, you ask? First, it was innovative. It was an early example of employing data from sport to test economic theory, in this case the theory of monopsony. The paper was also very clever. Scully observed that the reserve clause created conditions of monopsony in the baseball players’ labor market. Economic theory states that monopsony depresses wages below marginal revenue product, but if so, how much? Scully came up with a simple method to estimate the MRP of pitchers and hitters, and compared this estimate with their wages. Wages were well below Scully’s estimate of MRP.\nThe magnitude of the exploitation generated by the reserve clause — 80% or so — is a jaw-dropping figure. This result, combined with the imagination and execution of the analysis, warranted the article’s publication in the American Economic Review. But it is what happened next that fortuitously turned Scully’s paper into an example of the kind of scholarship that all economists should seek to emulate. Scully’s article was published in 1974, the year that Marvin Miller’s efforts began to shake the foundation upon which the reserve system was built. The development of free agency in baseball meant that Scully’s estimate would be put to the test, because a monopsony labor market would be replaced with competitive bidding for those players. As Rodney Fort subsequently showed, salaries soared for the “first family of free agents“, in many cases increasing by a factor of four or more, as Scully’s estimates implied.\nSports history had thus subjected Scully’s model to a stern test, which it passed with flying colors. It is not common for economic theory and evidence to produce an estimated effect that is so clear and so large as was Scully’s (for example, we are still arguing about the size of fiscal multipliers seventy-odd years after Keynes). It is even less common for such an estimate to be tested by events so promptly and directly, and in addition to have these events support the author’s work so convincingly.\n“Pay and Performance in Major League Baseball” will long be remembered for these reasons. So tonight I will raise a glass and toast the seminal contributions and insights of Gerald Scully. Here’s to you, Jerry.","Vehicle-grid integration (VGI) refers to the use of smart technologies to coordinate the charging operations of electric vehicles (EVs) to avoid negative impacts on the electric grid associated with additional electricity demand from EV charging. VGI can also help bring a smoother transition of the power sector toward net zero by leveraging EV batteries as distributed storage assets. Finally, VGI can enable cost savings to drivers and fleets. At the same time, there is a risk that VGI might exacerbate the divide between well-off early EV adopters and lower-income communities. Redressing this imbalance is particularly important because such communities have historically already shouldered a larger transportation cost burden and have been disproportionately affected by the negative externalities of transportation.\nIn this article, the authors explain why—despite government incentives—disadvantaged socioeconomic groups are excluded from VGI’s direct cost savings and other benefits. They go on to discuss the critical need for researchers and analysts to model the distributional impacts of VGI on different socioeconomic groups to help inform policies for advancing energy and transport justice.\nVGI technologies have been developed to overcome major grid impacts from EV deployment by exploiting the load flexibility and storage potential of EV batteries when EVs are parked. In particular, “smart charging” refers to systems that control EV charging in response to price signals and other information about the state of the grid. The term “vehicle-to-grid” (V2G) is used when both charger and EV have bidirectional capabilities and control systems, allowing them not only to import electricity from the grid but also to export (i.e., sell) it back. Often “V2G” is also used more generally to indicate any situation in which electricity is not only exported to the grid but used to partially cover electricity demand from specific local sources such as a home or a building.\nV2G and smart charging could offer whole systems benefits to power systems, e.g., mitigation of the reinforcement cost of power systems infrastructure, reduction of the cost and carbon footprint of keeping electricity demand and supply always in balance, and easier integration of renewable energy sources. At the same time, V2G and smart charging can provide multiple benefits to EV drivers and fleets. These include revenue generation, charging cost reductions, home energy cost reductions, reduction of personal carbon footprint, resilience during power outages, minimization of peak import capacity requirements at fleet depots, reduced battery degradation, and the symbolic value attached to contribution to societal benefits.\nThe International Energy Agency has described the rapid and continuous transition of road transportation toward EVs as a “bright spot of the clean energy transition.” This success will come down to policies that include purchase incentives (e.g., grants, tax credits, or rebates) and recurrent incentives (e.g., toll waivers and parking discounts), as well as fuel economy and pollutant standards, zero-emission vehicle mandates, taxation regimes, and bans on internal combustion engines.\nHowever, by and large, the transition kickstarted by these policies has not been an equitable one, as mostly high-income early adopters have benefitted from EV adoption incentives. A survey of California EV adopters between 2012 and 2017, for example, showed that 95 percent of EV adopters belong to high- or mid-high-income households. In Europe, a 2022 study on EV ownership in Ireland demonstrated a correlation between affluent areas and EV ownership. EVs symbolize luxury in geographical contexts as different as China and the Nordic regions in Europe. The uptake of EVs mostly among higher-income households shows that demand-side policies for EV adoption have benefited mostly the well off.\nPurchase incentives, often not accessible to or insufficient for lower-income families, are sometimes mirrored by stick approaches punishing the use of internal combustion (IC) cars (e.g., zero-emissions zones). As a result, the experiences of high- and low-income groups can be very different. Higher-income groups are able to tap both on purchase incentives and reduced operational costs of EVs compared to IC vehicles. Conversely, lower-income groups that are unable to afford expensive EVs are likely to be further penalized for using polluting vehicles when options to use other transportation modes are unavailable.\nEven when low-income consumers might be able to access EVs, there are no clear pathways for them to exploit their EVs as VGI assets, unlike homeowners with the ability to install chargers at their homes. Consider, for example, V2G-enabled energy arbitrage—charging an EV when the price of electricity is low and selling it back to the grid when the price is high. A recent UK study has shown that this could reduce by 28–67 percent the average price paid for charging by EV owners who own a bidirectional charger under wholesale-following time of day tariffs, compared to drivers subject to a flat tariff. However, such price reductions can only benefit those who not only own an EV but also have access to home charging—typically wealthier consumers with garages or driveways where they park their cars at home.\nHome charging station. Photo by Mario Roberto Durán Ortiz via Wikimedia Commons.\nIn principle, some cost reductions could be attained by EV drivers without access to home charging if shares of the cost reduction from smart charging at the workplace or public charging facilities are passed onto customers. However, most smart charging and V2G demonstrators’ projects have focused on residential customers and fleets with charging facilities at their premises. Business models for VGI at public charging stations or at the workplace require more research and development, as the benefit sharing between stakeholders is less straightforward.\nMoreover, as the direct economic benefits to drivers from VGI come from the energy arbitrage described above and are attained under time-dependent electricity prices, groups with more flexible activity-travel patterns, typically high-income earners, are likely to have more opportunities to choose when and where to charge. In contrast, socially disadvantaged groups, with less flexibility, may get locked into more costly charging arrangements.\nThe clean energy transition in road transport is in full swing thanks to government stimuli that have so far given greater benefits to wealthier drivers. It is critical to redress this initial imbalance as well as address the risk of VGI further exacerbating the inequality. Governments are adjusting their strategies, as exemplified by the US National Blueprint for Transportation Decarbonization, which puts equity and access among the guiding principles for the clean energy transition. Building out the evidence base could help translate these guiding principles into policy and practice, and may require addressing some specific questions, including:\nQuantitative approaches can help address such questions. While the energy justice literature has discussed the potential social implications of VGI, it has also singled out “social justice concerns” as a “notable gap” in VGI research. Current research, rooted in the transportation planning and energy planning disciplines, has focused on quantitative analyses of purely techno-economic aspects. In fact, the lack of agreed-upon quantitative metrics for equity impacts may hinder wider application of energy and transport justice analyses. On the other hand, development of quantitative metrics may be criticized for being reductionist and ignoring complex multidimensional interactions. A more effective integration between quantitative engineering approaches and qualitative approaches dominating the energy justice literature may help to devise quantitative metrics that address core energy and transport justice issues associated with transportation electrification and VGI.\n The VGI literature also refers to these systems as “vehicle-to-home” (V2H) and “vehicle-to-building” (V2B).\n Marko Aunedi and Goran Strbac, “Whole-System Benefits of Vehicle-To-Grid Services from Electric Vehicle Fleets,” Fifteenth International Conference on Ecological Vehicles and Renewable Energies, 2020, https://ieeexplore.ieee.org/document/9243032.\n Cenex, “A Fresh Look at V2G Value Propositions,” 2020, https://www.cenex.co.uk/app/uploads/2020/06/Fresh-Look-at-V2G-Value-Propositions.pdf.\n Scott Hardman et al., “Driving the Market for Plug-in Vehicles: Understanding Financial Purchase Incentives,” Plug-In Hybrid & Electric Vehicle Research Center, Institute of Transportation Studies, UC Davis, March 2018, https://ucdavis.app.box.com/v/purchase-incentives-guide.; Scott Hardman et al., “Driving the Market for Plug-in Vehicles: Understanding Reoccurring Incentives,” Plug-In Hybrid & Electric Vehicle Research Center, Institute of Transportation Studies, UC Davis, March 2018, https://ucdavis.app.box.com/v/reoccurring-incentives-guide; International Energy Agency, Global EV Outlook 2023, April 23, 2023, https://www.iea.org/reports/global-ev-outlook-2023.\n Jae Hyun Lee, Scott J. Hardman, and Gil Tal, “Who Is Buying Electric Vehicles in California? Characterising Early Adopter Heterogeneity and Forecasting Market Diffusion,” Energy Research & Social Science 55 (September 2019): 218–226, https://doi.org/10.1016/j.erss.2019.05.011.\n Brian Caulfield, Dylan Furszyfer, Agnieszka Stefaniec, and Aoife Foley, “Measuring the Equity Impacts of Government Subsidies for Electric Vehicles, Energy 248 (June 2022): 123588, https://doi.org/10.1016/j.energy.2022.123588.\n David Tyfield, Dennis Zuev, Ping Li, and John Urry, “Low Carbon Innovation in Chinese Urban Mobility: Prospects, Politics and Practices,” China Low Carbon Reports, November 2014, http://dx.doi.org/10.13140/RG.2.2.22865.48489; Lance Noel, Benjamin K. Sovacool, Johannes Kester, and Gerardo Zarazua de Rubens, “Conspicuous Diffusion: Theorizing how Status Drives Innovation in eCectric Mobility,” Environmental Innovation and Societal Transitions 31 (June 2019): 154–169, https://doi.org/10.1016/j.eist.2018.11.007.\n Tamara L. Sheldon, “Evaluating Electric Vehicle Policy Effectiveness and Equity,” Annual Review of Resource Economics 14 (October 2022), https://doi.org/10.1146/annurev-resource-111820-022834.\n James Dixon, Waqquas Bukhsh, Keith Bell, and Christian Brand, “Vehicle to Grid: Driver Plug-In Patterns, their Impact on the Cost and Carbon of Charging, and Implications for System Flexibility,” ETransportation 13 (August 2022): 100180, https://doi.org/10.1016/j.etran.2022.100180.\n Nathaniel Tucker and Mahnoosh Alizadeh, “Online Pricing Mechanisms for Electric Vehicle Management at Workplace Charging Facilities,” 2018 56th Annual Allerton Conference on Communication, Control, and Computing, 2018, 351–358, https://ieeexplore.ieee.org/document/8635950.\n Filipe Gonzalez Venegas, Marc Petit, and Yannick Perez, “Active Integration of Electric Vehicles into Distribution Grids: Barriers and Frameworks for Flexibility Services,” Renewable and Sustainable Energy Reviews 145 (July 2021): 111060, https://doi.org/10.1016/j.rser.2021.111060.\n US Department of Energy, “The US National Blueprint for Transportation Decarbonization,” https://www.energy.gov/sites/default/files/2023-01/the-us-national-blueprint-for-transportation-decarbonization.pdf.\n Benjamin Sovacool, Lance Noel, Jonn Axsen, and Willett Kempton, “The Neglected Social Dimensions to a Vehicle-to-Grid (V2G) Transition: A Critical and Systematic Review,” Environmental Research Letters 13 (January 2018): 013001, http://dx.doi.org/10.1088/1748-9326/aa9c6d."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:34e009ba-48ae-4a7f-80bc-ac2cd8d49c51>","<urn:uuid:dff66135-0945-47c2-967f-ee1f8f574983>"],"error":null}
{"question":"How does HVAC UV light technology prevent mold growth in air systems, and what are the key warning signs of hidden mold problems in older homes?","answer":"HVAC UV lights, specifically using UVC frequency at 253.7 nanometers, prevent mold by disrupting mold cell nuclei when installed inside furnaces or A/Cs. These lights shine 24/7 and are highly effective at preventing mold regrowth in HVAC systems, though visible mold should be cleaned first. As for warning signs of hidden mold in older homes, key indicators include musty odors, water stains on floors/walls, and mold can flourish in concealed areas like behind wallpaper, under drywall, on the back of paneling, and above ceiling tiles. Hidden mold typically develops in spaces where air and moisture become trapped between surfaces.","context":["HVAC UV Lights\nHow To Kill & Prevent Mold\nMold is a silent menace in homes.\nI would estimate that about 1-2 homes that I inspect have mold in their HVAC system — mainly the furnace. But I also frequently discover mold in bathrooms, basements, and other damp locations.\nMold cannot grow without the presence of water — either liquid water or high humidity (vapor).\nIn this guide, I will go over…\n- the best rated HVAC UV lights to kill and prevent mold\n- does UV light actually kill mold?\n- signs of black mold in air vents\n- pictures of mold in air ducts\n- getting rid of mold in your basement\nLet’s get started with this guide!\nWhat's In This Guide?\nUltraviolet light (specifically the UVC light frequency) has been in use for years in hospitals to sterilize operating rooms, equipment, and even food and water sterilization.\nHVAC UV light bulbs are installed inside the furnace or A/C and shines 24 hours a day disrupting the nuclei of mold cells.\nThe interior air handler is a central place for mold to grow because it has three things that mold loves: high moisture, darkness, and a food source (dust).\nEven though HVAC lights aren’t meant to clean mold from surfaces, they are a highly effective way to prevent mold from regrowing in the HVAC system (and spreading spores throughout the house).\nAlways clean your HVAC first if there is visible mold, then installing an HVAC UV light is highly recommended.\nHere is my detailed review on the best rated 5 HVAC UV lights to kill and prevent mold.\nDuring my home inspections, I have seen many signs of mold that indicated larger problems with the home.\nI would say that 75% of the time, when I see mold growing on air vents, I discover mold inside the furnace or air conditioner.\nMold (sometimes mildew) growing on air vents has a dark and organic appearance.\nWhen I am inspecting a home, it’s important to distinguish mold from just dust or debris.\nSometimes regular dust can accumulate, take on a dark appearance, and may be confused for mold. Either way, cleaning air vents (or replacing them) is a priority to maintain your indoor air quality. I also always recommend inspecting the interior of your A/C if you see signs of black mold in your air vents.\nOver my years as a home inspector, I have compiled many pictures of what mold looks like inside the air conditioner.\nWhenever I have indications that there may be a mold problem in a house, I always take off the HVAC furnace or A/C cover.\nIt is usually a simple process of turning a few latches, but sometimes it can be quite the ordeal.\nOnce the HVAC cover is off, I use a high powered flashlight to carefully inspect the blower fan, the evaporator coil, wiring, and the blower motor.\nI find that the blower fan frequently has mold growth that many people miss. One reason for this is that dust can accumulate on top of mold growth on the blower fan fins — so an HVAC contractor may think it’s just dust.\nSometimes it is brain-dead obvious that there is mold growth — a black organic substance growing on electrical wiring, insulation, and on the coils.\nIf you would like to see some high quality photographs of what mold looks like inside an HVAC system (not just vent covers), you can check out my article here.\nOne of the biggest fears that my clients have when buying a home is a moldy and leaky basement.\nI usually try to placate this fear by using my moisture meter to carefully check all of the basements exterior walls for signs of high moisture.\nSometimes, I even give my client the actual moisture meter so they can check the basement themselves (this tool is very easy to use).\nIf you have signs of mold in your basement, there are certain steps you want to take to clean and remove the mold. Of course, any extensive mold contamination (more than 10 square feet) should be handles by a qualified contractor with experience in mold remediation.\nBut in this guide, for small areas of mold growth, I will go over some of my tips to clean and remove mold. And also importantly — I will detail the most common causes of basement water intrusion that actually fuels the mold growth.\nSome of the steps are very simple, but quite significant; such as installing downspout extensions and improving the exterior landscaping.\nUltraviolet light has been in use for decades by hospitals and labs to sterilize biology and medical equipment from harmful microbial growth.\nThe actual light frequency is known as UVC and the wavelength is 253.7 nanometers.\nOver the years, UV light has also been used to sterilize water and to kill harmful microbes at waste water treatment facilities.\nOne common recommendation to my clients is to install an HVAC UV light inside their air conditioner when I see signs of mold. I always tell them to have the furnace (and air ducts) cleaned by a qualified contractor first, and then install a UV light.\nIf there is visible mold, it’s important to clean it first, because even though ultraviolet light will kill the mold, the dead mold will leave behind a dusty residue that can be an allergic irritant and cause health problems.\nMold is potentially a serious health and property problem.\nEven though everybody responds differently to mold (some people are hardly affected), some individuals can develop serious health effects. And beyond health problems, mold if left unchecked, can literally eat away at structural wood framing and other building materials.\nI once inspected a home (mainly for insurance purposes) where the entire basement was covered in mold. The owner went on vacation abroad for two months during the heat of summer, and he turned off the air conditioner.\nAir conditioning actually has a dehumidifying effect so you should never fully turn it off during the warm season. And on top of that, the water heater sprung a small leak releasing a lot of moisture into the air.\nMold was growing on the walls, in the flooring, everywhere. In this case, the whole basement had to be demolished. Usually, mold doesn’t get that bad.\nMy most common recommendation is to clean up the mold, reduce all the sources of excess moisture, and then to install an HVAC UV light to help prevent it from regrowing.","Mold and Moisture Problems in Old Homes\n“Mold and Moisture Problems in Old Homes\"\nis part of our series on\nAs you inspect the basement of your prospective home, one of the most important issues to keep in mind is the level of moisture. Moisture problems in old homes can be particularly difficult to fix, especially in homes with unfinished/dirt basements, and stone or brick foundations. As you’re surveying the property for any potential problems, the presence of moisture and mold should be a bit of a red flag. While not necessarily a reason to panic or walk away from a home you otherwise love, mold remediation and treatment of moisture issues in older houses can stack up to be a big project once the home is in your hands. Here’s everything you need to know about moisture problems in old homes: humidity, mold, how to prevent it, how to fix it, and when you should find a professional to help.\nMold: What’s the Big Deal?\nMold and mildew are organisms that exist in nature to help break down dead matter like trees and fallen leaves. They spread via airborne spores, which can enter an indoor space through the air or by attaching to a moist substance or object. Since mold spores are airborne, they are impossible to eliminate from any area completely. However, these spores can only grow or spread in the presence of moisture; so, the only way to tackle a mold problem is to tackle the underlying moisture or water problem.\nSince the function of mold is to help decay dead matter, a persistent mold issue can be dangerous to the longevity of your home and sentimental items. Mold gradually deteriorates whatever it grows on, so if you notice it growing, it needs to be fixed sooner rather than later. Additionally, mold can cause health problems in sensitive individuals. Allergies to airborne mold spores are quite common; when inhaled, mold can also worsen symptoms of asthma. In rare cases, certain types of mold can be toxic.\nHow to Maintain Healthy Moisture Levels\n- Treat Floods & Leaks. Unwanted water and moisture in your home can cause damage. Fast and thorough cleanup and treatment of floods and leaks prevents heightened moisture levels from growing into bigger issues.\n- Clean Gutters. Keeping rain gutters clean prevents unnecessary overflow and leaks from draining into your home. Regular maintenance on gutters is essential.\n- Proper Ventilation & Circulation. Vent appliances to outdoors if possible. Running a fan, or simply opening doors and windows, creates airflow and helps to prevent water from collecting and causing problems.\n- Air Conditioning & Dehumidification. A/C and dehumidifiers can reduce the amount of moisture in the air.\n- Insulate Cold Surfaces & Prevent Condensation. Cold water pipes and other cold surfaces can cause condensation to gather. Insulating these surfaces prevents the collection of moisture.\nWhat to Look For\nMoisture problems in old homes can be present with or without visible mold, so you should be on the lookout for these signs and signals:\n- Musty odor\n- Water stains on the floors/walls\n- Visible mold\nUh Oh! Watch Out for Hidden Mold\nIf you encounter signs such as odor and water stains, but can’t track down any visible mold, it’s possible that you’ve got a hidden mold problem on your hands. When lost of moisture is present, mold can grow in places undetected in plain sight. Mold can flourish in places like the back of wallpaper, under drywall, on the back side of paneling, on the upper side of ceiling tiles, and other places where air and moisture can become trapped between two surfaces. If you think you have a hidden mold issue, do not disturb the places where you think mold may be growing. Since mold can be a health hazard, it’s best to call in a professional to help get a grasp of the problem.\nSources of Persistent Water Issues\nIf you encounter a wet basement in an old home, there are steps you should address prior to hiring a waterproofing expert. Many contemporary waterproofing techniques can be damaging to old homes with brick or stone foundations. According to William Kibbel III, a celebrated old home restoration expert, delicately correcting the source of the problem is crucial. He says:\nToo often folks tell me that the waterproofing contractor they hired only offered a single option for correcting a water penetration issue. That option just happens to be the most intrusive and costly. If I were considering hiring a contractor to fix my wet basement, I would hope that the causes of the water were evaluated and remedies suggested. Here are some of the most common sources of water entry:\n- Inadequate or clogged gutters and downspouts not extended. There’s a significant amount of water that falls on a roof during heavy rains. This needs to be collected in a maintained system that discharges the water well away from old house foundations.\n- Improper grading. The soil near the foundation should slope away from the house. Current standards require that the ground should slope down at least 6 inches within 10 feet of the foundation. Many homes with basement water issues have been corrected by simply adding soil around the perimeter and grading it to shed the water away. Care should be taken if adding soil. Dirt should be kept well away from siding, trim, or basement window frames.\n- Foundation maintenance. Old stone and brick foundations typically need some re-pointing, when the old mortar has worked its way out of the joints. Many stone foundations also need the interior mortar coating restored.\nHow to Fix Serious Water, Moisture, & Mold Problems\nThe first step to fixing moisture problems in old homes is usually to identify any underlying structural problems. It’s common to think that remediating a mold issue is as simple as cleaning up the mold; that’s not usually the case. To ensure the safety of your future home, it’s important to take a hard look at any underlying problems like the ones outlined above. Next, determine what treatments or renovations are needed to reduce the moisture in your home (in addition to cleaning and repairing any damage done by mold). Generally, these will include installing proper drainage, creating ventilation systems to keep air circulating through and out of the house, and dehumidification of the air to remove airborne moisture and mold.\nSince mold can be dangerous to your health, these are usually not projects you should take on by yourself. Always call in a mold remediation or water damage specialist to help you safely and effectively tackle the problem, to ensure the health and safety of yourself, your family, and your home, for years to come.\nMuch of the information in this article was adapted from the EPA’s Guide to Mold, Moisture, and Your Home."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:16c60115-dba4-4fd3-b1c0-cd4b9929bcb2>","<urn:uuid:208cd134-156f-43eb-84a8-52401c5fb951>"],"error":null}
{"question":"How did drought conditions affect crop insurance claims vs FHB disease impact in 2021?","answer":"In 2021, drought conditions had different impacts: For crop insurance, there was significant deterioration of crops across Alberta since mid-June, leading to adjustments in the Low Yield Allowance to help farmers salvage crops for livestock feed. Meanwhile, regarding FHB disease, the drought conditions actually minimized disease impact across most of the U.S., with only a few major cases reported in Kansas, Missouri, Oklahoma, and Texas.","context":["Low Yield Allowance is a standard part of the production insurance program, and is meant for situations of extreme heat and severe drought. Alberta is doubling the low yield threshold to allow for additional cereal or pulse crops to be salvaged for livestock feed. For example, the barley crop threshold will be increased from 150 to 300 kg per acre.\n“Our Government is working in close collaboration with provinces to ensure farmers who are experiencing the impacts of extreme weather caused by climate change have access to the support they need. This adjustment to the crop insurance program will increase access to feed for livestock producers when they need it most, to ensure they can get through this crisis.\n“Alberta’s hardworking farmers and ranchers have been hit with dry conditions that threaten their livelihoods. We will do everything we can to help Alberta’s agriculture industry make the best of a terrible situation. We’ve heard industry’s call for help, and this change will help farmers and ensure some good can come out of these crops.”\n“We recognize that this year’s conditions have had a significant impact on our producers—and we’ve been here to listen, take feedback, and mobilize our teams as quickly and efficiently as possible.”\nSince mid-June, there has been a significant deterioration of crops across the province. Current weather conditions are not improving, and industry expects to see further deterioration in crops.\nTogether with the federal government, Alberta is making an adjustment to crop insurance that will allow farmers to put more poor quality crops towards livestock feed, helping ease feed shortages for livestock during the current drought.\nThis adjustment encourages producers to act swiftly to salvage crops for livestock feed rather than watch their fields deteriorate further, and risk harvesting nothing. As these crops would otherwise be covered by crop insurance, there will likely be minimal additional payments resulting from this decision.\nUnder the federal-provincial cost-shared Canadian Agricultural Partnership, there are a number of business risk management programs available through Agriculture Financial Services Corporation (AFSC) to help Alberta producers manage significant risks that threaten the viability of their farms, including crop insurance.\nAlberta’s AFSC has a total of 119 active adjustment team members following the addition of 21 new members in December 2020 and April 2021. The government has advised crop adjusters to be flexible and complete early assessments with affected crop and hay land; for example, offering alternative use of crops to address forecasted feed shortages in our livestock industry.\nLast week, Alberta, along with Saskatchewan, Manitoba and Ontario, received verbal commitment from the federal government that a joint AgriRecovery program will be initiated to support producers affected by drought conditions prior to a federal election. Assessments are currently underway in these provinces.\nAlberta announced a 20 per cent reduction in premium costs for crop, pasture and forage insurance earlier this year which protects against weather-related production loss. As a result:\n- Almost 400 additional farmers and ranchers enrolled in crop, pasture and forage insurance.\n- Almost 1,400 farmers and ranchers increased their level of crop, pasture and forage insurance coverage.\n- Almost 230 clients re-enrolled in crop, pasture and forage insurance after canceling their subscriptions in 2020 or prior years.\nThe adjustments to crop insurance add to the steps already taken by the government of Alberta to ensure farmers are supported during this difficult time. Other steps include:\n- Formally requesting the federal government initiate an AgriRecovery assessment.\n- Asking the federal government to ensure all significantly affected municipalities are included as eligible in the designation for the federal Livestock Tax Deferral provision. This would allow farmers who sell part of their breeding herd due to drought in a prescribed drought region to defer a portion of sale proceeds to the following year.\n- Increasing the percentage paid on interim payments under AgriStability.\nMaking a claim\nIt is extremely important that before clients put a crop to an alternative use, they talk to their AFSC branch office.\nPriority will be given to clients who want to use crops for immediate pasture or clients who want to silage or bale crops.\n- Under the Canadian Agricultural Partnership, AgriInsurance premiums for most programs are shared 40 per cent by participating producers, 36 per cent by the Government of Canada and 24 per cent by the Alberta government. Administrative expenses are paid 60 per cent by Canada and 40 per cent by Alberta.\n- Business risk management programs include: AgriInvest, AgriStability, AgriInsurance and AgriRecovery.\n- In Alberta, agriculture industry receives over $300 million in annual assistance from the provincial government through the BRM programs.","For Immediate Release\nContact: Michelle Bjerkness\nUSWBSI Director of Operations\nU.S. Wheat and Barley Scab Initiative’s 2021 Fusarium Head Blight Disease Impact Update Finds Drought Conditions Minimized Disease Impact Across Most of U.S.\nNovember 3, 2021 – The U.S. Wheat and Barley Scab Initiative (USWBSI) is pleased to announce the publishing of its 2021 Fusarium Head Blight Disease Impact Update. This year commentary from experts in 32 states indicated most growers dealt with drought conditions during the season causing reduced yields and test weights. While very few reports of Fusarium head blight (FHB, also known as scab) were received, a few isolated occurrences were found in some states. Major cases of FHB, however, were reported from Kansas, Missouri, Oklahoma, and Texas.\n“The USWBSI Fusarium Head Blight Disease Impact Update is a valuable annual overview of the impact of FHB on small grains crops and also serves to monitor the production conditions that U.S. wheat and barley growers experienced throughout the year” noted Ruth Dill-Macky, USWBSI Research Co-Chair. “Growers and researchers alike rely on this information to plan research in the years to come, identify production issues that might need attention, and to help validate tools used to mitigate FHB including the Fusarium Risk Tool, fungicides, and crop varieties with improved levels of resistance.”\nThe Fusarium Head Blight Disease Impact Update has been re-envisioned to provide an annual report of the crop growing conditions as well as the impact of FHB on wheat and barley in different regions of the United States. Additionally, photos from the different growing regions have been included to highlight specific crop conditions. The USWBSI has been releasing this update article annually since 2010 to provide insight into the experiences that small grains growers had with FHB during the most recent growing season.\nFor readers interested in learning more about Fusarium Head Blight of wheat and barley visit ScabUSA to find publications and resources by USWBSI funded researchers. Producers can also view the USWBSI FHB Risk Tool developed by researchers in the USWBSI, to monitor FHB risk during the growing season as well as review prior year data for planning. Real-time FHB Alerts sent out during the growing season are also provided by the USWBSI, learn more and subscribe on the website.\nThe USWBSI is a national multi-disciplinary and multi-institutional research consortium whose goal is to develop effective control measures that minimize the threat of Fusarium Head Blight (scab), including the production of mycotoxins, for producers, processors and consumers of wheat and barley. The USWBSI’s more than $8.5 million annual budget comes from Federal funds appropriated through the USDA-ARS and is distributed to 150 research projects in more than 30 states.\nFor more information about the USWBSI please contact the Networking & Facilitator Office (NFO) Director of Operations, Michelle Bjerkness, at 612.990.5032 or firstname.lastname@example.org. USWBSI is open to reprinting article or portions upon approval. Please contact Michelle Bjerkness.\nUSWBSI | 304 Stakman Hall | 1991 Upper Buford Circle | St. Paul, MN 55108\nscabusa.org | @USWBSI | www.linkedin.com/company/uswbsi"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:127ab6ac-f4bb-4f66-96da-da8ab735f317>","<urn:uuid:89110533-fde4-4e01-8ae7-ce176e233e66>"],"error":null}
{"question":"How CLT wood panels help make buildings more environmentally friendly? Can you explain step by step process?","answer":"CLT (cross-laminated timber) panels help make buildings environmentally friendly through the following process: 1) The panels sequester and store CO2 over long periods, 2) Timber has low embodied energy compared to other materials, 3) When used in construction, CLT panels turn buildings into 'carbon sinks', storing large quantities of carbon, 4) When used on brownfield sites, this construction method can help reverse negative environmental impacts of urban development.","context":["Sustainable Construction for Urban Infill Development Using Engineered Massive Wood Panel Systems\nAbstractPrefabricated engineered solid wood panel construction systems can sequester and store CO 2. Modular cross-laminated timber (CLT, also called cross-lam) panels form the basis of low-carbon, engineered construction systems using solid wood panels that can be used to build residential infill developments of 10 storeys or higher. Multi-apartment buildings of 4 to 10 storeys constructed entirely in timber, such as recently in Europe, are innovative, but their social and cultural acceptance in Australia and North America is at this stage still uncertain. Future commercial utilisation is only possible if there is a user acceptance. The author is part of a research team that aims to study two problems: first models of urban infill; then focus on how the use of the CLT systems can play an important role in facilitating a more livable city with better models of infill housing. Wood is an important contemporary building resource due to its low embodied energy and unique attributes. The potential of prefabricated engineered solid wood panel systems, such as CLT, as a sustainable building material and system is only just being realised around the globe. Since timber is one of the few materials that has the capacity to store carbon in large quantities over a long period of time, solid wood panel construction offers the opportunity of carbon engineering, to turn buildings into ‘carbon sinks’. Thus some of the historically negative environmental impact of urban development and construction can be turned around with CLT construction on brownfield sites.\nDownload InfoIf you experience problems downloading a file, check if you have the proper application to view it first. In case of further problems read the IDEAS help page. Note that these files are not on the IDEAS site. Please be patient as the files may be large.\nBibliographic InfoArticle provided by MDPI, Open Access Journal in its journal Sustainability.\nVolume (Year): 4 (2012)\nIssue (Month): 10 (October)\nContact details of provider:\nWeb page: http://www.mdpi.com/\nengineered timber; prefabrication; solid wood panel construction system; social acceptance; sustainable design;\nFind related papers by JEL classification:\n- Q - Agricultural and Natural Resource Economics; Environmental and Ecological Economics\n- Q0 - Agricultural and Natural Resource Economics; Environmental and Ecological Economics - - General\n- Q2 - Agricultural and Natural Resource Economics; Environmental and Ecological Economics - - Renewable Resources and Conservation\n- Q3 - Agricultural and Natural Resource Economics; Environmental and Ecological Economics - - Nonrenewable Resources and Conservation\n- Q5 - Agricultural and Natural Resource Economics; Environmental and Ecological Economics - - Environmental Economics\n- Q56 - Agricultural and Natural Resource Economics; Environmental and Ecological Economics - - Environmental Economics - - - Environment and Development; Environment and Trade; Sustainability; Environmental Accounts and Accounting; Environmental Equity; Population Growth\n- O13 - Economic Development, Technological Change, and Growth - - Economic Development - - - Agriculture; Natural Resources; Environment; Other Primary Products\nPlease report citation or reference errors to , or , if you are the registered author of the cited work, log in to your RePEc Author Service profile, click on \"citations\" and make appropriate adjustments.:\n- Guerra, Erick & Cervero, Robert, 2012. \"Transit and the \"D\" Word,\" University of California Transportation Center, Working Papers qt83f6q2nv, University of California Transportation Center.\n- Yuan, Xueliang & Zuo, Jian & Ma, Chunyuan, 2011. \"Social acceptance of solar energy technologies in China--End users' perspective,\" Energy Policy, Elsevier, vol. 39(3), pages 1031-1036, March.\nFor technical questions regarding this item, or to correct its authors, title, abstract, bibliographic or download information, contact: (XML Conversion Team).\nIf references are entirely missing, you can add them using this form."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:b6669301-5d66-45d0-bbe8-ce6bb21c143d>"],"error":null}
{"question":"How do neutron star observations contribute to both educational astronomy and gravitational wave detection research?","answer":"Neutron stars are studied in educational settings through virtual labs where students explore pulsar period datasets and spindown rates using interactive p-pdot diagrams with real astronomy data. In research, these same pulsating neutron stars serve as crucial tools for detecting gravitational waves from supermassive black hole binary systems, as pulsar timing arrays can detect when gravitational waves squeeze and stretch space between Earth and the pulsars, causing pulse arrival time variations at nanohertz frequencies.","context":["Virtual Laboratories for Introductory Astronomy\nby Michael Guidry, University of Tennessee and Kevin M. Lee, University of Nebraska\nThe Brooks/Cole Virtual Astronomy Laboratories consist of 20 virtual online astronomy laboratories (VLabs) representing a sampling of interactive exercises that illustrate some of the most important topics in introductory astronomy. The exercises are meant to be representative, not exhaustive, since introductory astronomy is too broad to be covered in only 20 laboratories. Material is approximately evenly divided between that commonly found in the Solar System part of an introductory course and that commonly associated with the stars, galaxies, and cosmology part of such a course. The VLabs are packaged with the Seeds, Pasachoff & Filippenko, and the Fraknoi, Morrison, & Wolff texts and also may be purchased as a standalone product. Please visit the Brooks/Cole Astronomy site for more information or contact your local Brooks/Cole representative.\nThis material was designed to serve two general functions: on the one hand it represents a set of virtual laboratories that can be used as part or all of an introductory astronomy laboratory sequence, either within a normal laboratory environment or in a distance learning environment. On the other hand, it is meant to serve as tutorial material supplemental to standard textbooks. While this is an efficient use of the material, it presents some problems in organization since (as a rule of thumb) supplemental tutorial material is more concept oriented while astronomy laboratory material typically requires more hands-on problem-solving involving at least some basic mathematical manipulations.\nAs a result, one will find material of varying levels of difficulty in these laboratories. Some sections are highly conceptual in nature, emphasizing more qualitative answers to questions that students may deduce without working through involved tasks. Other sections, even within the same virtual laboratory, may require students to carry out guided but non-trivial analysis in order to answer questions. In this manual, we shall provide some information about choosing portions of laboratories for particular environments by classifying the sections of the Vlabs according to three levels of difficulty, and by providing sample tracks through the material that would be appropriate for several different levels of course usage and student engagement.\nTable of Contents\n- Units, Measure, and Unit Conversion\n- Properties of Light and Its Interaction with Matter\n- The Doppler Effect\n- Solar Wind and Cosmic Rays\n- Planetary Geology\n- Tides and Tidal Forces\n- Planetary Atmospheres and Their Retention\n- Extrasolar Planets\n- Asteroids and Kuiper Belt Objects\n- The Spectral Sequence and the HR Diagram\n- Binary Stars\n- Stellar Explosions, Novae, and Supernovae\n- Neutron Stars and Pulsars\n- General Relativity and Black Holes\n- Astronomical Distance Scales\n- Evidence for Dark Matter\n- Active Galactic Nuclei\n- The Hubble Law\n- Fate of the Universe\nThe User's Guide contains a complete description and screen shots of each lab.\nSimulation Example: Asteroids and Kuiper Belt Objects -- Resonance\nOne of the intriguing features of the asteroid belt is the lack of asteroids with certain semimajor axis values. Some of these lacunae -- known as the Kirkwood Gaps -- are clearly visible in a histogram showing the distribution of semimajor axis values of asteroids (click to open). These gaps come about from resonant perturbations by Jupiter that quickly pull any asteroid that may have such a semimajor axis into a new orbit.\nThe Kirkwood Gaps have shown that resonance can influence the orbits of asteroids, but what about other small solar system bodies? In 1992 the first Kuiper Belt Object, or KBO, was discovered beyond the orbit of Neptune. Hundreds of these cold, icy objects have been discovered since then. Are resonance patterns apparent in the Kuiper Belt?\nA plot of the KBO distribution reveals the answer (click to open). In the graph we see the eccentricity versus semimajor axis plotted for over 500 KBOs. A striking feature of this graph is the column of KBOs at 39.4 AU. These are KBOs that are in 2:3 resonance with Neptune -- that is, they complete two orbits of the Sun for every three Neptune orbits. These are also called plutinos since Pluto is a member of this group.\nAlthough resonance is clearly affecting these KBOs, it must be different kind of resonance than that between Jupiter and the main-belt asteroids since these KBOs are selectively maintained in their orbits instead of being scattered. What is going on? In the VLab we encourage students to discover the answer by noticing how the closest approach distance to Neptune depends on the KBO's orbital parameters. KBOs that get too close to Neptune are at risk of being scattered over time. A KBO orbit simulator allows students to create asteroids with different eccentricities and semimajor axes and follow a chart showing the KBO-Neptune distance (click to open). Create an asteroid with a semimajor axis of 39.39 AU and eccentricity of 0.35 and you will see that the KBO never gets closer than about 23 AU to Neptune, even though its orbital path crosses Neptune's.\nThe use of computer simulations to allow students to discover relationships like KBO resonances is a central feature of the VLabs.\nInteractive DataSet Example: Neutron Stars and Pulsars -- Distribution of Periods\nPulsars are rapidly rotating neutron stars that emit very regular, short bursts of energy (usually in radio waves, but some pulsars also flash in x-rays and visible light). Careful observations of pulsars over time show that they slow down over time, that is, their periods increase. The rate the period changes is referred to as the spindown rate (labelled [p with dot], or \"p-dot\"). VLab students are encourged to explore the p and pdot values for pulsars in an interactive p-pdot diagram (click to open).\nAllowing students to explore datasets of real astronomy data is another mainstay of the VLabs.","Authors: Chengcheng Xin, Chiara M. F. Mingarelli, Jeffrey S. Hazboun\nFirst Author’s Institution: Columbia University, Department of Astronomy, 550 West 120th Street, New York, NY, 10027, USA\nStatus: Submitted to ApJ\nDisclaimer: I am a member of the NANOGrav collaboration but was not involved in this work.\nThis year’s Nobel Prize in Physics was, in part, awarded to Andrea Ghez and Reinhard Genzel for their discovery of the supermassive black hole (SMBH) at the center of the Milky Way. This was an amazing discovery, and has led many astronomers, including the authors of today’s paper, to look for SMBHs in other galaxies. However the authors of today’s paper are looking not for a single SMBH, but for supermassive black hole binary systems (SMBHBs) which may be formed when two galaxies merge. To do this, they utilize multimessenger astrophysics. First they analyze electromagnetic observations to identify candidate SMBHBs. Then they determine the strength of the gravitational waves emitted by the orbits of these SMBHBs, and predict if they will be detectable with current and future pulsar timing arrays.\nCombing for Candidates\nSome SMBHBs candidates belong to a class called active galactic nuclei , or AGN, which emit massive amounts of energy that we observe as electromagnetic waves. As the SMBHBs orbit each other, they can cause periodic changes in the brightness, or magnitude, of the light emitted over time. This time-variable light emission is known as a light curve. Many large scale surveys such as the SDSS V or the Catalina Real-Time Transient Survey (CRTS) regularly monitor the light from sources across the sky, resulting in light curves from potential SMBHB sources that span years, like that shown in Figure 1.\nThe authors of today’s paper compile a total of 149 of these SMBHB candidates from various catalogs, and then use the source’s distance and masses to predict the frequency and strength, or strain, of the gravitational waves that may be emitted by the binary.\nA Galaxy Sized Gravitational Wave Detector\nThe gravitational waves emitted by SMBHBs will not be like those that have been detected by the Laser Interferometer Gravitational-Wave Observatory (or LIGO), which are emitted at frequencies between 10 and 10,000 Hz. These will be emitted at nanohertz frequencies, which means these binaries will take years to complete one orbit. We can’t detect these low frequency gravitational waves with ground based detectors, so instead we use pulsar timing arrays. Pulsars are neutron stars that emit radio pulses that can be timed, or have their time of arrival at Earth predicted, extremely precisely. As the gravitational waves from these SMBHBs pass between Earth and the pulsar, they will squeeze and stretch the space, making the pulses arrive slightly earlier or later, then predicted. This signal will be different in each pulsar depending on where it is in space, and is discussed in more detail in this astrobite.\nCurrent pulsar timing array experiments such as those done by North American Nanohertz Observatory for Gravitational Waves (NANOGrav), the European Pulsar Timing Array (EPTA), and the Parkes Pulsar Timing Array (PPTA), which collectively make up the International Pulsar Timing Array (IPTA), all regularly search their data for these individual sources of gravitational waves, but none have been found so far. This makes searching for potential candidates for gravitational wave detection through mulitmessenger methods crucial for these experiments.\nPredicting the Future\nThe authors of this paper use previous limits set by these pulsar timing arrays, and then use models to extrapolate how sensitive they might be in the future, with particular attention to the IPTA and the Square Kilometer Array (SKA), an array of radio telescopes currently being built which is expected to find and time hundreds of pulsars. The authors then look to see if any of their most promising SMBHB candidates will be detectable in the future, shown in Figure 2 by the candidate points and sensitivity curves and in Figure 3 by the gravitational wave strain needed for the pulsar timing array to detect the source with a signal-to-noise greater than 3 in different parts of the sky. We can see from both of these figures that some SMBHB candidates very well might be detectable, though not for at least a few more years.\nBeing able to identify and predict the detectability of potential SMBHB candidates from light curves is crucial for confirming them in the future, as gravitational wave emission is an undeniable signature that the candidate really is a SMBHB. With telescopes like the Vera Rubin Observatory Legacy Survey of Space and Time (LSST) coming online soon, more and more of these candidates will be identified by their light curves. Finding these candidates also allows for targeted gravitational wave searches which may increase the sensitivity by a factor of two! This paper has helped lay the groundwork for multimessenger astronomy as a critical component for detecting SMBHBs in future pulsar timing array data, and shows there is a lot to look forward to!\nAstrobite edited by: Abby Waggoner\nFeatured image credit: AUI/NRAO, NAOJ, and Science/Nicole Rager Fuller"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:da7fe674-e42c-4bf6-ba30-ddcb131e688a>","<urn:uuid:b11b456d-db5c-41c0-8663-7ed46047766a>"],"error":null}
{"question":"Which species forms larger social groups - Callicebus nigrifrons or the opossum?","answer":"Callicebus nigrifrons lives in small family groups of 2 to 5 individuals and is monogamous and territorial. In contrast, the opossum (Didelphis virginiana) is often described as nomadic, with no mention of social grouping, suggesting a solitary lifestyle. Therefore, Callicebus nigrifrons forms larger social groups than the solitary opossum.","context":["|Scientific Name:||Callicebus nigrifrons|\n|Species Authority:||(Spix, 1823)|\nCallicebus personatus ssp. nigrifrons (Spix, 1823)\n|Taxonomic Notes:||Kobayashi and Langguth (1999) and van Roosmalen et al. (2002) recognize five species groups – cupreus, donacophilus, moloch, personatus and torquatus. Callicebus nigrifrons belongs to the personatus group which also includes: Callicebus barbarabrownae, Callicebus coimbrai, Callicebus melanochir and Callicebus personatus.|\n|Red List Category & Criteria:||Near Threatened ver 3.1|\n|Assessor(s):||Veiga, L.M., Kierulff, C.M., de Oliveira, M.M. & Mendes, S.L.|\n|Reviewer(s):||Mittermeier, R.A. & Rylands, A.B. (Primate Red List Authority)|\nListed as Near Threatened as it is reasonable to assume that the species has undergone a decline in the region of 20-25% over the past 25 years (three generations) due to extensive habitat loss in the Atlantic forest of Brazil. Almost qualifies as threatened under criterion A2c.\n|Previously published Red List assessments:|\n|Range Description:||Callicebus nigrifrons is distributed over a relatively wide area in south-eastern Brazil. Its range includes much of São Paulo, southern Minas Gerais, and eastern Rio de Janeiro states. It has the largest distribution of any species in the personatus group (van Roosmalen et al. 2002), It is found north of the Tietê and east of the Rios Paraná and Parnaíba, and on both margins of the upper Rio São Francisco. The species extends east as far as the Mantiqueira and Espinhaço ranges, where it meets the range of C. personatus.|\n|Range Map:||Click here to open the map viewer and explore range.|\n|Population:||The population density of C. nigrifrons in a 50-ha forest fragment was estimated at 80–100 individuals per km² (Soares 2006), which is six times higher than for C. personatus (a similar-sized species living in the same habitat). In the Atlantic forest, a mean density of 15.67 ± 6.51 individuals/km² has been recorded (Chiarello and Melo 2001).|\n|Current Population Trend:||Decreasing|\n|Habitat and Ecology:||An inhabitant of the Atlantic forest, occurring in both mature rain forest and disturbed fragments. The ecology of this species is relatively unknown, with most data coming from inferences based on studies of other members in the genus, which suggest that this species is frugivorous, monogamous, lives in small family groups (2 to 5 individuals) and is territorial (Cäsar et al. in press). A considerable amount of time is spent of foraging and feeding (47%), and a significant amount of time resting (21%) (Franco 2006).|\n|Major Threat(s):||This species inhabits the most developed and populous region of Brazil, and has suffered extensive habitat loss and fragmentation. Although the species is widespread, urbanization, expanding agriculture and logging practices have led to extreme fragmentation of the forests within its range and resulting small isolated populations. In many places, they have been locally or regionally extirpated even where forests patches remain.|\nOccurs in several protected areas, including the Serra do Mar reserve complex, the Área de Proteção Ambiental (APA Sul), in the municipality of Nova Lima, south of the metropolitan region of Belo Horizonte and even the Cantareira State Park in the centre of the city of São Paulo. The extensive fragmentation of remaining habitat throughout its range may necessitate a metapopulation management strategy in the future (Printes et al. in prep).\nIt is listed on CITES Appendix II.\n|Citation:||Veiga, L.M., Kierulff, C.M., de Oliveira, M.M. & Mendes, S.L. 2008. Callicebus nigrifrons. The IUCN Red List of Threatened Species 2008: e.T39943A10294282.Downloaded on 25 October 2016.|\n|Feedback:||If you see any errors or have any questions or suggestions on what is shown on this page, please provide us with feedback so that we can correct or extend the information provided|","Orlando Wildlife Trappers Information\nRACCOON: Procyon lotor\nSIZE - Average 12-20 lbs. in Florida\nLIFE EXPECTANCY - Up to 12 years in the wild\nBREEDING - Mating in December, 3-5 young born in April\nDIET - Omnivorous - meat, berries, grains, vegetation\nHABITAT - Mixed forests and especially urban areas\nFACTS - 40 teeth, nimble hands, black mask, very strong\nEASTERN GRAY SQUIRREL: Sciurus carolinensis\nSIZE - About 1 lb for adults\nLIFE EXPECTANCY - Up to 10 years, but usually shorter\nBREEDING - Two litters per year - Mating in December and June, 44 day gestation, 3-4 young born, weaned in 10 weeks\nDIET - mostly nuts and seeds\nHABITAT - Forests and suburban areas\nFACTS - 22 teeth, bushy tail, very agile\nOPOSSUM: Didelphis virginiana\nSIZE - Adults 10-14 lbs.\nLIFE EXPECTANCY - no more than 2-3 years\nBREEDING - Marsupial - mates in January, young are born very tiny after only 12 days, then they stay in the pouch and grow\nDIET - Omnivorous. Will eat almost anything but prefers meat.\nHABITAT - Often nomadic, but will use human buildings\nFACTS - 50 teeth, prehensile tail, oposable thumbs, plays dead\nNINE BANDED ARMADILLO: Dasypus novemcinctus\nSIZE - Adults 12-16 lbs.\nLIFE EXPECTANCY - Up to 15 years\nBREEDING - Mates in summer, delayed fertilization in November, 120 day gestation, gives birth to four identical quadruplets\nDIET - Digs up worms and grubs from the ground\nHABITAT - Southern fields and forests\nFACTS - Has bony, leathery shell. Can carry leprosy\nROOF RAT: Rattus rattus\nSIZE - Adults 6-10 oz. Body 8 in. tail 8 in.\nLIFE EXPECTANCY - Rarely more than one year.\nBREEDING - Females can mate year-round, produce up to 5 litters per year, composed of 6-10 young. Gestation is 21-28 days.\nDIET - Almost anything, prefers fruits and grains\nHABITAT - Lives above ground, prefers buildings\nFACTS - Nocturnal, great climbers, small home range\nEASTERN MOLE: Scalopus aquaticus\nSIZE - Adults average 3 oz. and 6 inches.\nLIFE EXPECTANCY - 2-3 years\nBREEDING - Breeds in January, gives birth to 2-4 young after a 45 day gestation.\nDIET - Almost entirely earthworms, also grubs\nHABITAT - Underground- fields, loose soil\nFACTS - Blind. Digs tunnels underground\nPIGEON: Columba livia\nSIZE - Average 12 inches long and 1 lb. weight\nLIFE EXPECTANCY - Average 3-5 years\nBREEDING - Nests any time of year. Mates for life. Young hatch in 19 days.\nDIET - Mostly seeds, but will eat a varied diet\nHABITAT - Mostly urban areas and architecture\nFACTS - Flying rats. Gutter Birds. Feathered friends.\nFREE-TAIL BAT: Tadarida brasiliensis\nSIZE - Wingspan about 6 inches, weight 0.5 oz.\nLIFE EXPECTANCY - Up to 16 years\nBREEDING - Breeds in fall, delayed fertilization, female gives birth to one pup in early June.\nDIET - Entirely insectivorous\nHABITAT - Caves and often human buildings near water\nFACTS - Not blind. Uses echolocation to help navigation.\nSIZE - Depends on species - 6 inches to 6 feet.\nLIFE EXPECTANCY - Up to 20 or more years, depending on species\nBREEDING - Some give birth to live young, some lay eggs\nDIET - carnivorous\nHABITAT - many habitats, though undeveloped areas are better\nFACTS - No eyelids. Smells with tongue. 4 venomous species in FL\nThe above facts are just the basics for many of the wildlife species that I deal with in the Orlando area. It is important to understand the behavior and breeding cycles of the wildlife that I deal with. For example, many\nanimals enter attics only when they bear young. Then when I trap and remove the wildlife, it's important that I know if the animal does have a nest of young, and if so, I should find and remove the baby animals. Knowledge\nof wildlife diet not only helps me use the best baits to catch them, but also may tell me what nearby food sources are attracting the animals to the area. The life expectancy of an animal gives me an idea of its conservation\nstatus. I'd rather kill a rat - which can have up to 50 young per year and which has a 95% first-year mortality rate, than kill a precious bat, which has only one young per year and can live up to 16 years. Information is\noften the best tool when dealing with wildlife - more valuable than a steel trap or snare pole! If you want to hire the best in Orlando trappers and animal trapping, give us a call."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c6d16992-b201-485a-b7d3-469d5cd4df3a>","<urn:uuid:081a5865-8d77-4208-823c-1bf949d0d781>"],"error":null}
{"question":"What is the recommended duration of rest after surgical treatment for flat feet compared to tennis elbow surgery?","answer":"For flat feet surgery, patients must eliminate all weight-bearing activities for 2-3 months. For tennis elbow surgery, the full recovery time may take 3-6 months.","context":["Flat feet, sometimes referred to as adult flat feet or flatfoot, are painful and progressive deformities affecting the arch of the foot.\nAccording to William Saar, DO, an orthopaedic surgeon specializing in foot and ankle conditions, flat feet result from the tearing and deterioration of the soft tissues in the arch. Repeated wear causes the arch to flatten and the toes to point outward. Learn more about the risk factors, symptoms, diagnosis, and treatment.\nNever Miss a Beat!\nSubscribe to Our HealthBeat Newsletter!\nGet Healthy Tips Sent to Your Phone!\nRisk Factors and Causes of Flat Feet\nFlat feet, or a lack of an arch, are normal among children and toddlers because some arches take longer to develop. In fact, some people never develop an arch and may or may not experience difficulties as a result. These flat feet are known as congenital flat feet, and no treatment is typically needed.\nIn other cases, however, the causes of flat feet range from pre-existing medical conditions. This includes the deterioration of tendons that support the arch, arthritis, diabetic foot disease, injuries, or even the wear and tear of aging.\nDr. Saar says any adult over the age of 50 who notices that one foot is getting flatter than the other should seek care from an orthopaedic foot and ankle specialist. Earlier treatment can avoid more extensive surgery in the future.\nFlat Feet Symptoms\nFlat feet often develop over time and involve symptoms that progress and may contribute to related conditions. Some patients with flat feet report no physical pain or changes. Others may experience a wide variety of symptoms, including:\n- Difficulty walking or bearing weight on the feet.\n- Inability to lift the heels off the ground while attempting to rise onto the toes.\n- Bumps on the side of the feet.\nTreatment for Flat Feet\nTreatment options for flat feet include:\n- Rest or avoidance of high-impact, repetitive, weight-bearing activities (i.e. running).\n- Anti-inflammatory pain medications.\nNonsurgical treatment options like shoe modifications, rest, and medications can be very effective in managing pain and preserving both function and mobility in your feet.\nSurgery for Flat Feet\nIn some cases, when conservative methods are not successful, surgery may be recommended. These surgical options include:\n- Lateral column lengthening.\n- Tendon reconstruction.\nFollowing any sort of surgical treatment for flat feet, you will have to eliminate all weight-bearing activities for a period of two to three months. This rest period can prevent further difficulties and potential complications.\nWhether you have a new injury or symptoms that include chronic foot, ankle, or heel pain, swelling, or limited movement, the foot and ankle experts at UPMC Orthopaedic Care can offer comprehensive treatment options to help you find relief. Visit our foot and ankle services page for more information, or call 1-866-987-6784 to schedule an appointment.\nEditor's Note: This article was originally published on , and was last reviewed on .\nAbout UPMC Orthopaedic Care\nWhen you are dealing with bone, muscle, or joint pain, it can affect your daily life. UPMC Orthopaedic Care can help. As a national leader in advanced orthopaedic care, we diagnose and treat a full range of musculoskeletal disorders, from the acute and chronic to the common and complex. We provide access to UPMC’s vast network of support services for both surgical and nonsurgical treatments and a full continuum of care. Our multidisciplinary team of experts will work with you to develop the treatment plan that works best for you. Our care team uses the most innovative tools and techniques to provide better outcomes. We also are leaders in research and clinical trials, striving to find better ways to provide our patients care. With locations throughout our communities, you can find a provider near you.","TENNIS ELBOW (LATERAL EPICONDYLITIS)\nTennis elbow, or lateral epicondylitis, is a type of tendinitis that causes pain in the elbow and arm, and is the most common reason that people see a physician for elbow pain. Tennis elbow is an overuse injury that causes inflammation to the tendon that joins the forearm muscles on the outside of the elbow. Despite its name, you do not have to be an athlete, or play tennis, to develop tennis elbow. Any repetitive gripping activities, especially movements using the thumb and first two fingers, can contribute to tennis elbow.\nTennis elbow affects the tendons attached to the muscles on the outer (lateral) side of the elbow, which work to extend the wrist backwards and straighten the fingers. In contrast, golfer’s elbow affects the tendons attached to the muscles on the inner (medial) side of the elbow, which work to flex the wrist and contract the fingers when you grip something.\nCOMMON CAUSES OF TENNIS ELBOW\nTennis elbow usually develops slowly over time due to repetitive stress placed on the tendons on the outside of the elbow. Continuous stress can eventually cause microscopic tears in the tendons that connect your forearm muscles to the bones in your elbow, resulting in inflammation and discomfort.Activities such as tennis, weight lifting, gardening, throwing, and swimming can all cause damage to the tendons in the elbow. People with jobs or hobbies that require repetitive arm movements or gripping that require turning the wrist can also develop tennis elbow over time. This would include hair stylists, carpenters, plumbers, knitting, painting, and more.\nCOMMON SYMPTOMS OF TENNIS ELBOW\nSymptoms of tennis elbow include pain and tenderness in the bony knob on the outside of your elbow. The injured tendon connects to the bone where this knob is located. Pain from tennis elbow can radiate into the upper and lower arm as well. Even though the damaged tendon is located in the elbow, painful symptoms are likely to occur when doing things with your hands. Patients often experience painful symptoms when:\n- Lifting objects\n- Gripping an object, such as a tennis racket\n- Opening a door or shaking hands\n- Raising your hand or straightening your wrist\nTo diagnose tennis elbow, your physician will first conduct a physical exam to assess symptoms such as swelling and tenderness in the elbow. The physician will also perform a series of movements to check your arm, wrist, and elbow to try and replicate any painful symptoms. X-rays help determine the condition of the elbow and can help rule out other potential problems. An MRI may also be needed in order to see of soft tissues (muscles, ligaments, tendons) within the elbow.\nTennis elbow, or lateral epicondylitis, is usually treated effectively with rest. The R.I.C.E. method is a simple self-care technique that helps reduce swelling, ease pain, and speed the healing process.\n- Rest: when you begin experiencing pain and discomfort in the elbow to help reduce inflammation and give the tendon time to heal\n- Ice and Cold Packs: can help reduce pain and swelling. Ice should be applied for about 15 minutes several times a day\n- Over-the-counter anti-inflammatory medication: such as Ibuprofen (Advil, Motrin) and naproxen (Aleve) can ease mild to moderate pain and reduce inflammation\n- Braces and Splints: supportive bracing on the forearm can help take pressure off the tendons in the elbow. A wrist splint worn at night can help rest the muscles and tendons in the lower extremities\nYour physician my also recommend a corticosteroid injection to treat painful symptoms and reduce inflammation in the elbow. Physical therapy can also help reduce pain, speed the recovery process, and reduce the risk of reinjuring the elbow.\nConservative treatments usually work for tennis elbow. However, if painful symptoms remain after six to twelve months of conservative treatment, you may need surgery. The surgical procedure for tennis elbow involves removing any damaged muscle and reattaching healthy muscle back to the bone. Physical therapy will be an important part of the recovery process. Full recovery time may take three to six months."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:147856cc-b8e8-42ec-8739-43204a039d22>","<urn:uuid:b98af60b-cde5-4947-bb6d-83e8698af43c>"],"error":null}
{"question":"What is the Food Safety Modernization Act's impact on farm oversight, and how does food typically become contaminated in agricultural settings?","answer":"The Food Safety Modernization Act (FSMA), passed in 2011, introduced the first-ever federal oversight of food production on farms, which were previously not inspected by the FDA or USDA. Under FSMA, the FDA will conduct both announced and unannounced inspections of farms, starting with large farms (produce sales > $500,000) and phasing in smaller farms over five years. As for contamination in agricultural settings, food can become contaminated in three ways: chemical contamination (from heavy metals in soil or agricultural chemicals), physical contamination (from foreign materials like metal fragments or stones during harvest), and biological contamination (from microorganisms introduced through wildlife defecation, improperly composted manure, or unclean handling).","context":["The Food Safety Modernization Act (FSMA)\nThe FDA Food Safety Modernization Act (FSMA) was passed on January 4, 2011. The Produce Safety Rule (PSR) is one of the rules within this regulation. The PSR focuses on setting the first-ever federal regulatory standards for the production, harvest, and handling of fruits and vegetables in an effort to prevent microbial contamination and reduce foodborne illnesses associated with fresh produce.\n- Frequently Asked Questions on FSMA\n- Background of FSMA\n- Overview of the FSMA Produce Safety Rule Requirements\nProduce Safety Standards\nAs a key element of its preventive approach, the FDA formalized science-based, minimum Produce Safety Standards for the safe growing, harvesting, packing, and holding of produce on farms to minimize contamination that could cause serious adverse health consequences or death. These standards form the foundation of the Produce Safety Rule.\nThe Produce Safety Alliance\nA primary resource for information about the Produce Safety Rule relevant training is the Produce Safety Alliance (PSA). The PSA is a collaboration between Cornell University, the FDA and the USDA that aims to prepare fresh produce growers to meet the regulatory requirements included in the Produce Safety Rule.\nIndiana Training Options\nIndiana produce growers can receive on-site compliance training as they work to meet Food Safety Modernization Act (FSMA) regulations. The courses are designed to meet the needs of growers around the state. Small private training opportunities are available upon request for groups of three or more. Visit our Training Webpage to learn more about the training we offer.\nProduce Safety Alliance Grower Training Courses\nProduce Safety Alliance (PSA) Grower Training courses are designed for growers and others interested in learning about produce safety, the FSMA Produce Safety Rule, official Good Agricultural Practices (GAP), and co-management of natural resources and food safety. These courses represent one way to satisfy the FSMA Produce Safety Rule requirement outlined in § 112.22(c).\nEach training session includes approximately seven hours of instruction time covering content contained in these seven modules:\n- Introduction to Produce Safety\n- Worker Health, Hygiene, and Training\n- Soil Amendments\n- Wildlife, Domesticated Animals, and Land Use\n- Agricultural Water (Part I: Production Water; Part II: Postharvest Water)\n- Postharvest Handling and Sanitation\n- How to Develop a Farm Food Safety Plan\nThe first six modules listed above align with sections outlined in the FSMA Produce Safety Rule. The seventh module is focused on helping growers develop a written farm food safety plan. Although the farm food safety plan is not required by the FSMA Produce Safety Rule, it is included in the curriculum, because growers expressed a need for a plan.\nProduce Safety Alliance Train-the-Trainer Courses\nProduce Safety Alliance Train-the-Trainer courses are also available, although they are not conducted through the Purdue Extension of Safe Produce Indiana. These courses are designed for fruit and vegetable growers who are interested in becoming PSA Trainers or PSA Lead Trainers. Those who become a PSA Trainer or PSA Lead Trainer are able to offer the PSA standardized curriculum to train fresh produce growers to meet the regulatory requirements in the FSMA Produce Safety Rule.\nAssociation Food And Drug Officials (AFDO) is a regulatory organization that connects food and medical-products safety stakeholders and impacts the regulatory environment by shaping sound, science-based rules, laws, regulations, and sharing best practices that protect the public health. Or, simply put, connect, share, and protect. Visit the AFDO website for additional information from AFDO about produce safety.","No one wants to get sick from contaminated food. From farmers to retailers, here is what you need to know about how the food industry and government agencies work to keep you healthy and trust the food you buy.\nContributing writers Susan Leaman and Diane Wetherington have extensive experience and knowledge in the food industry. Susan works with companies and associations to develop solutions that address produce-related food safety issues; and Diane is CEO of iDecisionSciences, LLC, a provider of specialty crop consulting services, and iFoodDecisionSciences, Inc., a software solutions provider for the food industry.\nOne of the most important underlying considerations to the food industry supply chain is food safety. As consumers, we are familiar with the more popular food recalls such as Listeria monocytogenes, pathogenic E. coli, and Salmonella—but here is a look at the bigger picture. Industry experts Susan Leaman and Diane Wetherington provide an overview of what food safety encompasses in the food supply chain.\nHow does food become contaminated?\nFood is considered contaminated – or “adulterated” in official jargon – when something potentially harmful is found in or added to it. Food contaminants are categorized in three ways: chemical, physical, or biological.\nChemical contaminants, such as heavy metals (e.g., cadmium, arsenic, lead), may occur naturally in the soil or as a component of an agricultural chemical and are taken up by the plant as it grows. Chemical contaminants may also be deposited in the soil and/or plants from the surrounding environment (i.e., from a nearby landfill or the smokestack of a nearby factory).\nForeign material, such as metal fragments, stones, rodents, bugs, hair, etc. in a finished food product is considered a physical contaminant and can contaminate the food crop during harvest and transportation or when it is in the processing or manufacturing facility.\nBiological contamination is what we most often hear or read about in the news. The most common biological contaminants are microorganisms – food safety professionals call them human pathogens and the rest of us commonly call them “germs”. Most of these germs are undetectable by smell or any visible signs caused by spoilage that may make the food inedible. You wouldn’t choose to eat a mushy apple over a crisp one, for example. But this does not necessarily mean the food is unsafe. However, some microorganisms such as parasites, viruses, and bacteria when consumed in a large enough amount, can make you sick. A sickness caused by eating food contaminated with parasites, viruses, and pathogenic bacteria is called foodborne illness. These germs can be introduced into food at any point from production through food preparation and delivery. For example, wildlife may enter a field and defecate on the crops, manure can be improperly composted, trucks may have contaminated cargo areas, and workers preparing food may have unclean hands.\nAny food that does not receive a treatment to kill harmful germs is more susceptible to contamination. Unpeeled, raw, or undercooked foods are most susceptible to germs due to inappropriate handling of raw food and cross-contamination onto cutting boards, knives, and kitchen counters. Cooking or pasteurizing food for a specific amount of time at a threshold temperature kills human pathogens. Proper kitchen safety is also important.\nWhich foods have been associated with foodborne illness outbreaks?\nIn 2013, the Center for Disease Control and Prevention (CDC) published an assessment of the foodborne illness outbreaks that occurred in the U.S. between 1998 and 2008 (Painter, 2013). The agency evaluated the outbreak-related data by the type of hazard (bacterial, chemical, parasitic, and viral) and food deemed responsible based on the outbreak investigation, and found that 34.2% were attributed to contaminated vegetables, 13.8% to dairy products, 12.2% to meat, and 11.7% to fruits and nuts. The majority of foodborne illness outbreaks were caused by viruses (57%) and bacteria (38%) while parasites and chemicals caused 2% and 3%, respectively.\nFoods Associated with Foodborne Illness:\n- Raw foods of animal origin, that is, raw meat and poultry, raw eggs, unpasteurized milk, and raw shellfish are the most likely to be contaminated.\n- Fruits and vegetables can also be contaminated with animal waste when manure is used to fertilize produce in the field, or unclean water is used for washing the produce.\n- Raw sprouts are particularly concerning because the conditions under which they are sprouted are ideal for growing microbes.\n- Unpasteurized fruit juices or cider can also be contaminated if there are pathogens on the fruit that is used to make it.\n- Any food item that is touched by a person who is ill with vomiting or diarrhea, or who has recently had such an illness, can become contaminated. When these food items are not subsequently cooked (e.g., salads, cut fruit) they can pass the illness to other people.\nFor the curious, the CDC’s foodborne illness outbreak online database (FOOD tool) is available here, and the U.S. Food and Drug Administration (FDA) also publishes a record of foods recalled due to contamination.\nSo are foodborne illness outbreaks really increasing in frequency?\nNot necessarily. We are more aware of food recalls because of reporting systems, testing programs, and the ability to detect pathogens.\nCommunicating foodborne illnesses through reporting systems that link health care providers and public health agencies have greatly improved since the mid-1990s. Surveillance of the food supply by government agencies and product testing by food manufacturers has also increased, leading to more product recalls. Finally, technological advancements have made it possible to detect pathogens in lower amounts.\nWhat is the food industry doing to prevent food contamination?\nThere are various stakeholders that play a role in ensuring the safety of our food supply. Although you may occasionally read the news of people who knowingly put contaminated food into commerce, by far, the vast majority of those involved in producing our food are very concerned about the safety of consumers – after all, their family and friends eat the food, too.\nAt the farm level, numerous food safety guidelines or good agricultural practices (GAPs) have been established by industry with the help of trade associations, government agencies, and other food safety experts. GAPs address the use of water, soil amendments, tools, and equipment; the hygiene of field personnel; what to do if a field floods or if animal feces are found in the field during harvest; and many other circumstances that compromise the safety of the crop. Some of these guidelines are the basis for voluntary food safety programs; some are enforced under mandatory programs (e.g., California Cantaloupe Safety Standards) while still others are mandatory after a company voluntarily joins the program (e.g., Arizona and California Leafy Green Handler Marketing Agreements).\nFood Service, Retailers, and Food Packing/Processing\nCompanies that pack, process, or manufacture foods implement various practices, processes, and procedures have been required by law to implement good manufacturing practices (GMPs). Although the rules have been updated over the years, current GMPs cover facility design, conditions of the grounds around facilities, cleaning and sanitation of facilities and equipment, pest control, worker hygiene, workflow, and other potential sources of contamination.\nNon-profit Organizations and Third-Party Auditors\nThere are also various international non-profit organizations that maintain food safety schemes established by working groups and committees of industry, government, and academic food safety professionals. The Global Food Safety Initiative is one of these international organizations that certifies food safety schemes against a set of industry-derived standards. Farms and food manufacturing companies subscribe to these schemes, often at the behest of their customers, and third-party auditing companies evaluate and score the manufacturer’s performance against the scheme’s standards.\nSince 2009, food manufacturing and processing facilities are required to be registered with the FDA and report when any microbial test results are positive. When microbial testing indicates that food is contaminated/adulterated with human pathogens, a recall is issued. In addition to food companies self-reporting contamination, the FDA routinely inspects food manufacturing/processing facilities, and if they find conditions that compromise the safety of the food (i.e., unsanitary conditions), the agency issues warning letters that are made public. Facilities then have an allotted amount of time to address the issues, and if they fail to do so, they can ultimately be shut down. This is exactly what happened in 2012 to a food manufacturing company that made organic nut-butters for Trader Joe’s. Based on numerous GMP violations, the company’s own product testing records, and the FDA’s microbial test results, the agency shut down operations after the facility failed to properly address the violations.\nThe Food Safety Modernization Act\nIn 2011, the Food Safety Modernization Act (FSMA) became law. The FSMA upgraded existing laws governing food manufacturers and processors, and more significantly, for the first time, included oversight of food production on farms. Prior to the FSMA, farms were not inspected by the FDA or the U.S. Department of Agriculture (USDA). Additionally, aside from a foodborne illness outbreak, government regulators were not authorized to inspect a farm. This means FDA regulators were unable to investigate the facilities, fields, coolers, food storage areas, etc., and to question the operators about their practices.\nAs part of FSMA, the FDA wanted to better understand how food is grown for every commodity. It was important for them not to make the rules so onerous that farmers would go out of business. Thus, the FDA visited farms, held public meetings to hear farmers’ and consumers’ concerns, and revised the rules they proposed based on the feedback from industry and the public. By 2018, the FDA will begin conducting both announced and unannounced inspections of farms. They will begin inspecting large farms (produce sales > $500,000) with inspections of smaller farms phased-in over the next five years. Farms making less than $25,000 are excluded from FDA inspection, and other small farms will have less stringent requirements if their food sales are less than $500,000, and 50% of their crop is sold directly to consumers or retail food establishments/restaurants.\nIn 2011, the Food Safety Modernization Act (FSMA) became law. The FSMA upgraded existing laws governing food manufacturers and processors, and more significantly, for the first time, included oversight of food production on farms.\nUnder FSMA, food manufacturers and processors will also be required to establish and implement a food safety plan and a supply-chain program that addresses how to reduce and minimize the potential for food to become contaminated from biological, chemical, or physical hazards and what to do when contamination occurs (i.e., corrective action procedures and product recall plans). As part of the food safety plans, food manufacturers and processors will need to verify that the implemented practices and procedures to control hazards and prevent contamination are indeed working. Verification procedures may involve testing raw or finished product and/or food production areas and equipment for microorganisms. And just like the new rules and standards for farms, announced and unannounced inspections of large food manufacturers and processors will begin next year with compliance deadlines phased in over the next several years for smaller companies.\nJoining the FDA in overseeing the safety of the nation’s food supply on the federal level is the USDA and the CDC. The USDA’s Food Safety and Inspection Service (FSIS) is the public health agency responsible for the safety of meat, poultry and egg products. USDA inspectors are on-site at meat production facilities to ensure that best practices are being implemented to reduce the potential for contamination. The CDC is the federal public health agency in charge of identifying and tracking foodborne illness outbreaks in the U.S. and assists other countries in doing so as well. Other government agencies that are involved in food safety are state and local health department and state agricultural departments.\nSo what do you do if you think you had a case of foodborne illness?\nThe CDC collaborates with the FDA and USDA in foodborne illness outbreak investigations, helping to identify what caused the outbreak and alerting the public when a source is identified.\nAs illustrated in the timelines, it all starts when sick individuals go to the hospital or their doctor or contact their local health departments to report illnesses that they believe occurred from consuming contaminated food. These reports, often accompanied by microbiological testing, are then relayed to state health departments and the CDC. The CDC collaborates with the FDA and USDA in foodborne illness outbreak investigations, helping to identify what caused the outbreak and alerting the public when a source is identified. Methods for connecting foodborne illness cases to one another and the food source have vastly improved over the past decade resulting in increased identification of foodborne illness outbreaks. This phenomenon, not higher risk of illness, is the primary reason you are hearing and reading more about foodborne illness outbreaks.\nThe Bottom Line:\nThe reality is the U.S. has one of the safest food supplies in the world! Food producers in the U.S. are continuously subject to third-party audits to stay certified under particular food safety schemes, to government inspections—both announced and unannounced and to buyer inspections to assure the safety of our food supply."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:8db1e6e5-b15f-4e93-b943-3211ac97b338>","<urn:uuid:fc8bba98-18dd-4ab9-b674-096a0cb71ef6>"],"error":null}
{"question":"In my environmental science research, I need to compare the climate impact multipliers of methane from food waste versus aviation's non-CO2 emissions. What's the data on this?","answer":"Food waste generates methane, which is 28 times more potent as a greenhouse gas than carbon dioxide. For aviation, while it accounts for 2.2% of carbon dioxide emissions in 2005, its total share of global warming was 4.9% that year due to additional climate pollutants besides CO2, showing that both sectors have significant multiplier effects beyond their direct CO2 emissions.","context":["Food waste accounts for eight percent of greenhouse gas admissions. The carbon footprint of U.S. food waste is greater than that of the airline industry.\nFood waste happens in supermarkets, in commercial kitchens, and in our own kitchens. We’ve all purchased fresh food that’s gone bad before we’ve eaten it. We’ve all been spooked by slimy things we find in the back of our refrigerators. Plus there are all those peelings, eggshells, coffee grounds, and other organic trimmings that typically get tossed in the trash.\nWhere does it all go? To landfills, where the food rots and produces huge amounts of methane—a greenhouse gas 28 times as potent as carbon dioxide.\nMore than 300 communities in the U.S. offer curbside composting and dozens more have compost drop-off programs. It’s not nearly enough, and until more community composting becomes available, some of us have to do the dirty work on our own.\nIn my household, we produce multiple wheelbarrows full of dark, rich compost every year, which we feed to our garden beds, young trees, and flower pots. People who want to get started with composting often ask me how I do it, so I will share my secrets.\nFirst, a word upfront: composting might seem like a lot of work, but it doesn’t seem that way to me. I think of it as a lifestyle choice. One of those “do your tiny part to save the world” habits.\nTo get started, you need a small compost bucket for your kitchen. A lot of people keep their bucket under the sink, but we use ours so often (almost every meal prepared) that we keep it on the countertop. It looks as pleasant as any other appliance and is definitely cuter than our toaster.\nEvery morning the coffee grounds go in, and so begins the day. Eggshells and edamame pods, carrot peels and radish ends, onion skins and watermelon rinds. The rotten peach. The moldy end of bread. The banana peel. Basically, every food scrap that isn’t meat or seafood based gets shoved into the container\nWhen the bucket is full, I empty it into a rotating drum we keep outside, not too close to the house. To keep the contents of the drum from getting too mushy, I add dried leaves I collect and bag in the autumn and keep stored throughout the year.\nThe daily routine: empty the bucket, rotate the drum. Repeat, repeat, repeat.\nThen comes the day when the drum is full and needs to be emptied. Look away if you’re the sensitive type. This is the fragrant, icky part, but there’s a good reason rubber gloves exist. And I know you’ve got the grit to handle the muck.\nI pull the compost out by hand and add it to my existing pile that I’ve shaped like a volcano opening. When finished, I quickly cover the new material and add a layer of dried leaves. This prevents the perfume from escaping.\nI don’t touch the pile for a couple of weeks, and then I begin the turning process. Every couple of days I turn the pile with one of my favorite tools, the humble hoe. I have come up with three pile designs: mountain, volcano, and plateau. It’s very entertaining and decent exercise. If the compost is wet and heavy, I add more dried leaves. After a month or six weeks of regular turning and letting nature do its thing, the time has come to harvest the compost.\nI use a screen set over a wheelbarrow, add shovelfuls of compost, and work it through the screen (rubber gloves again), then return the big chunks back to the pile. Now I have a wheelbarrow full of black gold, which I distribute to our appreciative garden.\nIn the winter, the compost pile isn’t active, but I continue to fill the drum and then a garbage can with kitchen scraps. Come spring, I empty it all into the pile and again cover it with leaves.\nToday, I screened compost and fed the new trees we planted on our property. They were smiling in the sun. Then I washed my hands.","Project Drawdown defines the efficient aviation solution as the increased use of technologies to reduce aircraft fuel burn. This solution replaces conventional aircraft with existing global fleet-wide fuel efficiency.\nIn 2005, all of aviation’s share of global warming was 4.9 percent despite being only 2.2 percent of carbon dioxide emissions in that year. This is due to aviation’s generation of other climate pollutants besides carbon dioxide that also cause warming. Considering aviation fuel consumed in 2016, and all global carbon dioxide emissions in 2016, all aviation is estimated to have caused approximately 2.6 percent of manmade carbon dioxide emissions in 2018. Growth in aviation’s share is causing increasing alarm. Airplane fuel efficiency efforts aim to reduce fuel use per passenger-kilometer of air travel. Though freight-only aircraft fuel efficiency is not analyzed here, part of the impact on air freight fuel use is accounted for in the large fraction of total air freight that is carried in the belly of passenger aircraft.\nThere are numerous technologies and operational approaches for reducing airplane fuel use; only the most impactful technologies in use today to improve fuel efficiency were included in this study. Therefore, well-publicized but noncommercial technologies such as aviation biofuels were excluded.\nThis analysis includes the newest, most fuel-efficient aircraft (called “intermediate generation”), as well as the use of fuel efficiency retrofits to existing aircraft. Intermediate generation aircraft are expected to be 15–20 percent more fuel-efficient than earlier models, in part as a result of more fuel-efficient engines, new wingtip devices, and light weighting approaches. Research suggests that the combination of these three technologies in a retrofit would amount to efficiency improvements comparable with a newer aircraft model. In this study, new and retrofitted aircraft are compared to conventional aircraft with the existing global fleetwide fuel efficiency.\nTotal Addressable Market\nThe total addressable market for efficient aviation is measured in terms of total interurban passenger travel by air, projected for every year of analysis (2020–2050), in billion passenger-kilometers. Current adoption was taken as the total passenger-kilometers provided by existing intermediate generation aircraft, in the single-aisle and twin-aisle categories.\nProjected adoption of fuel-efficient aircraft was based on the expected production of intermediate generation aircraft, according to published delivery rates of major suppliers. Delivery rates were assumed fixed for each aircraft type.\nImpacts of increased adoption of efficient aviation from 2020 to 2050 were generated based on two growth scenarios, which were assessed in comparison to a Reference Scenario where the existing fraction of higher-efficiency aircraft remains constant.\n- Scenario 1: Fuel burn is improved by 13 percent, Boeing and Airbus supply aircraft at their published rates, and an additional supplier starts adding comparably efficient aircraft to market. One hundred aircraft are retrofitted annually.\n- Scenario 2: Fuel burn is improved by 18 percent. Aircraft delivery rates, retrofitting, and retirement are similar to the Scenario 1. Global load factors increase to 83 percent (U.S. average).\nEmissions for each scenario were estimated using the fuel emissions factor taken from the Intergovernmental Panel on Climate Change (IPCC) guidelines, and applied to fuel consumption data from the International Council on Clean Transport (ICCT).\nCosts of adopting the intermediate generation aircraft are reported as the additional cost compared to adopting aircraft with average fleet efficiency. For each intermediate generation aircraft, an equivalent conventional aircraft was priced and the price difference was derived. The average difference for single-aisle aircraft was around US$11 million, and that of twin-aisle was US$40 million. Operating costs, which included fuel costs, were derived using historical data from the International Energy Agency (IEA). The solution’s operating costs were reduced by the efficiency improvements noted above.\nTo prevent double-counting, steps were taken to ensure that the total travel demand of all non-urban passenger Transport Sector solutions remained below the projected total non-urban travel demand.\nIn Scenario 1, a potential reduction of 6.3 gigatons of carbon dioxide-equivalent greenhouse gas was found from 2020 to 2050, which corresponds to an 80 percent adoption rate by 2050. Net costs over that time would be US$863 billion above the conventional approach. Efficiency improvements are estimated to bring lifetime operating savings of US$2.5 trillion, however. For the Scenario 2, the emissions avoided amounted to 9.2 gigatons with 85 percent adoption.\nThe use of more efficient aircraft is desirable for airlines in times of higher fuel prices. It would have direct bottom-line impacts, as fuel often represents a third of operating costs. Since 2015, however, fuel prices have been relatively low, and there is no assurance that prices will return to their previous levels of almost three times higher. At these lower fuel prices, the financial attractiveness of these aircraft efficiency improvements is not great, and in some cases result in negative net present values for airlines according to our calculations. However some of this may change with the coming implementation of the Carbon Offsetting and Reduction Scheme for International Aviation (CORSIA) program. To some extent, inclusion of fuel switching approaches such as aviation biofuels can help reduce the need for aircraft technology improvements.\nThere are limitations to this approach. For example, the potential of other technologies being implemented was excluded in this study. Also, the Reference Scenario conservatively assumes fixed fleet efficiency. These limiting assumptions were made to show the impact of existing technologies on the airline industry. The results indicate that airlines have a role to play in the planet reaching the point of drawdown.\n Lee, D. S., Fahey, D. W., Forster, P. M., Newton, P. J., Wit, R. C., Lim, L. L., ... & Sausen, R. (2009). Aviation and global climate change in the 21st century. Atmospheric Environment, 43(22-23), 3520-3537.\n From OECD/IEA (2018) World Energy Balances 2016, OECD/IEA, Paris\n According to Airbus, belly freight is about 52 percent of all air freight.\n Including the 787, 777X, and 737MAX family of Boeing, and the A320neo family, A330neo family, and A350XWB of Airbus.\n Also called “winglets” or “sharklets”, these devices cannot be installed on all older aircraft due to lack of sufficient wing strength and other limitations.\n For more on the Total Addressable Market for the Transport Sector, click the Sector Summary: Transport link below.\n Current adoption is defined as the amount of functional demand supplied by the solution in 2018. This study uses 2014 as the base year due to the availability of global adoption data for all Project Drawdown solutions evaluated.\n Delivery of a single-aisle aircraft is assumed to provide 247 million passenger-kilometers, and a twin-aisle aircraft 840 million passenger-kilometers, of adoption.\n This additional manufacturer can represent any or all of numerous nascent options, such as COMAC of China or the UAC of Russia. It produces around 5 percent of all efficient aircraft annually.\n For instance, the Airbus A320neo was considered a more efficient replacement for the A320, and the Boeing 777X-9 was considered a replacement for the 777-300ER. These relationships were determined through web searches for each efficient model.\n All monetary values are presented in 2014 US$.\n It is assumed that this differential represents the retrofit costs for each aircraft type, and acknowledged that airlines often pay different prices than the list prices due to negotiations that occur with the manufacturers.\n The fuel prices (2007–2018) were averaged, and this fixed average was used for the future projections.\n The net operating savings for the full lifetime of all units installed during 2020–2050.\n The potential for open rotor engines could be large, but estimates seem to indicate availability in the 2030s onward."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ff492c3b-95e0-417d-a8f5-48e8501c4e6e>","<urn:uuid:034d41d9-10c6-4686-b3da-477b79d07814>"],"error":null}
{"question":"How do screen time and mindful eating habits affect attention to food - what's the connection?","answer":"Both screen time and mindful eating significantly impact attention to food consumption. Research from The American Journal of Clinical Nutrition found that people who were distracted by screens while eating consumed more food and reported feeling less full than non-distracted participants. They also had poorer memory of how much they had eaten. Screen use during meals can cause people to 'tune out' and not notice when they're full. Conversely, mindful eating - paying attention to food's taste, smell, and texture - leads to eating more slowly, greater food enjoyment, and feeling satisfied with less food. Experts recommend practicing mindful eating at least once daily and limiting distracting screens during mealtimes.","context":["5 Steps to Healthier Eating\nIs that bag of potato chips your go-to snack? Or do you constantly surrender to your sweet tooth? If so, it’s time to kick the habit. Here are 5 steps to help you stay satisfied while achieving healthier eating habits.\n1. Power Up with Produce\nHealthy habits begin at the grocery store. So once you arrive, head straight to the produce section – it can literally save your life. Research published in The Journal of Nutrition reported that a a diet rich in fruit, berries and vegetables reduced risk of dying. And, a recent study published in the Journal of Epidemiology and Community Health, found that adults who ate 7 or more servings of produce daily were 42% less likely to die from any cause during the 7 ½ year study. In addition, those eating the most produce (7+ servings per day) reduced risk of dying from heart disease by 31% and from cancer by 25 percent. Remember, all forms of produce count toward your daily quota—including fresh, frozen (no added sugar), dried or 100% juice.\nWhen you bring your produce home from the supermarket, immediately wash, slice, and store them on your refrigerator shelves so that they are the first things you see when you open the door. While it’s true you might lose some nutrients when you cut into your produce, that’s more incentive to eat them quickly once prepped. The bright colors and easy access of these power foods will entice your appetite and decrease your chances of reaching for less healthy options.\n2. Pack Your Snack\nSnacking can be smart – it boosts energy levels and wards off hunger between meals. And while a whopping 97 percent of Americans snack daily (up from 71 percent thirty years ago), most people’s snacks are less than healthful. Instead of the typical “junk food” snacks, keep healthy snacks on hand (in your purse, car, gym bag, or office) so you have nutritious foods available when hunger hits.\nTop snacks to pack include some protein and fiber for staying power. Aim for a portion-controlled bag of trail mix, an apple and string cheese, a lowfat yogurt with berries, or a Suja Classic juice and a few almonds. By having healthful snacks on hand you’ll nourish your body and avoid overeating at your next meal.\n3. Eat Aware\n“Eat aware” means to make an effort to become more mindful when you eat—at least once a day. It might seem productive to multi-task to get more done, but when you eat and never pay attention to your food, you’re more likely to feel unsatisfied after your meal, and more likely to eat again later searching for that satisfaction.\nA study published in The American Journal of Clinical Nutrition showed that the presence of the distracting stimuli of playing solitaire on the computer during lunch caused participants to eat more as compared to nondistracted participants. The distracted participants also reported feeling less full and had a poorer memory as to how much they had eaten as compared to the nondistracted participants.\nWhile aiming to “eat aware” has its benefits, it doesn’t mean you have to assume a lotus position or savor the inner essence of every raisin you consume. All it takes is just a few extra minutes a day focusing on your food – its taste, how it smells, and the texture. You’ll find you eat more slowly, enjoy your food more, and are satisfied with less. That’s a good thing.\n4. Out of Sight, Out of Mind\nWhen you have a toddler, you childproof your home. So when you want to eat healthy, why not fat-proof your home? Of course you can’t control every environment, but you can take steps to help yourself. First, if you’re preparing food at home, don’t buy your “trigger” foods. If you want something sweet, go for some frozen grapes. If you love soda, stock your fridge with sparking water and add a splash of 100% juice. The idea is not to create an environment of deprivation, but to replace less healthful foods with better-for-you options you still enjoy.\nAnother safeguarding strategy is based on the old standby, “out of sight, out of mind.” In other words, don’t keep fattening foods where you can see them. If you tend to inhale the bread basket at a sit-down restaurant, ask your waiter not to bring it—or take it away after you have one piece. Remember that the more food that is put in front of you, the more likely you are to eat it—so make an effort to reduce your “food exposure” when you can. More often than not, if you don’t see it you won’t miss it.\n5. Deny Defeat\nHealthy eating habits are for the long term and involve a healthy and happy mindset. A single food or meal doesn’t need to turn into an unhealthy day. Don’t allow nonsupportive thoughts to distract you from everything you have accomplished. Go for a brisk 20-minute walk, take a Yoga class, or simply breath deeply for 5 minutes to rewire your thought patterns back to a healthy mindset.\nOyinlola Oyebode, Vanessa Gordon-Dseagu, Alice Walker, Jennifer S Mindell. Fruit and vegetable consumption and all-cause, cancer and CVD mortality: analysis of Health Survey for England data. J Epidemiol Community Health, 31 March 2014 DOI: 10.1136/jech-2013-203500","New tools, old rules: limit screen-based recreational media at home\nAmerican Heart Association Scientific Advisory\n- Screen time from computers, phones, tablet computers, video games, TV and other screen-based devices is associated with increased sedentary behavior in children and teens. Sedentary behaviors contribute to overweight and obesity.\n- TV viewing has decreased among young people, but overall screen media consumption has increased substantially in this age group.\nEmbargoed until 4 a.m. CT / 5 a.m. ET Mon., Aug. 6. 2018\nDALLAS, Aug. 6, 2018 — Screen time from computers, phones, tablet computers, video games, TV and other screen-based devices is associated with an increased amount of sedentary behavior in children and teens, according to a new scientific statement released by the American Heart Association and published in its journal Circulation.\nSedentary behaviors include sitting, reclining or laying down while awake -- activities which exert little physical energy – and contribute to overweight and obesity in children and teens.\nAmerican Heart Association scientific statements are developed by a panel of experts who review existing scientific literature and evidence to provide an overview of a topic related to cardiovascular disease or stroke. In this review, the writing group found that the available scientific literature is based almost entirely on self-reported screen time, with very few breaking down the type of device or the context in which it is used, which means that the studies are not designed to prove cause and effect.\nThe writing group determined that over the last twenty years, TV viewing by children and adolescents has declined but the recreational use of other screen-based devices, such as smart phones, tablet computers and others has resulted in a net increase in screen time overall. Current estimates are that 8- to 18-year-olds spend more than 7 hours using screens daily.\n“Still, the available evidence is not encouraging: overall screen time seems to be increasing -- if portable devices are allowing for more mobility, this has not reduced overall sedentary time nor risk of obesity,” according to Tracie A. Barnett, Ph.D., a researcher at the INRS-Institut Armand Frappier and Sainte-Justine University Hospital Research Center, in Montreal, Canada, and the chair of the writing group.\n“Although the mechanisms linking screen time to obesity are not entirely clear, there are real concerns that screens influence eating behaviors, possibly because children ‘tune out’ and don’t notice when they are full when eating in front of a screen. There is also evidence that screens are disrupting sleep quality, which can also increase the risk of obesity,” Barnett said.\nThe message to parents and children is to take steps to limit screen time. “We want to reinforce the American Heart Association’s long-standing recommendation for children and teens to get no more than 1-2 hours of recreational screen time daily. Given that most youth already far exceed these limits, it is especially important for parents to be vigilant about their child’s screen time, including phones.” Barnett said.\nRecommended interventions to minimize screen time emphasize the importance of involving parents. Parents can help their children reduce screen time by setting a good example with their own screen use and by establishing screen time regulations.\n“Ideally, screen-based devices should not be in bedrooms, especially because some studies have found that having screen-based devices in the bedroom can affect sleep. Maximize face-to-face interactions and time outdoors,” Barnett said. “In essence: Sit less; play more.”\nAccording to Barnett, more research is needed because the patterns of screen-based media use and their long-term effects on children and teens are not yet known. In addition, the authors report that not much is known about how to help youth be less sedentary and the appeal of screens is making this an even greater challenge. Future research should focus on how to achieve greater balance. Detailed information on the overall impact of today’s sedentary pursuits – especially with respect to screen-based devices – is needed, Barnett said.\nCo-authors are Aaron S. Kelly, Ph.D.; Deborah R. Young, Ph.D.; Cynthia K. Perry, Ph.D.; Charlotte A. Pratt, Ph.D., M.S., R.D.; Nicholas Edwards, M.D., M.P.H.; Goutham Rao, M.D.; and Miram Vos, M.D. M.S.P.H. Author disclosures are on the manuscript.\n- Available multimedia located on the right column of the release link: https://newsroom.heart.org/news/new-tools-old-rules-limit-screen-based-recreational-media-at-home?preview=87a5e3aab8bce40e6d75402c589eac9b\n- After Aug. 6, 2018, view the manuscript online.\n- Healthy Kids\n- Follow AHA/ASA news on Twitter @HeartNews\nThe American Heart Association/American Stroke Association receives funding mostly from individuals. Foundations and corporations donate as well, and fund specific programs and events. Strict policies are enforced to prevent these relationships from influencing the association’s science content. Financial information for the American Heart Association, including a list of contributions from pharmaceutical and device manufacturers and health insurance providers are available at www.heart.org/corporatefunding.\nAbout the American Heart Association\nThe American Heart Association is devoted to saving people from heart disease and stroke – the two leading causes of death in the world. We team with millions of volunteers to fund innovative research, fight for stronger public health policies, and provide lifesaving tools and information to prevent and treat these diseases. The Dallas-based association is the nation’s oldest and largest voluntary organization dedicated to fighting heart disease and stroke. To learn more or to get involved, call 1-800-AHA-USA1, visit heart.org or call any of our offices around the country. Follow us on Facebook and Twitter.\nFor Media Inquiries: 214-706-1173\nDarcy Spitz: 212-878-5940; Darcy.Spitz@heart.org\nFor Public Inquiries: 1-800-AHA-USA1 (242-8721)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2e876ba4-b773-48f9-8478-4911b4853304>","<urn:uuid:4da89069-c646-4037-9659-d799799f22db>"],"error":null}
{"question":"Do both quantum experiments at Delft University and Harvard University involve observing individual atoms?","answer":"No. While Harvard University's quantum gas microscope specifically observes single rubidium atoms in an optical lattice, the Delft University experiment focused on measuring entangled particle pairs separated by 1.3 kilometers, without specifically observing individual atoms.","context":["There were two examples of this from the field of quantum physics this week. Because quantum physics is already weird even without any embellishment or misinterpretation, it's been particularly prone to being co-opted by woo-woos in their search for explanations supporting (choose one or more of the following):\n- psychic abilities\n- the soul\n- \"chakras\" and \"qi\"\nBut you don't need to do any of this to make quantum physics cool. Let's start with an experiment regarding \"quantum entanglement\" -- the linking of two particles in a state describable by a single wave function. While this might seem uninteresting at first, what it implies is that altering the spin state of particle A would instantaneously change the spin state of its entangled partner, particle B -- regardless of how far apart the two were. It's almost as if the two were engaging in faster-than-light communication.\nThere is a further twist on this, and that's where things get even more interesting. Most physicists couple the entanglement phenomenon with the idea of \"local realism\" -- that the two particles' spin must have been pointing in some direction prior to measurement, even if we didn't know what it was. Thus, the two entangled particles might have \"agreed\" (to use an admittedly anthropomorphic term) on what the spin direction would be prior to being separated, simulating communication where there was none, and preserving Einstein's idea that the theories of relativity prohibit faster-than-light communication.\nScientists at Delft University of Technology in the Netherlands have closed that loophole. Using an extremely fast random number generator, they have altered the spin state of one of two entangled particles separated by 1.3 kilometers, and measured the effect on its partner. The distance makes it impossible for sub-light-speed communication between the two. This tosses out the idea of local realism; if the experiment's results hold -- and they certainly seem to be doing so -- the particles were indeed communicating faster than light, something that isn't supposed to be possible. Einstein was so repelled by this idea that he called it \"spooky action at a distance.\"\nTo quote the press release:\nWith the help of ICFO’s quantum random number generators, the Delft experiment gives a nearly perfect disproof of Einstein's world-view, in which \"nothing travels faster than light\" and “God does not play dice.” At least one of these statements must be wrong. The laws that govern the Universe may indeed be a throw of the dice.If this wasn't weird and cool enough, a second experiment performed right here at Cornell University supported one of the weirdest results of quantum theory -- that a system cannot change while you're watching it.\nGraduate students Yogesh Patil and Srivatsan K. Chakram cooled about a billion atoms of rubidium to a fraction of a degree above absolute zero, and suspended them between lasers. Under such conditions, the atoms formed an orderly crystal lattice. But because of an effect called \"quantum tunneling,\" even though the atoms were cold -- and thus nearly motionless -- they could shift positions in the lattice, leading to the result that any given atom could be anywhere in the lattice at any time.\nPatel and Chakram found that you can stop this effect simply by observing the atoms.\nThis is the best experimental verification yet of what's been nicknamed the \"Quantum Zeno effect,\" after the Greek philosopher who said that motion was impossible because anyone moving from Point A to Point B would have to cross half the distance, then half the remaining distance, then half again, and so on ad infinitum -- and thus would never arrive. Motion, Zeno said, was therefore an illusion.\n\"This is the first observation of the Quantum Zeno effect by real space measurement of atomic motion,\" lab director Mukund Vengalattore said. \"Also, due to the high degree of control we've been able to demonstrate in our experiments, we can gradually 'tune' the manner in which we observe these atoms. Using this tuning, we've also been able to demonstrate an effect called 'emergent classicality' in this quantum system.\"\nMyself, I'm not reminded so much of Zeno as I am of another thing that doesn't move while you watch it.\nSee what I mean? You don't need to add all sorts of woo-woo nonsense to this stuff to make it fascinating. It's cool enough on its own.\nOf course, the problem is, understanding it takes some serious effort. Physics is cool, but it's not easy. All of which supports a contention I've had for years; that woo-wooism is, at its heart, based in laziness.\nMe, I'd rather work a little harder and understand reality as it is. Even if it leaves me afraid to blink.","Quantum Gas Microscope Offers Glimpse Of Quirky Ultracold Atoms\nResearch creates a readout system for quantum simulation and computation\nPhysicists at Harvard University have created a quantum gas microscope that can be used to observe single atoms at temperatures so low the particles follow the rules of quantum mechanics, behaving in bizarre ways.\nThe work, published this week in the journal Nature, represents the first time scientists have detected single atoms in a crystalline structure made solely of light, called a Bose Hubbard optical lattice. It’s part of scientists’ efforts to use ultracold quantum gases to understand and develop novel quantum materials.\n“Ultracold atoms in optical lattices can be used as a model to help understand the physics behind superconductivity or quantum magnetism, for example,” says senior author Markus Greiner, an assistant professor of physics at Harvard and an affiliate of the Harvard-MIT Center for Ultracold Atoms. “We expect that our technique, which bridges the gap between earlier microscopic and macroscopic approaches to the study of quantum systems, will help in quantum simulations of condensed matter systems, and also find applications in quantum information processing.”\nThe quantum gas microscope developed by Greiner and his colleagues is a high-resolution device capable of viewing single atoms — in this case, atoms of rubidium — occupying individual, closely spaced lattice sites. The rubidium atoms are cooled to just 5 billionths of a degree above absolute zero (-273 degrees Celsius).\n“At such low temperatures, atoms follow the rules of quantum mechanics, causing them to behave in very unexpected ways,” explains first author Waseem S. Bakr, a graduate student in Harvard’s Department of Physics. “Quantum mechanics allows atoms to quickly tunnel around within the lattice, move around with no resistance, and even be ‘delocalized’ over the entire lattice. With our microscope we can individually observe tens of thousands of atoms working together to perform these amazing feats.”\nIn their paper, Bakr, Greiner, and colleagues present images of single rubidium atoms confined to an optical lattice created through projections of a laser-generated holographic pattern. The neighboring rubidium atoms are just 640 nanometers apart, allowing them to quickly tunnel their way through the lattice.\nConfining a quantum gas — such as a Bose”“Einstein condensate — in such an optically generated lattice creates a system that can be used to model complex phenomena in condensed-matter physics, such as superfluidity. Until now, only the bulk properties of such systems could be studied, but the new microscope’s ability to detect arrays of thousands of single atoms gives scientists what amounts to a new workshop for tinkering with the fundamental properties of matter, making it possible to study these simulated systems in much more detail, and possibly also forming the basis of a single-site readout system for quantum computation.\n“There are many unsolved questions regarding quantum materials, such as high-temperature superconductors that lose all electrical resistance if they are cooled to moderate temperatures,” Greiner says. “We hope this ultracold atom model system can provide answers to some of these important questions, paving the way for creating novel quantum materials with as-yet unknown properties.”\nOn the Net:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:fe598e5d-1a2c-4452-940f-133139471487>","<urn:uuid:4ebdbdf8-ebd4-43e7-a84d-8eb1c0cae4a8>"],"error":null}
{"question":"I've been reading about music history - could you explain how Rick Hall's approach to integration in the 1960s music industry compares to Leo Fender's relationship with musicians in developing instruments?","answer":"Rick Hall and Leo Fender had different but significant approaches to working with musicians. Hall actively broke down racial barriers in the 1960s by integrating his studio, focusing on people's potential rather than their skin color, and churning out hit after hit. He worked directly with diverse recording artists and became known as the 'Godfather of Muscle Shoals Music.' In contrast, Leo Fender, while not performing music himself (he couldn't even tune a guitar), collaborated closely with musicians by showing up at their gigs to swap out equipment. He worked particularly with country music guitarist Bill Carson, who became known as the 'test pilot of the Stratocaster.' Both men prioritized working with musicians to achieve success, though Hall focused on recording and producing while Fender concentrated on instrument development.","context":["MUSCLE SHOALS, Ala. – The world lost a music pioneer on Tuesday. Legendary music writer, producer, and publisher Rick Hall passed away in Colbert County. Hall created FAME Studios which put the “Muscle Shoals Sound” on the international stage.\nThe studio stands out just as much as the music he produced. Rick Hall’s dreams were realized inside the four walls of FAME Studio’s. He made Muscle Shoals “The Hit Recording Capital of the World”.\n“To this day, that sound. Everybody still talks about that sound and wants to be a part of that,” stated Dixie Griffin with the Alabama Music Hall of Fame.\nRick Hall was 85-years-old. Hall was raised in Franklin County the son of a share-cropper. He got his first taste of music when he moved to Illinois as a teen and got hooked.\nAfter serving in the Korean War, hall returned to the Shoals and a factory job.\nBut after some the loss of his father and new bride within weeks, Hall turned to music. His first songwriting success came in the late 50’s with a hit by George Jones.\n“Music comes from the soul, and it came from Rick’s soul,” explained Debbie Wilson with Muscle Shoals Sound Studios. “He had the ear and the heart for what would make something successful and he was very driven to do that, and to do that with others to help them be successful.”\nAfter a failed music studio partnership, Hall founded Florence Alabama Music Enterprises.\nRecording artists from around the world started hearing Hall’s soul and wanted a piece of it.\nEtta James, Wilson Pickett, Clarence Carter, and the Osmond’s all recorded in Studio “A” at Fame. That’s just a tasting of the big names who wanted Rick Hall behind their music.\nAnd his legacy has been passed down to today’s generation of recording artists.\nMusicians from around the world passed on their condolences Tuesday as word spread of Hall’s passing. Names such as Mac McAnally, Jason Isbell, and John Paul White; Shoals artists who learned from the “Godfather of Muscle Shoals Music”.\nEarly in his music career, Hall didn’t mind breaking down barriers. Churning out hit after hit in the 60’s, he didn’t shy away from integrating his studio. He saw the potential in people, not the color of their skin.\nFAME Studios, according to its website, has been involved in the recording or publishing of records that have sold more than 350 million copies over the past 50 years.\n“The songs that they created are classics. They have lasted over the years and they are the best music ever, and all ages listen to that music,” said Dixie Griffin.\nHis friends said Hall always remained humble about his music success.\nHe also leaves behind a huge legacy filled with determination and pride in his work.\n“He had a rough time. He had a lot of adversity in his life, and he really was an inspiration for turning that adversity around into something positive. And he was really an integrating force in the music industry,” Debbie Wilson said.\n“It’s that Muscle Shoals sound,” said Griffin. “It’s just a very humble man that did more than he ever really realized he did I think, yeah he was a great guy.”\nFAME Recording Studios posted a social media note early Tuesday saying “We hope the band in heaven is ready!!! If not, there’s going to be a problem!”\nThe funeral will be held Friday at Highland Park Baptist Church on South Wilson Dam Road. Visitation will begin at noon with services starting at 2:00 p.m.","I love music. I feel its beats in my bones. Even human voices are acapella treats – except those whose mission is spite and hate. Their sour voices sow discord.\nHere’s a discussion of some of my favorite musical instruments: Piano, tuba/sousaphone, flute, and guitar.\nWhen a piano is played, each key controls a hammer that strikes a taut-and-tuned string inside, under its elongated top. The sound resonates from strings, making it a stringed instrument like a harp or a guitar — but the action that produces the sound is a strike, putting it in the percussion category, along with other melodic instruments like the xylophone or steel drum. Thus, pianos are considered to be both a percussion and stringed instrument. Both describe how the piano works. Some consider the piano to be a form of hammered dulcimer, another hammered-string instrument that’s hard to pin down. It’s also a keyboard instrument, which are never just keyboard instruments alone — that would also include the pipe organ (wind), harpsichord (string), glockenspiel (percussion), and synthesizer (electronic).\nThe latter are fundamental to rock ‘n roll bands.\nPianos are large instruments, so they stand like a table on the floor. Another large instrument is a tuba, held, hug-like, in a seated player’s lap.\nIn a Harvard University basement, the King Tuba, possibly commissioned by John Philip Sousa, is about 7 feet tall and 100 pounds. It was restored in 2019 and occasionally makes appearances at performances. John Philip Sousa created the Sousaphone (duh) because he wrote march music and needed the bass instrument to be carried by a member of a marching band. The instrument has been popular for a century, featured high school and college football half-time entertainment. My grandpa played one, while my dad played trombone.\nAt the opposite end of the tonal spectrum of a band is the flute.\nFlutes were among the earliest musical instruments, present in ancient cultures throughout the world. The oldest one that modern archaeologists have discovered so far is 35,000 years old, uncovered in the Hohle Fels Cave near Ulm, Germany. It was created in the middle of the last Ice Age.\nFlutes have also been components of modern rock ‘n roll bands: Jethro Tull, the Moody Blue, and certain incarnations of Jefferson Starship.\nLeo Fender Didn’t Play Guitar\nFender is one of the most popular guitar and amplifier brands, perhaps best known for its enduring classic the Stratocaster electric guitar. The founder’s interest and expertise, however, was solidly in creation, not performance. Before inventing the first mass-produced solid-body electric guitar, Leo Fender was a radio repairman who sometimes tinkered with his musician friends’ instruments. He left the music to Les Paul.\nWhile he briefly played piano and saxophone as a youth, even after decades in the guitar business, he never actually learned how to play the instrument. Legend has it that he couldn’t even tune one. He was too busy tinkering with them: Country music guitarist Bill Carson (who has been dubbed the “test pilot of the Stratocaster”) told Reverb that Fender would show up to his gigs to swap out equipment.\nHere’s one of the most famous Stratocaster creations: “Layla” by Derek and the Dominos, aka Eric Clapton and misc. crew:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d910877a-4f73-46ce-a6d1-8d59c2f795de>","<urn:uuid:8cd76394-7a13-4418-a584-83fbfe11d824>"],"error":null}
{"question":"In the context of software quality management, how do Quality Assurance and Quality Control compare in terms of their responsibility distribution and execution requirements? Please elaborate on who is responsible for each and what their implementation entails.","answer":"Quality Assurance and Quality Control have distinct responsibility distributions and execution requirements. QA involves all people responsible for product development, as it focuses on establishing and maintaining quality processes throughout the development cycle. It may not necessarily involve executing the program or code, but rather focuses on verification-oriented activities. In contrast, QC is primarily the responsibility of the testing team and is conducted after QA activities are completed. QC always requires executing the final program or code and involves specific inspection points with detailed procedures, sampling plans, and quality records. While QA establishes processes that can be applied to all products, QC is specifically product or project-focused, involving concrete testing and review activities to identify defects in the final product.","context":["Quality Assurance vs Quality Control\nQuality is of paramount importance, especially when the consumer has a plethora of options at his or her disposal. When it comes to software, the consumer of today has no time to spare for slow performing, defective or bug riddled applications. Clearly, quality issues are a huge business risk, because of which there has been an increased emphasis on Quality Assurance and Quality Control. However, in our conversations with clients we have often noticed that when discussing product quality and software testing, the terms Quality Assurance (QA) and Quality Control (QC) are used interchangeably. While both QA and QC can be considered two sides of the same coin when it comes to managing quality, they are not one and the same thing. In this blog, we take a look at the main difference between QA and QC and understand the role each plays in the software development and testing process.\nQuality Assurance and Quality Control – What do they mean?\nTo begin with, let’s understand the roles of QA and QC.\nQuality Assurance Definition:\nQuality Assurance is a process that deliberates on ‘preventing’ defects while Quality Control deliberates on ‘identifying’ these defects. Given the adoption of new development methodologies such as agile, QA works towards improving and optimizing the development cycle by performing process audits, establishing process checklists for the project and establishing metrics to identify process gaps and ensure that the product works as per expectation.\nQuality Control Definition:\nQuality Control, on the other hand, focuses on identifying any defects in the product after it has been developed. The QC department is responsible for testing the end product and verifies that there are no discrepancies between the product requirements and its final implementation and that the end product performance is optimal.\nTo put is quite simply, Quality Assurance works on managing the quality of the product being developed and Quality Control validates the quality of the output.\nStrategy and Orientation – Prevention v/s Detection:\nQA is focused on a strategy of prevention and hence is more focused on planning, documenting and setting guidelines for product development to ensure a desired quality of the product. It is because of this that QA activities are undertaken at the inception of the project and have to ensure that software specifications meet the company and industry standards. Designing quality plans, conducting inspections, identifying defect tracking tools and training the responsible team in the defined processes and methods fall within the purview of Quality Assurance. This process is more proactive than reactive since the aim of the QA team is to prevent defects from entering the development cycle in the first place and to mitigate all the risks that have been identified in the specifications phase. Thus all people who are responsible for product development are responsible for QA.\nQC activities are more focused on defect detection and verifying the product output against the desired quality levels. This method is more reactive in nature as it identifies defects post the final production of the product. QC checks are conducted at different points in the development cycle to ensure that the final product meets and performs according to the agreed specifications.\nUnlike QA, which may or may not involve executing the program or code, QC activities will always involve executing the final program or code to identify defects and implement the fixes in order to achieve the desired quality of the product.\nQuality assurance and Quality Control Scope – Process v/s Product:\nThe scope of QA can be said to be more in the process of development rather than on the product itself. The aim of QA is to ensure that the development team is doing the right thing at the right time and in the right way. QA activities are verification oriented, they can be related to all products that will be created using that process.\nThe focus on QC, on the other hand, is only on the product and not necessarily on the development process. QC activities are primarily the responsibility of the testing team and are conducted once the QA activities are completed. QC is a line function that is product or project specific and involves activities such as testing and conducting reviews to identify defects in the final product.\nHaving listed out the key difference between QA and QC, it is also important to note that these two are equally important parts of a quality management system and ultimately have the same goal – to ensure a great quality, optimally performing product. Using Quality Assurance and Quality Control in conjunction with one another and developing a consistent feedback loop can help in identifying the root causes of defects and thereby allow teams to develop strategies that can entirely eliminate these problems at the development stage itself and achieve high-quality products.","What is Quality Control?\nWhat is quality control? Quality control are the systems in your\ncompany that detects defects. Quality control prevents customers (internal and external) from receiving defective products. This\nincludes inspection points for receiving inspection, inprocess inspection\nand final inspection. When establishing these systems you need to consider these items\n- The procedure for inspection\n- The sampling plan for inspection\n- The quality record for inspection\n- The accept / reject criteria or specification.\n- The plan for handling and identifying the defective material\n- The review and corrective action of the defective material (MRB)\nReceiving inspection are the controls you use to prevent defective materials from entering your production lines. Your company decides which materials are critical to your process. Critical materials directly affect product quality. These materials must be controlled.\nConsider these questions when creating your receiving inspection procedure.\n- How are the materials inspected?\n- What is the inspection procedure?\n- What is the sampling plan?\n- Where is the inspection documented?\n- How is the accepted material distinguished differently from the rejected material or unqualified material?\n- What happens to rejected material?\n- What is the process of notifying the supplier of rejected material?\n- How do you prevent good and rejected material from being mixed?\n- What is the staging process for unqualified material?\nYou might have inprocess inspection in various departments in your company\nas production is flowing through those departments. This could include\n- Inspecting machine settings,\n- Inspecting machine output\n- Inspecting the incoming material\n- Inspecting the outgoing material\n- Statistical Process Control\nWhen establishing an inprocess inspection stage consider these items\n- Do you need the inspection to ensure the quality of the part?\n- Do you have a operating procedure for the inspection tool?\n- Do you have specifications for the inspection?\n- Have you consider SPC for the inspection?\n- Have you determined the sampling plan?\n- Is there a quality record for the inspection?\n- Is someone reviewing the quality record?\n- Is there a Material Review Board process if issues are found?\n- Who is notified when issues are found?\n- Is production stopped when defects are found?\nFinal inspection should occur on the product prior to shipping to the customer or prior to storing in inventory. When implementing final inspection activities, review the above inprocess inspection information.\nFinal inspection is more dependent on customer requirements. The customer may have a specification or a drawing. Perhaps you are selling to a catalog or web page. Final inspection assures the customer is receiving the items they order.\nFinal inspection occurs on the product. If defects are found, then the product may need to be reworked or scrapped. Finding defects at this time can cost your company serious money because the process that caused the defect may still be making the same defect on fresh product. In addition all value added steps have been placed in the product. All money spent on creating that product will be wasted if the product is scrapped.\nAs a preventive measure, final inspection should review the inprocess inspection documentation to assure that the data is in order.\nDocumentation for final inspection should be formal as the customer may ask for the data. Design The quality record so it meets your customer requirements and formats. If the customer is asking for the data, be sure to get their acceptance to the format prior to sending the initial data to the customer.\nWhat is Quality Control inspection types?\nMechanical - These are dimensional measurements of the item\nPerformance - This is an inspection for the function(s) of the product.\nVisual - Looking for certain undesirable characteristics. These characteristics need to be defined\nWhat is quality control definition?\nThe operational techniques and\nindividual activities that focus on controlling or regulating processes\nand materials to fulfill requirements for quality. The focus is on\npreventing defective products or services from being passed on.\nAfter reviewing What is Quality Control, see here for the difference between Quality Assurance and Quality Control.\nQuality Assurance Solutions"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:4738af23-b693-4e84-baba-e354aa0886d4>","<urn:uuid:e06eed6f-c212-4c5b-8212-79b77d3ed478>"],"error":null}
{"question":"What research centers in California conduct major clinical trials and what types of trials do they perform?","answer":"There are several major research centers in California conducting clinical trials. The University of California Los Angeles (UCLA) has one of the largest medical research institutes in the state, conducting trials across California. Kaiser Permanente, with over 6,000 physicians in southern California, conducts trials in almost every specialty, funded by pharmaceutical companies, government, and foundations. The Cedars-Sinai Medical Center in Los Angeles, one of the largest private hospital research centers in the US, focuses on biomedical, cancer, and heart research, offering trials for both healthy and non-healthy volunteers.","context":["California Clinical Trials\nA listing of California clinical trials actively recruiting patient volunteers.\nIn California there are several research institutes and hospitals that conduct clinical trials. The University of California Los Angeles (UCLA) has one of the largest medical research institutes in the state and conducts clinical trials not only in Los Angeles but all across California. The larger cities in California, such as Los Angeles, San Francisco, San Diego, San Jose, and Sacramento all have ongoing clinical trials dedicated to a variety of conditions, including both paid and non-paid clinical trials.\nMatch to Clinical Trials\nMatch to Clinical Trials\nA Study Evaluating the Efficacy, Safety, Pharmacokinetics and Pharmacodynamics of Crovalimab in Adult and Adolescent Participants With Atypical Hemolytic Uremic Syndrome (aHUS)\nThis study aims to evaluate the efficacy and safety of crovalimab in adult and adolescent participants with aHUS.\nTrilaciclib, a CDK 4/6 Inhibitor, in Patients Receiving Docetaxel for Metastatic Non-Small Cell Lung Cancer (NSCLC) (PRESERVE 4)\nThis is a randomized, double-blind, placebo-controlled, global, multicenter, Phase 2 trial evaluating the effect of trilaciclib on overall survival when administered prior to docetaxel in patients with mNSCLC treated in the 2nd or 3rd line setting.\nMedia Project and Health Study - Spanish Version\nWe seek to test the efficacy of the Spanish version of the \"Catalina\" web-app intervention compared to an attention control web-app in reducing symptoms of depression and/or anxiety and motivate women to take action to get help.\nCapivasertib + Palbociclib + Fulvestrant for HR+/HER2- Advanced Breast Cancer (CAPItello-292).\nA Phase Ib/III Randomised Study of Capivasertib plus Palbociclib and Fulvestrant versus Placebo plus Palbociclib and Fulvestrant in Hormone Receptor-Positive and Human Epidermal Growth Factor Receptor 2-Negative Locally Advanced, Unresectable or Metastatic Breast Cancer (CAPItello-292).\nClinical Comparison of Two Daily Disposable Contact Lenses\nThe purpose of this study is to compare the clinical performance of PRECISION1™ contact lenses to Clariti® 1 day contact lenses.\nA Research Study to Find Out How Semaglutide Works in the Kidneys Compared to Placebo, in People With Type 2 Diabetes and Chronic Kidney Disease (the REMODEL Trial)\nWe are doing this study to learn more about how semaglutide may help fight chronic kidney disease in people with type 2 diabetes. We are doing this by looking into how semaglutide works in the kidneys. Participants will either get semaglutide or placebo (a 'dummy' medicine) - which treatment participants get is decided by chance. Semaglutide is a medicine doctors can prescribe in some countries for the treatment of type 2 diabetes. Participants will get the study medicine in a pen. Participan ...\nFeasibility of Teleyoga for Treatment of Lyme Disease\nPrimary Aims: Modify an existing teleyoga intervention to use with LD patients and address the technical challenges of at-home teleyoga\nBotanical Blend on the Gut Microbiome and Gut-Skin-Axis in Small Intestinal Bacterial Overgrowth (SIBO)\nThe purpose of this study is to assess how an oral botanical blend alters the gut microbiome and the skin biophysical properties in people with SIBO.\nA Study of LY3437943 in Participants With Type 2 Diabetes\nThe main purpose of this study is to evaluate the efficacy and safety of LY3437943 in participants with type 2 diabetes (T2D) who failed to achieve adequate glycemic control on diet and exercise alone or on a stable dose of metformin. This study will last about 43 weeks.\nA Phase 1/2 Study Evaluating MCLA-129, a Human Anti-EGFR and Anti-c-MET Bispecific Antibody, in Patients With Advanced NSCLC and Other Solid Tumors\nA phase 1/2 open-label multicenter study will be performed with an initial dose escalation part to determine the MTD and/or the RP2D of MCLA-129 as monotherapy in patients with NSCLC, or HNSCC or other solid tumors and who have progressed after receiving prior therapy for advanced/metastatic disease.\nPost-Marketing Study of Prucalopride Safety In Pregnancy\nThis study collects information on pregnant women with ongoing constipation who took prucalopride and those who did not take prucalopride. The main aim of the study is to learn if any medical problems in pregnant women or their infants might be related to taking prucalopride during pregnancy. Participants are not required to take prucalopride during the study. Participation begins after a pregnant woman has taken prucalopride. Women and their infants are followed during pregnancy and for 1 year ...\nProxalutamide (GT0918) Treatment for Outpatients With Mild or Moderate COVID-19 Illness\nThe purpose of this study is to assess the efficacy and safety of Proxalutamide (GT0918) as a treatment for outpatients COVID-19 subjects.\nKaiser Permanente houses over 6,000 physicians in southern California and it is also one of the largest research centers on the state. On its clinical trial division, Kaiser Permanente conducts clinical trials of almost every specialty funded by pharmaceutical companies, government, and foundations.\nThe Cedars-Sinai Medical Center located in Los Angeles is one of the largest research centers owned and operated by a private hospital in the United States. Focusing on biomedical, cancer, and heart research among other specialties, you can find almost any type of clinical trials for both health and non-healthy volunteers at the research division of Cedar-Sinai Medical Center.\nThere are around 100,000 clinical trials happening every year across the United States about almost every condition out there. While most clinical trials are testing drugs or medical devices there are also many behavioral and observational trials going on at any point in time. Many paid research studies are held at these California locations.\nClinical trials are incredibly important for making new medical and scientific discoveries, finding new and better methods for diagnosing medical conditions early, testing new interventions, and even developing safer and more effective surgical procedures.\nIn order for all these discoveries to me made available to the general population they need to go through years of rigorous testing both in healthy and non-healthy volunteers. During those years, medical professionals follow a protocol that has been previously approved by the FDA or other regulatory agency to test the effectiveness and safety of their discovery.\nOther people choose to participate in a clinical trial because they feel it is a good way to contribute to the advancement of medical knowledge and that their participation can help many people in the future. The following lists of trials are broken down by condition, and to find a specific clinical trial in your area click through to clinicaltrials.gov."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:981398e4-324f-42e9-bff6-2bd783781797>"],"error":null}