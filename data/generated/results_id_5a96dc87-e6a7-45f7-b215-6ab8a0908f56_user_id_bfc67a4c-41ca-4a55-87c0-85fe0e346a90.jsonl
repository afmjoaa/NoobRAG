{"question":"How does the diffusion of innovations theory relate to bridging the digital divide, and what social implications arise from technology adoption patterns?","answer":"The diffusion of innovations theory and digital divide are closely connected through adoption patterns and social implications. According to diffusion theory, innovations spread through social systems based on factors like socioeconomic status, education, and communication channels, with early adopters often being well-connected individuals while late adopters are typically disadvantaged or marginalized. This pattern directly relates to the digital divide, where underserved populations like low-income individuals and rural communities face barriers to technology adoption. The social implications include limited access to education, employment, and healthcare opportunities for those who adopt late or cannot adopt at all. The theory explains how communication channels and social structures influence adoption, while the digital divide demonstrates how these factors create and perpetuate inequality in access to digital resources.","context":["Diffusion of innovations, model that attempts to describe how novel products, practices, or ideas are adopted by members of a social system. The theory of diffusion of innovations originated in the first half of the 20th century and was later popularized by American sociologist Everett M. Rogers in his book Diffusion of Innovations, first published in 1962.\nKey elements of the theory include the innovation, the communication processes and channels of communication, the passage of time, the potential adopters, and the social system, all of which influence whether or not an innovation with be taken up by a given group. The innovation (the product, practice, or idea) need not be a new invention but must be perceived as new by individuals or other units of adoption. Its characteristics, such as relative advantage, compatibility, malleability, and complexity, influence the likelihood that it will be adopted. Communication is essential for the diffusion and subsequent acceptance or rejection of an innovation, which can alter the structure and function of a social system. Change is measured by the numbers of people, groups, or institutions adopting the innovation over time. Consequently, the key variables of interest are time (earliness of knowing about innovation), rate (adoption of different innovations in a social system or within and among different social groups), and innovativeness (the degree to which groups or organizations adopt new ideas). Analyses can establish the flow of influence, offer charts of the diffusion curve, develop mathematical models of the diffusion process, and test out contributions of key elements and characteristics.\nDevelopment of the theory\nThe diffusion model developed through an interest in social transformation and explorations of the consequences of the development, spread, and adoption or rejection of new products, activities, and ideas. For example, anthropologists studied the introduction of the horse within and among indigenous populations of North America, the spread and modification of dance ceremonies among Native American groups, and the spread of corn (maize) cultivation from America to Europe. Early sociological studies included the examination of social and legal trends, such as the influence of a city on surrounding areas, the diffusion of governing practices, and the use and consequences of technology. Rural sociologists focused on the spread of new ideas among farmers and subsequent changes in agricultural practices.\nLessons learned from diffusion studies in anthropology, sociology, education, folklore, communication, marketing, economics, and public health have helped contemporary scholars and practitioners transform the diffusion of innovation model from a descriptive model into a proscriptive one. Today, diffusion theory commonly is used as an analytic framework for understanding and measuring social change and, in practical application, to guide the design and evaluation of products, programs, and communication strategies.\nCharacteristics of innovations\nInnovations that are perceived to be of lower social or economic costs, that provide a good fit with values and current practices, and that are of low complexity are more likely to be adopted than those carrying high costs, representing a variance with common values, and appearing to be difficult to understand, to communicate, or to use. Those innovations flexible enough to withstand some change to provide a better fit with prevailing practices and cultures hold more appeal. This flexibility, sometimes labeled a reinvention, involves change in some of the characteristics of the innovation to increase compatibility of the innovation with the existing social system. It is also of importance whether or not potential users can observe as well as try the innovation without undue sacrifice or commitment.\nThe role of communication and time\nCommunication channels and the agents of change affect the diffusion process as well. Communication channels include face-to-face communication and mass communication, while agents of change are those individuals who bring innovations to members of a social system. Agents of change may be members of the community or individuals outside the social structure of the community. Diffusion analysis must consider who talks to whom, who is considered influential and trustworthy, and who has easy access to or is barred from various communication channels. Characteristics of the potential adopters are of critical concern. Overall, factors such as socioeconomic status, culture, gender, race, age, cultural norms, religion, education, social support, and family ties all influence access to and perceptions of the innovation.\nA vital aspect of the diffusion model, and one closely linked to an analysis of adopters, is the consideration of time. The population is often divided into groups based on the time it takes for people to adopt the innovation. The groups are innovators, early adopters, early majority, late majority, and late adopters. Innovators, for example, are often viewed as creative but marginal individuals. Early adopters, close to sources of communication, are often highly integrated into the social system. They often carry a high degree of opinion leadership, are respected by their peers, and serve as role models for others. It is only when these respected members of a community consider, discuss, and adopt the innovation that wide diffusion takes place. Those in the early majority generally interact frequently with peers and are exposed to various sources of information. Those in the late majority are people who are further removed from key communication channels or who remain skeptical and adopt only after pressure from their peers or out of economic necessity.\nIn older articulations of the model, those who are introduced to the innovation late and who adopt late were called laggards. Often, people who fall into this category are distant, disadvantaged, or marginalized. These members of the population are often further removed from key channels of communication than are those who are able to learn about and adopt the innovation early. They are also more likely to lack resources, including time and money, to take chances. Often, they are socially isolated.\nSocial systems and consequences of adoption\nDiffusion occurs within a social system. The analysis of the diffusion process considers the members or units of a social system, including individuals, groups, organizations, or subsystems. The social system includes structural, political, economic, as well as geographic characteristics. The structure may be considered the patterned arrangements of the various units, such as the formal hierarchical structure of a bureaucratic organization with formal laws and rules. Norms may be just as powerful in other, less formal groups. Social factors also consider how decisions are made and whether or not people have an array of options or, at minimum, the freedom to adopt or reject the proposed innovation. Decisions may be individually based, be communal (arrived at through consensus), or be mandated (made through the imposition of authority). Those designing public health safety programs, for example, see initial strong adoption of practices such as seat belt use with the passage of laws (an authority-based decision) but diminished use when consensus is not considered and if enforcement is lacking. In general, the structure of a social system can facilitate or impede diffusion of innovations and thereby influence the rate of adoption of the innovation over time.\nIn the last stage of diffusion studies, consequences of innovations are documented and analyzed. Three classifications of consequences are often considered: direct or indirect consequences, anticipated versus unanticipated consequences, and desirable or undesirable consequences.","Yes, digital divide is indeed a social issue as it exacerbates inequality and hinders access to opportunities and resources for marginalized individuals and communities. In today’s increasingly digital world, access to affordable internet, technology, and digital literacy skills are essential for participating in education, employment, healthcare, and social engagement.\nHowever, not everyone has equal access to these resources, creating a gap between those who have and those who do not. This digital divide disproportionately affects underserved populations such as low-income individuals, people with disabilities, rural communities, and elderly individuals, widening the existing social inequalities.\nBridging the digital divide is crucial for promoting social inclusion, equal opportunities, and overall societal progress.\nTable of Contents\nExploring The Impact Of Digital Divide On Society\nDigital divide is a prominent social issue that has a deep impact on society. It refers to the unequal access and usage of digital technologies and the internet among different groups and individuals. This divide is often seen as an indicator of social inequality, as it reflects disparities in socio-economic status, education, and opportunities.\nDigital skills play a crucial role in bridging this divide and reducing social exclusion. People who lack digital skills are more likely to be left behind in the digital era, limiting their access to information, services, and opportunities. Therefore, addressing the digital divide is essential to ensure equal participation and empowerment for all members of society.\nIt requires efforts to provide equitable access to digital technologies, improve digital literacy, and create an inclusive digital environment that benefits everyone.\nUnderstanding The Factors Contributing To The Digital Divide\nThe digital divide, a significant social issue, stems from economic barriers and limited access to technology. Affordability hinders equal participation. Lack of access to technology exacerbates the divide. Inadequate education and digital literacy further perpetuate the problem. Skills and knowledge are vital to bridging the gap.\nAdditionally, geographic location plays a role in internet access disparities. Urban areas usually have better connectivity compared to rural regions. Addressing these factors is crucial in reducing the digital divide, promoting inclusivity, and ensuring equal opportunities for all. Technology accessibility should be affordable, education must focus on digital literacy, and efforts should be made to improve internet connectivity in rural areas.\nclass=”wp-block-heading”>Examining The Social Implications Of The Digital Divide\nThe digital divide poses numerous social implications that are worth exploring. One notable impact is the effect on job prospects and economic opportunities. Limited access to digital resources can hinder individuals from acquiring the necessary skills for employment in an increasingly digitalized world.\nMoreover, the digital divide can exacerbate existing educational inequalities, as those without internet access may struggle to excel in academics. Access to online healthcare and information is also a concern, as a lack of digital connectivity may limit individuals’ ability to receive necessary medical advice and information.\nBridging the digital divide is crucial in order to create a more equitable society, where everyone has equal access to opportunities in the digital age.\nAddressing The Challenges Of The Digital Divide\nThe digital divide is a pressing social issue that needs to be addressed urgently. Governments can play a crucial role in bridging this gap by implementing effective policy changes. By bringing about the necessary reforms and ensuring equitable access to technology, governments can empower communities and individuals.\nNon-profit organizations and community networks also play a vital role in narrowing the digital divide through grassroots efforts. These organizations work at the ground level to provide access, training, and resources to underserved communities. Additionally, public-private partnerships have emerged as a collaborative approach towards a connected future.\nCollaboration between government bodies, private enterprises, and non-profit organizations can go a long way in ensuring that no one is left behind in the digital revolution. By adopting a comprehensive and inclusive strategy, society can work towards closing the digital divide and creating a more equitable and connected world.\nCase Studies: Success Stories And Lessons Learned\nThe digital divide is undoubtedly a pressing social issue that needs to be addressed urgently. Examining successful case studies provides us with valuable insights and lessons that can guide our future initiatives. Various countries have shown commendable efforts in bridging the digital divide and serving as positive examples.\nFrom providing affordable internet access to remote areas to implementing comprehensive digital literacy programs, these countries have made immense progress. Furthermore, local communities and organizations have played a crucial role in introducing promising initiatives aimed at reducing the divide. By actively engaging with disadvantaged communities and empowering individuals with digital skills, these initiatives have yielded positive outcomes.\nIt is essential that we learn from these success stories, replicate effective strategies, and continue to innovate in order to ensure equal access to the digital world for all.\nConclusion: The Digital Divide As A Social Challenge\nThe digital divide is undeniably a pressing social issue that requires collective efforts to close. The importance of digital inclusion cannot be underestimated in addressing this challenge. Recognizing it as a social issue is crucial, as it affects individuals and communities, hindering their access to information, education, and job opportunities.\nBridging the digital divide requires a comprehensive approach that involves government initiatives, private sector involvement, and community-driven solutions. Creating awareness about the digital divide and its impact on society is essential in mobilizing these collective efforts. Additionally, proactive measures, such as providing affordable internet access, digital skills training, and access to digital devices, can empower individuals and promote social equity.\nBy recognizing the digital divide as a social issue and working together, we can strive towards a more inclusive and equitable digital future for all.\nFrequently Asked Questions\nIs The Digital Divide A Social Issue?\nYes, the digital divide is a social issue as it creates unequal access to technology and information, impacting education, employment, and economic opportunities for marginalized communities. Lack of digital skills and resources can perpetuate social disparities and hinder social progress.\nIt is evident that the digital divide is indeed a social issue that needs to be addressed urgently. The disparities in access to technology directly contribute to unequal opportunities, hindering the progress of disadvantaged individuals and communities. Bridging this divide is crucial for ensuring equal access to education, employment, and various social services.\nGovernments, ngos, and tech companies must work together to implement initiatives that increase internet access and digital literacy, particularly in underprivileged areas. Additionally, promoting affordable internet plans and providing computer resources can help eliminate barriers and empower marginalized groups. Moreover, addressing the digital divide goes beyond just ensuring access; it requires creating an inclusive digital infrastructure where everyone can participate and benefit from the digital world.\nBy acknowledging the social implications of the digital divide and taking concerted actions, we can work towards a more equitable and inclusive society for all."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f613ea5f-9590-4de2-959b-a671f4d5da92>","<urn:uuid:21c38527-cf92-45a9-86e7-d6e39c66f165>"],"error":null}
{"question":"What role does technology play in both global diplomacy and personal stress management, and what are the potential drawbacks in both contexts?","answer":"In diplomacy, modern technology has contributed to the '24-hour news cycle' that affects how global summits operate, making private negotiations more difficult and increasing the emphasis on public posturing. Similar to diplomatic challenges, in personal contexts, technology use has shown concerning patterns - teens are using media for 9 hours daily, with 58% using screens for stress relief. However, this technological engagement has downsides in both spheres: in diplomacy, it can hinder effective negotiation, while for individuals, it can lead to cyberbullying (affecting 23% of teens), 'Facebook depression,' sleep disruption, and reduced family communication.","context":["So much emphasis about the global summit at Copenhagen has been on the substance-climate change-that the format of the meeting has been overlooked. Yet in diplomacy, format is central. The messy, almost unmanageable configuration of the Copenhagen summit is here to stay. We need to learn to make the most of it.\nDespite the aim of solving a problem facing the entire world, the summit’s composition best resembles a hodgepodge of national-or what in the United States are called domestic-interest groups. To some extent this is true of any international gathering. But in this case, the aim of the summit has been undercut by the lack of a fundamental consensus on the rules, norms and style of “global” diplomacy.\nDiplomatic summits can’t bring results if those present don’t share a common language or a common tradition, no matter how pressing the issue. They end up talking past each other.\nWith so many players at the table, summit meetings like this one have come to resemble rowdy legislatures with their emphasis on posturing for the home team. Whatever deals are struck usually happen in quiet conversations on the sidelines. But the size, complexity and tempo of such meetings tend to make these conversations extremely difficult to have in real time.\nSome people have argued that, like it or not, global summits of this sort are an inevitable sign of the times. Globalization, they say, demands nothing less. Gone are the days of private games of chess among statesmen. We have finally adopted, in other words, what Woodrow Wilson once heralded as the New Diplomacy: open covenants openly arrived at-in the full glare of the 24-hour news cycle.\nThe New Diplomacy was tried back in Wilson’s day, or soon after. The most notable attempts were the series of disarmament conferences sponsored by the League of Nations in the 1930s which followed a decade of similar naval arms talks. There were also conferences on subjects ranging from monetary policy to the narcotics trade. Nearly all of these efforts, like the League itself, were doomed by national-or, more precisely, nationalist-passions and interests.\nOf course, none of this is new. Diplomatic conferences, once called congresses, date back to the earliest days of modern diplomacy in the fifteenth century. Comparable events took place even earlier, as any reader of Thucydides or Xenophon will know.\nThe term “summit,” however, is largely a Cold War invention. It was applied bilaterally: the leaders of the Soviet Union and the United States would parley from their respective heights of superpowerdom to manage both their relationship and the rest of the world.\nIt didn’t take very long, however, for the term to migrate. By the late 1970s, the meetings of the more developed countries-then the G-5, later to become G-7, G-8 and now G-20-were called summits and even had “sherpas,” specially designated officials in each government who, like their Himalayan namesakes, played the role of scouts and porters. They prepared agendas, coordinated policies and carried institutional memories. Such people are essential. If summits like the one at Copenhagen are going to succeed, they will need a professional cadre of global sherpas whose relationships with one another deepen over time.\nIn the meantime, summits will continue to proliferate. They now cover issues from conventional arms control, the environment, gender discrimination and drug trafficking. Today there are, at least in the United States, everything from jobs summits to AIDS summits to, presumably, summits about summits. Many end up being talking shops, producing little in the way of results.\nThis is in part because most of the delegates to such meetings are not professional diplomats but rather cabinet officers, politicians and “special envoys” whose primary, domestic constituencies may be the least amenable to the necessary tradeoffs involved in any diplomatic negotiation.\nSo we shouldn’t be surprised to have seen so much parochialism at Copenhagen. For all that international relations have matured over the past several decades with the spread of a global consciousness, some things just don’t change.\nKenneth Weisbrode is a historian at the European University Institute in Fiesole, Italy, and the author of The Atlantic Century (2009). He is a writer for the History News Service.\nReprinted with permission from the History News Service.Click here for reuse options!\nCopyright 2009 LA Progressive","Hansa Bhargava MD FAAP\nStaff Physician, Children's Healthcare of Atlanta\nMedical Editor, WebMD\nPediatricians are seeing more and more teens suffering from stress. Whether they are complaining of it or having somatic symptoms such as headaches and stomach aches, it seems that stress and anxiety are on the rise. We know that over scheduling, homework, and the pressures of getting into college can contribute to this. But can media also affect it? Is screen time and media a stressor or a remedy for stress?\nIn a recent WebMD survey published in their Teens and Stress report, 54% of teens were stressed according to parents. Interestingly, 40% of parents turned to the screen for family stress relief while 58% of teens did. Social media and texting was used as stress relief by almost half the teens. This is on the heels of the Common Sense Media survey reporting that US teens were using media for 9 hours a day. Other recent reports have shown that 94% of teens with mobile devices are online daily with many online constantly.\nSo it seems that stress is on the rise and media use is on the rise. Although there may not be a direct relationship, some real issues impact stress and anxiety. Consider this: 23 % of teens report cyberbullying, especially girls. There have been reports of “Facebook depression” and loneliness, as kids who aren’t in social media conversations may feel left out. Other negative consequences can also have an impact: many teens are in front of a screen late at night or ‘sleep text’, both of which can contribute to lack of sleep, which in turn can decrease focus and potentially cause irritability and depression. And last but certainly not least, what about the time media consumes?\nTime spent on media is time often not spent communicating with family. Lately, when I’ve gone into a restaurant, I’ve observed that as soon as a family sits down, everyone pulls out a mobile device. No one is really talking. So even the short amount of time not doing homework, playing soccer, or at school is being compromised. Psychologists, community leaders and experts have long reported that family time can contribute to less depression, less anxiety, better academic performance and generally happier kids. But what if that family time is on media??\nAs the AAP reviews our screen time recommendations, I feel that we, as pediatricians should continue to advise parents about basic principles.\nParents need to lay down some parameters about when and how media is used. Media is a centerpiece of teens’ lives and is not going away, but just as we don’t give our kids a set of keys to our car and say “just drive”, we need to enforce appropriate media use. And good modeling is also critical: parents need to put down their mobile devices and simply communicate with their kids. Old fashioned parenting and just talking to your kids can build the foundation to a less stressful childhood and hopefully a happier life."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:405e5f26-62cd-4dc9-89c3-26086ae7b6a0>","<urn:uuid:9ee5a44c-ac0e-4471-bc2a-f3b31e423c46>"],"error":null}
{"question":"In English: How were the archaeological sites of Pompeii and Mohenjo-daro threatened by human activities? 用中文：这两个考古遗址面临什么人为破坏？","answer":"Both sites faced serious threats from human activities. In Pompeii, tomb raiders created a 60-meter long network of narrow tunnels (60cm wide) under a Roman villa, damaging the site while searching for frescoes and precious artifacts. At Mohenjo-daro, the site was threatened in 2014 when it was used for the Sindh Festival's inauguration ceremony, which would have involved mechanical operations, excavation, and drilling - activities banned under the Antiquity Act. Despite protests from national and international historians and educators, the festival was still held at the historic site.","context":["Donkeys, pigs, and dogs have all been found amongst the ruins of Pompeii, but the remains of a carbonized horse are the first example archaeologists have come across of that animal. While the discovery is great, the way it was found is unsettling. T\nhe Local.it reports the horse was found in a stable, complete with a trough, beside a large Roman villa. Unfortunately, archaeologists were not the first to make the discovery – tomb raiders are responsible for unearthing the horse. Nonetheless, Massimo Osanna, the director of the Pompeii site, calls the horse an \"extraordinary\" find.\nAuthorities found the looters had dug a 60 meter (196.85 ft.) long network of tunnels under the villa, to search for frescoes and other precious artifacts. Laser scanners show the tunnels measure just 60 cm (23.62 inches) wide, according to Independent.ie. Steps have been taken to find the looters and archaeologists have begun excavating the area properly to try to avoid further destruction.\nTraces of an iron and bronze harness were located beside the horse’s head, which archaeologists believe suggests the animal was probably a parade horse that was specially bred to fulfill that action and very expensive. The Telegraph mentions there is also the possibility that the animal was a prized racing horse.\nThe remains of the horse were uncovered by looters. (Antonio Ferrara and Riccardo Siano )\nThe recently discovered horse measures 150 centimeters (59.06 inches) tall at the withers, somewhat short if compared to a modern horse, but experts say it would have been a rather large adult horse in ancient Pompeii. It was carbonized following the eruption of Mount Vesuvius in 79 AD and no skeleton or flesh remains on its body. However, the form has been preserved through a technique which experts have been using to preserve casts of Pompeii’s human victims . The procedure involves injecting the empty body cavity with liquid plaster.\nThe Local.it says this is the first time archaeologists have found the complete outline of a horse at Pompeii. Experts were able to distinguish it as a horse, as opposed to a donkey, because of the left ear imprint which marks the ground under the animal’s head.\nThe imprint of the horse's left ear. ( Parco Archeologico di Pompei )\nA tomb dating to a later period was also found at the villa. It contained a man who died when he was 40-55 years old and Osanna says , \"It shows that even after the eruption, people continued to live and to farm in Pompeii, on top of the layer of ash which destroyed the city.\" Amphora shards, fragments of kitchen utensils, and part of a wooden bed were also found during excavations.\nThis is the second major discovery to be reported from Pompeii in the last few weeks. On April 25, Osanna announced that archaeologists had found the skeleton of a child who died during Vesuvius’ eruption . The seven or eight-year-old sought shelter from the volcanic ash, gas, and pumice by crouching inside a public thermal bath.\nThe child’s skeleton was found in a crouching position in the bath complex of the town. ( Parco Archeologico de Pompeii )\nTop Image: The remains of an ancient Roman horse have been found in Pompeii. Source: Parco Archeologico di Pompei\nBy Alicia McDermott","Mohenjo-daro is an archaeological site in the province of Sindh, Pakistan. Built around 2500 BCE, it was one of the largest settlements of the ancient Indus Valley Civilisation, and one of the world's earliest major cities, contemporaneous with the civilizations of ancient Egypt, Mesopotamia, Minoan Crete, and Norte Chico. Mohenjo-daro was abandoned in the 19th century BCE as the Indus Valley Civilization declined, and the site was not rediscovered until the 1920s. Significant excavation has since been conducted at the site of the city, which was designated a UNESCO World Heritage Site in 1980. The site is currently threatened by erosion and improper restoration.\nThe city's original name is unknown. Based on his analysis of a Mohenjo-daro seal, Iravatham Mahadevan speculates that the city's ancient name could have been Kukkutarma, the city of the cockerel [kukkuta]. Cock-fighting may have had ritual and religious significance for the city, with domesticated chickens bred there for sacred purposes, rather than as a food source. Mohenjo-daro may also have been a point of diffusion for the eventual worldwide domestication of chickens.\nMohenjo-daro is located west of the Indus River in Larkana District, Sindh, Pakistan, in a central position between the Indus River and the Ghaggar-Hakra River. It is situated on a Pleistocene ridge in the middle of the flood plain of the Indus River Valley, around 28 kilometres (17 mi) from the city of Larkana. The ridge was prominent during the time of the Indus Valley Civilization, allowing the city to stand above the surrounding flood, but subsequent flooding has since buried most of the ridge in silt deposits. The Indus still flows east of the site, but the Ghaggar-Hakra riverbed on the western side is now dry.\nMohenjo-daro has a hot desert climate with extremely hot summers and mild winters. The highest recorded temperature is 53.5 °C (128.3 °F), and the lowest recorded temperature is −5.4 °C (22.3 °F). Rainfall is low, and mainly occurs in the monsoon season that is July to September.\nMohenjo-daro was built in the 26th century BCE. It was one of the largest cities of the ancient Indus Valley Civilization, also known as the Harappan Civilization, which developed around 3,000 BCE from the prehistoric Indus culture. At its height, the Indus Civilization spanned much of what is now Pakistan and North India, extending westwards to the Iranian border, south to Gujarat in India and northwards to an outpost in Bactria, with major urban centres at Harappa, Mohenjo-daro, Lothal, Kalibangan, Dholavira and Rakhigarhi. Mohenjo-daro was the most advanced city of its time, with remarkably sophisticated civil engineering and urban planning. When the Indus civilization went into sudden decline around 1900 BCE, Mohenjo-daro was abandoned.\nRediscovery and excavation\nThe ruins of the city remained undocumented for around 3,700 years until R. D. Banerji, an officer of the Archaeological Survey of India, visited the site in 1919–20 identifying what he thought to be a Buddhist stupa (150–500 CE) known to be there and finding a flint scraper which convinced him of the site's antiquity. This led to large-scale excavations of Mohenjo-daro led by Kashinath Narayan Dikshit in 1924–25, and John Marshall in 1925–26. In the 1930s major excavations were conducted at the site under the leadership of Marshall, D. K. Dikshitar and Ernest Mackay. Further excavations were carried out in 1945 by Mortimer Wheeler and his trainee, Ahmad Hasan Dani. The last major series of excavations were conducted in 1964 and 1965 by George F. Dales. After 1965 excavations were banned due to weathering damage to the exposed structures, and the only projects allowed at the site since have been salvage excavations, surface surveys, and conservation projects. In the 1980s, German and Italian survey groups led by Michael Jansen and Maurizio Tosi used less invasive archaeological techniques, such as architectural documentation, surface surveys, and localized probing, to gather further information about Mohenjo-daro. A dry core drilling conducted in 2015 by Pakistan's National Fund for Mohenjo-daro revealed that the site is larger than the unearthed area.\nArchitecture and urban infrastructure\nMohenjo-daro has a planned layout with rectilinear buildings arranged on a grid plan. Most were built of fired and mortared bricks, some incorporated sun-dried mud-brick and wooden superstructures. The covered area of Mohenjo-daro is estimated at 300 hectares. The Oxford Handbook of Cities in World History offers a weak estimate of a peak population of around 40,000.\nThe sheer size of the city, and its provision of public buildings and facilities, suggests a high level of social organization. The city is divided into two parts, the so-called Citadel and the Lower City. The Citadel, a mud-brick mound around 12 metres (39 ft) high is known to have supported public baths, a large residential structure designed to house about 5,000 citizens, and two large assembly halls. The city had a central marketplace, with a large central well. Individual households or groups of households obtained their water from smaller wells. Waste water was channelled to covered drains that lined the major streets. Some houses, presumably those of more prestigious inhabitants, include rooms that appear to have been set aside for bathing, and one building had an underground furnace (known as a hypocaust), possibly for heated bathing. Most houses had inner courtyards, with doors that opened onto side-lanes. Some buildings had two stories.\nIn 1950, Sir Mortimer Wheeler identified one large building in Mohenjo-daro as a 'Great Granary'. Certain wall-divisions in its massive wooden superstructure appeared to be grain storage-bays, complete with air-ducts to dry the grain. According to Wheeler, carts would have brought grain from the countryside and unloaded them directly into the bays. However, Jonathan Mark Kenoyer noted the complete lack of evidence for grain at the 'granary', which, he argued, might therefore be better termed a 'Great Hall' of uncertain function. Close to the 'Great Granary' is a large and elaborate public bath, sometimes called the Great Bath. From a colonnaded courtyard, steps lead down to the brick-built pool, which was waterproofed by a lining of bitumen. The pool measures 12 metres (39 ft) long, 7 metres (23 ft) wide and 2.4 metres (7.9 ft) deep. It may have been used for religious purification. Other large buildings include a 'Pillared Hall', thought to be an assembly hall of some kind, and the so-called 'College Hall', a complex of buildings comprising 78 rooms, thought to have been a priestly residence.\nMohenjo-daro had no series of city walls, but was fortified with guard towers to the west of the main settlement, and defensive fortifications to the south. Considering these fortifications and the structure of other major Indus valley cities like Harappa, it is postulated that Mohenjo-daro was an administrative centre. Both Harappa and Mohenjo-daro share relatively the same architectural layout, and were generally not heavily fortified like other Indus Valley sites. It is obvious from the identical city layouts of all Indus sites that there was some kind of political or administrative centrality, but the extent and functioning of an administrative centre remains unclear.\nWater supply and wells\nThe location of Mohenjo-daro was built in a relatively short period of time, with the water supply system and wells being some of the first planned constructions. With the excavations done so far, over 700 wells are present at Mohenjo-daro, alongside drainage and bathing systems. This number is unheard of when compared to other civilizations at the time, such as Egypt or Mesopotamia, and the quantity of wells transcribes as one well for every three houses. Because the large number of wells, it is believed that the inhabitants relied solely on annual rainfall, as well as the Indus River's course remaining close to the site, alongside the wells providing water for long periods of time in the case of the city coming under siege. Due to the period in which these wells were built and used, it is likely that the circular brick well design used at this and many other Harappan sites are an invention that should be credited to the Indus civilization, as there is no existing evidence of this design from Mesopotamia or Egypt at this time, and even later. Sewage and waste water for buildings at the site were disposed of via a centralized drainage system that ran alongside the site's streets. These drains that ran alongside the road were effective at allowing most human waste and sewage to be disposed of as the drains tool the waste most likely toward the Indus River. It is theorized that the job of keeping the pipes clean and from getting piled up was either a job for slaves, or captured enemy soldiers, with others who believe it was a paid job for citizens of the city.\nFlooding and rebuilding\nThe city also had large platforms perhaps intended as defence against flooding. According to a theory first advanced by Wheeler, the city could have been flooded and silted over, perhaps six times, and later rebuilt in the same location. For some archaeologists, it was believed that a final flood that helped engulf the city in a sea of mud brought about the abandonment of the site. Gregory Possehl was the first to theorize that the floods were caused by overuse and expansion upon the land, and that the mud flood was not the reason the site was abandoned. Instead of a mud flood wiping part of the city out in one fell swoop, Possehl coined the possibility of constant mini-floods throughout the year, paired with the land being worn out by crops, pastures, and resources for bricks and pottery spelled the downfall of the site.\nNumerous objects found in excavation include seated and standing figures, copper and stone tools, carved seals, balance-scales and weights, gold and jasper jewellery, and children's toys. Many bronze and copper pieces, such as figurines and bowls, have been recovered from the site, showing that the inhabitants of Mohenjo-daro understood how to utilize the lost wax technique. The furnaces found at the site are believed to have been used for copperworks and melting the metals as opposed to smelting. There even seems to be an entire section of the city dedicated to shell-working, located in the north-eastern part of the site. Some of the most prominent copperworks recovered from the site are the copper tablets which have examples of the untranslated Indus script and iconography. While the script has not been cracked yet, many of the images on the tablets match another tablet and both hold the same caption in the Indus language, with the example given showing three tablets with the image of a mountain goat and the inscription on the back reading the same letters for the three tablets. Pottery and terracotta sherds have been recovered from the site, with many of the pots having deposits of ash in them, leading Archaeologists to believe they were either used to hold the ashes of a person or as a way to warm up a home located in the site. These heaters, or braziers, were ways to heat the house while also being able to be utilized in a manner of cooking or straining, while others solely believe they were used for heating. Many important objects from Mohenjo-daro are conserved at the National Museum of India in Delhi and the National Museum of Pakistan in Karachi. In 1939, a representative collection of artefacts excavated at the site was transferred to the British Museum by the Director-General of the Archaeological Survey of India.\nMother Goddess Idol\nDiscovered by John Marshall in 1931, the idol appears to mimic certain characteristics that match the Mother Goddess belief common in many early Near East civilizations. Sculptures and figurines depicting women have been observed as part of Harappan culture and religion, as multiple female pieces were recovered from Marshall's archaeological digs. These figures were not categorized correctly, according to Marshall, meaning that where they were recovered from the site is not actually clear. One of said figures, pictured below, is 18.7 cm tall and is currently on display at the National Museum of Pakistan, in Karachi. The fertility and motherhood aspects on display on the idols is represented by the female genitalia that is presented in an almost exaggerated style as stated by Marshall, with him inferring that such figurines are offerings to the goddess, as opposed to the typical understanding of them being idols representing the goddess's likeness. Because of the figurines being unique in terms of hairstyles, body proportions, as well as headdresses and jewelry, there are theories as to who these figurines actually represent. Shereen Ratnagar theorizes that because of their uniqueness and dispersed discovery throughout the site that they could be figurines of ordinary household women, who commissioned these pieces to be used in rituals or healing ceremonies to help aforementioned individual women.\nIn 1927, a seated male soapstone figure was found in a building with unusually ornamental brickwork and a wall-niche. Though there is no evidence that priests or monarchs ruled Mohenjo-daro, archaeologists dubbed this dignified figure a Priest-King. The sculpture is 17.5 centimetres (6.9 in) tall, and shows a neatly bearded man with pierced earlobes and a fillet around his head, possibly all that is left of a once-elaborate hairstyle or head-dress, his hair is combed back. He wears an armband, and a cloak with drilled trefoil, single circle and double circle motifs, which show traces of red. His eyes might have originally been inlaid.\nA seal discovered at the site bears the image of a seated, cross-legged and possibly ithyphallic figure surrounded by animals. The figure has been interpreted by some scholars as a yogi, and by others as a three-headed 'proto-Shiva' as 'Lord of Animals'.\nSir Mortimer Wheeler was especially fascinated with this artifact, which he believed to be at least 4,500 years old. The necklace has an S-shaped clasp with seven strands, each over 4 ft long, of bronze-metal bead-like nuggets which connect each arm of the 'S' in filigree. Each strand has between 220 and 230 of the many-faceted nuggets, and there are about 1,600 nuggets in total. The necklace weighs about 250 grams in total, and is presently held in a private collection in India.\nSurviving structures at Mohenjo-daro\nPreservation work for Mohenjo-daro was suspended in December 1996 after funding from the Pakistan government and international organizations stopped. Site conservation work resumed in April 1997, using funds made available by the UNESCO. The 20-year funding plan provided $10 million to protect the site and standing structures from flooding. In 2011, responsibility for the preservation of the site was transferred to the government of Sindh.\nCurrently the site is threatened by groundwater salinity and improper restoration. Many walls have already collapsed, while others are crumbling from the ground up. In 2012, Pakistani archaeologists warned that, without improved conservation measures, the site could disappear by 2030.\n2014 Sindh Festival\nThe Mohenjo-daro site was further threatened in January 2014, when Bilawal Bhutto Zardari of the Pakistan People's Party chose the site for Sindh Festival's inauguration ceremony. This would have exposed the site to mechanical operations, including excavation and drilling. Farzand Masih, head of the Department of Archaeology at Punjab University warned that such activity was banned under the Antiquity Act, saying 'You cannot even hammer a nail at an archaeological site'. On 31 January 2014, a case was filed in the Sindh High Court to bar the Sindh government from continuing with the event. The festival was held by PPP at the historic site, despite all the protest by both national and international historians and educators."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8a2a742b-18d2-4922-8079-e467e9818509>","<urn:uuid:618647da-1498-4ef8-9e6c-782b372bf482>"],"error":null}
{"question":"When considering durability for a home upgrade, how does marble countertop performance compare to outdoor landscaping elements?","answer":"Marble countertops and outdoor landscaping elements have different durability characteristics. Marble is a soft, highly-porous stone that's susceptible to staining but can last for decades with proper care and maintenance. While it offers impressive heat resistance, its softer composition makes it less suitable for heavy use, particularly in kitchens. On the other hand, outdoor landscaping elements need to be kept simple and maintainable - excessive features like numerous exotic plants can be problematic as they require specialized care. The key difference is that while marble offers superior long-term performance that can last decades, outdoor landscaping requires ongoing maintenance and may need regular updates to maintain its appeal.","context":["If you’re selling your home, then you might already know that staging is a very important part of the process. However, staging doesn’t just happen on the inside; you’ll need to work on your landscaping, too. Here are five tips for landscaping your home to sell at the highest possible value.\n#1 – Keep Up on Regular Maintenance\nWaiting until you have a scheduled showing to run out and make the yard look nice is a bad idea. If your home is listed on the market, then so is your address, and chances are good that people are driving by to get an idea of what the home looks like. If your yard is less than perfect, it’ll give them a very bad first impression. Keep up on your regular maintenance – mowing, weeding, twig removal, and even picking up pet droppings – in order to make your home look inviting.\n#2 – Make Wise Use of Plants\nAlthough you can do a lot to make your yard look nice, you can’t force your neighbor to paint his garage door or move her garbage cans. You can, however, plant things like bamboo, tall decorative grass, and other plants to hide them. Make sure that there are no trees too close to your home, though; this can put buyers off since tree roots cause significant structural damage to the foundation of your home. Consider cutting down any trees that are too close.\n#3 – Add Some Features\nYour goal should be to make your outdoor space as inviting and “livable” as your indoor space. If you don’t have any landscaping features, consider adding some. Patio furniture is a great investment in your future since it makes your yard seem far more inviting, and things like stepping stones, walkway lighting, fire pits, fountains, and more make things look incredibly luxurious. Don’t go overboard, but make your yard look nice, warm, and cozy.\n#4 – Remember Universal Appeal\nWhen you stage your home for sale, you’ll probably need to remove personal effects or repaint walls if they aren’t run-of-the-mill colors. This is because you’ll want your home to appeal universally to everyone. You should remember the same advice when it comes to your landscaping. Take down a playset you bought for your kids and take it with you. Remove any political campaign signs, sports team memorabilia, and other things that may put some folks off and keep them from buying.\n#5 – Simpler Is Better\nWhen trying to make your yard look nice, more is not better and better doesn’t always mean extravagant. The best thing you can do is keep things simple so that future homeowners can get ideas of how they would change things to suit their own needs. For example, 200 exotic plants might look absolutely stunning, but will prospective homeowners really be willing to hire someone to take care of all of them? When selling your home, the simpler the landscaping, the better.\nLandscaping your home to sell isn’t a difficult task. Just remember to keep things universal and simple, use plants to your advantage, and keep your yard maintained – even between showings. Curb appeal is just as important as the staging inside your home, so use it to your advantage.","About Marble Countertops\nMarble is known as a metamorphic rock that is formed from the recrystallization of carbonate materials beneath the earth’s surface. Most commonly, marble is formed from limestone and other base minerals that are naturally found in the earth’s crust.\nMarble’s unique color and veining are determined by the minerals that are present during its formation. For example, limestone with a high magnesium content typically produces a green hue. Other minerals like sand, clay, and even iron oxides will create unique hues within the marble stone. These hues are entirely dependent on the quantity and quality of these additional minerals\nKitchen Marble Countertops\nMarble is an inherently cool stone, and remains markedly cooler than room temperatures in any setting. Bakers and chefs alike love kitchen marble countertops because it is the perfect surface for preparing and rolling out doughs and pastries. But you don’t have to be a culinary expert to enjoy the benefits of marble kitchen countertops.\nMarble is also a soft, highly-porous stone, which means it’s incredibly susceptible to staining. It’s because of this inherent porosity that your kitchen marble countertops will need a moderate amount of maintenance and care.\nAcidic materials like alcohol, soda, citrus, and coffee should be promptly and thoroughly cleaned from your marble countertops to prevent etching and staining. In addition, it’s highly recommended to routinely apply a high quality sealant to help to protect your investment and keep it looking as gorgeous as the day it was installed.\nBathroom Marble Countertops & Vanities\nBecause of the softer composition of marble, this stone may not be the best choice for heavily-used kitchens. Bathroom marble countertops and vanities are a great choice for those who feel this stone may not be well-suited for kitchen use.\nIt’s well known that marble maintains a naturally cool temperature, however many are surprised to learn that this gorgeous stone offers impressive heat resistance as well. This means you won’t have to worry about damaging your bathroom marble countertops with the heat from hair styling implements and more.\nMarble Countertop Colors\nMarble can be found in a wide variety colors with a palette that’s both bold and plentiful. Regardless of your style, taste, or design, you can be sure to find a color that will perfectly compliment your vision for your new space. From creamy whites and soft pinks to blissful blues and deep, rich blacks, the possibilities are simply endless.\nMarble is quarried from all around the world. Some of the most popular regions include: Italy, Greece, Germany, and the United States. Each marble quarry will produce natural stones that are truly unique in appearance. Each site around the world is comprised of a unique blend of minerals that will determine the specific color. Once the stone stone from a particular quarry is gone, it can never be replicated or reproduced.\nBenefits of Marble Countertops\nWhen you choose marble countertops for your new project, you can enjoy a number of benefits. Marble countertop installation is a great choice for your kitchens, bathrooms, vanities, and more! This natural stone will add an unsurpassed beauty to any space, and with the proper care and maintenance your marble countertops will last for decades.\nMarble can be used to complement and accentuate any design style imaginable - from contemporary to modern and everything in between. Marble countertops offer decades of superior performance with a one-of-a-kind beauty that simply cannot be reproduced by man or nature. You’ll never have a need to replace your new countertops unless you simply want to!\nAnd, if you ever decide to place your home on the market, this high-end feature will help make your home stand out in a crowded market. Your marble countertops will appeal to a wider range of potential buyers who are looking for a new home with high-end features. In addition, the presence of this timeless, sophisticated stone may prompt serious buyers to place a higher offer, giving you the best return on your investment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7a0d8550-0bf6-4415-bf0e-0918c9fa66bc>","<urn:uuid:ef634163-5fd0-45cb-845b-5456709001db>"],"error":null}
{"question":"How does digital health tracking handle user data security and privacy in both clinical IVF systems and consumer fitness devices?","answer":"In clinical settings like IVF iClinic, patient data is managed through secure digital platforms with features like personal data protection consensus forms and centralized data collection systems. For consumer fitness devices, the data isn't protected under HIPAA standards and can be vulnerable through Bluetooth connections or shared with third parties. Users need to carefully manage privacy settings, use strong passwords, enable two-factor authentication, and be cautious about sharing location data or social settings that could expose sensitive information.","context":["Paperless IVF iClinic / iAssisting / is a digital, paperless platform established to manage the contemporary IVF Clinic.\nEach one of us knows what the iStore is. Having an operating system, you can take or buy many useful applications. In the same way in the IVF iClinic you are able to choose every useful Module and introduce it into the Unit operations and practice.\nThe digital platform is divided into three categories :\nLet’s try to illustrate the philosophy of paperless clinic using an example:Introducing the EN 15224 we are obliged to organize and monitor staff training process. Our intelligent, virtual iMeeting Module integrates all these functions with the possibility of recording the meetings, training topics, staff presences and absences and all Keynotes, PowerPoint, PDF or video presentations in the Cloud. Pressing the button on your Mac, the meeting category list is viewable allowing you to choose from trainings, clinical monitoring, general staff meetings or other categories. Old and new agendas and presentations are available there, organized by years, thematic blocks, categories, subcategories. Every clinical or scientific director is able to monitor staff presences and absences during each session and use the specific logbooks if it is necessary. Digitally signed logbooks help us stay informed about learning performances and progress of our staff. Accessibility of all training sessions and their data for doctors, nurses and all the staff are fundamental beliefs of every modern clinic.\nEasy to use and fully customized Online Appointment Scheduler with online Booking Calendar and Patient Registration System / demographic data form, couple’s medical history form.\nDigital, electronic connection with patient who chooses iTherapy Module. Tracking system of exchanging information between patient and iClinic including therapy schedules, exams results, notifications about drugs’ administration, visits, embryo cleavage tracking system information, prescriptions, medical statements and therapy feedback. iCloud Calendar functions include next visit, hormonal exams, triggering date and time, pregnancy test date, Pregnancy Calendar.\nBasic edition of IVF iClinic includes :\nFully integrated database of patient demographic data, personal data protection consensus forms, referrals data- iTherapy Module – centralized data collection system, sharing information between the staff, online monitoring treatment progress system with synchronization of every type of schedule in real time, lists of every type of treatment\nOnline visits, procedures, appointment scheduler\nOnline treatments, procedures organizer with daily, weekly or all treatment table view with tracking of hormonal results, ultrasound scans, drug doses, time of triggering, comments for patients.\nThe Lab online – data includes the statistics about the number and percentage of mature or immature eggs taken, the process of fertilization, cleavage, oocytes, embryos, blastocyst grading .RI IVF Witness system fully integrated with the database. RI and Miri-scope image and video clip capturing system. Logging and tracking of all laboratory supplies system\nThe Lab online – data includes the statistics about the number, quality sperm markers, DFI, Fertile, Seed results\nThe Bank online – detailed data about oocytes, embryos, sperm and tissue vitrification with grading system, statistics, gametes and bank quality markers. Independent lists and notifications with freezing renewal dates.\nsophisticated module which helps you organize and manage the donation program of the clinic. Donor patient database with validation program and matching donor with recipient system\nsophisticated module which helps you organize and manage the surrogate program of the clinic. Surrogate patient data base with validation program and matching donor with recipient system\nsophisticated module which helps you organize and manage your clinic Social Freezing program. Independent lists and notifications with freezing renewal dates\nData base integrated prescription system\nProgram that gathers financial data and generates patient invoices reports and quarterly financial statements.\nDigital non conformities monitoring system is one of the most important part of the Quality Management System. The online, real time, non-conformities module provides transparency and freedom of expressing thoughts and easiness in proposing one’s own and new ideas. This online Module allows incredible synchronization and all staff information about corrective actions taken, grouped by date or department.\nThe basic quality parameters, analyses and trends are available at any time to the Quality Manager and all authorized staff. By linking and analyzing data, you can closely monitor every individual patient, and you have the ability to analyze the effectiveness of protocols, or staff members, to identify areas of weaknesses or develop strengths.\nDigital system created to organize and monitor staff training process, meetings with possibility of recording them, training topics, staff presences and absences and all Keynotes, PowerPoint, PDF or video presentations in the Cloud. Viewable by categories, date, topics or thematic blocks\nDigital assistant for internal or external auditing of your IVF Clinic based on EN15224:2012 standards. Audits planning, inputs, stages, implementation, monitoring, conclusions, recommendations.\nWeb based easily accessible, digital and transparent job performance evaluation system with electronic questionnaire which supports the anonymity of corresponding employees as well as aggregated results available only to QMS team.","- Fitness trackers and apps from companies including Google, Apple, Garmin and Strava offer a convenient way to stay on top of health and wellness, monitoring body metrics like sleep quality and heart rate.\n- But even the biggest brands focused on security and reputation can be hacked or share personal data in other unintended ways with serious, sometimes devastating, consequences.\n- Data collected by a fitness app is not protected like health information under the law, making social and location settings, and login credentials, critical for a user to set properly before making these devices part of their daily life.\nFitness trackers, which help keep tabs on sleep quality, heart rate and other biological metrics, are a popular way to help Americans improve their health and well-being.\nThere are many types of trackers on the market, including those from well-known brands such as Apple, Fitbit, Garmin and Oura. While these devices are growing in popularity — and have legitimate uses — consumers don't always understand the extent to which their information could be available to or intercepted by third parties. This is especially important because people can't simply change their DNA sequencing or heart rhythms as they could a credit card or bank account number.\n\"Once the toothpaste is out of the tube, you can't get it back,\" said Steve Grobman, senior vice president and chief technology officer of computer security company McAfee.\nThe holiday season is a popular time to purchase consumer health devices. Here's what you should know about the security risks tied to fitness trackers and personal health data.\nStick to a name brand, even though they are hacked\nFitness devices can be expensive, even without taking inflation into account, but don't be tempted to skimp on security to save a few dollars. While a less-known company may offer more bells and whistles at a better price, a well-established provider that is breached is more likely to care about its reputation and do things to help consumers, said Kevin Roundy, senior technical director at cybersecurity company Gen Digital.\nTo be sure, data compromise issues, from criminal hacks to unintended sharing of sensitive user information, can — and have — hit well-known players, including Fitbit, which Google bought in 2021, and Strava. But even so, security professionals say it's better to buy from a reputable manufacturer that knows how to design secure devices and has a reputation to upkeep.\n\"A smaller company might just go bankrupt,\" Roundy said.\nFitness app data is not protected like health information\nThere can be other concerns beyond having a person's sensitive information exposed in a data breach. For example, fitness trackers generally connect to a user's phone via Bluetooth, leaving personal data susceptible to hacking.\nWhat's more, the information that fitness trackers collect isn't considered \"health information\" under the federal HIPAA standard or state laws like California's Confidentiality of Medical Information Act. This means that personally revealing data can potentially be used in ways a consumer might never expect. For instance, the personal information could be shared with or sold to third parties such as data brokers or law enforcement, said Emory Roane, policy counsel at Privacy Rights Clearinghouse, a consumer privacy, advocacy and education organization.\nSome fitness trackers may use consumers' health and wellness data to derive revenue from ads, so if that's a concern, you'll want to make sure there's a way to opt out. Review the provider's terms of service to understand the its policies before you buy the fitness tracker, Roundy said.\nDefault social, location settings may need to be changed\nA fitness tracker's default settings may not offer the most stringent security controls. To boost protection, look at what settings can be adjusted, such as those related to social networking, location and other sharable information, said Dan Demeter, security researcher at cybersecurity provider Kaspersky Lab.\nDepending on the state, consumers can also opt out of the sale or sharing of their personal information to third parties, and in some cases, these rights are being expanded, according to Roane.\nCertainly, device users should be careful about what they post publicly about their location and activities, or what they allow to become public by default. This data could be searchable online and used by bad actors. Even if they aren't acting maliciously, third parties such as insurers and employers could get access to this type of public information.\n\"Users expect their data to be their data and use it how they want it to be used,\" Roane said, but that's not necessarily the case.\n\"It's not only about present data, but also about past data,\" Demeter said. For instance, a bad actor could see all the times the person goes running — what days and hours — and where, and use it to their advantage.\nThere are also a number of digital scams where criminals can use information about your location to make an opportunity seem more plausible. They can claim things like, \"I know you lost your wallet at so and so place, which lends credibility to the scammer's story,\" Grobman said.\nLocation data can prove problematic in other ways as well. Roane offers the example of a women seeking reproductive health care in a state where abortion is illegal. A fitness tracker with geolocation services enabled could collect information that could be subpoenaed by law enforcement or be purchased by data brokers and sold to law enforcement, he said.\nUse strong password, two-factor authentication, and never share credentials\nBe sure to secure your account by using a strong password that you don't use with another account and enabling two-factor authentication for the associated app. And don't share credentials. That's never a good idea, but it can have especially devastating consequences in certain circumstances. For example, a domestic violence victim could be tracked by her abuser, assuming he had access to her account credentials, Roane said.\nAlso be sure to keep the device and the app up-to-date with security fixes.\nWhile nothing is foolproof, the goal is to be as secure as possible. \"If somebody tries to profit from our personal information, we just make their lives harder so it's not that easy to hack us,\" Demeter said."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d355a460-64ba-40aa-afe8-19147af2581e>","<urn:uuid:760aa365-b7f3-4b99-94d6-4a27433898e1>"],"error":null}
{"question":"What is the significance of the Empty Chair Ceremony in Masonic lodges, and how does it connect to the poppy as a symbol of remembrance?","answer":"The Empty Chair Ceremony, dating back to 1875, is a Masonic tradition that began after the American Civil War to honor those who didn't return from war. It has since been used by lodges at Remembrance Day to pay homage to fallen Brother Masons from various wars. During the ceremony, an empty chair is decorated with an officer's collar and a Master Mason's apron, symbolizing the spiritual presence of fallen brothers. The ceremony connects to the poppy symbol, which became a tradition of remembrance due to the poppies blooming in battlefields like Flanders Fields. Poppies were the first flowers to grow back after war disturbed the soil, and they became a powerful symbol of remembrance for those killed in service.","context":["The Empty Chair Ceremony A Ceremony Of Remembrance As Perfromed At Moira Lodge A.F. & A.M. #11 G.R.C. September 7th, 2001 The Empty or vacant Chair ceremony dates back to 1875, a decade after the close of the American Civil War when it was used in Masonic lodges to pay tribute to those who did not return from the war. Since then it has been used by many lodges at Remembrance Day to pay homage to those Brother Masons who fell during WWI, WWII, and other wars. The version below is an amalgamation of some of these ceremonies, the Masonic funeral service of Ontario and some material added on for our particular use. What follows is a transcript of the ceremony as performed in Moira Lodge A.F. & A.M. #11 G.R.C. on November 7th, 2001. Please feel free to copy it and modify it to suit your own ceremony of remembrance. Set Up Of Lodge ------------------- An empty chair is decorated with an officer's collar over the back. It is placed outside the door of the A podium and wreath are placed in South East angle of the lodge. Empty Chair Ceremony -------------------- WM: Brother Deacons you will approach the altar. Deacons approach altar WM: This evening we will be honoring the members of this lodge who fell during times of war. As they have gone on to the Grand lodge above and can no longer attend this lodge it will be your duty to escort their symbolic representation to us, that we may pay them the respect they are due. WM: Brother JW, you will permit the Brother Deacons to retire. JW & IG permit deacons to retire ALARM caused by deacons returning with chair IG: Brother JW there is an alarm JW: Brother IG you will ascertain the cause of the alarm. IG: Brother JW, Comrades and Brothers who have fallen in service of their country seek admission here, not in person, but through their spiritual presence they seek our continued remembrance. Through them they wish to honour the memory of our fallen Brother, William John Gillespie of the Canadian Infantry who fell at the Battle of Vimy Ridge. (substitute for your own lodge) JW: Worshipful Sir, Comrades and Brothers who have fallen in service of their country seek admission here, not in person, but it is through their spiritual presence that they seek our continued remembrance. They also wish to honour the memory of our fallen Brother, William John Gillespie of the Canadian Infantry who fell at the Battle of Vimy Ridge. (substitute for your own lodge) WM: You will admit them WM raises Brethren Deacons advance to altar. WM: Brother Deacons you will advance to the east and place the symbolic representation of our fallen Brethren next to the podium. Deacons advance to South East and place chair next to podium facing west WM seats Brethren WM: Brother Senior Warden, it is my command that in recognition of our fallen Brother's presence, and his status as a Master Mason, that the apron of a Master Mason be positioned as it would be were our Brother present in body as well as spirit. You, or a Brother, who has honorably served his country in uniform, will approach the seat of our Brothers' memory, and perform the honor. SW approaches podium SW: The Lambskin, or white apron, was the first gift of Masonry to our departed Brother. It is an emblem of innocence and the badge of a Mason. As our ritual tells us it is more ancient than the Golden Fleece or Roman Eagle, more honorable than the Star and Garter. This emblem I place on the seat of our deceased Brother, a symbol of recognition of his dedication to the highest ideals of the Craft during times of war. Apron Placed on chair by SW SW: By this act we are reminded of the Masonic ideals of our fallen Brother and his fellows. We see in clear vision the Noble thoughts, generous impulses, words of truth, acts of love and deeds of mercy. The Masonic Apron represents these highest aspirations of a Brother in all ways, as each Brother knows they give to man his only genuine happiness, his lasting satisfaction. To these precepts our Brother willingly and gladly subscribed. SW salutes and returns to his chair WM approaches the podium WM: Our Brother William John Gillespie gave himself freely not only to the obligations of the Degrees of Masonry, but also to the obligations of service to his country in a time of war. He garnered the honors of his peers, his superiors and those who looked to him for leadership at home as well as on foreign shores. I now place these medals of honor as decorations to his Masonic Apron. It is said a Man is made a Mason first in his heart. The Mason may have earned honors before, or after he is raised to the Sublime Degree. But as the world sees, those honors do not decorate his Masonry, but rather highlight the spirit, which made both a Mason and a man of service. WM places medals on apron returns to East WM: Brother Chaplain you will approach the altar. Chaplain advances to altar. WM: The Evergreen is an emblem of immortality. Beyond this world of shadows, man has a glorious destiny, since, within this earthly tabernacle of clay, there abides an imperishable immortal spirit, over which the grave has no power nor death dominion. Brother Chaplain you will advance to the East, place the evergreen on our Brother's apron, a symbol of the immortality of his soul and recite for the Brethren the 23rd psalm. Chaplain advances to chair and then to podium Chaplain: The Lord is my Shepherd; I shall not want. He maketh me to lie down in green pastures: He leadeth me beside the still waters. He restoreth my soul: He leadeth me in the paths of righteousness for His name' sake. Yea, though I walk through the valley of the shadow of death, I will fear no evil: For thou art with me; Thy rod and thy staff, they comfort me. Thou preparest a table before me in the presence of mine enemies; Thou annointest my head with oil; My cup runneth over. Surely goodness and mercy shall follow me all the days of my life, and I will dwell in the House of the Lord forever. Chaplain salutes and returns to his chair WM: approaches podium WM: The brother who we pay tribute to this evening was a member of this lodge. He was also a member of the Canadian infantry who fought valiantly for control of Vimy Ridge during the First World War. It was here that our Brother lost his life fighting for the freedoms we all can enjoy this evening in this lodge because of him and men like him. (substitute your own material here) Brethren at this time of year it is traditional to wear the poppy as a symbol of remembrance, a tradition that was began as a result of the poem \"In Flander's Fields\" by John McCrae. While it is also traditional to read that classic poem at this time of year, I would like to break with that tradition and instead read a poem written by Miss Moina Michael. Miss Michael was an American lady who was so moved by Mccrae's poem that she went out and bought 25 red poppies. She wore won herself and sold the other 24 thus starting the poppy fund as we know it today. She also wrote a poem of her own in response to In Flanders Fields, which I would like to now read to you. Oh You who sleep In Flanders' fields Sleep sweet to rise anew We caught the torch you threw And holding high we kept The faith with those who died. We cherish too, the poppy red That grows on fields where valor led It seems to signal to the skies That blood of heroes never dies But lends a luster to the red Of the flower that blooms above the dead In Flanders' fields. And now the torch and poppy red wear in the honour of the dead Fear not that ye have died for nought: We've learned the lesson that ye taught In Flanders' fields WM removes poppy and places on chair WM returns to east WM: Brethren we will now have two minutes silence for Brother William John Gillespie and for all Brethren who fell during foreign wars. WM raises brethren Moments silence signaled by 13 gongs of bell 10 seconds apart. WM seats brethren WM: Brethren this concludes our ceremony of remembrance I declare that order of business closed.\nCopyright: The Skirret, 2015","To all the Veterans who served and sacrificed, I wish you all a very heartfelt thank you. On the 11th day of the 11th month, at the 11th hour, thanks to all the sacrifices you made, we are free.\nIn Belgium, the In Flanders Fields Museum in Ypres, named after the poem and devoted to the First World War, is situated in one of Flanders’ largest tourist areas. “In Flanders Fields” is a poem written by John McCrae during the first World War.\nThe poem describes poppies blooming between gravestones. Poppies are considered the battlefield flower, because they are the first to grow back after war pockmarks the landscape. The war created prime conditions for poppies to flourish in Flanders and north-west France. Continual bombardment disturbed the soil and brought the seeds to the surface. Its powerful use of the symbol of the poppies blooming from the churned earth led to the tradition, to this day, of the poppy as a symbol of remembrance for those killed in service. By 1917, “In Flanders Fields” was known throughout the English-speaking world. Flanders Fields, the name of World War I battlefields in the medieval County of Flanders, which spans southern Belgium and north-west France. Flanders Field American Cemetery and Memorial, a World War I cemetery on the southeast edge of the town of Waregem, Belgium.\nIn Flanders Fields\nIn Flanders fields the poppies blow\nBetween the crosses, row on row,\nThat mark our place; and in the sky\nThe larks, still bravely singing, fly\nScarce heard amid the guns below.\nWe are the Dead. Short days ago\nWe lived, felt dawn, saw sunset glow,\nLoved and were loved, and now we lie,\nIn Flanders fields.\nTake up our quarrel with the foe:\nTo you from failing hands we throw\nThe torch; be yours to hold it high.\nIf ye break faith with us who die\nWe shall not sleep, though poppies grow\nIn Flanders fields.\nI painted these pictures a while ago and thought they were perfect for today.\nAs many of you know, my dad was originally from Melbourne, Australia. Many of his relatives fought in WWI, and died in the fierce battles of Gallipoli. The Gallipoli campaign\n, also known as the Dardanelles campaign\n, the Battle of Gallipoli\nor the Battle of Çanakkale\n: Çanakkale Savaşı\n). It was a campaign of the First World War\nthat took place on the Gallipoli\nin modern Turkey), from 17 February 1915 to 9 January 1916. The campaign is often considered to be the beginning of Australian and New Zealand national consciousness\n; 25 April, the anniversary of the landings, is known as ANZAC Day\n, the most significant commemoration of military casualties and veterans in the two countries, surpassing Remembrance Day\n). My dad himself ran away at the age of 14, lied about his age and fought in the Australian Army in the battles of North Africa during WWII. He was an ANZAC. The ANZACS were members of the Australian and New Zealand Army Corps. One of his proudest moments, one of the last times he went back home to Australia, was to march in the ANZAC Parade. When he emigrated to the United States, shortly after WWII, his life was in the Merchant Marines, where he also bravely fought in 2 additional wars. He did military sea lift command for both Korea and Vietnam. Thank you Daddy, and to all the other brave men and women who fought and served. You will NEVER be forgotten.\nMy daddy, John Bailey Jones, circa 1939, before heading off to North Africa."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:24c04503-b173-422d-9b3c-9e122ac19615>","<urn:uuid:a01de4bf-3134-469f-b742-09258bd978e8>"],"error":null}
{"question":"I want to understand the history vs future of ship emissions - what's the difference between the pollution from the 1908 Great White Fleet's coal-fired ships and the IMO's 2020 emissions standards? Pretty wild how far we've come!","answer":"The 1908 Great White Fleet's coal-fired ships produced severe pollution that was so significant that questions arose about how they maintained their white paint and gilded scrollwork. In contrast, the IMO's 2020 standards represent a dramatic improvement, requiring ships to reduce sulphur emissions from 3.5% to 0.5% of fuel content. Additionally, modern ships must comply with nitrogen oxide limitations, with technologies like SCR systems capable of reducing NOx emissions by 90-95%, showing a significant evolution in controlling ship-based pollution.","context":["Air Pollution at Station Pier\nThe cruise ship season extends into May. P & O’s Pacific Eden was in today.\nStation Pier is off limits to foot traffic on cruise ship days but the western wing pier is a more relaxed place to be anyway.\nHidden behind Station Pier’s unprepossessing restaurants, the wing pier is a good ‘get away from it all’ place. It always feels cooler there. You can get close to the water on the lower platforms. Seldom busy but usually frequented by a fisher or two, it’s a low key, easy kind of place – not over-designed or over-managed. The kiosk is now home base for the Waterfront Welcomers who help cruise ship passengers on their way to enjoy their time in Melbourne.\nNear the restored Stothert and Pitt crane, as state of the art in 1950 as the cranes at Webb Dock are in 2017, there is an air quality monitoring station.\nPeople living nearby have been complaining for some time about odours from ships berthing at Station Pier. Many cruise ships emit high levels of fine particles and sulphur dioxide, both of which can be harmful to human health and detrimental to local air quality.\nIn response to ongoing complaints, air quality testing was undertaken for the 12 months between January 2016 and January 2017 by the Port of Melbourne Corporation (now Victorian Ports Corporation) in co-operation with the EPA. The data gathered from the monitoring stations has been complemented by air quality diaries kept by people experiencing the problem. The data is being analysed before further steps are taken.\nSimilar problems were experienced by residents in Sydney. Following a high profile campaign, legislation was passed which introduced regulations to limit the sulphur content of fuel of ships calling at Sydney Harbour. Since 1 July 2016, the use of low sulphur fuel (0.1% or less) by cruise ships while in Sydney Harbour has been mandated. Reducing sulphur is the key means of reducing both fine particle and sulfur dioxide pollution.\nSeveral countries operate Emissions Control Areas including the Baltic Sea area, the North Sea area, and designated coastal areas of the United States and Canada.\nIn response to pressure on this issue, the International Maritime Organisation has recently committed to new requirements which will see sulphur emissions fall from the current maximum of 3.5 percent of fuel content to 0.5 percent by 2020.\nThis image of the U.S. battleships of the Great White Fleet which visited Melbourne in 1908 on their round the world mission is a reminder of just how severe the pollution from coal fired ships was. The ships were painted white as you can see, and decorated with gilded scrollwork with a red, white, and blue banner on their bows. How did they keep them so gleaming?\nWill special requirements be put in place for ships berthing at Port Melbourne or will we have to wait for the IMO regulations to take effect in 2020?\nBeacon Cove Neighbourhood Association Air Quality Information for residents\nDr David Stephens The Great White Fleet’s 1908 Visit To Australia","The continuous rise of Nitrogen Oxide (NOx) in the atmosphere is a matter of great concern. Several factors have been stated as the reason for the increase of this element in the air and toxic emission from ships is one of them.\nNitrogen Oxide (NOx) is formed in the atmosphere when fuels such as oil, gas, and coal are burned at a very high temperature. The pollution caused because of NOx is supposed to be known as one of the most dangerous forms, which eventually contributes towards global warming.\nNitrogen Oxide and its effects on the Environment\nA high-level of nitrogen oxide being released into the atmosphere can result in to:\n- Ground Level Ozone\n- Acid Deposition\n- Particulate Matter\n- Indirect Effect to Global Warming\nSignificant contributors to this toxic oxide are factories, coal-burning plants and emissions from motor vehicles and marine diesel engines.\nStudies show that shipping is the source of 18-30% of the world’s nitrogen oxides. Moreover, though several steps have been taken to reduce the emission of NOx from ships, there is still a long way to go to bring down emissions to zero or an acceptable level.\nIMO’s MARPOL Annex VI\nMARPOL (Marine Pollution) is one of International Maritime Organization (IMO)’s regulatory policies focuses on preventing different forms of marine pollution including oil, noxious liquid substances, harmful substances, waste water, garbage and emissions of sulphur oxides and nitrogen oxides at sea.\nMARPOL Annex VI Regulation 13 sets out the mandatory limitations on NOx. The regulation affects not only ships from signatory states but also ships entering MARPOL signatory-member waters.\nMARPOL Annex VI and ECAs\nApart from the mandatory NOx limitations for oceangoing vessels around waters of the member states, IMO also defines Emission Control Areas or ECAs where the MARPOL NOx emission standards will apply.\nThe ECAs includes:\n- Baltic Sea (2006)\n- North Sea (2007)\n- Waters in North American coasts that include waters adjacent to the Pacific coast, the Atlantic/Gulf coast extending to 200 nautical miles from US coast (2010)\n- Waters around Puerto Rico and the US Virgin Islands that are just recently designated by IMO (effective 2014)\n- Norway, Japan and Mediterranean areas are being considered for future ECA proposal.\nIn the past few years, IMO has become stricter in implementing norms to reduce shipping emissions and to minimize the effects of marine pollution.\nIs there a way to reduce NOx emission?\nIn the hope of complying with IMO’s global limits on nitrogen oxides and the enforcement of more stringent standards within the three Emission Controlled Areas marine diesel engine operators, ship owners and seafarers are left with no option but to find the best technology in reducing the amount of NOx from their ship’s exhaust systems and to take steps to make their ship “greener”.\nAmong the various emission control applications Selective Catalytic Reduction (SCR) System comes out to be the most efficient, effectively reducing ship’s NOx emission by 90-95%. By mixing a reagent (SCR 40 – 40% Marine Urea Solution) to the exhaust gas, nitrogen oxides are converted to Nitrogen (N2), water and Carbon Dioxide (CO2).\nGlobal Marine Urea Solution Suppliers\nSome marine SCR System providers offer the supply of urea solution with their application. A known SCR System provider in Japan is Hitachi Zosen while Miratech is from the United States. Both suppliers offer Marine SCR application that complies with Tier III NOx emission standards.\nFor marine urea solution requirements, there are growing urea markets in the US, China and recently in Singapore. Read here for more information regarding marine urea solutions in Singapore.\nSourced by ekomeri.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:b22a2864-24b9-4ac2-87e3-2c574cef0af2>","<urn:uuid:d61eb555-5360-4771-bb06-eedd73ac81f4>"],"error":null}
{"question":"Quick comparison: astrocytes CNS vs satellite cells PNS function?","answer":"Astrocytes in the CNS and satellite cells in the PNS serve similar support roles but in different locations. Astrocytes provide structural support to neurons, help establish blood-brain barrier and electrolyte balance, and may influence synapse formation. Similarly, satellite cells in the PNS provide structure and cushioning to neuronal bodies and mediate the exchange of materials between cell bodies and interstitial fluid.","context":["Study sets, textbooks, questions\nUpgrade to remove ads\nCHAPTER 12 - HSIAO - THE NERVOUS SYSTEM\nTerms in this set (111)\n3 key functions of nervous system\n- reception of sensory input\n- processing and integration of information\n- generation of motor output\ndividing in two systems : name them\n- central nervous system (CNS)\n- peripheral nervous system (PNS)\nresponsible for coordinating all voluntary and involuntary functions\ncentral nervous system\nresponsible for carrying information to the central nervous system, and passing along its commands\nperipheral nervous system\nperipheral nervous system is subdivided into 2 divisions : name them\n- sensory division\n- motor division\ncarries signals from the body to the CNS\nwhat are the 3 sensory division subsets\n- somatic sensory division\n- visceral sensory division\n- enteric sensory division\ncarries signals from superficial structures and the extremities\nsomatic sensory division\ncarries signals from the viscera of the thoracic and abdominal cavities (except below\nvisceral sensory division\ncarries signals from the GI tract and related nerve structures\nenteric sensory division\ncarries commands from the CNS to muscles and glands.\n3 subsets of motor divisions\n- somatic motor division\n- visceral motor division\n- enteric motor division\n(somatic nervous system) stimulates skeletal muscle.\nsomatic motor division\n(autonomic nervous system) stimulates cardiac muscle, smooth muscle, and glands\nvisceral motor division\nstimulates smooth muscle and glands related to the digestive system\nenteric motor division\nThe autonomic nervous system is subdivided into 2 divisions. What are they?\n- sympathetic division\n- parasympathetic division\n- responsible for the fight-or-flight response, a collection of effects preparing the body for physical exertion\n- The division is designed to coordinate the body in acting as a unified whole.\n- responsible for resting and digesting activities\n- acts to conserve energy in the body, and may activate only specific systems in the body\n- a certain tissue that is composed of two cell types\n- pass along information from cell to cell\n2 cell types in nervous tissue\nneurons and neuroglia\ngenerate and conduct nerve impulses, electrical signals that travel along neuronal membranes\n- known as glial cells\n- support, insulate, and protect neurons\n- derive their name from the Greek word for glue\n- found in all parts of the nervous system, and can be distinguished by their size, cellular processes, and intracellular organization\n- generally smaller than neurons, but are 5 to 25 times more numerous.\n- known as perikaryon or soma\n- contains typical cell components\n- aka chromatophilic subst\n- small clusters of rough ER found in the soma and dendrites that produce proteins for use in the entire cell\ngeneral term for any process emerging from the cell body\n3 types of nerve fibers in neuron\nthe receiving or input regions\nusually responsible for propagating output\ncytoskeleton structures extending from the soma into the dendrites and axon\nwhere is the axon normally joined to the cell body?\nthe axon hillock\nThe portion of the axon proximal to the cell body is called what?\npart of the initial segment\nAxons may branch multiple times, forming what?\nEach axon and its collaterals form a ___ ______\na collection of fine branches ending in synaptic bulbs\n7 neurons that are classified by structure and function\n- multipolar neurons\n- bipolar neurons\n- unipolar neurons\n- anaxonic neurons\n- sensory (afferent) neurons\n- motor (efferent) neurons\n- association neurons (interneurons)\n- have multiple dendrites and one axon.\n- most CNS and motor neurons are multipolar\n- have one dendrite and one axon.\n- Found in retina, inner ear, and olfactory region of the brain\n- have several dendrites and an axon fused into a single process.\n- Found as sensory receptors, and as part of cranial and spinal nerves.\nhave dendrites, but no axon\ncarry information towards the brain and spinal cord.\nsensory (afferent) neurons\ncarry information away from the brain and spinal cord\nmotor (efferent) neurons\n- located between sensory and motor neurons.\n- They analyze incoming sensory information, and generate appropriate responses\nassociation neurons (interneurons)\nFour types of neuroglia are found only in the CNS\n- ependymal cells\n- star-shaped cells with many processes\n- has two parts\nwhat are the 2 parts of astrocytes\n- protoplasmic astrocytes\n- fibrous astrocytes\nshort branching processes and are primarily found in gray matter\nlong unbranching processes and are primarily found in white matter.\nwhat do all astrocytes provide?\n- structural support to neurons\n- help establish the blood-brain barrier and electrolyte balance\n- may influence synapse formation\n- similar to astrocytes, but with fewer processes.\n- their processes are used to form myelin sheathes around axons in the CNS.\n- function as phagocytes in the CNS.\n- they remove cellular debris, microbes, and damaged nervous tissue.\n- line the ventricles of the brain.\n- they are believed to be responsible for producing and circulating cerebrospinal fluid\nTwo types of glial cells are found only in the PNS\n- schwann cells\n- satellite cells\n- responsible for encircling axons in the PNS.\n- They can myelinate an axon, or simply surround up to 20 unmyelinated axons to provide support and protection.\n- may also assist in axon regeneration.\n- surround the cell bodies of neurons in the PNS ganglia.\n- provide structural support and mediate the exchange of materials between cell bodies and interstitial fluid\n* - an insulating outer layer for axons\n- They increase nerve impulse conduction rate, and help with neuron maintenance and repair.\n- Schwann cells completely encircle PNS axons, forming a neurilemma around this.\n- Oligodendrocytes do not form a neurilemma around CNS axons.\ngaps between myelin sheathes on the axon\nnodes of ranvier\nnerve tissue organization\na cluster of cell bodies in the PNS\na cluster of cell bodies in the CNS\na cluster of axons in the PNS\na cluster of axons in the CNS\nIn the CNS, nerve tissue is primarily composed of what?\n- white matter\n- grey matter\n- mostly contains myelinated axons\n- good for communications\nmostly contains all other structures\nthe messages that neurons pass along\nwhat is the plasma membrane of neuron designed for\npores that allow the movement of ions across the plasma membrane, participate in conveying nerve impulses\nshort-range electrical signals, as they cannot reach more than a few millimeters\nwhat is it called when a neuron at rest maintains a difference in electrical charge across the plasma membrane\nresting membrane potential\nFour ion channels are involved in the conduction of nerve impulses : NAME THEM\n- leak channel\n- ligand-gated channel\n- mechanically gated channel\n- voltage-gated channel\nrandomly opens or closes to allow passage of the specified molecule\nopens or closes in response to the binding of a specific molecule, or ligand.\nopens or closes in response to mechanical stimulation\nmechanically gated channel\nopens or closes in response to changes in the membrane potential\n- a carrier protein that uses ATP to move sodium (Na+) and potassium (K+) ions across the membrane\n- actively transports 3 sodium ions for every 2 potassium ions\ninitial starting point\n- The restoration of the initial electrical charge\n- potassium pouring out\nsodium pouring in\nwhat happens after repolarization\nsodium and potassium ions are misplaced\noccurs when a neuron is in the middle of generating an action potential\n- the neuron is unable to initiate a new action potential at all.\n- The channel proteins allowing sodium ions to pass through the membrane are inactivated, and can not reopen\nabsolute refractory period\nthe neuron will initiate a new action potential only in response to a stronger stimulus\nrelative refractory period\na step-by-step depolarization and repolarization of each segment of the plasma membrane\noccurs in myelinated axons\nMyelinated regions do not require depolarization and repolarization, and conducts what?\nthe nerve impulse more quickly\nFewer sections of the membrane undergo depolarization and repolarization, resulting in what?\nthe more myelinated an axon is, the ______ it will propagate a nerve impulse\nThe larger the axon's diameter, the _____ it will propagate a nerve impulse.\nThe higher the body temperature, the ______ an axon will propagate a nerve impulse\n- transmission of a nerve impulse between neurons requires chemical signals.\n- These chemicals are called _____\nthe junction between a neuron and another cell.\nGap in the synapse is called\nthe postsynaptic neuron is more likely to depolarize (make more sensitive)\nthe postsynaptic neuron is less likely to depolarize (make less sensitive)\n2 forms of summation\n- spatial summation\n- temporal summation\n- the activity of all synapses on a neuron allows the control of neuronal responses on a very fine level.\n- This effect is called\ninvolves the integration of messages from multiple neurons\ninvolves the integration of multiple messages from a single neuron\n- found at both excitatory and inhibitory synapses.\n- It is found at neuromuscular junctions with an excitatory role.\ninactivation occurs through\nBoth NO and CO have what kind of effects\n- Most of the excitatory synapses in the CNS use _____\n- this activity is inhibited through re-uptake back into the synaptic bulb\nthe most common CNS inhibitory neurotransmitter\nγ-aminobutyric acid (GABA)\nused in the CNS to regulate sleep cycles, and is also involved in inflammation responses\nused in the digestive system to control intestinal movement, and in the CNS to regulate sensory perception, mood, appetite, and sleep\nare peptides that can function as neurotransmitters in excitatory or inhibitory roles\ninto which the new axon process may extend\nthe _______ process may not re-attach in the original location, resulting in potential loss of function\n- Formation of new synapses, remodeling of old synapses, and formation of new receptors may occur\n- Long-term memory usually involve changes in ___________\nSets with similar terms\nAnatomy Lab 14: Intro to the Nervous System\nAnatomy & Physiology I Chapter 12\nAnatomy & Physiology I Chapter 12\nmt vocab for nervous sys\nOther sets by this creator\nA&P II quizzes\nchapter 17 - endocrine system (weintraub)\nsoc - chapter 12\nSOCIOLOGY - CH 5\nRecommended textbook solutions\nHole's Human Anatomy and Physiology\nDavid N. Shier, Jackie L. Butler, Ricki Lewis\nAnatomy and Physiology Coloring Workbook\nElaine N. Marieb\nAnatomy and Physiology: The Unity of Form and Function\nHuman Anatomy and Physiology Laboratory Manual, Cat Version\nElaine N. Marieb, Lori A. Smith, Susan J. Mitchell","18.7: Glial Cells\nGlial cells are one of the two main types of cells in the nervous system. Glia cells comprise astrocytes, oligodendrocytes, microglia, and ependymal cells in the central nervous system, and satellite and Schwann cells in the peripheral nervous system. These cells do not communicate via electrical signals like neurons do, but they contribute to virtually every other aspect of nervous system function. In humans, the number of glial cells is roughly equal to the number of neurons in the brain.\nGlial Cells of the Central Nervous System\nGlia in the central nervous system (CNS) include astrocytes, oligodendrocytes, microglia, and ependymal cells. Astrocytes are the most abundant type of glial cell and are found in organized, non-overlapping patterns throughout the brain, where they closely associate with neurons and capillaries. Astrocytes play numerous roles in brain function, including regulating blood flow and metabolic processes, synaptic ion and pH homeostasis, and blood-brain barrier maintenance.\nAnother specialized glial cell, the oligodendrocyte, forms the myelin sheath that surrounds neuronal axons in the CNS. Oligodendrocytes extend long cellular processes that wrap around axons multiple times to form this coating. Myelin sheath is required for proper conduction of neuronal signaling and greatly increases the speed at which these messages travel.\nMicroglia—known as the macrophages of the CNS—are the smallest glial cell type and specialize in phagocytosis of both pathogens and debris. They protect the CNS from infectious agents and toxins and prune synapses during development. Although microglia are considered glial cells, they have a unique and separate origin compared to other glial cell types. Astrocytes and oligodendrocytes are produced by radial glia, whereas microglia originate from the yolk sac and migrate into the embryo early in embryonic development.\nLastly, ependymal cells are cube-shaped cells with cilia-like protrusions that line the ventricles, where they produce cerebrospinal fluid (CSF). Ependymal cells form a barrier between the brain and the CSF, filtering out potentially harmful substances. Like astrocytes and oligodendrocytes, ependymal cells originate from radial glia found near the lateral ventricles.\nGlial Cells of the Peripheral Nervous System\nIn the peripheral nervous system (PNS), similar yet distinct types of glial cells exist. For example, functions performed by CNS astrocytes are accomplished in the PNS primarily by satellite cells, glial cells that provide structure, cushioning, and nutrients to the neuronal bodies they associate with. Another PNS glial cell, the Schwann cell, functions like CNS oligodendrocytes by forming a myelin sheath around neuronal axons. Like myelination in the CNS, PNS axon myelination provides necessary insulation and conductivity for the proper transmission of electrical signals.\nImportance of Glia in Health and Disease\nGlial cells are critical nervous system protectors and regulators. Not only do they maintain homeostatic conditions and contribute to routine brain function, but they also respond to nervous system injury, infection, and disease. In addition, glia perform critical functions during embryonic development of the nervous system. These cells even contribute to the removal of unnecessary neuronal connections, a process called synaptic pruning. Due to the importance of glia in numerous aspects of brain function, defects in one or more glial cell populations can lead to severe and debilitating neurological conditions, including developmental disorders, Alzheimer’s disease, Parkinson’s disease, multiple sclerosis, and many others.\nDuring development, glial cells provide a scaffolding for neurons to properly migrate on and grow out their axons. Later in life, trauma or neurodegenerative diseases can cause loss of neuronal connections that cannot be regenerated, leading to impaired functioning or paralysis."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5ab2ede6-6cbd-4b60-9d3f-c71d2da36e91>","<urn:uuid:603afa7d-e116-41bd-b468-3b9dc041247a>"],"error":null}
{"question":"Got diabetes and starting insulin soon. How long does it take nurses to teach proper injection technique?","answer":"Teaching insulin administration during scheduled insulin doses takes only 5 to 10 minutes. Over the course of a hospital stay, these brief teaching sessions add up and can enable patients to self-administer insulin safely. This approach is much less stressful than teaching patients everything they need to know on discharge day.","context":["Despite the benefits of insulin therapy, many people with diabetes don’t adhere to treatment. Some avoid insulin therapy or refuse to start it. A recent study found more than a third of the roughly 25 million Americans with diabetes don’t take insulin as prescribed and 20% intentionally skip some doses, which can lead to serious health risks.\nA study of nurses and physicians found nurses can play a more important role in improving insulin adherence. As nurses, we’re well positioned to help patients improve adherence because we play a key part in patient education, spend proportionately more time with patients than do other healthcare providers, and may be more familiar with a patient’s health history. Although time constraints may pose a challenge for nurses, if we ask the right questions during outpatient visits and hospital stays and address patient concerns, we can help motivate patients to take control of diabetes and adhere to insulin therapy over the long term.\nUsing the strategies described in this article, you can help patients better understand and address the challenges of insulin therapy. Patients who receive more comprehensive information about their care are better able to monitor and manage their condition at home.\nTailor the treatment plan\nWhen talking with patients, ask about their lifestyle and concerns about treatment. Then work with the healthcare provider who manages their insulin to develop a treatment plan customized to each patient’s needs and abilities. Communication is especially crucial before and during initiation of insulin therapy. But you also should continue to communicate with patients after treatment begins or when it’s modified. In many cases, patients don’t ask for help or discuss concerns related to self-care while in the hospital, unless their admission is diabetes related. But a hospital stay is a good opportunity to ask about treatment challenges or elicit the patient’s questions.\nYou may want to consider using a motivational interview to encourage patients to identify and resolve concerns about their insulin regimen. The question-and-answer format of the motivational interview helps patients identify, discuss, and address challenges. For example, ask patients to describe insulin benefits and drawbacks and which factors typically cause them to change their injection behavior. Then offer positive reinforcement. Based on their answers, collaborate with the healthcare provider who manages the patient’s insulin to make appropriate changes to the treatment plan.\nBarriers to adherence\nMany patients find insulin therapy complicated, inconvenient, and painful. Some skip insulin doses or stop taking insulin altogether because of a mental barrier, such as fear of needles. To empower them and help them overcome their fears, provide targeted information. For example, various strategies can ease the fear of injection. To teach the proper technique, use an injection pad; have the patient practice inserting the needle without injecting medication. (See Six weeks in their patients’ shoes by clicking the PDF icon above.)\nComforting language can ease patient anxiety, too. When providing injection instructions, avoid such words as “shooting,” “spearing,” and “stabbing”; these may increase anxiety. Also, inform patients that certain techniques can reduce injection pain. (See How to reduce pain from insulin injection by clicking the PDF icon above.)\nChoosing the proper needle for the patient can reduce both anxiety and injection-site discomfort. Researchers have developed a shorter 4-mm pen needle with a narrow gauge (32G) that has been shown to reduce injection pain and anxiety in insulin delivery, especially in children. These smaller needles are less intimidating and less likely to reach muscle tissue in leaner patients. (In contrast, traditional longer needles commonly are injected into muscle tissue, which increases insulin absorption, negatively affecting both pain and blood glucose control.) Insulin pen needles are available in 4-mm (32G), 5-mm (31G), 8-mm (31G), and 12.7-mm (29G) sizes. Insulin syringes are available in 6-mm (31G), 8-mm (31G), and 12.7-mm (31G) sizes.\nAlthough some studies indicate patients can reuse needles, this practice could increase injection pain. (See Don’t reuse insulin needles or pen needles by clicking the PDF icon above.)\nDosing using pen needles makes injections easier for some patients, and studies report greater patient satisfaction with prefilled disposable insulin pens. In a randomized trial of 121 patients with type 1 or 2 diabetes, 74% preferred a prefilled insulin pen device over vials and syringes. In a crossover study of 60 patients older than age 60, 54 (90%) found the functioning of a prefilled insulin pen easy to understand and preferred it.\nNeeded: A renewed focus on patient education\nDespite widespread insulin use, several studies show patient education on many aspects of diabetes care (including drug-delivery options) is inadequate for many patients. For education to be effective, they should receive comprehensive information on the following:\n- review and “teach back” of the insulin regimen\n- options regarding different needle lengths\n- availability of insulin pens with shorter needles\n- injection-site selection, rotation, and care\n- proper insulin administration technique\n- how to identify and avoid injection-site complications\n- safe disposal of used sharps.\nBesides talking through these issues with a nurse, patients should have this information available for reference at home.\nTake advantage of teachable moments\nWith the right educational resources, you can take advantage of many teachable moments during patient interactions. In the hospital, insulin is given four times each day; this creates four teachable moments.\nA misconception exists that teaching a patient how to administer insulin takes more time than a nurse may have. But delivering teaching during scheduled insulin doses takes only 5 to 10 minutes. Over the course of a patient’s hospital stay, these 5 to 10 minutes per administration add up and can pay high dividends by enabling patients to self-administer insulin safely. This approach is much less stressful than teaching patients everything they need to know on discharge day.\nIn addition, healthcare settings should have a range of diabetes-care information material available to give to patients with questions or concerns. Materials should be written in language that’s easy to read and understand. For example, the BD Getting Started™ Take Home Kit contains an instruction book, glucose tablets, alcohol wipes, five pen needles or insulin syringes, and a personal diabetes care card.\nEmphasize overall self-care\nDiabetes self-management education is a collaborative process through which patients with diabetes gain the knowledge and skills they need to modify their behavior and self-manage their condition. According to the American Association of Diabetes Educators (AADE), when patients understand how best to manage their health, they can improve their health status and quality of life and reduce the need for costly health care. (See Self-care behaviors related to diabetes management by clicking the PDF icon above.) Also, suggest a family member or close friend become involved in the patient’s self-care. At-home support may help patients better adhere to treatment.\nTake steps to make a difference\nBy addressing patient concerns both proactively and reactively, you can help patients continue insulin therapy successfully, with less pain and anxiety. Establishing a team of diabetes nurse champions dedicated to self-management education can make these goals a priority. A clearly defined diabetes-education model with policies and protocols helps ensure nurses are familiar with and can easily reference the important steps necessary to empower patients to self-manage their diabetes.\nTo take these steps, nurses need to have adequate training to understand challenges to patient acceptance and adherence, as well as ways to address them. Training also helps nurses expand their focus on education without enduring even greater time constraints on their patient interactions. Finally, communicating with peers about best practices in diabetes self-management education can help make your teaching more effective and help you recognize when a patient needs additional teaching.\nA look at the reuse of insulin needles. Becton Dickinson.com. 2006. www.bd.com/us/diabetes/download/Reuse_White_Paper.pdf. Accessed July 19, 2013.\nAmerican Association of Diabetes Educators. Measurable behavior change is the desired outcome of diabetes education. (n.d.)\nwww.diabeteseducator.org/ProfessionalResources/AADE7. Accessed July 16, 2013.\nFrid A, Hirsch L, Gaspar R, et al.; Scientific Advisory Board for the Third Injection Technique Workshop. New injection recommendations for patients with diabetes. Diabetes Metab. 2010;36(suppl 2):S3-S18.\nGibney MA, Arce CH, Byron KJ, et al. Skin and subcutaneous adipose layer thickness in adults with diabetes at sites used for insulin injections: implications for needle length recommendation. Curr Med Res Opin. 2010;\nHirsch LJ, Gibney MA, Albanese J, et al. Comparative glycemic control, safety and patient ratings for a new 4 mm X 32G insulin pen needle in adults with diabetes. Curr Med Res Opin. 2010;26(6):1531-41.\nInstitute for Safe Medication Practices. Ongoing concern about insulin pen reuse shows hospital need to consider transitioning away from them. February 7, 2013. www.ismp.org/newsletters/acutecare/showarticle.asp?id=41. Accessed July 19, 2013.\nMeece J. Effect of insulin pen devices on the management of diabetes mellitus. Am J Health Syst Pharm. 2008;65(11):1076-82.\nPeyrot M, Barnett AH, Meneghini LF, et al. Factors associated with injection omission/non-adherence in the Global Attitudes of Patients and Physicians in Insulin Therapy study. Diabetes Obes Metab. 2012;14(12):1081-7.\nPeyrot M, Rubin RR, Kruger DF, et al. Correlates of insulin injection omission. Diabetes Care. 2010;33(2):240-45.\nRowe K, Weatherall R. Why do patients fail to follow long term treatment? Nurs Times. 2011;107(13):12-5.\nSiminerio LM, Funnell MM, Peyrot M, et al. US nurses’ perceptions of their role in diabetes care: results of the cross-national Diabetes Attitudes Wishes and Needs (DAWN) Study. Diabetes Educ. 2007;33(1):152-62.\nSiminerio L, Kulkarni K, Meece J, et al.; American Association of Diabetes Educators. Strategies for insulin injection therapy in diabetes self-management. 2011. www.diabeteseducator.org/export/sites/aade/_resources/pdf/research/AADE_MedEd.pdf. Accessed July 20, 2013.\nUlrich PA, Abner N. Diabetes under control: Meter, meds, meals, move, and more. Am J Nurs. 2010;110(7):62-5.\nKellie Antinori-Lent is a diabetes nurse specialist at the University of Pittsburgh Medical Center Shadyside in Pittsburgh, Pennsylvania."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c43e551d-0390-4151-bfda-25bef15a3bf5>"],"error":null}
{"question":"Could you compare how nanocapsules made of DNA and quantum dots are used for scientific research purposes, focusing on their size characteristics and delivery mechanisms?","answer":"DNA nanocapsules and quantum dots serve different research purposes with distinct size characteristics and delivery mechanisms. DNA nanocapsules are 20 nanometers across (1000 times smaller than a human hair) and are designed to deliver small molecules directly into cells, with the ability to release their payload when exposed to light. They can be targeted to specific cells using molecular 'addresses.' In contrast, quantum dots are even smaller, ranging from 5 to 25 nm for conventional Cd-based QDs, and are used for imaging and measurement purposes rather than delivery. They emit different wavelengths of light based on their size, with no payload delivery function, making them useful for biomedical imaging and screen development applications.","context":["Using a unique method involving nanocapsules made out of DNA, researchers studied a little-understood signaling process (lit up in color in the neurons above) that plays a key role in the brain.\nCredit: Veetil et. al\nA team of scientists from the University of Chicago designed a way to use microscopic capsules made out of DNA to deliver a payload of tiny molecules directly into a cell. The technique, detailed Aug. 21 in Nature Nanotechnology, gives scientists an opportunity to understand certain interactions among cells that have previously been hard to track.\nCells talk to each other in chemical whispers that occur too fast for scientists to accurately study, Krishnan said. Her team aimed at one class of such chemical communications, known as neurosteroids.\nScientists know neurosteroids are involved in neuronal health, but they’re difficult to study because they operate on hair triggers. “The moment you add a neurosteroid, the neuron’s already fired,” Krishnan said.\nResearchers want a blow-by-blow account of what happens in the cell as the neurosteroid plays its part in the intricate signaling dance inside a neuron. To do so, they needed to get the neurosteroids to the cell inside a little package, release them on cue and then track what happens. But it’s difficult to make a delivery system so airtight that it doesn’t leak a couple of molecules before everything’s set up.\nFind your dream job in the space industry. Check our Space Job Board »\nFor this task, Krishnan had a solution: Her lab builds tiny machines out of DNA. It’s a good material because like a set of Legos, it has standard interlocking pieces that make it easy to build into configurations. And since it’s made out of parts already in the body, it can dissolve harmlessly once its purpose is achieved.\nThe lab made tiny structures — icosahedral, like a 20-sided die — with two halves that clamp together around a payload of molecules to form a capsule. Each capsule is just 20 nanometers across; that’s a thousand times smaller than the width of a human hair.\nThe next step was to send them to key locations inside the body by finding the right molecular “addresses” to particular cells, and gluing them onto the capsule. (Scientists find these addresses by studying how viruses and bacteria hone in on particular parts of the body.) To release the payload from the capsule, the scientists simply shine a light on the cells.\nThey tested and confirmed the system in worms and were able to measure the kinetics of the neurosteroids, previously an elusive process, the authors said.\nSomeday, Krishnan said, the technology could be used to deliver drugs or treatment to certain parts of the body, but theirs was a case study to explore the method as a way to better understand our own bodies and how they work.\nCo-authors of the study include Sangram Sisodia, the Thomas Reynolds Sr. Family Professor of Neurosciences at the University, and UChicago researchers Aneesh Veetil, Kasturi Chakraborty, Kangni Xiao and Myles Minter.\nStory Source: Materials provided by University of Chicago Original written by Whitney Clavin.Note: Content may be edited for style and length.\nAneesh T. Veetil, Kasturi Chakraborty, Kangni Xiao, Myles R. Minter, Sangram S. Sisodia, Yamuna Krishnan. Cell-targetable DNA nanocapsules for spatiotemporal release of caged bioactive small molecules. Nature Nanotechnology, 2017; DOI: 10.1038/nnano.2017.159","Extensive analysis into fluorescent semiconductor nanoparticles, or quantum dots (QDs) as they are commonly known, is consistently expanding into a significant field of research.\nSome of the potential diverse and varied applications range from audiovisual and biomedical imaging to photovoltaic development. Fields in which optical properties rely heavily on QD size (usually, ≤30 nm), a good grasp of characterizing their acute dimensions is key.\nOne technique that presents itself as well-suited to such characterization is Dynamic Light Scattering (DLS), as it is a process that enables a representative size measurement of small nanoparticles (NPs) in suspensions.\nHowever, the QDs absorption properties can be difficult to measure for the range of typical DLS lasers. Therefore, this study presents a DLS set-up developed specifically for Cd-based QD size measurement as a way of bypassing such difficulties.\nThe measurement that takes place within the suspension vial to limit the toxic QD handling can be achieved by inserting a (NIR) 780-nanometers laser in the DLS apparatus. This enables accurate size determination of red-emitting QDs, even when their absorption band goes up to 650 nm.\nThe analysis of a green-emitting QD suspension shows that such a set-up can also be used for more classical DLS size measurements, even where the NP size is quite small (≃9 nm) compared to the NIR wavelength.\nThis study argues that extending the range of DLS is possible and has uses for fluorescent particles with distinct absorbance in the visible range, making it an encouraging characterization method for a fluorophore-based system, especially in the biomedical field.\n1.1 Quantum Dots (QDs)\nAssembled from semiconductor materials (typically, transition metal selenides, sulfide, or tellurides such as CdSe, CdS, ZnS, CdTe), quantum dots (QDs) are nanocrystals that induce unique optical properties according to size1, 2.\nAt the nanoscale, quantum confinement is apparent in these particles. The result is a discretization of the energy levels as well as in the material’s luminescent traits. Principally, quantum dots display fluorescence in the visible range of light when excited with UV (Figures 1 and 2).\nAs the gap between the conduction and the valence bands depends on NP size, the wavelength of the light emitted is linked precisely to the size of the nanoparticle. This makes it possible to tune the emission by altering the QD size toward a particular wavelength.\nThe wavelength of the QD emission (i.e., the color of the light produced), will be associated with the size polydispersity in the suspension. Indeed, a specific wavelength will be generated by each class of size.\nFor an emission in the visible range of light, conventional Cd-based QDs tend to have a size range between 5 and 25 nm. Therefore, because of these properties, QDs make up a very interesting field of research, which can lead to a wide range of applications such as biomedical imagery, screen development, or even photovoltaic material.\nFigure 1. Scheme of the fluorescence process in quantum dots. An exciton (electron/hole pair) is formed with the UV excitation and, during its recombination, produces light in the visible range.\n1.2 Dynamic Light Scattering (DLS)\nTo obtain accurate information with regards to the NP size, Dynamic Light Scattering (DLS) is a method based on the measurement of the scattered light fluctuations through colloidal suspension3.\nBy way of an analysis of the Brownian motion of the particles, it is possible to acquire the NP diffusion coefficient (D) and, subsequently, the hydrodynamic diameter (d) of the NPs. This is by virtue of the Stokes-Einstein equation:\nWhere D is expressed in m2.s−1, k is the Boltzmann constant (in J.K−1), T is the medium temperature (in Kelvin), η is the medium viscosity (in J.s.m−3), and d is the diameter of the particle (in m).\nFluctuations of the scattered laser light intensity can be correlated with a DLS device with an exponential-like correlogram, whose decay rate can be related to the diffusion coefficient and the scattering angle at intervals of the collected beam direction and the incident laser beam direction.\nOngoing DLS analysis can then generate a value of spherical NP size, as well as a perception of the polydispersity in sizes for a given colloidal suspension. Therefore, the DLS technique has been identified as an adequate characterization method for NP size measurement, often associated with parallel imaging techniques, such as Electronic Microscopies (TEM, SEM).\nWhile electronic microscopy can display NP shapes and sizes at the local scale, DLS provides a better indication of the NP size and size polydispersity over the whole suspension.\n1.3 DLS for QD Size Measurement\nDLS has demonstrated its value in the characterization of various nanosized species (metallic nanoparticles, vesicles, protein, and polymeric compounds). However, technical concerns arise regarding its use for QDs, despite the extensive advantages it could deliver in the characterization of such nanocrystals.\nIn the first approximation, where DLS is concerned, the light scattering cross-section is comparable to d6. This means the smaller the particle, the dimmer the signal tends to be.\nTherefore, it can be a difficult task to detect the size of QDs, especially the smaller ones (2-10 nm), even more so if larger objects (such as aggregates) are present in the suspension.\nMoreover, the high toxicity of Cd-based QDs must also be accounted for. The control of such species should be limited as much as possible for safety concerns.\nThe final issue is directly related to the QD optical properties; as they emit light in the visible range, they also bear a strong absorbance band that is dependent on the QD size. This band drifts towards higher wavelengths as the size increases.\nFor example, in the case of red-emitting CdSe QDs (typical size around 15 nm), the absorbance band is based at around 600-650 nm, the conventional wavelength of a typical DLS laser (635 nm). A significant decrease in the recovered signal is then relative to this absorbance, and thus, leads to a difficult size measurement.\nTo overcome such complications, an unorthodox set-up based on the use of a Near-Infrared (NIR) laser in a DLS equipped with a remoted head is proposed as the core of our classical DLS experiment.\n2 Experimental Protocol\n2.1 Colloidal Suspensions\nIn this study, two batches of [email protected] core/shell QDs have been analyzed. The first batch emits a glowing green-light fluorescence, while the second is emitting red-light (Figure 2). For continuity and descriptive purposes, these batches will be named “green QDs” and “red QDS,” respectively.\nFigure 2. Pictures of both green- and red-emitting QDs suspensions in natural visible light and under UV exposition.\nEach QD batch was initially supplied in powder form. Preparation of the QD suspensions (1 g/L) was achieved by dissolving the powders in toluene. Sonication (10 min, with an Emmi H22 sonicating bath; 120 W) was performed before DLS measurements were taken.\n2.2 UV-visible and Fluorescence Spectroscopies\nAnalysis of the QD suspension absorbance and fluorescence bands was performed at Laboratoire de Chimie des Polymeres Organiques (LCPO) in Pessac, France, using a Spectra Max M2 from Molecular Devices.\n2.3 Transmission Electronic Microscopy (TEM)\nUsing a LVEM-5 bench top TEM, TEM analysis was carried out at the laboratoire d’applications de Cordouan Technologies.\n2.4 DLS Set-up\nDynamic Light Scattering measurements were taken with a Vasco KinTM apparatus, developed by Cordouan Technologies, using either the ‘conventional’ 635-nanometers laser diode or a 780 nanometers laser (NIR) for comparison.\nMeasurements were conducted at a scattering angle of 170 (back-scattering) using the in-situ head. The remote head (in situ head) enables DLS measurements to be taken within the confinement of the NP container without the need to relocate the species in a particular cell.\nThis, then, prevents the lengthy manipulation of the toxic Cd-based QDs. A repeated multimodal analysis algorithm (SBL), developed by Cordouan Technologies, has been utilized to assess the size of the QDs for which a series of notable features were provided regarding the relevancy of the measure and analysis.\nThe β parameter (0≤ β ≤ 1) was operating at an indication of the ratio signal/noise. The closer β is from 1, the more accurate the measure will be, but a β close to 0 is indicative of a strong noise in the signal, making the measurement more difficult to interpret.\nAnother feature concerning the quality of the fit analysis could be seen in the residues as they corresponded to the difference between the algorithm fitting curve and the correlogram acquired from the measure.\nSmaller residues (arbitrarily, residues are said “good” when ≤ 0.01) made it more possible for the DLS to provide size values of improved value. With these tools, the prediction of the quality of DLS characterization was made possible.\nFigure 3. Picture of the DLS set-up, with the Vasco Kin in situ head, for green QD size measurement.\n3.1 Green QDs\nThe initial sample analyzed with this DLS set-up was a suspension of green-emitting [email protected] quantum dots in toluene. The average size of these particles was calculated using TEM, around 7 nm (Figure 4).\nFigure 4. Green QDs characterization.\nThe absorbance band for this suspension comes in at 500 nm, which should not impede a classical DLS laser, and even less so with the 780-nanometers laser wavelength.\nFigure 5 illustrates the correlogram recorded for the green QDs when analyzed with both 635-nanometers and 780- nanometers lasers. The Kin’s laser (635 nm) has an improved β, which was to be expected because the higher wavelength laser is likely to have a stronger noise.\nFigure 5. DLS correlograms for the green QDs analysis, either with the classical 635-nanometers laser or the 780-nanometers laser.\nTable 1. β and residues values for the DLS measurements on the green QDs suspension, made with both laser wavelengths.\n||Laser 635 nm\n||Laser 780 nm\nTable 1 compiles the value of there β values, as well as the residues values. Early on, it is noticeable that the measurements made by DLS, regardless of the laser wavelength and the algorithm fits, are relevant because the residues are very small (≤0.007).\nSize distribution analysis made with the 635-nanometers laser and the 780-nanometers laser (Figure 6) displays similar results. It can be noted that a difference of 4 nm can be established with the TEM size measurements.\nFigure 6. DLS size distribution of the green QDs either with the classical 635-nanometers laser or the 780-nanometers laser.\nTable 2. DLS size measurements for the green QDs for both laser wavelengths.\n||Laser 635 nm\n||Laser 780 nm\nThe stabilizing agents surrounding the NPs are the cause of the difference of a few nanometers. Therefore, with a thickness of up to 6 nm in diameter, such ligands can form a surface layer around the NPs4.\n3.2 Red QDs\nThe same NIR-laser DLS technique was used to investigate larger red-emitting QDs. Due to their increased size, seen in TEM at around 12 nm, they become easier to detect as their absorbance band (600-650 nm) is now in the usual DLS laser wavelength (Figure 7).\nFigure 7. Red QDs characterization.\nBy using a higher wavelength, it is possible to prevent any absorption of the laser. Moreover, the NIR-laser is a solution adapted to tackle absorption issues even though the collected signal remains noisy.\nFigure 8 shows the correlograms acquired with both types of laser. The β values (shown in Table 3) show that the 635-nanometers laser is of no use for the analysis of QDs due to the high-noise issues.\nFigure 8. DLS correlograms for the red QDs analysis, either with the classical 635-nanometers laser or the 780-nanometers laser.\nTable 3. β and residues values for the DLS measurements on the red QDS suspension, made with both laser wavelengths.\n||Laser 635 nm\n||Laser 780 nm\nHowever, the 780-nanometers laser can generate a proper correlogram with a β at an acceptable value, which can be later be analyzed for size measurement. No appropriate fit can be obtained via the measurements taken with the 635-nanometers laser.\nThis tendency is confirmed by the residues for both algorithms and underlines the importance of the measurement done with the NIR laser. The various size distributions marked by intensity for both lasers are shown in Figure 9.\nFigure 9. DLS size distribution of the red QDs with the 780-nanometers laser.\nTable 4. DLS size measurements for the red QDs for both laser wavelengths.\n||Laser 635 nm\n||Laser 780 nm\nWhile size values with the 365-nanometers laser measurement (Table 4) could not be recovered, those made with the 780-nanometers laser lead to a QD size of 15 nm. This was close to the size values obtained using TEM measurement, especially when taking into account the probable ligand layer surrounding the nanocrystals.\nConclusively, it has been made apparent that the NIR-laser can be a valuable asset in the DLS measurement of Cd-based QDs. Utilizing a laser with a wavelength set at 780 nm inhibits most of the absorption effects in the case of red-emitting QDs.\nIndeed, in order to get a strong signal for size-retrieving, an algorithm that has been adapted for such a system proves to be useful. Although the values given by DLS remain larger than those witnessed in TEM (difference attributed to an organic layer of stabilizing agents, not seen in the TEM), the same characteristic QD sizes can be retrieved.\nThe comparison with the green-emitting QDs demonstrates that the DLS is already an effective instrument for characterizing small objects (≤10 nm), such as nanocrystals. Furthermore, the addition of the NIR laser can extend its potential for absorbing materials.\nWith the careful set-up of the NanoKinTM device, which has a remoted head and a direct in situ measurement inside the sample-containing vial, it has proven to be a powerful tool for restricting the handling of toxic materials.\nWhile this study makes the first nascent steps in the DLS characterization of fluorescent nanoparticles, the same methodology has since been applied on red-emitting gold nanoclusters (3 nm in size), with very promising results.\nThe later results ensure the long-lasting viability of this characterization technique and enhance the potential for DLS. Some of the prospective applications include bio-medical imagery/treatments, and the characterization of strong visible-absorbing particles such as plasmonic NPs or various dyes/fluorophores used in the biological field.\nFlorian Aubrit*, David Jacob, Sylvian Boj\n* actually at Organic Polymers Chemistry Laboratory (LCPO), CNRS-University of Bordeaux, 33600 Pessac, France\nReferences and Further Reading\n- C. B. Murray, C. R. Kagan, and M. G. Bawendi. Synthesis and Characterization of Monodisperse Nanocrystals and Close-Packed Nanocrystal Assemblies. Annual Review of Materials Research, 30(1):545–610, 2000.\n- M. Nasilowski, P. Spinicelli, G. Patriarche, and B. Dubertret. Gradient CdSe/CdS Quantum Dots with Room Temperature Biexciton Unity Quantum Yield. Nano Letters, 15(6):3953–3958, 2015.\n- B. J. Berne and R. Pecora. Dynamic Light Scattering. Willey, New York, 2nd edition – 2000.\n- F. Aubrit. Films nanocomposites plasmoniques auto-assembles´ . PhD thesis, Universite de Bordeaux, 2017.\nThis information has been sourced, reviewed and adapted from materials provided by Cordouan Technologies.\nFor more information on this source, please visit Cordouan Technologies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:dcfd411b-3e99-454c-8c44-ae20dbaf3254>","<urn:uuid:bc0bb0c1-c9a2-4943-8111-74b1971f2b01>"],"error":null}
{"question":"What are the key differences between primary fermentation and kettle souring in terms of pH targets and oxygen requirements? Please provide specific pH values and explain the oxygen management approach for each method.","answer":"In primary fermentation, brewers target an initial pH of around 5.2, and oxygen is actually required at the start as yeast quickly utilize it to produce sterols for culture expansion. Conversely, in kettle souring, while the initial pH target is also 5.2, the process aims to reach a much lower final pH of 3.2-3.4. Regarding oxygen, kettle souring requires excluding oxygen as much as possible, often using inert gas to blanket the kettle top, as oxygen can promote unwanted bacteria. This is opposite to primary fermentation, where oxygen is necessary at the beginning before the yeast switches to the anaerobic phase.","context":["Fermentation is the heart of the brewing process. During fermentation, wort created from raw materials is converted to beer by yeast. Fermentation is usually divided into three stages: primary, secondary, and conditioning (or lagering). Fermentation is when yeast produce all of the alcohol and aroma and flavor compounds found in beer. Manipulation of temperature, oxygen levels, and pitch rate as well as yeast strain selection will all dramatically affect the production of aroma and flavor compounds produced during fermentation.\nThe primary stage of fermentation begins when the yeast is introduced into cooled, aerated wort. The yeast quickly utilize the available oxygen to produce sterols, a vital compound for culture expansion. When the oxygen is gone, the yeast switch to the anaerobic phase where the majority of wort sugars are reduced to ethanol and CO2. Yeast growth occurs during primary fermentation. The extent and rate of yeast growth is directly related to the production of aroma and flavor compounds.\nPrimary Fermentation Summary:\n• Depletion of dissolved oxygen\n• Acidification / reduction in pH\n• Yeast growth or culture expansion\n• Ethanol and CO2 production\n• Production of flavor compounds such as esters, diacetyl, sulfur containing compounds, etc.\n• Consumption of most wort sugars\nThe temperature of the primary fermentation should be regulated according to the desired flavor and aroma profile. The following is a guideline:\nPrimary Fermentation Temperatures:\n• Ales: 62°F – 75°F (17°C – 24°C)\n• Lagers: 46°F – 58°F (8°C – 14°C) *Note: Lager fermentations can be started warmer (~60°F, 15.5°C) until signs of fermentation (gravity drop, CO2 production, head formation) are evident. Cool to desired fermentation temperature once signs of fermentation are observed.\n• Wheat and Belgian styles: 62°F – 85°F (17°C – 29°C)\nThe secondary stage of fermentation refers to the stage of fermentation after the majority of the wort sugars have been consumed and there is a sharp decrease in the rate of fermentation. During this period, most of the final sugars are depleted and some secondary metabolites are converted by the yeast. Yeast flocculation and settling begins to occur due to the increase in alcohol content and the depletion of sugar and nutrients. Diacetyl reduction takes place during secondary fermentation and during the diacetyl rest that some brewers incorporate into the secondary stage of fermentation.\nSecondary Fermentation Summary:\n• Decreased rate of ethanol and CO2 production\n• Diacetyl Conversion\n• Reduction of some flavor compounds by yeast metabolism or CO2 scrubbing\n• Terminal gravity is reached\n• Yeast flocculation and settling begins\nSecondary Fermentation Temperatures:\n• Ales: Same as primary fermentation (higher temperatures will increase diacetyl reduction rates)\n• Lagers: 40°F - 60°F (4°C - 15°C). Some brewers allow the beer to increase in temperature to speed the diacetyl reduction. This increased temperature is usually only sustained for 24 to 48 hours.\n• Wheat and Belgian Beers: Same as primary fermentation (higher temperatures will increase diacetyl reduction rates).\nThe conditioning stage takes place when the terminal gravity has been reached and the tank is cooled to refrigeration temperatures (31°F - 38°F, 0°C - 3°C). During this time the yeast continues to flocculate and settle. The yeast also conditions the beer by reducing various undesirable flavor compounds. Ales do not benefit from long conditioning times like lagers do. The desirable flavors in ales will decrease with age and therefore it is recommended that conditioning be as short as possible before packaging. Exposure to oxygen at this stage is extremely detrimental to beer quality.\n• Most of the yeast is removed from beer\n• Formation and precipitation of haze forming proteins\n• Reduction and mellowing of harsh flavors\n• Reduction of sulfur compounds, diacetyl, and acetaldehyde\n• Flavor stabilization","Soured beer can be produced in the same amount of time as non-soured styles with the help of Lactobacillus. Generally, this is done using the kettle-souring method for styles like Gose and Berliner weisse, however other methods and styles are sometimes in play too. Three pros share some sweet tips on souring with Lacto.\nFal Allen, Brewmaster at Anderson Valley Brewing Co. in Boonville, California.\nAt Anderson Valley we have about eight years of kettle souring experience. We use this method for making our Goses and, on occasion, other beers (like our Tropical Hazy Sour beer). We also do a fair amount of barrel souring beers — we have about 1,200 wood barrels in our sour beer production. This process differs greatly from the kettle souring process. It can take anywhere from nine months to four years to sour a beer this way; in contrast kettle souring can be done in less than 8 hours. The flavor that each process creates can be very different as well. We use kettle souring to create clean, sharp, bright tartness and we use our barrel souring process to create a more complex, deeper, funkier range of flavors. For kettle sours, we keep our temperature at about 108–112 °F (42–44 °C). We use a strain of Lactobacillus delbrueckii that seems to work best at that temperature, but each strain is different.\nWe have tried several sources for Lacto, but we prefer the one we get from a lab. It creates flavors we like, and it creates these flavors more consistently on a regular basis. It is also healthier than some other sources, which makes it easier to grow up to the proper pitching rate.\nPrior to pitching the Lacto we look for the same pH as we would in any of our other beers (about 5.2). We do a very large pitch of lactic acid bacteria (LAB) and expect to see the pH drop in to our desired range within 6–8 hours. We also exclude oxygen from the process as much as possible and this helps retard unwanted bacteria with no deleterious effects on the LAB. We do not add extraneous acid to our kettle souring process other than to adjust the pH of the brewing water prior to mash-in (which we do for all our beers as our water is quite high in pH).\nAfter the Lacto does its thing we prefer a wort pH of 3.35 to 3.25, but will accept wort between 3.4 and 3.2. We are looking for a clean, bright acidity and don’t want the pH to be less than 3.2 or above 3.4. We also use titratable acidity to judge acidity and its quality of impact. Future fruit additions play into our target sourness level to a lesser extent. We are trying to achieve a harmonious balance of flavors and if a fruit has a very low pH we will factor that into our kettle souring process.\nOther important factors for kettle souring is to get the wort off the grain (as you would with a “normal” beer) and into the kettle. This needs be done to avoid too much bad funkiness that you would probably get in your wort if you did not sour it fast enough. The second thing is sour your wort fast (in less than 24 hours). To achieve that you need to pitch a good amount of LAB into the kettle; slightly more than one million cells per mL per degree Plato. So about 12 million cells per mL for a 11 or 12 degree Plato wort (1.044–1.048 specific gravity). And the third thing is to exclude oxygen as much as you can. This will help keep the bad funk in check. We blanket the top of the kettle with an inert gas to help keep oxygen out of the process.\nFor much more on the subject, I wrote the book on Gose for the Brewer’s Association’s style guideline series and there is a lot of information about kettle souring in there.\nNicole Reiman (right), Head Brewer & Amanda Oberbroeckling (left) Head of the quality control lab at Odd13 Brewing in Lafayette, Colorado\nAt Odd13 we’ve been kettle souring for about five years. We employ this method any time we produce a sour with the intention of canning. The kettle souring method is perfect for our process because it provides us with a finished product that maintains the same flavor through the shelf life of the beer. Conversely, we use traditional souring methods with small experimental batches. Most of these are long-term souring processes, typically using Lactobacillus and Pediococcus and aging in barrels or foeders. We have also experimented with open fermentation in foeders.\nWe get our Lacto from Inland Island Yeast Laboratories, out of Denver, Colorado. We prefer L. delbrueckii because it gives us a quicker sour, keeping the wort at 115 °F (46 °C). In the past we have also worked with several other Lacto species at various temperatures, including L. brevis, and have found that at incorrect temperatures there is either less activity or too much activity. We chose our current temperature based on the recommendation from our supplier.\nOur process across all of our production is to acidify our wort to a pH of 5.2 using lactic acid. Then, after the souring phase, we typically target a pH around 3.5; however, the decision to stop the souring process is ultimately determined from sensory evaluation of the wort. As we expand our portfolio to include more fruited sours we do take into consideration the acidity level of the fruit that will be added to the beer, and adjust the target pH accordingly. This typically results in a higher pH wort so that fruit additions don’t turn the beer too sour.\nThe most important lesson we’ve learned is that kettle souring is its own process that requires great attention to detail, and has its own intricacies that don’t necessarily carry over from traditional brewing methods. The kettle souring process introduces the possibility of less common off flavors, such as isovaleric acid and butyric acid. We’ve learned that it’s important to make sure the process is dialed in — do your homework and come up with a plan before just jumping in. Monitor and control the process, including cleanliness, wort temperature and pH, and gas levels . . . oxygen makes Lacto angry!\nJoe Mashburn, Head Brewer at Night Shift Brewing in Everett, Massachusetts\nWe don’t kettle sour, but rather have a Lactobacillus fermentation phase, followed by a brewer’s yeast fermentation for our Weisse Series releases. We’ve done a single kettle sour and weren’t happy with it, so we moved on from that. Now, we never denature the Lacto and really like the consistency and results of this approach. We produce between 200–300 bbls (6,200–9,300 gallons/235–352 hL) of this style each month. We’ve also used Lacto for more time-intensive Lacto/Brettanomyces/brewer’s yeast fermentations, which are generally destined for oak barrels that will have adequate time for the Lacto to produce acid and the Brett to do its thing.\nFor our Weisse Series beers, we target a pH of 5.2 in the kettle, just from normal mashing/sparging procedures. We haven’t played around with acidifying prior to Lacto additions, mostly because the 5.2 gives us the results we’re looking for.\nWe use a Lacto culture from Lallemand, but we’ve also tried White Labs and Brewing Science Institute. The Lacto from Lallemand is incredibly easy to use and it comes as a dry pitch so it has a very long shelf life. For that addition we knock out at 100 ˚F (38 °C). This has been the same for the two different types of Lacto we’ve tried. When the pH reaches 3.2–3.3 we then pitch the brewer’s yeast. Our sours are heavily fruited, so that low of a pH helps the acid come through.\nWhen working with Lacto, watch your diacetyl production. We’ve had success eliminating and minimizing diacetyl by adding fruit early in fermentation (day 2). Additionally, during the Lacto fermentation, try to eliminate all oxygen. You could hook up CO2 to an oxygen stone and continue to purge during knockout. If you don’t have an oxygen stone, try not to splash during knockout."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1d44c5cc-d4b6-4c24-bf9b-cfbabb30f707>","<urn:uuid:1e805fc3-8d79-4d8c-b995-262e985e0b54>"],"error":null}
{"question":"What are the key differences between break and continue statements in loop control versus the expression customization capabilities in animation systems - how do they approach program flow modification?","answer":"Break and continue statements in loops and expression systems handle program flow modification differently. In loop structures, break statements cause an immediate exit from the loop (preventing the else part from executing), while continue statements only stop the current iteration to start the next one. In contrast, expression systems like SeExpr focus on customizing inner loops and operations through mathematical concepts, allowing artistic control and flexibility within specific operations rather than controlling overall program flow. The expression approach is about customizing specific computations, while break and continue are explicit control flow mechanisms within structured loops.","context":["Arithmetic expressions appear in almost every animation system ever created. Being able to embed an expression language in a piece of custom software allows an amazing degree of artistic freedom. At Disney artists have enjoyed using expressions because they allow just enough flexibility without being overwhelming to non-programmer users. Developers have enjoyed them too for quick prototyping and deployment of fixes to production needs.\nAt Disney there have been various expression languages. SeExpr started as a language for our procedural geometry instancing tool, XGen. Work was done to generalize it into something that could be used in other contexts. Later it was integrated into paint3d, our texture painting facility, which opened the door to procedural synthesis. More recently, we have integrated it as a way of defining procedural controls to physical dynamical simulations and render time particle instancing.\nExpressions can be seen as a way of allowing customization of inner loops. This is contrast to scripting which is mostly aimed at gluing large parts of code base together. So in this sense, C++ forms the center of your application, python could be used to put pieces of it together, and SeExpr is used to customize tight inner loops.\nWe have packaged several demo applications with SeExpr to show some potential uses. We include a simple ASCII graphing calculator (.mov), a Qt graphing calculator (.mov), a RenderMan shader and shadeop (.mov) to access expressions and a Qt particle simulator (soon to be released). These are all available in the src/demos directory of the source tree.\nArtistic control is very important for film making. Expressions though essentially a mathematical concept can readily be used to create artist directed procedural content. One key we found was creating an interface around the expression language that automatically exposed parameters on defined variables. For example on the right you see a color ramp, slider, and spline ramp that are all automatically generated from just typing the expression below. Also, libraries of expressions are created by expert users and shared with other users. While we are not releasing the GUI for the expression editor, the expression language at its core is SeExpr. A demonstration of the expression editor can be seen in this video (YouTube) at time 3:18. Also take a look at our editor (.mov) and library (.mov) in action.\nSeExpr can be used in many evaluation contexts, and in each context it may have different bound variables, different customizable functions. As an example, a programmer could allow a user to generate an image by evaluating an expression at each point of the image and binding a u and v variable for the u,v parameter of the image. Then the user could write\nNoise Driving Color Interpolation\nRay Tracing Sphere\nThis is an example of something you probably should not do with expressions. At the same time it is sometimes nice to prototype functions and techniques in expressions because of the interactivity.\nFuture PlansRecently we have been exploring a LLVM backend for native compilation on both GPUs and CPUs. We are also looking at adding a limited set of new language features.","General Structure of a Loop\nMany algorithms make it necessary for a programming language to have a construct\nwhich makes it possible to carry out a sequence of statements repeatedly. The code\nwithin the loop, i.e. the code carried out repeatedly is called the body of the loop.\nPython supplies two different kinds of loops: the while loop and the for loop.\nMost loops contain a counter or more generally variables, which change their values in the course of calculation. These variables have to be initialized before the loop is started. The counter or other variables, which can be altered in the body of the loop, are contained in the condition. Before the body of the loop is executed, the condition is evaluated. If it evaluates to True, the body gets executed. After the body is finished, the condition will be evaluated again. The body of the loop will be executed as long as the condition yields True.\nA Simple Example with a While Loop\nIt's best to illustrate the operating principle of a loop with a simple example.\nThe following small script calculates the sum of the numbers from 1 to 100.\nWe will later introduce a more elegant way to do it.\n#!/usr/bin/env python n = 100 sum = 0 i = 1 while i <= n: sum = sum + i i = i + 1 print \"Sum of 1 until %d: %d\" % (n,sum)\nReading Standard InputBefore we go on with the while loop, we want to introduce some fundamental things on standard input and standard output. Normally, the keyboard serves as the standard input. The standard output is usually the terminal or console where the script had been started, which prints the output. A script is supposed to send its error messages to the standard error.\nPython has these three channels as well:\n- standard input\n- standard output\n- standard error\nThe following script shows how to read with a while loop character by character from standard input (keyboard).\nimport sys text = \"\" while 1: c = sys.stdin.read(1) text = text + c if c == '\\n': break print \"Input: %s\" % textThough it's possible to read input like this, usually the function raw_input() is used.\n>>> name = raw_input(\"What's your name?\\n\") What's your name? Tux >>> print name Tux >>>\nThe else Part\nSimilar to the if statement, the while loop of Python has also an optional else part.\nThis is an unfamiliar construct for many programmers of traditional programming languages.\nThe statements in the else part are executed, when the condition is not fulfilled anymore. Some might ask themselves now, where the possible benefit of this extra branch is. If the statements of the additional else part were placed right after the while loop without an else, they would have been executed anyway, wouldn't they. We need to understand a new language construct, i.e. the break statement, to obtain a benefit from the additional else branch of the while loop.\nThe general syntax of a while loop looks like this:\nPremature Termination of a while Loop\nSo far, a while loop only ends, if the condition in the loop head is fulfilled.\nWith the help of a break statement a while loop can be left prematurely, i.e.\nas soon as the control flow of the program comes to a break inside of a while loop\n(or other loops) the loop will be immediately left.\n\"break\" shouldn't be confused with the continue statement. \"continue\" stops the\ncurrent iteration of the loop and starts the next iteration by checking the condition.\nNow comes the crucial point: If a loop is left by break, the else part is not executed.\nThis behaviour will be illustrated in the following example, a little guessing number game. A human player has to guess a number between a range of 1 to n. The player inputs his guess. The program informs the player, if this number is larger, smaller or equal to the secret number, i.e. the number which the program has randomly created. If the player wants to gives up, he or she can input a 0 or a negative number.\nHint: The program needs to create a random number. Therefore it's necessary to include the module \"random\".\nimport random n = 20 to_be_guessed = int(n * random.random()) + 1 guess = 0 while guess != to_be_guessed: guess = input(\"New number: \") if guess > 0: if guess > to_be_guessed: print \"Number too large\" else: print \"Number too small\" else: print \"Sorry that you're giving up!\" break else: print \"Congratulation. You made it!\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3e83db50-d2a3-467b-9a69-bb1184adf54a>","<urn:uuid:de14b6eb-51e9-4b47-8bd9-d87418cd0d22>"],"error":null}
{"question":"What challenges did the Canadian indoor hockey team face in their preparation for the 2021 Pan American Cup?","answer":"The Canadian team faced several challenges in their preparation. They only had 52 hours of total training time together, which was insufficient for international matches and World Cup qualification. The country was in lockdown, limiting athletes' access to gym and training facilities. They needed to obtain government exemptions just to get the team together, and could only centralize immediately before the tournament. Additionally, many athletes were involved with the outdoor hockey team heading to the Tokyo Olympics.","context":["The 2021 Indoor Pan American Cup for men and women are taking place at the end of June and excitement is mounting ahead of the event.\nThree men’s teams and five women’s teams will head to Spring City, Pa. to contest both the title and the right to represent the Pan American continent at the FIH Indoor Hockey World Cup in Belgium next year.\nIn the men’s competition, Canada, Argentina and USA will be facing each other in a double round-robin, followed by a Final. The participating women’s nations are USA, Argentina, Canada, Uruguay and Guyana and their competition is a single round-robin, followed by classifications matches..\nAs with all hockey competitions, preparations for this year’s event have been disrupted and sporadic, so athletes and coaches are understandably apprehensive about how they all perform at the competition.\nJohn de Souza is lead coach for the Canada men’s team. He explained the challenges facing the teams: ‘The Canadian team will not get very much training time. The team will be together for a total of 52 hours, which is not enough time to play international matches let alone a World Cup Qualifier. The team will have no match practice so has little room for error in its planning to avoid injury.’\nLike many other nations, Canada has been in lockdown for the past months and this has had an impact on the athletes’ ability to use gym and training facilities. A selection camp was held on May 1, government exemptions for the athletes to get together were obtained and the team will be centralized for the period immediately leading up to the tournament.\nWith many athletes involved in the outdoor hockey team heading to Tokyo for the Olympics, the coach is understandably feeling the pressure. He remains upbeat however, about his team’s ability to be successful.\nAnother upbeat person heading to Spring City is one of the USA’s longest-serving athletes. Pat Harris is a long-serving athlete with his national team. Harris had racked up a multitude of caps for USA both indoor and outdoor. He also knows what it is like to play at an Indoor World Cup, having competed at both the 2003 Indoor World Cup in Leipzig and the 2011 event in Poznan, Poland. Despite all that experience, the athlete has his concerns about a lack of preparation.\n‘The preparations for the tournament have been very limited. Most of us have not played indoors since last winter, while a small group of US-based players has been training domestically for the last month. The entire team will meet a few weeks prior to the tournament. Not Ideal, but I'm sure other teams are dealing the similar preparation challenges.’\nWhile USA men are the lowest ranked men’s team at the event, this is the year that anything could happen. Both Argentina and Canada will lose some athletes to the outdoor team. For Harris, the prospect of representing USA at a World Cup is hugely exciting. ‘It would be an amazing achievement to get back to an Indoor World Cup. To be able to compete at the highest stage against the world's best would be an awesome opportunity.\n‘But the challenge is a steep one. I'm sure Canada and Argentina will also have strong teams, so every game will be challenging.’\nAlison Lee has represented both the Canada women’s indoor and outdoor national teams since 2013 and, like Harris, has experienced an Indoor World Cup (Leipzig 2015). She echoes many of de Souza’s points: ‘Our preparation leading into this event has not been ideal. Heading into the original tournament date in March of 2020 we had just come off a training tour in Belarus and were able to play some of the European teams as they prepared for the European Championships. We felt extremely prepared heading into last March, this year it is quite a different story.\n‘We have had to deal with a lot of restrictions from our government which did not allow teams to train unless they were going to the Olympics, so it took a long time for us to get an exemption to be allowed to train. In spite of all that, we will be ready and raring to get back on the court! it’s never an easy battle, there are a bunch of solid teams coming out of Pan Am, but we are looking forward to getting back to international competition and we will be fighting for that spot at the World Cup.’\nUruguay is a nation with a decent history of indoor hockey at the Pan Am Cup. The South American team finished in second place at their debut in 2010, finished fourth in 2014 and third in 2017.\nOne of the newest members of the bronze medal-winning team from 2017 was Constanza Barrandeguy. Four years later and she is excited at the prospect of pitting her talents against the likes of USA, Canada and Argentina.\n‘We are training six days a week for two hours each day. We couldn’t start training as early as we wanted, so our coach decided this was the best way to get a good, intensive preparation in place.\n‘All the players love representing Uruguay and we consider this a very special tournament because we are finally returning to international competition after all the suspended games. We are a team that puts a lot of emphasis on human values, we are very strong in that respect. Also, we are convinced that we want to qualify for the Indoor World Cup, so those two values – respecting each other and the opposition, combined with a determination to win – those are the things I see as real strengths to the Uruguay side.’\nBarrandeguy is in no doubt about the strength of the opponents but she is adamant that Uruguay has a game plan that suits them. ‘We need to play all the teams the same way. There may be some changes to the tactical and strategic aspects but it doesn’t matter who we play against, we will be playing our game, and playing our way.’\nIn talking to players and coaches as the teams prepare for the Pan Am Indoor Hockey Championships there is no doubt that, following 18 months of upheaval and disruption, this could be one of the most open tournaments ever played. This is point highlighted by John de Souza: ‘It's really difficult to say how we will fare at this tournament given the lack of training and match play. I do know that we will be competitive and will play an exciting brand of indoor hockey.\n‘I think this is going to be a very competitive tournament and that all the teams will be underprepared. I think the team that can find chemistry and executes their game plans the best will be the successful teams.’\nFrom talking to the coaches and players, it is also obvious that there is a real deep passion for this version of the game. For Alison Lee, who both coaches and plays the game, it offers skills and tactics that make the indoor format so exciting but also enhances the outdoor game.\n‘Indoor hockey teaches valuable skills that can be transferred across the various formats,’ she said. ‘I have been able to bring a lot of what I learned playing indoor to my outdoor game. The indoor game is very quick and can change on the turn of a dime, so mentally it is super important to be able to stay \"tuned in\" to the game for the full 40 minutes because one lapse can change the game.\n‘The different tools, and strategies, and tactics used across all the different formats bring a different perspective to how we play 11 v 11 outdoor. I think playing the sport across different formats is extremely valuable. It equips us with tools that we can use across all different formats and elevates the entire game of hockey.’"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0d394590-122a-407a-86a5-616de7ffa60e>"],"error":null}
{"question":"Buenos días, tengo un pozo de agua en casa. I'm wondering if green nanotechnology and copper water contamination share any environmental impacts or safety concerns that should worry me?","answer":"Both green nanotechnology and copper contamination pose environmental and safety concerns that need to be monitored. Nanotechnology manufacturing can produce harmful pollutants and have adverse environmental effects, though these can be mitigated through green technology approaches. There are also ongoing investigations into nanomaterial toxicity, with particular concern about how materials like nanosilver can be absorbed by plants in soil. For copper, high levels can enter the environment through mining, farming, manufacturing operations, and wastewater releases into water bodies. While copper is essential for life in trace amounts, excess copper exposure can cause health problems including gastrointestinal disturbance, nausea, vomiting, and potential liver or kidney damage with long-term exposure above the Action Level of 1.3 parts per million. People with Wilson's Disease are particularly sensitive to copper contamination.","context":["Nanotechnology has impacted all the major industrial sectors, including electronics, medicine, agriculture, chemicals, coatings, cosmetics and energy. By incorporating nanomaterials into their products, companies can make new technologies which are more sustainable. Not only does nanotechnology improve the performance and design of these products, there are also large revenues associated with nanotechnology enhanced product sales. A good example of this is Lux Research which has achieved over $1 trillion from the commercial sales of the nanotechnology-enabled products in 2015 in Europe and the USA.\nImage Credits | shutterstock.com/g/Kotkoa\nThe manufacturing process of nanomaterials can be compared to the chemical manufacturing process and is known to produce harmful pollutants as well as adverse environmental effects. Although these can be mitigated during the initial design stage, it has not been seen as a priority for current industries. However, as companies become more environmentally conscious, these harmful pollutants should be reduced, especially when manufacturing on a commercial scale. By using green technology, companies can minimize the environmental effects of the process.\nGreen technology works with nanotechnology to develop significant and practical protocols when manufacturing nanotechnology-enabled products. By modifying the production process, the risks that are associated with nano-products are minimized and the green products can be used for environmental applications. This can be through direct applications, such as nano-enabled sensors, wastewater and drinking water treatment and remediation of hazardous waste using nanomaterials. On the other hand, nanotechnology can also have indirect environmental applications such as saving energy during transport by using lighter nanomaterials or reducing the waste by designing products to be smaller.\nUsing green nanotechnology in the production process aims to make nanomaterials more environmentally friendly as well as using nanomaterials in order to make current processes involving chemicals less harmful. There are plenty of ways to do this such as using supercritical CO2, water, or ionic liquids to replace a volatile organic solvent. Another way to eradicate waste in the manufacturing process of nanomaterials is by implementing self-assembly or templating.\nOne of the simplest ways to create greener nanotechnology is by using renewable or non-toxic replacements of non-renewable starting materials. Furthermore, using techniques like microwaving, facile thermal and hydrothermal can aid the conservation of energy. Both catalytic and photocatalytic reactions are also able to increase the efficiency of the manufacturing process while also decreasing harmful byproducts. It should be noted that engineered nanomaterials are able to be used as catalysts in chemical processes. They can also be used as separation membranes.\nHowever, many researchers are investigating the toxicity of nanomaterials. Nanosilver and its effects on the ecosystem are of particular interest as nanosilver can be absorbed by plants if found in the soil. According the US Environmental Protection Agency, this is difficult because the existing tests “may not work to test the safety of nanomaterials.” In addition to this “they [nanomaterials] have unique chemical properties, high reactivity, and do not dissolve in liquid”\nNanotechnology and green technology can work together to create a more environmentally friendly process and products. As the world becomes more ecologically aware, the use of green technology in the planning and design stage will become more prevalent in the industry. However, in order to create truly green products, the entire lifecycle must be considered and how nanotechnology affects all of this is still not known.\nGrossman, E. (2016, January 14). Tiny materials, big questions: How green is nanotechnology? Retrieved from GreenBiz: https://www.greenbiz.com/article/tiny-materials-big-questions-how-green-nanotechnology\nKarn, S. W. (n.d.). Ensuring sustainability with green nanotechnology. Retrieved from IOPScience: https://doi.org/10.1088/0957-4484/23/29/290201\nUnited States Environmental Protection Agency. (n.d.). Research on Nanomaterials. Retrieved from EPA: https://www.epa.gov/chemical-research/research-nanomaterials","The third post in our series highlighting some of the contaminants that can be found in water wells. Northeast Water Wells is available to collect samples and test your well water for contaminants anytime.\nIf you have a private well, regular water quality testing is very important. Northeast Water Wells recommends testing your well at least every two years. Many contaminants cannot be identified by taste or odor, making it difficult for homeowners to know if the water quality of their well has changed.\nWhat is copper?\nCopper is a reddish metal that occurs naturally in rock, soil, plants, animals, water, and sediment. Since copper is easily shaped or molded, it is commonly used to make coins, electrical wiring, and household plumbing materials. Copper compounds are also used as agricultural pesticides and to control algae in lakes and reservoirs. All living organisms including humans need copper to survive; therefore a trace of copper in our diet is necessary for good health. However, some forms of copper or excess amounts can also cause health problems.\nHow does copper get into drinking water?\nThe level of copper in surface and groundwater is generally very low. High levels of copper may get into the environment through mining, farming, manufacturing operations, and municipal or industrial waste water releases into rivers and lakes. Copper can get into drinking water either by directly contaminating well water or through corrosion of copper pipes if your water is acidic. Corrosion of pipes is by far the greatest cause for concern.\nWhy is Copper regulated?\nIn 1974, Congress passed the Safe Drinking Water Act. This law requires EPA to determine safe levels of chemicals in drinking water which do or may cause health problems. These non-enforceable levels, based solely on possible health risks and exposure, are called Maximum Contaminant Level Goals. The MCLG for copper has been set at 1.3 parts per million (ppm) because EPA believes this level of protection would not cause any of the potential health problems described below.\nSince copper contamination generally occurs from corrosion of household copper pipes, it cannot be directly detected or removed by the water system. Instead, EPA is requiring water systems to control the corrosiveness of their water if the level of copper at home taps exceeds an Action Level. The Action Level for copper has also been set at 1.3 ppm because EPA believes, given present technology and resources, this is the lowest level to which water systems can reasonably be required to control this contaminant should it occur in drinking water at their customers home taps.\nWhat are the health effects of excess copper in drinking water?\nCopper is an essential nutrient, required by the body in very small amounts. However, the EPA has found copper to potentially cause the following health effects when people are exposed to it at levels above the Action Level. Short periods of exposure can cause gastrointestinal disturbance, including nausea and vomiting. Use of water that exceeds the Action Level over many years could cause liver or kidney damage. People with Wilson’s Disease (an inherited disorder that causes too much copper to accumulate in your liver, brain and other vital organs) may be more sensitive than others to the effect of copper contamination and should contact their health care provider.\nHow do I remove copper from my drinking water?\nHeating or boiling your water will not remove copper. Because some of the water evaporates during the boiling process, the copper concentrations can actually increase slightly as the water is boiled. Additionally, chlorine treatments will not remove copper.\nIf water tests indicate copper is present in drinking water, the first course of action is to try to identify the source. If possible and cost-effective, eliminate the source. If the source is the water system piping, this is generally not practical.\nIf the source of copper is the in-home plumbing system, flushing the water system before using the water for drinking or cooking is a practical option. Flushing the system means anytime the water from a particular faucet has not been used for several hours (approximately six or more), it should be run until it becomes as cold as it will get. Flush each faucet individually before using the water for drinking or cooking. Water run from the tap during the flushing can be used for non-consumption purposes such as watering plants, washing dishes or clothing or cleaning. Avoid cooking with or consuming water from hot water taps as hot water dissolves copper more readily than cold.\nIf flushing the water system does not reduce copper levels to an acceptable level or is not an alternative of choice, consider an alternative drinking water source such as bottled water or water treatment. If the water is corrosive because of low pH (acidic), a neutralizing filter can be used to raise the pH of the water. This will reduce or eliminate corrosion problems.\nReverse osmosis and distillation treatment can be used to remove copper from drinking water. Typically, copper removal by reverse osmosis or distillation is used to treat water at one faucet.\nNortheast Water Wells offers a variety of testing packages to take care of all of your water needs. Call today to set up a time for us to collect a sample of your water. All of our testing is done through a state certified analytical lab.\nIf you live in Massachusetts you can view the guidelines for Well Water Testing here\nIf you live in New Hampshire you can view the guidelines for Well Water Testing here\nArticle written by Karen Provencher, Northeast Water Wells"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:f2581957-b432-4722-80ba-693f34ccd8f3>","<urn:uuid:63226b96-f370-4b85-8b9d-4c7973be37b8>"],"error":null}
{"question":"Can you explain what tools R provides for numerical integration and how it relates to computational stability analysis? Looking to understand both aspects!","answer":"R provides various tools for numerical integration, including adaptive Gauss-Kronrod quadrature for univariate functions, Romberg integration, Newton-Cotes formulas, and Clenshaw-Curtis quadrature rules. It also supports integration in multiple dimensions through packages that handle integration over hyper-rectangles and spheres. Regarding computational stability analysis, this is typically examined through heuristic analysis methods that involve expanding finite-difference equations in Taylor series and examining truncation errors. The stability analysis helps identify conditions where numerical solutions might exhibit computational instability, such as when solutions grow rapidly or oscillate in ways that don't resemble the expected solutions from the partial differential equations they approximate.","context":["This task view on numerical mathematics lists R packages and functions\nthat are useful for solving numerical problems in linear algebra and\nanalysis. It shows that R is a viable computing environment for\nimplementing and applying numerical methods, also outside the realm of\nThe task view will\ncover differential equations,\noptimization problems and solvers, or packages and functions operating\non times series, because all these topics are treated extensively in\nthe corresponding task views\nTimeSeries. All these task\nviews together will provide a good selection of what is available in R for\nthe area of numerical mathematics.\nThe task view has been created to provide an overview of the topic.\nIf some packages are missing or certain topics in numerical math\nshould be treated in more detail, please let the maintainer know.\nNumerical Linear Algebra\nAs statistics is based to a large extent on linear algebra, many\nnumerical linear algebra routines are present in R, and some only\nimplicitly. Examples of explicitly available functions are vector and\nmatrix operations, matrix (QR) decompositions, solving linear equations,\neigenvalues/-vectors, singular value decomposition, or least-squares\nThe recommended package\nprovides classes and methods\nfor dense and sparse matrices and operations on them, for example\nCholesky and Schur decomposition, matrix exponential, or norms and\nconditional numbers for sparse matrices.\nadds generalized (Penrose)\ninverses and null spaces of matrices.\ncomputes the exponential, logarithm, and square root\nof square matrices, but also powers of matrices or the Frechet\nis to be preferred to the function\nwith the same name in\nprovides classes and methods for sparse matrices\nand for solving linear and least-squares problems in sparse linear\nprovides R bindings to state-of-the-art implementations\nof singular value decomposition (SVD) and eigenvalue/eigenvector\nwill obtain sparse SVDs using an\niterative thresholding method, while\napproximate singular values/vectors of large matrices.\neigenvalues and -vectors for pairs of matrices, and QZ (generalized\ngenerates matrices with a given set of\neigenvalues ('inverse eigenvalue problem').\ncontains routines for manipulating\nquaternions and octonians (normed division algebras over the real\nnumbers); quaternions can be useful for handling rotations in\nintegration of the C++ template libraries 'Armadillo' resp. 'Eigen'\nfor linear algebra applications written in C++ and integrated in R\nfor performance and ease of use.\nMany special mathematical functions are present in R, especially\nlogarithms and exponentials, trigonometric and hyperbolic functions, or\nBessel and Gamma functions. Many more special functions are available in\nprovides an interface to the 'GNU Scientific\nLibrary' that contains implementations of many special functions, for\nexample the Airy and Bessel functions, elliptic and exponential\nintegrals, the hypergeometric function, Lambert's W function, and\nAiry and Bessel functions, for real and complex numbers, are also\ncomputed in package\nBessel, with approximations for large\nincludes special functions, such as\nerror functions and inverses, incomplete and complex gamma function,\nexponential and logarithmic integrals, Fresnel integrals, the\npolygamma and the Riemann zeta function.\ncomputes Gauss' 2F1 and Appell's F1 hypergeometric\nfunctions for complex parameters and arguments quite accurately.\nThe hypergeometric (and generalized hypergeometric) function, is\nhypergeo, including transformation formulas\nand special values of the parameters.\nElliptic and modular functions are available in package\nelliptic, including the Weierstrass P function and Jacobi's\nThere are tools for visualizing complex functions.\nFunction polyroot() in base R determines all zeros of a polynomial,\nbased on the Jenkins-Traub algorithm. Linear regression function lm()\ncan perform polynomial fitting when using\nin the model\nformula (with option\nraw = TRUE).\nfunctionality for manipulating univariate polynomials, like\nevaluating polynomials (Horner scheme), differentiating or integrating\nthem, or solving polynomials, i.e. finding all roots (based on an\nfits univariate polynomials to given data,\napplying different algorithms.\nFor multivariate polynomials, package\nvarious tools to manipulate and combine these polynomials of several\nfacilitates symbolic manipulations on\nmultivariate polynomials, including basic differential calculus\noperations on polynomials, plus some Groebner basis calculations.\nconsists of a collection of functions\nto construct orthogonal polynomials and their recurrence relations,\namong them Chebyshev, Hermite, and Legendre polynomials, as well as\nspherical and ultraspherical polynomials. There are functions to\noperate on these polynomials.\nDifferentiation and Integration\nin base R compute derivatives\nof simple expressions symbolically.\nimplements an approach for numerically\nintegrating univariate functions in R. It applies adaptive Gauss-Kronrod\nquadrature and can handle singularities and unbounded domains to a certain\nsets the standard for numerical differentiation\nin R, providing numerical gradients, Jacobians, and Hessians, computed\nby simple finite differences, Richardson extrapolation, or the highly\naccurate complex step approach.\nextracts features from functional data, such as\nfirst and second derivatives, or curvature at critical points.\ncontains functions for computing numerical\nderivatives, including Richardson extrapolation or complex step.\ncomputes numerical derivatives of higher orders.\nhas several routines for numerical integration:\nadaptive Lobatto quadrature, Romberg integration, Newton-Cotes\nformulas, Clenshaw-Curtis quadrature rules.\nintegrates functions in two dimensions, also for domains characterized\nby polar coordinates or with variable interval limits.\ncontains a collection of functions to\nperform Gaussian quadrature, among them Chebyshev, Hermite, Laguerre,\nand Legendre quadrature rules, explicitly returning nodes and weights\nin each case. Function\ndoes a similar job.\nAdaptive multivariate integration over hyper-rectangles in\nn-dimensional space is available in package\nadaptIntegrate(), based on C code by Stephen\nJohnson. The integrand functions can even be multi-valued.\nintegrate functions over unit spheres and balls in n-dimensional\nspace; conversion routines for multivariate, spherical, and polar\ncoordinates are provided.\nMulti-dimensional numerical integration is also covered in package\nR2Cuba, employing a C library by Thomas Hahn.\nit includes an approach to Monte Carlo\nintegration based on importance sampling.\nprovides another approach to\nmultivariate integration in high-dimensional spaces. It creates sparse\nn-dimensional grids that can be used as with quadrature rules.\nholds some routines for numerical\nintegration over polygonal domains in two dimensions.\nInterpolation and Approximation\nBase R provides functions\nfor constant and linear\nfor cubic (Hermite) spline\nperforms cubic spline\napproximation. Base package splines creates periodic interpolation\nsplines in function\nInterpolation of irregularly spaced data is possible with the\nfor univariate data,\nfor data on a 2D\nrectangular domain. (This package is distributed under ACM license and\nnot available for commercial use.)\ndiscrete data, notably\nfor linear, spline, and\nfor piecewise cubic Hermite\nprovides barycentric Lagrange interpolation\n(in 1 and 2 dimensions) in\nbarylag2d(), 1-dim. akima in\nand interpolation and approximation of data with rational functions,\ni.e. in the presence of singularities, in\ninterpolation based on piecewise rational functions by applying\nStineman's algorithm. The interpolating function will be monoton in\nregions where the specified points change monotonically.\nuniroot(), implementing the Brent-Decker algorithm, is the\nbasic routine in R to find roots of univariate functions. There are\nimplementations of the bisection algorithm in several contributed\npackages. For root finding with higher precision there is function\nin the multi-precision package\nAnd for finding roots of multivariate functions see the following two\nFor solving nonlinear systems of equations the\nprovides (non-monotone) Barzilai-Borwein spectral methods in\nsane(), including a derivative-free variant in\ndfsane(), and multi-start features with sensitivity\nsolves nonlinear systems of equations\nusing alternatively the Broyden or Newton method, supported by\nstrategies such as line searches or trust regions.\ndefines a common interface for solving a set of\nDiscrete Mathematics and Number Theory\nNot so many functions are available for computational number theory.\nNote that integers in double precision can be represented exactly up to\n2^53 - 1, above that limit a multi-precision package such as\nis needed, see below.\nprovides functions for factorization, prime\nnumbers, twin primes, primitive roots, modular inverses, extended GCD,\netc. Included are some number-theoretic functions like divisor\nfunctions or Euler's Phi function.\ncontains various utilities for evaluating\ncontinued fractions and partial convergents.\npackage enumerates additive partitions\nof integers, including restricted and unequal partitions.\ngenerates all permutations or all\ncombinations of a certain length of a set of elements (i.e. a vector);\nit also computes binomial coefficients.\nthrough all permutations or combinations\nwithout retrieving them all at one time.\ncreates and investigates magical squares.\nMulti-Precision Arithmetic and Symbolic Mathematics\nMultiple precision arithmetic is available in R through package\nthat interfaces to the GMP C library. Examples are\nfactorization of integers, a probabilistic prime number test, or\noperations on big rationals -- for which linear systems of equations\ncan be solved.\nMultiple precision floating point operations and functions are\nprovided through package\nusing the MPFR and GMP\nlibraries. Special numbers and some special functions are included,\nas well as routines for root finding, integration, and optimization\nin arbitrary precision.\naccesses the symbolic algebra system 'SymPy'\n(written in Python) from R.\nIt supports arbitrary precision computations, linear algebra and\ncalculus, solving equations, discrete mathematics, and much more.\ninterfaces the computer algebra system\n'Yacas'. It supports symbolic and arbitrary precision computations\nin calculus and linear algebra.\nMATLAB, Octave, and Python Interfaces\nInterfaces to numerical computation software such as MATLAB\n(commercial) or Octave (free) will be important when solving difficult\nnumerical problems. Please note that the commercial programs SAS and\nMathematica do have facilities to call R functions.\nemulation package contains about 30 simple\nfunctions, replicating MATLAB functions, using the respective MATLAB\nnames and being implemented in pure R.\nprovides tools to read and write MAT\nfiles, which is the MATLAB data format. It also enables a\none-directional interface with a MATLAB process, sending and\nretrieving objects through a TCP/IP connection.\nprovides an interface to Octave (a\nMATLAB clone). It enables calling Octave functions, passing variables\nbetween R and Octave, and browsing Octave documentation.\nPython, through its modules 'NumPy', 'SciPy', and 'pandas', has\nelaborate and efficient numerical tools available. R package\npermits calls from R to Python, while project\nmodule 'rpy2') interfaces R from Python.","Finite-difference equations may have rapidly growing and oscillating solutions that in no way resemble the solutions expected from the partial differential equations they are meant to approximate. Such solutions are said to exhibit computational instability. Clearly, it is desirable to avoid these numerical disasters. For linear difference equations with constant coefficients, computational stability can be determined using a Fourier method pioneered by von Neumann (see the article in this series “Computational Stability.” Unfortunately, most equations of physical interest are either nonlinear, or have non-constant coefficients, or both.\nHeuristic Analysis Methods\nIn this article a simple heuristic analysis method is described for investigating the computational stability of such finite-difference equations. An important by-product of this type of analysis is that it often suggests simple ways to eliminate the instabilities and at the same time increase the accuracy of the approximations.\nThe approach described here is called “heuristic” because it is not rigorous or complete, but it often works and can provide a great deal of useful information. Reference  is the original publication describing the heuristic stability method from which much of this article has been taken.\nHeuristic analysis is based on the rather simple idea of reducing a finite-difference equation back to a partial differential equation by expanding each of its terms in a Taylor series and keeping only terms to a certain order in the expansion. This expansion is in powers of the space and time increments, which are assumed to be small to begin with.\nCertainly such an expansion must, to lowest order, reproduce the original partial differential equation, otherwise, it would not be a good approximation. Oftentimes this requirement is referred to as the “consistency” of the approximation. Terms beyond the lowest order in the expansion are referred to as truncation errors.\nThe basic concept of a heuristic analysis is that the Taylor-expanded equation is a more accurate representation of the difference equation than the original partial differential equation. Even keeping only a few truncation error terms should result in a partial differential equation that is more closely related to the difference equation. With this in mind, the following discussion will show that an examination of the truncated equation can sometimes reveal properties shared with the difference equation such as stability problems, necessary initial conditions and/or serious inaccuracies.\nTo begin, we consider the same linear partial differential equation that was discussed in the first article on stability: Computational Stability.\nLinear Equation Example\nThe equation for one-dimensional advection-diffusion of a variable u(x,t) is\nThe convection velocity c and the diffusion coefficient ν are assumed to be constants. Solutions of this equation are known to be bounded and otherwise well-behaved.\nWhat will be shown here is that the stability of a simple finite-difference approximation to Eq. 1 can be determined from an examination of the truncations errors resulting from a Taylor series expansion of a the difference equation. Not only does this process reveal that there are two basic types of instability, but we shall be able to make a direct comparison between the heuristic method and the von Neumann type of Fourier analysis carried out in Computational Stability. This comparison provides a useful rule-of-thumb for which truncation error terms to keep and which to eliminate from the Taylor expansion in order to evaluate the difference equation’s stability.\nThe simple, explicit finite-difference equation approximating Eq. 1 discussed in Computational Stability is\nwhere, e.g., ujn denotes u(jδx,nδt). This is called a forward-in-time approximation that allows all j location values to be computed at time step n+1, provided all the j values at step n are known. In other words, the difference equation requires one initial condition to start things off, just as the original partial differential equation also requires a single initial condition because it only involves a single time derivative.\nIt may be observed that difference equation, Eq. 2, has the property that each space and time location (jδx,nδt) will affect points at time step n+1 at locations j-1, j and j+1. That is, point (jδx,nδt) has a region of influence at later time bounded by lines having slopes ±δx/δt in x-t space. These are similar to characteristic lines along which signals can propagate. For example, the original equation, Eq. 1, has a characteristic line with slope c along which a disturbance advects. In the discrete equation, however, the characteristic lines are not physical characteristics but computational ones defining the region where the difference equation changes data values resulting from a change in value at a particular point.\nWe saw in the Computational Stability article that a Fourier series technique could be used to determine a set of three stability conditions for the difference equation, Eq.2. Here we shall see what can be learned from looking at the truncation errors associated with the approximating equation, Eq. 2.\nTruncation Error Evaluation\nAssume that each term in Eq. 2 is a continuous and differentiable function of x and t. Then, for example, “uj+1,n would be u(xj+δx,tn) and can be expanded about the point (xj,tn) in a Taylor series in powers of δx. Carrying out the expansion in δx and δt for all the terms in Eq.2 yields,\nAll second and higher order terms in δx and δt have been lumped into the order symbol O(δx2 ,δt2). This is a consistent approximation because it reduces to the original partial differential equation, Eq. 1, when δx and δt tend to zero.\nComparison of Fourier and Truncation Error Analysis\nIn the article Computational Stability a typical Fourier mode of the form\nwas substituted into the difference equation, Eq.2, to obtain an equation for r,\nComputational stability of the difference equation requires that the magnitude of r remain less than or equal to 1.0.\nIf we insert a Fourier mode of the form exp(i(kx+wt)) into the truncated Eq. 3, it will be seen that the result is the same as Eq. 4 with r=exp(iwδt) and then expanded in powers of wδt, plus the sine and cosine expanded in powers of kδx. This confirms that the two results are the same, as they should be to O(δx2,δt2) retained in Eq. 3.\nHowever, the comparison also indicates that to keep the basic form of r in Eq. 4, with its real and imaginary parts, we must keep at least the first non-zero terms from the sine and cosine when they are expanded in powers of kδx. The first non-zero term in the imaginary contribution to r comes from sin(kδx) and is proportion to kδx, which corresponds to the first derivative with respect to x in Eq.3. The first non-zero term in the real part of r (other than 1) comes from cos(kδx) and is proportional to (kδx)2, which corresponds to the second derivative with respect to x in Eq. 3.\nThese observations lead to the rule-of-thumb that for the truncated equation to reproduce the lowest order real and imaginary parts of the amplification factor r, it is necessary to retain the lowest order even and odd derivatives with respect to each independent variable in the truncation error. In Eq. 3 there is only one first order term proportional to δt and it is a second derivative with respect to t. There are no first order terms proportional to δx.\nExamining the Truncated Equation for Stability\nUsing the above rule-of-thumb, the truncated equation is,\nThe first important thing to note is that this is not identical to the original partial differential equation, Eq. 1. The claim made here is that Eq. 5 is a better approximation of the finite-difference equation than Eq. 1 and because of this we can obtain information about the stability properties of the difference equation. This, in fact, is the case.\nRecall that the difference equation propagated information into a region of influence bounded by lines whose slopes are dx/dt=±δx/δt. Similarly, the truncated Eq. 5 has a hyperbolic (i.e., wave) character because of the second space and second time derivatives, and the effective wave speeds are ±(2ν/δt)½. If the difference equation is to have any hope of approximating the truncated equation then its region of influence must at least encompass the region of influence of the truncated equation, which leads to the condition\nCourant, Friedrichs and Lewy  used a similar region of influence condition, now called the Courant condition, which restricts the distance a wave travels in one time increment to less than one space increment. A violation of the Courant condition leads to an oscillating and exponentially growing instability. Condition Eq. 6 is precisely one of the stability conditions found from Fourier analysis in Computational Stability.\nA similar Courant-type condition can be inferred from the two first order derivative terms (the advective terms) in the truncated Eq. 5, which propagate information with speed c,\nThis stability condition, also identified in Computational Stability, likewise leads to an oscillating and growing instability when violated.\nTo uncover a third stability condition we must first rewrite the truncated equation by converting the δt term to have space instead of time derivatives, but in a way that still maintains the first order of the expansion. This is done by differentiating Eq. 3 by t and neglecting all first and higher order terms,\nNext replace the first time derivative of u by t in this equation using Eq. 1 to obtain\nFinally, rewrite the truncated Eq.5 using this result for the δt term\nThis result is identical to what would have been obtained by Taylor expanding the original finite-difference equation about the point x=jδx and t=(n+½)δt (and would probably have been easier).\nAccording to our rule-of-thumb the last two terms on the right side proportional to δt can be dropped because they involve higher order derivatives than what is in the first δt term on the right side, which leaves,\nThis is an alternative form for the truncated equation that retains only the lowest order (first) truncation errors and only those that contain the lowest even and odd derivatives with respect to each independent variable.\nEquation 11 is nearly the same as the original Eq. 1, except for a modified diffusion coefficient. The significant thing here is that the diffusion coefficient can be negative. As long as the diffusion coefficient is positive solutions of Eq. 11 exhibit exponentially damped behavior, but with a negative coefficient solutions have an exponentially growing character, i.e., a computational instability! Thus, a further condition for computational stability is that the diffusion coefficient remains positive,\nIn this case the instability is a pure growing one without the oscillations in sign associated with the two earlier region-of-influence conditions. If instability is encountered, knowing whether it is exhibiting an oscillation in sign or not will identify it as either a region-of-influence violation or a negative diffusion coefficient. Having this knowledge makes it easier to find a remedy for the instability.\nApplication to Two-Dimensional Fluid Flow\nA two-dimensional example (x,z) of water flowing under a laboratory scale sluice gate offers a test for examining a computational instability arising from non-linearity in the governing equations. The physical problem consists of water held behind a gate with an elevation of 0.9ft. Downstream (right) of the gate there is a water pool of depth 0.14 ft. Gravity is 32.2 ft/s2 in the negative z direction (down). At time t=0 the gate is raised up a distance of 0.125ft and water surges out into the pool. Figure 1 shows the resulting flow obtained with a Navier-Stokes solver  at t=0.35s. The solver used for this example has been optimized to automatically eliminate instabilities so none are apparent in this case, but it is possible to force the program to use non-optimum settings.\nTo demonstrate some unstable behavior we first examine a heuristic analysis performed on the vertical velocity equation used in the simulation. Focus is on the effective diffusion coefficients for the z direction velocity w, while all other truncation errors are ignored,\nThe diffusion of w in the x and z directions are expressed by the two terms on the right side of Eq. 13, where ν is the fluid viscosity and α is a parameter that modifies the numerical approximation of the term describing the u advection of w, i.e., the second term on the left side of the above equation. When α=0 the finite-difference advection approximation is said to be centered about the location of w, but when α=1 an upstream or “donor cell” approximation is used.\nThe first thing to notice is that if ν=0 and a centered difference approximation is also used (α=0) then the lowest order term in the two effective viscosity coefficients are proportional to δt and are negative. This clearly leads to unstable behavior, and is a well known property of the central difference approximation. Adding enough viscosity to keep the diffusion coefficient positive is also an established procedure to gain stability, but at the possible cost of introducing too much diffusion. The upstream difference option, α=1, is a reasonable compromise; provided the condition wδt<δx is maintained, the diffusion coefficients are positive (provided the δx2 and δz2 terms are small) and the simulation will be stable.\nIf the δx2 and δz2 terms in the diffusion coefficients are not small there is a possibility of unstable behavior. To demonstrate this we set the viscosity to zero and reduce the amount of upstream differencing by setting α=0.05. To keep the negative δt term less than the a term a very small time step δt=0.00025 is used. With these settings the resulting simulation is shown in Fig. 2. An instability in the z velocity has developed just upstream of the sluice gate, which is shown close up in Fig. 3 (where color indicates the z velocity magnitude).\nThis instability is a result of a negative x-direction diffusion coefficient, which is coming from the δx2 term. A negative value results from the fact that the flow upstream of the gate is compressing in the z direction, but expanding in the x direction, which means that the x derivative of u in the δx2 term is positive in this region resulting in a net negative diffusion coefficient.\nA check on this conclusion can be made by adding in a little viscosity ν=0.0093 to compensate for the negative δx2 term. Figure 4 shows that this change does, indeed, stabilize the flow.\nThis example demonstrates that truncation error terms arising from non-linear terms in the original equation influence the computational stability of the difference equation. This type of instability cannot be found by a von Neumann type Fourier analysis. Perhaps most important of all is that when troublesome truncation errors are found to exist this knowledge can be used to alter the finite difference equations to eliminate those errors.\nTo summarize, it has been shown that all the stability conditions associated with a linear finite-difference equation, Eq.2, can be identified using a heuristic truncation error approach. This approach not only identifies the instabilities, it also indicates what can be done to eliminate them. For instance, for a region-of-influence violation only a reduction in the time-step increment will solve the problem, but if there is a negative diffusion coefficient then adding more diffusion to compensate for the errors is one way to regain stability. Knowing the origin of a negative diffusion error may also suggest how the original finite-difference equation might be modified to avoid this problem.\nThe most significant aspect of the heuristic approach is that it is not limited to linear equations with constant coefficients, as was shown in connection with the example of flow under a sluice gate. No special assumptions were necessary to form the approximating truncated equation. The goal was simply to reverse the procedure of writing a difference equation to approximate a partial differential equation, and instead to write a partial differential equation that approximates the difference equation. A simple rule-of-thumb was described for constructing the truncated equation. This approximating equation was then used to check for region-of-influence violations and for possible negative diffusion coefficients both features that lead to unstable solutions.\nSeveral additional examples involving compressible and incompressible fluid dynamics simulations can be found in the original heuristic stability paper , which further show how the heuristic approach can be applied to real, practical, non-linear problems."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f55da3ff-2f82-457f-be4a-c636e74f4b30>","<urn:uuid:7e10e861-9686-47fb-8819-3b6460b482ba>"],"error":null}
{"question":"Which ink was more damaging to manuscripts - iron gall ink or the purple dye made from fermented urine?","answer":"Iron gall ink was more damaging to manuscripts. The iron gall ink deteriorates over time, flaking off and burning through the parchment or paper it's written on, literally eating through the material as shown in examples of medieval documents. In contrast, while the purple dye made from fermented urine (orcein) was used in the Codex Purpureus Rossanensis, there is no mention of it causing damage to the manuscript - in fact, the manuscript has survived for 1,500 years.","context":["Many people love gazing at the glittering and vibrantly painted images in medieval manuscripts. For some, including myself, there is no better way to glimpse aspects of medieval life than getting lost in the details of illuminations depicting the fashions, pastimes, professions, and objects of every day life. But this post is dedicated to the ink that made the composition of all of those beautiful manuscripts possible.\nYesterday I came across a wonderful little recipe for iron gall ink held in The UK National Archives.\nIron gall ink is a purple-black ink, that turns a rusty-brown colour over time (notice how the ink in the image below looks brown). It was used across Europe until at least the nineteenth century and vast numbers of medieval and renaissance manuscripts were written with it. The transcript of the recipe on the National Archive’s website has a few errors, so here’s my own:\nTo make hynke. Take gall\n& coporos & or vitrial quartryn\n& gumme of eueryche a quartryn\noþer helf quartryn & a halfe\nquartryn of gall more &\nbreke þe gall a ij oþer a iij\n& put ham togedere euery-\nche one in a pot & stere hyt\nij wykys after Ʒe mow\nwryte þer wyþ.\n& yf Ʒe have a quartryn of\neueryche take a quarte of\nwatyr yf halfe a quartryn\nof eueryche þan take half\na quartre of watyr.\nThe recipe instructs that four substances should be mixed together in equal measure: oak galls, copperas (aka iron sulfate, ferrous sulfate or iron vitriol), gum arabic, and water. The mixture should be stirred often over a two week period, after which time it is ready to use.\nWhen soaked in water (or, in some recipes, wine!), the oak galls release gallic acids and tannins, which, when mixed with the iron sulfate, produce a black pigment. The addition of gum arabic acts as a binder to fix the pigment, it helps the ink to flow better and bind to the parchment or paper, and it gives a richer tone to the colour of the ink.\nThough incredibly popular with medieval scribes, iron gall ink deteriorates over time, flaking off and burning through the parchment or paper it’s written on. This is seriously bad news for researchers working with original medieval documents and manuscripts and great care has to be taken to reduce the texts’ exposure to humidity and severe temperature fluctuations. The image below shows just how corrosive the ink can be over time; it has literally eaten through the parchment containing music.\nSo, next time you find yourself captivated by a beautiful medieval illumination, take a few moments to appreciate the text that it accompanies. It wants to be seen – to be read and admired – before it slowly and silently disappears.\nFor more information about Iron Gall Ink, and the implications it has for the long-term preservation of manuscripts, see http://www.irongallink.org\nUPDATE: to include a link to my segment ‘The Ink That Helped to Write the History of Our World‘ in BBC Four’s Oak Tree: Nature’s Greatest Survivor. The full programme can be purchased from the BBC Store.","Fermented Urine Dye Discovered in One of the Oldest Illuminated Manuscripts\nA mixture of urine and weeds has been discovered in the dye used on the stunning 1,500-year-old Byzantine text known as the Codex Purpureus Rossanensis, one of the oldest surviving illuminated manuscripts of the New Testament. A recent analysis of the pigments revealed that fermented urine was used to create the purple color that the manuscript is famous for.\nThe manuscript is incomplete, but it tells the life of Jesus according to the gospels of Mark and Mathew. It dates back to the 5 th or 6 th centuries AD, and consists of 188 parchment sheets. The manuscript became famous due to the mystery of its purple color. Usually the Tyrian purple extracted from sea snails (Murex) was used to dye parchment sheets. However, in this case, the purple color was created with orcein, a natural dye extracted from the lichen Roccella Tinctoria, and processed with fermented urine.\nThe Codex Purpureus Rossanensis is famous for its purple color, which came from fermented urine ( public domain )\nAccording to Seeker, the analysis of the dye was carried out during the restoration project of the sacred text at the Diocesean Museum of Rossano. The restoration was aimed at repairing damage that occurred during the previous restoration attempts in 1917-1919, which modified the aspect of the illuminated sheets. This time, the manuscript was restored without using invasive procedures. They limited the intervention to stitching cuts, tears and small gaps. Moreover, they prepared natural dyes to repair damage to colors using recipes described in the Stockholm papyrus, a manuscript written in Greek c. 4 th century AD, which contains 154 recipes for the manufacture of colors and dyes. They also used the X-ray fluorescence to rule out the presence of bromine, which is characteristic of the Tyrian purple.\n\"Even though early medieval illuminated manuscripts have been deeply studied from the historical standpoint, they have been rarely fully described in their material composition,\" Marina Bicchieri, director of the Icrcpal's chemistry lab said.\nThe restorers used dye recipes that came from the Stockholm Papyrus, otherwise known as the Papyrus Graecus Holmiensis ( World Digital Library )\nUsing fiber optics reflectance spectra, the researchers discovered that the purple parchment of the codex contained a dye created with orcein (processed with fermented urine) and sodium carbonate, which was most probably the mineral natron, the same substance which ancient Egyptians used in mummification. An examination with the raman spectroscovy revealed that the red-mauve and violet shades discovered in the miniatures were obtained using an elderberry-based lake, which is the first known example of such a practice in an ancient illuminated manuscript.\n- Han Purple: A 2,800-year-old artificial pigment that quantum physicists are trying to understand\n- Money Does Not Stink: The Urine Tax of Ancient Rome\n- Mummy Brown – the 16th century paint made from ground up mummies\nThe red-mauve and violet shades were obtained using an elderberry-based lake ( public domain )\nThe Codex Purpureus Rossanensis was found in 1879 in the sacristy of the Cathedral of Rossano in Calabria, a region in southern Italy. The pages of the manuscript are filled with miniatures and Greek text written in silver and gold ink. According to the researchers, the volume is only a half of the original book. They suppose that the other part of the book, most probably two more gospels, was lost during the fire at the Cathedral of Rossano in the 17 th century. It is now kept in the Diocesean Museum of Rossano collection.\nUrine was used in many surprising ways since antiquity. As Bryan Hill, a writer of Ancient Origins, explained in his article:\n''While today we flush or urine away without giving it a second thought, in ancient times it was considered a valuable commodity. Urine contains a wide array of important minerals and chemicals such as phosphorus and potassium. The Romans believed that urine would make their teeth whiter and keep them from decaying so they used it as a mouthwash and mixed it with pummis to make toothpaste. In fact, urine was so effective that it was used in toothpastes and mouthwashes up until the 1700s.\nAs far as the Romans were concerned, the best and therefore the most expensive urine on the market came from the country of Portugal. It was supposedly the strongest urine in the world and thus, the choice for whitening teeth. Though most people today would decline the option of a urine-based toothpaste, it actually worked! This is because urine contains ammonia which is used in many household cleaners today. If you leave urine out in an open vat it turns stale and produces ammonia through interaction with the air. In Roman times, this was then used for laundry. Due to the ammonia content, urine was also important for the textiles industry, which was a booming trade during the Roman Empire. Often urine was used to bleach wool or linen and tan leather.''\nTop image: Pages from the Codex Purpureus Rossanensis ( public domain )"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fcaa0c4e-b389-483f-b273-7e45b5da6d26>","<urn:uuid:f9648124-6a3d-4240-a7ab-656302cee189>"],"error":null}
{"question":"Compare treatment approaches: Can mild cognitive impairment and rheumatoid arthritis both be treated with lifestyle changes?","answer":"Yes, both conditions can be treated with lifestyle changes, though in different ways. For mild cognitive impairment, treatment involves staying hydrated, eating nutritious foods (especially brain-healthy foods like spices, whole grains, seafood, and fresh fruits), staying physically and socially active, getting enough sleep, and limiting alcohol and smoking. For rheumatoid arthritis, while there is no cure, lifestyle modifications like maintaining a healthy diet and weight can help manage symptoms, though medical treatments like DMARDs, NSAIDs, and biologics are typically necessary for controlling the disease.","context":["As the population most at risk for complications from the novel coronavirus (Covid-19), many seniors have spent several months self-isolating to reduce their risk of exposure, causing many caregivers to express their concern that the lack of social engagement among seniors is creating a new crisis – widespread cognitive decline.\nWhat is Mild Cognitive Impairment or MCI?\nThe Mayo Clinic describes Mild Cognitive Impairment (MCI) as “the stage between the expected cognitive decline of normal aging and the more serious decline of dementia.” Seniors with MCI may struggle with remembering words or daily tasks, and you may notice an overall slip in cognitive awareness among your loved one. Mild Cognitive Impairment is not a form of dementia, and while some individuals who develop MCI are at higher risk of dementia, many seniors’ cognitive abilities remain stable for years, or improve after intervention.\nCan isolation accelerate cognitive decline?\nResearchers estimate that between one third and one half of all seniors experience loneliness and social isolation during their late adulthood. This number can be greatly affected by many external factors, including a public health emergency such as the current coronavirus pandemic that is particularly dangerous to seniors. As more seniors spend time away from their families, peers, and social hubs, it raises concerns about isolation and subsequent cognitive decline.\nWhile researchers cannot definitively prove causation between isolation and cognitive impairment in older adults, studies have shown that loneliness may have a direct impact on seniors’ brains and overall health. Additionally, there is plenty of research that shows individuals who have more regular social opportunities are less likely to develop several age-related conditions like Alzheimer’s disease, osteoporosis, arthritis, heart disease, depression and several cancers.\nIs there a way to prevent or reverse mild cognitive impairment?\nWhile there is no cure for dementia, the good news is that there are several ways you can help protect yourself or your loved ones from cognitive decline in late adulthood. One important step is making sure that you are staying hydrated and eating healthy, nutritious foods. Research has shown that eating brain healthy foods, such as spices, whole grains, seafood, and fresh fruits and vegetables can lead to better brain function and slow the effects of memory loss.\nAnother way to help boost brain health is to stay physically active, as well as socially active. Getting frequent exercise and engaging in social activities with others, such as going for a walk with friends or catching up on the phone, can help reduce stress in addition to potentially reducing the risk of dementia. Getting enough sleep and limiting alcohol intake and smoking are also contributing factors in helping to prevent memory loss. For seniors who may not be able to cook nutritious meals or seek out social activities on their own anymore, a Retirement or Assisted Living Community can offer a more supportive living environment that helps seniors stay sharp and independent in a secure, home-like setting.\nWhat can I do if my Mom or Dad is isolated due to Covid-19?\nIf your loved one is feeling isolated due to the current coronavirus pandemic, there are many ways that you can help.\n- Setting up virtual visits using a smartphone or laptop can help ease loneliness, and there are now more ways than ever to make these visits fun! Consider playing a round of online trivia, having group Zoom calls with the grandkids, or hosting a virtual social hour with friends.\n- If you are planning an in-person visit with someone experiencing memory loss, consider following some of these tips to help keep the conversation going.\n- You may also consider dropping off meals or groceries for your loved one, or signing up for a grocery delivery service. Ensuring that your mom or dad has access to healthy meals will help prevent poor nutrition and decrease their risk of cognitive decline during self-isolation.\nThe Bottom Line\nWith the novel coronavirus impacting all of our daily lives, it is important that we take time to check in on our senior loved ones to prevent isolation and cognitive decline. Loneliness is one of the most difficult parts of aging for many seniors, but it doesn’t have to be. Taking time to visit, checking in to make sure your loved one is staying healthy, and dropping off nutritious foods can all go a long way toward reducing feelings of isolation, and potential memory loss, in a senior family member. If your mom or dad would benefit from a more supportive living arrangement, a move to Assisted Living may be able to help offer them the nutritious diet, social engagement, physical activity and supportive services that can help them thrive in late adulthood.\nRead more: Covid and Assisted Living: Is Now the Time to Make a Move?","Rheumatoid arthritis (RA) is an inflammatory autoimmune condition in which the body’s immune system attacks the joints, resulting in pain, swelling, and deformity. RA is chronic — there is currently no cure.\nRheumatoid arthritis is an autoimmune disorder in which the immune system mistakenly attacks the joints in the same way that the immune system normally would fight viruses or bacteria. In other words, the joint damage in RA is caused by the body’s immune system, most commonly those of the hands, feet, wrists, elbows, ankles, and knees. With RA, damage is typically symmetrical — when a joint is impacted, it tends to be on both sides of the body. For instance, both knees or both ankles are usually affected.\nThe cause of RA is unknown, although it likely involves both hereditary and environmental risk factors. Environmental factors, such as cigarette smoking and obesity, increase the risk of developing RA. Read more about the causes of RA.\nPeople have had rheumatoid arthritis since the early days of recorded history. As early as 1500 B.C., a disease similar to RA was described in the Ebers Papyrus, an ancient Egyptian scroll documenting knowledge of medical herbs. RA has been discovered in Egyptian mummies, discussed in Indian literature dating as far back as 300 B.C., and identified in the subjects of early paintings. The term \"rheumatismus,\" meaning “to suffer from a flux,” was mentioned by famed Greek physician Galen sometime in the second century of the common era. In the Western world, bloodletting and leeches were popular treatments for RA. In the East, acupuncture was used. At least two ancient treatments — gold compounds and willow bark (which contains the active compound in Aspirin) — are effective and continue to be used today.\nIn 1819, English surgeon Sir Benjamin Collins Brodie described how RA affects the joints and tendons, as well as the progressive nature of the disease. The term rheumatoid arthritis was first mentioned by Archibald Garrod in \"Treatise on Rheumatism and Rheumatoid Arthritis\" written in 1890.\nEarly in the 20th century, rheumatoid arthritis was believed to be caused by an infection. Gold compounds and sulfa drugs were the mainstays of treatment during this period. Acetylsalicylic acid was isolated from willow bark and marketed as Aspirin in the early 1900s, followed later in the century by other nonsteroidal anti-inflammatory drugs (NSAIDs), such as Ibuprofen. Disease-modifying treatments came into use in 1938, when Sulfasalazine was formulated to treat RA. Methotrexate and corticosteroids became common treatments for RA in the 1980s. Other 20th century breakthroughs include research proving RA is an autoimmune disease and the identification of a genetic component. Anti-tumor necrosis factor drugs (or anti-TNFs), also known as biologics, first became available in the 1990s, and new biologics continue to be created.\nRheumatoid arthritis is the most common type of inflammatory arthritis, affecting more than 1.3 million people in the U.S. Rheumatoid arthritis affects women about three times as often as men and typically develops between the ages of 30 and 60.\nRheumatoid arthritis affects each person a little differently. Symptoms depend on the stage of the condition and can appear or disappear at any time. Symptoms generally focus on the joints and tend to occur symmetrically, on both sides of the body. Symptoms worsen during disease exacerbations (also called flares) and may subside during periods of remission.\nThe most common symptoms of RA include swollen, stiff, painful joints; joint damage; and limited mobility. Morning stiffness, fatigue, and inflammation in other parts of the body — especially the eyes, mouth, blood vessels, and lungs — are also typical in cases of RA. Many people with RA experience weight loss and depression, which is common in people living with any type of chronic illnesses. Read more about rheumatoid arthritis symptoms.\nAs part of the evaluation for rheumatoid arthritis, blood tests are conducted to check for anti-cyclic citrullinated peptide (anti-CCP). Around 60 percent to 80 percent of people diagnosed with RA test positive for anti-CCP, an antibody (protein made by the immune system) that attacks the joints. Presence of anti-CCP leads to a classification of seropositive RA. A negative result leads to a diagnosis of seronegative RA, which is often a milder form of the disease.\nYour blood may also be tested for the presence of rheumatoid factor, another autoantibody. Since rheumatoid factor may also be found in the blood of those with other rheumatic diseases or unrelated conditions, it is less reliable as a diagnostic criterion. The presence of elevated C-reactive protein (CRP) or erythrocyte sedimentation rate (ESR) in the blood indicates inflammation, and may support a diagnosis of RA or another autoimmune condition, such as psoriatic arthritis.\nIf RA is suspected, the doctor may perform a physical exam to check for the presence of synovitis (joint inflammation) and rheumatoid nodules, and order X-rays to assess joint damage.\nWhen children are diagnosed with RA, it is called juvenile RA or juvenile idiopathic arthritis. Juvenile RA may or may not disappear as the child grows up.\nRead more about types of RA.\nRheumatoid arthritis is commonly diagnosed and treated by a rheumatologist. RA can also be treated by an internal medicine specialist.\nTreatment of rheumatoid arthritis is typically aimed at modulating the immune system to calm inflammation, reduce autoimmune attacks on the joints, and ease symptoms (such as pain and swelling) while avoiding side effects. Some RA treatment options are taken orally, while others are administered by injection.\nMild joint pain and swelling may be relieved with over-the-counter treatments such as Ibuprofen or Aspirin. There are many classes of disease-modifying antirheumatic drugs (DMARDs) prescribed for RA that can effectively reduce disease activity and prevent damage. Surgery, such as joint replacement, can restore function when RA has caused severe joint destruction. Occupational therapy can help people with RA overcome disability and maintain mobility and function.\nSome people with rheumatoid arthritis feel better when they change their diet or try complementary or alternative therapies, such as acupuncture or acupressure. Read more about rheumatoid arthritis treatments.\nThere is currently no cure for RA. While symptoms may appear during disease flares and subside during remission periods, RA itself does not go away. However, there are effective treatments to manage many of the symptoms of rheumatoid arthritis and to help slow progression.\nRheumatoid arthritis itself is not considered a fatal disease. However, severe RA can cause lung disease or cardiovascular complications that raise the risk of death. People living with RA also have a higher risk for developing diabetes, which can lower life expectancy.\nRheumatoid arthritis symptoms can come and go with flares. Common triggers for flares include stress, overexertion, infection, and lack of sleep.\nNo single diet has been proven by clinical studies to improve symptoms in everyone with RA. However, eating a healthy diet can help people with RA achieve and maintain a balanced weight, lower the risk of heart disease, and fight inflammation. Read more about diet recommendations and other rheumatoid arthritis treatments.\nEasily manage your subscription from the emails themselves.\nWant to stay up to date on the latest news and articles about RA?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8ee09843-90d8-4084-849e-ac8d3bfad15c>","<urn:uuid:5dfbe2d4-9abf-40bc-ac72-078897c96254>"],"error":null}
{"question":"What kind of materials exist in historical archives about both the US Postal Service's electronic initiatives and Western Union's operations?","answer":"The historical archives contain different types of materials about both organizations. For the U.S. Postal Service, there are documented policy plans and initiatives, including records of their 1845 telegraph operations, the 1892 mailgram service proposal, the 1959 Speed Mail experiment, and the 1982 E-COM system. For Western Union, the archives contain photographs, scrapbooks, notebooks, correspondence, stock ledgers, annual reports, and financial records that document the evolution of the telegraph and the development of the Western Union Telegraph Company. These materials collectively describe the history of both companies and the telegraph industry's importance to technological development in the nineteenth and twentieth centuries.","context":["Imagine that the U.S. Postal Service was in charge of e-mail. Sound absurd? It does to most people-until they realize that it almost happened.\nIn 1977, when I first came to Washington, DC, I joined the communications policy program of the Aspen Institute, where I was assigned to research how the impending telecommunications revolution would affect postal service. Instead of muddling through backwater, I uncovered a goldmine of plans and policy dilemmas.\nThe Postal Service had considered electronic mail ever since the invention of the telegraph. The 1845 telegraph line between DC and Baltimore was operated by the Post Office Department, which urged that the government run the telegraph system. A provision in the telegraph legislation of 1866 authorized the government to purchase existing telegraph plants after 1871.\nIn 1892, Postmaster General John Wanamaker proposed, in effect, a mailgram service: postal telegraph messages would be collected and delivered by the Postal Service, with the long-distance telegraph service provided by private companies under contract to the Postal Service. Wanamaker even suggested that the postal telegraph system might someday offer a facsimile service. None of these proposals made headway, however, because of Western Union’s then-powerful private monopoly with revenues in 1890 of $20 million, which represented one-third of the postal system’s total revenues.\nAside from one brief wartime operation in 1918, the next government experiment with electronic mail delivery occurred in 1959 when the Post Office tested Speed Mail. This was a facsimile experiment between Washington, DC and Chicago in which government agency mail was transmitted using facilities supplied by private telecommunications carriers. Western Union strongly protested, and in 1962 the Kennedy Administration killed the experiment.\nBut only after 1971, when the U.S. Post Office Department was replaced by the newly-formed U.S. Postal Service (USPS), did the Postal Service look hard at e-mail as an opportunity. The argument for USPS authority in electronic mail service stemmed from the rather broad provisions of the Postal Reform Act of 1970. The act required the Postal Service to “promote modern and efficient operations and [avoid] any practicewhich restricts the user of new equipment or devices which may reduce the cost or improve the quality of postal services…”\nThe House Subcommittee on Postal Facilities, Mail and Labor Management stated that “In the next few years, postal managers will make vitally important decisions which will determine the direction which the USPS will take when technology in the form of electronic transfer and other advances change the role of the organization.”\nE-mail to the Rescue?\nIn the mid-1970s I argued that the USPS could be a logical manager of a household electronic message delivery system, but added this cautionary note: “The USPS has not developed the skills to capitalize on whatever its charter may allow in the telecommunications area. It seems most likely, therefore, that we shall continue to let private enterprise lead the way to electronic mail service to the home. Such private competition has the promise of the most important benefit-the fullest possible play for innovative technology and services.”\nIn January 1982, my worst fears concerning the Postal Service began to unfold when it introduced Electronic Computer-Originated Mail. E-COM was a message system designed to serve volume mailers, such as Shell Oil and Merrill Lynch, by generating mail from data stored electronically. The service rolled out to 25 post offices and transmitted the messages to other cities, which then transformed them into hard copy and delivered them within two days. The Postal Service was to be the active agent in E-COM, involved with all aspects of management.\nImmediately, I took to my trusty Smith Corona typewriter to let the world know that the camel’s nose was moving into the tent. The New York Times published my op-ed piece, in which I called for the Reagan Administration to “curb big mail, too” by proposing legislation that would bar the Postal Service from providing future end-to-end electronic mail service.\nAmazingly, I soon had my answer. The Postmaster General of the United States, William F. Bolger, wrote a Shermanesque reply pledging that “the Postal Service be prohibited by law from entering the Generation III’ (terminal-to-terminal) business. That aspect is the proper domain of the telecommunications industry. Our mandate for 206 years has been the delivery of hard-copy messages. That will remain our function.”\nSo the war ended shortly after the first battle began. But with a different turn of events, we might all be logging on to our Postal-Service-controlled e-mail messages. Or perhaps we would not have taken to that new medium at all, kept our Smith Coronas and waited for our friendly postal carrier to knock on the door and announce, “You’ve got mail.”\nDeepMind’s cofounder: Generative AI is just a phase. What’s next is interactive AI.\n“This is a profound moment in the history of technology,” says Mustafa Suleyman.\nWhat to know about this autumn’s covid vaccines\nNew variants will pose a challenge, but early signs suggest the shots will still boost antibody responses.\nHuman-plus-AI solutions mitigate security threats\nWith the right human oversight, emerging technologies like artificial intelligence can help keep business and customer data secure\nNext slide, please: A brief history of the corporate presentation\nFrom million-dollar slide shows to Steve Jobs’s introduction of the iPhone, a bit of show business never hurt plain old business.\nGet the latest updates from\nMIT Technology Review\nDiscover special offers, top stories, upcoming events, and more.","Western Union Telegraph Company Records\nWestern Union Telegraph Company\nThe collection documents in photographs, scrapbooks, notebooks, correspondence, stock ledgers, annual reports, and financial records, the evolution of the telegraph, the development of the Western Union Telegraph Company, and the beginning of the communications revolution. The collection materials describe both the history of the company and of the telegraph industry in general, particularly its importance to the development of the technology in the nineteenth and twentieth centuries. The collection is useful for researchers interested in the development of technology, economic history, and the impact of technology on American social and cultural life.\nRobinson and Via Family Papers\nPapers documenting the farming and family life of the Robinson family of Prince George's County and after 1975, Charles County, Maryland. Papers documenting the farming and family of the Via family of Greene County, Virginia, Washington, D.C., Prince George's and Calvert Counties, Maryland, by 1949.\nKraushaar Galleries records\nThe records of New York City Kraushaar Galleries measure 106.3 linear feet and 0.181 GB and date from 1877 to 2006. Three-fourths of the collection documents the gallery's handling of contemporary American paintings, drawings, and sculpture through correspondence with artists, private collectors, museums, galleries, and other art institutions, interspersed with scattered exhibition catalogs and other materials. Also included are John F. Kraushaar's estate records; artists' files; financial ledgers documenting sales and gallery transactions; consignment and loan records; photographs of artwork; sketchbooks and drawings by James Penney, Louis Bouché, and others; and two scrapbooks. There is a 6.0 linear foot unprocessed addition to this collection donated in 2022 that includes correspondence with artists, galleries, organizations and individuals regarding works of art, filed alphabetically by year. Materials date from circa 1959-1999, with the bulk from 1990-1999.\nRockwell Kent papers\nThe Rockwell Kent papers measure 88.0 linear feet and date from circa 1840 to 1993 with the bulk of the collection dating from 1935 to 1961. The collection provides comprehensive coverage of Kent's career as a painter, illustrator, designer, writer, lecturer, traveler, political activist, and dairy farmer.\nNorcross Greeting Card Collection\nTuck, Raphael, fl. 1880s\nNorcross Greeting Card Company (New York (N.Y.))\nRust Craft Publishers (Boston, Mass.)\nCollections consists of the records of both the Norcross Greeting Card Company founded in New York City in the 1920s and The Rust Craft Greeting Card Company, founded in Kansas City, Missouri, 1906. Both the Norcross and Rust Craft companies collected antique greeting cards. Also includes a small number of modern cards by other manufacturers, circa 1930-1980. Collection represents development of the greeting card industry, social trends in the United States and technology of the printing industry from 1924 through 1978.\nSmithsonian Folklife Festival records: 1993 Festival of American Folklife\nThe Smithsonian Institution Festival of American Folklife, held annually since 1967 on the National Mall in Washington, D.C., was renamed the Smithsonian Folklife Festival in 1998. The materials collected here document the planning, production, and execution of the annual Festival, produced by the Smithsonian Center for Folklife and Cultural Heritage (1999-present) and its predecessor offices (1967-1999). An overview of the entire Festival records group is available here: Smithsonian Folklife Festival records.\nFrank A. Taylor Papers\nFrank A. Taylor (1903-2007) was a Curator of Engineering and Industries and an administrator at the Smithsonian. He was born in 1903 in Washington, D.C., where he grew up. Taylor began his career at the Smithsonian in 1922 as a Laboratory Apprentice in the Division of Mechanical Technology of the United States National …\nThe bulk of this collection was processed by Jane Livermore, a devoted and tireless volunteer in the Smithsonian Institution Archives between 1995 and 2004. Livermore is a former Science Service employee. She worked in the organization's library, oversaw the educational project \"THINGS of Science,\" and served as Assistant to the Director. The …\nDonald H. Sultner-Welles Collection\nThis collection is primarily the work of one individual, Donald Harvey Sultner, known professionally as Donald Sultner-Welles (1914-1981). The collection forms a written and visual record of Sultner's family, life, and career from 1913-1980. Its major strength is Sultner's photographic documentation of the world during his travels, ca. 1950-1980. Work by other photographers …\nAnton Refregier papers\nThe papers of Woodstock area painter, muralist, and designer, Anton Refregier (1905-1979) date from circa 1900 to circa 1990 and measure 35.9 linear feet. The collection records Refregier's early commercial work and murals for the Works Progress Adminstration (WPA) and documents his career through to the 1970s with records of commissions for many public and private buildings, exhibitions in the United States and abroad, teaching positions, essays and publications, and extensive travel, particularly to the Soviet Union and Mexico. The collection contains scattered biographical material, personal and business correspondence, notes and writings, 15 diaries and journals, mural and tapestry files, exhibition files, personal business records, printed material, 10 scrapbooks, artwork including sketches and cartoons for murals, and photographs of Refregier, his friends, family and travels."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6388f35a-7732-4175-8d5e-63c82ee55592>","<urn:uuid:c508ea41-08a1-4a7a-922d-45c2449971c0>"],"error":null}
{"question":"Hey im starting high school and wanna know when do I need to start worrying bout NCAA eligibility and what happens with scholarships after I get one?","answer":"You should start focusing on NCAA eligibility in ninth grade by meeting with academic counselors to ensure you're taking the correct college-prep courses. After receiving an athletic scholarship, you need to know it may be a 1-year or multi-year contract depending on the college. To maintain your scholarship, you must meet minimum required courses per quarter/semester and complete a certain percentage of your degree each year. If you don't meet these continuing eligibility requirements, you risk losing your athletic scholarship. Additionally, coaches often divide scholarships among multiple players rather than giving full scholarships to fewer athletes.","context":["NCAA Eligibility Requirements\nWho Needs to Worry About NCAA College Eligibility?\nPotential student-athletes who are planning to participate in NCAA collegiate sports after high school.\nAthletes that are looking to participate in NCAA Division I athletics.\nAthletes that are looking to participate in NCAA division II athletics.\nWhat You Need to Become Eligible\nNCAA Eligibility Rules State that student-athletes will need to meet athletic and academic requirements for the division they wish to compete in/ Requirements will include:\n- Meeting core course requirements specific to NCAA division I or division II.\n- Meeting grade-point averages on a sliding scale when compared to ACT and SAT test scores for NCAA division I or meeting grade and test requirements for division II.\n- Completion of Amateurism certificate.\nWhat are the Core Course Requirements with the NCAA?\nNCAA colleges require prospective student-athletes to complete and pass what they have titled “core courses” in order to be eligible to compete at the NCAA division I and division II college level. In addition to completing required core courses, potential recruits will also have to maintain at least a 2.0 GPA for course courses (which is subject to increase with new NCAA requirements)\nWhen to Begin Focusing on NCAA Eligibility?\nStudent-athletes will need to begin thinking about their athletic future when they begin ninth grade. This is when student- athletes can meet with their academic counselors to ensure that they are taking the correct college-prep courses. NCAA division I and division II levels require that students pass all required core courses. If you are not sure of where you would like to attend college, but know that you would like to participate in sports than hold off on registering with the NCAA eligibility center until you are sure.\nNCAA Eligibility Clearinghouse\nThe NCAA Eligibility Center, formally known as the NCAA Eligibility Clearinghouse is where recruits will need to register and be cleared in order to participate in college sports. College sports programs will be unable to offer Division I and II recruits athletic scholarships until they have registered with the NCAA Eligibility Center.\nEach of the three NCAA divisions have separate NCAA eligibility Rules in which potential recruits are required to meet. Student-athletes need to decide which Division level they will be best matched for in order to determine which NCAA eligibility Rules they will fall under.\nAthletes who are serious about getting recruited will need to do more than just registering with the NCAA eligibility center. Athletes need to keep in mind that registering with the eligibility center is only necessary if they are planning to compete at the NCAA division I or division II level.\nCollege coaches need to know more about each individual athlete before they will consider giving the athlete an athletic scholarship. Be sure to take the appropriate steps to get recruited; don’t wait around for a coach to find you.\nNCAA Eligibility Rules\nFor possible college recruits to meet NCAA Scholarship Rules they will need to fulfill all college prep core courses set in place by NCAA eligibility .\nAthletic Scholarships that are awarded to NCAA Division I and II athletes will need to be aware of 1-year contract or if they will be eligible for multi-year, according to the college that is offering the scholarship.\nIf you wish to transfer and be released from your college scholarship you will need to ask to be released from your current institution.\nOnce apart of the college Student-Athletes must meet the minimum required courses per quarter or semester in order to uphold athletic scholarship and institution financial aid.\nNCAA Athlete Rules\nMaintaining Amateurism Status\nBefore Competing at the college level as part of the NCAA Eligibility Rules, potential student-athletes are asked to answer several questions regarding their sports participation, in order to ensure that all recruits meet NCAA Athlete Rules and compete on fair ground.\nOnce an athlete is registered with the NCAA eligibility center, the recruit will be given a NCAA ID number that will be used by DI or DII college coaches recruiting the athlete in order to officially be offered an NLI Division III amateurism is completed individually by the college or university the potential recruit is interested in attending.\nAfter being recruited to play college sports this is where athletes will hear the terms “continuing eligibility” and “progress toward degree” which means that student-athletes need to stay on track in order to maintain progress toward a baccalaureate or equivalent degree to stay eligible to at the NCAA level.\nStudent athletes at the NCAA division I and division II level are required to complete a certain percentage of their degree each year they are enrolled in college. If requirements are not met the student-athlete will be in violation of NCAA Scholarship Rules and could be in jeopardy of losing their athletic scholarship.\nLearn more about registering with the NCAA Clearinghouse / Eligibility Center here\nAuthor: David Frank","Many athletes and parents mistakenly believe that if you can’t get an athletic scholarship, you can’t play in college. The reality is that most college athletes do not have athletic scholarships. The demand is way too high, and supply is way too low. There are far more college athletes than there are athletic scholarships.\nWhile coaches would love the opportunity to give every athlete a scholarship, they are limited. Unless you’re one of the country’s top athletes, don’t expect a team to provide you with a full athletic scholarship. A partial scholarship is more likely. College coaches are strictly limited to a certain number of total scholarships based on sport and level. Scholarships are often divided their scholarships among multiple players. If you aren’t offered an athletic scholarship at a college of your choice, it doesn’t mean you should give up your dream of playing in college or remove that college from your list. It means to keep working and pushing to accomplish your goal.\nWhen an Athletic Scholarship Is Not Available\nThe NCAA regulates athletic scholarships to specific limits per University Sport, and at many schools, Division III or Ivy League, there are no scholarships offered. Another factor that must be considered is whether a school funds the full allotment of scholarships per year. Like everything else, don’t let tuition stop you from chasing your dreams – there are usually payment alternatives. This is where you and your parents need to be creative and persistent and reach out to the school’s financial aid office. They can help you find options like scholarships that fit your specific skill sets and background, federal grants, and loans.\nAs mentioned before, a college may offer athletic scholarships, but not fully fund the NCAA/NAIA allowance. For example, a Division I women’s soccer team is allowed a total of 12 scholarships in a given year. The coach can divide those 12 scholarships among as many players as she wishes. So 12 players may get full scholarships, or 24 players might get half scholarships. Coaches tend to choose the latter, dividing scholarships among a larger number of players, allowing more assistance to more players. While this doesn’t fully cover the cost of education expenses, it will help offset and will enable you to play collegiate sports.\nIn your early correspondence with coaches, determine what kind of scholarship money might theoretically be available for you. Find out the size of the scholarship pools from which you’d be drawing. Determine if your candidate teams are fully funded—i.e. if the team you’re researching has the full allotment of scholarships. This can be a strong indicator of a school’s financial commitment to the team and of the team’s commitment to you. It says a lot if a team offers you a 75% scholarship out of its pool of four total scholarships.\nLater In The Process…\nOnce you are well into your recruiting process and the coach at one of your colleges has a strong sense of your ability, discuss your scholarship outlook more specifically. Build on your earlier discussions where you determined how serious the coach is about getting you on the team. Ask the coach if they will offer you an athletic scholarship—and in what amount. Assess the offer and how much your family will have to supplement.\nThe Rules of Athletic Scholarships\nMany rules govern athletic scholarships. If you’re offered a scholarship, go to the NCAA’s website and familiarize yourself with some of the rules. Download a copy of the NCAA Guide for the College-Bound Student-Athlete. Learning about scholarships will prevent you from following dead-end leads and breaking rules that endanger your eligibility.\nInstitutional financial aid is available at many schools as need-based, academic, minority, and departmental scholarships—to name a few categories. These awards are a great way to deal with skyrocketing tuition, though competition for this money is intense. For each college on your list, research the non-athletic scholarship opportunities through the admissions department and financial aid office.\nIn recent years some colleges have found loopholes in NCAA regulations. They offer murky “leadership” or “activities” scholarships that allow them to circumvent NCAA bylaws. Think long and hard if you’re offered a sketchy scholarship and discuss its legality with the coach, admissions department, and perhaps even a lawyer if you feel there is a red flag. If the NCAA determines that it is a violation, you could lose your eligibility."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8048e396-23b1-4c1a-ba24-5033fd3c4044>","<urn:uuid:04e3e906-b1b5-42ec-8c0b-a3f52e7dd5e3>"],"error":null}
{"question":"What are the aesthetic innovations in modern opera productions, and how do these compare to technological advancements in vertical farming methods?","answer":"Modern opera productions like 'Monkey: Journey to the West' blend traditional opera with innovative elements like animation, acrobatics, and electronic music, featuring cartoon segments, martial arts, and amplified vocals in Mandarin. Similarly, vertical farming incorporates technological innovations through automated sensors, imaging techniques, and artificial intelligence for crop monitoring and environmental control. Both fields represent a departure from conventional approaches - opera moving beyond classical forms to include pop culture elements, while vertical farming shifts from traditional horizontal agriculture to controlled indoor environments with precise management of growth factors like light, temperature, and nutrients.","context":["Opera of the future???\nJim Retherford / The Rag Blog\nOpera Meets Animation to Tell a Chinese Tale\nBy Daniel J. Wakin / May 26, 2008\nCHARLESTON, S.C. — The opening scene is a vast cartoon, projected on a scrim, with viewers zooming past clouds and mountain peaks to an egg, which falls, bursts and gives birth to Monkey. The scrim goes up, and the cartoon dissolves into a stage full of flipping acrobats, a monkey tribe flying from bamboo pole to bamboo pole. The score pulsates with an electronic beat.\nIt’s not every opera that manages to fit in contortionists: “Monkey: Journey to the West” combines Eastern and Western traditions, animation and live action.\nSo begins “Monkey: Journey to the West,” a newfangled sort of opera that is making its American debut here at the Spoleto Festival U.S.A. Based on an old Chinese tale, it traces the Monkey King’s search for wisdom and immortality with singing, acrobatics, martial arts and cartoon segments. It is circus spectacle striving to become art — or maybe art infused with spectacle.\nThe show was conceived by the Chinese actor and director Chen Shi-Zheng, with music provided by Damon Albarn, the lead singer of the British pop band Blur, and design, animation and costumes by Jamie Hewlett, who collaborated with Mr. Albarn on the popular animated “cartoon band” Gorillaz.\nThe work is a recent example of the blending of pop strains and classical opera form, a trend seen particularly in Britain, where the English National Opera commissioned the hip-hop Asian Dub Foundation to make a work about the Libyan leader Col. Muammar el-Qaddafi.\nBut do not compare the Qaddafi opera with “Monkey” in front of Mr. Albarn. “I loved what they wanted to do,” he said in an interview at a hotel here, still groggy after a nap. “Great idea, but badly executed.” And never suggest “Monkey” resembles Cirque du Soleil. “No!” he said quickly. “I just don’t want to be pigeonholed.” Not exactly opera, musical theater or circus, “Monkey,” Mr. Albarn said, is a “new kind of thing.”\nAnd it is true: most operas do not have acrobats playing crustaceans in shopping carts juggling parasols with their feet, or extended fight sequences like Hong Kong kung-fu movies, or contortionists wrapping their legs around their heads. And most circuses do not have pit orchestras, a narrative or opera singers.\nThe score is a mix of hip-hop beats, washes of electronic sound, dissonant brass fanfares, sweet Chinese pop melodies and percussive effects. Ten vocalists sing in Mandarin with Chinese opera inflection. There is little dialogue. The heavy amplification made it difficult to tell who was singing at times, or whether the voices were even coming from the stage.\nThe show was a hit when it first played, at the Manchester International Festival in England last year, and the creators have high hopes of cloning it in commercial sites and other opera houses, particularly in Asia. On Saturday night the audience was somewhat tentative, not quite sure whether to applaud after a particularly stunning circus routine. And the opera’s presence on Spoleto’s calendar did not receive a completely enthusiastic response.\nGood programming “doesn’t necessarily represent the tastes of the team doing the programming,” said Emmanuel Villaume, the festival’s orchestra and opera director.\n“There is an incredible entertainment value of ‘Monkey,’ ” he said. “The music is an accompaniment for the visual effects. I won’t say more. People who know about these things say it’s a good score.”\nBut he said the fact that Spoleto was giving “Monkey” its American premiere made it a welcome addition, and it is drawing a diverse audience. “It’s doing the job we want it to do,” he said.\nIts presence has also done something else: It has brought a genuine pop-culture celebrity to Charleston, someone easily recognizable to a younger crowd. (Mr. Albarn became a favorite among the college students at the gelato shop across from the theater, where he would repair to watch British soccer games.) Rarely do festival participants have musicians like Mr. Albarn, whose record sales number in the five-million-plus range.\nBut Mr. Albarn is taking a break from that world. Working on “Monkey,” he said, has taught him about orchestration, and he is working on another “classical” piece, which he calls a tone poem. The chance to make musical works longer than a pop song is liberating, he said.\n“It’s definitely changed my course,” he said. “I love being the author of stuff now,” he said.\n“I just want to keep doing things that take longer to listen to, that doesn’t just happen in three minutes.” His compositional technique involves creating demo tapes or sitting at a keyboard and playing a passage, which an orchestrator then expands upon and notates.\nFor “Monkey,” he said, he created a system to spin out lines of the pentatonic scale’s five notes organically. To demonstrate he drew a five-pointed star and assigned numbers to each point. The numbers stood for notes. Then he started the numbering at different points on the star, creating a sequence of numbers at each. Some passages were built from those kinds of sequences.\nThe production was born when Jean-Luc Choplin, director of the Théâtre du Châtelet in Paris, and Mr. Chen discussed the possibility of a new production. Mr. Chen said he had long wanted to do an opera about the Monkey story, and when the idea appeared possible, he traveled back to China as much as possible for inspiration. (He has lived in New York since leaving China in the late 1980s.) He was introduced to Mr. Albarn and Mr. Hewlett, who themselves were fans of the cult-hit “Monkey” live-action show from Japan that was on British television in the 1980s.\nThe men made long trips to China together. Mr. Albarn recorded sounds, and Mr. Hewlett made sketches. Mr. Chen scouted circuses and found the “very young, very hip kids” he needed in Dalian, the port city in northeast China. Most of the singers are young graduates of Chinese conservatories who studied either Chinese opera techniques or music theater singing or both.\nMr. Chen said he knew he couldn’t use Western classical music. “It’s a story children love,” he said. “The instrumentation has to be very eccentric. The monkey is like a boy. It has to be a boy’s sound.”\nThe characters were shaped by Mr. Hewlett’s cartoon sketches, and the opera essentially turns animated characters into live ones. Makeup for some of them takes more than two hours. Before Saturday night’s performance, the Monkey, Li Bo, sat in a chair having white and red makeup applied to his face. He got up and mugged for the mirror. Mr. Li was supposed to take on performances later in the run, but the main Monkey, Fei Yang, suffered an injury the night before: one of many mishaps because of the elaborate action, Mr. Chen said. A practitioner of Chinese medicine and a Chinese masseuse are on hand for the run.\nDescribing his ambitions for the show to clone itself, Mr. Chen, watching the makeup application, said, “I think we have to have a Monkey training camp.”\nThe Rag Blog","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:5f5f2433-bd1b-4418-9cee-e46ff5c72cf0>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"Could you compare the maximum freezing time for dairy milk versus almond milk? I need these timeframes for organizing my long-term food storage.","answer":"Regular dairy milk can be frozen for up to 3 months when stored properly in mason jars filled to 3/4 full or to the freeze line. In contrast, almond milk can be frozen for up to 6 months when stored in a freezer-safe container with enough space for expansion. However, it's important to note that unlike dairy milk which maintains its quality well when frozen, almond milk's texture may change significantly after freezing and thawing.","context":["Stop letting good food go bad! Get storage savvy and check out this list of foods that you may not know you could freeze for later use. Just remember to freeze items right away in order to maintain maximum freshness, and mark each bag/container with the contents and freeze date.\nFreeze Up to 1 Year\nYou probably know that cooked potatoes freeze well, but what if you want to store potatoes longterm and still have your choice of cooking options? Blanch before you freeze! This allows potatoes to remain in a mostly uncooked state until you're ready to boil, roast, fry, or whatever you'd like to do with them later.\nLess starchy (i.e. waxy) potatoes such as Red Bliss and New Potatoes are preferred for this method over starchy potatoes like Russet and Idaho. For more information on freezing uncooked potatoes, visit this article.\nFirst, make sure they are uniform in size (small potatoes can be left whole). Then blanch by dropping into boiling water for 4 to 7 minutes depending on the size of the potatoes or potato pieces, then transfer right away to ice water to stop the cooking process.\nLet them cool completely, peel, place in a freezer bag and freeze up to one year. No need to thaw them out before cooking.\nBlanching is standard for freezing uncooked vegetables in general, as this allows them to retain both their color and nutrient content while frozen.\nFreeze Between 1 - 3 Months\nSome types of sandwiches (with low-moisture fillings) can be assembled in advance, frozen, and then thawed out in the fridge the day before eating them. Really handy for preparing lunches for weeks (even months!) ahead of time.\nFillings that freeze well:\n- Deli meats\n- Hard cheeses\n- Tuna salad\n- Chicken salad\n- Nut butters (minus the jam/jelly)\n- Egg salad, as hard-boiled egg whites become rubbery\n- Moisture-rich sandwich fillers like tomatoes, lettuce, pickles, and sprouts which will make the bread soggy and/or get wilted. Add these in after thawing.\n- Jellies or jams\nWrap whole sandwiches as airtight as possible (without smushing it!) with plastic cling wrap.\nThen place inside freezer bags. The day before you'd like to eat your sandwich, place it into the refrigerator and allow to thaw overnight.\nFresh Citrus Slices\nFreeze Up to 6 Months\nIf you've got a citrus tree, you probably end up with an overabundance of fruit. Instead of spending loads of time juicing them, slice them up to use for drinks and garnishes later.\nSimply place slices in between pieces of freezer/wax/parchment paper (so they're easy to separate), stack in a freezer-safe container and use as needed. You can add frozen citrus slices directly into drinks, a drink pitcher, or punch bowl. To use as a garnish, place in the refrigerator to thaw for a couple hours or at room temperature for about 30 minutes before using.\nFreeze Up To 6 Months\nHard cheeses such as gouda, cheddar, and Swiss stand up well to freezing. Soft cheeses may be frozen as well for shorter periods of time (a couple weeks to a month), but there may be a significant change in texture.\nWrap well in plastic wrap, place in freezer bags and freeze for up to six months. Allow to thaw completely in refrigerator around 24 hours before eating.\nFreeze Up to 2 months\nIt can be difficult to judge how much whipping cream you need for topping desserts and other applications, especially when it's in a liquid state. No need to worry about waste, though, because you can easily freeze any extra whipped cream for later use.\nPlace individual dollops of whipped cream on a freezer/wax/parchment-lined baking sheet or roasting pan. Place entire sheet in the freezer just long enough for the dollops to harden (for easy handling).\nFinally, transfer the frozen dollops into a freezer bag or freezer-safe container.\nIf using to top a dessert, you can place frozen dollops directly on top of the dessert and let sit at room temperature for about 20 minutes, or in the refrigerator for a couple of hours. You can also place frozen dollops directly on top of hot cocoa, if you don't mind it cooling down your drink a bit. If you'd rather keep your cocoa piping hot, let the whipped cream thaw in the fridge for a couple hours, or around 20 minutes at room temperature before using.\nFreeze Up to 3 Months\nIt's surprising how many people let milk go to waste when it can easily be frozen and stored. The preferred method is in mason jars for their freezer-safe and preservation qualities and because they have a freeze line on them which makes it easy to know how much space to leave within the container to allow for expansion of the frozen liquid.\nFill a mason jar to the freeze line or if using another freezer-safe container, fill up to 3/4 full. Let defrost 24 hours in the refrigerator before using.\nAs with all frozen food items, don't forget to mark the freeze date on the container.","Whether you’ve run out of milk or are looking for a way to extend the shelf life of your almond milk, freezing it may be an option for you.\nKeep in mind that while freezing will prolong the shelf life of your almond milk, it is not a permanent solution. Thawed almond milk will spoil sooner than fresh almond milk would.\nWith that in mind, let’s look at how to freeze almond milk and some tips on how to use it once it has been thawed.\nHow Long Can You Freeze Almond Milk?\nAlmonds are used to make almond milk, a dairy-free substitute for cow’s milk. It has a mild, nutty flavor and can be used like cow’s milk.\nHowever, unlike cow’s milk, almond milk does not contain cholesterol or lactose and is, therefore, suitable for those who are lactose intolerant or have high cholesterol levels.\nAlmond milk can be bought ready-made or made at home by blending almonds and water in a blender. It can be stored in the fridge for up to seven days or frozen for up to six months.\nWhen freezing almond milk, it is essential to use a freezer-safe container and to leave enough space at the top of the container to allow for expansion. Once thawed, you should not refreeze almond milk.\nIs It Safe to Freeze Almond Milk?\nMost people are familiar with the process of freezing milk to prolong its shelf life. However, many are unaware that they can freeze almond milk in the same way.\nFreezing almond milk is an effective way to preserve its freshness and flavor. While some change in texture is inevitable, frozen almond milk will not make it unsafe to drink.\nHowever, it is essential to note that you should use thawed almond milk within a few days. Therefore, it is best to use frozen almond milk for smoothies or baking rather than for drinking straight from the glass.\nHow Long Can You Keep Almond Milk in the Refrigerator?\nMost commercial almond milk products have a shelf life of about seven to ten days, although this may vary depending on the manufacturing process and the addition of preservatives.\nHomemade almond milk will last for 7-10 days in the fridge. It’s important to note that almond milk can go bad before its expiration date if it is not stored correctly. Exposure to heat, light, or oxygen can cause the milk to spoil more quickly.\nTo prolong almond milk’s shelf life, keep it in a cool, dark place in an airtight container. When choosing a commercial almond milk product, always check the “sell by” date and make sure that it has been stored in a cool, dark place.\nIf you are unsure whether or not your milk has gone bad, it is always best to err on the side of caution and throw it out. Trust your senses- if the milk smells off or tastes sour, it has probably gone bad and should not be consumed.\nAlthough almond milk is a delicious and nutritious alternative to cow’s milk, it does have a relatively short shelf life. By following these storage tips, you can help ensure that your milk will stay fresh for as long as possible.\nHow Long Does It Take Almond Milk to Unfreeze?\nWhen thawing almond milk, it is essential to use lukewarm water rather than hot water. Hot water can cause the milk to become too warm, affecting flavor and texture.\nLukewarm water will help to thaw the milk more quickly and evenly without affecting its quality. Depending on the amount of milk, it should take 2-3 hours to thaw completely. Once thawed, the milk should be used immediately or refrigerated for later use.\nWhy Shouldn’t You Freeze Almond Milk?\nAlmond milk is a popular dairy-free alternative to cow’s milk. However, many people are unaware that they should not freeze almond milk.\nWhen exposed to cold temperatures, the fats and proteins in almond milk undergo biochemical changes. These changes can alter the milk’s texture, color, and flavor, making it less enjoyable to drink.\nIn addition, freezing and thawing almond milk can cause it to separate and become watery. For these reasons, it is best to consume almond milk within a few days of opening the carton. When stored in the fridge, almond milk will remain fresh and flavorful for up to seven days.\nCan You Freeze Almond Milk for Smoothies?\nIs it possible to freeze almond milk and then use it to make smoothies? The answer is yes, freezing almond milk is perfectly fine. Freezing almond milk can help to prolong its shelf life.\nWhen freezing almond milk, use an airtight container. An airtight container will help prevent freezer burn and keep the milk from picking up unwanted flavors from other foods in the freezer.\nWhen thawing frozen almond milk, it’s best to do so overnight in the refrigerator. Once thawed, the milk may have a slightly different texture, but it will still be safe to drink. So go ahead and stock up on your favorite brand of almond milk – your smoothies will thank you later!\nWhich Is Healthier, Oat Milk or Almond Milk?\nOat milk and almond milk are two popular options for those looking for a healthy and delicious alternative to cow’s milk. Both are rich in nutrients and low in calories, but which is the better choice?\nWhen it comes to nutrient content, oat milk is the clear winner. It is a good source of fiber, protein, and vitamins, including A, D, and B12.\nIn contrast, almond milk is relatively lacking in nutrients, except for calcium, vitamin E, and a bit of protein. What it does have going for it is that it is lower in fat than oat and cow’s milk. So, if you are watching your fat intake, almond milk may be the better choice.\nIn terms of taste, both oat milk and almond milk are similar to cow’s milk, although some people find that oat milk has a slightly sweeter flavor.\nUltimately, the decision of which is better depends on your personal preferences. If you are looking for a nutritious option, go with oat milk.\nOn the other hand, if you are watching your fat intake, opt for almond milk. Whichever you choose, you can rest assured that you are making a healthy choice.\nCan You Freeze Almond Milk to Make Ice Cream?\nCan you freeze almond milk to make ice cream? Unfortunately, the answer is not as simple as it might seem. Almond milk is a dairy-free alternative to cow’s milk made from almonds that have been soaked and ground into a paste.\nWhile it can be a good choice for lactose intolerants or people allergic to dairy, it does not freeze well. When frozen, almond milk tends to separate and form clumps. As a result, it is not ideal for making ice cream.\nHowever, some brands of almond milk are specially formulated to freeze well. If you are interested in making ice cream with almond milk, it is best to check the label to see if the product is designed for freezing.\nCan You Freeze Almond Milk Creamer?\nCan you freeze almond milk creamer? Yes, you can. Although freezing almond milk creamer may cause it to separate and become grainy, it will still be safe to drink.\nIf you are going to freeze almond milk creamer, it is best to do so in an airtight container. This will help to prevent freezer burn and keep the creamer fresh for longer. When thawing frozen almond milk creamer, it is best to do so overnight in the refrigerator.\nOnce thawed, the creamer may not be as smooth as it was before freezing, but it will still be safe to consume.\nWhat Happens if You Drink Almond Milk After Seven Days?\nAfter seven days, almond milk begins to spoil. This is because the milk has no preservatives and is made from raw almonds. As a result, the nutrients in the almond begin to break down, and the milk starts to sour.\nWhile some people may still choose to drink sour almond milk, it is not recommended as it can cause digestive issues.\nIf you choose to drink spoiled almond milk, check for signs of food poisoning, such as vomiting or diarrhea. If you experience any of these symptoms, please seek medical attention immediately.\nCan Unsweetened Almond Milk Be Frozen?\nAlmond milk is a popular dairy alternative made from almonds and water. It is relatively low in calories and fat and a good source of vitamins and minerals such as calcium and vitamin E.\nWhile almond milk can be enjoyed fresh or simply chilled, some people prefer to freeze it for a later date. However, it is essential to note that freezing almond milk can cause it to separate and thicken.\nAs a result, it is not recommended to freeze unsweetened almond milk unless you are planning to use it for baking or cooking purposes.\nIf you choose to freeze unsweetened almond milk, thaw it in the refrigerator overnight before using it. Otherwise, you may end up with clumps of milk solids in your final product.\nCan You Freeze Almond Milk in the Carton?\nAlmond milk is a tasty and healthy alternative to dairy milk and can be used in various ways. However, one question that often comes up is whether or not almond milk can be frozen.\nUnfortunately, the answer is no. The carton that almond milk is typically packaged in is not designed to withstand the freezing process, and as a result, the milk will expand and burst the carton.\nThis can cause a messy cleanup and wasted milk. If you need to freeze almond milk, transfer it to a freezer-safe container before placing it in the freezer.\nHow to Freeze Almond Milk\nThere are two ways to freeze almond milk. You can make them into ice cubes or freeze the milk in containers.\n1. Make Them Into Ice Cubes\nFrozen almond milk cubes are a great way to have fresh almond milk on hand at all times. They are easy to make and only require a few simple ingredients. To freeze almond milk, first, pour the milk into an ice cube tray. Then place the tray in the freezer for 2-3 hours.\nAfter the ice cubes are set, you can pop them out and put them in a zip-lock bag. Lastly, remember to label the bag with a date so that you’ll know when it expires. Frozen almond milk cubes are a convenient and delicious way to enjoy fresh almond milk any time of day.\n2. Freeze in a Container\nFreezing is an excellent option if you’re looking for a way to extend the shelf life of your almond milk. Follow these steps to freeze your almond milk so it will be ready when you need it:\n- Measure your containers to know how much milk they can hold. This way, you’ll know how many containers you need to use.\n- Pour your almond milk into the containers. Be sure to leave enough headspace so the milk can expand as it freezes.\n- Label your containers with the current date. This will help you keep track of when the milk expires.\n- Pop the containers in the freezer overnight.\nIn the morning, you’ll have delicious frozen almond milk that will last for months.\nCan You Freeze Almond Milk Yogurt?\nYes, you can technically freeze Almond milk yogurt, but it’s essential to know that the quality will be altered after thawing. Separation will occur, so the texture and consistency will be different from what it was initially.\nIf you still want to freeze almond milk yogurt, make sure to do so in an airtight container to prevent freezer burn. When ready to eat, thaw the yogurt in the refrigerator overnight before consuming it. Remember that it’s best to consume frozen yogurt within two months for optimal quality.\nAlthough freezing almond milk is possible, it is not recommended because the milk will change in texture and taste.\nIf you must freeze almond milk, be sure to thaw it slowly in the refrigerator so that you can enjoy its smooth consistency and delicious flavor."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3a2bbfca-6509-4828-a2f1-5305a1e8c795>","<urn:uuid:9e67cd2d-c24e-4fd5-8ebc-aaa03895030f>"],"error":null}
{"question":"Fellow materials engineers! Need a quick comparison between notch depth specs for standard impact testing vs bottle thickness measurement capabilities of CHRocodile M4 - minimum requirements?","answer":"The standard notch depth in impact testing methods (both Charpy and Izod) is precisely 2mm, while the CHRocodile M4 system is designed for measuring glass bottle thickness inline during production, without any notch requirements since it's a non-contact optical measurement system. The M4 system measures thickness and roundness directly on the glass body during the production process.","context":["Optical Probes for thickness and distance measurements\nThe measurement range of the chromatic optical probes reaches frome a few hundreds of microns up to millimeters allowing you the perfect tune to your applications\nHigh numerical aperture and small spot diameter are ideal for the most uses in metrology.\nThe interferometric optical probes show off …\nOptical Sensor: Distance and thickness measurement with 10 channels\nThe optical sensor CHRocodile M10 offers up to 10 independent measuring chanels. They can be used to measure the distance to any surface as well as the thickness of transparent materials.\nWith the CHRocodile M10 the thickness of non transparent materials like metal bands and paper can be measured quickly and efficiently.\nThe outstanding properties of the PRECITEC sensors, the quality and simple setup and ease of production process integration remain the same. The CHRocodile M10 can also be used in the laboratory and for offline applications.\n5 Channel Optical Sensor: Contactless measurement of wafers and solar cells\nThe new CHRocodile MI5 is the modular 5 channel version of the already well established CHRocodile IT sensor. It performs high-precision, non-contact distance and layer thickness measurements on wafers and solar cells at up to 5 different locations. With one measuring head per channel, it is capable of measuring up to 1 mm thick silicon from one side. Measurements can also be taken on other common infrared transparent materials, such as GaAs.\nThe basis for this nondestructive measuring method is an interferometric sensor which examines the substrate with infrared light. The CHRocodile MI5 is designed for simple and direct integration into the production process. One p…\nVery high lateral resolution to measure intricate structures. Optimized for inline measurement of thick foils.\nptical Sensor for Non-contact distance thickness measurement\nThe new sensor CHRocodile S, with its modern design, represents the latest generation of optical distance and thickness measuring devices. It is applicable for highly accurate distance and topography measurements as well for measuring optically transparent materials (glass, films, varnish etc.). Its low price now enables customers of all budgets to benefit from the numerous advantages of the CHRocodile sensors.\nThe sensor can be used like a table top unit but it is also easy to integrate into production facilities and machines. All measuring data are available for further processing on its multitudinous interfaces (USB, RS232/422, analog). The synchronization can be done by using trigger and encoder si…\nThe new CHRocodile IT measuring system registers the silicon thickness with just one sensor\nThe new CHRocodile IT from Precitec Optronik now offers very simple and at the same time highly accurate layer thickness measurement for wafers and chips. It is capable of contact-free silicon scanning with just one measuring point and can measure a wafer precisely to a thickness of 1 mm.\nThe background to this new, non-destructive measuring process is a sensor that works with interferometry, using infrared light and not, as is usual, white light. Advantage: One measuring point with very bright light allows measuring speeds up to five times faster under th…\nContact-free measurement of bottle thickness and roundness with the new CHRocodile M4\nThe new thickness measurement system CHRocodile M4 from Precitec Optronik now provides a particularly economical solution for inline inspections during the production of bottles and container glass.\nThe system from the successful CHRocodile range of sensors directly records the thickness and roundness of the glass body without contact and with high precision during the ongoing production process. This means that it is not necessary to take and measure samples during the production process, so as to readjust the system if necessary following the production of a l…\nPushed by Xenon\nIf your target is topographic evaluation of large areas: try the high speed sensor CHRocodile X. This task is perfectly fulfilled up to 14 times faster compared to the former model CHR 150 E. With its Xenon technology the CHRocodile X achieves a significantly higher light emission than with the previously used Halogen bulbs in the CHR 150 E. This leads to the maximum scanning rate of 14 kHz on almost all surfaces.\nThe new generation of optical sensors from Precitec Optronik “CHRocodile” sets new standards in optical distance and thickness measurements. With it?s fast 4 kHz technology the CHRocodile provides advantages particularly in topographic evaluations on high reflecting surfaces: The duration of measurement is reduced by factor 4. The CHRocodile E is an advancement of the reliable sensor CHR 150 E. The method of mea…\nApplication: thickness measurement of wafer, PET-bottles and plastic foils\nThe new optical sensor for thickness measurement from Precitec Optronik is the CHRocodile IT 18 – 3000.\nThe CHRocodile IT 18 – 3000 covers the largest measuring range in the IT series of proven and highly precise CHRocodile sensors. The measuring range in air extends between 18 – 3000 µm. This corresponds to a measuring range of 5 – 800 µm in silicon (refractive index n ≈ 3.6). For film or plastic (n ≈ 1.5) the measuring range is between 12 and 2000 µm.\nWith the new CHRocodile IT 18 – 3000 the user can measure inline the thickness of the following materials:\nRough-cut or textured wafers\nOpaque and scattering materials, for example PP- and PET-bottles\nMultilayer systems, for example foils and bonded wafers","This set of Engineering Materials & Metallurgy Multiple Choice Questions & Answers (MCQs) focuses on “Impact Tests on Metals, Fatigue Tests”.\n1. The tendency of a ductile material to act as brittle is known as __________\nd) Notch sensitivity\nExplanation: Some ductile materials are likely to behave as a brittle material in the presence of notches. This behavior is termed as notch sensitivity of the material.\n2. What is the V-notch angle found on an impact testing machine?\na) 30 degrees\nb) 45 degrees\nc) 60 degrees\nd) 90 degrees\nExplanation: Both Izod and Charpy tests are conducted on the same machine with only a minor change. Both tests share the V-notch angle characteristic which is 45 degrees.\n3. What is the size of the specimen used Charpy test?\nExplanation: Charpy and Izod are the two impact testing methods used for impact testing of materials. The Charpy test employs the usage of a test specimen of size 55 mm * 10 mm * 10 mm.\n4. The specified fixed in the vise signifies a ________ form.\nb) Fixed beam\nc) Simply supported beam\nd) Overhanging beam\nExplanation: Izod test uses a cantilever specimen of size 75*10*10 (mm) for impact testing. The specimen is placed in the vise such that it is a cantilever.\n5. What is the depth of notch seen in impact testing methods?\na) 2 mm\nb) 5 mm\nc) 9 mm\nd) 12 mm\nExplanation: Charpy and Izod are the two impact testing methods used for impact testing of materials. Both the methods share a V-notch angle of 45 degrees and the same depth of notch of 2 mm.\n6. Which of the following is not applicable to fatigue fracture?\na) Loss of strength\nb) Loss of ductility\nc) Loss of electrical conductance\nd) Uncertainty in strength and service life\nExplanation: Fatigue fracture is defined as the fracture that takes place under repeatedly applied fatigue stresses. It results in loss of strength, ductility, and increased underinsured strength and service life.\n7. The fatigue fracture occurs __________\na) Below yield stress\nb) Below tensile stress\nc) At stress-strain point\nd) Above tensile stress\nExplanation: Fatigue fracture is defined as the fracture that takes place under repeatedly applied fatigue stresses. It generally occurs at stress well below the tensile stress of the materials.\n8. What kind of stress does this graph show?\na) Reversed stress\nb) Fluctuating stress\nc) Irregular stress\nd) Constant stress\nExplanation: There are basically three kinds of stress cycles of fatigue loading. The graph in the question illustrates that of a fluctuating stress. Reversed stress is similar to fluctuating stress but it travels in both quadrants of the graph. Irregular stress is the third kind of stress cycle observed.\n9. Fatigue of materials is characterized by ________ curve.\nExplanation: In the case of high-cycle fatigue, a Wohner curve is used to depict its performance. This is also known as an S-N curve. This curve plots of the stress applied to the number of cycles till failure occurs.\n10. How can the mean stress of fatigue be evaluated?\na) Pascal’s equation\nb) Soderberg equation\nc) Heisenberg’s equation\nd) Goodman relation\nExplanation: The Goodman diagram is used to plot the mean stress against the alternating stress. This is used to show when a material fails under a number of cycles. This is also known as the Haigh diagram or Haigh-Soderberg diagram.\n11. The stress below which the material does not fail is called _________\na) Fatigue stress\nb) Endurance limit\nc) Fatigue life\nd) Total stress\nExplanation: Fatigue limit or endurance limit is defined as the value of stress below which the material does not fail even as it is loaded for an infinite number of cycles. The total number of cycles required to bring about the final fracture is called fatigue life.\nSanfoundry Global Education & Learning Series – Engineering Materials & Metallurgy.\nTo practice all areas of Engineering Materials & Metallurgy, here is complete set of 1000+ Multiple Choice Questions and Answers.\n- Get Free Certificate of Merit in Engineering Materials and Metallurgy\n- Participate in Engineering Materials and Metallurgy Certification Contest\n- Become a Top Ranker in Engineering Materials and Metallurgy\n- Take Engineering Materials and Metallurgy Tests\n- Chapterwise Practice Tests: Chapter 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n- Chapterwise Mock Tests: Chapter 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n- Buy Engineering Materials Books\n- Practice Metallurgical Engineering MCQs\n- Apply for Metallurgical Engineering Internship\n- Apply for Engineering Materials Internship\n- Buy Metallurgical Engineering Books"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:97832191-3f12-4fba-a51d-3a5c8bbc1160>","<urn:uuid:d488803a-7844-454b-9e3d-4f30f0ce985b>"],"error":null}
{"question":"How do the California Jazz Conservatory and Folkwang University of the Arts differ in their approach to offering public music performances, and what impact does this have on their students' real-world experience?","answer":"Both institutions actively integrate public performances into their educational model, but with different scales and focuses. Folkwang University produces over 300 public events annually across its eight in-house stages, providing extensive performance opportunities for its 1,600 students. The California Jazz Conservatory, on the other hand, operates on a more intimate scale with a single 100-seat venue where faculty and students perform throughout the year. While Folkwang partners with cultural institutions around its region, the CJC uses its venue as a 'real-world incubator' specifically designed to help students gain audience exposure in the competitive Bay Area environment.","context":["Folkwang University of the Arts\n- Study music, theatre or dance at a combined art and music university in Germany.\n- Gain knowledge of the professional and everyday world of artists.\n- Interact and learn with students from all over the world.\n- No German language required.\n- Collins Scholarship available. See Further Reading below for more information.\nThe Ruhr region, in the state of North Rhine-Westphalia, has a population of approximately 5.2 million and is the largest urban conglomeration in Germany. It consists of several large, formerly industrial cities bordered by the rivers Ruhr to the south, Rhine to the west, and Lippe to the north. For 2010, the Ruhr region was named one of the European Capitals of Culture.\nStudents live in Essen but can easily take courses at any of the four Folkwang campuses - Essen, Duisburg, Bochum and Dortmund. In addition to Folkwang University of the Arts, Duisburg-Essen, Bochum and Dortmund all have multiple higher educational institutions in the cities, making this region rich with student life and interesting activities.\nFor more than 80 years Folkwang has stood for outstanding teaching, learning and production in the arts. The \"Folkwang School of Music, Dance and Speech\" was founded in 1927 by opera director Rudolf Schultz-Dornburg, stage designer Hein Heckroth, and choreographer Kurt Joos. In 1928 the school merged with the National School of Trade and Applied Arts and later became the Folkwang University of the Arts. Its main campus is in the former Werden Abbey in Essen, with additional facilities in Duisburg, Bochum and Dortmund. All four campuses are open to KU students.\nFolkwang hosts about 1,600 students from almost every country in the world and produces more than 300 public events each year. It is known for its innovative and interdisciplinary approach to the arts and its role as a training ground for developing young artists. The university has eight in-house stages and partners with cultural institutions around the region.\nFolkwang offers a full spectrum of arts disciplines. Refer to the Academic Information section below for additional information on fields of study.\n- Communication Design\n- Industrial Design\n- Music and Instrumental Training with the following instruments: Accordion, Baroque cello, Baroque flute (Traverso), Baroque violin, Bassoon, Cello, Clarinet, Double Bass, Flute, Guitar, Harp, Harpsichord, Horn, Oboe, Organ, Percussion, Piano, Recorder, Trombone, Trumpet, Tuba, Viola, and Violin.\n- Integrative Composition: Composition and Visualization, Electronic, Instrumental, and Pop.\n- Jazz: Performing Artist, Artistic Producer, and Improvising Artist\n- Music of the Middle Ages\n- Voice: Concert, Song, Oratorio, and Music Theatre\n- Orchestral Playing\n- Music Teaching\n- Music Pedagogy\nTheatre and Dance\n(placements more limited in these subjects, but still possible)\n- Physical Theatre\n- Dance: Choreography/Interpretation/Movement Notation/Movement Analysis\n- Dance Pedagogy\nKU students may take courses at any of the four Folkwang campuses. Many courses are offered in English, although theory courses (not applicative work) and some others require knowledge of German. Students studying music will need to identify a faculty mentor if they desire to take individual instrument/voice lessons. Some mentors may have limited availability to accept exchange students. Students not needing individual lessons can take regular courses offered at the university. To learn more about being an exchange student at Folkwang see the Incomings section.\nStudents typically earn 12 to 18 KU credit hours during one semester abroad. You are encouraged to use the following resources to research available courses and to determine if these courses will meet your academic requirements at KU:\nFolkwang Study Courses:\nView Folkwang Study Courses for a comprehensive list of available areas of study at Folkwang.\nFolkwang Course Catalog:\nUse the Folkwang Course Catalog, called the \"Folkwang Organizer\", to search specific course options. Individual and group lessons may not be included.\nKU Course Equivalency Database:\nSee KU Course Equivalency Database for a list of courses currently pre-approved for KU credit (approved equivalency) and courses that students have had approved for KU credit in the past (possible equivalencies). Use 'Folkwang' in the search field.\nGerman Language Courses:\nGerman language courses are offered to exchange students before and during the semester. More information on German language course options is provided by the Folkwang International Office upon acceptance to the program.\nCredit Approval Process\nVisit Credit Approval Process for detailed information on the procedure for obtaining KU course equivalency approval for courses you plan to take abroad. All courses taken and grades earned while abroad must be posted as KU equivalents on a student’s KU transcript and grades will figure into a student’s KU GPA. The relevant KU academic department will make the final decision regarding posting of credit, and their application toward graduation.\nFolkwang guarantees affordable student accommodation for all exchange students. Housing is arranged through the Studentenwerk Essen-Duisburg (Essen-Duisburg Student Union) and most exchange students will live in Essen. Both private and shared student units are available; price varies based on the room type. Students use kitchen facilities in their accommodations to prepare meals, or purchase inexpensive student meals in campus cafeterias.\nSAGE is dedicated to creating international engagement opportunities that meet the needs of all our students and providing resources to support you through the process of studying abroad. Study abroad is achievable for students of all identities including our BIPOC and LGBTQIA+ students, students with disabilities, and students with religious/non-religious viewpoints. Before selecting a program, consider how your identities may impact your experience abroad in unique ways and talk with your Program Coordinator about any program-specific concerns. Students with documented disabilities should discuss any accommodation needs with their Program Coordinator early in the planning process.\nCheck our Identity Abroad page and resources listed below for information specific to you and other students who may be on your program.\nOpen to KU undergraduate and graduate students with a minimum 2.75 cumulative GPA (exceptions considered). Students should be able to produce a portfolio sample upon request by Folkwang.\nDrs. Dean T. and Elisabeth Collins Scholarship\nFund created by estate gift from the late Dean and Elisabeth Collins. Awards up to $10,000 for students on semester or $20,000 for year long programs to Germany. Open to all majors but preference for students studying music and particularly opera. Minimum 2.5 GPA requirement. Apply online via the Collins Scholarship application.\nGerman Academic Exchange Service (DAAD)\nA wide range of scholarships are also available through the German Academic Exchange Service. Please contact KU Study Abroad & Global Engagement for more information.","The California Jazz Conservatory (F/K/A Jazzschool, Inc.) is an innovative nonprofit organization dedicated to the study and performance of jazz — America’s indigenous art form — and related styles of music from around the world. Founded in 1997, the institution has recently been granted accreditation by the National Association of Schools of Music (NASM), making it the only accredited, stand-alone conservatory devoted to jazz studies in the United States. The school is located in the heart of the vibrant Downtown Berkeley Arts District, home to Berkeley Rep, the Freight & Salvage and many other arts organizations.\nHailed as one of the nation’s most comprehensive schools of jazz and related styles of music, the CJC offers instrumentalists and vocalists of all ages and skill levels a broad spectrum of performance ensembles, lectures, workshops and private instruction. Courses are designed for the professional musician, the serious student and the jazz aficionado seeking personal enrichment. A multifaceted institution, the CJC comprises several key components:\nCALIFORNIA JAZZ CONSERVATORY\nThe California Jazz Conservatory (formerly the Jazzschool Institute) is a music school offering the aspiring professional a comprehensive 4-year degree program culminating in the Bachelor of Music in Jazz Studies. The CJC provides a challenging curriculum of performance, jazz theory and improvisation, ear training, composition, arranging, transcription and analysis, and music history courses, taught by professional artists and educators. Coursework also includes music business, music technology and financial planning, thoroughly preparing students for a fulfilling career in the performing arts.\nJAZZSCHOOL COMMUNITY MUSIC SCHOOL (JCMS)\nThe JCMS provides music education and performance opportunities for people of all ages and levels of ability. Consisting of the JCMS Adult Music Program and the JCMS Young Musicians Program, the JCMS operates year-round on a quarterly basis and offers over 100 ensembles, courses and workshops to build all aspects of musicianship. Students are routinely featured in concert performances, open to the public. With a formidable faculty of renowned musicians and educators, the JCMS is a cornerstone of the Bay Area’s thriving music scene.\nJCMS ADULT MUSIC PROGRAM\nThe Jazzschool Adult Music Program offers life-enriching opportunities to learn about and play jazz and related styles of music from throughout the world. Courses are available for both instrumentalists and vocalists, and everyone from young adult to senior citizen is welcome. Instruction and performances occur primarily in late afternoons, evenings and on weekends, making the program easily accessible to the working professional.\nJCMS YOUNG MUSICIANS PROGRAM\nThe Jazzschool Young Musicians Program offers middle and high school students of all skill levels a wide variety of combos, big bands and vocal performance courses throughout the entire year, as well as camps during the summer. Select ensembles participate in national competitions, and Jazzschool bands have routinely taken top awards from DownBeat, the Monterey Next Generation Jazz Festival and others.\nVisiting artists and Jazzschool faculty lead fascinating and practical workshops, typically a single session of two or three hours in length and held on a Sunday. Content may focus on any specific aspect of jazz theory or performance, such as playing in odd time signatures or the art of vocal improvisation. Past workshops have been taught by prominent musicians from Bobby McFerrin to Dave Liebman, Janis Siegel, Peter Erskine, Sheila Jordan, Nguyên Lê and others.\nThe CJC facility includes an intimate venue with seating capacity of 100. Throughout the year, Jazzschool faculty and students perform for the public, providing a real-world incubator for developing new concepts and gaining audience exposure in the competitive Bay Area environment. High-profile artists have performed at the Jazzschool, including Terence Blanchard, Art Lande, Kim Nalley, Taylor Eigsti, Sachal Vasandani and many more.\nThe CJC’s in-house retail store is a treasure trove of jazz-related reading and listening materials. It provides students, teachers and the public with convenient access to extraordinary books, recordings, videos and accessories."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3f670d9e-e36b-46a4-bef0-9ab173eaf1f7>","<urn:uuid:93006c19-9d65-4511-94b2-1a260b5c5925>"],"error":null}
{"question":"Compare email authentication protocols vs attack chain disruption in preventing cyberattacks?","answer":"Email authentication protocols like DMARC work by verifying domain ownership and using SPF and DKIM standards to authenticate emails, helping prevent BEC attacks through email phishing and domain spoofing. It offers three policy levels: monitor, quarantine, and reject, with reject providing the strongest protection by completely blocking unauthorized emails. In contrast, breaking the attack chain (or kill chain) focuses on identifying and blocking malware before it can execute by understanding infiltration and deployment methods. This requires a multi-layered endpoint protection strategy since traditional signature-based security alone is insufficient against sophisticated modern threats. Both approaches are important but target different aspects of cybersecurity - email authentication prevents impersonation attempts while attack chain disruption stops malware execution.","context":["Technology and its rapid strides have made cybersecurity and related crimes a major area of concern for businesses. Emails are a key source of infiltrating businesses and their internal IT systems. Commonly referred to as BEC or Business Email Compromise, attackers use emails-based malicious tactics to gain access to the company’s business email account and commit fraud against its partners, customers, or employees. BEC attacks also make target organizations vulnerable to future security breaches, massive data losses, and financial asset compromise.\nBEC attacks are not just for enterprise-level organizations or big MNCs. In actuality, they also target small and medium-sized enterprises (SMEs) too. Here are some more details about BECs to help you better understand this cybersecurity threat:\nCommon Types of BEC Attacks\n- Fraudulent CEOs – Attackers impersonate the company’s CEO or any other executive and send an email to finance personnel requesting the transfer of money to a bank account they control.\n- Compromised Account – The email account of a senior executive or employee is stolen and used to solicit invoice payments from vendors mentioned in their email contacts. The funds are subsequently transferred to bogus bank accounts.\n- Attorney Impersonation – Attackers pose as a lawyer or a member of a law firm and make fraudulent requests for sensitive and confidential information. Requests are usually made via email or phone.\nUnderstanding Email Authentication\nEmail authentication includes deploying standards and protocols to gather verifiable information on the origin of emails. This is accomplished by verifying the domain ownership of the mail transfer agent(s). The industry standard for email transfer, Simple Mail Transfer Protocol (SMTP), does not have a built-in mechanism for message authentication. As a result, cybercriminals can easily launch domain spoofing and email phishing attacks by abusing the absence of security. This emphasizes the significance of email authentication protocols such as Domain-Based Message Authentication, Reporting, and Conformance (DMARC).\nHow to Prevent BEC Using DMARC?\nStep 1: Implementation\nConfiguring DMARC on your domain is the first step in preventing BEC attacks such as email phishing and domain spoofing. SPF and DKIM authentication standards are used by DMARC to authenticate emails sent from an organization’s domain. It gives the domain owner complete control of the receiver’s response and tells receiving servers how to handle emails that fail one or both of these authentication tests.\nTo implement DMARC:\n- Check all authorized email providers for your domain.\n- To configure SPF for your domain, publish the SPF record in your domain.\n- To configure DKIM for your domain, add a DKIM record to your DNS.\n- To configure DMARC for your domain, add a DMARC record to your DNS.\nFor a step-by-step guide to configuring DMARC on your domain, head to DMARC Setup Guide.\nStep 2: Enforcement\nYou can configure your DMARC policy to:\n- Monitor Policy (p= none)\nThe first policy is the monitor policy, i.e., p= none. This policy, as the name suggests, mainly focuses on monitoring and providing insights into the email channel. The configuration won’t impact the deliverability of the email but will only give information on who’s sending the email on behalf of the domain.\n- Quarantine Policy (p= quarantine)\nThe second policy is the quarantine policy, i.e., p= quarantine. If an email passes the DKIM and/or SPF authentication checks, then it is directly placed in the primary inbox. But if the email fails authentication, then it is sent to the junk or spam folder. Thus, the quarantine policy mitigates the impact of spoofing, but it does on entirely block the spoof email.\n- Reject Policy (p= reject)\nThe third policy is the reject policy, i.e., p= reject. If a message fails the DKIM and/or SPF authentication checks, then this policy instructs the receiver to delete the message entirely. However, if an email passes the authentication check, then it is delivered to the primary inbox of the receiver. This policy entirely eradicates the threat of spoofing.\nImplementing and enforcing DMARC on your business email domain helps you drastically lower BEC attack risks for your organization. For more in-depth information on DMARC and its policies, head to What is DMARC?","Financial Costs of a Ransomware Attack and Breaking the Attack Chain Ransomware is a type of malware that typically utilizes encryption to impede or restrict admittance to information until a payoff is paid.\nFor organizations that experience the ill effects of a ransomware attack, the blow-back to income is in many cases more terrible than the size of the payoff and regardless of whether to pay it. The monetary harm can be far reaching and go a long ways past how much the payment.\nThe Financial Costs of a Ransomware attack can be huge. In addition to the ransom, businesses lose business due to downtime. These losses can quickly compound and spiral out of control. For example, one ransomware attack affected Maersk and left the company unable to operate for weeks. The company estimates that the downtime cost them $300 million.\nSpecialists suggest that organizations don’t pay ransoms as it gives cybercriminals a thought process to proceed. Organizations that really do wind up paying the payoff are frequently disheartened with the outcomes.\n- The information they recuperate is harmed.\n- The aggressors request more cash.\n- The aggressors disappear, and they don’t recuperate their information.\nLate investigations by Sophos and Pao Alto put the normal ransomware attack costs at somewhere in the range of $570,000 and $812,360.\nAs cybercriminals now utilize hilter kilter encryption strategies, having the option to unscramble the information is profoundly impossible. If you would rather not pay the payment, you will either need to recuperate the information from reproductions or reinforcements or lose it through and through.\nAt the point when you experience a ransomware attack, it is smarter to pick up and move on and follow your occurrence reaction plan. In the event that you have a viable recuperation plan set up, you might have the option to recuperate your information with negligible disturbance, and you won’t have to pay the payment. A recuperation plan typically includes five stages: survey, moderate, answer, convey, and hindsight.\nCounteraction is in every case better compared to attempting to manage the broad harm a ransomware attack can cause. Figure out more about how to diminish the gamble of turning into a ransomware casualty in any case at Discernment Point.\nCyber Insurance Academy\nThe cyber insurance industry is in a state of flux as more organizations are exposed to ransomware attacks. These attacks see hackers infect an organization’s computer network and demand money in return for control. These attacks have resulted in a massive increase in ransom payments, increasing by three-fourths to $412 million by 2020. In response, the cyber insurance industry has started an educational program to prepare students for such attacks.\nCyber insurance could take a cue from other forms of insurance. One professor at King’s College London studied the phenomenon of kidnap for ransom insurance and discovered that the insurance company’s “disruptive bargaining” strategy helped reduce kidnap gangs’ demands.\nWhile ransomware is becoming increasingly widespread, some insurers are beginning to reconsider their policy and stop covering ransom payments. For example, AXA, which insures several large French companies, has decided to stop covering payments to ransomware attackers for future policyholders. This move has come in response to government pressure.\nGovernments are eager to provide adequate cyber insurance coverage but don’t want insurers’ solvency to suffer. The failure in either direction could lead to a financial burden on governments. Cyber insurance companies are increasingly working with outside firms to vet their insureds’ security protocols and procedures.\nBy restoring your computer and all of its data with NeuShield, you can stop ransomware attacks from destroying your information. Ransomware typically attempts to encrypt and wipe a disk, but NeuShield protects your data by restoring it to the original state. It does this by creating an undetectable overlay over your data. The attacker only has access to the data that is in the overlay – all of your original files are preserved. Moreover, NeuShield can be restored to a previous state, allowing you to regain access to your information quickly and easily.\nAnother key aspect of defending your computer is breaking the attack chain. By understanding the structure of cyber attacks, you can identify and block them before they start. The attack chain is also known as the kill chain, and it is originally a military concept. It describes the methods of malware infiltration, deployment, and execution. Basically, breaking the attack chain means to prevent the attacks before they begin, but it is not as easy as it sounds. Whether you are a small or large business, the idea of breaking the attack chain is important and a vital part of cyber security.\nCybercrime has continued to evolve rapidly, and the scope of its attacks increases. As a result, future attacks will be much harder to detect and respond to. This arms race between attackers and defenders is becoming more apparent. In many ways, this is why cyber insurance premiums have skyrocketed.\nWhile traditional endpoint security can prevent the majority of cyberattacks, it may not be enough. Malware has become so sophisticated and evasive that traditional signature-based endpoint security cannot keep up. As a result, organizations need to employ a multi-layered endpoint protection strategy to combat the latest threats. By deploying a multi-layered endpoint security strategy, you can stop threats across multiple attack chains.\nAn innovative multistakeholder approach can effectively disrupt the financial capabilities of malicious actors and help reduce the global impact of ransomware attacks. Such an approach would leverage information-sharing and pooled resources to assess the costs and organizational vulnerabilities of ransomware.\nRansomware is a highly disruptive and costly cyberattack that can lead to crippling downtime and substantial productivity losses. The costs of downtime and recovery are enormous – some estimates estimate that they are 50 times higher than the ransom demands. The downtime from a ransomware attack can affect a business for months and may cause it to file for bankruptcy.\nThe recovery process from a ransomware attack is complicated and requires all hands on deck. Depending on the size and scope of the attack, it may involve internal teams, incident response companies, forensic experts, and even the assistance of local and federal law enforcement agencies. Each step in the recovery process carries its own set of costs. Some companies may choose to pay the ransom instead of dealing with the recovery process, avoiding the financial consequences of losing customer data.\nThe impact of ransomware attacks is becoming increasingly widespread and complex. As a result, CISOs are losing confidence in the ability to mitigate ransomware attacks. Moreover, 73 percent of respondents said that failure to mitigate the risk of cyberattacks could expose organizations to fines and legal action. Cybersecurity Ventures estimates that ransomware attacks will cost $265 billion by 2031, which makes it crucial for organizations to prepare for the costs of these attacks.\nRansomware supply chains have become more sophisticated. As a result, cybercriminals are now targeting companies with extensive digital networks. This means that ransomware supply chain attacks will become more prevalent in the future. The SolarWinds ransomware attack, for example, should serve as a warning to all companies with global supply chains. This attack affected 18,000 corporate customers, including Fortune 500 companies and U.S. government agencies.\nIn January 2022, the BlackCat ransomware group infected 233 gas stations in Germany and forced the oil company Shell to reroute supplies. The attack used two vulnerabilities in two different software applications to encrypt data and exfiltrate intellectual property. The resulting disruptions rendered more than half of the organization’s systems inoperable for 48 hours, forcing Shell to hire security experts to restore access to their systems. The attack was so widespread that German intelligence services feared that the attackers had penetrated the networks of gas stations and oil companies to steal information. In Baltimore, the ransomware attack affected the city’s official email servers and rendered critical systems inaccessible.\nMany small and medium-sized companies have limited resources to protect themselves from ransomware. Security is costly and time consuming, so they often put off investing in it in favor of other critical business requirements. Unfortunately, the longer they wait to secure their networks, the more expensive it will be to repair the damage. This negative feedback loop has left many organizations unprepared for attacks and paved the way for predatory ransomware groups.\nThe costs of ransomware attacks can be staggering. In the United States, companies face billions of dollars in ransomware losses every year. As a result, ransomware has become a top concern for businesses, affecting both organizations and consumers. In addition to the financial burden of downtime, ransomware can damage a company’s reputation, and cause customers to lose trust in it.\nThe cost of ransomware attacks is escalating – a single attack can cost up to $265 million. Those figures are staggering, and many companies that have been affected go out of business within a year of being infected. Fortunately, there are ways to mitigate the risk of a ransomware attack.\nThe financial costs of a ransomware attack can be staggering. The Cybersecurity Ventures report predicted that ransomware attacks would cost more than $5 billion in 2017, up from $325 million in 2015. This represents a 15X increase in just two years, with a projected total of $8 billion in 2018 and $11.5 billion in 2019. By 2021, ransomware attacks are expected to cost $20 billion, and every 11 seconds, an average business will be hit by ransomware.\nIn addition to the financial cost, ransomware attacks can also have a huge impact on an organization’s business. For example, an attack on the Colonial Pipeline Company caused a panic buying of fuel on the East Coast because a compromised password had given access to their IT system. The attack resulted in the shutdown of the company’s operational technology networks and IT systems for several days. Fortunately, the company was able to recover a significant portion of the $4.4 million ransom.\nIn addition to monetary costs, the recovery and downtime costs associated with a ransomware attack are also enormous. This is why it is essential to seek outside legal counsel when dealing with ransomware. A seasoned attorney will be able to guide you through the process and minimize your risk.\nTrend Micro has also recently discovered a new vulnerability impacting e-commerce websites. In April 2021, the company found that BIQS software was vulnerable to an XSS vulnerability, which could allow threat actors to inject malicious code on the servers. In August, the company also discovered that Atlassian Confluence servers were vulnerable to a local file inclusion vulnerability, allowing threat actors to insert arbitrary code on its servers.\nDowntime and labor costs\nWhile your frameworks are down, you will experience monetary misfortunes. Most associations require essentially a week and frequently significantly longer to recuperate information. Until it is reestablished, your entire situation is probably going to be disabled. Client information is essential to maintaining a business easily, and without it, you will fight to sell items, administration clients and substantially more. A regular efficiency misfortune can really depend on 20% during free time.\nIn a 2021 ransomware attack, the Kaseya assault, around 1,500 oversaw specialist organization clients were impacted. This shows how store network assaults cause more broad harm than assaults against single people.\nIT groups frequently need to stay at work past 40 hours to reestablish frameworks, and there is normally an overabundance of work all through an association because of an absence of admittance to information. Extra counseling or expert help might be expected to determine information issues.\nThe cost to brand reputation\nA harmed brand notoriety is difficult to fix, and this can have a broad monetary effect. Any bad exposure about an information break can influence the relationship with clients as well as with representatives, financial backers and different partners. Research from the Public Digital protection Coalition shows that around 60% of little to medium organizations leave business in no less than a half year of encountering an information break.\nThere’s a developing pattern for cybercriminals to take steps to uncover delicate information they exfiltrate before encryption. Where the information is strategic, for example, in medical clinics, government or crisis call focuses, this can really hurt.\nIn certain enterprises, clients can guarantee direct pay for an information break. Scripps Wellbeing, retail goliath Target, and gas organization Frontier Pipeline are only a portion of the organizations that have confronted legal claims.\nMost cases are privately addressed any remaining issues as organizations would rather not face extended court fights. Administrative and legitimate fines can be especially high for the spilling of individual wellbeing information, monetary data like charge card subtleties, and actually recognizable data.\nData loss and collateral damage\nYou might lose an information totally due to a ransomware attack. The deficiency of information might address many long stretches of work. Regardless of whether you can reestablish records from reinforcements, there’s an opportunity they were not supported totally or accurately. Today there are ransomware variations that likewise target reinforcement frameworks so you can’t reestablish information.\nYou should figure out how cybercriminals accessed your frameworks. There are numerous ways they can do as such, from conveying phishing messages and setting up counterfeit sites to straightforwardly going after programming weaknesses.\nContaminated machines might need to be totally reformatted, and programming reinstalled. You will likely need added assurance to ensure another information break doesn’t happen.\nIn the ongoing monetary circumstance with expansion and downturn, every one of the costs of a ransomware attack might cause a huge monetary misfortune. In 2020 different reports demonstrated that the normal expense of tidying up after a ransomware attack could depend on $1.85 million. In the event that you don’t tidy up your information and fix any fundamental issues, you could gamble with another assault.\nStep by step instructions to prevent ransomware attacks\n- Having security frameworks set up, representative preparation, and powerful design the executives are a portion of the ways of forestalling ransomware attacks.\n- Keeping awake to date with the most recent working software is vital.\n- Ensure you have total and exceptional reinforcements as they can assist you with recuperating information.\n- Stay up with the latest, and remember to apply security patches.\n- Persistently look at security to ensure you have the right estimates set up.\nIT experts need to adopt a protection strategy as once programmers get inside your association, limiting the damage can be hard. You want to safely safeguard each channel, with email frequently being quite possibly of the most weak one.\nCybercriminals keep on utilizing perpetually complex methods to convey ransomware by means of email. You want to search for cutting edge email security arrangements that utilization quick and successful unique filtering. Arrangements ought to likewise can identify dangers covered somewhere inside happy.\nRansomware can be monetarily harming to organizations in various ways, including pay-off costs, personal time costs, work costs, notoriety harm and legitimate expenses. Associations need to investigate their network protection safeguards. Distinguishing and managing likely dangers and channels, for example, email and cloud coordinated effort apparatuses, can assist with alleviating ransomware attacks.\n- Also Read: Price In India Emo Robot What Is The Cost?\n- Also Read: Kbm 25 Com Know The Latest Authentic Details!\n- Also Read: Is My Derma Dream Legit? Authentic Review!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d9d88e90-90e3-42d8-ae3c-639c566b85de>","<urn:uuid:a4aad484-a3a3-4984-8feb-404e3a6bba70>"],"error":null}
{"question":"Hey stats nerds - what's the relationship between first-pitch strikes and walk rates in baseball, and how does this connect to the energy efficiency debate in crypto mining?","answer":"First-pitch strikes have a strong negative correlation (-0.719) with walks per nine innings (BB/9) in baseball, meaning pitchers who throw more first-pitch strikes tend to issue fewer walks. This demonstrates a clear statistical relationship between getting ahead in the count and control. Similarly, in cryptocurrency mining, there is a clear relationship between operational approach and efficiency - while traditional proof-of-work mining consumes massive amounts of energy (estimated at 128.84 TWh annually for Bitcoin alone), alternative approaches like proof-of-stake don't require solving complex mathematical problems and are therefore much more energy efficient. Both cases show how initial conditions (first pitch location or consensus mechanism choice) significantly impact final outcomes (walk rates or energy consumption).","context":["As many of you now know, last week we unveiled some tremendous new metrics. Available on individual player pages as well as the leaderboards, you now have access to plate discipline metrics for pitchers and pitch type statistics for hitters. The former includes information along the lines of how often a pitcher induced a swing out of the zone, in the zone, as well as his percentage of first-pitch strikes. The latter includes the percentages, and velocities, of pitches seen for hitters, as well as his percentage of first-pitch strikes seen.\nI wrote a bit of an introduction to these new statistics last week, and David has written several glossary-type entries as well. This is the type of information that has piqued my interest for a long, long time, and it now adds another dimension to evaluations. For instance, did you know that Johan Santana posted an O-Swing % (percentage of pitches out of the zone that batters swung at) of 30.1 in 2005 and 2006, which decreased to 28.2% in 2007, and 26.8% this past season?\nUsing the new statistics, I decided to run some correlations to see if certain statistics held strong relationships to each other. First, here are the results for correlations run between the percentage of first-pitch strikes and six prominent evaluative statistics:\nK/9: 0.194 BB/9: -0.719 WHIP: -0.515 BABIP: 0.096 ERA: -0.31 FIP: -0.406\nThe results here are not that shocking, or at least they should not be. Getting ahead of the hitter is generally considered key for the pitcher. Doing so, in theory, should correlate quite strongly to any metric involving walks. As we can see, there is a very strong relationship between the percentage of first-pitch strikes and the walks per nine innings issued by pitchers. The relationship loses a bit of its strength when hits allowed are added to the equation in the form of WHIP, but the -0.719 correlation between F-Strike% and BB/9 is actually the strongest of any that I ran. Here are the results for O-Swing % and the same six evaluative metrics:\nK/9: 0.281 BB/9: -0.493 WHIP: -0.462 BABIP: 0.036 ERA: -0.362 FIP: -0.428\nHere, the results are a bit different. Nothing is incredibly strong or on the same wavelength of strength as the FStrike-BB/9, but we have a few relationships of moderate strength. What exactly is O-Swing? It is the percentage of pitches that a pitcher threw out of the zone, that a hitter swung at. With this in mind, we might initially expect that pitchers with the highest percentages in this area would strike more batters out, walk less, and therefore be very effective in the ERA and FIP department. One thing to keep in mind, though, is the percentage of pitches that these pitchers throw in and out of the zone.\nJake Peavy and Barry Zito, for instance, were amongst the bottom in terms of percentage of pitches thrown in the zone, at around 47%. However, Peavy induced many more swings on these pitches than Zito, which is a big reason for the difference between the two, since their percentages of pitches in and out of the zone were virtually identical. When we have pitchers with different percentages in the mix, as is the case in the correlations using O-Swing, the results should not be as concrete. Overall, the strongest relationship here also involves BB/9, as the idea goes back to the Peavy/Zito example: Peavy gets swings and outs on pitches out of the zone, Zito does not. The higher the percentage is of swings out of the zone, the better the chance is that the BB/9 will be lower.\nLastly, Z-Swing%, which is still a bit curious. For instance, does a pitcher want a higher or lower percentage here? I would venture a guess that a lower percentage would be better, as the pitch is already in the zone and therefore very likely to be called a strike. A hitter failing to swing will take a called strike. It probably is not as important as FStrike or O-Swing, but here are the correlations:\nK/9: -0.067 BB/9: -0.014 WHIP: -0.037 BABIP: -0.150 ERA: -0.027 FIP: 0.087\nWell, I guess it really doesn’t matter for pitchers, as the percentage of swings induced on pitches in the strike zone does not share anything close to a strong relationship with any of the above six metrics. Interestingly enough, the highest correlation for Z-Swing involved BABIP, which was the lowest for F-Strike and O-Swing. The -0.150 isn’t significant by any means, though, so nothing should be taken away by that. At the very least, these results show what we would generally expect: the more first-pitch strikes, the lower the rate of walks or vice versa, and inducing swings out of the zone can result in better rate and run prevention stats.","As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c9087128-5025-4e92-98ad-d2a71681748c>","<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>"],"error":null}
{"question":"Bonjour! Could you tell me about deux artistes who worked with different materials - what mediums did Wayne Thiebaud and Marc Chagall use in their art?","answer":"Wayne Thiebaud worked primarily with thick paint to create his Pop Art pieces, particularly of sweets and desserts, applying paint so thickly it resembled real frosting. In contrast, Marc Chagall worked across multiple artistic media, including watercolor paintings, book illustrations, stained glass, stage sets, ceramics, and tapestries. His most notable works include stained glass windows for cathedrals and the United Nations building, as well as a massive 2,400 square foot ceiling painting for the Paris Opera.","context":["The 5th grade students learned about the life and artwork of Claude Monet through a power point. Students learned that Impressionist painters were more focused on how the light affected their piece of art. Monet would paint the same subject matter over and over but at different times during the day and different times of the year. Students were able to view Monet’s Haystack series and discuss what season or time of day they thought the painting was done. Students were able to create lays of paint using different techniques and Analogous colors to create an Impressionist piece of art like Claude Monet.\nThursday, June 6, 2013\nThe 5th Grade students studied the Pop Art Movement through a PowerPoint. We focused on the artist Wayne Thiebaud and learned about him through a video. Wayne Theibaud was an American Pop Artist who loved to paint sweets and deserts. Students were able to see Thiebaud apply his paint so thick that it even looked like real frosting! Theibaud was turned down by several art galleries before he ended up getting his first art show, which was an instant success. The 5th grade students were able to design their own cake keeping in mind how many layers and flavors they each would like for their cake. Students reviewed VALUE and learned how to create tints and shades using chalk pastels. Students also learned that colors that are next to each other on the color wheel are the best colors to blend together. Students were able to add a variety of details to their cake. Finally students designed a background trying to use colors that were not in their cake to create contrast to make their cake stand out.\nThe 4th Grade students learned about the life and artwork of Georgia O’Keeffe. Georgia O’Keeffe grew up on a farm in Wisconsin and has become one of today’s most famous painters. Her mother thought art was very important, so growing up O’Keeffe had many art lessons and was encouraged to go to Art College. O’Keeffe loved nature, so most of her paintings were of flowers, mountains, seashells and even animal bones. O’Keeffe often simplified her paintings, creating more emphasis on the beauty of nature. Eventually O’Keeffe moved to New York to start showing her artwork. After being in New York for a while, O’Keeffe took a teaching job in Texas and loved the clear skies and the bright sun there. After teaching for a few years Alfred Stieglitz, a famous photographer, offered Georgia to come back to New York and he would pay for her expenses and all she had to do was paint. It was hard for O’Keeffe to leave the beauty of Texas, but she could not pass up the offer and this was the beginning of O’Keeffe’s fame!\nThe 4th Grades students learned about sculpture through a power point. A sculpture is a 3-Dimensional object that can be made of various materials and can be any size. 3-D means you can see ALL three sides of the object; it pops out and is not flat. We looked at the difference between shape and form. Students were able to create an armature (the base for their sculpture) from a balloon and poster board. Students then used ANALOUGOUS colors to paint their fins. Finally they used glitter gloss and sequins to finish their fish sculpture.\nThe 3rd Grade students reviewed the Warm and Cool Colors. Students also reviewed what TEXTURE is and used texture plates to create texture in the background using only the cool colors. Students then learned what the word organic means and used only curvy/organic lines overlapping their texture. To create CONSTRAST students used yellow and orange watercolor to create their background. Next students added a tree and branches and added shadows to make it look more realistic. Finally students were about to draw a bird native to Wisconsin sitting in their tree. Students colored their birds using tints and shades. We talked about which direction the light is coming from and what part of the bird the light will hit first. This would be the tint and the farthest part of the bird from the light would be were the shading would be.\nThe 3rd Grade students learned about Hot Air Balloons through PowerPoint. They learned that hot air balloons fly because hot air weighs less than cool air, so by heating the air inside the balloons allows it to float. Students were able to look at and see various examples of hot air balloons and their bright, bold patterns. We reviewed horizontal, vertical and diagonal lines. Students were able to use different lines and different shapes to create a unique design for their hot air balloon. Students had their photo taken and added themselves into their hot air balloon. Then they used tissue paper and water to create a textured sky background. Finally students had to write at least three sentences starting with, “If I could fly I would . . . .”\nThe 2nd Grade students learned about The Mona Lisa through a short video done by a little girl named Sophia. Students loved this video! http://www.youtube.com/watch?v=Ww2yJ2grPBk&safety_mode=true&persist_safety_mode=1\nStudents brainstormed the sentence, \"If I could take Mona Lisa anywhere I would take her to . . . .\"\nWe wrote various examples on the board and students had to choose a place and create a background. I then used the ipad app. Masterpiece and too the students photo, which puts their face into the Mona Lisa. I printed them off and students added them on top of their background.\nThe 2nd Grade students learned about the life and artwork of Henri Matisse. Students learned that Matisse loved to use his scissors to make various shapes overlapping to create a collage. He rarely threw any scraps away. Students learned the difference between positive and negative space. Students also learned the difference between geometric and organic shapes. The students created a collage with many overlapping geometric and organic shapes using the positive and negative images just like Henri Matisse did!\nROMEARE BEARDEN MUSIC COLLAGE\nThe 2nd grade students learned about the life and artwork of Romeare Bearden through a PowerPoint. They learned that he was very interested in jazz music and this influenced his art. We discussed the many different instruments that there are in a Jazz Band. The students also reviewed what a collage is because Romeare Bearden overlapped fabric, photos and paint to create beautiful collages inspired by music. Students were able to create a music collage using the piano keys pattern for their boarder and designing their own guitar. Then they overlapped musical notes and sheet music. Finally we reviewed the various types of lines and students had to fill the rest of their negative space repeating different colors of lines.","May 4 - 10, 2014: Issue 161\nNOT A CHAGALL!!\nby George Repin\nMarc Chagall, a Russian Jewish artist, was born at Liozna near Vitebsk (in what is today Belarus) on 24 June, 1887 (Julian calendar). Some of his early paintings today sell for around £500,000 while major works bring prices in the millions. He is remembered not only for his distinctive painting style, as an influential modernist painter, but also for outstanding works in other artistic media including book illustrations, stained glass, stage sets, ceramics and tapestries.\nIn 1992 a no-nonsense Yorkshire businessman, Martin Lang, bought a watercolour painting of a reclining nude, believing it to be a Chagall dating from around 1909 to 1910 for £100,000 ($187,000), which he displayed in his home.\nAt the urging of his son he sent the painting to Paris for authentication. Apparently he was not aware of the French law sometimes referred to as “the moral law of the artist”. French law is possibly the world’s strictest when it comes to protecting the works of painters and sculptors. The destruction of counterfeit paintings is not only legal but expected.\nThe painting was sent to the Chagall Committee which is run by the artist’s grandchildren to protect his legacy. They ruled it was a forgery (apparently after scientific analysis indicated that some of the pigments dated from the 1930s) and that it should be destroyed under the French law.\nApparently French committees judging the authenticity of art works generally ask the owners to acknowledge in writing that they understand that their artwork could be destroyed if declared a forgery. There is some doubt whether this was made clear to Mr\nLang. Following the decision of the committee he asked the committee to agree to the painting being marked as a forgery and returned to him. His request was refused and he appealed. At the time of writing the matter remained unresolved.\nIn Text: Photograph of his painting provided to the media by Martin Lang\nChagall worked in Russia (1906-1910), France (1910-1914), Russia and Soviet Belarus (1914-1922), France (1923-1941), USA (1941-1948) and finally in France (1948-1985) where he died but he never forgot and was greatly influenced by the memories of the town where he was born, which in 1887 had a population of 66,000, half of whom were Jewish. At the start of World War II the population of Vitebsk was 240,000 but only 118 survived the German occupation.\nThe art critic Robert Hughes referred to Chagall as the quintessential Jewish artist of the twentieth century.\nOutstanding works of Chagall include stained glass windows in the cathedrals of Reims and Metz, stained glass windows for the United Nations building in New York and the painting of the new ceiling of the Paris Opera (Palais Garnier) in 1963-1964 which took the 77 year old artist a year to complete. The final canvas was 2,400 square feet and required 400 pounds weight of paint. It was in five sections glued to polyester panels which were hoisted up to the 70 foot high ceiling.\nChagall spent his final years and died in Saint-Paul-de-Vence a charming commune of the Alpes-Maritimes department in south-eastern France. It is one of the oldest medieval towns on the French Riviera and is known for modern and contemporary art museums and galleries.\nChagall’s last work was a commission for the Rehabilitation Institute of Chicago involving a maquette painting and a tapestry. The painting had been completed. Yavett\nCauquil Pierce, who was weaving the tapestry under Chagall’s supervision, was at his home discussing and matching the final colours from the painting to the tapestry. She left at 4pm. He died that evening.\nPhotographs of Saint-Paul-de-Vence by George Repin in 2011\nA lane in Saint-Paul-de-Vence.\nThe cemetery at Saint-Paul-de-Vence\nThe grave of March Chagall, his second wife Vava (nee Valentina Brodsky) and her brother, Michel Brodsky.\nAll Photos by George Repin. Copyright George Repin 2014. All Rights Reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:09dc8c49-3d6f-4e99-98b3-039dd35617dc>","<urn:uuid:caaa0f8b-f036-4749-a400-deb2db9cfd27>"],"error":null}
{"question":"What role does light sensitivity play in modern digital camera sensors compared to historical photographic processes?","answer":"Light sensitivity plays a crucial role in both modern digital sensors and historical photographic processes, but in different ways. In modern cameras, CMOS sensors have decreased light sensitivity because part of their sensor is covered with noise-filtering circuitry, resulting in a lower fill factor. CCD sensors, conversely, offer better light sensitivity with their 100% fill factor. Historical processes like daguerreotypes required extremely long exposure times of several minutes, compared to today's typical 1/100 of a second exposures. However, daguerreotypes achieved remarkable detail because their pure surface allowed light exposure to be determined by individual molecules rather than groupings of molecules, which is why they could exceed the detail of even sophisticated modern digital cameras.","context":["Photos. We all have them. With the advent of the digital camera we find ourselves snapping them like there is no tomorrow. Millions of pixels stored on something the size of your thumbnail... prints available in just one hour. It wasn't always this simple.\nThough often considered an art, photography has historically been all chemistry. The first commercialized photographic imaging technique, the daguerreotype, consisted of a copper plate, coated in silver and then exposed to iodine vapor.\nOnce the plate had been exposed to light (and we are talking exposures on the order of minutes - far longer than the 1/100 of a second our cameras do today) the image was rendered with mercury vapor and made permanent with a salt mixture. That's a lot of chemistry right there.\nExposing the polished Silver coating to iodine vapor produces the following reaction:\n2 Ag + I2 --> 2 AgI\nThe layer of AgI is light sensitive. Exposure to light produces the following reaction:\nAgI + h? --> Ag + I\nSo the portions of the plate exposed to light consist of Ag, the ones unexposed consist of AgI.\nExposing the plate at this point to mercury vapor causes an amalgam to form in the portions exposed to light:\nAg + Hg --> Ag(Hg)\nThe last step, the water and salt mixture wash, removes the unexposed AgI:\nAgI + 2S2O32- --> [Ag(S2O32-)2]3- + I-Recently Wired magazine had a great article (\"History Exposed\" Aug 2010) about a series of daguerreotypes taken in 1848 along the Cincinnati waterfront. The daguerreotypes are not only beautiful, but have preserved a moment in history and offer the present a glimpse into what life was like in the city in the middle of the 19th century with amazing detail (far exceeding that of the most sophisticated digital camera on the market today). Some of these images can be viewed here.\nMost modern black and white prints are gelatin silver prints - where silver halide salt crystals are suspended in a thin gelatin layer (both film and paper). These crystals are the reason why some images can appear grainy (larger crystals, higher film ISO, more light sensitivity). Daguerreotypes don't have this issue because the surface of the plate is pure and light exposure is decided by each individual molecule as opposed to a whole grouping of molecules.\nAfter the daguerreotype, countless new methods for capturing and preserving images came to light: salted paper prints, Wet plates, dry plates, cyanotypes, albumen prints and gum bichromate prints just to name a few.\nThe gum bichromate process (also known as the gum dichromate process) is actually pretty simple to perform. It requires a mixture of Potassium Dichromate solution, gum Arabic and pigment to be brushed onto a paper and dried. It is simple to make a contact print from a large format negative: place the negative on top of the light sensitive paper and lay it out in the sun for approx 20 minutes.\nThe sunlight causes the exposed emulsion (the bichromate specifically) to undergo a redox reaction which causes the gum Arabic to cross-link and become insoluble. The unexposed emulsion rinses off with water .\nThough many people have made the switch to digital to preserve their memories of family and friends, some artists and hobbyists still use some of the older techniques, continuing to use them for the beauty and chemistry of it all.\n(daguerreotype photo) http://www.flickr.com/photos/30505400@N00/2386215865\nMerritt, M. Photographic Processes. Feb 2001. http://www.wellesley.edu/Chemistry/Chem&Art/Topics/Photography/photo_processes.pdf\nChemistry of Photography. http://www.cheresources.com/photochem.shtml\nRelated articles by Zemanta\n- Aug. 19, 1839: Photography Goes Open Source (wired.com)\n- CAMERA: A History of Photography from Daguerreotype to Digital (digital-photography-school.com)\n- 1848 Daguerreotypes Bring Middle America's Past to Life (wired.com)","Staff Patrol offers two types of cameras, CMOS and CCD. It is important to understand the difference because cost and qualify are a big consideration in choosing a camera system. Each of our camera systems indicate the type of system used.\nThis type of image sensor is relatively inexpensive and offers a good quality image. The cost of this system is usually less and the overall system can be very economical. While CMOS sensors excel in the capture of outdoor pictures on sunny days, they suffer in low light conditions. Their sensitivity to light is decreased because part of the sensor is covered with circuitry that filters out noise and performs other functions. The amount of space on a sensor devoted to collecting light is called the pixel’s fill factor. The design of CMOS cameras creates a much lower fill factor and therefore they are less sensitive. CMOS cameras do have the advantage when it come to power consumption. Because there are less components in the camera board, it draws less power and will run longer if used on a battery. For general purpose viewing in good light, they are a good choice. If there are budgetary constraints, it’s possible to get more features in the overall system using CMOS cameras.\nCCD cameras have a 100% fill factor and therefore offer a brighter and sharper image. Like most most electronics, this increased quality does add to the manufacturing costs. CCD cameras produce a higher quality picture mainly because the image sensor is used strictly for collecting the image and all of the other necessary processing is handled by other components on the camera board. CCD cameras on the other hand have more components and draw more power. This may be a consideration when using battery power.\nThese images are representative of the differences you can expect between the two styles of cameras. Individual lighting and other conditions will effect the actual image.\nCMOS sensors offer average image quality in good light, have lower power requirements, and are substancially less expensive. CCD sensors offer excellent images in most lighting conditions, have higher power requirements, and are typically more expensive. Evaluate your needs in a surveillance system and we can offer you the best in either category.\nGlossary of Video Terms\nAGC – Automatic Gain Control is an electronic system found in many types of devices. Its purpose is to control the gain of a system in order to maintain adequate performance over a range of input signal levels. Or an electronic circuit that tries to keep the video signal at a constant level 1 (Ivolt peak-peak). Useful on cameras working at low light levels .\nAI – Auto Iris is an electronic circuit that acts as an iris on CCD cameras by electronically shuttering the CCD sensor. Or an automatic method of varying the size of a lens opening in response to changes in scene illumination.\nAWB – Auto White Balance is an electronic process used in video cameras to retain true colors. It is performed electronically on the basis of a white object in the picture.\nCCD – Charged Coupled Device: It is analog technology. The CCD camera has a higher resolution than CMOS. The camera also functions better in low light. A CCD camera drains a little bit more power than the CMOS cameras. It typically uses 12v instead of 9v. CMOS cameras are smaller than a CCD camera and works longer with the 9v battery.\nCMOS – Complementary Metal Oxide Semiconductor: A lower resolution camera compared to a CCD model. The advantage of a CMOS camera is that it uses lower operation current.\nHAD CCD – Hole Accumulation Diode is a type of CCD sensor with a layer designed to accumulate holes (in the electronic sense), thus reducing noise level.\nS to N Ratio – Signal to Noise Ratio is simply the ratio of the signal power and noise power, expressed in decibels (dB). Or a measure of noise on a video signal. It is represented in Decibels as the level of the video signal compared to the level of noise present on that signal. The higher the signal to noise ratio the better.\nBLC – Balance Light Control is a method to compensate for bright spots in a picture. It is also important to consider whether there are bright spots in the picture such as car headlights which can make identification of the vehicle registration or model impossible. This can also be a major problem where it is necessary to identify a person who is moving from bright daylight into artificial light. This could result in the subject becoming an unidentifiable silhouette.\nOSD – On Screen Display is a method of displaying set-up information or instructions on to a display monitor.\nResolution – Resolution measures the cameras ability to reproduce an image. The higher the resolution, the better the picture quality.\nLUX – LUX is the measurement of low light needed for the camera to view and record properly.\nFPS – Frames Per Second is the number of still frames (pictures) that give the illusion of motion, which appear in a single second of time. 3O fps is considered “Real Time”. So for Real Time viewing of your cameras, you need at least 3O fps for each camera.\nIR – Infrared, IR LEDS are used on Day/Night cameras which allow the camera to see in the dark.\nFocal Length – the distance from the surface of a lens and its focal point.\nFrequently Asked Questions\nHow do hidden cameras work? A small board camera is built into an everyday item. The camera can be wired which means it is connected to the DVR or VCR using a cable. The camera can also be wireless, in this case the camera transmits a signal to a receiver that is connected to the DVR or VCR.\nCan I get audio in my hidden camera? – No you cannot. According to United States federal laws, audio should not be used in a surreptitious manner. One example of surreptitious interception is audio in a hidden camera. This includes pinhole board cameras and all covert or hidden cameras; i.e., a clock radio. Audio in a hidden camera or board camera is only available to law enforcement agencies. Title 18, Section 2512.\nWhat is the difference between a wired and a wireless camera? – Wired cameras have a video cable that runs from the camera to your recording or viewing device such as a DVR, VCR or monitor. Wireless cameras have a built-in transmitter that sends the video signal to a receiver. The receiver connects to your recording or viewing device.\nHow far can a wireless hidden camera transmit? – Standard wireless hidden cameras can transmit up to 1000 feet and high-powered wireless hidden cameras can transmit up to 2000 feet.\nHow many wireless cameras can you have in one location? – You can have up to four wireless cameras in one location. You can view all cameras at once using four receivers or you can use one receiver and switch to each camera. You will only be able to view one camera at a time if you only use one receiver. If you want to install multiple wireless cameras in one location it’s best to order them at the same time so that we can put them on different channels.\nWill a cordless phone interfere with wireless cameras? – Cordless phones that operate on the 2.4 GHz frequency will cause interference with 2.4 GHz cameras. Interference should be minimal and usually occurs if the phone is between the camera and receiver.\nWhat is a Quad? – A Quad splits your monitor into 4 sections allowing you to view 4 cameras at once.\nWhat is a 2.4 GHz wireless frequency? – A frequency is used to transmit a signal or data like video. 2.4GHz is the specific wireless frequency that our transmitters use to send video signal to a receiver. All of our wireless cameras operate on the 2.4 GHz frequency. Range varies from 200′ to 700′ depending on environmental conditions. These units are FCC approved.\nAre the transmitters FCC approved? – The transmitters used in all of our wireless systems are FCC and Industry Canada Certified.\nWhat is CCD? – CCD is used in professional cameras because of it’s high resolution quality and it’s ability to record in low-light situations. CCD is basically a small silicon chip that receives light and turns it into voltage variations which makes up an image, it’s usually measured in inches with 1/3″ CCD being the standard. They are higher priced but are great for cameras that may require vision in near darkness.\nWill a Quad allow four cameras to record at the same time? – Yes you can record all four cameras at the same time. You also can record using a switching monitor, which will record the camera that is showing at that moment.\nDo the receivers work through walls up to two feet thick? – Yes as long as there isn’t excessive amount of metal in the wall.\nDo the plug and play connections require extra wire to run the signal back to the VCR or TV? – Yes, you need to buy the length of cable you need we offer various lengths up to 100′.\nWhat is the difference between a DVR and a VCR? – A Digital Video Recorder (DVR) system records high-resolution digital images to a hard disk drive (HDD) and eliminates the requirement of maintaining VHS tapes. Since the video images are stored digitally, the image quality will not degrade overtime, as would a VHS tape when recorded over multiple times. The time-saving search capabilities of a DVR will enable the user to locate the desired video clips via user defined parameters (camera, time, date, etc.) versus the fast forward and rewind functions of a VCR. A DVR can be accessed remotely from anywhere in the world using the Internet.\nHow many hours will a DVR record? – The amount of time a DVR will record for is based on the size of the DVRs hard drive, the number of cameras recording and the number of frames per second ‘ it is recording at. One camera recording in Real Time uses 1 GB per day. So a 16 camera system will use 16 GB in one day.\nDo I need a VCR from you or can I use my own VCR? – You can use your own VCR for recording. However, the VCR will have to be recording all the time. Most VCRs can only record for about 10 hours but we have special VCRs that can record up to 1280 hours.\nWhat is a DVR card? – DVR Cards enable the user to convert their computer into a Digital Video Recorder. The DVR Card(s) is typically installed in an available PCI slot of a computer. DVR cards are bundled with video surveillance software, which allows the user to record and display multiple cameras simultaneously from the camera site or a remote location.\nWhat is a Plug and Play connection camera? – It is an RCA Video plug and a power plug on the camera for easy connection to VCR or TV. This is done by running the RCA (Aux) line into the VIDEO IN of the VCR or TV. The TV or VCR must be set on the correct channel to view the VIDEO IN picture.\nHow many cameras can I hook up to one TV? – You can hook up as many cameras as your TV has inputs. Most TVs have 2 inputs but when using a quad you can hook up four cameras.\nDo you need a VCR to record or will the cameras record? – You must have a VCR or DVR if you want to record.\nBetween what temperatures is it safe for cameras to operate in? – It is safe for B/W cameras to operate between -10F to +122F. It is safe for color cameras to operate between -10F to +104F. It is safe to store cameras between -22F to +158F.\nWhat is the operating voltage of a camera? – Our cameras range from 9 volts (CMOS) to 12volts, and also 24 volt professional models.\nWhat is a varifocal lens? – A varifocal lens is one where the focal length of the lens can be varied. This is a fancy way of saying it is a zoom lens. Most varifocal lenses have, and in almost all circumstances should have, an auto iris feature.\nWhat is a fixed lens? – A fixed focal length lens cannot zoom. The focus is fixed. A fixed focal length lens usually allows more light to pass through the lens at a given focal length than a varifocal, or zoom lens. This can be important in low light situations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d89700c2-a610-4126-8bfa-9e957e9aa4a8>","<urn:uuid:4692ba3c-3e59-4d3a-a353-3dfc96f504cb>"],"error":null}
{"question":"As an experienced hunter looking to optimize gear choices, what are the ideal shotgun specifications for hunting waterfowl, and what environmental impacts does lead ammunition have on these birds?","answer":"The ideal shotgun for waterfowl hunting is a 3-inch, 12 gauge gas gun with an alloy receiver and a 26-inch barrel with choke tubes. For ammunition, hunters should use 1 1/4 ounces of steel 2 or 3 shot at 1450-1500 fps for most waterfowling. Regarding environmental impact, lead ammunition is highly toxic to waterfowl - even ingesting just one or two shot pellets can cause death within weeks. This led to a 1991 nationwide ban on lead shot for waterfowl hunting, which resulted in a 64 percent reduction in lead poisoning cases and saved an estimated 1.4 million birds. The ban was critical since lead is a powerful neurotoxin that damages tissues, organs, and immune and reproductive systems in wildlife.","context":["Photograph by John Hafner\nSo here’s the first question. It’s liable to have a very long answer so you might consider responding to it in installments.\nShotguns are the weapon of choice (often by necessity) for a large variety of winged game. But winged game can mean anything from twenty pound wild turkeys to tiny bobwhite quail. Like the birds, the guns and ammo used to hunt them will usually be quite variable. Leaving brand names aside, can you tell me what type equipment you think best fits each kind of bird? Specifically, I would be interested in your choice of shotgun style (e.g. pump, auto, etc.), gauge, and ammo. And, of course, the reasons why. (Perhaps a spread sheet would be useful?) Here’s the list:\n1. Turkeys (obviously in a class of their own)\nc. Prairie Chickens\nd. Sage Hens\ne. Sharptailed Grouse\nii. Valley Quail\niii. Mearns Quail\niv. Scaled Quail\nv. Mountain Quail\nh. Mountain Grouse\ni. Ruff Grouse\nii. Franklin Grouse\niii. Blue Grouse\na. Freshwater divers\nb. Sea ducks\nd. Puddle ducks\ng. Sandhill Cranes\nh. Miscellaneous (coots, etc.)\n_There! That should keep you busy for a while! _\nAnonymous in Canada\nThank you for the that question. It is way longer than my answer, which is depressingly simple: a 3-inch, 12 gauge gas gun with an alloy receiver and a 26-inch barrel and choke tubes will work for every bird in North America. It will be light enough to carry, hit hard enough for the biggest of birds, possess the ability to be loaded down to 28-gauge levels, and the gas system will reduce recoil.\nThe shorter barrel length (I prefer 28 inches on my semiautos) is a concession to the need for a slightly shorter, lighter gun for upland and turkey hunting in the woods. Throw in a slug barrel and you can shoot deer and pigs with it, too.\nI’ll even name names: Browning Maxus, Winchester SX3, Beretta A400 Action, Remington V3.\nFor those who prefer inertia and the tradeoff is better function in harsh conditions vs more felt recoil, I’ll add the Benelli M2 or Montefeltro, along with the Franchi Affinity and the new Browning A5.\nOn to ammo, and while I can’t recommend one all-purpose load for everything, we’ll get it done with six loads for our one gun.\n1 1/8 ounces of lead 7 1/2s at 1200 fps for almost every upland bird save pheasants and sage grouse, who get 1 ¼ ounces of 5 or 6 shot at 1300.\nOne ounce of steel 6s at 1300 fps if you hunt doves, snipe, rails etc and non-toxic shot is required, plus all teal hunting.\nFor all other non-toxic upland shooting and most waterfowling, 1 ¼ ounces of steel 2 (maybe 3) shot at 1450-1500 fps.\nFor geese, swans, cranes, 1 ¼ ounces of steel BBs at 1450-1500, or HeviMetal BBs.\nFor turkeys, 1 ¾ ounces of 5 shot at 1250 fps or so, and make it Winchester Long Beard.\nThat answer is the truth, but I wouldn’t be much of a gun nut if I let it be the last word. Therefore, the upcoming Installment two will suggest a five-gun battery for North American wingshooting.","Hunters and anglers are switching to nonleaded ammunition and fishing tackle, a move that benefits wildlife, the environment and human health.\nIn a Montana duck blind, Dan “Rooster” Leavens and his dog, Willie, await their quarry. Waterfowl hunters are required to use nonlead shot to prevent poisoning wetlands, wildlife, even retrievers.\nFROM A STAND OF MICHIGAN JACK PINE, a ruffed grouse flushes skyward. Marc Smith, Great Lakes director of conservation partnerships for the National Wildlife Federation, raises his shotgun and brings down the grouse with a load of steel shot. He then retrieves the bird to cook for dinner. “I stopped using lead shot years ago,” says Smith. “Using steel shot ensures that the game I hunt is safe to eat.”\nSmith also wants to ensure that the fields, forests, wetlands and lakes where he hunts and fishes are safe—meaning free of lead ammunition and fishing tackle, which can poison land, water and the wildlife that depend on them.\nA growing number of hunters and anglers likewise are switching to nontoxic hunting ammunition and fishing tackle to help wildlife and protect habitats. In fact, a 2013 study of dove hunters found that more than half would be willing to use nonlead shot if scientific evidence showed it would help wildlife. Today, that evidence is overwhelming.\nAs early as 1894, conservationist George Bird Grinnell raised the alarm about ducks, geese and swans dying from lead poisoning after ingesting lead shot. A powerful neurotoxin, lead damages tissues, organs and immune and reproductive systems, often causing paralysis and death. Waterfowl that ingest as few as one or two shot pellets can die in weeks, and studies document the deaths of raptors and scavengers that eat the carcasses or gut piles of game shot with lead. Worldwide, studies document more than 130 wildlife species that are negatively affected by lead.\nIn North America, pheasant, wild turkey, woodcock, mourning dove, bobwhite and ruffed grouse have all been reported with lead poisoning from ingesting lead shot likely mistaken for seed. The Missouri Department of Conservation estimates that 9 million to 15 million mourning doves die each year of lead poisoning, a toll as great or greater than the annual legal harvest by hunters. Lead can poison raptors such as bald eagles, hawks, owls and endangered California condors if they prey on lead-shot carrion or fish containing lead sinkers or jigs. Last August, a bald eagle was found in Michigan’s Upper Peninsula hanging upside down from a tree, emaciated and dehydrated, with a blood-lead level more than three times that likely to be fatal. And from 1991 to 2018, the University of Minnesota Raptor Center admitted 2,036 eagles; 70 percent of them tested positive for lead and more than 560 had lead toxicity requiring treatment. Many of those did not survive.\nFishing gear, including lead jigs and weights, is a significant source of lead poisoning for loons, which often ingest lead when eating fish that are trailing line and tackle after breaking free from anglers. Such poisoning can have population-level effects. A 2018 New Hampshire study, for example, estimated that lead poisoning over a 24-year period led to a 43 percent reduction in the state’s loon population. “An alarming number of loons die each year after ingesting lead fishing tackle,” says NWF senior wildlife biologist John Kanter. “The growth and recovery of this iconic species may be compromised if anglers don’t switch to nontoxic alternatives.”\nHumans, too, can be exposed to lead by eating venison or game birds that contain lead shot or bullet fragments. A 2009 study in North Dakota tested ground venison from meat-processing plants and found that nearly 6 percent was contaminated with lead. And a study by the Centers for Disease Control and Prevention found elevated blood-lead levels in people who ate wild game. The findings prompted warnings that children and pregnant women should avoid game harvested with lead, which can cause brain damage, developmental defects, neuropathy, attention deficit disorder, even death. The impacts are most pronounced in children, whose brains are still developing—one reason why lead additives in paint and gasoline were banned long ago.\nThe National Wildlife Federation has long been a leader in the effort to reduce lead in the environment. In 1985, it sued the U.S. Fish and Wildlife Service (FWS) to ban lead shot. The suit failed, but it helped prompt FWS in 1986 to begin a phased-in ban on lead shot for waterfowl hunting, which took full effect in 1991. Just nine years later, one study found a 64 percent drop in lead poisoning in waterfowl, which saved an estimated 1.4 million birds.\nSeveral of the Federation’s 52 state and territorial affiliates also have taken steps to reduce lead. The Minnesota Conservation Federation, for example, recently passed a resolution supporting state agencies regulating lead when studies indicate population-level impacts on wildlife, and the New Mexico Wildlife Federation (NMWF) is now using only nonlead ammunition in its shooting-instruction programs. “This is consistent with the leave-no-trace ethic,” says NMWF Executive Director Jesse Deubel. “By choosing lead-free ammunition, our membership continues to be proper stewards of both the land and the wildlife it harbors.”\nMore than 30 states have adopted policies to steer hunters and anglers away from lead. Many require nontoxic shot for species such as rail, snipe and woodcock on certain state and federal lands, including wildlife management areas or national wildlife refuges. In South Dakota, target shooting with lead shot is prohibited in some areas, and Arizona offers incentives for hunters within the range of the California condor to use nonlead ammunition or to remove gut piles of field-dressed game, a voluntary program that has had an 88 percent compliance rate over 10 years.\nCalifornia originally banned lead hunting ammunition within the range of the California condor but then expanded the ban, which became statewide in 2019. And in 2016, New Hampshire implemented a ban on the sale or use of lead sinkers or fishing weights weighing one ounce or less.\nHunters and anglers can choose to use nonlead alternatives no matter where they live as nontoxic alternatives are increasingly available in stores and online. Hunters can choose steel, bismuth and tungsten shot and copper bullets, which stay more intact than lead upon impact, leading to cleaner wild game meat. And anglers can readily find tin and tungsten fishing weights to replace lead sinkers, jig heads, bead heads, wire fly wrap and dumbell eyes.\nNontoxic options are more expensive than lead products, but less so than might be assumed. At one major retailer, steel shot costs about 50 cents more per box for a comparable brand, but many hunters view that marginal cost as well worth the benefits to wildlife and habitats. And there’s mounting evidence that nontoxic options are as effective as lead. In 2015, the Wildlife Society Bulletin published a study showing no statistical difference in effectiveness of steel and lead shot for dove hunters. Wyoming hunter Chris Madsen sees the same with copper. “I’ve hunted elk with copper bullets and they’ve opened perfectly,” he says, “and the cost [of the bullets] is utterly insignificant in the overall cost of a hunt.”\nThrough its Lead-Free Landscapes initiative, the Federation works with hunters and anglers to increase knowledge about how lead can poison wildlife and habitats and discusses nontoxic alternatives for hunting and fishing. The program also collaborates with partners, NWF’s state affiliates, other conservation groups and the outdoor industry to host information sessions and share knowledge through traditional and social media.\nUltimately, the move away from lead is a question of personal responsibility. As noted conservationist Aldo Leopold wrote in A Sand County Almanac: “A peculiar virtue in wildlife ethics is that the hunter ordinarily has no gallery to applaud or disapprove of his conduct. Whatever his acts, they are dictated by his own conscience.”\nLearn more about hunting and fishing with nonlead ammunition and tackle at nwf.org/leadfreelandscapes.\nSince 1975, the National Wildlife Federation’s state and territorial affiliates have adopted several policy resolutions urging a shift away from lead gear, citing lead’s toxic impacts on wildlife and habitats. In 1993, for example, they encouraged anglers to stop using lead sinkers and asked the fishing industry to make nontoxic tackle more available. And in 2007, they urged the U.S. Fish and Wildlife Service to require nontoxic shot for hunting wetland-obligate species such as snipe and rail and to study lead impacts on mourning doves. Visit: affiliates.nwf.org/policy-resolutions\nDrew YoungeDyke wrote about invasive Asian carp in our August–September 2019 issue.\nNWF Committed to Reducing Lead Poisoning in Wildlife »\nNWF Report: Game Changers–Climate Impacts to America's Hunting, Fishing, and Wildlife Heritage »\nFrom Field to Stream to Table »\nFive ways to participate in the 50th anniversary celebration!Read More\nTake the Clean Earth Challenge and help make the planet a happier, healthier place.Learn More\nPromoting more-inclusive outdoor experiences for allRead More\nA groundbreaking bipartisan bill aims to address the looming wildlife crisis before it's too late, while creating sorely needed jobs.Read More\nMore than one-third of U.S. fish and wildlife species are at risk of extinction in the coming decades. We're on the ground in seven regions across the country, collaborating with 52 state and territory affiliates to reverse the crisis and ensure wildlife thrive."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b8fefded-5f2d-4c23-80d5-34d013971d3b>","<urn:uuid:2eb4c379-2fdb-40b6-b0e3-333b9a76c62d>"],"error":null}
{"question":"What are the benefits of new plant breeding regulations for farmers, and what risks do patents pose to breeding access?","answer":"New plant breeding regulations like UPOV91 benefit farmers by providing wider seed selection and boosting global competitiveness - for instance, allowing access to global wheat genetics that weren't previously available in Canada. However, patents on conventional breeding can restrict access to biological diversity, which is crucial for breeding. Patent holders can prevent the use of patented plants and animals for further breeding without their permission, potentially limiting the freedom to operate for breeders, gardeners, and farmers involved in conventional breeding and food production.","context":["CANADA SIGNS ON TO UPOV’91\nIMPORTANT AMENDMENTS TO Canada’s Plant Breeders’ Rights (PBR) Act have been passed into law. These changes bring Canada in line with other countries who have signed on to the 1991 Act of the Union for the Protection of New Plant Varieties (UPOV91) and unlock doors for the development of new and improved seed varieties within Canada and internationally.\nFarmers will benefit from a wider seed selection, which means a boost in global competitiveness, says Crosby Devitt, vice-president of strategic development for Grain Farmers of Ontario. “For example, only four per cent of global wheat is presently grown in Canada, so there are a lot of genetics that could work or be tested here.”\nFor many years before the amendments were passed, companies already located in Canada were reluctant to invest in breeding new varieties of many crops such as cereals because of intellectual property concerns, explains Patty Townsend, CEO of the Canadian Seed Trade Association (CTSA).\nIn addition, international breeders were unwilling to send their superior varieties to Canada, even though they saw market potential. Now, all that has changed — but how soon can farmers realistically expect to see access to more varieties with a wider range of characteristics?\n“We are looking to the Plant Breeders’ Rights Journal (PRBJ) that came out at the end of April for the first varieties protected under the new legislation,” Townsend says. The selections in the PBRJ include various cereal, oilseed, and potato varieties developed in Canada and other countries.\n“We are hopeful that there will be some important new varieties available at that point for Canadian farmers, most likely internationally- developed varieties. It will take a little longer for us to see the impact of the changes on varieties developed in Canada. We are all pretty familiar with the length of time it takes to bring a new variety to market,” Townsend adds.\nDELAYS AND CHALLENGES\nWhy did it take until 2015 for Canada to ratify UPOV91? The Government of Canada did introduce Bill C-80 in 1999, which contained appropriate amendments to the Plant Breeders’ Rights Act, but parliament was prorogued before the bill was passed.\n“It seemed that there wasn’t the political will to try again until a number of things happened,” says Townsend.\nThese changes included the restructuring of government plant breeding and research objectives, with a greater emphasis on private sector breeding and variety development, particularly in cereals. Large breeding companies made it clear that they had no plans to invest in cereal breeding in Canada as it would not likely generate a return on investment.\nCanada’s outdated plant variety protection laws became the subject of discussion during some bilateral and regional trade negotiations. The European Seed Association made it clear that European breeders would not send varieties to Canada because its laws were outdated. Some breeding companies made it known that they were unable to get access to improved varieties for Canada, and the specific reason was outdated plant variety protection.\nOUTREACH AND EDUCATION\nCSTA is undertaking an outreach and education campaign to make sure everyone understands the new landscape. “All varieties granted PBR protection after February 27, 2015, are protected under the new legislation, called PBR 91,” Townsend explains. “All varieties protected before February 27 will continue to be protected under the old legislation (PBR 78). So, for some time, farmers are going to be able to choose varieties that carry different protection. Since there are some substantial differences between PBR 78 and PBR 91, there is potential for confusion.”\nShe says, for example, that while the amendments entrench farmers’ privileges to clean, store, and use grain of PBR-protected varieties grown on their own farms for seed, they do not supersede any other intellectual property tools like patents and contracts.\n“If farmers signed contracts or other agreements that included an agreement to not save seed, then they still cannot save seed, even if the variety is also protected by PBR,” Townsend says.\nIn addition, the amendments mean that additional stakeholders along the value chain now have obligations. “We want to make sure that farmers understand the changes, and that others like seed conditioners and grain buyers also understand their obligations and can prepare,” Townsend explains.\nThe CSTA has developed a website www.PBRfacts.ca as well as fact sheets with side-by-side comparisons of the two different types of protection. In addition, presentations are being created that can be used by CSTA value chain partners.\nTownsend adds, “Canadian farmers have demonstrated time and time again that given the right tools they can be world leaders. Canada is the world’s largest producer and exporter of flax seed, canola, pulses, and durum wheat. The updated PBR legislation gives farmers access to a very important tool: new, more productive varieties to increase competitiveness. It’s an exciting time to be in Canadian agriculture.” •","23 April 2020 / No Patents on Seeds! published a new report today on patent applications for conventional plant and animal breeding which, according to European patent law, are excluded from patentability. Nevertheless, from beginning of 2018 until the end of 2019, more than 100 patent applications were filed on conventional breeding. In the report we present eleven examples in more detail, highlighting cases of biopiracy, claims on vegetables, beer and barley and farm animals. If conventionally bred plants and animals are patented as ‘inventions’, they cannot be used for further breeding without the permission of the patent holder. However, access to biological diversity is crucial for breeding.\n“We found examples such as specific pepper plants, originally collected in Mexico, whose use in breeding could now be covered by monopolistic patent claims. Other examples concern natural resistance to plant diseases derived from wild populations of basil, musk melons with intensely red coloured flesh, and endive plants with reduced browning after harvesting,” says Katherine Dolan from ARCHE NOAH. “Such methods of plant breeding are in no way technical inventions.”\nThere are further patent applications claiming spinach, maize, tomatoes, alliums, artichokes, eggplants, beet, broccoli, cassava, cauliflower, celery, cotton, potatoes and rice. Patent applications were also filed for animals used in food production covering cattle, pigs, sheep, horses, goats, rabbits and poultry.\nAll the examples are derived from conventional methods of breeding, based on random processes and crossing and selection, but not on genetic engineering. While at the moment, these are just pending applications, many will be granted if the legal chaos at the European Patent Office (EPO) is not resolved.\nThe wording of European patent law prohibits the patenting of non-technical breeding. However, a decision taken in 2017 by the Administrative Council at the EPO, created a great deal of legal uncertainty, which prohibited some, but not all, patents on plants and animals derived from conventional breeding. In addition, in 2018, as a result of a patent on pepper, the EPO ruled that in general patents could be granted on all plants and animals derived from conventional breeding. Faced by its own legal contradictions, the EPO in 2019, ceased all further patenting in the field of conventional breeding. Now, in 2020, it is expected that further decisions will be taken.\n“The No Patents on Seeds! report shows how companies will try to expand their control of food production in Europe if the current problems are not solved, and no clear distinction is made between technical inventions or random processes,” Christoph Then says for No Patents on Seeds!\nNo Patents on Seeds! demands that the ‘freedom to operate’ is safeguarded for all European breeders, gardeners and farmers involved in conventional breeding, growing and conservation of food plants and farm animals. Access to biological diversity needed for further breeding must not be controlled, hampered or blocked by patents.\n- Christoph Then, Spokesperson No Patents on Seeds!, Tel +49 (0) 151 54638040, email@example.com\n- Johanna Eckhardt, Project coordination No Patents on Seeds!, Tel + 43 (0) 680 2126343, firstname.lastname@example.org\n- Katherine Dolan, Head of Policy, ARCHE NOAH, Tel +43 (0) 676 557 4408, email@example.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:12098797-148c-47d7-8754-51a73ca6c690>","<urn:uuid:f971567b-9040-4553-b6ae-3207538c3e85>"],"error":null}
{"question":"What caused ancient cats to become domesticated, and what factors are leading to genetic erosion in modern livestock breeds? ¿Cuáles son las consecuencias de la pérdida de diversidad genética en el ganado?","answer":"Cats became domesticated due to their role in controlling pests around early human settlements, particularly mice and rats that infested grain stores of the first farmers. This was different from other animals that were domesticated specifically for agriculture or labor. As for modern livestock breeds, genetic erosion is caused by multiple factors including inappropriate aid programs, product-focused selection, changes in land use, technological changes, and economic shifts. The consequences of this genetic erosion include reduced capacity for sustainable livestock production in extreme environments, decreased ability to meet unforeseen challenges, and loss of important traits like disease resistance and environmental adaptation.","context":["Ian Sample, science correspondent guardian.co.uk,\nThe ancestry of the world’s household cats can be traced to an ancient region\nThe ancestry of the world’s household cats can be traced to an ancient region of the near east, suggesting an unusually exotic origin for one of the most aloof animals ever to be domesticated by humans.\nA major genetic survey of nearly 1,000 feral and domestic cats has revealed that every breed of household cat alive today originates from just five lineages which lived alongside ancient settlers in the Fertile Crescent, an area stretching from the eastern Mediterranean to the Persian Gulf.\nThe earliest archaeological evidence for cat domestication dates to 9,500 years ago, when cats were thought to have been kept as pets in parts of Cyprus. But the researchers believe domestication started 3,000 years earlier, with the family feline having broken ranks with its wild relatives as long as 130,000 years ago.\nUnlike pigs, cows and sheep, which were domesticated for agriculture, and horses and donkeys, which were exploited to pull farming equipment, cats began co-existing beside humans by feeding on mice, rats and other pests that infested the grain stores of the first farmers.\nA team of scientists led by David Macdonald at Oxford University’s wildlife conservation research unit analysed genetic samples from 979 cats from Europe, Asia, the Middle East, Africa and China.\nThe researchers focused on DNA in the cats’ mitochondria, the tiny power-generating structures found in cells that contain their own genetic material and are inherited only down the maternal line.\nSearching through the genetic code sequences, the scientists looked for variations at a number of defined “marker” spots. This enabled them to determine which wild and domestic cat lineages were most closely related.\nThe cats fell into distinct groups, one of which included all domestic cats and the near eastern wildcat, suggesting the two were linked. The other cats fell into four groups including the European wildcat, the central Asian wildcat, the sub-Saharan African wildcat and the Chinese desert cat.\nCarlos Driscoll, a scientist on the study, which was published in Science yesterday, said: “What our work shows is that cats were not domesticated anywhere else in the world, but that they became pets for people living in the Fertile Crescent before being carried to other parts of the world by humans.” The Fertile Crescent gains its name from land irrigated by the waters of the Nile, Jordan, Tigris and Euphrates, where hunter-gatherers first began to settle. Different civilisations occupied the region, including the Sumerians, Assyrians and Babylonians.\nThe project, which has taken more than six years, began as an attempt to identify genetic differences between the Scottish wildcat and other native species, but expanded to encompass all species of cat around the globe. The information will help conservationists develop more effective strategies to protect rare species, including the Scottish wildcat.\n“These genetic insights offer hope for the wildcat’s future,” said Professor Macdonald. “In terms of practical conservation our next move is to use this marker to find out how many wildcats are left in Scotland.”\nThere is no way the 130,000 years figure is right. 13,000 maybe. If the 130,000 year figure was right, they’d have been snuggling down around the hearth fires in the Levant with the first modern human population out of Africa (Skuhl) that’s generally believed to have failed.\nMaybe they were.\nThe Near Eastern Origin of Cat Domestication\nCarlos A. Driscoll,1,2* Marilyn Menotti-Raymond,1 Alfred L. Roca,3 Karsten Hupe,4 Warren E. Johnson,1 Eli Geffen,5 Eric H. Harley,6 Miguel Delibes,7 Dominique Pontier,8 Andrew C. Kitchener,9,10 Nobuyuki Yamaguchi,2 Stephen J. O’Brien,1* David W. Macdonald2*\nThe world’s domestic cats carry patterns of sequence variation in their genome that reflect a history of domestication and breed development. A genetic assessment of 979 domestic cats and their wild progenitors—Felis silvestris silvestris (European wildcat), F. s. lybica (Near Eastern wildcat), F. s. ornata (central Asian wildcat), F. s. cafra (southern African wildcat), and F. s. bieti (Chinese desert cat)—indicated that each wild group represents a distinctive subspecies of Felis silvestris. Further analysis revealed that cats were domesticated in the Near East, probably coincident with agricultural village development in the Fertile Crescent. Domestic cats derive from at least five founders from across this region, whose descendants were transported across the world by human assistance.","Genetic diversity of livestock is being lost. The number of breeds has markedly declined over the past half century. Up to 30% of global mammalian and avian livestock breeds (i.e., 1,200 to 1,500 breeds) are currently at risk of being lost and cannot be replaced.\nBreeds become rare, either because their characteristics do not suit contemporary demand or because their qualities have not been recognised. When a breed population falls to about 1,000 animals, it is considered rare and endangered.\nExamples given in Thrupp, L.A. (1998) serve to emphasise the extent of the problem.\nLocal breeds are better adapted to their environment\nCauses of genetic erosion in domestic animals\n|Inappropriate Aid||Lack of appreciation of the value\nof indigenous breeds and their importance in niche adaptation.\nIncentives to introduce exotic and more uniform breeds from industrialised countries\n|Product-focused selection||Undue emphasis placed on a specific product or trait, leading to the rapid dissemination of one breed of animal at the expense of others|\n|Changes in land use||Conversion of rangelands and mixed farming systems for agriculture, game parks, and industrial use|\n|Changes in knowledge||The idea that \"modern/imported is best\" has led to the loss of knowledge about traditional livestock husbandry practices and to the erosion of domestic animal diversity|\n|Change in Technology||Replacement of animal draught and transport by machinery, leading to permanent change of farming system, artificial insemination and embryo transfer leading to rapid replacement of indigenous breeds|\n|Change in Economy||Decline in economic viability of traditional livestock production systems|\n|Intensification||Livestock populations that rely\non veterinary services and on improved feeding conditions. Heavy investment\nin preventative and curative veterinary measures, and in feeding, housing\nMultipurpose local species and breeds replaced by those with higher milk, meat, egg production (including cross-breeds and pure-bred exotics)\n|Cross-breeding||Predominance of sires from a few selected breeds in widespread cross-breeding programmes can lead to loss of features expressed by specialised breeds|\n|Storage||Failure of cryopreservation equipment (used to freeze semen, ova and embryos) or lack of refrigerant, inadequate maintenance of frozen semen from breeds that are not in demand|\n|Conflict||Wars and other forms of socio-political instability can lead to livestock owners moving their stock out of their usual area, thus increasing the possibility of mixing with other breeds thereby potentially losing a location-specific breed|\n|Disaster||Natural disasters such as floods, drought or famine can result in whole breeds dying out|\nThese trends are supported by:\nIronically, the loss of indigenous breeds that are able to exploit vegetation in the more extreme environments also seriously affects the capacity of human society to live in large areas of the world in a sustainable manner.\nDeclining livestock diversity has serious consequences for current livestock production and future capacity to meet unforeseen challenges and opportunities. Livestock diversity is being lost partly because of commercial production.\nFor instance, commercial production of egg chickens, meat chickens, and turkeys is dominated by fewer than 10 multinational breeding companies. Breed-level diversity within egg and meat-producing types is low because common breed origins and intense selection for similar production goals have promoted genetic uniformity. Similarly, China possesses at least 50, and perhaps over 100, unique pig breeds, but many of these are becoming endangered as they are replaced with western breeds.\nExamples given in Thrupp (1998) also serve to emphasise the current status of livestock diversity. \"FAO estimates that somewhere in the world at least one breed of traditional livestock dies out every week. Many traditional breeds have disappeared as farmers focus on new breeds of cattle, pigs, sheep, and chickens. Of the 3,831 breeds of cattle, water buffalo, goats, pigs, sheep, horses, and donkeys believed to have existed in this century, 16 percent have become extinct, and a further 15 percent are rare. Some 474 of extant livestock breeds can be regarded as rare. A further 617 have become extinct since 1892. Over 80 breeds of cattle are found in Africa, and some are being replaced by exotic breeds.\"\nTraditional pastoralists have often tended to foster biodiversity, in both plants and animals. Many pastoral societies have developed elaborate systems that result in the preservation of genetic resources. Pastoralists have deliberately developed livestock to meet different needs and conditions. For example, a least 12 breeds of camel are known from southern Sudan alone. However, wealthier sectors of society are now accumulating large livestock holdings through purchase of animals from different areas and tribal groups - with the resulting cross-breeding making camels of one generic type.\nIt is clear that livestock breeds are not biological taxa but rather represent the outcome of social processes. They are therefore unlikely to survive outside the social contexts and production systems that formed them. However, these losses weaken the potential of breeding programs that could improve hardiness of livestock.\nCommercial breeds of livestock possess greater genetic variability than most crop varieties do. This diversity allows intensification of selection within breeds to be a fruitful approach for improving livestock productivity. However, if continued emphasis on breed replacement and increasing selection intensity (e.g. for greater productivity) take place at the expense of maintenance of genetic diversity, including the advantages of disease resistance and environmental adaptation, there may be significant long-term costs. As an example, Holstein cattle have become the pre-eminent dairy breed world-wide and have enjoyed sustained improvements in milk production potential, but only at the cost of declining genetic diversity within the breed.\nDespite significant advances in the preservation of genetic diversity of crop varieties, for example through ex-situ preservation of germplasm and seed banks, little attention has been paid to conserving the genetic diversity of livestock species. The current dependence on in situ conservation by hobbyists is inadequate. Moreover, this form of breed preservation is currently largely limited to Europe and America. The significant livestock diversity in Africa, Asia and South America is largely unprotected.\nState is therefore characterised by:\nIt is important that the genetic diversity of rare and endangered livestock breeds and their wild relatives and ancestral lines be preserved as insurance for future needs.\nFormal government-sponsored international programs for in-situ and ex-situ preservation of livestock genetic diversity need to be established. In addition, the native habitats of the wild relatives of livestock species must be preserved.\nInvestments in preserving this natural capital could yield net payoffs in both agricultural productivity and profitability. Such investments should be considered in any economic cost-benefit analyses of alternative production regimes.\nA move towards sustainable agriculture requires changes in production methods, concepts, and policies, as well as the participation of local people. Scientific advancements in genetics and \"improved\" varieties can have important roles. However, these need to be reoriented towards conserving and using diversity in farming systems - rather than replacing diversity with uniformity. The following principles are important:\nReferences and Further Reading:\nCattle Diversity Database. http://www.ri.bbsrc.ac.uk/cdiv_www/accessdb.htm\nCollins, W.W. & Qualset, C.O. (1999). Biodiversity in agroecosystems. CRC Press. (CRC Press LLC, 2000 Corporate Blvd., N.W. Boca Raton, Florida 33431, USA).\nConvention on Biological Diversity. Click to view document\nDepartment of Animal Science - Oklahoma State University. Breeds of Livestock. http://www.ansi.okstate.edu/breeds/\nIntermediate Technology (1996). Dynamic Diversity: Livestock keepers safeguarding domestic animal diversity through their animal husbandry. Intermediate Technology Development Group, Myson house, Railway Terrace, Rugby, CV21 3HT, UK. (Email: email@example.com)\nMason, I.L. 1996. A World Dictionary of Livestock Breeds, Types and Varieties. Fourth Edition. C.A.B International. 273 pp.\nMcNeely, J.A., Gadgil, M., Leveque, C., Padoch, C. & Redford, K. (1995). Human Influences on Biodiversity. Pp 711-821 in. Heywood, V.H. & Watson, R.T. Global Biodiversity Assessment. UNEP. Cambridge University Press. ISBN 0-521-56481-6.\nRare Breeds Canada. c/o Trent University Environmental & Resource Studies Program Peterborough, ON, Canada, K9J 7B8. Email: RAREBREEDSCANADA@TRENTU.CA. Web Site: http://www.trentu.ca/rarebreedscanada/\nRome Declaration on World Food Security and World Food Summit Plan of Action. World Food Summit, Rome 13-17 November 1996.\nThe Analysis of Genetic Diversity in Cattle to Preserve Future Breeding Options. http://www.ri.bbsrc.ac.uk/cdiv_www/homepage.htm\nThrupp, L.A. (1998). Linking Biodiversity and Agriculture: Challenges\nand Opportunities for Sustainable Food Security. World Resources Institute.\nFAO Global Strategy for the Management of Farm Animal Genetic Resources\nFAO. The FAO Global Strategy for the Management of Farm Animal Genetic Resources. Domestic Animal Database. http://dad.fao.org(opens in a new browser window) and also the following related documents.\n|Brochure: The Global Strategy for the Management of Farm Animal Genetic Resources.|\n|Some detail on the Global Strategy for the Management of Farm Animal Genetic Resources|\n|Primary Guidelines for Development of National Farm Animal Genetic Resources Management Plans|\n|Secondary Guidelines for Development of National Farm Animal Genetic Resources Management Plans. Animal Recording for Medium Input Production Environment|\n|Secondary Guidelines for Development of National Farm Animal Genetic Resources Management Plans. Executive Brief|\n|Secondary Guidelines for Development of National Farm Animal Genetic Resources Management Plans. Management of Small Populations at Risk|\n|World Watch List for Domestic Animal Diversity, 2nd Edition|\n[Livestock & Environment Toolbox Home]"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:87716267-a179-477e-9e0b-9957fd0dbab7>","<urn:uuid:2fc65303-0c4b-4597-9bf1-9699207de399>"],"error":null}
{"question":"How did historians document Rosa Parks' arrest versus Martin Luther King's early life?","answer":"Rosa Parks' arrest was documented through primary sources including police reports, arrest records, booking photos, and court documents from the Browder v. Gayle case, which are preserved by the National Archives. These records show she was charged with 'refusing to obey orders of bus driver' and was booked, fingerprinted, and briefly incarcerated. In contrast, Martin Luther King Jr.'s early life was documented through biographical records showing he was born on January 15, 1929, in Atlanta, Georgia to a Baptist minister father and grew up to become a Baptist pastor himself at Ebenezer Baptist Church.","context":["Presentation on theme: \"The Historian’s Toolbox\"— Presentation transcript:\n1 The Historian’s Toolbox Discovery, Analysis, Interpretation, Communication\n2 What Do Historians Do?Obviously historiography [writing history] cannot be a science. It can only be an industry, an art, and a philosophy – an industry by ferreting out the facts, an art by establishing a meaningful order in the chaos of materials, a philosophy by seeking perspective and enlightenment.\" - Will and Ariel Durant, The Lessons of History (1968)What Historians DoHow They Do ItDiscoverLocate primary sources and ferret out the facts.AnalyzeExamine primary sources and establish a meaningful order in the chaos of materials.InterpretExplain the meaning of primary sources and seek perspective and enlightenment.CommunicateShare insights with others.\n3 The Building Blocks of History : Primary Sources Primary sources are actual records that have survived from the past. They are pieces of information created from direct experience that help us to understand history: letters, diaries, public documents, photographs, remnants of clothing, furniture, tools, coins, and other artifacts.Primary sources are created by people who witnessed or participated in an event and recorded it in some way.This photo was taken about 100 years ago at the turn of the century. It shows Laura May Wilson and her bike.Note: Any item created in the past which provides information about the period is also considered a primary source (e.g., a newspaper advertisement from the 1940s, a political cartoon from the 1920s,or a recipe from the 1800s.)\n4 Using Primary SourcesThe photograph on the left shows Laura May Wilson on her wedding day. Through using documents such as a Certificate of Marriage (below the picture), we can learn more about this event. For example, she was married on March 14, 1917, in Coon Rapids, Carroll County, Iowa. From this document, we also know that her two sisters Hazel and Rhoda witnessed the marriage.\n7 Locating Primary Sources Looking for primary documents is like a treasure hunt. Historians often have to go to many places to collect materials including libraries, museums, government agencies, and historical societies. They even may create their own documents by interviewing relevant people. (Audio and video tapes are primary sources too.)Resources at the Library of Congress, Washington, D.C.1933 Chicago World's Fair View Book, Boston Museum of Natural HistoryAbove (right): 16th Amendment to the U.S. Constitution: Federal Income Tax (1913), National ArchivesGroup Listening to V-E Day Radio Commentary, State Historical Society of Wisconsin, MadisonSlaves who fled their masters, 1862, Library of Congress\n8 Broadening the SearchToday, many historians use digital reproductions of original materials. A digital reproduction is an electronic version of an artifact such as a diary, letter, newspaper clipping, object, or original photograph. Digital reproduction allows the original to be stored, protected, and preserved, while making the resource widely available for study.Photographer Les Goodey creating digital reproductions, The Taylor-Schechter Genizah Research Unit, Cambridge University LibraryDocuments from the Genizah Collection help to shed light on the medieval world. Its 140,000 manuscript fragments are mainly in Hebrew and Arabic.\n9 More Examples of Digital Reproductions The article on the right is a digital reproduction of a newspaper article. The article notes that Mrs. Laura Wilson Anderson had her poems published in The Poetic Voice of America. The original article was scanned.The pictures show the \"Always Ready Class\" at the Star Methodist Church where Clara May Wilson taught. A number of scans were completed. First, the photo is displayed in a black photo album. The back of the photo was also scanned. The close-ups are of Clara and Glenn Bolger, Mrs. Anderson’s niece and nephew.\n10 The Limitations of Digital Reproductions Reading a scanned copy of the marriage certificate yields similar information to the original. But it doesn't allow us to see the reverse side of the sheet unless that side is scanned too. So the exploration may be incomplete when examining digital reproductions. Some historians also miss the smell and touch of an original item.\n11 Some Rules for Transcribing TranscriptionsMany historical primary resources are transcribed into a digital form to make them easier to access and search. This is a diary entry made by Eileen Kinnick on January 1, 1936, when she was 17 years old. A scanned digital reproduction of the diary page is at the top. The transcription is below it.Wednesday, January 1 Up to Edna's all day. Gertrude's, Lillian's and Lucille's and we were there. At nite read book and listened to Gracie Allen. \"The Music goes Round & Round.\"Some Rules for TranscribingMake no attempt to correct spelling or other \"mistakes.\"Use capital letters where the writer used capital letters.Make educated guesses when unsure of a word. However, use brackets [ ] where wording is questionable. If you're unable to decipher the words, indicate [illegible].Match the punctuation used by the author.You may or may not choose to maintain the formatting of the document such as line breaks.\n12 Errors in Transcription Examine this example from Ruth West's 1920 diary and see if you can identify issues or concerns with transcription. Errors in transcriptions are common.\n14 Examining Primary Sources Historians go to primary sources in search of evidence to answer questions about what happened in the past and why. When working with primary sources, answering a series of basic questions can help us judge their quality and draw more accurate conclusions.The Document Analysis Worksheets on the following pages were developed by the National Archives for educators and young researchers to assist in the evaluation of primary sources of various types.Questions for Analyzing Primary Sources Recommended by the Library of CongressWho created the source and why? Was it created through a spur-of-the-moment act, a routine transaction, or a thoughtful, deliberate process?Did the recorder have firsthand knowledge of the event? Or, did the recorder report what others saw and heard?Was the recorder a neutral party, or did the creator have opinions or interests that might have influenced what was recorded?Did the recorder produce the source for personal use, for one or more individuals, or for a large audience?Was the source meant to be public or private?Did the recorder wish to inform or persuade others? (Check the words in the source. The words may tell you whether the recorder was trying to be objective or persuasive.) Did the recorder have reasons to be honest or dishonest?Was the information recorded during the event, immediately after the event, or after some lapse of time? How large a lapse of time?\n19 Interpreting Primary Sources Interpretation is the process of explaining primary sources by revealing their context, meaning, and significance. Let’s look at an example involving Civil Rights activist Rosa Parks.\n20 The Arrest Records of Rosa Parks The documents shown here relating to Mrs. Parks’ arrest are copies that were submitted as evidence in the Browder v. Gayle case. They are preserved by the National Archives and Records Administration-Southeast Region in East Point, Georgia, in Record Group 21, Records District Courts of the United States, U.S. District Court for Middle District of Alabama, Northern (Montgomery) Division. Civil Case 1147, Browder, et al v. Gayle, et al.This booking photo, taken at the time of Mrs. Parks' arrest, was discovered in July 2004 by a deputy cleaning out a Montgomery County Sheriff's Department storage room.\n26 Telling the Story Behind the Primary Sources Authors Stacey Bredhoff, Wynell Schamel, and Lee Ann Potter studied Rosa Park’s arrest records and combined their new knowledge with what they already knew about the Civil Rights movement and published this article: \"The Arrest Records of Rosa Parks.\" Social Education 63, 4 (May/June 1999):\n27 Rosa Park’s Arrest Records From: \"The Arrest Records of Rosa Parks.\" Social Education 63, 4 (May/June 1999)On December 1, 1955, during a typical evening rush hour in Montgomery, Alabama, a 42-year-old woman took a seat on the bus on her way home from the Montgomery Fair department store where she worked as a seamstress. Before she reached her destination, she quietly set off a social revolution when the bus driver instructed her to move back, and she refused. Rosa Parks, an African American, was arrested that day for violating a city law requiring racial segregation of public buses.Note: In this section, highlighted passages indicate interpretive statements.\n28 In police custody, Mrs. Parks was booked, fingerprinted, and briefly incarcerated. The police report shows that she was charged with \"refusing to obey orders of bus driver.\" For openly challenging the racial laws of her city, she remained at great physical risk while held by the police, and her family was terrified for her. When she called home, she spoke to her mother, whose first question was \"Did they beat you?\"\n29 Mrs. Parks was not the first person to be prosecuted for violating the segregation laws on the city buses in Montgomery. She was, however, a woman of unchallenged character who was held in high esteem by all those who knew her. At the time of her arrest, Mrs. Parks was active in the local National Association for the Advancement of Colored People (NAACP), serving as secretary to E.D. Nixon, president of the Montgomery chapter. Her arrest became a rallying point around which the African American community organized a bus boycott in protest of the discrimination they had endured for years. Martin Luther King, Jr., the 26-year-old minister of the Dexter Avenue Baptist Church, emerged as a leader during the well-coordinated, peaceful boycott that lasted 381 days and captured the world’s attention. It was during the boycott that Reverend Martin Luther King, Jr., first achieved national fame as the public became acquainted with his powerful oratory.\n30 After Mrs. Parks was convicted under city law, her lawyer filed a notice of appeal. While her appeal was tied up in the state court of appeals, a panel of three judges in the U.S. District Court for the region ruled in another case that racial segregation of public buses was unconstitutional. That case, called Browder v. Gayle, was decided on June 4, The ruling was made by a three-judge panel that included Frank M. Johnson, Jr., and upheld by the United States Supreme court on November 13, 1956.JudgmentAfter trial on the merits and careful consideration of the evidence therein adduced and after oral arguments and submission of briefs by all parties, the Court, being fully advised in the promises, found in an opinion handed down on June 5, 1956, that the enforced segregation of Negro and white passengers on motor buses operating in the City of Montgomery as required by Section 301 (31a, 31b and 31c) of Title 48, Code of Alabama, 1940, as amended, and Sections 10 and 11 of Chapter 6 of the Code of the City of Montgomery, 1952, violates the Constitution and laws of the United States.\n31 For a quiet act of defiance that resonated throughout the world, Rosa Parks is known and revered as the \"Mother of the Civil Rights Movement.\"February 4, October 24, 2005February 4, October 24, 2005\n33 A Lesson from HistoryEven ordinary citizens can serve as agents of constructive change. Conventional wisdom says that if you want to play a significant role in history, you have to do something big. But it's small acts of leadership – refusing to move to the back of the bus, circulating a petition, organizing a strike – that eventually move mountains. Small acts of leadership, not big heroic acts, performed by like-minded people ultimately add up. Small acts of leadership slowly and effectively bring about constructive change – ESM\n34 Bibliography\"Analysis of Primary Sources.\" The Historian's Sources. The Library of Congress. 14 Nov <http://lcweb2.loc.gov/ammem/ndlpedu/lessons/psources/analyze.html>.District Court of The United States for the Middle District of Alabama-Northern Division. \"Browder v. Galye.\" National Park Service. 22 Dec National Historic Site, National Park Service, U.S. Department of the Interior. 15 Nov <http://www.nps.gov/malu/documents/browder_v_gayle.htm>.Education Staff. \"Document Analysis Worksheets.\" ARCHIVES.GOV. U.S. National Archives and Records Administration. 14 Nov <http://www.archives.gov/education/lessons/worksheets/index.html>.Education Staff. \"Teaching with Documents: The Arrest Records of Rosa Parks.\" ARCHIVES.GOV. U.S. National Archives and Records Administration. 15 Nov <http://www.archives.gov/education/lessons/rosa-parks/#documents >.\"History and Culture: Questions and Answers.\" Open Door: Ideas and Voices from MIT Massachusetts Institute of Technology. 15 Nov <http://alumweb.mit.edu/opendoor/200211/dower.shtml>.Lamb, Annette, and Larry Johnson. \"Analyzing Primary Sources.\" E-Scrapbooking. Feb Nov <http://escrapbooking.com/primarysources/index.htm>.\"Using Primary Sources.\" Do History: History Toolkit. Film Study Center, Harvard University, and Center for History and New Media, George Mason University. 14 Nov <http://dohistory.org/on_your_own/toolkit/primarySources.html>.","On January 15, 1929, Martin Luther King, Jr. is born in Atlanta, Georgia, the son of a Baptist minister.King received a doctorate degree in theology and in 1955 helped organize the first major ...\nOver 10 years in the making, Anderson's biographical graphic novel has the weight and depth of a lifetime of research. The book is a compelling and often moving narrative of the life of Martin Luther King Jr. Anderson follows King from boyhood through college and into the stormy civil rights movement.\nDr. Martin Luther King Jr. (January 15, 1929 – April 4, 1968) was an American minister and activist who was one of the most effective and visible activists for the American Civil Rights Movement. He emphasized civil disobedience and nonviolent resistance and protest. In honor of the contributions Martin Luther King made to society, Martin Luther King Jr. Day has been an American federal ...\nMartin Luther King Jr. Martin Luther King Jr. was born in Atlanta, Georgia in 1929 into a Baptist family. At the time he received the honor in 1964, he was the youngest recipient of the Nobel Peace Prize at age 35; he donated all of the prize money to the civil rights movement to which he dedicated, and ultimately gave, his life.\nDr. Martin Luther King Jr. and his wife, Coretta Scott King, sit with three of their four children in their Atlanta, Ga, home, on March 17, 1963. (AP Photo) King was born Michael Luther King in Atlanta on Jan. 15, 1929 — one of the three children of Martin Luther King Sr., pastor of Ebenezer Baptist Church, and Alberta (Williams) King, a ...\nMartin Luther King Jr Trivia and Fun Facts. January 20, 1986 was the first national celebration of King's birthday as a holiday. Martin Luther King Jr. received a \"C\" in his public speaking class. Although with his passion and overcoming adversities he delivered one of the most famous speeches of all time. Won Nobel Peace Prize on December 10 ...\nMartin Luther King, Jr. (born Michael King, Jr.; January 15, 1929 – April 4, 1968) was an American pastor, activist, humanitarian, and leader in the Civil Rights Movement.He was best known for improving civil rights by using nonviolent civil disobedience, based on his Christian beliefs. Because he was both a Ph.D. and a pastor, King is sometimes called the Reverend Doctor Martin Luther King ...\nMartin Luther King, Soundtrack: Freedom Writers. Martin Luther King Jr. was born on January 15, 1929, in Atlanta, Georgia. He was the son of Alberta Christine (Williams), a schoolteacher, and Martin Luther King Sr. a pastor of the Ebenezer Baptist Church in Atlanta. For Martin the civil rights movement began one summer in 1935 when he was six years old.\ndl.ueb.edu.vn/bitstream/1247/12633/3/Martin Luther King Biography.pdf\nMartin Luther King and Malcolm X briefly meet in 1964 before going to listen to a Senate debate about civil rights in Washington. (image Wikicommons) Speeches of Martin Luther King Jr Martin Luther King was an inspirational and influential speaker; he had the capacity to move and uplift his audiences. In particular he could offer a vision of hope.\nI would say Clayborne Carson’s ‘The Autobiography of Martin Luther King Jr.’ It was published in 2001. Dr. King’s family commissioned him to write create the book. Also, Dr. Carson is a historian and the director of the Martin Luther King Research..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3338d96b-d64b-4896-9df8-328a6fb2c603>","<urn:uuid:53806527-9d5f-4596-899d-1633da62ba5a>"],"error":null}
{"question":"What defines a dangerous substance in workplace health and safety context?","answer":"A dangerous substance is defined as anything that might cause harm to the health of anyone in the area.","context":["Health and Safety in the workplace\n– Assessing and addressing the risks.\nOccupational health and safety is a very important element in any UK workplace. Its purpose is to help educate and empower employees so that they are less likely to suffer an accident or injury while working.\nHealth and Safety involves a series of rules and guidelines whose main goal is to reduce the risk to anyone working in the area. Hopefully, by assessing and addressing the risk well in advance, the situation will not develop into a potential catastrophe. These rules and Regulations are not only good for employees. They have also been shown to be a positive benefit for employers, as keeping employees safe and comfortable can be greatly beneficial for business.\nSeveral examples could be given as useful applications of health and safety. One might be asbestos Surveys and testing. It is important to ensure that employees are not exposed to asbestos or any other dangerous substance. A dangerous substance is defined simply as anything that might cause harm to the health of anyone in the area. If this danger is inevitable due to the nature of the work under scrutiny, then measures will have to be taken to reduce the risk. This might include offering the employees suitable protective equipment.\nHealth and safety can be seen very easily on a building site. These are dangerous environments with many potential hazards. Suitable protective gear must therefore be worn at all times. This includes hard hats, steel-toe-cap boots, high-visibility vests and the like. It is arguable that it is never possible to eliminate risk completely, since even something minor and unremarkable could pose a hidden danger and escalate into a life-threatening situation.\nHowever, the goal of health and safety is to reduce risk as much as possible. While it may never be possible to ensure the safety of employees fully, it has been very successful in improving it a great deal.\nEmployees And Employers\nOccupational health and safety does not only help employees. While some people may view the rules as unnecessarily restrictive or imposing, in practice they have been shown to be a net positive for employers. This is because even though more time, effort and money must be spent in advance to ensure employees’ safety, these costs must be seen in context.\nThe increased levels of health and safety and reduced risk lead to greatly reduced compensation and medical bills resulting from accidents that could easily have been avoided.\nOccupational health and safety aids both employees and employers. As a system for assessing risk and attempting to reduce it as much as is realistically possible, it works very well. Some people may feel that it imposes unnecessary restrictions on workers and businesses, and in some cases that may be true.\nOverall however, it has been highly successful in ensuring that people stay in the workplace and out of the hospital. Everyone involved, both managers and workers, should pay close attention to the suggested guidelines in order to keep themselves and everyone around them safe. Doing so will increase long-term satisfaction and enjoyment no end."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:83506830-f9ef-4fb0-944e-77d8230376f0>"],"error":null}
{"question":"How is duck confit traditionally prepared? I've heard it's a French dish but don't know the details!","answer":"Duck confit is prepared through a centuries-old preservation process. The meat is first rubbed with salt, garlic, and sometimes herbs like thyme, then refrigerated for up to 36 hours. After rinsing and patting dry, the meat is slowly poached in its own fat at a low temperature (76-135°C/170-275°F) for four to ten hours until meltingly tender. The meat is then cooled and submerged in the cooking fat for preservation.","context":["This article's lead section may be too long for the length of the article. (March 2018)\nThis article needs additional citations for verification. (February 2013) (Learn how and when to remove this template message)\nDuck confit (French: confit de canard [kɔ̃.fi d(ə) ka.naʁ]) is a French dish made with the whole duck. In Gascony, according to the families perpetuating the tradition of duck confit, all the pieces of duck are used to produce the meal. Each part can have a specific destination in traditional cooking, the neck being used for example in an invigorating soup, the garbure. Duck confit is considered one of the finest French dishes.\nWhile it is made across France, it is seen as a specialty of Gascony. The confit is prepared in a centuries-old process of preservation that consists of salt curing a piece of meat (generally goose, duck, or pork) and then cooking it in its own fat.\nTo prepare a confit, the meat is rubbed with salt, garlic, and sometimes herbs such as thyme, then covered and refrigerated for up to 36 hours. Salt-curing the meat acts as a preservative.\nPrior to cooking, the spices are rinsed from the meat, which is then patted dry. The meat is placed in a cooking dish deep enough to contain the meat and the rendered fat, and placed in an oven at a low temperature (76 – 135 degrees Celsius/170 – 275 Fahrenheit). The meat is slowly poached at least until cooked, or until meltingly tender, generally four to ten hours.\nThe meat and fat are then removed from the oven and left to cool. When cool, the meat can be transferred to a canning jar or other container and completely submerged in the fat. A sealed jar of duck confit may be kept in the refrigerator for up to six months, or several weeks if kept in a reusable plastic container. To maximize preservation if canning, the fat should top the meat by at least one inch. The cooking fat acts as both a seal and preservative and results in a very rich taste. Skipping the salt curing stage greatly reduces the shelf life of the confit.\nConfit is also sold in cans, which can be kept for several years. The flavourful fat from the confit may also be used in many other ways, as a frying medium for sautéed vegetables (e.g., green beans and garlic, wild or cultivated mushrooms), savory toasts, scrambled eggs or omelettes, and as an addition to shortcrust pastry for tarts and quiches.\nA classic recipe is to fry or grill the legs in a bit of the fat until they are well-browned and crisp, and use more of the fat to roast some potatoes and garlic as an accompaniment. The potatoes roasted in duck fat to accompany the crisped-up confit is called pommes de terre à la sarladaise. Another accompaniment is red cabbage slow-braised with apples and red wine.\nUse in other dishes\nDuck confit is also a traditional ingredient in many versions of cassoulet.\n- Times-Picayune, Ann Maloney NOLA com | The. \"How to make duck confit: A simple, if time-consuming dish\". NOLA.com. Retrieved 2018-12-07.\n- How to make duck confit\n- Duck confit\n- Kerry Saretsky. \"Potatoes Sarladaises Recipe\". Serious Eats. Retrieved 10 June 2018.\n|Wikimedia Commons has media related to Confit de canard.|\n|Wikibooks Cookbook has a recipe/module on|"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:34e9608d-4d13-4f30-ad1a-4ee147a8bb39>"],"error":null}
{"question":"What's the best aperture setting for surf shots, and when do you need permission to photograph people?","answer":"For surf photography, even when shooting from the beach with an f/2.8 lens, both surfer and wave will be in focus. For underwater photographs, an aperture between f/4.0 and f/8.0 is recommended. When using basic lenses, it's better to avoid shooting at maximum aperture - for instance, with a 300mm f/5.6 lens, shoot at f/6.3 or f/7.1. Regarding photography permissions, you don't need permission to photograph people in public spaces, but you do need consent in situations where there's a reasonable expectation of privacy (like in homes or hospitals). Additionally, if you plan to use photos for commercial purposes or advertising, you must obtain a model release from the subject.","context":["5 Surf Photography Tips in 2021\nIn recent years, surfing as a sport has become increasingly popular in many countries. Here are some tips on photography techniques applied to surfing for you that even as an amateur photographer, you can make the most of your equipment:\nPhotometerNowadays, all digital cameras have a photometer, which measures the amount of light that reaches the camera's sensor. The sensor replaced the photographic film and is responsible for \"capturing\" the image. The photometer is that pointer around zero that varies according to the light input, indicating your photo exposure. So if the pointer is at zero, it means that your image is \"balanced,\" and for surfing photographs, if you can keep the pointer at zero or a little more clear, it is enough for good photometry. And the three settings that regulate your camera's sensor exposure to light are the sensor's sensitivity to light, shutter speed, and aperture. To be able to adjust the three parameters, your camera must be in Manual mode.\nThe sensor's sensitivity parameter, which is called ISO, in the input cameras usually starts at 100 and goes up to 6,400, with the possibility of being expanded. Thus, the lower the ISO value in your configuration, the less sensitivity to light your sensor will have. In other words, if you want to take a photo in broad daylight, with the sun in full view, the ideal ISO setting will be 100. Especially because, the higher the ISO, the more noise is embedded in your image. Then, for surfing photographs, use the lowest possible ISO, maximum 400, to ensure good image quality.\nThe second parameter mentioned is your camera's shutter speed, measured in seconds, varying from the 30s to 1 / 4000s. The shutter speed adjustment is what gives the blurred effect in your photo, when used at low speed, or determines the complete freezing of the image where even the drops of water can be very clear. So speeds from 1 / 1000s will guarantee freezing of the image in critical and fast maneuvers such as strikes and aerials. In longboard photos or even for a more classic surfboard, slower shutter speeds from 1 / 640s can be used to freeze the image satisfactorily.\nThe third parameter of photometry is the diaphragm's aperture, which is measured in \"f / x\" and is therefore inversely proportional to its value. The higher the value of x, the smaller the aperture and vice versa. In addition to regulating the amount of light that reaches the sensor, the aperture is responsible for regulating the depth of your photograph's focal field. The larger the aperture, the smaller the depth of field. But for surfing, even if you're shooting from the sand with an f / 2.8 aperture lens, the surfer and the wave will be fully focused. Except for underwater photographs, where the aperture between f / 4.0 and f / 8.0 is recommended A very important tip to get clearer photos, especially for those who use the most basic entry lens, is not to shoot at the maximum lens aperture. That is, if you use a 300mm lens with a maximum aperture of f / 5.6, try shooting at f / 6.3 or f / 7.1 and depending on the ambient light, and being careful not to raise the ISO too much. The same goes for lenses used in water, except for the 50mm f / 1.8 that performs better between f / 4.0 and f / 5.6.\nWhich Lens to Use?\nIf your camera has a built-in lens, try to use only optical zoom, as the digital zoom usually degrades the image. The range will depend on the type of camera you are using. And for those who use removable lenses, from 200mm onwards, you already have a reasonable range, working very well on peaks that break closer to the sand.","From the client’s perspective, this is one of the most searched and asked questions and it is not easy to answer. There are lots of things that we need to consider, but if you are looking for an answer that applies in most situations then here it is:\nIf a photographer took a photo of you on the street and there were other people around you, he can use that photo without your permission in most cases but there are some exceptions.\nFirst, did you hire a photographer?\nIf the answer is YES, then legally speaking he can’t use the photos without your consent. In order for the photographer to use the photos, he would need to sign a model release. You need to have some kind of a written agreement with your photographer, but in most cases even if you don’t have a written agreement he does NOT have the permission.\nHe does not have permission to use the photos if you have a reasonable expectation of privacy.\nWhat does this mean?\nIf you are in the privacy of your home, in a hospital, public restroom or any similar act, that is considered by a definition to be a reasonable expectation of privacy.\nDisclaimer: These laws are generally true for most countries, but just to be safe do NOT rely on them. If you have this kind of problem seek legal advice, don’t take matters into your own hands.\nThe rest of the article will focus on the photographer’s perspective, it is also advisable for you to read the whole article in order to get a better grasp of the photography laws.\nPhotography Laws – What You Can, Can’t or Might Be Able to Do as a Photographer\nGenerally, a lot of the laws are the same, but it makes sense to check up on your own state laws or those of the state where you will be doing some work.\nFrom forms to copyright laws, there is a myriad of information on the web and sifting through it can be mind-boggling. It is worth the time and effort to know what your rights are, as well as the rights of those you are photographing.\nBelow are a few pointers, but please make sure to learn about the laws where you live before you start snapping.\nCan You Photograph Anything on Public Land?\nYes, you can.\nAs long as you are standing on public land when taking the photograph, you can take it. Even if you spot a celebrity running for cover or the police arresting someone it is fair game.\nYou can take pictures of an accident scene, just don’t get in the way of emergency services. But you wouldn’t, would you?\nWhat can’t you photograph?\nAnything that would make it easier for someone with ulterior motives to achieve their objectives – so nothing to do with the military or power infrastructure for instance. And of course, take into consideration the places where people would expect to have privacy – for instance hospitals and houses.\nRemember your equipment can be a safety hazard, so don’t just haul it out and set up.\nA lot of the national parks have their own guidelines so please check if you will be taking any photographs using other equipment.\nWhen in doubt, ask!\nAlso, remember that people will approach you to find out what you are doing. Be prepared to show them and tell them your intentions.\nHand out your business card. Private citizens cannot by law stop you from taking photographs.\nBeing polite and approachable will go a long way to allay suspicion, especially in the times we live in now.\nWhat Are You Allowed to Photograph on Private Land?\nWhat is private land? All business and public buildings.\nAnywhere where people gather, such as theaters, sports stadiums and concerts will have their own regulations and policies need to be checked first.\nShopping malls are private land and no shop is going to allow you to walk in and start photographing what they are selling.\nAlso, governmental buildings and departments will have strict rules about photographs, so try not to ruffle any feathers otherwise you will be swapping stories with your new cellmates!\nCan I Photograph Law Enforcement?\nYes, you can.\nMany police officers might be uninformed about the regulations in your state and tell you that you cannot take photographs. Even in stressful situations, police do not have the right to stop you.\nObviously, you would need to use your discretion in these situations so that you do not get in the way.\nCan they force you to delete your photographs?\nCan they confiscate your equipment?\nNo and no.\nBut be calm and respectful especially when things are going down.\nAsk what law you are breaking?\nThey could, of course, ask you to leave – which you might consider unless of course, you miss those cellmates mentioned previously!\nDo I Always Need a Model Release?\nMany people think you need one no matter who and where you are photographing but that is not true.\nIt would be impossible to find everybody in a crowd and get them to sign a release. BUT it would be needed if the person you are photographing’s right to privacy has been compromised and if you are going to use their image for commercial use.\nPhotography Law and Social Media\nSocial media sites today encourage people to share photographs. This means that copyright infringement is now a widespread problem.\nSo how do social media platforms terms and conditions affect your rights when your photos are posted and shared publicly?\nOr what if they are shared by someone other than yourself?\nCan you use some other photographer’s pictures?\nIt seems that it would depend on the platform’s terms and conditions.\nLet’s look at Facebook – when you place a photo on Facebook, you are granting it the right to use the content you post. But if you are a copyright owner of a photograph posted on Facebook, you do not give up all your property rights.\nIf you post on Instagram – you need to comply with copyright laws and acknowledge you own the photos you are posting. But, you are also giving Instagram a sublicense (remember Creative Commons) to use the uploaded content.\nCheck the terms and conditions on any social media platform you use.\nIn any event, the rules can blur, and it can be easier just to carry on regardless.\nIf you have any doubts about posting your photographs, using someone else’s photos, rather do some homework first.\nThe best way forward is always to obtain permission and that works both ways.\nDo I Need a Permit for Commercial Photography?\nSome locations will ask for a permit and sometimes even proof of insurance, for instance, the California State Parks. Some historical parks also require permits.\nIf your photography involves sets or props or models, often a permit will be involved. Also remember that if your photography will involve any disruption, a permit may be needed.\nThis would also be applicable if you needed access when the area is normally closed, or you would like to use a restricted area.\nRemember, If a photograph shows private property where someone can identify the owner of the property, you should get a property release if you are going to use that photograph for advertising or commercial purposes.\nAre You Allowed to Sell Your Photographs?\nAnything taken on public land, yes you can. Unless you have taken pictures through windows or open doors and violated what is someone’s private space.\nBut NOT if you are trying to sell your photo for commercial use.\nYou cannot take a shot of a gorgeous guy and try to sell it to an advertising agency. You would need his agreement first before anything can happen.\nIf you have taken a picture of a crowd at an event and sell them to a magazine, for instance, that should be ok – although they might ask for a model release if the crowd turns out to be two people.\nAny photo taken on public land that you are not selling for commercial use (advertising for instance) can be sold.\nThe Big C! Copyright Laws and Creative Commons\nCopyright for photographers means owning photographs which are deemed as your property. And when you own something, you get some exclusive rights to that property.\nThis is what those rights entail with regards to your photographs:\n(1) You have the right to reproduce your photographs;\n(2) You can create other works, based on your photographs;\n(3) You can either sell your photographs to the public or transfer ownership to someone else; and\n(4) You have the right to display your photographs publicly.\nAnother type of licensing is called Creative Commons.\nSo, what is that?\nIt means that you give others some of the rights that you have to one or more of your photographs.\nLet’s use an example.\nMaybe someone wants to use your photograph in an issue of a magazine – you can allow that, but you still own the copyright.\nThere are different licensing packages under Creative Commons, so you have some flexibility with your licensing – you can decide if you will allow commercial use of your photograph or whether you want to be acknowledged in some way for your work.\nWho Owns the Copyright Once the Picture Is Taken?\nGenerally, once you have taken the picture, you own the copyright.\nThere are exceptions such as the “work for hire” category. This can be when the photographer is an employee hired to take photographs for the employer, such as a photojournalist who works for a newspaper or a photographer who is hired to take photographs for a compilation and has signed a written agreement.\nSo, a work-for-hire status only applies when you have contractually agreed to it.\nWhile you might own your photograph, there is something called Fair Use.\nThis means that in a limited way, your photos can be used without gaining your permission. This means they can be used for teaching and research for example.\nIf an infringement occurs, then the court will decide if it Fair Use or not.\nWhat If I Did Not Register the Copyright and Someone Uses My Photograph?\nIf you have not registered your photograph with the US Copyright Office and someone uses your photo without permission, you can still recover some damages.\nThese will be determined by the court.\nAre Americans the Only Ones That Need to Register Their Copyright with the Library of Congress in the United States?\nThe answer is a surprising no!\nIt does not matter what nationality you are, all unpublished photos are protected in the United States.\nThe US has copyright law and any photo, even those taken in other lands are protected by this law and can be registered.\nDo I Need to Put the © or the Word Copyright on My Photos?\nYou might see this © sign or the word “copyright” with a date and name of the copyright owner—on any type of creative work.\nThere are three parts to a proper copyright notice:\nit needs to have the ©, the word “Copyright,” or “Copr.”\nThen it needs the year when the work was published and then the name of the copyright owner.\nYou do NOT have to by law use this, but it can be a good idea to use it as it might stop someone from using your photo as the copyright sign stays on the photo and spoils the look of the product.\nAlso, there is no recourse for someone using your photograph if the sign is on the photograph and you may be able to claim for DMCA damages if your copyright notice is removed to hide a violation.\nYou also do not have to register your work with the U.S. Copyright Office to use the copyright sign or notice.\nLet’s Get Legal: Forms and Contracts\nWhat Is a Portrait Agreement?\nIt is like a service level agreement between two people detailing specifics such as – your name and the name of the client; how much the service will cost; what services have been decided upon; the cancellation policy; what is the expected time-frame for the end product; how the product will be delivered. And most importantly – a notification of copyright.\nWhat Is a Model Release?\nThis was discussed earlier, but it is good to understand what it is exactly and when it is needed.\nIt is a release signed by the subject of the photography or if the subject is a minor, then the parent/guardian. This signed form means you can take and display any photographs taken.\nThis form is not always necessary, but if you are going to display the photographs on social media or in public areas then you need it.\nWhat Is a Print Release?\nIf you are selling digital files to a client, then you can get a print release signed. This means you are giving permission for the client to reproduce the digital files bought.\nThis does not mean you are giving up your copyright – you are still the owner.\nWhat Is a Copyright Notice?\nThis should be included in your portrait agreement.\nThis can also be given to clients when you are handing over their photographs to remind them of the copyright laws in your country or state. Many clients are unaware of copyright law – you might have to educate them from the beginning.\nIf you are a wedding or other type of photographer, there are photography specific contracts that you will always need in your business.\nMake sure that you are up-to-date with the laws in your area or country and keep all the forms handy.\nSome Interesting Photography Laws and Stories – in the US and Other Countries\nTexas has an improper photography’ statute where it is illegal to photograph anyone without their consent if the intent is ‘to arouse or gratify the sexual desire of any person’.\nIn Budapest, you have to get permission from anyone and everyone you want to photograph. This also includes the police.\nIt is illegal to photograph the Eiffel Tower at night as the light show is protected by law.\nIn Iran, you will be encouraged to take photographs – even inside mosques! But photographing women is not allowed.\nIn Romania and Poland, if you are taking your camera into a tourist attraction, you have to pay “extra” fees.\nYou can photograph anything in the Philippines, but the malls, even if you are standing on their grounds.\nIn the US, a documentary photographer was arrested for photographing street artists – apparently, he was documenting graffiti.\nIn Australia, there are many no photography signs in the cities that have been put up by companies, limiting what can and can’t be photographed more than ever before.\nSince 9/11, there have been many changes in photography law.\nIn the UK, taking photographers of public places can get you into trouble – fears of security and paranoia mean that photography is being restricted.\nWhile some countries still view a person with a camera as an artist, many others are clamping down.\nIt is hard to imagine that simply taking photographs could involve so much legalism and implications that could land you or other photographers in hot water!\nAfter all, is said and done, the most important thing is to do your research – make sure you have the correct forms, phone a lawyer if need be – and when in doubt – ask and get signed permission.\nProtect yourself and your photographs. It is your right."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5ac48d32-0844-498d-9e1c-f70874b5f6a8>","<urn:uuid:1c7547fa-895d-4496-bcbe-9b7ca6563733>"],"error":null}
{"question":"Hello! I'm researching why modern businesses face increased cybersecurity threats and how cloud storage impacts their security posture. Can you explain the current trends in cyber attacks and describe how cloud DLP solutions address these evolving challenges?","answer":"Modern businesses are experiencing an escalating threat landscape, with U.S. companies facing an average of 270 cyber attacks per company in 2022, a 30% increase from 2020. Many businesses lack the knowledge, skills, and technology to prevent these attacks. This evolving threat environment is further complicated by cloud storage adoption, where employees work from various locations and may use unapproved services (Shadow IT). Cloud DLP solutions address these challenges by providing essential protection measures: they scan and encrypt sensitive data before cloud transmission, continuously monitor uploaded files, maintain visibility for compliance purposes, and offer automated controls for sensitive data protection. These solutions help organizations maintain security without sacrificing the scalability and efficiency benefits of cloud computing.","context":["No matter what type of business you own, you need to understand the importance of IT security. IT security, or cybersecurity, refers to a series of techniques and procedures to guard a business’s confidential data and essential systems against online data infringements and cyber threats. Here are some of the main reasons why this is crucial for businesses:\n1. Cyber Crimes Are Becoming More Common\nIn 2022, businesses in the U.S. experienced an average of 270 cyber attacks per company. This figure is over 30% higher than in 2020, meaning that cyber crimes are increasing. One of the main reasons for this increase is that many businesses are unaware of how to prevent these attacks from happening. They do not have the knowledge, skills, or technology to stop cybercriminals from accessing their systems.\nThankfully, there is a way around this issue. Businesses can outsource their IT security to a Managed Services Provider. These companies have up-to-date knowledge, skills, and technology to reduce the chances of cybercrimes occurring.\n2. Cyber Crime Can Impact Your Business\nCompanies have many assets that cybercriminals can take advantage of. For example, they can retrieve personal information about customers or businesses or gain access to your money. Once they have this information, they can then exploit it. Here are some of the ways this can negatively impact your business:\n- It can cause you to have financial losses – it is estimated that by 2025, cybercrime will cost companies $10.5 trillion worldwide. This has increased by $7.5 trillion since 2015.\n- It may lead to a dip in productivity\n- Data breaches can significantly damage your reputation – particularly if customer data is stolen.\n- It can affect customer retention – once a brand reputation gets damaged by a data breach, customer retention will also reduce. Customers often lose faith in a business that has suffered a data breach.\n- It can cause business continuity problems.\n- Legal liability – businesses may be liable when a data security breach occurs. However, this depends on certain factors, including how the information was stored and how well it was protected before the breach occurred. If you can prove you did everything possible to prevent a breach from happening, you may not be legally liable.\n- It may force you out of business – an estimated 60% of businesses affected by significant data breaches go out of business.\n3. To Ensure Your Business Complies With Regulations\nAnother reason businesses need to take IT security seriously is to comply with government regulations. In recent years, governments have increased their regulations for cybercrime. One example of this is the GDPR (General Data Protection Regulation). Though not drafted in the U.S., this regulation imposes obligations on businesses in Europe. It forces businesses to communicate any data breaches, appoint a data protection officer, anonymize data for privacy, and require user consent to process information.\nIn this modern, technologically advanced world, IT security has become necessary for businesses of all sizes. This is because, no matter how big or small a business, they all possess confidential data that is at risk of being stolen by cybercriminals. Not protecting this data can cause many issues, including the ones we’ve listed above.","What Is Cloud DLP?\nA Definition of Cloud DLP\nData loss prevention (DLP) is a process for protecting sensitive data at rest, in-transit, and on endpoints to reduce the likelihood of data theft or unauthorized exposure. DLP solutions aim to prevent sensitive data and confidential information from being stored, used, or transferred insecurely.\nCloud DLP solutions specifically protect organizations that have adopted cloud storage by ensuring sensitive data does not make its way into the cloud without first being encrypted and is only sent to authorized cloud applications. Most cloud DLP solutions remove or alter classified or sensitive data before files are shared to the cloud to ensure that the data is protected when in transit and cloud storage.\nBenefits of Cloud DLP\nToday, data is put at risk as organizations move to the cloud and employees work from various locations, accessing corporate files from anywhere and at all hours. Employees also collaborate using the cloud, but they may do so using unapproved services and cloud storage apps – otherwise known as “Shadow IT.” That’s why it’s critical for organizations to protect sensitive data not only on their own networks and devices, but in the cloud as well.\nKey benefits of leading cloud DLP solutions include:\n- Integrate with cloud storage providers to scan servers, identify, and encrypt sensitive data before the file is shared in the cloud\n- Scan data already stored in the cloud and audit it at any time\n- Accurately discover sensitive data in the cloud\n- Continuously audit uploaded files\n- Automatically apply controls (prompt, block, encrypt) to sensitive data in accordance with enterprise policies\n- Instantly alert appropriate administrators and data owners when data is put at risk\n- Maintain the visibility and control needed to comply with privacy and data protection regulations\nChallenges of Cloud Data Protection\nOrganizations that do not implement a cloud DLP solution essentially leave cloud data protection up to their cloud storage providers. Problems can arise, however, when those providers fail to take security measures commensurate with the organizations’ data protection needs – such as not offering cloud encryption, multi-factor authentication, or strict access controls. What’s more, a compromise at a cloud storage provider can lead to a compromise of an organization’s data if the organization hasn’t taken steps to secure it before it was sent to the cloud.\nThere are enough challenges associated with cloud storage providers handling cloud data protection that many organizations choose to be proactive with protecting their data in the cloud by implementing a cloud DLP solution to secure their sensitive and confidential information, rather than placing trust in cloud services providers that their DLP and other security measures are adequate for meeting company security requirements and compliance standards.\nChoosing Cloud DLP Providers\nWhen selecting a cloud DLP solution, organizations should be sure it offers the following key features:\n- Content- and context-aware monitoring and inspection policies\n- Detailed activity logging and reporting\n- Device-level control\n- Auditing, alerting, prompting, blocking, and removing remediation actions\n- Encryption of sensitive data prior to cloud upload\n- API integration with cloud storage providers to extend data security policy enforcement to the cloud\nThe reality is that businesses and their employees need to be able to conduct business in the cloud in order to remain productive. However, cloud adoption can also put data at risk of loss or unauthorized access. This risk has led to the need for cloud DLP solutions, because businesses must be assured their sensitive data is being protected while they benefit from the scalability and efficiency of the cloud. Choosing a cloud DLP solution that is offers exceptional data discovery and visibility in the cloud and delivers the protective controls required for safeguarding cloud data is critical."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:318caa43-164f-45a7-b806-d977953e7f48>","<urn:uuid:70fd4c7b-e3a0-43b4-8153-e2f70cd959a3>"],"error":null}
{"question":"Having studied the 2013 Giro d'Italia route, how does the strategy for racing Stage 15's Col du Télégraphe compare to the pacing recommendations for a recreational sportive over the same climb?","answer":"On Stage 15 of the 2013 Giro, riders face the Col du Télégraphe as part of a 150km high mountain stage, climbing 848 meters over 12km at an average 7% gradient (max 11%) from St. Michel de Maurienne. For sportive riders, the approach should be much more conservative - they're advised to maintain a conversational pace and high cadence of 80-100rpm, using easier gearing especially on the 11% sections. This is particularly important when Télégraphe is part of a longer event, as pushing too hard early can lead to excessive fatigue. The climb should be approached with consideration for what's ahead, maintaining consistent effort rather than racing it at maximum intensity.","context":["Stage 15 of Giro d’Italia 2013 edition is a High Mountain stage with summit finish. It will start from Cesana Torinese and finish at legendary Col du Galibier (from Valloire). The length of the course is 150 kilometers. The route also contains epic climbs of Col du Mont Cenis and Col du Télégraphe.\nTable of Contents\nDATE: May 19 2013, Sunday\nSTAGE TYPE: High Mountain with Summit Finish\nSTART-FINISH: Cesana Torinese (1345 m) > Col du Galibier (Valloire) (2642 m)\nLENGTH OF THE COURSE: 150 km\nThe stage is starting in Italy and finishing in France.\nThere are 3 main climbs in the stage route, and all of them are in France:\n- Col du Mont Cenis (59.5th km, 2095 m): starting at 33.8th kilometer near Susa (380 m). Riders will gain 1715 meters in 25.7 kilometers (average 6.7%). The peloton will pass the Italia-France border at 51st kilometer, with the elevation of 1728 meters.\n- Col du Télégraphe (127.1st km, 1566 m): starting at 115.1st kilometer at St. Michel de Maurienne (718 m). Riders will gain 848 meters in 12 kilometers (average 7%, max. 11%).\n- Col du Galibier (from Valloire) (150th km, 2642 m), Summit finish: starting at 132nd kilometer at Valloire (1405 m). Riders will gain 1237 meters in 18 kilometers (average 6.8%, max. 11%).\nCesana Torinese is a comune (municipality) in the Province of Turin in the Italian region Piedmont, located about 70 km west of Turin, on the border with France. (wiki)\nMont Cenis (Italian: Moncenisio) is a massif and pass (el. 2081 m / 6827 ft) in Savoie in France which forms the limit between the Cottian and Graian Alps.\nThe pass connects Lanslebourg-Mont-Cenis in France in the northwest with Susa in Italy in the southeast. It was used as part of the border between the two countries until the 1947 Treaty of Paris, but is now located completely in France. (wiki)\nCol du Télégraphe\nCol du Télégraphe is a mountain pass in the French Alps situated above the Maurienne valley between the eastern end of the massif d’Arvan-Villards and the massif des Cerces. The pass links Saint-Michel-de-Maurienne to the north and Valloire to the south, as well as forming an access point to the col du Galibier via its north face. The route is often used during the ascent to Col du Galibier in the Tour de France, and is thus popular with cyclists. (wiki)\nCol du Galibier\nCol du Galibier (el. 2,645 metres (8,678 ft)) is a mountain pass in the southern region of the French Dauphiné Alps near Grenoble. It is the ninth highest paved road in the Alps and the sixth highest mountain pass. It is often the highest point of the Tour de France. (wiki)\nGiro d’Italia will reach at the top of Galibier first time in history.\nThe Col du Galibier was first used in the Tour de France in 1911; the first rider over the summit was Emile Georget, who, with Paul Duboc and Gustave Garrigou were the only riders not to walk. The original summit was at 2556 m.; while the tunnel was closed from 1976 until 2002, the tour route went only over the pass closer to the mountain peak at 2645 m. In 2011, the Tour de France went through the tunnel for the first time during the 19th stage from Modane Valfréjus to L’Alpe d’Huez (Pierre Rolland won the stage). (wiki)\n- 5 Cross-Training Sports to Make You a Better Cyclist - June 1, 2020\n- 2020 Granfondo Stelvio Santini on Rouvy: Conquer The King of Climbs Virtually - May 28, 2020\n- Cycling to Recycling: Spray-Painting an Old Bike - May 27, 2020","The correct pacing strategy is a massively important aspect of cycling. Regardless of this it is an often overlooked factor even though the performance gains that can be achieved are huge.\nPacing For A Personal Best\nExcellent pacing is the key to unlock your personal best on Alpe d'Huez and mountainous events.\n\"The correct pacing strategy is a massively important aspect of cycling. Regardless of this it is an often overlooked factor even though the performance gains that can be achieved are huge\". Quote and article by Mike Gaddas.\nDuring this short blog post I am going to split my article into two parts. The first part will discuss how to pace for a max effort time trial style hill climb. The second part will discuss the importance of pacing for an all day sportive style event. This post will be written with an Alpine theme but they key points are relevant across all areas and disciplines of cycling.\nPacing for a personal best on Alpe d’Huez\nPacing for fast time up a climb like Alpe d'Huez is a daunting task. Most decent amateur cyclists aim to climb this mythical beast in under 60 mins. In order to achieve this, you need a high level of aerobic fitness and a solid pacing strategy. Research suggests the quickest way to the top is often an evenly paced effort or even a negative split. For this post I am assuming the reader does not have a power meter.\nThe first 15 mins. The start of a climb is often the most important. The adrenaline is flowing and the temptation to stamp on the pedals is strong. The key to nailing the start is to throttle back and find a sustainable rhythm almost immediately. If you go over your threshold at this early stage your body will start to produce more lactate and you can end up in oxygen debt. This will eventually lead to a slower overall time. Your effort level or perceived exertion (RPE) should feel like a 7/10 at this stage.\nThe middle 30 mins. This is the main chunk of the climb. Hopefully you have a nice rhythm established and you can build on that during this period. Try to stay seated and spin an easy gear of around 80-100rpm. Feel free to get out of the saddle for brief periods every couple of minutes to spread the workload around different muscle groups. But ensure that the vast majority of the climb is undertaken in the saddle as this is the most efficient way to the top. By now the climb is starting to feel very challenging., The perceived effort level (RPE) should feel like an 8 or 9/10.\nThe last 15 mins. Finish strong. This is where mental toughness starts to become very important. The last section of a climb is going to hurt, a lot. You need to hold your form, stay relaxed and focus on breathing. Try breathing from your belly, this allows your lungs to expand fully which results in more oxygen being available for working muscles. The finish line is almost in sight but the amount of time that can be lost or gained in this period is significant. You must be willing to give everything in order to achieve your fastest time. Keep the pressure on the pedals and try to up your power for the last few minutes. Perceived effort should be a solid 10/10 for this. Sprint for the line and then you can hopefully bask in the glory of tackling Alpe d'huez in under 1 hour.\nPacing for a multiple mountain day / Marmotte sportive\nA full day cycling in the mountains can be a joyous or torturous experience. The correct pacing will go a long way to ensuring the latter. With an event such as La Marmotte, the correct pacing is the difference between finishing the event and not making it past halfway.\nThe ‘start easy finish strong’ ethos has never been more true than in La Marmotte. Go to hard on the first climb and a miserable day lies ahead of you. Ensure you know the climbs very well and always pace for what is to come not just what is in front of you. When you are on climb number 1, the Glandon, always have in the back of your mind that you still have the Telegraphe, Galibier and Alpe d’Huez to come.\nThere are many benefits of riding at lower intensities. It spares muscle glycogen, because your body by burns a greater proportion of fat as fuel. You will also produce less heat, meaning you sweat less. Eating and drinking on the bike is also easier because your breathing is under control and you aren't gasping for breath. The importance of these factors are huge over an event as long as the Marmotte. If you can finish the Glandon properly hydrated and with glycogen levels topped up you are in a strong position to press on with your day.\nMuscular fatigue is almost inevitable on an event as big as a Marmotte. However, there are some measures you can take to ensure you avoid excessive fatigue earlier than necessary. Pushing out less power on the first few climbs of the day is going to pay dividends later on. But don't underestimate the run in to the climb on the flat valley road. The starter sounds and people tend to sprint out at a pace that is not sustainable. Try to relax and ignore the people overtaking you. There is a high possibility you will be seeing them later. The first few hours should be taken at a ‘conversational pace’, meaning you should be able to chat without gasping for oxygen. This means precious glycogen reserves will be saved for when you need them most. Ensure to eat and drink plenty in these early stages.\nHaving a high cadence of between 80-100rpm is very important. Studies have shown that lower cadences cause muscle fibers to fatigue more because there is more muscular force required per pedal stroke when compared to high cadences. Sometimes riding at low cadences is inevitable on particularly steep sections of climbs. But the correct gearing will go a long way to helping avoid this. Riding with a compact chainset with a 32 tooth rear cassette is recommended. However, there are some issues with using such easy gears. If you stay in the 34x32 when the climb starts to flatten out, you can often find yourself going much slower than necessary. A climb that would originally take an hour can easily take 1hour 15 mins. Over an event like the Marmotte an extra 15 mins per climb is an extra hour or more on the bike. When the climb dips below 10%, don't be afraid to click down a gear a two and keep the same intensity as the steep parts.\nPacing tools are a great addition to help with pacing. By far the most common pacing tool is the heart rate monitor. These can be useful in an event like the Marmotte but there are also major issues with pacing off heart rate. Dehydration, altitude, adrenaline, caffeine, core body temperature, ambient temperature, eating and cardiac drift all have significant effect on heart rate values. If you have planned to climb the Telegraphe and Galibier in ‘zone 3’, you may start off cycling at 144 bpm but you could be up near 163 bpm by the time you are near the top even though you are cycling at the same intensity. This could be due to the fact you are slightly dehydrated or the fact there is 25% less oxygen available towards the top of the Galibier. I personally like using heart rate but understanding how external factors affect it is crucial.\nThe gold standard in pacing tool is a correctly calibrated power meter. This shows you the force you are actually putting through the pedals. The benefit of power is it isn't affect by external factors. If you know and understand your numbers it can make pacing the climbs on an event like the Marmotte slightly more straightforward. There will be more blog posts up in the coming weeks going into more details on pacing tools, nutrition and hydration so I am going to leave it there for now.\nThe key takeaway message is START SLOW! An all day event like the Marmotte or Etape is hard enough on its own. There is no need for you to make it any harder for yourself by going out hard. I wish you the best of luck, whether you are trying to sub 60 minutes up Alpe d’Huez, an event like the Marmotte or trying to finish a challenging multi day trip. All are massive achievements but certainly achievable with the correct preparation, nutrition and pacing.\n09 Jan 2020 07:38:00\nIf you have a power meter how would you translate your approach to the start , middle and finish sections to your power output ? Is it as simple as keeping your effort at or around FTP given most rides will last an hour or so or would you vary it as you have indicated for riders without a meter ?\n09 Jan 2020 17:39:04\nGreat question Gordon. I think I can safely say that it really depends upon your goal for the ride or event, and the type of terrain expect throughout. Pre ride/event it’s great to have an understanding of the course so you can plan to ride and fuel accordingly.\nFor rides that are longer than a couple of hours, it’s important to think about starting out at a pace that’s sustainable and can be built upon as the ride progresses. Food intake and hydration play a big role in the level of intensity that you choose or are forced to ride at later in the day. If you’re sensible with pacing early in a ride/event and fuel correctly, then there’s no reason at all why you can’t ride longer climbs at or around threshold and shorter ones at an even higher intensity level, even if they come towards the back end of the ride.\nThe essential thing here is to understand the cost of your efforts. The higher the intensity level the more energy you will expend. This is one of many great reasons to train and ride with a power meter because there’s a direction correlation of your efforts to the fuel that’s required. If you have a specific event in mind like your Ride Across America next year, the way you train over the coming 12 months, will have a significant impact on how you pace yourself next year. If it’s a multi day event having a solid understanding of how your body recovers days after day will also play a role. Using power, Heart Rate + Perceived exertion levels are the key.\nI’ve dropped you an email and if you’d like to know more please just let me know."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:7df0e2b2-27e7-4b85-a775-43f3aa85e25f>","<urn:uuid:6d9dd505-063a-48ad-ad63-3017be6e813f>"],"error":null}
{"question":"Hi! I'm interested in learning about Zidon - what was its historical importance as a city, and how is it connected to recent interfaith initiatives in Lebanon?","answer":"Zidon was a major Mediterranean coastal city and the first home of the Phoenicians in Palestine, located 25 miles north of Tyre. It was historically significant as a commercial center famous for its manufactures, arts and commerce. While initially influential, its glory began to wane after David's time as Tyre rose to prominence. Today, it continues to influence Lebanese society, as evidenced by recent interfaith initiatives in the country. The Interfaith Eldercare Group in Lebanon, which includes representatives from eight different religious communities (Druze, Shiite Muslims, Sunni Muslims, Armenians, Greek Orthodox, Maronites, Latin Catholics, and the Evangelical Church), is working to overcome historical religious divisions and improve elderly care services in the region.","context":["Zidon A fishery, a town on the Mediterranean coast, about 25 miles north of Tyre. It received its name from the \"first-born\" of Canaan, the grandson of Noah (Gen 10:15, Gen 10:19). It was the first home of the Phoenicians on the coast of Palestine, and from its extensive commercial relations became a \"great\" city (Jos 11:8; Jos 19:28). It was the mother city of Tyre. It lay within the lot of the tribe of Asher, but was never subdued (Jdg 1:31). The Zidonians long oppressed Israel (Jdg 10:12). From the time of David its glory began to wane, and Tyre, its \"virgin daughter\" (Isa 23:12), rose to its place of pre-eminence. Solomon entered into a matrimonial alliance with the Zidonians, and thus their form of idolatrous worship found a place in the land of Israel (Kg1 11:1, Kg1 11:33). This city was famous for its manufactures and arts, as well as for its commerce (Kg1 5:6; Ch1 22:4; Eze 27:8). It is frequently referred to by the prophets (Isa 23:2, Isa 23:4, Isa 23:12; Jer 25:22; Jer 27:3; Jer 47:4; Eze 27:8; Eze 28:21, Eze 28:22; Eze 32:30; Joe 3:4). Our Lord visited the \"coasts\" of Tyre and Zidon = Sidon (q.v.), Mat 15:21; Mar 7:24; Luk 4:26; and from this region many came forth to hear him preaching (Mar 3:8; Luk 6:17). From Sidon, at which the ship put in after leaving Caesarea, Paul finally sailed for Rome (Act 27:3, Act 27:4). This city is now a town of 10,000 inhabitants, with remains of walls built in the twelfth century A.D. In 1855, the sarcophagus of Eshmanezer was discovered. From a Phoenician inscription on its lid, it appears that he was a \"king of the Sidonians,\" probably in the third century B.C., and that his mother was a priestess of Ashtoreth, \"the goddess of the Sidonians.\" In this inscription Baal is mentioned as the chief god of the Sidonians.\nZif Brightness; splendour; i.e., \"the flower month,\" mentioned only in Kg1 6:1, Kg1 6:37, as the \"second month.\" It was called Iyar by the later Jews. (See MONTH.)\nZiha Drought. (1.) The name of a family of Nethinim (Ezr 2:43; Neh 7:46). (2.) A ruler among the Nethinim (Neh 11:21).\nZiklag A town in the Negeb, or south country of Judah (Jos 15:31), in the possession of the Philistines when David fled to Gath from Ziph with all his followers. Achish, the king, assigned him Ziklag as his place of residence. There he dwelt for over a year and four months. From this time it pertained to the kings of Judah (Sa1 27:6). During his absence with his army to join the Philistine expedition against the Israelites (Sa1 29:11), it was destroyed by the Amalekites (Sa1 30:1, Sa1 30:2), whom David, however, pursued and utterly routed, returning all the captives (Sa1 30:26). Two days after his return from this expedition, David received tidings of the disastrous battle of Gilboa and of the death of Saul (2 Sam. 1:1-16). He now left Ziklag and returned to Hebron, along with his two wives, Ahinoam and Abigail, and his band of 600 men. It has been identified with 'Asluj , a heap of ruins south of Beersheba. Conder, however, identifies it with Khirbet Zuheilikah, ruins found on three hills half a mile apart, some seventeen miles north-west of Beersheba, on the confines of Philistia, Judah, and Amalek.\nZillah Shadow, one of the wives of Lamech, of the line of Cain, and mother of Tubal-cain (Gen 4:19, Gen 4:22).\nZilpah Drooping, Leah's handmaid, and the mother of Gad and Asher (Gen 30:9).\nZilthai Shadow (i.e., protection) of Jehovah. (1.) A Benjamite (Ch1 8:20). (2.) One of the captains of the tribe of Manasseh who joined David at Ziklag (Ch1 12:20).\nZimmah Mischief. (1.) A Gershonite Levite (Ch1 6:20). (2.) Another Gershonite Levite (Ch1 6:42). (3.) The father of Joah (Ch2 29:12).\nZimran Vine-dressers; celebrated, one of the sons of Abraham by Keturah (Gen 25:2).\nZimri Praise-worthy. (1.) A son of Salu, slain by Phinehas, the son of Eleazar, because of his wickedness in bringing a Midianitish woman into his tent (Num 25:6). (2.) Murdered Elah at Tirzah, and succeeded him on the throne of Israel (Kg1 16:8). He reigned only seven days, for Omri, whom the army elected as king, laid siege to Tirzah, whereupon Zimri set fire to the palace and perished amid its ruins (Kg1 16:11). Omri succeeded to the throne only after four years of fierce war with Tibni, another claimant to the throne.","BELIEVING: COLLABORATING FOR CARE\nBELIEVING: COLLABORATING FOR CARE\nIn any thriving organization, there are always things going on that are motivated by the passions of the employees that may not become wholly evident to management until they reach a critical mass of ideas or initiatives. Such is the case with this month’s blog post by Philippe Saad, a newly named Associate Principal of the firm. Philippe came to me years ago and asked about doing some marketing in Lebanon in our market sector of senior living. I encouraged him, but explained that he would need to get the ball rolling and push it for a good while because none of us had any contacts or experience there to help him and we were focused on business in the US. Undeterred and full of youthful energy and enthusiasm, he began slowly, as you will see, to build a network and become a conduit to connect others. Simultaneously in Lebanon, there was movement to bridge religious differences in search of a better model of care for seniors. This year it all came together when Philippe was asked to go to Lebanon to speak at a gathering of the Interfaith Eldercare group. Below, follow the story of a young architect finding his calling in making the world more beautiful, functional, and sustainable for seniors, and finding a way to give back to his home country.\nThe Interfaith Eldercare Group in Lebanon convened in April, and I was invited to attend and speak about design for seniors–its evolution, nuances and benefits. I still remember when I first made the connection between what I saw in aging communities in my day job, places where residents thrive in supportive environments, and what I knew about elderly “asylums” Ma’wa Ejjaz in Lebanon, a term used to describe the places where the elderly were ‘put’ by their children to live the last few months/weeks of their lives. Being in a ma’wa is still seen today by many as a reflection of elderly abandonment by the family, or a result of one’s terminal illness and last resort.\nPresenters and organizers in Deir El Kamar village\nAs I was gaining expertise in senior housing over the past 10 years, I was also seeing my late mother’s and my father’s health gradually decline. Their ability and their willingness to socialize, learn new things and even be outside began to dwindle. At the same time, I was meeting 90 year olds living in communities who were singing, performing, planting, getting involved in their communities and falling in love with the new comers! For many years I have been cold calling Lebanese elderly care organizations and talking to development visionaries about the need to enhance and develop their offering. Everyone applauded the concepts we discussed, admired the beautiful examples of the aging communities DiMella Shaffer had designed over the years, but none was willing to take that first step! Believing in this need for Lebanon, I was determined to find a way to challenge the stigma around aging communities in Lebanon, and show how enriching life can be in a senior community. I kept advocating for it on many levels: amongst my Lebanese friends and with my own family, who particularly struggled to find an appealing nursing home for my grandmother when she could no longer be cared for at home. I also had many conversations with US-based organizations who had started lending their expertise to other countries, in particular China, hoping to find a way to do the same with Lebanon. As stubborn and dedicated as I am, I kept nurturing those relationships for long periods of time, through simple emails, sharing articles, phone calls, or a coffee during my visits to Lebanon. I remember almost 4 years ago, taking a trip to Dallas, TX with very short notice, to show Maha, director of the Greek Orthodox Nursing Home whom I had communicated with numerous times before, The Legacy at Willowbend, a luxury community DiMella Shaffer had just completed.\nThat’s why, years later, being invited to be part of the discussion about change in the elderly care field of Lebanon is rewarding, humbling and surreal! It is surreal because this is a country where any interfaith activity or initiative presents a formidable challenge, if it is at all possible. Growing up during the many years of civil war, religious militias fought each other, kidnapped civilians based on religion, and looked only after their own communities. In the 25 years that followed the official end of the war, religious differences have been only tolerated, mostly if dictated by foreign countries or forced by local powers. If we no longer see the scars of war on buildings in Beirut, they are still visible beneath the surface of the Lebanese society. That’s why I was positively impressed and pleased to find a young, ambitious Interfaith Group of collaborators whose main goal is to work together to better serve the elderly of Lebanon.\nJoyce Eid, managing director of Moadieh Evangelical Center, an assisted living community in Beirut, is the power behind this Interfaith Eldercare Group. A nurse by training, she started her new position by connecting with foreign organizations to learn how best to serve the elderly of Lebanon. Palm Village Community in California and its CEO David Reimer have been welcoming to her, and have guided Joyce in acquiring the knowledge and skill set to lead this Interfaith Eldercare Group. The group is composed of nine religion-based elder care organizations representing eight religious communities: Druze, Shiite Muslims, Sunni Muslims, Armenians, Greek Orthodox, Maronites (Eastern Catholics of Mount Lebanon), the Latin Catholics (Roman Catholics of Lebanon), and the Evangelical Church of Lebanon. After meeting for the first time in Cyprus in 2015, this Interfaith Eldercare Group, not yet incorporated, has since visited each other’s communities, learning from one another, referring residents to one another when appropriate, and even developing Memoranda of Understanding for future collaborations.\nI started working at DiMella Shaffer as a graduate architect with no desire to work in senior living. A few years later, I found my calling to this field after being assigned to the large project of repositioning Orchard Cove, a Life Plan Community in the suburbs of Boston. Since then I have dedicated almost 10 years of my professional career to learning the best and most innovative practices in designing for seniors. As my friend Aline Russotto told me once, none of us chooses the senior care industry; we somehow stumble upon it, fall in love with it and became its best advocates. This also seems to be the case with the many dedicated people I met on my trip to Lebanon.\nAs an architect focusing on the design of aging communities, I was asked to share my personal professional experience and also the latest senior living development strategies, including Dr. William Thomas’s Green House Model and its many adaptations. I shared my perspective on the residential model in senior housing, provided a guiding approach to focusing on wellbeing and independence through design, and also focused on the local demography of an incoming senior population boom (different than the US baby boomer population). I was accompanied by two other speakers: Nadeem Abi Antoun, a Lebanese expatriate Chief Operating Officer of Presbyterian Homes in Chicago, and Rick Stiffney, Chief Executive Officer of Menonite Health Services. We were scheduled to tour two communities the first two days, and then gather for the conference at Mzaar Intercontinental Resort over the weekend. The setting, away from the city life and away from the different organizations’ territories, was purposely chosen so the gathering was closer in nature to a retreat, a time of learning and sharing.\nDiscussing the concept of creating a home in relation to Lebanese Residential Environments\nDuring the tour of Hamlin Home of the Evangelical Church in Hammana, perched at a 900m altitude 35 km east of Beirut, I was observing the desire of Sanaa Koreh, director of the community, and Reverend Suheil Saoud, Pastor of the community, to change the perception and stigma of the elder care industry in Lebanon. Hamlin even bears a double stigma as it was founded by American missionaries as a tuberculosis hospital in the late 1800s. The building benefits from 4m-wide glazed corridors leading to the individual bedrooms and an uninterrupted view to the nearby mountains and agricultural fields. With little means available for capital improvements, Sanaa has made a huge impact on the renovated floors. The airy light curtains filter a beautiful mid-morning light into this newly furnished veranda-corridor. The glazed double bedroom doors were naturally held open allowing for air movement and natural light penetration, and residents can come in and out as they please. It was renovated to reflect the mountain living of Lebanon at the time of the Grand Hotels that were dispersed in the Lebanese mountains. These structures were built in concrete and stone, had marble or mosaic (terrazzo) floors and featured open airy living spaces. They were summer homes for Beirutis of the beginning and mid-20th century who were escaping to the mountains to avoid the heat of the city.\nLeft: Hamlin’s first building, erected in the late 1800s, to care for tuberculosis patients. Right: Hamlin courtyard.\nLeft:Preparing our lunch at Hamlin. Right: The view from Hamlin.\nThe Veranda at Hamlin outside residents’ bedrooms\nAIN WAZEIN MEDICAL VILLAGE\nThe previous day, we visited a different model of elderly care, Ain Wazein Medical Village. The community adorns a hilltop in the Chouf Mountains of Lebanon, 48 km from Beirut, in the heart of the Druze Community. When we first walked into the main lobby, after the initial conversations with its general manager Dr. Zuheir al Murad, we were told that the mission of Ain Wazein Medical Village expands beyond its geographical boundaries and its immediate Druze Community. It is an organization that aims to serve the elderly of Lebanon, its sick and its handicapped.\nI was then reminded of the old Chinese proverb, recited by Gandhi and later Hubert Humphrey.\n“The measure of a civilization is a reflection of how it treats its weakest members, those who are in the dawn of life, the children; those who are in the twilight of life, the elderly; those who are in the shadows of life; the sick, the needy and the handicapped”\nAin Wazein started as an elderly home and expanded during the war time into a hospital serving its immediate community. With a vision beyond its boundaries, it developed a medical continuum of care from primary to acute care. Its mission to care for the elderly is still guiding its evolution through its expansion. It has on staff three gerontologists (30% of Lebanon’s gerontologists, no joke), one of them being the hospital’s primary care physician. The community has completed phase one of its expansion of elderly housing wings, with two floors of skilled nursing beds arranged around a light well/courtyard, following the medical model. I met Waseem, a resident, who showed me her room, a photo of her long past mother and introduced me to her roommate. What struck me most about Ain Wazein was an energy that captivated me as soon as we arrived. It was the enthusiasm of all the people–staff and management of the organization–wanting to serve the elderly resident population in the absolute best way they can; this attitude begins with their welcoming mission, and extends to their desire to always do better and to the respect they have for the all the residents.\nWell equipped OT/PT room at Ain Wazein\nLeft: Organizers (Joyce Eid, David Reimer, Amal Rasamny) with Presenters (Nadeem Abi Antoun, Philippe Saad, Rick Stiffney) by a millennial cedar tree at the Chouf Cedar Reserve. Photo credit Julie Pimentel, Celeste/Daniels Advertising and Design. Right: Chouf Cedar Reserve.\nMOADIEH EVANGELICAL CENTER\nWe did not formally visit Moadieh Evangelical Center but it was a meeting point for all the group’s activities. I took the initiative to walk around one morning when I arrived early to the tour. The center houses 30+ bedrooms and three one-bedroom suites. The center is a reflection of Joyce’s hard work to provide a home for the residents she serves. The building’s small floor plate accommodates a household per-floor, with a living room shared by seven residents. The top floor has a larger living room with TV and a dining room. The spaces are furnished as urban Beirut homes, with a contemporary feel.\nMoadieh Evangelical Center Living Room\nUnderstanding the importance of creating a strong bond amongst the members of the Interfaith Eldercare Group, and respecting the varied traditions of religious groups represented, some being more conservative than others, Joyce Eid purposely invited spouses on the trip. On the last day of the conference, a leisure excursion was scheduled to tour the millennial city of Byblos followed by a traditional meze meal by the Mediterranean – It was in Byblos that the alphabet was said to have been invented, a common pride of all factions of Lebanese society. Throughout the trip, but in particular on this last day, I observed the jovial and friendly atmosphere amongst all attendees. Trust continued to be built amongst one another; sincere interest in the other’s struggles and successes was growing; and above all there was a noticeable desire to continue investing in this journey together. The group agreed that the next steps would be to become a legal entity in order to have a louder voice to advocate for change with concerned lawmakers and legislative bodies, and to work together as an organization.\nI was humbled in this last trip to Lebanon, as I was asked to serve my native country. I saw aspects of the Lebanese society that I had forgotten and was reminded that caring is a universal skill and duty. During my presence at the Interfaith Eldercare Group Conference, I shared what I know, but most importantly I learned about the energy among the members of the group and the desire of the different religious organizations to collaborate. In America, collaboration is the backbone amongst the not-for-profit senior care providers, through organizations like LeadingAge. In Lebanon, collaboration is rarely part of any business, marketplace or strategy of organizations serving similar targets, let alone an interfaith collaboration.\nThe millennial archaeological site of Byblos"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:44b7c39f-2447-4f13-a2ee-7fc148cb7667>","<urn:uuid:ae92a227-766c-4cbe-9d72-9807f9049478>"],"error":null}
{"question":"What role does evidence-based practice play in managing mass incidents, and how has this evolved with modern threats?","answer":"Evidence-based practice in managing mass incidents has evolved significantly. For mass sociogenic illness, experience shows that symptoms typically resolve quickly on their own, but early recognition and proper handling are crucial to save resources and prevent escalation. The poison control center can help coordinate care when victims are transported to different hospitals. Regarding modern threats, the first responder community now has decades of response and trauma data that forms the foundation for evidence-based best practices. This has led to the development of integrated response programs that incorporate lessons learned from various incidents worldwide. Communities can adopt these established practices rather than 'reinventing the wheel,' with model programs available for different types and sizes of jurisdictions.","context":["Mass sociogenic illness: Itís real, yet it isnít\nBy Anne Louise Bannon\nThe call comes in: possible gas leak at a high school, two teachers and about 15 students presenting with nausea, vomiting, headache and difficulty breathing. The hazmat team is on the scene, and 12 more students and another teacher start exhibiting symptoms.\nAs more and more people start getting sick, the media shows. Rumors are starting about chemical warfare and terrorist attack. Or there’s something terribly wrong with the building that the administration has refused to do anything about. And more women and girls are getting sick.\nExcept that, way in the back of the school building, a group of students in the auto shop are working away, perfectly well and oblivious to the chaos in the rest of the school. Which is the second clue that there is not, in fact, a gas leak. That there isn’t even a physical cause for the illness striking the school.\nThe incident described above happened in Tennessee in 2000, explains Timothy Jones, M.D., deputy epidemiologist for the State of Tennessee. At the time he investigated the case, he was an epidemic intelligence officer for the Centers for Disease Control and Prevention.\n“Off in the auto shop, where it was noisy, ano one got sick,” Jones says. “If it was a toxic gas, it’s going to get everyone. It doesn’t prove anything one way or another, but it can be a clue.”\nDos and don’ts for emergency responders\nThis sidebar accompanied an article by Robert E. Bartholomew on mass psychogenic illness, published in the March 2006 issue of Crisis Response Journal\nIt’s a little-studied phenomenon that used to be called mass hysteria (and still is in some circles), in which a group of people suddenly become ill, believing they’ve been exposed to a toxin or disease, even though there’s no evidence that such an exposure has occurred.\nSchools and workplaces tend to be the most common locations for such outbreaks, but there are records of entire cities feeling the effects. Symptoms can mimic panic attacks and generally include nausea, vomiting, dizziness, headache and hyperventilating. The incidents come on very suddenly, and the symptoms usually resolve themselves within an hour or two.\nBut because there hasn’t been a lot of study on the phenomenon, there isn’t really a single name for it. It’s referred to as epidemic hysteria, mass psychogenic illness or mass sociogenic illness, with the latter two terms becoming more common.\nPaula Madrid, Psy.D., a clinical psychologist at Columbia University who is currently in Mississippi working with victims of Hurricane Katrina, says the confusion stems from the fact that not a lot is known about the syndrome. “There are different ways of looking at epidemic hysteria, because it’s not very widely studied. It’s not in the diagnostic books. But it can impact large numbers of people very quickly.”\nA long history\nSociologist Robert E. Bartholomew, formerly of James Cook University in Queensland, Australia, has written more than most about the subject and says interest is growing.\n“Mass hysteria is not as widely studied as say, PTSD [post-traumatic stress disorder], but it is becoming an increasingly scrutinized subject, especially in light of recent terror attacks and scares and the potential for mass disruption caused by social delusions.”\nAnd in fact, many historical cases of mass psychogenic illness have occurred in times not unlike ours, in terms of social upheaval, nervousness due to an active war, not to mention economic uncertainty.\nIncidents do not appear to be on the rise, however. Jawaid Akhtar, M.D., an emergency physician and medical toxicologist at the Pittsburgh Poison Center, as well as the University of Pittsburgh Medical Center Presbyterian Hospital, says he hasn’t seen an increase in cases.\n“We hear about it once or twice a year through the poison center.”\n“I have not noticed an increased number of reports, but no one keeps track of them,” says Jones. He notes that during the anthrax scares right after Sept. 11, 2001, there would be incidents at post offices, where employees became sick after handling suspicious packages, and cites another case that occurred during the first Gulf War. “It’s sort of hard to say it’s going up or down.”\nThat said, it’s probably far more common that most people think.\n“If you talk about it to a room full of public health people, they all know about it,” Jones says.\nMadrid even mentions that she may have had a psychogenic reaction recently while living in a trailer home in hurricane-ravaged Mississippi. Public concerns about the safety of the tap water in the area had her wondering if she really had a stomach ache from drinking it.\nResponse and treatment\nBecause the only way to make a diagnosis of mass sociogenic illness is to exclude all other causes, and because there is always the possibility that there is a physical cause for an incident, first responders must investigate the incident right away, which in turn, usually makes the situation worse.\n“You’re damned if you do and damned if you don’t, in terms of the actions you take,” Bartholomew says. “You have to assume a worst-case scenario in the initial assessment of any situation.”\nUnfortunately, part of what’s making the people sick is seeing fire-fighters and police tearing around in gas masks, looking for leaks.\nAnother problem for EMTs called to an incident is that, with rare exceptions, the worst thing they can do is insist that the victims are reacting to a psychological stimulus and will be all right as soon as they calm down. People simply do not like being told that their symptoms are all in their heads.\n“It’s a very difficult thing to explain,” Jones says, partly because patients really are feeling dizzy and nauseous, and their hearts really are racing. “They’re not faking them. It’s a true physiological response.”\nIt’s just that the cause is not what they think it is.\n“They feel like they’re being told, ‘I have a psychiatric disease,’” Jones says.\nThe irony, he points out, is that people who do have psychiatric diseases, such as schizophrenia and bipolar disorder, don’t tend to fall prey to mass psychogenic illness.\nThe good news is that the symptoms themselves almost always resolve very quickly on their own. But that in itself creates problems for first responders on a couple of levels.\n“It takes away a lot of resources,” Akhtar says, and not only among police, fire fighters and emergency departments. “The place where it happens closes down. School hours are lost. Work hours are lost.”\nAnd while there is no evidence this has happened, because events can and do recur, the possibility exists that later events may not be investigated as thoroughly.\n“I think that can go either way,” Jones says. “Either they blow off something that turned out to be real, or the other fear is that after it happens one or two or three times, every time school is back in session, people are going to be so nervous and scared, it’s going to happen again.”\nAwareness among first responders is probably the best antidote.\n“Early recognition helps save resources,” says Akhtar, which is why he recommends contacting a poison control center early on, especially if victims are transported to several different hospitals. “It can help coordinate the care, if we’re getting four to five calls about the same thing.”\nJones says that while all calls must be handled as if there is a true physical threat, having the possibility of sociogenic illness in the back of people’s minds would help. So, for example, first responders notice that victims are getting sick within minutes of each other, that most of the victims are female, that victims start getting better when they’re separated from other victims and are given oxygen.\n“If all those things are going for you, I would hope that it would be in the minds of the providers,” Jones says.\nThe tricky part is in not labeling the syndrome, as such, while still being reassuring and caring.\n“How do you respond?” asks Madrid. “You try to calm people down, help people to feel safe. People will react in a more positive way if they feel that they’re being take care of. There’s something in the placebo effect.”\nBartholomew also insists that EMTs, when interviewing patients, not volunteer any symptoms, as that almost always leads to those symptoms showing up.\n“First responders need to treat the symptoms and be sympathetic, even when it becomes obvious that it was mass hysteria,” he says. “I think a better-tolerated term to use would be anxiety response or reaction, as it would tend to reduce the stigma associated with ‘mass hysteria.’”\nIn fact, some have suggested that even “psychogenic illness” can produce patient backlash and are leaning toward using “sociogenic illness.”\nBut no matter what the phenomenon is called, it is a very real event, and it calls for very real processes to help victims and the surrounding community heal.","As the recent events in Paris so tragically demonstrate, we continue to face ongoing threats in an uncertain world. Active shooter events, the use of improvised explosive devices (IEDs), and the threat of complex attacks like those seen in Beslan, Russia; Mumbai, India; and now, Paris, France must be considered as at least plausible, if not probable.\nA pre-planned, integrated response by all first responder disciplines is required in order to maximize effectiveness and improve the survivability of those injured in such attacks. Some of the considered actions may seem contrary to those responders indoctrinated in the time-honored doctrine of “scene safety trumps all”. The truth is that the first responder community now has decades of response and trauma data to be used as the foundation for evidence-based best practices.\nMany communities have already established integrated response programs, and there is no reason for any jurisdiction to “reinvent the wheel” from scratch. While there is no “one-size fits all” solution to the challenge of integrated response, there are already model communities of different sizes and compositions. From rural to urban, volunteer to paid, and everything in between – somewhere, in a system a lot like yours, someone has already done much of the ground work. And the benefits of integrated response extend well beyond those realized during the rare terror attack. Relationships between police, fire, and EMS agencies that are formed (and enhanced) during pre-planning and training will pay great dividends during all requests for service from routine calls to natural disasters.\nBelow are just some of the many resources that already exist to help guide those communities who have decided to pursue a more prepared community through integrated response.\nIntegrated response (culture change):\nFirst Responder Guide for Improving Survivability in Improvised Explosive Device and/or Active Shooter Incidents\nFire/Emergency Medical Services Department Operational Considerations and Guide for Active Shooter and Mass Casualty Incidents\nImproving Active Shooter/Hostile Event Response: Best Practices and Recommendations for Integrating Law Enforcement, Fire, and EMS (Includes a list of technical experts who may provide best practices)\nA Study of Active Shooter Incidents in the United States Between 2000 and 2013\nActive Shooter Study: Quick Reference Guide\nHemorrhage Control (Early care saves lives)\nSee Something, Do Something: Improving Survival – Strategies to Enhance Survival in Active Shooter and Intentional Mass Casualty Events: A Compendium\nAn Evidence-Based Prehospital Guideline for External Hemorrhage Control: American College of Surgeons Committee on Trauma\nTourniquet and Hemostatic Gauze Training (includes CoTCCC equipment recommendations)\nPrevailing Response Models and Concepts:\nAdvanced Law Enforcement Rapid Response Training\nTactical Emergency Casualty Care\nImproving Active Shooter/Hostile Event Response: Best Practices and Recommendations for Integrating Law Enforcement, Fire, and EMS\nTactical Combat Casualty Care\nBystander Preparedness and Response:\nStop the Bleed (DHS)\nStop the Bleed (DoD)\nBleeding Control for the Injured (B-Con)\nPurchase of Ballistic Protective Equipment (BPE) for Fire and Emergency Medical Services (EMS) Personnel in Support of Active Shooter and Mass Casualty Incidents (AS/MCIs)\nU.S. Department of Justice Bulletproof Vest Partnership\nFederal Bureau of Investigation – Active Shooter Incidents\nNAEMT Courses – B-Con/LEFR-TCC/TCCC/TECC\nE912: Preparing Communities for a Complex Coordinated Attack – IEMC: Community Specific\nTo view the full DHS OHA one pager, please click here: http://www.interagencyboard.org/system/files/resources/Active%20Shooter%..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:05086ae6-d25d-49f8-b9b1-0b481a8905fe>","<urn:uuid:9e617547-5ef5-456f-ba3b-da8405521c50>"],"error":null}
{"question":"When was Godshead Rock first established and why was it given this name?","answer":"Verdial travellers established Godshead Rock in 8480 AR, when they reached the top of what was then known as the Greenknot Plinth. The name was chosen because the location resembled a large statue head when approached from the direction of the Eastern Tesseract.","context":["Godshead Rock is a Vale Verdial city-state located in sunny (yet cold) Caudal B. Known to many as the 'Intercostia of the Vale Periphery,' Godshead Rock serves as a free trade zone, resupply station, and neutral ground at the doorstep of the War of Reunification's Caudal Front.\nDespite the geography of the region, Godshead Rock is home to a permanent population of around 3,000 people. Most of the permanent population is Vale Verdial in extraction, though there is a sizeable minority of verdialized outsiders as a result of local family ties or other long-term residency. Like every region with a large verdial presence, Forgism is the most common religious affiliation in Godshead Peak, though the Church of the Unexpected maintains a temple devoted to Cosmeon there.\nGuilds and Factions\nWhile Godshead Rock is nominally a direct democracy, three factions wield outsized influence within the city: the Petalcap Vale Customs Authority the Navigator's Guild, and MartMart International. The PVCA maintains the skystation PVCA North in the lnflection layer which leads to Medial C. Therefore, the PVCA exercises a degree of oversight over airship travel between the Verdial Arc and Human Arc, the two regions being adjacent to one another over Godshead Rock. While the PVCA generally does not exercise favoritism, mostly performing inspections to ensure that the two desmenses do not cross-contaminate one another with invasive species or hazardous waste, they have the power to shutter all skyports in Godshead Rock should an emergency of this nature arise. Because inter-tesseract travel generally requires the services of trained navigators, the Guild both maintains a guild hall in Godshead Rock and maintains a field office aboard PVCA North to facilitate connecting flights. Many navigators, wary of living in a warring state or in the many lawless port towns which dot the Manifold (such as Intercostia or Eusiormapu), chose to live in Godshead Rock for their own safety and prosperity. As such, while the Guild maintains it's much-vaunted political neutrality, the large number of its employees which also hold Godshead Rock citizenship mean that the Guild's ideology carries enormous clout at city hall. MartMart International, beyond having a successful location in Godshead Rock, also makes judicious use of the city-state's free trade policies to pass normally grey- or black-market goods on to locations throughout the region. If a mechanic in Craterhold wants to get his fix on Colonade, for example, the soda he ultimately buys at his friendly local MartMart will likely have passed through Godshead Rock at some point along the way. This rankles regulators both in the Coalition of Breakaway Colonies and in Voxelia, but neither organization is aggravated enough by this brazen war profiteering to risk starting an unneccesary front in the War with the as-of-yet untested military of the Vale.\nVerdial travellers reached the top of the rock (then known the Greenknot Plinth) in 8480 AR and established the settlement of Godshead Rock. The name was chosen because of the location's resemblance for a large statue head when approached from the direction of the Eastern Tesseract. The top of the rock maintained a persistent colony of constantly-stressed Penrose fescue which, in addition to helping maintain the shape and topsoil cover of the rock, also served as a source of hearty flour for these early settlers. Raised high and isolated from the rest of Vale society, Godshead Rock had an insular population of no more than 150 people until the arrival of Eqai Voiranoi in 9858 AR ushered in an era of trade between Petalcap Vale and the rest of the Manifold Sky. Early airships lacked the range of larger, modern airships after crossing an inflection layer, making waystations valuable sources of trade and, therefore, revenue for the communities which supported them. in 9860 AR, Godshead Rock acquired a small fuel biosynthesis plant and a mooring tower for airships making the Caudal B/Medial C transit. Over time, the rest of the community grew in connection with this plant, with farming moved to the surrounding lowlying plains and most of the rock's 'scalp' becoming covered with residences, businesses, and a small network of roads. Eventually, as the city continued to expand, residents were forced to build multi-story structures, with skywalks between these structures facilitating the increase in foot traffic. A rail line was laid between the municipality of Petalcap Vale and Godshead Rock, with the rail depot on the Godshead end of the line being hewn into the the 'head's nape'; goods and people are still hoisted onto and off of the rock via large cranes rather than direct road access, however, as the locals seem to prefer isolation from 'groundly' affairs. The last major alteration to Godshead Rock's infrastructure occurred in the year 9910 AR, where a series of air raid bunkers and underground passages were chiseled into the rock in response to the War of Reunificaiton happening just beyond the inflection layer above; time will tell if these will ever be needed or, if so, will prove effective in the event of bombardment.\nAs a major waypoint on the route to Petalcap Vale, a geologically and architecturally unique location, and an independent city just out of reach of wartime violence, Godshead Rock is a very popular airship tourism destination. During the summer peak season, there may be as many as one tourist or other traveller in town for every three permanent residents. Voluntarist verdials often visit Godshead as a way to immerse themselves in the culture (and Caudal lichen spores) without having to endure the questioning glares of the illiberal citizens of the Vale's core.\nBuildings in Godshead rock tend to be tall and narrow, with dark gray stone brick walls, high-peaked roofs designed to slough off snow, and numerous skywalks defining the local architectural style. A perimeter wall, known colloquially as 'Creator's Crown,' encircles the city proper, completing the vaguely medieval aesthetic - despite the fact that the latticed airship mooring spires are also visible from anywhere in town. The locals tend to decorate their homes and businesses with local wood and wrought iron, prepetuating the medieval feel in part because it draws the eyes of tourists and in part because it feels natural to them through long practice. Notably, the city hall (where local refferenda are written and voted upon), main airship mooring point, and PVCA air traffic control occupy the same large tower in the center of town.\nGodshead Rock sits atop large, rougly cylindrical rock structure around a mile high and wide. This geological formation is believed to have been formed by ancient erosive forces, where the looser soil and debris at the foot of a long-extinct volcano were blown away, exposing the sheer sides of the solid stone core. the top of the structure is rounded and some portions of the sheer rock faces have broken away over time, creating a shape vaguely like that of a human head when viewed from certain angles. The top of the rock structure supports an isolated alpine ecosystem, with the surrounding hills and meadows being notably more green in the growing season than the rock itself.\nWhile the old Penrose fescue fields have been reduced to parks and other green spaces within the Godshead Rock municipality, the region does have another exploitable natural resource: crystal. Because the rock itself was once a column of lava which cooled over an extended period of time, the insides of the 'skull' are filled with large quartz crystals and deposits of semi-precious stones. Mining efforts are ongoing, but must proceed with caution so as not to weaken the city's foundations. In general, vertical shafts are immediately filled with new construction, with the largest near the city's center converted into an agri-mine."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4e4aaf46-a970-443b-aade-50a06952293d>"],"error":null}
{"question":"请问科学方法的基本定义是什么？Could you explain the fundamental definition of the scientific method?","answer":"The scientific method is a series of processes that people can use to gather knowledge about the world around them, improve that knowledge, and attempt to explain why and/or how things occur.","context":["What takes humans one step beyond this simple empirical knowledge-building. The best formal system that we have ever devised for this is the scientific method, which operates based on a combination.\nThey might not undergo the rigors of the scientific method, the argument goes. They talk about all the precautions and steps they take before performing their analysis. It can sound impressive —.\nScientific Method Definition. The scientific method is a series of processes that people can use to gather knowledge about the world around them, improve that knowledge, and, through gaining knowledge, attempt to explain why and/or how things occur.\nTo make the most of your approaching winter break, follow these three AP-focused steps. Review sample responses for open. For instance, the scientific method would be one such core concept for any.\nAug 24, 2015. Various activities can be a problem that must be solved through scientific methods. What kind of steps that must be taken away? Following the.\nAug 10, 2011. ART AND THE SCIENTIFIC METHOD OF EXPERIMENTING I believe. GENERAL STEPS FOR EXPERIMENTATION: Guidelines for artists\nThe scientific method does not work for past events; the forensic scientific method. Rather, scores of scientists line up and step forward with varying and diverse.\nStudent Answers. Hypothesis: based on the results from step 6, propose a solution to the problem. Verification: challenge the validity of your hypothesis. Conclusions: if your hypothesis is correct go to step 10, if partially wrong go to step 5 and try another solution or if totally wrong examine step 2-4 and take another path. Present.\nPivoting is easy when the workflows are manual but once automation steps in, you have already invested a lot. Related: How I Built a Company the Lean Way — by Using the Scientific Method The.\nTHE SCIENTIFIC METHOD. The scientific method is a systematic way of studying a problem. Use of this method is not limited solely to scientists. In fact, you have probably used the scientific method from time to time even though you were not aware of it. We can divide the scientific method into five broad categories of activity. Identifying a.\nYou’ll have scientific method steps free printables, scientific method for kids videos and a worksheet to help save you time in gathering resources and making them kid-friendly. With different ways to present the method for kids, you’ll be teaching science with more ease.\nAug 30, 2018 · Kids question the world around them every day and there is so much to learn through experimentation with simple materials! You can absolutely begin Using scientific method with young kids! Below we’ll share with you how and when to introduce using the scientific method.\nThe next step, however — one absolutely required by the scientific method — is to apply your equation to a fresh sample to see whether it actually works [.] But Gottman never did that. Each paper he.\nQuotes from Supporting Organizations: “The Scientific Integrity Act is an important step forward for safeguarding scientific. threat of substituting political science for the scientific method. Our.\nOrganism Job In Its Environment Career Opportunities. What is Marine Biology? AmosTexTerrapin web Marine biology is the study of marine organisms, their behaviors and interactions. Researchers apply molecular techniques to many environments ranging from coastal. So the important point here is not to assume that just because organisms have autonomous capacities that they don’t rely on their environment for their\nWhat is the first step of the scientific method? Apply this step to the scenario involving Thomas’s keys. 10 points The first step in scientific method would be observation. He realized his keys are missing, then asked himself “ Where are my keys? ” What is the second step of the scientific method? Apply this step to the scenario involving Thomas’s keys. 10 points Ask myself where did.\nTheory basics. Any scientific theory must be based on a careful and rational examination of the facts. Facts and theories are two different things. In the scientific method, there is a clear distinction between facts, which can be observed and/or measured, and theories, which are scientists’ explanations and interpretations of the facts.\nU Of R Neuroscience Major University of Notre Dame, College of Science, undergraduate science education, research, Neuroscience is a relatively young, exciting, and fundamentally. For a good chunk of her sophomore season at Roger Williams University, the Ashland-native was sidelined. first round of the Commonwealth Coast Conference Tournament in Bristol, R.I. A season prior. Cancer was the major contributor. and\nDec 20, 2018 · Steps of The Scientific Method. Create a step by step procedure and conduct an experiment that tests your hypothesis. The experiment should be a fair test that changes only one variable at a time while keeping everything else the same. Repeat the experiment a number of times to ensure your original results weren’t an accident.\n\"There’s a broader problem with the scientific method that’s being increasingly acknowledged, and the test we’ve developed can at least play a small role, and I hope a big role, in addressing it.\".\nThey’re building wacky machines, debunking viral science stories and testing clever ideas to revisit concepts such as the scientific method, without the boring. He follows the steps outlined in the.\nThis was a step removed from the modern scientific method, which relied on sophisticated experiments. More than a century later, there is little reason for us Indians to harbour an inferiority complex.\nSo they peer review it and say it’s a study, they don’t apply the scientific method. Even the Republicans who might. or wax rhapsodic about “beautiful clean coal” than to seriously consider steps.\nWhere Do Forensic Entomologist Work Forensic toxicologists work in laboratories, often those operated by government agencies or law enforcement, to identify chemicals and compounds that could have contributed to crimes or have other administrative or legal consequences. Medical examiners, also referred to as forensic pathologists, are medical doctors who perform autopsies and other investigations to determine a cause of death.\nThe high-level steps that make up our process. At this point, you might be thinking “This process sounds like the Scientific Method!” There is a good reason for this. It’s because it represents the.\nThe scientific method is a five-step process used in scientific investigation. Sometimes the steps are combined or added to, but the five are the basic structure for any endeavor to answer a.\nTheir method will now be published in its entirety in the scientific journal Nature Protocols. The method, which researchers Tuulia Hyötyläinen and Matej Orešič have worked on and developed for more.\nThe scientific method provides scientists with a well structured scientific platform to help find the. Method: A list of the steps followed to carry out the experiment.\nPeople for the Ethical Treatment of Animals (PETA) India also advised the government to adopt only humane and scientific methods to protect crops and villages or to translocate the animal to another.\nTry it a few more times with successively bigger bites, maybe add a bit of open-fire cooking; you’ve got a scientific method going. He’s human experiment. is to be done with the work of researchers.\nFor instance, a poem can help break down the steps of the scientific method or a complicated math concept, like the Fibonacci sequence, in an easy, digestible format. Michelle Parisi, a sixth-grade.\nSteps of the Scientific Method Detailed Help for Each Step; Ask a Question: The scientific method starts when you ask a question about something that you observe: How, What, When, Who, Which, Why, or Where? For a science fair project some teachers require that the question be something you can measure, preferably with a number.\nThis research is an important early step towards improving the robustness of AI systems. We are committed to the scientific method and the integrity of our work. AI may indeed transform nearly.\nNov 13, 2015. The study of scientific method is the attempt to discern the activities by. as a fixed four or five step procedure starting from observations and.\nMar 17, 2019 · What is the scientific method and how is it used in psychology? The scientific method is essentially a step-by-step process that researchers can follow to determine if there is some type of relationships between two or more variables.\nThe scientific method is the process by which science is carried out. As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.\nMar 29, 2019 · The scientific method is the backbone of all rigorous scientific inquiry. A set of techniques and principles designed to advance scientific research and further the accumulation of knowledge, the scientific method has been gradually developed and honed by everyone from the philosophers of ancient Greece to the scientists of today.\nand they did not go through (which is far and away the most securely scientific method to accurately determine the value of a trade, you know?). Harry was highly productive in college; he showed a.\nScientific Method Steps Made Simple Scientific language is often needed to explain natural phenomena as accurately as possible. Unfortunately, this can also.\nThe scientific method is a set of ordered steps that scientists follow in order to solve a particular problem. The first step is to conduct a background research on the topic and make observations.\nScience and math series Following the Scientific Method. Observe * Research * Hypothesize * Test * Conclude. The scientific method is a process for forming and testing solutions to problems,\nWhat is method acting? Method acting is a technique where actors. The study provides the first steps towards a scientific understanding of the neuroscience behind acting and role-playing, and it’s.\nThus, the scientific method is not a linear process of steps, but a method of inductive reasoning. Introduction In terms of science, the scientific method is a process used to step through the task of examining patterns in the world, forming a hypothesis that explain these patterns, and then gathering data to test the hypothesis.\nSome of the most important debates in the history of scientific method center on: rationalism, especially as advocated by René Descartes; inductivism, which rose to particular prominence with Isaac Newton and his followers; and hypothetico-deductivism, which came to the fore in the early 19th century.\nUnderstanding and Using The Scientific Method. The Scientific Method is a process used to design and perform experiments. It’s important to minimize experimental errors and bias, and increase confidence in the accuracy of your results.\nThe processes or steps which are taken in scientific methods are specific in nature, mentions of which are as follows: 1. The first step consist of arousing a sense of need in the mind of learner to make use of scientific method to solve out the existing problems with which he is being confronted."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:25e9886a-de9f-46bb-b5f3-54e090c5f781>"],"error":null}
{"question":"I'm studying poetry and anthropology - which published poet also worked as an anthropologist - Wanggo Gallaga or Nathaniel Tarn?","answer":"Nathaniel Tarn worked as an anthropologist. He studied anthropology at various institutions including the Musée de l'Homme, did doctoral fieldwork in Guatemala, and became a Lecturer in South East Asian Anthropology at the School of Oriental and African Studies, University of London from 1960-1967. While Wanggo Gallaga is a poet, there is no indication in the documents that he worked as an anthropologist.","context":["Review by John Toledo\nLast night, I received in my message box an e-mail from a guy named “wanggo.”\nInside was the free copy of his poetry collection, Remnants. I remembered e-mailing him a day ago about the possibility of reviewing his poems. I thought how nice he is to trust me in reading his exclusive collection in epub format (for he never sold it on print but on Flipreads, Amazon, Kobo, Apple iTunes, Google Play, and Barnes & Noble).\nWho is Wanggo Gallaga?\nIn December 2008, he was known as the guy who disclosed his HIV status to the public. He’s the son of director Peque Gallaga and he’s known as one of the revered and popular spoken word poets in today’s generation. He was a judge of the “Scarlet Letters from Baguio” which is AIDS Society of the Philippines’ HIV slam poetry competition in Mt. Cloud Bookshop, Baguio. He has written the films Sonata and T’yanak. His poems have been published in some of the prestigious publishers like Philippine Free Press, Panaroma, The Philippine Graphic, Anvil Press, and Dagda Publishing.\nWanggo experienced near death experiences with the virus twice (in 2008 and 2010). He shares in the “Introduction” that his poems are not explicitly about HIV, but “It’s about the living aspect of Person Living with HIV (PLHIV). It’s the questions that nobody asks in the forums. It’s the stuff that is only discussed with close friends and family, and sometimes, not even.”\nBefore reading his collection, I only knew Wanggo in a YouTube video. He was sharing his poetics in a forum about children’s poetry. I would never have thought this tall, dark, and handsome guy on the screen is definitely an expert in the craft of restraint. The poetry in Remnants range in what I would imagine to be his life’s work, which as he mentioned were compilations of his poems since 1999.\nThe collection opens with a mood of grief and distraught. Yet, as one submerges in the depth of words, there is a transition from darkness to light. It reminds me of Elizabeth Kübler-Ross’s Stages of Grief: Denial and Isolation, Anger, Bargaining, Depression, and Acceptance.\nMeanwhile, in Wanggo’s style, the arrangement is divided into themes of pain, suffering, bargaining, hope, and acceptance as signified by each sections of his life, “Navigating Loss”, “Disillusionment”, “The Threshold”, “Delirious Script”, “Grace”, and “Remnants”. Wanggo’s poems considered as remnants or small quantities of his memories are magnanimous in vision as it approached and represented the microscopic and mundane experiences of a Person Living with HIV.\nTake for instance the microscopic details of dried leaves in “Dry”,\ncrackle as they break\nunderneath my left shoe\nand the weight of my left leg.\nAll of nature is so brittle\nwhen it is dry and bereft\nIt waits for rain;\nall things wait for rain\nThe rain resonates with what is not said. The dried leaves signify brittleness and how such dryness yearn for moisture, a certain kind of wetness that reveals aspirations, traumas, and anguish.\nAs I read through the collection, I never restrained from crying in moments where the bittersweet lyrical self speaks of a situation that near the comforts of our own beds. We become home in Wanggo’s unhomeliness. In “Shape”, for instance, the pain and struggle of the body to curve is make sense in the understanding of loss. In the twists and turns of his positions, we understand the shape of tension and the bitter reality that our bodies may also articulate our sense of loss,\nIt is in the curve of my back\nwhen I lie in bed;\nit is in the way my knees\nmeet my chest and how my hands\nalmost touch my feet\nas I reach out for that end of me.\nThis twisted form that I make\nas I try to sleep\nhas no resemblance\nto anything in nature.\nIt is there in the tension in my spine.\nIt is there in the almost-breaking of it.\nIt is there in my open mouth\nand the difficulty of my breathing.\nIt is there in how sleep evades me.\nIt is there.\nCan you not see it?\nBetween the trembling\nand the convulsions\nand the loss of breath,\nEvery night, it is there,\nthese symptoms of loss.\nThis is the new shape\nthat you gave to me\non that night I never saw coming\nand the words that were said\nthat shattered bone\nand punctured hearts.\nThe architecture of our bodies\nis drawn by what we choose\nto define us.\nLosing a person or being left behind is a universal experience that when captured in the voice of a poet is heard as a shout that shatters. Like any PLHIV, Wanggo also experienced the grief of loving or losing, while confronted by the complex battle of his body with the virus, especially with the fits of symptoms that has the possibility of worsening in the future. I have never read a poem yet, especially in Ladlad anthology, which captures this sharpness and twistedness of a conflicted body, the experience that only Wanggo can articulate.\nThere are lots of witty and metaphorical poems in Remnants that also caught my attention. In fact, the strength of the whole collection is not in how it showed a life of a PLHIV in poetry but in its craft of restraint, of wrapping through the seams and cloths of metaphor, his insights about love and loss.\nIn the case of “Humpty Dumpty”, he used the egg as metaphor to the what he called the “stupid” self when at the moment he fell,\nAnd all the king’s horses and all the king’s men…\nWell they came to try and put him together again.\nAnd boy did they come! They came in droves.\nThey came from places never heard or seen before.\nThey came from unexpected places.\nSome were forgotten faces.\nBut they couldn’t. They didn’t know how.\nHumpty wouldn’t let them in. Humpty wouldn’t let them close.\nHumpty didn’t think his yoke was made of gold.\nHumpty thought everything had to be bought and sold.\nHe didn’t know how to get the thing he needed most.\nHis fierce vigilant tones of the lyrical “I” is common in the collection. His sarcasm strikes as witty when he commented on the demise of Humpty Dumpty as a stupid egg, who “Had to fall and break wholly apart./ All because he didn’t know how to use his heart.” Was this the poet blaming himself? We can only surmise.\nThe poems in Remnants were strategically arranged to espouse a theme, some of which might have gone as far as 1999 when he was still a Literature student practicing his craft. The latter poems in the collection have transitioned to a certain kind of “Grace” and his acceptance of these in the case of the poem “Remnants”.\nI believe that “The Garden” is one of his strongest poems in the batch. In “The Garden”, he recollects the mundane experience of nature. His concreteness is crisp that the message itself bleeds even in the first image of daybreak. He likens his awakening from grief to a lush garden and at this moment of illumination, he lets go of the burdens that comes with the weight of his identity,\nEnter the morning sun.\nI opened my eyes and saw the garden.\nThis garden, I did not know was in my keeping,\nhad grown wild and verdurous.\nAll these wonders — blades of grass,\nferns, shrubs, bushes, flowers, trees –\nturn their bodies towards the sun\nin a graceful, poised arabesque.\nDancers celebrating the brilliance of life.\nThey did not just spring from the ground.\nThey live so that I can breathe,\nso that I have something to see,\nso that I have something to nurture,\nand something to emulate.\nThis garden I must have tended\nand then I must have forgotten\nstill yields a lush and verdant bounty\nthat knows no price and has a value\nthat is immeasurable.\nThis is why the morning has come,\nthis is why I wake and why I leave\nthe safe confines of my room.\nThis garden has taught me how to live:\nTo choose fertile ground,\nto pull the weeds\nfrom the earth by hand,\nto take root,\nand then be unafraid\nto bask in the glory\nof the sun.\nThis is more than revelation. It is joy that is found in nature, and might be at par with English and American Romantic poets who valued nature and the freedom of the individual. He is the Bard of the PLHIV. But more than that, he is the poet that speaks of hope and freedom. The weeds, roots, earth, and glories of the sun, reminds us not to be afraid. There is life even as an HIV-positive.\nDefinitely, Wanggo Gallaga is one of the new poets of this age to voice out the unbearable lightness of his once “heavy” being. He realizes in “Grace” that there is an end to darkness, distraught, quietness, fear, and resentment. All of these recollections of his “memories of being trapped there are nothing more than just memories. They become stories you tell and they no longer define you.”\nWanggo’s defiance with the conventions of HIV-AIDS poetry (as compared to the distraught poems of Rafael Campo, Thomas Dunn, and Mark Doty who used the material realities and experiences of AIDS patients as their subjects) proves that there is always a rainbow after the rain. Commonly, stereotypes of brokenness, hopelessness, and wasted-ness are attributed to PLHIV. Wanggo subverts the norm and creates a fresh autobiographical poem, which at one point is memoir and in another, a forgetting.\nI quote Prof. J. Neil Garcia who once mentioned about the absence of HIV-AIDS as themes in Ladlad, “It would take individual efforts by Filipino gay writers (some of whom are HIV-positive, or “Pozzie Pinoys,” themselves) over the next few years to produce the Philippines’s own version of “AIDS literature,” which can only be formally and epistemologically different from what existed in the West when the then-lethal epidemic was in full swing.”1\nWanggo’s work is a beginning of that dream, of that version that can reclaim the Filipino experience of what it means to be HIV-AIDS stricken in our stigma-infested country, confronted by corruption, oppression, and discrimination.\nI suggest first time readers of Remnants to explore and read the poems in its entirety. You might drown in the sea of his metaphors but let him lead you there, for these memories give that avenue of diving deeper into the heart of his soul.\nThere is hope for AIDS Literature in the Philippines. We need more poets like Wanggo. Remnants shows that PLHIVs recover from shock through acceptance, forgetting, wisdom, and strength.\nShare his ideas not for the sake of Wanggo but for the sublime power of his words. This is the unbearable lightness of a struggling being. For the end of grief is acceptance, we might one day also say like Wanggo,\nWe leave remnants of things to be picked up in the future;\nor we leave it there and never pick it up again.\nWe are not beholden to singular moments, or remnants\nof the good old days.\n1 Garcia, J. Neil. “Introduction: Saving the Best for Last.” The Best of Ladlad: An Anthology of Philippine Gay Writing. J. Neil Garcia and Danton Remoto, eds. Mandaluyong: Anvil Publishing, 2014.","Nathaniel Tarn (born 1928 in Paris) is an American poet, essayist, translator and anthropologist.\nNathaniel Tarn was born in Paris, 1928, lived there until age 7, then in Belgium (Lycée d’Anvers) until age 11. He was educated at Clifton College, U.K. then, as a scholar of King’s College, at Cambridge where he studied History and English. He returned to Paris and, after some journalism and radio work, discovered anthropology at the Musée de l’Homme, the Ecole des Hautes Etudes and the Collège de France. A Fulbright a grant took him to Yale and the University of Chicago where Robert Redfield sent him to Guatemala for his doctoral fieldwork (1951-2). He completed this work as a graduate student at the London School of Economics (1953-8).\nIn 1958, a grant from the Rockefeller Foundation administered by the Royal Institute of International Affairs sent him to Burma for 18 months after which he became Lecturer in South East Asian Anthropology at the School of Oriental and African Studies, University of London (1960-1967).\nTarn published his first volume of poetry \"Old Savage/Young City\" with Jonathan Cape, London in 1964; a translation of Pablo Neruda’s “The Heights of Macchu Picchu” in 1968 and began building a new poetry program at Cape. He left anthropology in 1967. From 1967-9, he joined Cape as General Editor of the international series Cape Editions and as a Founding Director of the Cape-Goliard Press, specializing in contemporary American Poetry with emphasis on Charles Olson, Robert Duncan, Louis Zukofsky and their peers and successors. He brought a great many French, other European and Latin American titles to Cape and made many visits to the U.S. as a Cape Editor. He taught English at S.U.N.Y. Buffalo in the summer of 1969.\nIn 1970, with a principal interest in the American literary scene, he immigrated to the U.S. as Visiting Professor of Romance Languages, Princeton University, and eventually became a citizen. Later he moved to Rutgers. Since then he has taught English and American Literature, Epic Poetry, Folklore etc. etc. at inter alia the Universities of Pennsylvania, Colorado, New Mexico, Manchuria (PRC), reading and lecturing all over he world: Paris, Heidelberg, Freiburg, Berlin, Rome, Messina, Prague, Budapest, Sydney, Melbourne, etc. etc. He has set foot in every state of the U.S., with especially long study in Alaska. Extensive travels over the years in all seven continents has informed his poetry from the start.\nAs poet, literary & cultural critic (two volumes: “Views from the Weaving Mountain” and “The Embattled Lyric”), translator (he was the first to render Victor Segalen’s “Stèles” into English, continued work on Neruda, Latin American and French poets) and editor (with many magazines), Tarn has published some thirty books and booklets in his various disciplines. He has been translated into some ten foreign languages. His poetry possesses a remarkable range of voice and reference, fusing archaic myth with contemporary concerns and moving from complex hieratic visions to the deeply personal.\nIn 1985, he took early retirement as Professor Emeritus of Poetry, Comparative Literature & Anthropology from Rutgers University and has since lived some twenty minutes N.W of Santa Fe, New Mexico. His interests range from bird watching, gardening, classical music, opera & ballet, and much varied collecting, all the way over to aviation and world history.\n- Old Savage/Young City. London, Cape, 1964; New York, Random House, 1966\n- Penguin Modern Poets. London, Penguin Books, 1966\n- Where Babylon Ends. London, Cape Goliard Press, and New York, Grossman, 1968.\n- The Beautiful Contradictions. London, Cape Goliard Press, 1969; New York, Random House, 1970.\n- October: A Sequence of Ten Poems Followed by Requiem Pro Duabus Filiis Israel. London, Trigram Press, 1969.\n- The Silence. Milan, M'Arte, 1970.\n- A Nowhere for Vallejo: Choices, October. New York, Random House, 1971; London, Cape, 1972.\n- The Persephones. Santa Barbara, California, Tree, 1974.\n- Lyrics for the Bride of God. New York, New Directions, and London, Cape, 1975.\n- The House of Leaves. Santa Barbara, California, Black Sparrow Press, 1976.\n- From Alashka: The Ground of Our Great Admiration of Nature. With Janet Rodney. London, Permanent Press, 1977 .\n- The Microcosm. Milwaukee, Membrane Press. 1977.\n- Birdscapes, with Seaside. Santa Barbara, California, Black Sparrow Press, 1978.\n- The Forest. With Janet Rodney. Mount Horeb, Wisconsin, Perishable Press, 1978.\n- Atitlan / Alashka: New and Selected Poems, the *Alashka* with Janet Rodney. Boulder, Colorado, Brillig Works Press, 1979.\n- Weekends in Mexico. London, Oxus Press, 1982.\n- The Desert Mothers. Grenada, Mississippi, Salt Works Press, 1984.\n- At the Western Gates. Santa Fe, Tooth of Time Press, 1985.\n- Palenque: Selected Poems 1972-1984. London, Oasis/Shearsman Press, 1986.\n- Seeing America First. Minneapolis, Coffee House Press, 1989.\n- The Mothers of Matagalpa. London, Oasis Press, 1989.\n- Drafts For: The Army Has Announced That From Now On Body Bags Will Be Known As \"Human Remains Pouches\" . Parkdale, OR, Trout Creek Press, 1992.\n- Flying the Body. Los Angeles, Arundel Press, 1993\n- A Multitude of One: The Poems of Natasha Tarn. (N.T. Editor). New York, Grenfell Press, 1994.\n- I Think This May Be Eden, a CD with music by Billy Panda. Nashville & Small Press Distributors, 1997.\n- The Architextures: 1988-1994. Tucson, Chax Press, 2000.\n- Three Letters from the City: the St. Petersburg Poems. Santa Fe, The Weaselsleeves Press and St. Petersburg, Borey Art Center, 2001.\n- Selected Poems: 1950-2000. Middletown, Wesleyan University Press, 2002.\n- Recollections of Being. Cambridge and Sydney, Salt Publishing, 2004.\n- Avia: A Poem of International Air Combat, 1939-1945. Exeter, Shearsman Books, 2008.\n- Ins and Outs of the Forest Rivers. New York, New Directions, 2008.\n- Stelae, by Victor Segalen, Santa Barbara, Unicorn Press, 1963 .\n- The Heights of Macchu Picchu, by Pablo Neruda. London, Cape, 1966.\n- Con Cuba: An Anthology of Cuban Poetry of the Last Sixty Years. London, Cape Goliard Press, 1969.\n- Selected Poems: A Bilingual Edition, by Pablo Neruda. London, Cape, 1970.\n- Pablo Neruda: Selected Poems. London, Penguin Books, 1975 .\nCRITICISM & ANTHROPOLOGY\n- Views from the Weaving Mountain; Selected Essays in Poetics & Anthropology. Albuquerque, University of New Mexico Press, 1991.\n- Scandals in the House of Birds: Priests & Shamans in Santiago Atitlan, Guatemala, Guatemala, New York, Marsilio Publishers, 1997.\n- The Embattled Lyric; Essays & Conversations in Poetics & Anthropology, with a biographical & bibliographical essay by, and a conversation with, Shamoon Zamir. Stanford, Stanford University Press, 2007.\n- Roberto Sanesi in Le Belle Contradizzioni, Milan, Munt Press, 1973\n- \"Nathaniel Tarn Symposium\" in Boundary 2 (Binghamton, NY.), Fall 1975\n- \"The House of Leaves\" by A. Kingsley Weatherhead, in Credences 4 (Kent, OH.), 1977\n- Ted Enslin and Rochelle Ratner, in American Book Review 2 (New York, NY), 5, 1980\n- Translating Neruda by John Felstiner, Stanford, Stanford University Press, 1980\n- \"America as Desired: Nathaniel Tarn's Poetry of the Outsider as Insider\" by Doris Sommer, in American Poetry I (Albuquerque, NM. ), 4, 1984\n- \"II Mito come Metalinguaggio nella Poesia de Nathaniel Tarn\" by Fedora Giordano, in Letteratura d'America (Rome), 5(22), 1984.\n- George Economou, in Sulfur (Ypsilanti, MI.), 14, 1985.\n- Gene Frumkin, in Artspace (Albuquerque. NM.), 10(l), 1985.\n- Lee Bartlett, in Talking Poetry, Albuquerque, University of New Mexico Press, 1987\n- \"The Sun Is But a Morning Star\" by Lee Bartlett, in Studies in West Coast Poetry and Poetics, Albuquerque, University of New Mexico Press, 1989.\n- “An Aviary of Tarns” by Eliot Weinberger, in Written Reaction, New York, Marsilio Publishing, 1996\n- Shamoon Zamir: \"Bringing the World to Little England: Cape Editions, Cape Goliard and Poetry in the Sixties. An Interview with Nathaniel Tarn. With an afterword by Tom Raworth,\" in E.S. Shaffer, ed., Comparative Criticism, vol 19: \"Literary Devolution.\" Cambridge, UK: Cambridge University Press, pp.263-286, 1997.\n- Shamoon Zamir: \"On Anthropology & Poetry: an Interview with Nathaniel Tarn,\" Boxkite, no.1, Sydney, Australia, 1998.\n- Shamoon Zamir: \"Scandals in the House of Anthropology: notes towards a reading of Nathaniel Tarn\" in Cross Cultural Poetics, no.5, (Minneapolis, MN.), 1999, pp.99-122.\n- Brenda Hillman: Review of “Selected Poems” in Jacket magazine #28, (internet) Sydney, Australia, 1999.\n- Joseph Donahue: Review of “The Architextures” First Intensity #16, 2001 (Lawrence, KS).\n- Peter O’Leary: Review of “The Architectures” in XCP Cross Cultural Poetics #12, 2003 (Minneapolis, MN).\n- Martin Anderson: Review of “Recollections of Being” in Jacket magazine #36, (internet) Sydney, Australia, 2008.\n- Daniel Bouchard: Conversation with NT, in Zoland Poetry #3, 2009 (Hanover, NH): Steerforth Press, 2009.\n- Isobel Armstrong: Review of “Avia” in Tears in the Fence #50, Blanford Forum, Dorset, UK, 2009.\n- Joseph Donahue: review of “Ins & Outs of the Forest Rivers” in “A Nathaniel Tarn Tribute”: Jacket magazine #39, (internet) Sydney, Australia, 2010.\n- Richard Deming: Essay on “The Embattled Lyric” & “Selected Poems” in “A Nathaniel Tarn Tribute”: Jacket magazine #39 (internet) Sydney, Australia, 2010.\n- Lisa Raphals: Reading NT’s “House of Leaves” in “A Nathaniel Tarn Tribute”: Jacket #39 (internet) Sydney, Australia, 2010.\n- Nathaniel Tarn entry: Stanford University Libraries, Stanford California \n- Nathaniel Tarn \n- “Review: Scandals in the House of Birds”. Zamir Shamoon (1998). \n- “Flight from ‘The Ten Thousand Things’ ”. Martin Anderson (2008). \n- The Embattled Lyric: Essays and Conversations in Poetics and Anthropology. \n- Nathaniel Tarn: Avia. \n- ‘Ins and Outs of the Forest Rivers’ (Review). Joseph Donahue (2009). \n- A Tarn poem \n- Lee Bartlett: Nathaniel Tarn: a Descriptive Bibliography. Jefferson, NC and London, McFarland, 1987. \n- Zamir, Shamoon(ed.) A Multitude of One: Celebrating Nathaniel Tarn \n- New Directions Publishing Corp., publisher of Tarn’s “Ins and Outs of the Forest Rivers \n- Nathaniel Tarn papers, ca.1939-2005  call numberM1132.) are housed in the Department of Special Collections and University Archives  at Stanford University Libraries.\nCategory:1928 births Category:Living people Category:Alumni of the University of Cambridge Category:American poets Category:American people of French descent Category:American people of Romanian descent Category:American people of British descent Category:University of Chicago alumni Category:University of Paris alumni\n|This article about an anthropologist is a stub. You can help Wikipedia by expanding it.|\n|This American poet–related article is a stub. You can help Wikipedia by expanding it.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:99d05bc6-1c34-4761-be9e-701023a553be>","<urn:uuid:46279376-4066-4fd4-b7fa-1921e174ecd7>"],"error":null}
{"question":"What are the benefits of parallel processing in computing systems, and what security vulnerabilities arise when implementing parallel processing in cloud environments? 🤔","answer":"Parallel processing offers several benefits: it allows multiple tasks to be processed simultaneously through CPU multi-threading (with modern CPUs having twice as many threads as cores) and enables distributed computing across multiple machines, which can significantly reduce processing time. Specific applications include matrix multiplication, training multiple models simultaneously, and efficient parameter iteration. However, when implemented in cloud environments, this same parallel processing through virtualization technology creates security vulnerabilities. Virtual machines splitting physical servers means other organizations could potentially access stored data. This risk requires implementation of robust security measures like end-to-end encryption, secure data transfers using SSL, and protection against DDoS attacks through multi-layered security controls.","context":["Summary: Data can be processed in parallel by using multiple threads on a single CPU or by passing code to the data in systems like the Hadoop Distributed File System.\nImagine you’re driving your car on the way to work. You keep an eye on the road ahead, the side mirrors, the rearview mirror, and even what’s on the radio.\nYou’re changing the channel on the radio and you like the song but not sure if you want to stay on this station. Suddenly, a car pulls up from behind you and moves into your blind spot. While the music still plays in the background, you see him start veering into your lane. You deftly avoid a mild collision and, after a second to compose yourself, you go back to scanning for a good song on the radio.\nA Single CPU with Multiple Threads\nThe CPU is the brain of your computer. It does all of the processing from running your operating system to doing calculations for your deep neural network. All of this processing is done by executing commands at a mind-numbingly fast pace.\nHowever, a single CPU needs to get its data to process from somewhere. A CPU looks for data in a few places. In order from last to first…\n- Hard Drive: Slow to access, last to search\n- RAM: Faster to access\n- CPU Cache: Fastest to access, first to seearch\nIf the CPU can’t find the data it needs in the cache, it moves up the storage hierarchy. As a result, your processing stops as the data is being pulled. A multithreaded CPU, which most modern chips are, can multitask while waiting.\nWhen a task is not running due to waiting for data, a new task can be started and ran until it completes or needs data from outside the cache. Usually you have twice as many threads as the number of cores on your CPU.\nBack to the driving example, you really can only do one thing at a time while driving. You can look ahead, to the side, or behind you or you can start scanning the radio for a good song. We do these things very quickly, but for milliseconds, your attention has to be focused on only one of these things.\nThe same idea applies to CPU tasks. One task stops, the other one picks up where it left off. As another car pulls out from behind you, your attention is diverted to that car. Once that care passes you (safely) your attention returns to the road ahead of you.\nYour attention likely flits back and forth between the car passing you and the road ahead of you. You can think of this as your brain waiting for more data and thus turning its attention to the next task that’s ready.\nBack to the technical details…\nMy personal PC has 6 cores but 12 threads. This allows me to do an incredible amount of processing in a short amount of time. However, if the algorithm or process isn’t designed to be done across multiple cores, I’m stuck with running on a single thread…\nParallel Processing Across Multiple Machines\nWhen you involve multiple machines you can cut the processing time down. Instead of relying on one machine and maybe the multi threading capability, you can distribute the work across many machines.\nThe Hadoop Distributed File System (HDFS) is an example where you are sending code to multiple machines. This differs from what a normal PC file system does.\n- The CPU is essentially the driver\n- A regular PC will move data from disk to cache\n- Then the CPU continues its program with the new data\nDistributed computing instead has lots of data stored on multiple machines and the processing instructions are sent to those machines. This decreases the amount of time spent moving data around.\nThe only trick is that each computer in HDFS only has a part of the data and thus needs to eventually send their data (hopefully aggregated in some way) across the network to be further refined (or reduced) into what the user wants.\nIntuitively, letting more machines, which have more CPUs, do the work makes sense. If you have so much data and have a lot of processing needs, you can add more machines to potentially speed up the processing.\nSome examples of useful applications of distributed computing:\n- Matrix Multiplication: Classic example. Take chunks of a matrix A and multiply it by the corresponding parts of matrix B. This creates smaller chunks of data and you can combine them on one machine.\n- Training Multiple Models: If you’re building an ensemble, you can build one model on one machine and another on the next. Then, once they’re all trained, you can bring them back to one machine for prediction.\n- Iterating Through Lots of Settings: If you need to brute force your way through something, like trying hundreds of different parameters in an algorithm, you can send the task to multiple machines and it will be obviously faster than building the models one at a time.\nSome examples where adding more machines won’t be much help:\n- Calculate the Median (or any specific percentile): In order to calculate the median, you need to sort the data and find the middle value. This requires you to have all the data on one machine and know the order.\n- Working with many small files: If you’re working with thousands of small files and doing some light processing of the data, having multiple machines might create more work than necessary.\nOne of the biggest challenges in parallel processing is that some algorithms need to be changed. Think about a decision tree. In a parallel world, you don’t have access to all the data on one machine. So how do you determine the best split?\n- Each machine gets a representative sample of the data.\n- Each machine evaluates the possible splits on that training set.\n- Bring all the possible best decisions together on one machine.\n- Evaluate which of these finalist decisions is the best.\n- Repeat the process with the updated decision tree.\nIf you want a really detailed understanding of the decision tree implementation in Spark, take a look at this presentation (slides and video links at the bottom).\nSometimes distributed algorithms have to make smart shortcuts because of the distributed nature of the problem.\nDespite some limitations, parallel processing can be done on a wide variety of problems. It just takes more thought on how the algorithm will be implemented. As the user of these algorithms, you should be aware of any shortcuts or nuances that make it different than a single-machine implementation.\nFor those who want to learn more about adapting machine learning algorithms to work in parallel, try looking at the textbook Mining of Massive Datasets.","There are two sides to every coin, and nowhere is that more evident than on your cloud server. The exact same attributes that make cloud storage servers so cost-effective and convenient also make them much more difficult to secure.\nIn the cloud, maintenance and upgrade management falls entirely to your cloud hosting provider. On one hand, this frees up internal resources, meaning less time (and money) is spent on routine maintenance. On the other hand, you don’t have control over server configurations and upgrades, which could spell trouble where your business security is concerned.\nSimilarly, the same virtualization technology that allows you to rapidly deploy new cloud servers also puts you at risk for data breaches and other cybercriminal events. In cloud computing, physical servers are split into virtual machines, which means other organizations could theoretically access the data stored on them—especially if your provider is lax with security controls.\nThese issues highlight just how diligently business IT support teams must work to secure cloud storage servers and databases. Beating hackers at their own game requires robust, thorough security strategies like those described below.\nEnd-to-end data encryption\nImplementing encryption solutions is one of the most crucial moves you can make to protect your data from leaks and ransomware attacks. Most cloud storage services automatically encrypt data while in transit; however, once that data is saved to your cloud server, there’s no guarantee that it’s secure. And even if a third-party cloud storage service does encrypt the data stored on its servers, the company ultimately holds the key to that information, not you. In other words, if that third party is compromised, your data could go down with it.\nThese facts make a good case for implementing some kind of encryption method before you store data in the cloud, but regardless of whether you use a cloud storage service or have a separate cloud environment, protecting your data with encryption software offers security against brute-force attacks, data leaks and ransomware. Your business IT support provider should be able to help you design an encryption technique for your cloud storage solutions—for instance, at MyITpros, we offer whole-disk encryption for our cloud services.\nSecure data transfers\nKeep in mind that data is not only at risk when it’s sitting on cloud storage servers, it’s also vulnerable when in transit (i.e. while being uploaded, downloaded or moved on your server). Although most cloud service providers encrypt data transfers as a rule, this is not always a given.\nTo ensure data is protected while on the move, make certain that transfers go through secure HTTP access and are encrypted using SSL. Your business IT support provider should be able to help you obtain an SSL certificate and configure your cloud service to use it. You may also want to install HTTPS Everywhere on all devices that connect to your cloud.\nLocal data backups\nThe cloud often lulls business owners into a false sense of security where data integrity is concerned. After all, if one of the main cloud benefits is that your data is backed up automatically, there’s no need to save it locally, right?\nNot necessarily. Hackers know that many businesses don’t save data locally, and they exploit this to their advantage when they launch ransomware attacks. Without local backups, you might feel pressure to surrender large sums of money to get your data back.\nHowever, the FBI recommends that you don’t pay ransoms. Not only is there no guarantee that hackers will play fair and return your data (these are criminals, after all), paying up could brand you as an easy target and make you more likely to be hacked in the future. Backing up your data locally will give you the confidence you need to refute a ransomer’s fee.\nDistributed denial-of-service protections\nYou may already be familiar with distributed denial-of-service (DDoS) attacks in which a hacker drowns your server, website or application with multiple requests for data, essentially rendering it useless. Cybercriminals have been launching denial-of-service attacks for at least 20 years, but today’s access to bots and IoT devices makes it even easier for hackers to coordinate the attacks through multiple systems, hence the term “distributed.”\nYou can fight back by building multi-layered protections into your networks and systems. Using web application firewalls, intrusion protection systems (IPS), load balancers and other tools, you’ll be able to better detect and prevent DDoS attacks and handle high-volume requests that might otherwise paralyze your network. A good managed service provider will be intimately familiar with these controls and able to add them to your infrastructure and servers.\nHackers are constantly at work, which means your IT support provider must be, too. To keep your company’s data, applications, websites and networks secure, you’ll need to stay one step ahead of cybercriminals.\nEssentially, this boils down to identifying system weaknesses before they cost you. You may want to engage your business IT support team in performing a vulnerability assessment, which will involve your provider testing your cloud storage networks to locate weaknesses that may allow hackers in and then developing a plan to address these. Performing these assessments regularly keeps your networks like a shark: always swimming.\nAn unfortunate truth about security threats is that they don’t necessarily come from outside a business. Internal team members with access to sensitive data on the cloud can wreak havoc in their own right, whether they take the form of disgruntled employees stealing confidential data for malicious purposes or negligent personnel accidentally exposing information housed in the cloud.\nRole-based access controls (RBACs) help you regulate the access given to various employees, allowing you to designate which servers and files individual users can open, edit or copy. Administrators can assign roles and privileges to users based on need, authority and the employee’s position within the company. Implementing these controls may also be required to stay compliant with statutory and regulatory requirements for your industry.\nSo what if there are two sides to the cloud computing coin? With the right cloud security controls, the odds will be in your favor every time!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7e2f1818-2150-47bd-a218-0c1bac032b6d>","<urn:uuid:7485cb1f-5a98-4332-a04e-ffa03d3c8686>"],"error":null}
{"question":"¿Cuál es la diferencia principal entre fuerzas de tensión y compresión in structural analysis?","answer":"Tensile forces act outwards and tend to extend the member, and are assigned a positive sign. Compressive forces act inwards and tend to compress the member, and are assigned a negative sign. Although the opposite sign convention can be used with equal validity, the same convention must be maintained throughout the analysis of a structure.","context":["- slide 1 of 5\nStructures are made to support loads. Structure is an assembly of number of members arranged in certain manner. When load acts on a structure this load is distributed to the constituent members of the structure in different proportions. Members experiencing large forces can be made stronger, members experiencing less force can be made lighter and redundant members with no force to support can be removed altogether. Thus static force analysis of structures can help to build cost effective, light and strong structures.\n- slide 2 of 5\nForce acting on a member of a structure can be compressible or tensile. For the purpose of force analysis a sign convention can be assigned to the forces. Tensile forces, acting outwards the members and having a tendency to extend the member, is assigned positive sign. The force acting inwards any member and tending to compress the member is called as compressive force and assigned negative sign. Although a sign convention opposite to this one can also be followed with equal validity but the same sign convention should be adhered to throughout the analysis of a structure.\nThere are mainly two approaches for static force analysis in structures. One approach is to section the structure under consideration and find the unknown forces by balancing the forces. Other approach is based on the principle that net force at any joint or node for static structure is zero. In either of the approaches force calculation is started from the support points as it is easier to determine the forces at the support points and further calculation of forces in the members of the structure becomes easier after knowing the forces at supports.\n- slide 3 of 5\nIn the Section Approach the structure under consideration is sectioned at certain part such that the number of unknown forces is not more than two, for two dimensional structures. Unknown forces are assigned variables and components of the forces are taken along and perpendicular to any one of the unknown forces. For each of the two directions force balance equations are framed and solved for the unknowns. The components of the forces can also be taken along any fixed coordinate axis. For three dimensional structures the section taken can have up to three unknown forces.\n- slide 4 of 5\nIn the other approach, to find forces in the members of a structure, net force at any joint is set to zero. Any joint connecting two or more members can be called as a node. One by one different nodes are considered for force analysis. To start with such a node will be taken which has not more than two unknown forces. Unknown forces are determined by writing net forces along any set of orthogonal axes and equating them to zero.\n- slide 5 of 5\nFor a simple structure one of the two approaches may be sufficient to determine the forces in members. But for complex structures single force analysis approach can become cumbersome for force analysis, therefore, a tricky combination and use of the two approaches can simplify the static force analysis in structures.\nForce Analysis in Statics in Engineering\nA bicycle moves down the street and carry us with a little effort on our part. Water falls from the height of dam and run large turbines. How these machines work? How the different bodies interact? Engineering Mechanics, the study of forces and motion of bodies in mechanisms, answers these questions"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:8e980f80-cf16-4be9-9cac-4bb166f80bc3>"],"error":null}
{"question":"What's the difference between ground water and surface water treatment requirements, and what role do flocculants play in each case?","answer":"Ground water systems often require minimal or no treatment to meet federal requirements, sometimes needing only chlorine addition, while surface water systems require more extensive treatment due to their exposure to land runoff and atmospheric contamination. Flocculants, which can be either inorganic (iron and aluminum preparation series) or organic (including synthetic polymers like polyacrylamide and natural organic polymers), play a crucial role particularly in surface water treatment. They are used in the flocculation/sedimentation stage of the treatment process, where they combine small particles into larger ones that can settle out of the water, helping to remove contaminants and prepare the water for subsequent treatment steps like filtration and disinfection.","context":["How to choose flocculant in wastewater treatment?\nHow to choose a flocculant in wastewater treatment should be selected according to the characteristics of the wastewater in a specific industry, and at the same time, it also depends on where the flocculant is added and what purpose it is used for.\nOverview and Applications of Bisacrylamide\nBisacrylamide is a chemical substance with a molecular formula of C7H10N2O2. Bisacrylamide has two identical and very active reactive functional groups. It can be polymerized with a variety of ionic monomers to prepare ion exchange resins, and is polymerized with nitrogen and sodium acetate. In addition, it can also be used in medical, cosmetics and paper processing fields, and it has a wide range of uses.\nThe application of microbial flocculants in actual water treatment\nThe main component of microbial flocculants is microbial cells or their secondary metabolites. It has the characteristics of wide application range, high flocculation activity, safety, harmlessness, and easy biodegradability, so it can be widely used in the treatment of water supply, sewage and wastewater.\nPoly(Dimethyl diallyl ammonium Chloride) has good dewatering performance for activated sludge\nPoly(Dimethyl diallyl ammonium Chloride) is a cationic organic polymer with dimethyldiallylammonium chloride as the main body. It has good water solubility, and the aqueous solution is neutral, It has good dewatering performance for activated sludge.\nApplication of PDMDAAC-Bentonite in the Treatment of Reactive Dye Wastewater\nPDMDAAC bentonite prepared by modifying bentonite with Poly(Dimethyl diallyl ammonium Chloride) has greatly improved its adsorption performance compared with the original soil. The decolorization rate of printing and dyeing wastewater is slightly higher than that of activated carbon, but the treatment time is far less than that of activated carbon.\nSolvent effects of siloxanes on donor–acceptor interactions\nResearchers at Kanazawa University report in Chemical Communications how solvents influence the strength of donor–acceptor interactions. They found that silicone solvents, providing low compatibility, intensify donor–acceptor interactions between aromatic molecules compared to hydrocarbon solvents.\nFactors affecting the flocculation effect of microbial flocculants\nThe degree of treatment of water supply, sewage, and wastewater mainly depends on the flocculation characteristics of the microbial flocculant. Therefore, a better flocculation effect can be achieved through certain treatment conditions.\nResearchers have made new progress in the research of smart multicolor fluorescent polymer hydrogels\nThis research work preliminarily proves the feasibility of using the biomimetic multilayer core-shell structure to prepare smart multicolor fluorescent polymer hydrogels, which is expected to play an important role in the application of seafood freshness detection, information security and encryption, and flexible discoloration skin.\nPrecautions for the use of polyacrylamide\n1. The size of the flocs, 2. The characteristics of the sludge, 3. The ionicity of polyacrylamide, 4. The dissolution of polyacrylamide, 5. When using a composite flocculant, you must pay attention to the order of addition and the time interval of dosing .\nAIE-active Fluorescent Polymeric Hydrogel Emerged as Research Hotspot\nBased on previous findings on AIE-active FPHs, Prof. CHEN Tao and colleagues at the Ningbo Institute of Materials Technology and Engineering (NIMTE) of the Chinese Academy of Sciences (CAS), systematically summarized the recent progress in this young but flourishing research area, with particular focus on the design and preparation of AIE-active FPHs. The study was published in Aggregate.\nWhat are the application fields of polyacrylamide？\nPolyacrylamide applications include: water treatment, sewage treatment and industrial water treatment; oil field chemical treatment agents, widely used as retention aids, filter aids, and leveling agents in the papermaking field; and used in medical materials; food industry, etc.\nFactors affecting the viscosity of polyacrylamide\nThe viscosity of polyacrylamide solution mainly reflects the internal frictional resistance between liquid molecules due to flow or relative movement. The internal friction resistance is related to the structure of the polymer, the nature of the solvent, the concentration of the solution, temperature and pressure and other factors.\nCharacteristics and applications of polyacrylamide\nPolyacrylamide (PAM) is a linear polymer with the chemical formula (C3H5NO)n. Polyacrylamide has a wide range of applications in industries such as petroleum exploration, water treatment, textiles, papermaking, mineral processing, medicine, and agriculture.\nFixing mechanism of fixing agent\nThe cationic group in the colour-fixing agent molecule and the anionic group in the dyestuff molecule are used to form an ionic bond to form a deposit on the fiber, thus reducing the water-solubility of the dyestuff and improving the color fastness of the dyed fabric.\nCommon types and choices of reactive dye fixing agents\nThe fixing agent can improve the washing fastness of the dye and resist the bad influence of the atmosphere on the dye. Common types of fixing agent: metal salt fixing agent, cationic fixing agent, reactive fixing agent.\nPolyacrylamide, a widely used flocculant in water treatment\nFlocculants are widely used in water treatment. They are synthetic polyacrylamide series products, mainly divided into anionic, cationic, nonionic and zwitterionic. Polyacrylamide (polyacrylamide), often abbreviated as PAM.\nMain Types of organic polymer flocculants\nMain Types of organic polymer flocculants: 1. It is made based on natural polymer organic matter and chemical treatment to increase its active group content. 2. Polyacrylamide series products synthesized by modern organic chemical methods. 3. It is made by grafting (or copolymerizing) natural raw materials and polyacrylamide.\nMain categories and characteristics of inorganic flocculants\nMainly divided into two categories: iron preparation series and aluminum preparation series, of course, also includes its cluster of high polymer series.\nBrief introduction and main classification of flocculants\nAccording to its chemical composition, flocculants can be divided into inorganic flocculants and organic flocculants. Among them, inorganic flocculant includes inorganic coagulant and inorganic polymer flocculant; organic flocculant includes synthetic organic polymer flocculant, natural organic polymer flocculant and microbial flocculant.","Treating Raw Water\nThe amount and type of treatment applied by a public water system (PWS) varies with the source type and quality. Many ground water systems can satisfy all federal requirements without applying any treatment, while others need to add chlorine or additional treatment. The U.S. Environmental Protection Agency (EPA) is developing a ground water rule that will specify the appropriate use of disinfection and will address other components of ground water systems to assure public health protection. Because surface water systems are exposed to and fed by direct land runoff and to the atmosphere, they are therefore more easily subjected to contamination. Federal and state regulations require that these systems treat this water to meet health-based standards.\nDisinfection of drinking water is one of the major public health advances of the 20th century. However, the disinfectants themselves can react with naturally occurring materials in the water to form unintended byproducts which may pose health risks. A major challenge for water suppliers is balancing the risks from microbial pathogens and disinfection byproducts. The EPA’s Stage 1 Disinfectants and Disinfection Byproducts Rule and the Interim Enhanced Surface Water Treatment Rule together address these risks.\nWater suppliers use a variety of treatment processes to remove contaminants from drinking water. These individual processes may be arranged in a “treatment train” (a series of processes applied in sequence).\nThe most commonly used treatment processes include filtration, flocculation and sedimentation, and disinfection for surface water. Some treatment trains also include ion exchange and adsorption. Water utilities select a combination of treatment processes most appropriate to treat the contaminants found in the raw water used by the public water system.\nTypes of Treatment\nFlocculation/Sedimentation Flocculation refers to water treatment processes that combine or coagulates small particles into larger particles which settle out of the water as sediment. Alum and iron salts, or synthetic organic polymers (used alone or in combination with metal salts), are generally used to promote coagulation. Settling or sedimentation occurs naturally as flocculated particles settle out of the water.\nFiltration Many water treatment facilities use filtration to remove or reduce suspended solids and particles from the water. Those particles include clays and silts, natural organic matter, precipitates from other treatment processes in the facility, iron and manganese, and microorganisms. Filtration clarifies water and enhances the effectiveness of disinfection.\nIon Exchange Ion exchange processes are used to remove inorganic contaminants if they cannot be removed adequately by filtration or sedimentation. Ion exchange can be used to treat hard water. It can also be used to remove arsenic, chromium, excess fluoride, nitrates, radium and uranium.\nAbsorption Organic contaminants, unwanted coloring, and taste and-odor-causing compounds can stick to the surface of granular or powder activated carbon and are thus removed or reduced from the drinking water.\nDisinfection (chlorination/ozonation) Tap water is often disinfected before it enters the distribution system to ensure that potentially dangerous microbes are killed. Chlorine, chloramines, or chlorine dioxide are most often used because they are a cost-effective approach to disinfection where public distribution pipelines provide water to our homes and businesses. Ozone is a powerful and very effective disinfectant, while ultraviolet radiation is an effective disinfectant and treatment for relatively clean source or processed waters that are typical in bottled water processing. Neither of these is effective in controlling biological contaminants in the distribution pipes of public water systems.\nBottled Water Treatment\nNatural water (bottled water derived from protected ground water sources) is minimally treated via filtration and sanitization (ozonation or UV) only, if necessary. The addition of any chemicals or the application of any other techniques that would alter the natural balance of minerals is not allowed.\nFor processed or purified waters, a purification process is applied, in addition to the filtration and sanitation. This purification process typically consists of distillation or reverse osmosis. Both techniques are designed to further purify the water in order to provide the desired high-value beverage."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:a8253484-48f7-4e9b-a5c2-7fc71bf34d99>","<urn:uuid:9ba8d726-cf15-4d3b-9dbb-a9a333a53f6c>"],"error":null}
{"question":"What is the purpose of primer in UV vacuum coating, and which pipe colors should be avoided for outdoor use?","answer":"The primer in UV vacuum coating serves to close the plastic substrate, prevent volatile substances during plating, create a smooth mirror effect, improve adhesion properties, and act as a thermal buffer. For outdoor use, orange pipes should be avoided as they show poor UV resistance and tend to match yellow tones after exposure. Red pipes are especially unsuitable for outdoor use or near large windows/doors as they demonstrate very low UV resistance and undergo the most drastic color changes.","context":["UV metal coating\nUV vacuum electroplating coating is mainly composed of UV curing resin, monomer, photoinitiator, pigment, leveling agent, adhesion promoter, etc. According to the division of application surface, it is mainly composed of UV vacuum electroplating primer and UV vacuum electroplating topcoat. In order to match the modern new electroplating technology, some of them also have vacuum electroplating coating.\nThe effect of primer UV vacuum coating coating primer is coated on the surface of the plastic substrate coating, its role is to close the plastic substrate, to prevent the volatile substances in the substrate during vacuum plating affect the quality of the coating.Plastic surface is rough, through the primer coating can obtain smooth smooth mirror effect, is conducive to obtain a thicker and uniform vacuum metal coating, show higher metallic luster and reflection effect.For some substrates with lower polarity, better adhesion properties can be obtained by vacuum electroplating of metal coating through primer transition.The primer also ACTS as a thermal buffer for some plastic substrates with poor heat resistance.\nPrimer coating primer UV vacuum plating of the above, requires a primer must be on the plastic base material and vacuum coating metal layer, good adhesion, is both good and weak polarity plastic coating substrate adhesion is good, and has good adhesion with polarity metal, the plastic coating is more demanding than simple, also indirectly proves that most of the UV vacuum plating resin are same or higher than applicable to simple UV plastic coatings.\nAt the same time, in order to guarantee the high gloss and mirror effect of the metal coating, the primer must have excellent leveling performance.In addition, the primer curing should not have volatile small molecules, otherwise it will destroy the quality and performance of the metal coating, so the primer formula of macromolecular quantization is an important consideration.For some of the heat resistant base material, UV primer with its thermal performance is also very important, such as heat resistance of BMC requirements up to 180 °.In addition, in order to prevent coating cracking or jumping off, the system flexibility of primer also appears particularly important.\nPrimer of formula design according to the performance requirements of the UV vacuum coating primer, primer formula on choosing oligomer to fully consider the flexibility, adhesion, flowing property and molecular weight, so the need to give attention to two or more things, generally choose two or three faculties of polyurethane acrylic resin as the main body, such as the market popular 6158 and UA202, also considering the cost and performance factors, tend to select certain or polyester epoxy resin using collocation, monomer with TP commonly as the priority, the initiator for the longer wavelengths is first selection, the other the right amount of phosphate and adhesion promoter can be appropriate to add.\nThe function of finish and requirements UV vacuum electroplating finish is coated on the vacuum coating metal coating, it plays a protective role in the coating and a certain decorative role.So the surface paint requires good adhesion to the metal coating film, to have enough performance to resist the external friction, scratch, scratch and other external forces, and provide a certain resistance and resistance.In addition, most of the topcoat or coating is the need to add color modification, the entire coating system of wettability requirements of the color filler is also very high.\nFinish the formula design of UV vacuum plating finish and primer is different, in order to assure the mechanical properties of coatings, tend to choose high some functionality resin to compatibility, and give full consideration to add color, will allow to add high functionality to improve wear resistance of resin, in addition, for pigments added convenience in order to guarantee system, polishing performance good resin is preferred, Such as the import of Changxing chemical 6071 and domestic Qianchen material UA378, Europe and the United States have a good material selection..In terms of additives, HEMA auxiliary properties can be selected in small amounts on the premise of ensuring the performance.","Technical DataHOW DO WE APPLY E-COATING ON FLEXPIPE PARTS AND WHAT IS E-COATING?\nTechnical DataPLASTIC COATING COMPARISON (PE VS. ABS VS. PVC VS. PP)\nTechnical DataHOW QUALITY CONTROL IS CONDUCTED ON STAMPING PRODUCTION\nThere’s one essential fact that we should begin with: a component’s desaturation will not affect the structural capacity of your structure. Now, is pipe color important for color coding in your business? Do you plan on moving structures outside? If so, then you should be aware that specific Flexpipe colors have better UV resistance than others.\nWe sent out some Flexpipe tube and joints to Micom Laboratories to test out the resistance of their coatings to 1000 hours of UV exposition. The test was done in a Weather-Ometer – device measuring controlled ventilation under the influence of light, oxidation, and moisture – with the following conditions: ASTM D2565 cycle 1. Check out the results below to see the Flexpipe parts that have the best resistance to UV. Also, you’ll learn about specific situations where you want to be extra careful when choosing a color.\nWARNING! The colors you’re about to see aren’t an exact match of those on Flexpipe pipes and joints. They were selected from images sent to us by Micom Laboratories in the same conditions as those mentioned previously. The purpose of these images is to show you the change in color rather than the colors themselves.\nBoth the black and nickel-plated joints we got back from the test showed no apparent change in color, which is why there is no image provided. It’s worth mentioning though that the black coating joints did lose some of their glossy finish. Since we performed these tests, we have changed our nickel coating for chrome. Since the chrome plating process is exactly the same as the previous nickel plating, only with an additional chromium-plating step, we can estimate that the result is very similar.\nAs you can see, the blue pipe color changed slightly during the test. We consider the blue pipes to have a good resistance to UV. Furthermore, an experiment with the blue pipe outside Flexpipe’s facility demonstrated no bigger change than what is shown below after multiple years of being exposed to outdoor elements.\nBlack pipes are probably one of the most UV resistant parts of the Flexpipe system. We consider these pipes to be highly resistant to UV exposure.\nA small difference can be observed on gray pipes after UV exposition. However, the difference is small enough that we consider Flexpipe’s gray to have a good resistance to UV rays.\nSince these colors were selected from a photo taken without perfect light conditions, the white pipes look a little gray. As you can expect, this shade is desaturated as an absolute white would be the absence of any color. In this case, we can see that there is a slight desaturation when exposing the pipes to UV rays. Therefore, we consider white pipes to have good UV resistance.\nYellow pipes have shown little to no change at all during the Weather-Ometer test. We consider the yellow pipe to be highly resistant to UV rays.\nYou’ll want to avoid using orange pipes if your structure will be used outdoors and if the stability of its color matters. We were surprised to see just how close the orange matched the yellow pipe’s tone after the test. We consider this color to have a poor resistance to UV exposure.\nThe red pipes are by far the color that underwent the most drastic change. We recommend you avoid this color when creating structures that are meant to be used outside or located near large windows or doors. As a result, we consider the red pipes to have a very low resistance to UV exposure.\nFlexpipe accessories and decking showed no significative change after being exposed to UV rays.\nWe hope that this information will prove to be helpful in your decision process. If you would like more information about Flexpipe parts’ UV resistance, contact us to get the complete Micom Laboratories report with all the data."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:932bf1b7-90e2-4aec-a178-e34afb2109b5>","<urn:uuid:6b0e0f54-e54f-49f3-8105-02df4e33e97e>"],"error":null}
{"question":"What mechanical features does the world globe have for controlling rotation, and what modern technological capabilities are included in the telescope mount for automated movement?","answer":"The world globe has a ratchet-and-pawl mechanism that controls rotation direction, with a ratchet ring containing 24 raised indicators mounted at the North Pole and a deflectable pawl finger that allows counter-clockwise rotation only when viewed from the North Pole. The telescope mount features larger 144mm diameter worm wheels for smooth operation, internal optical sensors on both axes for remote operation, and WiFi compatibility that allows control via the SkyPortal mobile app. It also includes StarSense Technology for automatic alignment within 3 minutes by capturing and comparing images of the night sky against its database.","context":["|3997980||Globe and measuring device||December, 1976||Rogers||434/141|\n|3827233||GEOGRAPHICAL TIMEPIECE OR CLOCK||August, 1974||Villar Eschevarria et al.||434/142|\n|3516243||GLOBE-CLOCK WITH SINGLE BEARING||July, 1969||Hazard||368/24|\n|3241252||Orbital attachments for terrestrial globes||March, 1966||Baalson||434/140|\n|2339385||Illuminated globe mounting||January, 1944||Dupler||434/145|\n|2099518||Globe chronometer||November, 1937||Hazlett||434/142|\n|1949403||Navigation instrument||March, 1934||Ashlock||434/142|\na ratchet comprising a ring with a plurality of indicator means arranged radially from the center thereof, said ratchet being affixed to a rotatable world globe about one of its centers of rotation thereof; and\na deflectable pawl member held by the support frame of the world globe, said pawl member having a pawl finger abutting said indicator means of the ratchet at an angle to the upper surface of the ratchet, the angle of the pawl finger positioned so that the finger is pointing in an eastwardly direction so that the world globe may rotate so that its surface moves freely counter-clockwise as viewed from the North Pole but is restrained from rotating in a clockwise direction as viewed from the North Pole.\na one-way rotational member affixed between the globe and the support therefor, said one-way rotational member being affixed so that the surface of the world globe moves freely only in a counter-clockwise direction as viewed from the North Pole; wherein the one-way rotational device comprises a ring-shaped ratchet member affixed to the surface of the world globe, and a pawl member is affixed to the support frame of the world globe.\nThe field of the invention is educational devices and the invention relates more particularly to world globes.\nWorld globes form a part of essentially all geography classes. Invariably, the globe is rotated randomly without an appreciation of the actual direction of rotation of the world. There is also, typically, no indication on the globe as to the amount of rotation which comprises the equivalent of an hour's time.\nIt is an object of the present invention to provide a device which restrains the incorrect direction of rotation of a world globe.\nIt is another object of the present invention to provide a device which gives an audible indication of the amount of rotation of the earth in one hour.\nThe present invention is for a device for controlling the direction of rotation of a world globe, said device comprises a ratchet ring with a plurality (such as 24) of raised or indented surface irregularities arranged radially from the center thereof. The ratchet ring is affixed either about the North Pole or the South Pole of the world globe. A deflectable pawl member is held by the support frame of the world globe, and the pawl member has a pawl finger which abuts the notches or bumps, and the pawl is oriented at an angle which points in an eastwardly direction so that it readily flexes when it passes over a notch or bump, but abuts the notch or bump when an attempt is made to rotate the globe in an incorrect direction. Preferably, there are twenty-four surface notches or bumps so that the pawl will make an audible click during a one/twentyfourth (one hour) rotation of the globe.\nFIG. 1 is a side view of a world globe including the rotational direction member of the present invention.\nFIG. 2 is an enlarged top view taken along line 2--2 of FIG. 1.\nFIG. 3 is an enlarged side view showing the ratchet and pawl of the globe of FIG. 1.\nFIG. 4 is a view taken along line 4--4 of FIG. 3.\nFIG. 5 is a cross-sectional side view of an alternate embodiment of the ratchet and pawl portion of the rotational direction member of the globe of FIG. 1.\nFIG. 6 is a plan view of an alternate embodiment of the ratchet portion of the rotational direction member of FIG. 1.\nFIG. 7 is a perspective view of the clamp and pawl assembly of the rotational direction member of the present invention.\nA world globe containing the device for controlling the direction of rotation thereof is shown in side view in FIG. 1 and indicated by reference character 10. Globe 10 has a support stand 11 which supports a support band 12 in a conventional manner. Globe 10 is affixed conventionally at the North Pole 13 and the South Pole 14 to band 12 which in prior art globes permits the rotation of the world globe in either direction.\nWorld globe 10 has a direction-limiting assembly mounted at the North Pole 13. The direction-limiting device comprises a ratchet ring 15 which is glued, or otherwise affixed, to the globe, and ring 15 has a plurality of indicator means 16. In the versions depicted in FIGS. 2, 3 and 4, the indicator means comprise ridges or bumps. In FIG. 5, the indicator means indicated by reference character 16' comprises notches. A flexible pawl finger 17 is shown best in FIG. 4 and is mounted on support band 12 by a removable clamp 18. Pawl finger 17 is angled in an eastwardly direction as indicated by arrow 19. An eastwardly direction arrow is also indicated by reference character 19 in FIG. 1. It can readily be seen that as the globe is rotated, pawl finger 17 is moved upwardly by the indicator means, such as ridges 16, and snaps downwardly after it passes over a ridge 16. This provides an audible click and, preferably, twenty-four notch means are provided, as shown in FIG. 2, so that the amount of rotation of the globe during one hour can easily be audibly indicated. It is also preferable that indicia 20 be provided, as shown in FIG. 2, to further teach the relationship between time and rotation of the earth.\nThe means used to hold pawl finger 17 is shown best in FIG. 4. Removable clamp 18 with its tightenable screw 21 provides an easily removed assembly which can be added whenever the subject is being discussed in a classroom for those applications where the globe is being used in a teaching environment. Furthermore, a concave indentation 22 is formed in arm 24 which is integral with removable clamp 18. This indentation surrounds pin 23 which holds the globe to band 12. This results in the accurate positioning of pawl finger 17 over the indicator means.\nPreferably, pawl finger 17 is fabricated from a polymer such as ABS or nylon. It should be fabricated from a strong polymer since students will have a tendency to see what happens when it is rotated in the wrong direction. Although pawl finger 17 is shown in FIG. 7 as an integral member, it could be a separate member affixed to a clamp body.\nAs indicated above, an alternate embodiment of the ratchet ring 15 is shown in FIG. 5 in a fragmentary side view to illustrate clearly that the notch indicator can be either raised or depressed. Also, although the device is shown mounted at the North Pole 13, it could alternatively be mounted at the South Pole 14. Furthermore, it is within the purview of the present invention to mount a ratchet about the equator and the pawl on the support band also at the equator. The pole positions are preferred, however, as there is less geographical information to portray at the poles as compared to the equator. It is also possible that the ratchet could be formed about a shaft affixed to the North or South Pole (preferably the South Pole for this embodiment) and the pawl supported on the exterior thereof. The version depicted in the drawings, however, is preferred in that it may readily be added to existing globes, and the ratchet ring 15 is curved in different sizes for the more common size of globe such as 12\" and 16\" diameter globes. Ratchet ring 15 is preferably injection molded from plastic, although it could be cast from aluminum. Although the term \"ratchet and pawl\" has been used herein, other one-way rotation means can also be used. For instance, a clutch band surrounding a frictional surface is another means of permitting rotation only in one direction.\nAlthough the ratchet ring 15 may be generally placed over an opening at the North or South Pole, for globe construction when this is not possible, a two-part ring 25 and 26 is shown in FIG. 6. This may be glued, or otherwise affixed, about pin 23 without disassembling the globe from band 12.\nThe present embodiments of this invention are thus to be considered in all respects as illustrative and not restrictive; the scope of the invention being indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are intended to be embraced therein.","Celestron StarSense AutoAlign Accessory for Automatic Alignment - 94005\nCelestron's newly designed CGX-L Computerized Equatorial Mount is the latest addition to their fully computerized equatorial mount series. Capable of carrying Celestron’s largest optical tubes with ease, CGX-L is designed for backyard observatories and remote imagers looking for exceptional load capacity to weight ratio, compact design, and innovative features that fully support large telescopes, imaging kits, and the latest in wireless tech accessories.\nKey design goals included larger 144 mm diameter worm wheels which provide smoother operation and can more accurately drive heavier loads, longer 270 mm dovetail saddle to support larger optical tubes, remote operation friendly features such as home and limit optical sensors, easier polar alignment adjustments, and better cable management overall. In addition to that, we’ve made mechanical and ergonomic improvements throughout to make the mount sturdier, easier to use, and easier to transport. COMPACT. ERGONOMIC. INNOVATIVE.\n- 9.25\" Schmidt-Cassegrain Optical Tube Assembly.\n- WiFi & StarSense Technology Compatible .\n- Aluminum Optical Tube.\n- Celestron's premium StarBright XLT coatings.\n- 2350 mm focal length (f/10).\n- 25 mm eyepiece included (94x).\n- Visual back allows for use with 1.25\" accessories.\n- 6x30 finderscope to help accurately find objects.\n- Star diagonal provides more comfortable viewing position.\n- Includes dovetail rail compatible with CGE mount.\nComputerized Mount Features:\nIncreased 75 lb load capacity. Exceptional load capacity to weight ratio.\nAdditional Aux accessory ports and autoguider port on the Dec axis for better cable management.\nLarger 144 mm diameter worm wheels to better and more accurately drive heavier loads.\nLonger 270 mm dovetail saddle to better support larger optical tubes.\nMassive 70 mm stainless steel tripod legs with a wider stance for improved stability under heavy loads.\nNew accessory tray holds three 1.25” and two 2” eyepieces with an upright stand for your smartphone or tablet.\nHeavy duty, 31.5 mm stainless steel counterweight shaft to support additional counterweights.\nCompact, lower profile equatorial head provides improved stability and portability. Sturdier and more rigid with quicker dampening time.\nImproved motors provide more torque, better slewing and tracking under heavy loads.\nHeavy duty belt-drive system minimizes backlash while providing smooth motor operation at full capacity.\nClear windows built into motor covers so pulleys and belts can be monitored.\nSpring-loaded brass worm wheel and stainless steel worm gear reduce friction and provide optimum gear mesh.\nInternal cabling for worry-free remote operation. Power input and lower accessory ports remain stationary while the mount slews to avoid snags.\nInternal hard stops for both axes prevent cable tension and tripod strike.\nInternal optical sensors on both axes for simple and safe remote operation.\nSoftware Polar Align: “All-Star Polar Alignment” allows quick and very precise polar alignment. Align on any bright named star without extra polar alignment accessories, apps, or other assistance.\nAdjustable EQ head position to optimize center of gravity over the tripod and fully utilize the increased 3º-65º latitude range.\nDual-fit Vixen/CG-5 and Losmandy/CGE dovetail saddle.\n+20º of additional tracking past the meridian on either side.\nRedesigned tripod has a widened stance for added stability. The minimum tripod height has also been lowered per user feedback.\nTripod legs can be collapsed with accessory tray installed for faster setup and transport.\nCelestron’s telescopes reach a new level with integrated WiFi! Leave your hand control behind and slew to all the best celestial objects with a tap of your smart device. Connect your device to your WiFi telescope’s built-in wireless network and explore the universe with Celestron’s SkyPortal mobile app for iOS and Android.\nCelestron’s newest planetarium app is an astronomy suite that redefines how you experience the night sky. Explore the Solar System, 120,000 stars, over 200 star clusters, nebulae, galaxies, and dozens of asteroids, comets, and satellites—including the ISS. SkyPortal includes everything you need to experience the night sky in an exciting new way.\nWith Celestron’s patented StarSense Technology, telescope alignment has never been easier! Just attach the StarSense AutoAlign accessory, push the Align button on the hand control or smart device, and StarSense begins capturing and comparing images of the night sky against its internal database.\nIn about three minutes, it’s gathered enough information to triangulate its position and align itself. Then, press the Sky Tour button: StarSense will automatically slew to all the best stars, planets, galaxies, and more currently visible in the night sky.\nAll-new ergonomic adjustment knobs with reduced friction for smooth and easy positioning under full loads. Stationary latitude knob with a newly-designed linkage system greatly reduces the amount of friction when polar aligning the mount. Redesigned azimuth adjustment with low-friction Teflon bearings to make adjustments easy under full load. No more binding, rough adjustments!\n- Integrated handles for easy transport and portability.\n- All-new ergonomically designed dovetail clamping knobs.\n- Innovative and improved polar alignment adjustment system.\n- Easy to read latitude scale.\nCGX-L Equatorial Head, CGX-L Tripod, Accessory Tray, 1 x 22 lbs Counterweight, NexStar+ Hand Control, DC Power Cable (plugs into cigarette lighter socket), 8 mm Allen Wrench\nDimensions & Specifications\nMagnification of Eyepiece 1:\nHighest Useful Magnification:\nLowest Useful Magnification:\nLimiting Stellar Magnitude:\n0.59 arc seconds\n0.49 arc seconds\nLight Gathering Power:\nSecondary Mirror Obstruction:\nSecondary Mirror Obstruction by Diameter:\nOptical Tube Length:\nOptical Tube Weight:\nInstrument Load Capacity:\nTripod Height Adjustment Range:\n35.75\" x 52.75\"\nTripod Leg Diameter:\nEQ Head Weight:\nTotal EQ Head Weight:\nWe stand behind all the products we sell, and your satisfaction is our top priority. If you're not satisfied with the quality of your purchase, simply send the item back to us in its original packaging within 30 days of its delivery.\nFor two years, Celestron will cover repairs or replacement of this product in cases of defective components. This warranty shall be void and of no force of effect in the event a covered product has been modified in design or function, or subjected to abuse, misuse, mishandling or unauthorized repair. Further, product malfunction or deterioration due to normal wear is not covered by this warranty.\nReturn & Exchange Policy\nAt our site, returns are easy! You may return new, unused, and resalable items for a refund or exchange. Simply ensure that the item is returned in its original product packaging within 30 days of delivery.\nMost Helpful Reviews\nMost Critical Reviews\nDisplaying Reviews 1-10 of"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:79481ed0-c954-40a3-87ca-682e71fe6b61>","<urn:uuid:db71f249-aa55-4b7f-b59d-6fbcafe72e08>"],"error":null}
{"question":"¡Hola! Me interesa saber más sobre la diferencia entre lacquers and varnishes, y sus niveles de VOC. ¿Me pueden explicar?","answer":"There is actually no difference between lacquers and varnishes - the trade tends to refer to them as 'lacquers' while the public calls them 'varnishes'. The main distinction is that solvent-based lacquers are mainly used by trade professionals, while water-based varnishes are recommended for home use. Regarding VOC levels, these products are regulated by the EPA with maximum allowed levels of 250 g/l for flat finishes and 380 g/l for non-flat finishes, though some regions like California have stricter limits of 100-150 g/l.","context":["If you don't see the answer to your question please contact us (please let us know the relevant website page if applicable). If it is a popular question we can include an answer in this section.\nWhat is the difference between wax polish, oil and varnish?\nHow can we tell if the wood has been waxed, oiled or varnished?\nIf a wooden surface looks like it already has been finished with a product it is always a good idea to find out what that finish is as it may affect which product you should apply to the wood. For example, you could not really apply an oil over a varnished floor, but an existing floor that has been oiled can be re-coated with an oil (once it has been cleaned of course!). Here's a few pointers to find out what your wood has been finished with:\nWhat is the advantage of an oil?\nThe main advantage of oil is that they are the easiest of all wood finishes to repair. A patch repair, for example, where a drink's stain has occurred can easily be removed by simply sanding just the affected area and applying more oil.\nWhat is the advantage of a wax?\nA wax looks really nice, with more of a sheen than a gloss. It can be re-waxed as many times as you like and buffed until the required sheen level is obtained or indeed not buffed if a low sheen is preferred.\nWhat is the advantage of a varnish?\nIt is hard wearing and easily wiped down with damp cloths.\nIs there a difference between a lacquer and a varnish?\nNo, the trade tend to refer to them as \"lacquers\" and the public tend to refer to them as \"varnishes\".\nWhy do you have both a lacquer and varnishes section on your site?\nThe public often feel they don't need a lacquer and sometimes the trade feel they don't need a varnish so this way we cater for both markets. In our lacquers section we have mainly included solvent based lacquers which are mainly used by the trade and in our varnishes section we have water based products which are recommended for use in the home.\nWhat type of wood stain do I need?\nHow do you choose the right wood stains for the job? It's something we're asked all the time. Thankfully there are some awesome products specially designed to cater for different wood types, finishes and situations. How do you choose? To give you a helping hand, our product pages make it nice and clear which products are suitable for what type of wood and project. We've also written a useful blog post about choosing the right kind of wood stain.\nWhat is the best way to match different coloured woods?\nBy using a product from the same range on each type of wood, that needs to be matched, you can obtain a closer colour match. Where possible, it is best to use a clear coating on the darkest wood and a coloured coating on the lightest wood. Oil is a good option because it penetrates well into the wood, colouring it and protecting it at the same time. With an oil, you can also apply a second coloured coat if the match is not close enough after the first attempt. Hard wax oil is a superior oil that is also available in colours. For example, to match new pine to old pine use 1 part whiskey hard wax oil to 1 part clear hard wax oil on the new wood and just clear on the old wood.\nWhich product offers the clearest finish?\nIf you are looking for a clear finish and do not need extra durability we can recommend 2 types of Wax polish depending on the project you are working on: Supreme Wax Polish and Fiddes Beeswax Polish.\nWhat if a wax isn't durable enough and I want a clear finish?\nExtra tough clear varnish or hard wax oil offer a clear alternative to the wax products. The difference is that they will slightly darken the colour of the wood, applying a little water to the wood will give you an indication of the final finish. We have had samples of varnishes that claim to keep the wood \"Looking exactly as it is before applying the varnish\" but they don't actually work. If you know of a product that does keep the wood looking the same but protected then we would love to hear from you. N.B. There is a product called Bona Naturale that is as clear as you can get but it is well over £100 for a 5L tub which will cover about 3 coats on an area that is 15 square metres. We can obtain this product if it interests you. Roughly speaking it will make your wood look somewhere between how it is before anything has been applied to it and how it looks once it has been wetted with water.\nWhat is the difference between a dye and a stain?\nEssentially there is no difference between a stain and a dye, they are both coloured liquids that don't offer a sheen or protection, they simply colour the wood. Our dyes are very concentrated and are available in the primary colours. Sometimes a product may have the word stain in its' title even though it is a protective coating also. However we believe that a stain should simply stain the wood and not do anything else because a better result will be achieved if clear coats are applied on top of a stain.\nWhat is the benefit of a water-based stain?\nThey are well priced and very easy to apply. They have very little smell and a large range of colours are available.\nWhat is the benefit of a solvent-based stain?\nThey are quick drying (about 5 minutes) and the grain of the wood is not raised.\nIs grain raising (with water based products) something to be concerned about?\nIn short - no, but it is a question of personal taste. The grain doesn't feel rough to the touch, it's more of a textured feel. In other words the pattern of the wood can be felt through the wood finish. Some customers really like to be able to feel the wood. If several coats of varnish are being applied then grain raising is not really an issue because the varnish tends to seal over the grain that has been raised, thus creating a smooth surface.\nWhat is the best way of achieving a white wood finish?\nWhite wood finishes can be varied in terms of the type of white, along with intensity, ranging from a subtle white wash where the wood is still clearly visible, through to a solid opaque white finish. There are a number of products available to help achieve the desired look, details of which can be found in our blog post about interior white wood finishes.\nHow can I best achieve a black wood finish?\nBlack wood finishes can differ greatly in terms of how black and how opaque the final look will be. There are a wide range of products for both interior and exterior wood to help achieve almost any look, details of which can be found in our blog post that covers interior and exterior black wood finishes.\nHow long will my wooden garden furniture last?\nHow long is a piece of string? The better you look after your wooden garden furniture, the longer it'll last. Why not check out our blog post about exterior furniture maintenance?\nHow do I restore my garden shed?\nFirst you need to fix the roof, clean all the surfaces and make sure the windows and doors are sealed nice and tight. Then it's time to pick your wood preservation weapons! Check out our blog post about garden shed renovation and maintenance.\nWhat are the most common problems with wood decking maintenance?\nPart two in our series of three blog posts about garden decking maintenance takes a look at the most common issues faced by DIYers who want to make their decking beautiful, as well as recommending some of the best-performing treatments.\nWhat should I use to clean decking?\nThe appearance of wood decking can quickly change due to the ravages of our British winters and sun damage. There is also a good chance that you will see signs of fungus growing on the surface, which will tend to blacken the wood. But there are some great products which have been specifically developed to maintain your decking all year round, and we stock only the best. Please refer to our blog post on how to clean decking for a full breakdown of products to keep your garden deck looking fantastic.\nHow do I prepare my garden decking for a new wood finish?\nIt depends on the type of wood and the finish you want to achieve. Have a look at the first in our series of three blog posts (Garden Decking Stain Treatments - Part #1) about garden decking treatments, how to prepare the wood beforehand and make a great job of it.\nWhat is the best way of achieving a solid black finish on decking, garden sheds, barns, and fences?\nBlack wood finishes on decking, sheds and fences have become more popular as they provide a striking contrast against the green of grass, trees and other garden plants. Black wood finishes also help to keep the traditional look of exterior wooden beams, barns, doors and window frames. Read more about achieving the perfect black wood finish in our blog.\nWhat is the best way to clean and maintain my floor?\nA concentrated cleaner can be mixed with water then applied by slightly dampening a cloth or mop. It is important to let the cleaner and not the water do the work. Using too much water on a varnish, wax or oil will take a little of your finish off each time it is used which in turn means your floor will have a shorter life. Visit our wood cleaners page to see our range of easy to use products.\nWhat is the best way to strip paint from wood?\nWe are often asked questions about the best way to strip paint and varnish from wood. Luckily there are some great products available today that make it a breeze. Basically, there are 3 approaches that you might take. Sanding, a hot air gun or some form of stripper. But there are things that you need to know in order to make the right choice. For example, a hot air gun is not good for removing varnish as it will simply melt the varnish and make it gooey.\nMy wooden kitchen cabinets and surfaces are looking tatty. How do I bring them back to life?\nThere are all sorts of fabulous products you can use to bring wood kitchen surfaces back to their former glory. You might like to read our blog post about kitchen cabinet maintenance.\nWhich type of filler should I use to repair interior and exterior wood?\nWe've put together some useful tips and advice on how to use wood filler on both interior and exterior woodwork in our blog.\nHow do I choose the best wood finishing product for my project?\nThere are some fantastic products around, each designed to handle specific wood finishing jobs under specified circumstances. Making the right choice means your wood will look beautiful for longer, and have exactly the right kind of protection for its location and function. Take a look at our blog post: Types of Wood Finishes - Making Your Wood Beautiful - covering some of the most popular types of wood finishes, those our customers ask about most often.\nHow would you describe a satin sheen?\nAnother way to describe satin is eggshell. In other words the sheen level is about the same as the surface of an egg when 1 coat has been applied. 2 coats will be a bit shinier.\nHow would you describe a matt sheen?\nNo shine or very little shine. However 3 coats of a matt varnish will produce a sheen that is almost the equivalent of a satin.\nHow would you describe a gloss sheen?\nVery shiny, if you hold an object against a piece of wood that has had 2-3 coats of a gloss varnish applied there will be a clear reflection of that product in the wood.\nWhat is the best way to apply finishes to a floor?\nWe've got a great range of floor applicators that will save on time, money and back ache!.\nIs PVA glue safe on skin?\nPolyvinyl acetate adhesive (PVA glue) is a remarkably versatile, water based glue that dries quickly at room temperature with good air circulation. It is non-toxic, doesn't emit harmful fumes and doesn't cause skin irritation. But it should not be ingested. Read more about the versatility of PVA adhesive and its remarkable and varied uses in our blog.\nDisclaimer: Whilst every attempt has been made to give information that is as accurate as possible, it is not fair to say that any advice can apply to every wood finishing situation. All woods have a different make up and influence top coats in their own way. For example a species of wood that is new will colour differently to the same species that has been cut for 2 years and one that has been cut for 2 years will colour differently to one that has been cut for 10 years so wood finishing advice is always a guideline only.","What are Volatile Organic Compounds (VOC's)?\nVolatile organic compounds, also known as VOC’s, are hazardous gasses emitted from a variety of common products including paints, paint strippers, sealants, adhesives, cauk, laminates, and wood finishes such as varnish. Thousands of chemicals are considered to be VOC’s by the United States Environmental Protection Agency. Recently efforts have been made to lower the quantity of VOC’s in paint and painting materials using regulation, while in the private sector more and more companies are offering low VOC products to keep up with an increasingly environmentally friendly and health conscious consumer base.\nHistorically the drawbacks of low VOC painting products included inferior performance and higher production costs; however, advances in technology have increased the performance of low-VOC paints while lowering production costs. Today many low-VOC or no-VOC paints have performance on par with traditional paints which contain higher levels of VOC’s.\nWhat exactly does Low-VOC, VOC-Free or No-VOC mean?\nClaims of ‘Low-VOC’ and ‘No-VOC’ are not legally enforced, in other words they don’t necessarily mean anything. However manufacturers are legally required to disclose the actual VOC concentration of their paint somewhere on the can, so if you are looking for a low VOC paint, look for one that has a concentration of less than 50 g/l. ‘No-VOC’ is somewhat of a misnomer in the sense that almost no paints are VOC free. When a manufacturer claims in good faith that their product has ‘No-VOC’ or is ‘VOC-Free’ it usually means the paint has a VOC concentration of 5 g/l or less.\nAre VOC quantity measurements accurate?\nNot entirely; unfortunately 'Method 24', the official method used by the EPA and paint manufacturers to evaluate the VOC contents of paint products, is outdated and was in fact never designed for use in substances with VOC concentrations of less than 100 g/l. There are other more accurate testing processes, some of which have been adopted by local states and municipalities who enforce their own VOC regulations, but federal regulations and most paint manufactures still rely on the outdated ‘Method 24’ for their results. Also testing does not account for any VOC heavy tints that are sometimes added at point-of-sale.\nWhere do VOC’s Come From?\nThe majority of VOC’s are found in paint’s solvent. Oil based paints tend to have high levels of VOC’s, whereas Latex paints whose base is water, have low levels of VOC’s. Other sources of VOC’s include performance enhancing additives such as fungicides and mildewcides, as well as tints which are sometimes added to the paint at the point-of-sale.\nWhat are current VOC regulations?\nThe US Environmental Protection Agency currently regulates the max level of VOC’s in paint products depending on their finish type (learn more about paint finishes in our article: Types of Paint Finish).\nThe levels are as follows:\n- Flat Finish: 250 grams per liter (g/l)\n- Non Flat Finish: 380 grams per liter (g/l)\nHowever several states and municipalities enforce even more stringent requirements. California in particular has strict requirements, 100 g/l for flat finishes and 150 g/l for all other finishes. Even stricter than that is California’s South Coast Air Quality Management District (AQMD) around Los Angeles whose limits are set at 50 g/l for all paint finishes.\nWhat is Green Seal certification?\nGreen Seal is a non-profit organization founded in 1989 which creates ‘green’ standards for various products, including painting products. Green Seal certification for paint products includes maximum VOC levels depending on paint type:\n- Non-flat finish: 150 g/l\n- Flat Finish: 50 g/l\n- Non-flat finish: 200 g/l\n- Flat Finish: 100 g/l\nGreen Seal also specifically prohibits the use of dozens of other hazardous materials and heavy metals. Green seal certified paint products are required to meet minimum performance requirements including abrasion resistance (durability), opacity (hiding power), and washability."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:58ce8e2e-6625-4a2f-972f-3d69c362ae5e>","<urn:uuid:c2e87560-a982-4011-8c99-cb2ff5127f1b>"],"error":null}
{"question":"How should investors balance risk tolerance and time horizon when choosing mutual funds, and what specific factors determine a fund's risk profile?","answer":"Investors should align their risk tolerance with their investment time horizon - those with longer horizons may have higher risk tolerance, while shorter-term goals require more conservative approaches. A fund's risk profile is determined by several factors, including absolute risk (measured by standard deviation compared to peer groups) and relative risk (measured through Sharpe ratio, alpha, and information ratio). The Morningstar risk assessment emphasizes downward variation, categorizing funds into five risk levels from High to Low. Additionally, risk evaluation should consider the fund's investment style (growth, value, or blend) and portfolio diversification across sectors, industries, and geographic regions to help mitigate risk.","context":["According to Morningstar, there are 22,689 mutual funds, 1,457 ETFs, 14,422 stocks, and I couldn’t even venture a guess as to the number of municipal, corporate and government securities on the market. Moreover, there are a number of additional investments such as UITs, CEFs, REITs, MLPs, tangible assets and more. For advisors, sifting through the multitude of information in the investment universe is a daunting task to say the least.\nFor advisors who use mutual funds and exchange-traded funds, this article should hold special interest. In it I will share a process that I began developing about four years ago. After much tweaking, it has been refined to a point where my ability to select and maintain high-quality funds has been greatly enhanced. I call this tool my Fiduciary Scorecard. Unlike the majority of programs used to select funds, the Fiduciary Scorecard utilizes a ranking system rather than the problematic screen.\nScreens Have Holes\nMost investment programs utilize screens. For example, let’s say you were searching for large-cap funds with a trailing one-, three- and five-year category rank in the top 50th percentile. Some funds may meet the criteria in two periods, but miss it in the third and would be eliminated. This is true even if a fund missed the third time period by only one percentage point. Screens will eliminate everything that doesn’t meet their strict guidelines. This may not pose a problem if your screen only contains one criterion, but if your screen includes multiple criteria, you could be eliminating many good funds.\nAlthough my process begins with a screen, it is a very basic one. For example, I will screen for large-cap domestic value stocks with more than 80% invested in America. The result is a fairly large universe. Then, each fund is ranked by the Fiduciary Scorecard. In this way, a fund won’t be eliminated, but if it happens to be a regular visitor in the third or fourth performance quartile, it may receive a lower score.\nWhen I embarked on my road to independence several years ago, I needed software to help with fund selection. Although most applied some sort of screen, there were a few outliers. One such tool was from Klein Decisions. Klein has developed a system whereby you can select the data you deem most relevant. Another program I especially liked was from fi360 that analyzed and scored funds according to fiduciary standards. Although both programs were good, being the financially frugal person I am and considering they used data I didn’t believe to be important, I decided to create an alternative.\nWhen you consider the amount of information available, you quickly get a sense of the challenge one faces when selecting funds. In the course of my research, I discovered that some data is more important than others. This belief was reinforced after reading Russel Kinnel’s book, “Fund Spy.” Kinnel is the director of mutual fund research at Morningstar. He found certain data held greater significance in identifying future outperformers.\nThe process of building a portfolio of funds and ETFs involves three basic steps. First, you must identify which categories have the greatest potential to outperform given your particular global macro economic outlook. Next, you must search for the best funds and ETFs within these categories. Finally, you must determine the amount to be allocated to each fund. We will focus our attention on fund selection.\nIn the scoring process, each fund is measured against its specific peer group and not against the entire universe. Also, a fund with a higher score is preferred to a fund with a lower score and all data is from Morningstar.\nThe Five Broad Categories\nThere are five main areas included in a fund’s fiduciary score: absolute risk, relative risk, expense, stability and relative performance (See Figure 1, below). Let’s drill down and take a closer look at each category and how points are awarded.\nAbsolute Risk. Absolute risk is a measure of a fund’s fluctuation without regard to performance. To gauge a fund’s absolute risk over the past three and five years, I use standard deviation. If a fund’s standard deviation is 25% or more lower than its peer group average, it receives two points. If it is 0% to 24% below its category average, it receives one point. Otherwise, no points are awarded.\nAbsolute Risk’s contribution to final score: 6.6%.\nRelative Return. Relative risk is a measure of how well a fund performs given the amount of risk it assumes. Funds with greater risk should compensate investors with a higher return. Each fund is ranked within its peer group on a risk-adjusted basis using Sharpe ratio, alpha, information ratio and risk/return.\n- Sharpe Ratio. The Sharpe ratio is a risk-adjusted measure used to determine an investment’s excess return per unit of risk. Its formula incorporates the standard deviation and the risk-free rate. If a fund’s Sharpe ratio is greater than the average for its category over the trailing three-year period, it receives four points. Otherwise, no points are assigned.\n- Alpha. Alpha can be useful in determining how much value a manager has added. Alpha is most useful when comparing funds with similar investment objectives. If a fund’s alpha exceeds its category average for the trailing three-year period, it receives four points. Otherwise, no points are awarded.\n- Information Ratio. Unlike the Sharpe ratio, this risk-adjusted ratio replaces standard deviation with a specific benchmark index. Because the IR is affected by a fund’s tracking error, and because I don’t believe tracking error is particularly useful, it receives a lower point total than the other relative risk measures. If a fund’s IR is above its peer group average for the trailing three-year period, it earns three points. Otherwise, no points are given.\n- Morningstar Risk/Return. Morningstar risk is an assessment of the variations in a fund’s monthly returns with an emphasis on downward variation. I’ll forgo divulging their secret sauce, but will mention that the top 10% of funds are given a risk score of five or “High”; the next 22.5% are scored four or “Above Average”; the next 35% are scored three or “Average”; the next 22.5% are scored two or “Below Average”; the bottom 10% are scored one or “Low.”\nMorningstar return is an assessment of an investment’s excess return over a risk-free rate (e.g., the return of the 90-day Treasury bill), the results of which are similarly divided into one of the five aforementioned categories.\nUsing the trailing three- and five-year periods for each, here’s how I score them: Let’s assume a fund has a risk profile of Average. We would expect the return to be at least Average and hopefully higher. If a fund has a risk profile of Low and a return profile of High (the best possible outcome), the difference between the two is four sigma. In scoring, if there is four sigma difference, it receives the maximum number of points: five. If there is a three sigma difference, it receives four points, and so on. If the risk and return are equal, it receives one point. If, however, the risk is higher than the return (not desirable) no points are awarded.\nRelative Risk’s total contribution to final score: 35%.\nExpense. Morningstar analyzes several different expense ratios. I prefer to use the prospectus net expense ratio. This measures the percentage of fund assets paid for operating expenses and management fees. In contrast to the gross expense ratio, the net expense ratio reflects any fee waivers in effect during the time period. Therefore, I believe it to be a more accurate measure. If a fund’s expense ratio is more than 25% below its peer group average, eight points are assigned. If the ratio is 0% to 24% below, four points are awarded. Otherwise, no points are given.\nExpense’s contribution to final score: 13.5%.\nStability. Stability is determined by the length of time the fund’s management has been in place. If a fund manager has been with the fund for 10 years or more, four points are awarded. Funds with management in place for five to nine years earn three points, and those with managers of three or four years receive two points. If a manager has been in place less than three years, no points are awarded.\nStability’s contribution to final score: 6.6%.\nRelative Performance. The final area is relative performance, which includes category performance and category rank.\n- Category Performance. This category includes how well a fund’s returns compare with its category average over the trailing one-month, three-month, and most recent five calendar years. The one- and three-month returns have a lesser impact on the score than the calendar years. I also believe it’s important to evaluate a manager’s performance during difficult periods. Therefore, 2008 has a greater impact on the score than the other four calendar years.\nIf a fund’s return was greater than its category average for the one- or three-month period, one point is awarded. Two points are earned for above average returns in most calendar years with the exception of 2008, where four points is the maximum.\n- Category Rank. Morningstar ranks a fund’s performance in percentiles for various time periods. If a fund is ranked in the first percentile, then it has outperformed 99% of its peers. Conversely, a rank of 100 indicates it was at the very bottom of its peer group. I use the trailing one-, three- and five-year periods in my Scorecard. If a fund ranks in the top quartile, it receives three points. A second quartile fund earns two points and third quartile ranking gets one point. If a fund ranks in the bottom quartile, no points are awarded.\nRelative Performance’s contribution to final score: 38.3%.\nBy using so many different points of data, each with a different weighting, knee-jerk reactions can be avoided.\nThe Fiduciary Scorecard and My Practice\nAs fiduciaries, we will be measured more on our process than absolute performance. In other words, “Was there a good reason to do what we did?” The Fiduciary Scorecard fits this need for a well-defined process perfectly. Once funds have been selected, monitoring becomes the key issue.\nFund Watch List\nTo monitor the funds I’ve selected, I download all funds held across all accounts from my custodian’s portal and run their score. If a fund has a fiduciary score of 75% or higher, it receives a rating of “Pass+.” Funds scoring between 50% and 75% are rated “Pass.” Funds from 40% to 49% are a “Watch One.” Funds that score between 30% and 39% are a “Watch Two,” and a score below 30% is a “Watch Three.” If a fund is a Watch One, no action is taken, but I will pay a little closer attention to it. If a fund falls to a Watch Two, I’ll phone the fund company and discuss it. If I believe the fund has a reasonably good chance of improving, I’ll hold it. If a fund falls to a Watch Three, it’s liquidated.\nThe Fiduciary Scorecard report includes the fund’s name and ticker symbol, its sub category, the total allocation I have in the fund, its fiduciary score and its broad ranking (see Figure 2, above). Another column displays the percentage of total possible data for the fund. For example, if a fund does not have a five-year track record, it won’t be penalized. The Scorecard will simply use the data it does have. However, if a fund has less than 50% of possible data, no score is awarded and “Insufficient Data” is displayed. The output is formatted as a table and, as such, can be sorted by any heading.\nThere’s one additional item of significance. If I held a fund with a low score and had a lot of client money in it, that would be a concern. It’s also important to know what percentage of funds are on the Watch List.\nThe Fiduciary Scorecard has become a regular part of account reviews. Along with performance, composition, etc., I’ll include a Scorecard for the funds held in the client’s account. Clients appreciate the information, and it gives them a sense of security and comfort to know we have a good due diligence process in place. Clients can easily relate to a ranking system which ranges from. one to 100.\nEven if you are not inclined to develop a similar system or pay the money to subscribe to one of the aforementioned products, it is still helpful to focus your search on the areas outlined above. Just be careful not to be too rigid with your screen.\nCheck out more stories on Mutual Funds at AdvisorOne, including:","Looking for the right mutual fund may seem like a daunting task, but fret not! Choosing the perfect mutual fund that aligns with your financial goals is not as complicated as it may appear. In this article, we will walk you through the essential factors to consider when making this crucial decision. How to choose the right mutual fund for you? Don’t worry, we’ve got you covered. So, let’s dive right into the world of mutual funds and equip you with the knowledge you need to make an informed investment choice.\nHow to Choose the Right Mutual Fund for You\nChoosing the right mutual fund can be a daunting task, especially with the vast array of options available in the market. However, with careful consideration and understanding of your financial goals and risk tolerance, you can confidently select a mutual fund that aligns with your investment objectives. In this article, we will explore the key factors to consider when choosing a mutual fund and provide you with valuable insights to help you make an informed decision.\n1. Define Your Investment Goals\nBefore diving into the world of mutual funds, it’s crucial to clearly define your investment goals. Ask yourself what you want to achieve with your investments. Are you looking for long-term growth, generating income, or preserving capital? Defining your objectives will help narrow down the mutual fund options available to you.\n2. Understand Your Risk Tolerance\nAnother important aspect of choosing the right mutual fund is understanding your risk tolerance. Every investment carries a certain level of risk, and different mutual funds have varying risk profiles. Assess your comfort level with market fluctuations and potential losses to determine the appropriate risk level for you. Generally, if you have a longer investment horizon, you may have a higher risk tolerance, whereas shorter-term goals often require a more conservative approach.\n3. Consider Your Time Horizon\nYour investment time horizon plays a significant role in selecting the right mutual fund. Determine how long you intend to stay invested before needing to access your funds. If you have a longer time horizon, you may have more flexibility to invest in funds with higher growth potential. For shorter-term goals, it’s advisable to opt for more stable and conservative options.\n4. Evaluate Fund Performance\nOne of the key indicators of a mutual fund’s potential is its historical performance. While past performance is not a guarantee of future returns, it can provide insights into a fund’s ability to generate consistent results. Look for funds that have demonstrated strong performance over different market cycles and compare their returns to relevant benchmarks. Keep in mind that consistency over time is often more desirable than a one-time exceptional performance.\n5. Assess Fund Expenses\nMutual funds have associated costs known as expense ratios, which cover the fund’s operating expenses. These expenses can significantly impact your overall returns, so it’s important to carefully evaluate them. Compare the expense ratios of different funds within the same category and choose those with lower costs. However, it’s essential to consider other factors such as fund quality and performance alongside expenses.\n6. Determine the Fund’s Investment Style\nMutual funds can have various investment styles, such as growth, value, or a blend of both. Understanding the fund’s investment style is crucial as it determines the types of securities the fund manager will invest in. If you prefer companies with strong growth potential, a growth-oriented fund might be more suitable for you. On the other hand, if you seek undervalued stocks, a value-oriented fund may be the better choice.\n7. Consider Fund Management\nThe fund manager’s experience and expertise can greatly influence the fund’s performance. Research the fund manager’s track record and investment philosophy to gain insights into their investment approach. Determine whether the fund manager has successfully navigated different market conditions and if their investment strategy aligns with your goals and risk tolerance. Consistency in fund management is a desirable attribute.\n8. Identify the Fund’s Investment Holdings\nExamining the composition of a mutual fund’s portfolio is crucial in determining the type of assets it holds. Look for diversification across different sectors, industries, and geographic regions. A well-diversified fund can help mitigate risk by spreading investments across various holdings. Additionally, consider the fund’s top holdings and assess whether they align with your investment objectives.\n9. Evaluate Fund Size and Assets Under Management (AUM)\nThe size of a mutual fund can have implications on its performance and flexibility. While larger funds may provide stability and expertise, they can also face challenges in generating significant returns due to their size. Smaller funds, on the other hand, might be more agile and have the potential for higher growth. It’s essential to strike a balance and choose a fund that aligns with your investment goals.\n10. Take Advantage of Professional Advice\nIf you feel overwhelmed or unsure about choosing a mutual fund, consider consulting a financial advisor. A professional advisor can help assess your financial situation, goals, and risk tolerance to recommend suitable fund options. They can provide valuable insights and expertise to guide you in making an informed decision. However, it’s important to choose a reputable advisor who acts in your best interest.\nIn conclusion, choosing the right mutual fund requires careful consideration of your investment goals, risk tolerance, time horizon, fund performance, expenses, investment style, fund management, investment holdings, fund size, and professional advice. By thoroughly evaluating these factors, you can select a mutual fund that aligns with your objectives and sets you on the path toward financial success. Remember, investing involves risks, and it’s essential to regularly review your investments to ensure they remain aligned with your goals and risk tolerance.\nHow Do I Pick the Right Mutual Funds?\nFrequently Asked Questions\nFrequently Asked Questions (FAQs)\nHow do I choose the right mutual fund for me?\nChoosing the right mutual fund for you involves considering several factors:\n- Identify your financial goals and investment objectives.\n- Assess your risk tolerance and time horizon for investing.\n- Research different types of mutual funds, such as equity funds, bond funds, or balanced funds.\n- Analyze the fund’s past performance and compare it with similar funds in the category.\n- Consider the fund’s expense ratio and fees.\n- Check for any minimum investment requirements.\n- Read the fund’s prospectus to understand its investment strategy and holdings.\n- Consult with a financial advisor if needed.\nWhat are the different types of mutual funds?\nMutual funds come in various types:\n- Equity funds: Invest in stocks and aim for capital appreciation.\n- Bond funds: Primarily invest in fixed-income securities like corporate or government bonds.\n- Money market funds: Invest in short-term, low-risk securities.\n- Index funds: Mimic the performance of a particular index, such as the S&P 500.\n- Sector funds: Concentrate investments within a specific industry or sector.\n- Target-date funds: Shift allocation over time based on the investor’s retirement target date.\nWhat is the importance of considering risk tolerance?\nConsidering your risk tolerance is crucial when choosing a mutual fund because it determines the level of volatility you are comfortable with. Higher-risk funds may have greater potential for returns but also higher chances of losses. Understanding how much risk you can tolerate will help you select a mutual fund that aligns with your investment goals and personal comfort level.\nHow can I evaluate a mutual fund’s past performance?\nWhen evaluating a mutual fund’s past performance, consider the following:\n- Look at the fund’s performance over different time periods (e.g., 1, 3, 5 years, or longer).\n- Compare the fund’s performance to relevant benchmarks or similar funds.\n- Assess if the fund has consistently generated positive returns or if it experienced any significant losses.\n- Consider the fund manager’s track record and experience.\n- Remember that past performance does not guarantee future results.\nWhat is an expense ratio and why is it important?\nAn expense ratio represents the annual fee charged by the mutual fund company to manage the fund. It covers operating expenses, management fees, administrative costs, etc. It is important to consider the expense ratio because it directly affects the fund’s returns. Lower expense ratios mean more of your investment stays invested, potentially leading to higher long-term returns.\nShould I consider the fund size when selecting a mutual fund?\nWhile fund size is a factor to consider, it should not be the sole determining factor. Small or large funds both have their advantages. Larger funds may offer more stability and resources, while smaller funds might be more agile and have the potential for growth. It’s important to focus on other factors like performance, expense ratio, and investment strategy in addition to the fund size.\nWhat should I look for in a mutual fund’s prospectus?\nWhen reviewing a mutual fund’s prospectus, pay attention to the following:\n- Investment objectives and strategies of the fund.\n- Top holdings and sector allocation.\n- Risk factors associated with the fund.\n- Expense ratio and fees.\n- Historical performance.\n- Management team and their experience.\n- Any specific requirements or restrictions.\nIs it necessary to consult a financial advisor?\nWhile it is not necessary, consulting a financial advisor can provide valuable guidance and personalized recommendations based on your specific financial situation and goals. A financial advisor can help you navigate through the available options, explain complex concepts, and assist in making informed decisions about mutual fund investments.\nChoosing the right mutual fund for you is crucial for long-term financial success. Start by identifying your investment goals, risk tolerance, and time horizon. Conduct thorough research on different mutual funds, analyzing their past performance, expense ratios, and fund managers’ expertise. Diversify your portfolio by investing in a mix of equity, debt, and balanced funds. Consider the fund’s investment strategy and align it with your financial objectives. Regularly review and rebalance your portfolio to maintain a suitable asset allocation. Consult with a qualified financial advisor if needed. Making informed decisions ensures that you effectively choose the right mutual fund for you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:2baf0ae3-f7be-4aa4-b70a-aacf15726da3>","<urn:uuid:6dfcbb5b-c110-4d2c-bde1-2ce3ccdaf65d>"],"error":null}
{"question":"Getting laser hair removal soon. How long can I expect results to last on different body areas, and what are the essential pre and post-treatment protocols I need to follow?","answer":"Results vary by body area - body hair removal can be permanent or extremely long-lasting with sparse, thin regrowth. Facial hair removal isn't usually permanent, with regrowth likely on chin, neck and other facial areas, possibly due to hormonal changes. Some people report no regrowth for 10+ years, while others need yearly touch-ups. For pre-treatment, you must: avoid sun exposure and tanning, stop retinoid creams 2 days before, avoid waxing/plucking for a month before, shave the day before, and avoid bleaching products and cosmetics. Post-treatment requirements include: avoiding sun exposure for 6 weeks, using sunscreen especially on facial areas, avoiding creams/lotions/deodorants the day after, and using ice packs if needed for erythema and edema. The physician may prescribe topical corticosteroids if excessive inflammation occurs.","context":["Is Laser Hair Removal Permanent?\nLaser hair removal is a noninvasive cosmetic treatment used to eliminate undesirable body and facial hair.\nLaser hair removal on the body might produce permanent or near-permanent outcomes for certain individuals. Others may see a significant decrease in the number and thickness of hair that regrows over time.\nWhile permanent results are unlikely for the face, regrowth may take years.\nThe outcomes vary depending on a number of variables. These include the locations that are being treated as well as variable hormonal levels.\nHow long does it take for hair to regrow?\nUnderstanding hair development patterns may help you estimate how long it will take for your hair to regrow.\nHair growth stages\nHair passes through four phases of development. They are as follows:\nThe growth phase is catagen, the transitional phase is telogen, the resting period is exogen, and the shedding phase is anagen.\nAt any one moment, you have hairs in all four stages.\nLaser hair removal vaporizes existing hairs under the epidermis and at the root.\nAs a result, it can only target hairs during their anagen (growing) phase. That is why it requires numerous treatments, spaced apart, to reach all of the hair that develops in a certain area.\nDuring laser treatments, you will continue to see hair. Some of this will be regrowth, but the majority will remain untreated hair.\nAfter your treatment is over, you may not notice regrowth for several years.\nLaser hair removal on the body may produce permanent or extremely long-lasting results. Any hair that grows back should be sparse and thin over time.\nLaser hair removal on the face isn’t usually permanent, although it may be.\nSome individuals claim that no hair grows back after ten years or longer. Others have faster regrowth and depend on yearly touch-up procedures to keep unsightly hair at bay.\nDoes hair come back quicker in particular regions of the body or on the face?\nHair will most likely come back on the chin, neck, and other parts of the face after laser hair removal.\nThis might be related to hormonal swings and androgens like dehydroepiandrosterone (DHEA) and testosterone-reactivating hair follicles.\nHair regrowth on the chin, neck, upper lip, and sideburns may occur in some women after menopause when estrogen levels fall.\nBody hair regrowth is also possible. It may be more common on the arms, legs, or chest in certain persons than on the bikini line, stomach, or underarms.\nWhen hair grows back on the body, it is usually sparse and fine.\nIn rare cases, laser hair removal may result in thicker, darker hair growing or regrowing in an area next to the one being treated. This is referred to as paradoxical hypertrichosis. Anywhere on the face or body might develop paradoxical hypertrichosis.\nDoes the color of one’s hair or skin make a difference?\nYour hair or skin color may influence how successful laser treatments are for you at first, but they have no bearing on how fast your hair comes back once treatments are completed.\nLaser hair removal is currently not considered effective for blond, white, or gray hair.\nIs there anything you can do to prevent hair growth?\nHair regrowth may be controlled using laser therapy touch-ups as required.\nElectrolysis is another option if hair growth is minimal. Electrolysis treats individual hairs rather than whole regions.\nKeeping your hormones regulated if you have a disease like polycystic ovarian syndrome (PCOS) will help lessen the probability of hair regrowth. You may help regulate these hormones by taking drugs like Metformin or making dietary changes like limiting your carbohydrate consumption.\nWill repeated laser treatment sessions ultimately hinder hair growth?\nLaser treatments are sometimes marketed in bundles of four or more sessions. It may take 12 treatments or more to entirely stop hair growth.\nFollowing that, touch-ups should be minor. Initially, some individuals may need semiannual treatments. Others may discover that they don’t need a touchup for five years or more.","Permanent hair removal with the use of laser or intense pulsed light devices is a technique that evolved greatly since its introduction in the late 1980s and its approval by the FDA in 1996. Nowadays, such devices are widely used in dermatology and aesthetic medicine practices with an increased demand for permanent hair removal procedures. With the development of longer wavelength laser devices it is now possible to treat all skin types safely. \nWhat does permanent hair removal mean?\nLaser or IPL hair removal includes two different concepts:\n- Temporary hair loss\n- Permanent hair reduction\nTemporary hair loss is defined as a delay in hair regrowth that can last up to 3 months, depending upon what phase of the hair follicle cycle the treated hair is.\nConversely, permanent hair reduction is defined as a significant reduction in the number of hair following an hair removal treatment and that is stable for longer than a full hair follicle cycle (4-12 months).\nLaser or IPL hair removal causes an initial complete and temporary hair loss, up to 3 months following the treatment, and a subsequent partial permanent hair reduction. Thus, the treatment is long term effective in reducing the total number of hair and concurrently the hair left become thinner, lighter and grow more slowly.\nThe hair growth cycle or hair life cycle\nThe hair growth cycle is divided into 3 phases:\n- Anagen phase, or active hair growth phase\n- Catagen phase, or transitional regression phase\n- Telogen phase, or resting phase (also known as shedding phase)\nDuring the anagen phase, the cells in the root part of the hair divide rapidly, leading to hair shaft growth. During this phase, the amount of melanin in the bulb, which is already greater than the concentration in the skin, is increased further.\nCatagen phase is a short transitional phase during which the hair detaches from the lower structures of the papilla which in turn enters a shrinking process stopping melanin production and starting apoptosis (programmed cell death) of the follicular melanocytes, reaching 1/6th of its original size when compared to the anagen phase. During this phase, the hair strand’s blood supply is cut off due to its detachment from the papilla and even though it is not growing anymore, its apparent length increases due to being pushed toward the surface of the skin.\nLastly, the telogen phase or resting phase sees an almost completely inactive hair follicle and a dead fully keratinized hair still attached to the body. Towards the end of this phase, the hair detaches completely from the skin and falls, while the follicle restarts its cycle entering the anagen phase.\nThe duration of all these phases is variable and influenced by many variables such as genetics, the region of the body and environmental factors. In general, anagen phase lasts 3-6 months for body hair versus 3-7 years for scalp hair. Catagen phase lasts between 10 days and 3 months. Lastly, telogen phase lasts about 3 months.\nAll these phases are not synchronized, reason why we don’t get temporary bald or hairless in our lifetime: at each moment, about 80-90% of hair are in the anagen or active growth phase, 1-2% are in the catagen or regression phase and about 10-15% are in the telogen or resting phase.\nHow does permanent laser or intense pulsed light (IPL) hair removal work? Selective photothermolysis\nPermanent hair removal with the use of laser or IPL technologies works due to the selective photothermolysis principle. This means that the devices are calibrated on the wavelength that is selectively absorbed by the hair, which will consequently absorb the light emission transforming it into thermal energy causing thermal damage to the treated structures. The selectivity of the wavelength is such that there is no generalized damage to all neighboring structures in the treated area, but only to the specific targets, which in this case are the hair and more precisely the melanin within the hair. For this reason, laser or IPL devices are effective only in treating hair in their anagen phase, when the hair follicle is active and the melanin concentration in it is greater. For the same reason, it is not possible to treat all hair at the same time and several sessions (3-8) might be needed to achieve the desired result, a couple (4-8) weeks apart from each other, to allow the hair not in their active phase and thus not targeted by the previous treatment to enter the anagen phase and be eliminated with the following one.\nDifferences among permanent hair removal devices\nThe differences among the many types of permanent hair removal devices are mainly 6:\n- The wavelength of the device\n- The power of the device, intended as the amount of energy emitted per time unit\n- The dimension of the treated area, known as spot size, which varies between few millimeters and a couple centimeters.\n- The pulse duration, which is the amount of time during which the tissue is exposed to the light energy souce\n- The fluence, which is the ratio between the power and the area (spot size) upon this power is delivered\n- The presence of skin cooling systems.\nIn general, diode laser and alexandrite laser devices are considered the most effective, followed by IPL devices and Nd:YAG laser.\nDiode laser can be used safely on I-IV skin phototypes.\nAlexandrite laser can be used on I-IV skin phototypes and produces better results on thin hair, when compared to diode laser, but its use is preferred on lighter complexions.\nNd:YAG laser has a greater wavelength than the previous devices, thus a lower affinity for melanin. For this reason, although less effective, it is safer for use on V-VI phototypes and is able to produce positive results. \nIntense Pulsed Light (IPL) works in a slightly different way: laser emit a single wavelength with a single beam and all the light going in the same direction; IPL devices use a broad spectrum lamp controlled by a computer, with filters placed in front of it to select the desired wavelength. Moreover, the light emitted by IPL devices goes in all directions and only with the use of reflective surfaces it can be focused in a narrow area. IPL is safer in lower skin phototypes, due to its lower skin penetration ability and can be used also on lighter hair thanks to its broad spectrum, although with less satisfactory results when compared to its use on dark hair.\nLaser hair removal is indicated for the removal of excess unwanted hair, such as facial hair in women, or for the cosmetic removal of hair which can be due to many personal reasons. Moreover, it is used in the treatment of hypertrichosis and hirsutism. It is used as a medical treatment for chronic infections such as hidradenitis suppurativa, pseudofolliculitis and folliculitis. Lastly, it is also used to remove hair from skin flaps for reconstruction purposes, when the donor area has hair which can interfere with the functionality or aesthetics of the recipient area.\nAlthough there is no minimum age indication for this treatment, it is generally not advised under 15 years of age. Due to its mechanism of action, it is also not advised for elderly patients, with white hair, as those have no melanin in it, thus can’t absorb the energy emitted by the laser or IPL devices.\nIt is also contraindicated or caution is advised for:\n- Patients with photoaggravated skin disorders and autoimmune disorders, such as Systemic Lupus Erythematosus (SLE)\n- Patients with active infections or wounds on the area needing treatment, like herpes or staphylococci infections\n- Patients with keloids or keloid tendencies\n- Patients with psoriasis and vitiligo, due to the risk of developing the Koebner phenomenon.\n- Patients using photosensitizing medications\n- Pregnant women, although there is no evidence of any risk\n- Patients using oral isotretinoin (general consensus suggests waiting 6 months to 1 year after discontinuing the drug)\n- Areas with hyperpigmented lesions or tattoos\nBefore the treatment\nBefore the hair removal treatment with laser or IPL devices, sun exposure should be avoided as the skin should not be tanned during treatment. The use of retinoid creams must be suspended at least 2 days before the treatment. You should not wax, use an epilator or pluck your hair for at least one month before the session and the day before the treatment you should shave your hair with a razor or using a depilatory cream. Pay attention to the drugs used in the weeks and days before the treatment as many medications such as antibiotics, anti-hypertensives, anti-inflammatories are photosensitizing and increase the risk of post-treatment hyperpigmentation. Do not use bleaching products before the sessions. Avoid creams, talc, deodorants, cosmetics as the skin needs to be clean.\nBefore the session the doctor will perform a physical examination to determine the settings to use on the device as well as will ask questions about your medical history, medication use, etc. to evaluate any contraindication. You physician may also perform a patch test on a small hidden area of your body to test your skin’s response to the treatment as well as the efficacy and safety of the chosen device settings.\nCooling techniques might be used to avoid pain and major discomfort, such as ice packs, cooling creams, cooling sprays or ventilation systems.\nAfter the treatment\nSun should be avoided for 6 weeks and the use of sunscreen is advised especially on unprotected areas such as the face. Ice packs can be applied to reduce erythema and edema as well as to attenuate the low pain or discomfort, which are normal following the treatment. It is best to avoid the use of creams, lotions, deodorants also on the day following the treatment. If after the session the physician should detect signs of excessive inflammation on the treated area, topical corticosteroids might be prescribed to lessen the side effects.\nComplications and Side Effects\nThe most common side effects of laser or IPL hair removal are erythema and edema. Pain, burns, blistering and scarring are also possible. Hypo- or Hyper-pigmentation are not rare, but in the majority of cases are just temporary. Paradoxical hypertrichosis, whose cause is not yet clear, is reported in medical literature. \nHow many sessions are needed for satisfactory results?\nThere is no general consensus among professionals regarding the number of treatments needed nor regarding the timespan between each treatment.\nThe number of treatments varies depending on the area to be treated, the patient’s characteristics and the device used.\nAs an estimate, about a 20-30% hair number reduction is achieved permanently with each treatment. Generally, 3 to 8 treatments are recommended to be performed 4 to 8 weeks apart from each other. \nHow much does it cost?\nIn a country like Thailand, the price varies between US$ 30-170 depending on the device used, the clinic where the treatment is performed and the size of the area to be treated.\nIn South Korea prices start at about US$ 45, with the same variables as above.\nTips and Advices\nLaser and IPL treatments are offered also by non-medical personnel, like beauticians, and in beauty salons and SPAs. The devices used in these cases have less power and lower efficacy when compared to the medical grade ones. Moreover, there won’t be any professional pre-treatment consultation, inspection of the area that needs the treatment and in general there is less guarantee of achieving satisfactory results and there is an increased risk of complications.\nThere are also a number of home devices sold for domestic use. These too have technical characteristics that make them far inferior to the medical grade ones. What might seem like a good saving in the beginning, might actually be a complete waste of money. \nThe best time of the year to perform this treatment is in Autumn or Winter, when the skin is not tanned and there is a lower risk of sun exposure.\n Vachiramon V, Brown T, Mcmchael AJ. Patiente satisfaction and complications following laser hair removal in ethnic skin. J Drugs Dermatol. 2012;11(2):191–5.\n Gan SD, Graber EM. Laser hair removal: a review. Dermatol Surg. 2013;39(6):823–38.\n Lim SPR, Lanigan SW. A review of the adverse effects of laser hair removal. Lasers Med Sci. 006;21:121–5.\n Casey AS, Goldberg D. Guidelines for hair removal. J Cosmet Laser Ther. 2008;10(1):24–33.\n Thaysen-Petersen D, Bjerring P, Dierickx C, Nash J,Town G, Haedersdal M. A systematic review of lightbased home-use devices for hair removal and considerations on human safety. J Eur Acad Dermatol Venereol. 2012;26(5):545–53.\nAlmeida Issa MC, Tamura B. Lasers, Lights and Other Technologies. Springer, Cham 2018. https://doi.org/10.1007/978-3-319-16799-2\nMedical Blogger committed to accuracy, I am passionate about delivering my readers critical insights into the medical field.\nFormer paramedic, instructor and medical school student.\nYou can read more about me in the dedicated section of the website."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:af2835e0-ace1-4d65-acf4-5dac0403ee07>","<urn:uuid:0d9f067c-3a36-45c7-8506-4e0cd4b550bd>"],"error":null}
{"question":"What different perspectives on WWI emerged between propaganda posters and later war literature?","answer":"Propaganda posters presented an idealized view of war by appealing to masculinity, heroism, and national pride while deliberately obscuring the realistic aspects of combat. In contrast, later war literature offered harsh depictions of battlefield realities. This shift is evident in poets like Wilfred Owen, whose 1917 work 'Anthem for Doomed Youth' portrayed soldiers 'dying as cattle,' and Siegfried Sassoon's 1918 'Counter-Attack,' which described corpses wallowing 'face downward, in the sucking mud.' This realistic portrayal aligned with artists like Nevinson who showed the grim reality of dead soldiers in the battlefield mud.","context":["- The Charge of the Light Brigade, by Alfred Lord Tennyson.\n- \"Honour the Charge they made!\" \"While horse and hero fell\" - patriotic, presents war positively.\n- \"Someone had blunder'd\" - blames Generals - interesting as it is before WWI, about the Crimean War (The Battle of Balaklava), a war with extensive trench warfare and more modern artillery use- a precursor to World War One in many tactical respects. So perhaps realistic in this sense?\n- Vitai Lampada, by Sir Henry Newbolt\n- Compare war to a game, \"But the voice of a schoolboy rallies the ranks, 'play up! play up! and play the game!'\"\n- Realistic, \"The sand of the desert is sodden red/Red with the wreck of a square that broke/The gatling's jammed and the colonel dead/And the regiment blind with dust and smoke\". Despite this, see above point; the euphemism of war as a game enters here, comparing it to cricket - idealised.\n- Henry V, by William Shakespeare.\n- Ok, so at first glance it seems increbily stupid. But, IIRC, this did come up in an exam paper once, so hey....\n- Idealises war\n- Theme of comradeship, \"We band of brothers\" - this can also be seen in some WWI poetry such as Owen's \"Greater Love\" and \"In Memorium\" by E. A. Mackintosh.\nThe Volunteer- Herbert Asquith 1912\nThis poem suggests that war is a glorious thing, and that it helps many people to achieve their life ambitions.\n- “Here lies a clerk who half his life had spent Toiling at ledgers in a city grey.”\n- “And now waiting dreams are satisfied…his lance is broken but he lies content…”\nEarly War literature (1914-1915)\n- Julien Grenfell is a good example; \"we are all awfully well, except those who have stopped something\", he wrote in a letter. \"Stopped something\" was slang for being shot!\n- Jessie Pope (a.k.a Owen's arch-nemesis! ) is another good one - incredibly pro-war.\n- \"Who's for the trench - are you, my laddie? Who'll follow the French - will you, my laddie?\" (The Call, Pope)\n- And Rupert Brooke's famous \"The soldier\" is also in this period, and rather idealised; \"some corner of a foreign field that is forever England\"\nFor the Fallen- Laurence Binyon 1914\nThis poem is anti war, which is unusual, although not unique for the time it was written. Unlike other anti war poems however, it shies away from the reality of war.\n- “England mourns for her dead across the sea.”\n- “They were staunch to the end against odds uncounted. They fell with their faces to the foe.”\n- \"They shall not grow old, as we that are left grow old. Age shall not weary them.”\nLater literature (1916-1918)\n- The poets that everyone knows, and I won't go into detail on - Sassoon and Owen. Glory of Women, *Dulce et Decorum Est, \"They\", The General, Disabled, Greater Love and Survivors are all good poems by these two.\n- \"In Flander's fields\" by John McCrae, while written in 1915, is still more realistic than its contemporaries, \"We are the Dead. Short days ago we lived, felt dawn, saw sunset glow\" - it's more in line (in themes etc.) with the later war poetry, but feel free to disagree with me\n- Isaac Rosenberg - \"Returning, we hear the larks\" - \"but hark! joy - joy - strange joy\" \"Death could drop from the dark as easily as song, but only song fell\"\nGlory of Women - Siegfried Sassoon 1917\nCriticises those at home, particularly the women.\n- “You love us when we’re heroes, home on leave…”\n- “Trampling the terrible corpses, blind with blood”\n- “O German mother…while you are knitting socks to send your son His face is trodden deeper in the mud.”\nWar Girls- Jessie Pope 1916\nVery jingoistic, praises the role of women during war time whilst the men were fighting.\n- “No longer caged and penned up, they’re going to keep their end up ‘Til the khaki soldier boys come marching back. “\n- “Beneath each uniform, beats a heart that’s soft and warm”\nA Dead Boche Robert Graves 1916\nWirtten from an anti war perspective, graphic descriptions show the true horror of war.\n- “’War’s hell’”\n- “Sat a dead Boche, he scowled and stunk”\n- “Big-bellied, spectacled, crop haired, Dribbling black blood from nose and beard”\nSuicide in the trenches - Sassoon\n- Written in a simple rhyming scheme (AABBCC etc) which suggests innocence (or the loss of) as it reminds us of a nursury rhyme.\n- Very anti war - \"You smug-faced crowds with kindling eye Who cheer when soldier lads march by,\"\n- Portrays the loss of youth and innocence gradually through each verse, which is a major typicality of anti-war poetry. - \"I knew a simple soldier boy\n- Who grinned at life in empty joy,\" at the beginning becomes \"In winter trenches, cowed and glum\"\nDisabled- Wilfred Owen 1917\nShows a strong anti-war view, criticises those at home who cannot see past the 'glory' of war. Poem shows a young boy who has been disabled by the war.\n- “Some cheered him home, but not as crowds cheer a goal”\n- “The women’s eyes passed from him to the strong men that were whole”\n- “Why don’t they come?”\nDulce et Decorum Est- Wilfred Owen 1917\nAgain anti-war, satirises the view that war is a glorious thing, and that it is an honour to die for ones country.\n- “Bent double, like beggars under sack, knock kneed and coughing like hags.”\n- “As under a green sea, I saw him drowning... gargling from the froth corrupted lungs”\n- “The old lie: Dulce et Decorum est Pro patria mori”\nBase Details- Siegfried Sassoon\nThis poem criticises those in charge of the war.\n- “if I were fierce and bald and short of breath…speed glum heroes up the line to death. “\n- “I’d toddle safely home and die- in bed\nThe Send Off- Wilfred Owen 1917\nThis poem has a melancholic tone, which has a sinister effect as the poem focuses on the death and destruction caused by war.\n- “lined the train with faced grimly gay”\n- \"Their breasts were stuck all white with wreath and spray, as mens are, dead.”\n- “like wrongs hushed up they went”\n- “A few, too few for drums and yells may creep back, silent… up half known roads.”\nYes, there's more!\n- Blackadder Goes Forth was used in an exam paper, and is good to quote, e.g. General Melchett (or General Insanity Melchett, as Blackadder calls him): \"Anyone can see he's as sane as I am! Baaaaah!\"\n- A quick google will bring up the script, and pretty much throughout there are links to later war poetry - the insanity of the generals, greater love, etc.\n- Journey's End is a very good play, with references to war as a game, with Raleigh's \"rugger\", love between officers and men when Osborne & Raleigh die, and realism through the deaths, Stanhope's alcohol problem, etc. Like Blackadder, open it at almost any page and you'll find a good quote to use.\n- Oh What a Lovely War. Another play, this is much more humorous than JE, but still as hard-hitting and good to use, \"Ye Gods! What's that?' 'Oh, it's a Jerry, sir' 'what?' 'It's a leg, sir' 'well, get rid of it man. You can't have an obstruction sticking out of the parapet like that!' 'Hartcastle, remove the offending limb' 'Well, we can't do that sir; it's holding up the parapet!\"\n- Recent books like Birdsong, Pat Barker's Regeneration trilogy (\"shotvarfet!\"), Strange Meeting, etc. can all be used as well.\nThese notes are aimed at A Level English Literature students at A2 level.\nOriginally written by Forgotmytea, *~vicky~* and Manalishi on TSR Forums.","A Brief Introduction to WWI and Its Representation\nWhen shots rang out in Sarajevo on June 28, 1914, Europe began hurtling towards one of the deadliest conflicts the world has ever seen. Gavrilo Princip’s assassination of Austro-Hungarian Archduke Franz Ferdinand and his wife, Sophie, brought long-brewing political tension to a head. By August 4, 1914, the Central Powers (Germany, Bulgaria, Austro-Hungary, and the Ottoman Empire) and the Entente or Allied Powers (France, Britain, Russia, Italy, and later the United States), were officially engaged in the First World War.\nThe crisis would last until November 11, 1918 and claim millions of lives, with battlefronts in Europe, European waters, and in the Middle and Near East. World War I, also known as the Great War, was a distinctly modern conflict in many ways. However, like wars throughout history, it inspired a tremendous amount of creative output from artists and writers, civilians and combatants, men and women. Though we may read about dates and numbers when we study historic conflicts, the lived reality of a war becomes much more vivid when we look at the diverse creative expressions it inspires. The varied perspectives represented in the First World War art show us that there was not one single uniform war experience for Britons, whether on the battlefield or at home. In fact, we could say that British artists and writers witnessed and experienced different wars even though only one conflict is recorded in history. War-related art also had many purposes, whether to document, commemorate, appeal, revise, expose, obscure, or protest.\nBritain at War, August 5, 1914, letterpress on newsprint, 76.3 x 50.7 cm, published by The Times (Imperial War Museum)\nThe Conflict Between Real and Ideal\nPropaganda posters often urged men to enlist in the British Army by appealing to ideals of masculinity, heroism, pride, and loyalty. These widely circulated posters relied on a viewer’s positive response to imagery of healthy, stalwart soldiers or emblems of the British nation. A propaganda poster asks its viewer to identify with what is depicted, which usually concerns political, gender, and/or social identity. As part of making such an appeal, propagandists often obscure more realistic aspects of combat or service.\nIn contrast, many war artists offered harsh but realistic visual depictions of the death and destruction that resulted from combat. For example, when we look at C.R.W. Nevinson’s stark painting, Paths of Glory, irony comes to the forefont. Though the piece has an idealistic-sounding title, we shudder at the sight of two dead soldiers lying in the battlefield mud. We cannot identify with, or even identify these soldiers at all. Their faces are obscured and their bodies merge with the murky earth, suggesting the loss of identity and the waste of young lives. The brownish grey mud almost threatens to rise up and swallow the entire scene.\nC.R.W. Nevinson, Paths of Glory, 1917, oil on canvas, 46.7 x 60.9 cm (Imperial War Museum, London)\nPaul Nash’s 1917 work, The Menin Road, depicts a ruined Belgian landscape. Before us, dead tree trunks rise in a wasteland of mud and standing water. This spooky, alienating, place includes strange clouds of smoke penetrated here and there by searchlights. Despite these beams of light, we cannot see anything past the immediate scene. Here is chaos, irrevocable change, and devastation.\nPaul Nash, The Menin Road, 1919, oil on canvas, 182.8 x 317.5 cm (Imperial War Museum, London)\nCarrying poetry to war\nFirst World War literature also presents a range of perspectives. Rupert Brooke’s patriotic ‘1914’ sonnet sequence became hugely popular in the early years of the war. At the outset of the war, many Britons were touched by the heroic sentiments of the poems, in particular, “The Soldier.” This poem’s combatant speaker assures the reader that his death in battle will mean that “there’s some corner of a foreign field/That is for ever England.” Brooke’s poems pictured military service and death as purifying and noble. At the start of the war, when such nationalistic feeling was strong, many British soldiers departed for training with a copy of Brooke’s poems tucked into their kits. However, after years of devastating losses and with no clear resolution to the seemingly endless fighting, poets depicting the hard reality of the soldier’s experience gained more recognition. Wilfred Owen’s gloomy 1917 “Anthem for Doomed Youth” pictures the war’s fallen “d[ying] as cattle,” for example. Siegfried Sassoon’s 1918 piece, “Counter-Attack,” offers us the gruesome vision of a battlefield “place rotten with dead” where corpses “face downward, in the sucking mud,/Wallow…” Sassoon’s shocking verbal image recalls the horrible tableau of Nevinson’s dead soldiers lying facedown in the mud.\nA Note on WWI and Modernism\nMark Gertler, Merry-Go-Round, 1916,oil on canvas, 189.2 x 142.2 cm (Tate)\nDuring the years leading up to the war, many modernists began to turn their attention to their media; writers and authors broke free of traditional parameters of form and imagery and brought the very materials of their crafts to the forefront. They questioned the solidity of the bond between representation and meaning. Works like T.S. Eliot’s poem, \"The Waste Land,\" Mark Gertler’s Merry-Go-Round (above), or Virginia Woolf’s Mrs. Dalloway sought to shock, alienate, or provoke audiences and to thereby explore new sensory and intellectual effects in art and literature.\nWhile the modernist movement had begun prior to the war, the conflict’s vast scale, brutality, and costs fascinated many artists and writers. The war definitively ended many social and cultural traditions that survived the nineteenth century and made clear the modern, mechanized world we were entering, a world where the older expressive forms and techniques no longer seemed adequate, appropriate, or compelling.\nWomen Writers and Artists\nWomen artists and writers played a significant role in documenting civilian and service experiences. Vera Brittain, who volunteered as a nurse, recorded her impressions of work and loss in her memoir, Testament of Youth, one of the war’s most recognized autobiographical works. Women artists documented other civilian realities such as female workers in factories—doing jobs vacated by men in the military— who had become crucial for war-related production.\nFlora Lion for example, shows us a canteen for women munitions (weapons) workers in her painting, Women’s Canteen at Phoenix Works, Bradford. We can see the exhaustion that the workers are feeling. The women here look somewhat relieved for their tea break. Their resigned expressions and slouching posture underscore the mental and physical fatigue of this critical but dangerous line of work, but they also make us recognize the more emotional weariness of the civilian war experience.\nFlora Lion, Women's Canteen at Phoenix Works, Bradford, 1918, oil on canvas, 106.6 x 182.8 mm (Imperial War Museum, London)\nWWI’s Aftermath: Public and Private Commemoration\nWhen the war concluded in November 1918, nearly a million Britons were dead. British soldiers killed in action were buried overseas, so that public officials and grieving families were challenged to represent both personal and national losses. To recognize individual sacrifices, the British government issued memorial bronze plaques and paper scrolls to the family of each serviceperson who died as a result of the war. And, on November 11, 1920, a solemn ceremony dedicated two of Britain’s most famous public war monuments, Edward Lutyens’s Whitehall Cenotaph and theUnknown Warrior, buried in Westminster Abbey.\nEdward Lutyens, Whitehall Cenotaph, unveiled 1920, portland stone, London (photo: Godot13, CC BY-SA 3.0)\nThe creative work of giving textual, visual, or plastic form to First World War experiences would go on into the 1930s and after. Even as Britain neared the\nfearful prospect of a second major international conflict, the Great War continued to haunt those who had lived through it.\nHorace Nicholls, The Cenotaph at Whitehall, 1920, photograph (Imperial War Museum, London)\nEssay by Dr. Fiona Robinson\nWant to join the conversation?\n- We read, \"As part of making such an appeal, propagandists often obscure more realistic aspects of combat or service.\"\nThe above sentence triggered some questions in my mind:\nIn order for imagery to be described as \"propaganda\", must it be false? In other words, could an image depict truths and still be considered propaganda? Is propaganda, like art, judged via the \"eye of the beholder\" or is there something more objective with regards to how we can look at \"propaganda\"?(2 votes)\n- The best propaganda uses the truth, or at least a small slice of the truth. Consider the media coverage of the former Yugoslavia countries in the early 1990s. While it was true that the Serbs were committing atrocities, the reporting neglected to mention all the history that led up to those events, including events that people alive at the time could remember.\nOr just look at the painting, 'Paths of Glory' in this article, an accurate depiction of what was going on in France, and potentially a powerful bit of anti-war propaganda.(3 votes)\n- In Paragraph II, C.R.W. Nevinson, Paths of Glory, 1917, oil on canvas, 46.7 x 60.9 cm (Imperial War Museum, London) shows a lot of realism in oil painting. What was the point in showing the dead and wasted bodies?? This is based on a war that had happened in-- well it didn't say when the war had happened specifically, but I think it had said probably WWI. Did this painting become famous very easily or did it become famous in the recent 2000's??(1 vote)\n- If Nevinson painted this in 1917, it would have been while the Great War was still going on. I don't how popular it became, or when. It likely was popular with those who were sick of the carnage, and the inept tactics and leadership of the British Army, and unpopular with the bunch who saw this as necessary for the glory of England. Just as an aside, nobody call that war World War I, possibly because no one could contemplate anything like that could happen again.(2 votes)\n- Regarding Paths of Glory; the author of this essay says \"We cannot identify with... these soldiers at all.\"\nIsn't it possible we are supposed to identify with the painter and his experience of war?\nDoesn't that statement rather exclude veterans from your intended audience? Do service members and veterans lack some essential quality to appreciate art?(0 votes)\n- The author's intent was not exclusionary in any way. The full sentence you quote is, \"We cannot identify with, or even identify these soldiers at all.\" This is because the two soldiers depicted are dead and half lost in the mud. We can't identify with them because they are shown lifeless and we can't identify them as particular individuals from our vantage point because they are obscured. This was all to the larger point to contrast with celebratory images of the Great War. The author made absolutely no statement suggesting that soldiers or veterans have less appreciation for art than anyone else.(3 votes)\n- Not a question, but I'm interested in knowing more about Flora Lion; this article has caught my interest. She seems like an Impressionist painter, but was dropped into the Futurism/Dada section...(1 vote)\n- I think that the date of the Menin Road is 1919, after the end of the War 11.11.18. the commission being given in April 1918, there are more details about it on the following link from the Imperial War Museum\n- Wasn't all of this is just mere propaganda to raise support and morale for the British during the war?(0 votes)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:de3547ff-827f-414d-add2-3ff3a895d08d>","<urn:uuid:75ee0461-901b-49cb-af52-4314361c4640>"],"error":null}