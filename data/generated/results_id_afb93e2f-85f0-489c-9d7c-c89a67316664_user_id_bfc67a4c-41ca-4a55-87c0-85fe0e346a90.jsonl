{"question":"How do researchers study stellar phenomena like coronal mass ejections, and what are their key findings?","answer":"Researchers study stellar phenomena using coordinated networks of instruments - for example, capturing the first stellar coronal loop image required 13 radio telescopes linked by computer, including arrays in Hawaii, Virgin Islands, Iowa, Germany, and New Mexico. For coronal mass ejections (CMEs), scientists have found they typically reach velocities between 20-3200 km/s with an average speed of 489 km/s. CMEs follow a pattern of slow pre-acceleration followed by rapid acceleration to a constant velocity. The frequency varies from one every five days during solar minimum to 3.5 per day during solar maximum. Advanced missions like NASA's WIND and STEREO spacecraft have enabled stereoscopic imaging and detailed measurements of solar wind particles and electromagnetic radiation.","context":["Jan. 13, 2010\nIMAGE: This is a composite image of a large radio coronal loop superimposed on an artist's conception of Algol, a binary star system. Credit: William Peterson.\nUI astronomers capture first-of-kind image at distant star\nTwo University of Iowa researchers have made the first direct radio image of a stellar coronal loop at a star, other than the sun, thereby providing scientists with information that may lead to a better understanding of how such phenomena as space weather affect the Earth.\nRobert Mutel, professor in the University of Iowa College of Liberal Arts and Sciences Department of Physics and Astronomy, and his graduate student William Peterson of Marshalltown, Iowa, spearheaded the research, which included astronomers from New Mexico and Switzerland. They published their findings in the Jan. 14 issue of the journal Nature.\nMutel said that the image of the coronal loop (roughly resembling a rainbow) was made of the star Algol, a well-known variable star in the constellation Perseus. Algol (Arabic for demon) is also known as the Demon Star and is one of the first eclipsing binary stars and variable stars to have been discovered. Its brightness as seen from Earth temporarily decreases roughly every 69 hours.\n\"We imaged the coronal loop using a global array of radio telescopes,\"\n\"Earlier attempts to image stellar coronal loops in visible light resulted in fuzzy blobs, but we used a global array of radio telescopes to make a series of images over a six-month period. High resolution radio interferometry allows us to image features which would otherwise be undetectable.\"\nThe instrument Mutel and Peterson used is actually a combination of 13 radio telescopes linked by computer. They include the 10-telescope VLBA (Very Long Baseline Array) composed of telescopes in Mauna Kea, Hawaii, St. Croix in the Virgin Islands, and North Liberty, Iowa; a 100-meter instrument at the Max Planck Institute for Radio Astronomy near Bonn, Germany; the National Radio Astronomy Observatory (NRAO) at Green Bank, W. Va.; and the NRAO's Very Large Array (VLA) in New Mexico.\nDespite the impressive coordination of telescopes dedicated to capturing information from Algol, making sense out of all the data is difficult. \"Learning how to take radio data and turn it into an image is a challenge,\" Peterson said.\nInterpreting the data is perhaps just as challenging. Mutel noted that the coronal loop at Algol is similar to those at the sun, but the magnetic field at Algol is about 1,000 times more powerful.\nPeterson said that the larger-than-predicted size of the coronal loop is probably due to the tidal effects of the companion star distorting the loop and stretching it. Additionally, the companion star causes the coronal loop to continually face the companion star.\nMutel said that a better understanding of Algol's coronal loops might help us to better understand the sun, something that could benefit a wide range of human activities.\n\"We really need to understand our sun,\" he said. \"The sun is close to us and can be studied, but it is only one star. By studying other stars, we will be able to put its behavior into a broader context.\n\"Coronal loops at the sun are associated with sunspots. Sunspots, in turn, are associated with space weather, a constant stream of charged particles flowing outward from the sun. The intensity of solar radiation can affect everything from communications systems that rely on satellites to the health of astronauts who must sometimes work in space.\"\nMutel said that future research likely will focus on obtaining coronal loop images at other stars.\n\"Perhaps we can work toward predictions of space weather. Maybe we can better understand the physics of space weather through a study of coronal loops,\" he said.\nSTORY SOURCE: University of Iowa News Services, 300 Plaza Centre One, Iowa City, Iowa 52242-2500\nMEDIA CONTACT: Gary Galluzzo, 319-384-0009, email@example.com","Coronal mass ejection\nCoronal mass ejections are often associated with other forms of solar activity, most notably solar flares, but a causal relationship has not been established. Most ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. Near solar maxima, the Sun produces about three CMEs every day, whereas near solar minima, there is about one CME every five days.\n- 1 Description\n- 2 Cause\n- 3 Impact on Earth\n- 4 Physical properties\n- 5 Association with other solar phenomena\n- 6 Theoretical models\n- 7 Interplanetary CMEs\n- 8 Related solar observation missions\n- 9 History\n- 10 See also\n- 11 References\n- 12 Further reading\n- 13 External links\nCoronal mass ejections release huge quantities of matter and electromagnetic radiation into space above the sun's surface, either near the corona (sometimes called a solar prominence), or farther into the planet system, or beyond (interplanetary CME). The ejected material is a plasma consisting primarily of electrons and protons, but may contain small quantities of heavier elements such as helium, oxygen, and even iron. The theory of heavier element emissions during a CME is speculative information and requires further verification. It is highly unlikely that a CME contains any substantial amount of heavier elements, especially considering that the sun has not yet arrived at the point of helium flash and thus cannot begin to fuse elements heavier than helium.\nRecent scientific research has shown that the phenomenon of magnetic reconnection is responsible for CME and solar flares. Magnetic reconnection is the name given to the rearrangement of magnetic field lines when two oppositely directed magnetic fields are brought together. This rearrangement is accompanied with a sudden release of energy stored in the original oppositely directed fields.\nOn the sun, magnetic reconnection may happen on solar arcades—a series of closely occurring loops of magnetic lines of force. These lines of force quickly reconnect into a low arcade of loops, leaving a helix of magnetic field unconnected to the rest of the arcade. The sudden release of energy in this reconnection causes the solar flare. The unconnected magnetic helical field and the material that it contains may violently expand outwards forming a CME. This also explains why CMEs and solar flares typically erupt from what are known as the active regions on the sun where magnetic fields are much stronger on average.\nImpact on Earth\nWhen the ejection is directed towards Earth and reaches it as an interplanetary CME (ICME), the shock wave of the traveling mass of solar energetic particles causes a geomagnetic storm that may disrupt Earth's magnetosphere, compressing it on the day side and extending the night-side magnetic tail. When the magnetosphere reconnects on the nightside, it releases power on the order of terawatt scale, which is directed back toward Earth's upper atmosphere.\nSolar energetic particles can cause particularly strong aurorae in large regions around Earth's magnetic poles. These are also known as the Northern Lights (aurora borealis) in the northern hemisphere, and the Southern Lights (aurora australis) in the southern hemisphere. Coronal mass ejections, along with solar flares of other origin, can disrupt radio transmissions and cause damage to satellites and electrical transmission line facilities, resulting in potentially massive and long-lasting power outages.\nHumans at high altitudes, as in airplanes or space stations, risk exposure to relatively intense cosmic rays. Cosmic rays are potentially lethal in high quantities. The energy absorbed by astronauts is not reduced by a typical spacecraft shield design and, if any protection is provided, it would result from changes in the microscopic inhomogeneity of the energy absorption events.\nA typical coronal mass ejection may have any or all of three distinctive features: a cavity of low electron density, a dense core (the prominence, which appears as a bright region on coronagraph images embedded in this cavity), and a bright leading edge.\nMost ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. These regions have closed magnetic field lines, in which the magnetic field strength is large enough to contain the plasma. These field lines must be broken or weakened for the ejection to escape from the sun. However, CMEs may also be initiated in quiet surface regions, although in many cases the quiet region was recently active. During solar minimum, CMEs form primarily in the coronal streamer belt near the solar magnetic equator. During solar maximum, they originate from active regions whose latitudinal distribution is more homogeneous.\nCoronal mass ejections reach velocities between 20km/s to 3200km/s with an average speed of 489km/s, based on SOHO/LASCO measurements between 1996 and 2003. The average mass is 1.6×1012kg. The values are only lower limits, because coronagraph measurements provide only two-dimensional data analysis. The frequency of ejections depends on the phase of the solar cycle: from about one every fifth day near the solar minimum to 3.5 per day near the solar maximum. These values are also lower limits because ejections propagating away from Earth (backside CMEs) can usually not be detected by coronagraphs.\nCurrent knowledge of coronal mass ejection kinematics indicates that the ejection starts with an initial pre-acceleration phase characterized by a slow rising motion, followed by a period of rapid acceleration away from the Sun until a near-constant velocity is reached. Some balloon CMEs, usually the slowest ones, lack this three-stage evolution, instead accelerating slowly and continuously throughout their flight. Even for CMEs with a well-defined acceleration stage, the pre-acceleration stage is often absent, or perhaps unobservable.\nAssociation with other solar phenomena\nCoronal mass ejections are often associated with other forms of solar activity, most notably:\n- Solar flares\n- Eruptive prominence and X-ray sigmoids\n- Coronal dimming (long-term brightness decrease on the solar surface)\n- EIT and Moreton waves\n- Coronal waves (bright fronts propagating from the location of the eruption)\n- Post-eruptive arcades\nThe association of a CME with some of those phenomena is common but not fully understood. For example, CMEs and flares are normally closely related, but there was confusion about this point caused by the events originating beyond the limb. For such events no flare could be detected. Most weak flares do not have associated CMEs; most powerful ones do. Some CMEs occur without any flare-like manifestation, but these are the weaker and slower ones. It is now thought that CMEs and associated flares are caused by a common event (the CME peak acceleration and the flare impulsive phase generally coincide). In general, all of these events (including the CME) are thought to be the result of a large-scale restructuring of the magnetic field; the presence or absence of a CME during one of these restructures would reflect the coronal environment of the process (i.e., can the eruption be confined by overlying magnetic structure, or will it simply break through and enter the solar wind).\nIt was first postulated that CMEs might be driven by the heat of an explosive flare. However, it soon became apparent that many CMEs were not associated with flares, and that even those that were often started before the flare. Because CMEs are initiated in the solar corona (which is dominated by magnetic energy), their energy source must be magnetic.\nBecause the energy of CMEs is so high, it is unlikely that their energy could be directly driven by emerging magnetic fields in the photosphere (although this is still a possibility). Therefore, most models of CMEs assume that the energy is stored up in the coronal magnetic field over a long period of time and then suddenly released by some instability or a loss of equilibrium in the field. There is still no consensus on which of these release mechanisms is correct, and observations are not currently able to constrain these models very well.\nCMEs typically reach Earth one to five days after leaving the Sun. During their propagation, CMEs interact with the solar wind and the interplanetary magnetic field (IMF). As a consequence, slow CMEs are accelerated toward the speed of the solar wind and fast CMEs are decelerated toward the speed of the solar wind. Fast CMEs (faster than about 500 km s−1) eventually drive a shock wave. This happens when the speed of the CME in the frame of reference moving with the solar wind is faster than the local fast magnetosonic speed. Such shocks have been observed directly by coronagraphs in the corona, and are related to type II radio bursts. They are thought to form sometimes as low as 2 Rs (solar radii). They are also closely linked with the acceleration of solar energetic particles.\nRelated solar observation missions\nNASA mission Wind\nOn 1 November 1994, NASA launched the WIND spacecraft as a solar wind monitor to orbit Earth's L1 Lagrange point as the interplanetary component of the Global Geospace Science (GGS) Program within the International Solar Terrestrial Physics (ISTP) program. The spacecraft is a spin axis-stabilized satellite that carries eight instruments measuring solar wind particles from thermal to >MeV energies, electromagnetic radiation from DC to 13 MHz radio waves, and gamma-rays. Though the WIND spacecraft is nearly two decades old, it still provides the highest time, angular, and energy resolution of any of the solar wind monitors. It continues to produce relevant research as its data has contributed to over 150 publications since 2008 alone.\nNASA mission STEREO\nOn 25 October 2006, NASA launched STEREO, two near-identical spacecraft which from widely separated points in their orbits are able to produce the first stereoscopic images of CMEs and other solar activity measurements. The spacecraft orbit the Sun at distances similar to that of Earth, with one slightly ahead of Earth and the other trailing. Their separation gradually increased so that after four years they were almost diametrically opposite each other in orbit.\nThe largest recorded geomagnetic perturbation, resulting presumably from a CME, coincided with the first-observed solar flare on 1 September 1859, and is now referred to as the Carrington Event, or the solar storm of 1859. The flare and the associated sunspots were visible to the naked eye (both as the flare itself appearing on a projection of the sun on a screen and as an aggregate brightening of the solar disc), and the flare was independently observed by English astronomers R. C. Carrington and R. Hodgson. The geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a crochet, an instantaneous perturbation of Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays by Röntgen and the recognition of the ionosphere by Kennelly and Heaviside. The storm took down parts of the recently created US telegraph network, starting fires and shocking some telegraph operators.\nFirst clear detections\nThe first detection of a CME as such was made on 14 December 1971, by R. Tousey (1973) of the Naval Research Laboratory using the seventh Orbiting Solar Observatory (OSO-7). The discovery image (256 × 256 pixels) was collected on a Secondary Electron Conduction (SEC) vidicon tube, transferred to the instrument computer after being digitized to 7 bits. Then it was compressed using a simple run-length encoding scheme and sent down to the ground at 200 bit/s. A full, uncompressed image would take 44 minutes to send down to the ground. The telemetry was sent to ground support equipment (GSE) which built up the image onto Polaroid print. David Roberts, an electronics technician working for NRL who had been responsible for the testing of the SEC-vidicon camera, was in charge of day-to-day operations. He thought that his camera had failed because certain areas of the image were much brighter than normal. But on the next image the bright area had moved away from the Sun and he immediately recognized this as being unusual and took it to his supervisor, Dr. Guenter Brueckner, and then to the solar physics branch head, Dr. Tousey. Earlier observations of coronal transients or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.\nOn 1 August 2010, during solar cycle 24, scientists at the Harvard-Smithsonian Center for Astrophysics (CfA) observed a series of four large CMEs emanating from the Earth-facing hemisphere of the Sun. The initial CME was generated by an eruption on 1 August that was associated with sunspot 1092, which was large enough to be seen without the aid of a solar telescope. The event produced significant aurorae on Earth three days later.\nOn 31 August 2012 a CME connected with Earth's magnetic environment, or magnetosphere, with a glancing blow causing aurora to appear on the night of 3 September. Geomagnetic storming reached the G2 (Kp=6) level.\n- List of Coronal Mass Ejections\n- Forbush decrease\n- Health threat from cosmic rays\n- Magnetic cloud\n- Orbiting Solar Observatory\n- Solar and Heliospheric Observatory\n- Space weather\n- Christian, Eric R. (5 March 2012). \"Coronal Mass Ejections\". NASA.gov. Retrieved 9 July 2013.\n- Nicky Fox. \"Coronal Mass Ejections\". Goddard Space Flight Center @ NASA. Retrieved 2011-04-06.\n- \"Coronal Mass Ejections: Scientists Unlock the Secrets of Exploding Plasma Clouds On the Sun\". Science Daily.\n-  NASA Science\n- \"The Mysterious Origins of Solar Flares\", Scientific American, April 2006\n- Baker, Daniel N., et al. (2008). Severe Space Weather Events – Understanding Societal and Economic Impacts: A Workshop Report. National Academies Press. p. 77. ISBN 978-0-309-12769-1. \"These assessments indicate that severe geomagnetic storms pose a risk for long-term outages to major portions of the North American grid. John Kappenman remarked that the analysis shows \"not only the potential for large-scale blackouts but, more troubling, ... the potential for permanent damage that could lead to extraordinarily long restoration times.\"\"\n- Wired world is increasingly vulnerable to coronal ejections from the Sun, Aviation Week & Space Technology, 14 January 2013 issue, pp. 49–50: \"But the most serious potential for damage rests with the transformers that maintain the proper voltage for efficient transmission of electricity through the grid.\"\n- Wilson, J. W.; Wood, J. S.; Shinn, J. L.; Cucinotta, F. A.; Nealy, J. E. A proposed performance index for galactic cosmic ray shielding, materials. Washington, DC: NASA; Report No. TM-4444; 1993a\n- Carroll, Bradley W.; Dale A. Ostlie (2007). An Introduction to Modern Astrophysics. San Francisco: Addison-Wesley. p. 390. ISBN 0-8053-0402-9.\n- Andrews, M. D., A search for CMEs associated with big flares, in Solar Physics, 218, pp. 261–279, 2003\n- Vourlidas, A., Wu, S.T., Wang, A. H., Subramanian, P., Howard, R. A. \"Direct Detection of a Coronal Mass Ejection-Associated Shock in Large Angle and Spectrometric Coronagraph Experiment White-Light Images\" in the \"Astrophysical Journal\", 598, 2, 1392–1402, 2003\n- Manchester, W. B., IV, T. I. Gombosi, D. L. De Zeeuw, I. V. Sokolov, ;, Roussev I., I., K. G. Powell, J. Kóta, G. Tóth, and T. H. Zurbuchen (2005). Coronal Mass Ejection Shock and Sheath Structures Relevant to Particle Acceleration. The Astrophysical Journal, Volume 622, Issue 2, pp. 1225–1239. 622 2: 1225–1239\n- Spacecraft go to film Sun in 3D BBC news, 2006-10-26\n-  NASA Mission Page, STEREO\n- AW & ST, Wired world . .\n- R.A. Howard, A Historical Perspective on Coronal Mass Ejections\n- Obit with brief bio for Dr. Brueckner\n- \"NASA's SDO Sees Massive Filament Erupt on Sun\". NASA. 4 September 2012. Retrieved 11 September 2012.\n- \"August 31, 2012 Magnificent CME\". NASA/Goddard Space Flight Center. 31 August 2012. Retrieved 11 September 2012.\n- \"Space Weather Alerts and Warnings Timeline: September 1–16, 2012 (archive)\". NOAA. Retrieved 24 September 2012.\n- Chillymanjaro (6 September 2012). \"Geomagnetic storming levels back to normal\". The Watchers. Retrieved 11 September 2012.\n- Gopalswamy, Natchimuthukonar; Mewaldt, Richard A; Torsti, Jarmo, eds. (2006). Solar Eruptions and Energetic Particles. Geophys. Monograph Series 165. Am. Geophys. Union. doi:10.1029/GM165. ISBN 0-87590-430-0.\n- Internet articles\n- Bell, Trudy E; Phillips, Tony (6 May 2008). \"A Super Solar Flare\". Science@NASA. NASA.gov.\n- Phillips, Tony (27 May 2008). \"Cartwheel Coronal Mass Ejection\". Science@NASA. NASA.gov.\n- Odenwald, Sten F; Green, James L (28 July 2008). \"Bracing the Satellite Infrastructure for a Solar Superstorm\". Scientific American.\n- Lavraud, Benoit; Masson, Arnaud (21 November 2007). \"Cluster captures the impact of CMEs\". ESA Science & Technology. ESA.int.\n- Morring Jr., Frank (14 January 2013). \"Major Solar Event Could Devastate Power Grid\". Aviation Week & Space Technology.\n|Wikimedia Commons has media related to Coronal mass ejection.|\n- NOAA/NWS Space Weather Prediction Center\n- Coronal Mass Ejection FAQ\n- STEREO and SOHO observed CME rate versus the Sunspot number (PNG plot) / (text version)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:413c69b6-f78a-4095-8001-1d37931a829a>","<urn:uuid:d5f7cd70-9e3a-439b-a3a7-e16bb9abdede>"],"error":null}
{"question":"What is the relationship between strategic business risks and credit expansion in the financial sector, and what are the potential consequences of excessive credit growth?","answer":"Strategic business risks arise from competition, market forces, and unexpected fluctuations in volumes, margins, and costs, affecting earnings volatility and long-term business goals. These risks can emerge from choosing wrong strategies or slow reactions to market changes. Regarding excessive credit expansion, when there is large bank credit growth, funds may flow to borrowers with poor credit quality, including both households and non-financial firms. This reduced borrower quality exposes banks to increased default risks, which might only become apparent after significant economic deterioration.","context":["Principles of Risk Management\nRisk management aims to identify threats that may affect the implementation of the Group’s strategy or jeopardise the achievement of the capital adequacy or profitability targets. The objective of risk management is to minimise the probability of unexpected monetary losses and realisation of reputational risk. The parent company`s Board of Directors has confirmed the company’s risk management policy and supervises risk management.\nThe organisational structure is flat, and each employee has one of the company’s partners as his or her superior. Thus, risk management is the responsibility of each employee, and each employee must understand the risks related to his or her duties and try to minimise them.\nTogether with other compliance officers, the Group’s General Counsel ensures that the Group operates in compliance with legislation and official regulations. The Group has a risk management function which prepares matters related to risk management and reports them to the parent company’s Board of Directors. The Board provides risk management guidelines in internal instructions upon request by the Group’s risk management function. Compliance with the instructions is monitored through inspections carried out according to the relevant rules and regulations.\nThe Group’s business is not deliberately exposed to market, credit or counterparty risks. Instead, the company’s operations in its key business areas (investment banking and private equity funds) are fee-based. The company also has a low risk exposure when investing its own assets.\nThe following sections describe the main risks of investment service companies from the viewpoint of the ICECAPITAL Group.\nRealisation of a credit risk means that a counterparty has failed to fulfil its contractual financial obligations. The Group’s counterparties include partners and customers who purchase its services. The impact of credit risk on the company’s own assets is minor as the Group’s customers are solvent and have a well-established position.\nMarket risks arise from fluctuations in the equity, fixed-income, real estate and currency markets and their impact on the Group’s results. The impact on the ICECAPITAL Group is fairly minor due to the investment policy applied by the company. The company’s assets have been invested with moderation and a long-term investment span.\nLiquidity risk refers to a situation in which the company’s cash and cash equivalents are insufficient and no additional financing is available. The ICECAPITAL Group is highly solvent. In practice, the bulk of the Group’s assets is held in bank deposits and in liquid shares. Moreover, since the Group has relatively steady and predictable cash outflows, it can prepare well for its liquidity needs. Consequently, liquidity does not constitute one of the Group’s main risks.\nOperational Risks and Reputational Risk\nIf they materialise, operational risks cause damage as a result of inadequate or incorrect routines, processes or conduct, or through unexpected external events. Operational risks also include legal risks, such as invalidity of contracts already signed and damage caused by incomplete documentation or violation of laws or official regulations. The Group’s expenses from operational risks have been low to date.\nOperational risks are monitored continuously, and the actual risks are reported. The business units are responsible for managing the operational risks of their own area. The results of the self-assessment of operational risks carried out by the units are used as a basis when trying to prevent operational risks. Efforts are made to reduce the risks observed by developing internal instructions and processes, training personnel and auditing operations. In addition, the ICECAPITAL Group has insurance to cover damage caused by mistakes, misconduct and criminal activity. Contingency planning keeps the Group prepared for any major interference in operations.\nOne of the most significant types of damage caused by operational risks could be reputational risk concerning the Group. Operating in the financial markets requires the absolute trust of customers and partners, and the realisation of reputational risk could have a detrimental effect on the Group’s business. In order to avoid this risk, ICECAPITAL monitors and follows legislation concerning its operations and good market practice and strives to continuously develop its procedures.\nStrategic Risk and Business Risk\nStrategic risks and business risks arise from competition, internal pressures or market forces which result in unexpected fluctuations in volumes, margins and costs, thus affecting the volatility of earnings and the achievement of long-term business goals. They may also arise from opting for a wrong strategy, from mismanagement and inadequate monitoring or from slow reaction to changes in the operating environment.\nThe Group forms an opinion about new products and services, even if future demand for them is uncertain. One of ICECAPITAL’s core competencies is predicting the future needs of investors and companies before others do. To minimise the risk, all significant strategic decisions and investments are made only after comprehensive surveys and risk assessments.\nThe ICECAPITAL Group’s international risks are related to the ICECAPITAL Nordic Secondary Fund S.C.A., SICAR fund established in 2010. Although external investors in the funds accept the risk of losses with the hope of future profits, the occurrence of significant losses would have a detrimental impact on the company’s reputation.\nThe amount of assets tied up in ICECAPITAL Securities (Sweden) AB, which provides Corporate Finance advisory services in Sweden, is reasonable in view of the Group´s solvency and will not jeopardise the continuity of operations.\nAccording to the Capital Requirements Regulation (Regulation [EU] No. 575/2013), the requirements for own funds must be met as follows: CET1 capital must account for at least 4.5%, T1 capital for at least 6% and total capital for at least 8 % of the total risk exposure amount. On 31 December 2019, ICECAPITAL Securities Ltd´s total capital accounted for 94,4 per cent of the total risk exposure amount.\nICECAPITAL has drawn up an equity plan in accordance with the capital adequacy regulations, which describes the risk-based capital needs, sufficiency of capital and solvency. The Group aims to maintain a capital adequacy ratio at least 1.5 times that required.","What does expansion of credit mean?\nWe call this the “credit-driven household demand channel.” An expansion in the supply of credit occurs when lenders either increase the quantity of credit or decrease the interest rate on credit for reasons unrelated to borrowers’ income or productivity.\nWhat are the steps of credit creation process?\nA bank keeps a certain part of its deposits as a minimum reserve to meet the demands of its depositors and lends out the remaining to earn income. The loan is credited to the account of the borrower. Every bank loan creates an equivalent deposit in the bank. Therefore, credit creation means expansion of bank deposits.\nWhat is the credit process?\nThe process of assessing whether or not to lend to a particular entity is known as the credit process. It involves evaluating the mindset of the potential borrower, underwriting of the risk, the pricing of the instrument and the fit with the lenders portfolio.\nIs credit expanding or contracting?\nThe credit cycle is the expansion and contraction of access to credit over time. This, in turn, can threaten the solvency and profitability of the banking system itself, resulting in a general contraction of credit as lenders attempt to protect themselves from losses.\nHow long is the credit cycle?\nThe National Bureau of Economic Research (NBER) reports that there have been 11 credit cycles since World War II with an average length of 69 months. The current run of 76 months in the expansion phase tops the average length of an entire cycle.\nWhat were the results of this excessive expansion of credit?\nWhen there is a large bank credit expansion in the economy, credit may flow to borrowers with poor credit quality, either households or non-financial firms. Reduced borrower quality exposes banks to increased default risks, which may be realized only after a substantial deterioration in the economy.\nWhat is the formula of credit creation?\nTotal Credit creation = Initial deposits x 1/LPR. Money Multiplier: It means the multiple by which total deposit increases due to initial (primary) deposit. Money multiplier (or credit multiplier) is the inverse of Legal Reserve Ratio (LRR). If LRR is 10%, i.e., 10/100or 0.1, then money multiplier = 1/0.1 = 10.\nWhat is RBI function?\nIn the Indian context, the basic functions of the Reserve Bank of India as enunciated in the Preamble to the RBI Act, 1934 are: “to regulate the issue of Bank notes and the keeping of reserves with a view to securing monetary stability in India and generally to operate the currency and credit system of the country to …\nWhat is the 5 Cs of credit?\nUnderstanding the “Five C’s of Credit” Familiarizing yourself with the five C’s—capacity, capital, collateral, conditions and character—can help you get a head start on presenting yourself to lenders as a potential borrower. Let’s take a closer look at what each one means and how you can prep your business.\nWhat are the 7 Cs of credit?\nThe 7Cs credit appraisal model: character, capacity, collateral, contribution, control, condition and common sense has elements that comprehensively cover the entire areas that affect risk assessment and credit evaluation.\nWhat is credit tightening?\nFrom Wikipedia, the free encyclopedia. A credit crunch (also known as a credit squeeze, credit tightening or credit crisis) is a sudden reduction in the general availability of loans (or credit) or a sudden tightening of the conditions required to obtain a loan from banks.\nHow long is a credit cycle?\nWhat is credit expansion and how does it work?\nCredit expansion is the policy where the central bankproduces additional moneyin order to purchase debtfrom the governmentor from entrepreneurs, such as banks. In a system where gold is used as money there exist strict limits for money producers when it comes to creditexpansion, due to the natural scarcity of the precious metal.\nHow to explain the process of credit creation?\nTo explain the process of credit creation, we make the following assumptions: 1. There are many banks, say A, B, C, etc. in the banking system. 2. Each bank has to keep 10 per cent of its deposits in reserves. In other words, 10 per cent is the required ratio fixed by law. 3.\nWhen did the policy of credit expansion begin?\nThe policy of credit expansion has been pursued by governments time and time again. It has become prevalent in the United States under President Woodrow Wilson after the establishment of the Federal Reserve Bank under the Federal Reserve Act during the Christmas Holiday of December 1913.\nHow do banks create credit in the economy?\nEvery bank loan creates an equivalent deposit in the bank. Therefore, credit creation means expansion of bank deposits. The two most important aspects of credit creation are: Liquidity – The bank must pay cash to its depositors when they exercise their right to demand cash against their deposits. Profitability – Banks are profit-driven enterprises."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:1c4cc122-4c22-4b97-8edc-bcec51d231a8>","<urn:uuid:9b5a6cd7-3e29-49a1-b43a-9cfbf7b86939>"],"error":null}
{"question":"What are the applications of optical tweezers in biological research, and how do they compare to traditional gene insertion methods?","answer":"In biological research, optical tweezers have multiple applications, particularly in studying DNA mechanics. They are used to investigate how genetic information in chromosomal DNA is read during cell reproduction by mechanically unwinding DNA from histones. Scientists can observe distinct unwinding stages and measure the forces required at each stage. For gene insertion, optical tweezers offer a more precise alternative to traditional methods. While conventional techniques like gene guns or laser perforation treat large populations of cells with limited control and potential cell damage, optical tweezers combined with femtosecond lasers allow researchers to precisely manipulate individual cells. This enables them to study exactly how gene transfection works in single cells, though the success rate is lower than mass transfection methods.","context":["Unwind a chromosome to see how it’s put together? Sort cells with a light beam? Make a model of a molecular motor? All these and more—welcome to the world of optical tweezers, where cells and even individual molecules are manipulated with laser light.\nHow does it work? When light passes through an object, the light refracts, that is, it changes direction. It does this as it enters the object, as it passes from one substance to another inside, and as it exits. Photons, the quanta of light, carry momentum, and the light’s momentum is changed by being bent as it passes through the object. To conserve the total momentum, the object itself acquires momentum equal to that lost by the photons, and this momentum can be used to move the object into a trap in the optical system.\nTo make a trap, a laser beam is set up with an intensity that diminishes moving out from the center of the beam, as shown in the diagram. Lenses bring the beam to a focus, the point of maximum light intensity.\nThe next diagram shows a translucent sphere placed in the laser beam between the lens and the focus. The solid and dotted line show the paths of a typical pair of photons that pass through the object. Because the two photons wind up going more sideways than initially, they lose momentum in the downward direction. To conserve momentum, the object acquires momentum in the downward direction, towards the focus.\nBeam Intensity— A schematic drawing of the laser beam brought to a focus in optical tweezers. The beam intensity decreases going out from the beam axis.\nTrapped Sphere — Two light paths in the optical tweezer laser beam passing through a translucent object. Note that the path that originates closer to the center of the beam will have the greater number of photons per second. The changes in momentum of photons on these paths gives the sphere momentum toward the beamaxis and also toward the focus .\nPhysicists use optical tweezers to investigate how the genetic information in chromosomal DNA is “read” during cell reproduction. The base sequences that carry the genetic code lie between the two sugar-phosphate backbones of the DNA molecule, and these bases must be exposed to replicate the DNA. In the chromosome, though, the DNA is wound around and around proteins, which in higher organisms are called histones. This wrapping is functional, since it enables the long DNA molecules to fit into the cell, but the DNA must be unwrapped before it is copied. In life, the enzyme DNA polymerase makes the copy, and associated proteins unwrap the DNA biochemically from the histone.\nOptical tweezers hold up one end of the DNA/histone complex, while the other end is fastened to a microscope slide. The slide is slowly moved to unwrap the DNA from the histone. (drawing courtesy of Michele Wang/Cornell University)\nA model of how DNA unwraps. Note that initially several turns of the DNA are wrapped around the histone, and that only after complete unwrapping does the histone break off. (image © Cornell University, used with permission).\nA group of biophysicists at Cornell and the University of Massachusetts investigated the beginning of this process by unwinding DNA from the histone mechanically. They attached one end of a DNA strand to a microscope slide and the other to a tiny transparent sphere. Then they pulled on the sphere with optical tweezers, like a mini tractor beam, to unwind the DNA, as shown in the diagram.\nThe group found that the DNA unwound in distinct stages, with increasing force required at each stage (see second diagram). Remarkably, even with 156 bases unwound from the histone, releasing the force enabled the DNA to spontaneously rewrap and return to its original state. These results showed that the enzymes that unwrap the DNA must be “molecular motors,” capable of exerting forces, and some of these have indeed been found.\nThis work is just one of the many applications of optical tweezers.\nUniversity of Chicago\nLawrence Berkeley Laboratory\nSt. Andrews University\nEight spheres are manipulated into a cubic arrangement with overlapping optical tweezers. When the laser is turned off, the cubes gradually lose their order. (image courtesy of Optics.org)","The applications of gene therapy and genetic engineering are broad: everything from pet fish that glow red to increased crop yields worldwide to cures for many of the diseases that plague humankind. But realizing them always starts with solving the same basic scientific question - how to \"transfect\" a cell by inserting foreign DNA into it. Many methods already exist for doing this, but they tend to be clumsy and destructive, not allowing researchers to precisely control how and when they insert the DNA or requiring them to burn through large numbers of cells before they can get it into one.\nA team of scientists in South Korea have now developed the most precise method ever used to insert DNA into cells. The method combines two high-tech laboratory techniques and allows the researchers to precisely poke holes on the surface of a single cell with a high-powered \"femtosecond\" laser and then gently tug a piece of DNA through it using \"optical tweezers,\" which draw on the electromagnetic field of another laser. The team's approach, which is a breakthrough in precision and control at the single-cell level, was published today in the Optical Society's (OSA) open-access journal Biomedical Optics Express.\n\"What is magical is that all this happens for one cell,\" said Yong-Gu Lee, an associate professor in the School of Mechatronics at the Gwangju Institute of Science and Technology in South Korea and one of the researchers who carried out the study. \"Until today, gene transfection has been performed on a large quantity of agglomerate cells and the outcome has been observed as a statistical average and no observations have been made on individual cells.\"\nCommon techniques to force DNA into cells can be clumsy or even violent, Lee said. For instance, researchers often use so-called \"gene guns\" to fire particles coated with strands of DNA known as plasmids at large populations of cells. Alternatively, scientists may puncture the membranes of individual cells with lasers, place the cells in a plasmid soup, and let the genes diffuse into the perforated cells on their own. While either method can transfect some fraction of a population, researchers cannot control whether any individual cell will incorporate the desired genes, and large numbers of cells may be damaged or destroyed in the process.\nIn the new study, the researchers sought to safely transfect an individual cell. To manipulate the foreign DNA, the scientists used optical tweezers, which essentially tweaks a laser beam whose electromagnetic field can grab hold of and transport a plasmid-coated particle. The researchers first moved the particle to the surface of the cell membrane. Guided by the trapped particle, they then created a tiny pore in the cell membrane using an ultra-short laser pulse from a femtosecond laser. While another laser beam detected the exact location of the cell membrane, they pushed the particle through the pore with the tweezers. Using this technique, the scientists were able to ease a microparticle right up to the pore in the membrane and drop it into the cell, like a golfer sinking an easy putt.\nTo determine whether their method had succeeded, the researchers inserted plasmids carrying a gene that codes for a green fluorescent protein. Once inside the cell, the gene became active and the cell's machinery began producing the protein. The researchers could then detect the green glow using a fluorescence microscope. They found that approximately one in six of the cells they studied became transfected. This rate is lower than that recorded for some other methods, but those are less precise and involve many cells at a time.\nLee hopes the work will allow other researchers to investigate the effects of transfection on individual cells, not just large populations. With the new technique, \"you can put one gene into one cell, another gene into another cell, and none into a third,\" he said. \"So you can study exactly how it works.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:f5dd9362-cac3-4867-b4d5-7b2e7f289553>","<urn:uuid:8190e65b-97d3-4713-95c0-096dc187cdac>"],"error":null}
{"question":"How do the early career paths of Carlos Alcaraz and Lydia Ko compare in terms of sport specialization?","answer":"Lydia Ko and Carlos Alcaraz had different approaches to early specialization. Ko's family decided at age 5 that she would focus solely on golf, leading her to become the youngest golfer of either gender to be ranked #1 in the world at age 17. In contrast, Alcaraz, while showing early tennis talent at age four, continued developing his skills more gradually, winning his first tournament at age eight and turning professional at age twelve, before reaching world #1 ranking at age 19.","context":["To rerun this excerpt/article in a magazine, newspaper, website, or e-newsletter with permission from Human Kinetics, Inc., please contact the marketing department at 1-800-747-4457 or MarkA@hkusa.com.\nWe now live in an age where it is common to see young athletes being pushed to play a single sport year-round. David Leadbetter, coach of the world's most dominant golfer Lydia Ko, recently shared that her family decided at age 5 she was going to be a golfer. It's hard to argue with the results. At 17 Ko became the youngest golfer, of either gender, to be ranked #1 in the world and is the first woman golf player to earn at least $2 million in prize money each of her first three years on tour.\nNo sport or country has escaped the global push to pressure young athletes and their families to pick just one sport year-round. Oftentimes coaches are the source of such pressure.\nFor coaches who still push for early sport specialization and point to the Lydia Ko's of the world as the path to sporting excellence, Jordan Speith provides an equally compelling counterargument. Speith, like Ko, attained the world #1 ranking in golf at an early age. At the age of 22 he became the youngest winner of the U.S. Open in nearly 100 years. The following year he set the record for annual earnings by winning over $22 million on the PGA tour, including a $10 million bonus for finishing first in the FedEx Cup standings.\nYet, unlike Ko, Speith's parents decided to avoid early specialization in golf even though Jordan wanted to play golf year-round at an early age. As his mom, Chris Speith, explained, “He hung up his golf clubs during football and baseball season—he was a quarterback and a pitcher—two pretty big roles. Then, he'd pick up golf again when summer rolled around.”\nSo, why does early specialization persist despite all the scientific evidence and position statements by leading sport organizations to the contrary? Yes, gymnastics and figure skating favor early specialization because maneuvers possible only at a young age are more highly rewarded, but those sports are the exceptions. The consensus is that athletes should refrain from specializing in one sport year-round as long as they can; at least until age 14 and preferably all the way through high school.\nWise coaches understand the long-term value of playing multiple sports and prize multi-sport athletes. The growing list of multisport athlete proponents includes highly regarded coaches like Joe Maddon and Mike Matheny in baseball, and Nick Saban and Urban Meyer in football. Super Bowl and college football championship coach Pete Carroll said it best when he shared what he looks for when recruiting athletes for his teams: “The first questions I'll ask about a kid are, ‘What other sports does he play? What are his positions? Is he a big hitter in baseball? Is he a pitcher? Does he play hoops?' I really, really don't favor kids having to specialize in one sport.”\nSome of the many benefits of playing multiple sports include fewer overuse injuries, improved decision-making skills and mental health, better overall athleticism, a more diverse peer social network, and exposure to different coaching styles. It should come as no surprise then that there is an over-representation of athletes from small towns in every professional sport league. Attending school in a small town often requires athletes to play multiple sports just to ensure schools can field teams in each sport.\nAlthough wise coaches encourage athletes to play multiple sports we must acknowledge the realities of this early specialization age. The pressure put on athletes and their parents from those who run elite travel clubs and coaches who are concerned about losing their best athletes to other sports is very real and increasing all the time. The following suggestions are offered to help coaches push back against the early specialization trend.\n1. Require an off season\nAt minimum coaches should consider enforcing a 75:25 guideline of participation in a single sport each year. Consider mandating an annual off season that is four months (25%) long. This guideline for participation in a primary sport will help athletes stay fresh and return to their primary sport with renewed energy and enthusiasm. When coaching athletes 12 and under consider adjusting the ratio to 50:50 (6 months each year for an off season from a primary sport). At younger ages it is even more critical to take time away from a single sport to help build a strong foundation of fundamental movement and basic sport skills that will transfer across sport, while reducing the likelihood of injuries that occur from repeating the same movements and skills all the time. A mandatory off season does not mean no sport – it means transferring to other activities for at least part of the year.\n2. Mandate multiple position or event sampling within a sport\nSome sports lend themselves to this strategy more than others. In baseball it is relatively easy to require athletes to play an infield and an outfield position, and in track & field to have them participate in both a track and a field event. Playing multiple positions helps athletes build better overall ‘game sense' because they develop knowledge of the demands of other positions. For example, soccer players allowed to play as a defender as well as forward will gain a better appreciation of how to support their teammates and anticipate ball movement. Playing different positions in a sport builds athlete confidence and willingness to step into any role needed to best support the team, which is especially helpful when teammates get injured or are absent. Conversely, early position specialization erodes young athletes' willingness to take risks and step out of their comfort zone to support their team.\n3. Model multisport participation as a coach\nThe most powerful way to teach others about the value of multisport participation is to model it yourself as a coach. In school sport settings, if the primary sport you coach is a fall season sport, ask to join the coaching staff of a different sport in the winter or spring season. Assuming the head coaching role for multiple sports each year may be impractical in many settings, but coaches can easily make time to coach alongside coaches in other sports, serving as assistants or position coaches. Besides serving as a powerful message to your athletes and their parents, this strategy has the added benefit of growing your coaching network and building a deeper repertoire of coaching skills. Building a multisport coaching network also brings awareness of the demands and opportunities unique across a range of sports. This awareness can then be used to help inform parents and athletes of suitable off season sport options to pursue, and enhance your ability to implement strategy #1.\n4. Raise parents' awareness\nNone of the aforementioned coaching strategies will work unless parents of the athletes you coach are informed about the perils of early sport specialization and the value of multisport participation. Although coaches can educate themselves about these issues by reviewing the sources provided at the end of this commentary, most parents are unlikely to have the time or interest in sifting through the literature. Instead, armed with what you learn from reading the literature you can set aside a few minutes at the preseason parent meeting to share your insights. Provide a 1-page written summary with links to online resources to give parents an opportunity to digest the information and to dig deeper if they so desire. For example, basketball coaches can share the recently launched Youth Basketball Guidelines webpage created by USA Basketball and the NBA. I also highly recommend coaches of any sport to have parents take the 12-item free online sport specialization test.\nDespite promises of athletic glory, athletic scholarships and the outlier athletes like Lydia Ko, the risks and drawbacks associated with playing a single sport year-round far outweigh the potential benefits claimed by those who push early sport specialization. So take a stand against this trend, and be a coach who best serves your athletes and your program by actively promoting multisport participation.\nBergeron, M. F., Mountjoy, M., Armstrong, N., Chia, M., Côté, J., Emery, C., … & Engebretsen, L. (2015). International Olympic Committee consensus statement on youth athletic development. British Journal of Sports Medicine, 49, 843-851.\nCôté, J., MacDonald, D.J., Baker, J., & Abernethy, B. (2006). When size matters: Birthplace effects on the development of expertise. Journal of Sport Science, 24, 1065-1073.\nFields, B. (2016, November 16). The shaky state of the women's golf game in America. ESPNW. Retrieved from http://www.espn.com/espnw/sports/article/18062758/the-shaky-state-women-golf-game-america\nGilbert, W. (2017). Coaching better every season: A year-round system for athlete development and program success. Champaign, IL: Human Kinetics.\nLloyd, R. S., Cronin, J. B., Faigenbaum, A. D., Haff, G. G., Howard, R., Kraemer, W. J., Micheli, L. J., Myer, G. D., and Oliver, J. L. (2016). National Strength and Conditioning Association Position Statement on Long-Term Athletic Development. Journal of Strength & Conditioning Research, 30(6), 1491–1509.\nNBA Youth Guidelines. (2016). Youth basketball guidelines. Retrieved from https://youthguidelines.nba.com/\nO'Sullivan, J. (2015, June 23). Jordan Speith's multi-sport path to golf stardom. Changing the Game Project. Retrieved from http://changingthegameproject.com/jordan-spieths-multi-sport-path-to-golf-stardom/\nO'Sullivan, J. (2016, September 27). Let's stop the early sport specialization madness! Changing the Game Project. Retrieved from http://changingthegameproject.com/lets-stop-early-sport-specialization-madness/\nPositive Coaching Alliance. (2016). An argument against early specialization. PCA Development Zone. Retrieved from http://devzone.positivecoach.org/resource/video/argument-against-early-specialization\nRees, T., Hardy, L., Gullich, A., Abernethy, B., Côté, J., Woodman, T., Montgomery, H., Laing, S., & Warr, C. (2016). The Great British Medalists Project: A review of current knowledge on the development of the world's best sporting talent. Sports Medicine, 46, 1041-1058.\nSmith, M. (2016, October 19). 5 reasons you want your kid to be a multi-sport athlete. ESPNw. Retrieved from http://www.espn.com/espnw/voices/article/17831948/5-reasons-want-your-kid-multi-sport-athlete\nSports and Society Program: The Aspen Institute Project Play. (2015). Sport for all play for life: A playbook to get every kid in the game. Retrieved from: http://youthreport.projectplay.us/\nUnited States Olympic Committee. (2015). American Development Model. Retrieved from: http://www.teamusa.org/About-the-USOC/Athlete-Development/American-Development-Model\nVealey, R. (2016). Athlete development through multiple sport participation [Webinar]. Human Kinetics Coach Education. Retrieved from http://www.humankinetics.com/human-kinetics-coach-education-webinars/human-kinetics-coach-education-webinars/athlete-development-through-multiple-sport-participation\nZilis, A. (2016, May 8). Specialization isn't stopping three-sport athletes. The News-Gazette. Retrieved from http://www.news-gazette.com/sports/prep-sports/baseball/2016-05-08/specialization-isnt-stopping-three-sport-stars.html","Carlos Alcaraz, the Spanish tennis phenom, has taken the tennis world by storm, currently holding the coveted world No. 1 ranking by the Association of Tennis Professionals (ATP). At just 19 years old, he has already broken records set by the legendary Rafael Nadal, becoming the youngest player to break into the top 10 since Nadal’s breakthrough in 2005. Discover more about the Tennis Phenom Carlos Alcaraz.\nFurthermore, Alcaraz’s triumph at the Masters 1000 events in Miami and Madrid in 2022 made him the youngest player since Nadal to achieve such a feat. His aggressive playing style, lightning-fast court coverage, and thunderous groundstrokes make him an exhilarating player to watch, evoking comparisons to his fellow countryman Nadal.\nEarly Life and Career: A Tennis Prodigy Emerges\nIn the charming town of El Palmar, Spain, in 2003, a future tennis legend was born. Carlos Alcaraz’s passion for the sport ignited at the tender age of four, and his remarkable talent quickly became evident. By the time he was eight, Alcaraz celebrated his first tournament victory, foreshadowing the extraordinary journey that lay ahead.\nA mere 12 years old and already ranked among the world’s top 100 junior players, his ascent seemed unstoppable. In 2018, Alcaraz made the pivotal decision to turn professional, captivating audiences with his youthful exuberance and fierce determination. The tennis world braced itself for the meteoric rise of this prodigious young player.\nPlaying Style: The Nadal-like Force to be Reckoned With\nOn the hallowed courts, Alcaraz’s playing style reverberates with echoes of greatness. An aggressive baseline player, he showcases lightning-fast speed, unleashing devastating groundstrokes that leave opponents reeling. With a two-handed backhand and a single-handed forehand, Alcaraz’s technique embodies both power and finesse.\nHis remarkable footwork grants him a masterful command of the court, while his deft volleying skills elevate his game to new heights. Many experts draw comparisons to the legendary Rafael Nadal, acknowledging Alcaraz as the heir apparent to his throne. As spectators marvel at his explosive energy and fearless approach, the tennis world braces itself for the extraordinary feats this young phenom will undoubtedly accomplish.\nAccomplishments: A Trailblazer Shattering Records\nIn his meteoric rise, Alcaraz has already etched his name in tennis history. Claiming four ATP Tour titles, including two illustrious Masters 1000 crowns, he defied the notion of age barriers. Stepping into the footsteps of his idol, Rafael Nadal, Alcaraz became the youngest player since 2005 to break into the coveted top 10 rankings.\nHis relentless pursuit of greatness led him to the quarterfinals of the French Open and the semifinals of the US Open, announcing his arrival among the tennis elite. With every milestone surpassed, Alcaraz continues to redefine what is possible for a young prodigy with limitless potential.\nWimbledon 2023: A Historic Triumph for the Ages\nThe hallowed lawns of Wimbledon bore witness to a defining moment in tennis history. In 2023, the world stood in awe as Carlos Alcaraz claimed his maiden Grand Slam title, toppling the mighty Novak Djokovic in a gripping five-set battle.\nThe echoes of cheers resounded through the grounds as Alcaraz’s victory became one of the greatest upsets the tournament had ever seen. This resounding triumph solidified his place as one of the world’s finest athletes. With eyes fixed on the horizon, Alcaraz’s victory at Wimbledon served as a testament to his unwavering spirit, inspiring a generation of aspiring tennis champions.\nFuture Prospects: A Path Paved with Promise\nAs Alcaraz continues his tennis odyssey, the world awaits the unfolding of an extraordinary career. Though still in the dawn of his professional journey, his rapid ascent and exceptional talent have captivated fans and experts alike. Widely regarded as one of the most thrilling young players in the world, Alcaraz possesses the ingredients necessary to become the next luminary of men’s tennis. As he refines his skills and seeks greater consistency, the tennis realm eagerly anticipates the magnificent heights he will scale in the years to come. The sky’s the limit for this young maestro, and with every swing of his racket, he solidifies his status as the beacon of tennis’s glorious future.\nWrapping It Up\nCarlos Alcaraz, a name destined to be etched in the annals of tennis history. At just 19 years old, this prodigious talent has already accomplished what many only dream of. His path to greatness has been paved with resilience, skill, and an unwavering determination to surpass expectations. The tennis world stands in awe of his achievements and eagerly anticipates the captivating performances that lie ahead. With each passing tournament, Alcaraz captures the hearts of fans worldwide, his every stride an affirmation of his status as the future of men’s tennis. As we embark on this thrilling journey, we revel in the knowledge that the best is yet to come from this extraordinary young athlete."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:c4a6e020-25a8-4d76-a42b-58ba6caa24fc>","<urn:uuid:180a602c-bed4-486b-9bc3-5d57bb9ca3d8>"],"error":null}
{"question":"I'm comparing different molecular mechanisms in biology. What's the difference between how PilB ATPase and organophosphate pesticides interact with enzymes in living organisms?","answer":"PilB ATPase and organophosphate pesticides interact with enzymes in fundamentally different ways. PilB ATPase functions by breaking down ATP (adenosine triphosphate) and has a newly discovered role in signal processing, helping bacteria respond to environmental changes. In contrast, organophosphate pesticides work by inhibiting the acetylcholinesterase (AChE) enzyme, preventing it from breaking down the neurotransmitter acetylcholine. This inhibition causes acetylcholine to accumulate, leading to increased nerve impulse transmission and ultimately nervous system failure. While PilB ATPase helps in cellular information processing and bacterial movement, organophosphate pesticides disrupt normal enzyme function in a way that can be lethal to organisms.","context":["Enzyme may help bacteria become craftier and more dangerous\nAugust 8, 2017\nVirginia Tech researchers have discovered a new role for an enzyme that is well-known for fueling chemical reactions in bacterial cells.\nThe enzyme, an ATPase known as PilB, is part of a biological chain reaction that allows bacteria to respond to changes in the environment.\nThe finding, published in Scientific Reports, will help scientists better understand how dangerous bacteria, such as pathogenic E. coli and Pseudomonas, create biofilms to colonize the human body and medical implants, including pacemakers. Bacteria in biofilms can make infections hard to treat, resulting in serious medical problems.\nIt was previously known that PilB is used to break down adenosine triphosphate, or ATP, a small energy-storing molecule. But now, researchers know that it can help bacteria process information as well, such as how to respond to changes in their environment.\n“There is a hidden way for cells to do things that we didn’t know before,” said Zhaomin Yang, an associate professor of microbiology in the College of Science at Virginia Tech, Fralin Life Science Institute affiliate, and lead author on the paper. “This is an alternative way signals can be processed, much like cell phones process and transmit signals differently from landlines.”\nYang and his colleagues made the discovery while studying how myxobacteria form biofilms. These bacteria, which are commonly found in the environment, colonize by secreting a sticky, glue-like substance that allows them to spread on moist solid surfaces to look for nutrients. Likewise, pathogenic bacteria form biofilms in nutrient-rich environments, such as the human body. In both of these situations, bacteria use environmental signals – the presence or absence of nutrients – to determine whether to produce this matrix material for biofilms.\nIn this study, the myxobacterium Myxococcus xanthus was found to regulate biofilm formation and bacterial movement through the PilB ATPase enzyme, which was already known to be essential for bacterial movement on surfaces.\n“The surprising finding here that an ATPase can function as a signaling protein will likely impact biological and biomedical research in profound ways,” Yang said. “There are typically dozens if not hundreds of ATPases in a given organism. It is exciting to envision the prospect of these enzymes forming the wiring of signal processing networks in a cell, from bacteria to animals.”\nScientists have known that ATPase enzymes harvest energy from ATP in order to fuel such processes as movement of a cell or organisms, explained Yang, but prior to this discovery, little was known about whether they could directly function as signaling proteins in biological systems. Other proteins, called G proteins, have been the ones known to play this role.\n“Before now, ATPases were thought to provide energy for various reactions in the cell,” Yang said. “This could be the beginning of uncovering many new ways cells manage signals, significantly broadening the spectrum of how cells process and respond to signals.”\nIn addition to Yang, other members of the research team include Wesley Black, a postdoctoral associate in microbiology; Lingling Wang, a visiting scientist from China; and Birgit Scharf and Florian Schubot, both associate professors of biological sciences in the College of Science and affiliates of the Fralin Life Science Institute.\nThis research was supported in part by the National Institutes of Health, the American Heart Association, and the National Science Foundation.\nWritten by Cassandra Hockman","Professor M Sumarjit Singh\nPesticides are substances that are meant to control pests. The term pesticide includes all of the following: herbicide, insecticides nematicide, molluscicides, piscicide, avicide, rodenticide, bactericide, insect repellent, animal repellent, antimicrobial, fungicide, and lampricide.\nThe emphasis today is solely on productivity- high input in exchange for high returns and productivity (mostly diminishing now however for farmers worldwide). Four important considerations— what happens to the land, food it produces, the people who eat it and the communities which lose out— are overlooked.\nThe environmental impacts of pesticides are often greater than what is intended by those who use them. Over 98% of sprayed insecticides and 95% of herbicides reach a destination other than target species, including nontarget species air, water, bottom sediments, and food. Pesticide contaminates land and water when it escapes from production sites and storage tanks, when it runs off from fields, when it is discarded, when it is sprayed aerially, and when it is sprayed into water to kill algae. During spraying or dusting of pesticides only a small portion is adhered to the crop and rest either fall down or taken up into the atmosphere by the current or turbulence. Volatilization is a major pathway for the loss of applied pesticides which depend upon the vapour pressure. The soil is contaminated with the pesticides either through deliberate application of them for controlling soil inhabiting pests or through run off from plants, and dumping of empty containers.\nThe amount of pesticides that migrate from the intended application area is influenced by the particular chemical’s properties: its propensity for binding to soil, its vapour pressure, its water solubility, and its resistance to being broken down over time. Factors in the soil, such as its texture, its ability to retain water, and the amount of organic matter contained in it, also affect the amount of pesticide that will leave the area. Some pesticides contribute to global warming and the depletion of the ozone layer.\nSeveral agricultural chemicals used for controlling various diseases, insect pests and weeds are highly toxic and their application adversely affects the soil micro flora and fauna. Prolonged persistence of these pesticides in soil is bound to lower the soil fertility both directly and indirectly.\nList of pesticides/pesticides formulations banned in India\nA. Pesticides banned for manufacture, import and use (27 Nos.)\n1. Aldrin, 2. Benzene Hexachloride, 3. Calcium Cyanide, 4. Chlordane,\n5. Copper Acetoarsenite, 6. Clbromochloropropane, 7. Endrin,\n8. Ethyl Mercury Chloride, 9. Ethyl Parathion, 10. Heptachlor,\n11. Menazone, 12. Nitrofen, 13. Paraquat Dimethyl Sulphate,\n14. Pentachloro Nitrobenzene, 15. Pentachlorophenol,\n16. Phenyl Mercury Acetate, 17. Sodium Methane Arsonate,\n18. Tetradifon, 19. Toxafen, 20. Aldicarb, 21. Chlorobenzilate,\n22. Dieldrine, 23. Maleic Hydrazide, 24. Ethylene Dibromide,\n25. TCA (Trichloro acetic acid), 26. Metoxuron, 27. Chlorofenvinphos\nB. Pesticide / Pesticide formulations banned for use but their manufacture is allowed for export (2 nos.)\n28.Nicotin Sulfate, 29. Captafol 80% Powder\nC. Pesticide formulations banned for import, manufacture and use (4 nos.)\n1. Methomyl 24% L, 2. Methomyl 12.5% L, 3. Phosphamidon 85% SL,\n4. Carbofuron 50% SP\nD. Pesticide Withdrawn (7 nos.)\n1. Dalapon, 2. Ferbam, 3. Formothion, 4. Nickel Chloride,\n5. Paradichlorobenzene (PDCB), 6. Simazine, 7. Warfarin\nList of pesticides refused registration\n1. Calcium Arsonate, 2. EPM, 3. Azinphos Methyl, 4. Lead Arsonate, 5. Mevinphos (Phosdrin), 6. 2,4, 5-T, 7. Carbophenothion, 8. Vamidothion, 9. Mephosfolan, 10. Azinphos Ethyl, 11. Binapacryl, 12. Dicrotophos, 13. Thiodemeton / Disulfoton, 14. Fentin Acetate, 15. Chinomethionate (Moretan), 16. Chinomethionate (Morestan), 17. Ammonium Sulphamate, 18. Leptophos (Phosvel) Pesticides Restricted for use in India 1.Aluminium Phosphide, 2.DDT, 3. Lindane, 4. Methyl Bromide, 5. Methyl Parathion, 6. Sodium Cyanide, 7. Methoxy Ethyl Mercuric Chloride (MEMC), 8. Monocrotophos, 9. Endosulfan, 10. Fenitrothion, 11. Diazinon, 12. Fenthion, 13. Dazomet\nSource: Web portal of ‘Directorate of Plant Protection Quarantine & Storage, Faridabad’ as on 15.03.2011\nMost of the insecticides, herbicides and fungicides used in agriculture and for domestic purposes belong to a class of pesticides called organophosphates. The most common organophosphates are malathion, chloropyrifos, dimetheoate and phosphamidon. These degrade fast and are fast-acting on the target pest. However, their toxicity is not specific to any particular pest, and lethal effects have often been observed in other non-targeted organisms, especially birds and humans.\nOrganophosphates trigger both leukaemia (cancer of the bone marrow) and lymphoma (cancer that originates in lymphocytes). Lymphoma is broadly divided into two categories- Hodgkin’s and non-Hodgkin’s lymphoma. It has been observed that these pesticides trigger non-Hodgkin’s lymphoma on a larger scale. Non-Hodgkin’s lymphoma poses a greater threat as it does not respond to treatments like chemotherapy and bone marrow transplant. (Dr. M.R. Ray, Head of the Experimental Haematology Unit).\nOrganophosphate pesticides also affect the nervous system by inhibiting acetylcholinesterase (AChE) enzyme activity. This enzyme breaks down the neurotransmitter acetylcholine. Change in AChE’s molecular configuration makes it impossible for it to function properly, causing acetylcholine to accumulate. This increases nerve impulse transmission, thereby leading to nervous system failure. The respiratory muscles are the most affected muscle group, whose paralysis often causes death.\nThe writer is Professor at Department of Agronomy, College of Agriculture, Iroisemba, C.A.U. Imphal\nFor further details contact:-\nPublic Relation & Media Management Cell, CAU, Imphal. Email: [email protected]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:505bc32b-0f4a-4590-a288-49bf8ee23aef>","<urn:uuid:6c4a4efe-9032-4095-a7a2-3544cfdf2052>"],"error":null}
{"question":"Academic question: What are the key differences in how De Viris Illustribus and Plutarch's Lives were used as educational resources - one in late 4th century religious contexts versus modern classical education?","answer":"De Viris Illustribus served primarily as a reference work documenting Christian scholarly achievements, completed in 392-3 CE, with brief biographical entries focused on authors' literary contributions to the Church. In contrast, Plutarch's Lives became an educational tool in modern classical education, particularly in Charlotte Mason's approach, where it was initially considered part of History lessons but later categorized under Citizenship studies. The texts were used differently - while Jerome's work provided straightforward biographical information, Plutarch's Lives were used to provoke discussions about character, leadership, and civic duty through examination questions that encouraged students to analyze historical figures' actions and motivations.","context":["De Viris Illustribus (Jerome)\nDe Viris Illustribus (On Illustrious Men) is a collection of short biographies of 135 authors, written in Latin, by the 4th-century Latin Church Father Jerome. He completed this work at Bethlehem in 392-3 CE. The work consists of a prologue plus 135 chapters, each consisting of a brief biography. Jerome himself is the subject of the final chapter. A Greek version of the book, possibly by the same Sophronius who is the subject of Chapter 134, also survives. Many biographies take as their subject figures important in Christian Church history and pay especial attention to their careers as writers. It \"was written as an apologetic work to prove that the Church had produced learned men.\" The book was dedicated to Flavius Lucius Dexter, who served as high chamberlain to Theodosius I and as praetorian prefect to Honorius. Dexter was the son of Saint Pacianus, who is eulogized in the work.\nListed below are the subjects of Jerome's 135 biographies. The numbers given are the chapter numbers found in editions.\nJerome's account of his own literary career\nAt the conclusion of De Viris Illustribus, Jerome provided his own biography as the latest example of the scholarly work of Christians. In Chapter 135, Jerome summarized his career to date:\n|“||I, Jerome, son of Eusebius, of the city of Strido, which is on the border of Dalmatia and Pannonia and was overthrown by the Goths, up to the present year, that is, the fourteenth of the Emperor Theodosius, have written the following: Life of Paul the monk, one book of Letters to different persons, an Exhortation to Heliodorus, Controversy of Luciferianus and Orthodoxus, Chronicle of universal history, 28 homilies of Origen on Jeremiah and Ezekiel, which I translated from Greek into Latin, On the Seraphim, On Osanna, On the Prudent and the Prodigal Sons, On Three Questions of the Ancient Law, Homilies on the Song of Songs two, Against Helvidius, On the perpetual virginity of Mary, To Eustochius, On Maintaining Virginity, one book of Epistles to Marcella, a consolatory letter to Paula On the Death of a Daughter, three books of Commentaries on the Epistle of Paul to the Galatians, likewise three books of Commentaries on the Epistle to the Ephesians, On the Epistle to Titus one book, On the Epistle to Philemon one, Commentaries on Ecclesiastes, one book of Hebrew questions on Genesis, one book On places in Judea, one book of Hebrew names, Didymus on the Holy Spirit, which I translated into Latin one book, 39 homilies on Luke, On Psalms 10 to 16, seven books, On the captive Monk, The Life of the blessed Hilarion. I translated the New Testament from the Greek, and the Old Testament from the Hebrew, and how many Letters I have written To Paula and Eustochium I do not know, for I write daily. I wrote moreover, two books of Explanations on Micah, one book On Nahum, two books On Habakkuk, one On Zephaniah, one On Haggai, and many others On the prophets, which are not yet finished, and which I am still at work upon.||”|\n- \"This work [De viris illustribus], as he reveals at its start and finish, was completed in the fourteenth year of Theodosius, that is, between 19 January 392 and 18th January 393.\" A.D. Booth, \"The Chronology of Jerome's Early Years,\" Phoenix 35 (1981), p.241.\n- Louis Saltet, \"St. Jerome,\" Catholic Encyclopedia, New York: 1910.\n- Irondequoit Catholic Communities - - Pacian\n- Jerome, De Viris Illustribus, chapter 135.\n- De Viris Illustribus (On Illustrious Men) - Full English version.\n- The Catholic Encyclopedia, Published 1910 in New York by Robert Appleton Company.\n- This article incorporates text from the Encyclopædia Britannica Eleventh Edition, a publication now in the public domain.\nOther text and translations\n- Jerome and Gennadius: Lives of Illustrious Men, English translation by Ernest Cushing Richardson\n- Jerome's De Viris Illustribus: Latin text (includes an informative introduction, in Latin)\n- Jerome's De Viris Illustribus: Greek version\n|Wikisource has original text related to this article:|\n- Jerome's De Viris Illustribus of Matthew, Mark, Luke\n- Catholic Encyclopedia: Gennadius of Marseilles (continuator of Jerome's De viris illustribus)","Plutarch was a Greek writer who lived from 46 to 120 AD. To quote from the Philip's World History Encyclopedia, \"his best-known work is his Parallel Lives, which consists of paired biographies of famous Greeks and Romans. Shakespeare used it as the source for his Roman history plays.\" Dr. George Grant of King's Meadow Study Center has written an article that addresses this called \"Why Read Plutarch?\"\nSome eager Year Four students will be able to begin Plutarch; others may choose to read Emily Beesly's Stories from the History of Rome (online at archive.org or googlebooks). Students who have moved up to reading Shakespeare's plays in the original will probably be ready.\nWe take one of the biographies each term and study that person's life in detail. To make it easier, we have created study notes for many of the Lives we use, breaking each one down into twelve readings, suggesting parts that parents will probably want to omit, giving vocabulary help, and offering discussion questions.\nFor various reasons (such as the cost of books and the high frequency of mature content in Plutarch), Charlotte Mason seemed to expect that these lessons would be read aloud. Most of us have found that, since these Lives offer many topics for discussion, students (and parents) get more out of them when they are studied together. Older students may prefer to read independently.\nNo, you do not have to buy a volume of Plutarch's Lives. Each term's study notes gives a link to an online version or an (included) edited version of the text.\nThe translation Charlotte Mason recommended was that by North, which is the one Shakespeare would have used and which is full of nice, rich, Shakespearish language. (purchase; There are page images of all ten volumes online). (Advisory member Anne White is typing these to use with her Study Guides.) The poet Dryden re-translated Plutarch, and in the 1800s that translation was edited by Arthur Hugh Clough. The first versions of the study notes here (written by Advisory member Anne White) mostly used Dryden because it was easily accessible online. However, these are being revised to use North's translation, and the revised notes contain the text within the lessons.\nIn the early years of the Parents' Union School, Plutarch was counted as part of the History lessons, because there was no real category that seemed to fit. In later years, after Charlotte Mason had written her own book (Ourselves) for students, and other books came along that dealt with what we might call practical civics and economic history, these were grouped together and called Citizenship. What does one need to do, or what character did one need to have, to be both a good subject and a great leader? When is it right to fight against tyranny? How do human beings manage their affairs, wisely and justly or not?\nThat is not to say that you can't learn a great deal of history from them; and in fact, Plutarch is one major source of the historical information we do have on many events. But for our purposes, we read Plutarch for the ideas and life lessons his biographies offer, rather than as a history course. It's a look at what motivated some of the famous figures of the ancient world, what they did right, and where they went wrong.\n\"Plutarch's Lives, . . . I think, stand alone in literature as teaching that a man is part of the State, that his business is to be of service to the State, but that the value of his service depends upon his personal character. (Parents Review article by Charlotte Mason)\nThe key to understanding Plutarch's Lives, as Charlotte Mason used them, can be found in the examination questions that were given. Something as innocent-sounding as Form III's \"How did Alexander spend his days at a time of leisure?\" may end up being, as Charlotte would say, suggestive. Note that there is nothing more there than a question--but it's a thought-provoking one. Did he make the dull time count for something, or was it unproductive? Form II's (younger students) were asked \"Why and how did Alexander teach his men 'to acquaint themselves with hardness?'\" They were not being asked to write essays on the importance of fortitude, they were simply asked to recount what was said or done; but the idea was there, to be taken or not.\nAlcibiades, someone who had \"great courage and quickness of understanding, but had many great faults and imperfections\" inspired an examination question that went both ways: \"Tell a story to illustrate (a), his courage, (b), his envy.\" Often the students were asked to describe someone's behaviour in a particular situation; again, not necessarily telling why this was good or bad, but telling the story, giving examples of particular virtues or vices.\nIn many cases the examination questions were based on a quotation, which may suggest to us as the \"course facilitators\" that we may want to make sure such sayings are noticed and remembered, in whatever ways seem the most natural for us to do so.\nFrom Alexander again:\n\"Alexander loved to remember and reward the worthy deeds of men.\" Give two instances in details,\nOR, \"To live at pleasure is a vile thing, and to travel is princely.\" Why did Alexander thus rebuke his friends? Tell the whole story.\nIn the high school years, the quotations and the writing were at a higher level. From the Life of Julius Caesar, for Form IV (Grade 9):\n1. Sketch briefly the character of Julius Caesar, and say to what events in his life the following sayings refer,--\n(a), \"A man can be but once undone, come on.\"\n(b), \"Time of war and law are two things.\"\n(c), \"Thou has Caesar and his fortune with thee.\"\nAnd from Cleomenes, a question which seems to sum up much of Plutarch and Citizenship:\nWhat were some of the things that Cleomenes \"thought most fit and honourable for a prince\" in private and in public life?\nFirst of all, don't underestimate your children (and yourself). Many of us have gone into Plutarch studies with great trepidation, and have ended up finding it one of the favourite and best-remembered books in the upper years.\nThat said, yes, there are later translations *, and there are abridgements for young people, and even some children's versions online. There are three childrens' versions of Plutarch (Our Young Folk's Plutarch, by Rosalie Kaufman, Plutarch's Lives for Boys and Girls by W. H. Weston, and a 2-volume Children's Plutarch by F.J. Gould divided between Tales of the Greeks and Tales of the Romans) that may be helpful in the same way as Lamb's Tales from Shakespeare or Bible story books are. (See chapter titles for some of these retellings below.) There is also John S. White's Boys and Girls Plutarch on Project Gutenberg which is basically the Dryden/Clough translation with omissions of material not suitable for children. However, like Bible story books, sometimes the retellings feel like they're missing the original flavour or intent of the story, and on occasion they will even substitute a gorier word or phrase than one that earlier translators used!\nOne thing we sometimes suggest in the notes is not trying to focus on details of names or other unfamiliar references, but just trying to get the main idea of what's going on in the story. The same thing can happen with Shakespeare if you get so caught up in understanding all the vocabulary that you can't just read the script. Plutarch does tell good stories; you have to get into his style, though. (One advantage of doing Plutarch that many parents have found is that it takes some of the fear out of reading other older historical books.)\nStop frequently and have your student(s) narrate, or at least make sure they're is clear on what just happened. It can be a good idea to to start Plutarch lessons with an overview of what's going to happen, even if sometimes a \"spoiler\" has to be included--it helps to know what kind of a story they'll be listening to before they start. You could also suggest one or two things to listen for in the reading.\nTerm 1: September - November\nTerm 2: January - March\nTerm 3: April - June\nAmblesideOnline schedules these terms as a group to facilitate our Artist, Composer, Plutarch, Shakespeare, Folksongs and Hymns studies through sharing resources and experiences on the list. Study Guides with Thomas North's \"text included and divided are the work of AO Advisory member Anne White.\nIf you are just beginning Plutarch, you may wish to begin with Publicola (listed below), as it has has study notes for beginners. Anne White's study guide with the text of Thomas North's Life of Publicola is also available for purchase from amazon.com: The Publicola Primer by Anne White: $\n2015-2016 School Year (Purchase all 3 of this year's Lives with Anne White's study guides as a single book: $)\nTerm 1: Plutarch's Life of Marcus Cato the Censor (Study Guide with Thomas North's text; text only)\nTerm 2: Plutarch's Life of Philopoemen (Study Guide with Thomas North's text; text only)\nTerm 3: Plutarch's Life of Titus Flamininus (Anne White's Study Guide with Thomas North's text; text only)\n2016-2017 School Year\nTerm 1: Plutarch's Life of Pyrrhus (Study Guide with Thomas North's text; text only)\nTerm 2: Plutarch's Life of Nicias (Anne White's Study Guide)\nTerm 3: Plutarch's Life of Crassus (Study Guide with Thomas North's text; text only)\n2017-2018 School Year\nTerm 1: Plutarch's Life of Julius Caesar\nTerm 2: Plutarch's Life of Agis and Cleomenes\nTerm 3: Plutarch's Life of T. Gracchus and C. Gracchus\n2018-2019 School Year\nTerm 1: Plutarch's Life of Demosthenes (Study Guide)\nTerm 2: Plutarch's Life of Cicero (Study Guide)\nTerm 3: Plutarch's Life of Demetrius (Anne White's study guide and student-friendly edited text) Although a good leader, he also had a more sordid side, which Plutarch does not sanitize; we recommend using an edited text. Wikepedia.org has a brief bio.\n2019-2020 School Year\nTerm 1: Plutarch's Life of Alexander (summary of Alexander by AO student)\nTerm 2: Plutarch's Life of Dion - \"Justice and bravery, together with a healthy feeling of self-sacrifice, and a preparation for and fulfilment of public responsibilities, breathe through the whole of the \"Life of Dion.\" - Miss Ambler (Anne White's Study Guide)\nTerm 3: Plutarch's Life of Marcus Brutus (Anne White's Study Guide)\n2020-2021 School Year\nTerm 1: Plutarch's Life of Solon (Anne White's Study Guide) (summary of Solon by AO student)\nTerm 2: Plutarch's Life of Poplicola/Publicola (Study Guide with Thomas North's text; text only) [Purchase Publicola Primer $]\nTerm 3: Plutarch's Life of Pompey\n2021-2022 School Year\nTerm 1: Plutarch's Life of Themistocles (Anne White's Study Guide and edited text)\nTerm 2: Plutarch's Life of Pericles (Anne White's Study Guide)\nTerm 3: Plutarch's Life of Fabius (Anne White's Study Guide; edited text)\n2022-2023 School Year\nTerm 1: Plutarch's Life of Alcibiades (Study Guide with Thomas North's text; text only)\nTerm 2: Plutarch's Life of Coriolanus (Anne White's Study Guide)\nTerm 3: Plutarch's Life of Cato the Younger\n2023-2024 School Year\nTerm 1: Plutarch's Life of Timoleon (Study Guide with Thomas North's text; text only)\nTerm 2: Plutarch's Life of Aemilius Paulus (Study Guide with Thomas North's text; text only)\nTerm 3: Plutarch's Life of Aristides (Anne White's in-progress Study Guide: lessons 1-6 complete)\nOptional Alternative Lives:\nPlutarch's Life of Theseus (use a child-appropriate version, such as this one)\nPlutarch's Life of Romulus (Anne White's in-progress Study Guide and Text divided into 12 weekly readings)\nThese retellings could be used with younger children if Plutarch is read as a family study, or as a supplement.\nPlutarch's Lives for Boys and Girls by W. H. Weston (not a complete collection, but the least abridged; not a childish edition.)\nThe Gracchi: Tiberius Gracchus, Caius Gracchus and Cornelia, their mother\nOur Young Folks Plutarch by\nRosemary Kaufman, online at www.mainlesson.com and Heritage History (significantly more condensed than Weston's, but not as easy as Gould's; meant for fairly young children)\nCaius Marcius Coriolanus\nMarcus Cato (also called Cato the Stern, or Cato the Elder, or Cato the Censor)\nCato the Younger\nTales of the Greeks: The Children's Plutarch by F. J. Gould (the shortest, most condensed version for the youngest ages; includes pronunciations of names.)\nThe Hardy Men of Sparta - Lycurgus\nThe Wise Man of Athens - Solon\nThe Just Man - Aristides\nThe Savior of Athens - Themistocles\nThe Admiral of the Fleet - Cimon\nThe Man Who Made Athens Beautiful - Pericles\nThree Powers - Lysander\nThe Man with Many Faces - Alcibiades\nIn Old Persia - Artaxerxes\nA Lame King - Agesilaus\nA Martyr King - Agis\nA Valiant Helper - Pelopidas\nThe Man Who Saved Sicily - Timoleon\nThe Orator - Demosthenes\nThe Conqueror - Alexander\nA Servant of the City - Phocion\nGolden Shoes and Two Crowns - Demetrius\nUp the Scaling-Ladders - Aratus\nA Fighting King - Pyrrhus\nThe Last of the Greeks - Philopoemen\nTales of the Romans: The Children's Plutarch by F. J. Gould\nThe Twins - Romulus\nWhat the Forest Lady Said - Numa\nWhy the Romans Bore Pain - Brutus\nThe Second Founder of Rome - Camillus\nThe Man Who Waited - Fabius\nHow a Woman Saved Rome - Coriolanus\nThe Triumph - Aemilius Paulus\nA Roman Undismayed - Marcellus\nCato the Stern - Marcus Porcius Cato (Cato the Elder, or Cato the Censor)\nThe General Who Ate Dry Bread - Caius Marius\nThe Red General - Sulla/Sylla\nBattle-Fields and Gardens - Lucullus\nThe Man Who Loved Gold - Crassus\nThe White Fawn - Sertorius\nThe Conqueror of Pirates - Pompey\nCaesar and His Fortune - Caesar\nThe Man Who Seldom Laughed - Cato the Younger\nThe Noble Brothers - Tiberius Gracchus and Caius Gracchus\nTully - Cicero\nThe Man Who Looked Like Hercules - Antony\nCaesar's Friend and Enemy - Brutus\n* Louise Ropes Loomis translated a version titled \"Plutarch: Selected Lives and Essays.\" It has a 1951 copyright date and is not online as an etext, but used copies can be found from online booksellers. This is an unabridged yet modernized, easy to read version. Because it is unabridged, it will require some on-the-fly parental editing. It includes these Lives: Lycurgus; Numa Pompilius; Themistocles; Camillus; Pericles; Fabius Maximus; Alcibiades; Gaius Marcius Coriolanus; Demosthenes; Cicero.\nPlutarch's Lives of Illustrious Men translated from the Greek by John and William Langhorne, 1856 (page images; may be more complete than Dryden's; this is the version quoted in the Parents' Review article above, which may indicate that it was one used by CM's students.)\nCopyright © 2002-2013 AmblesideOnline. All rights reserved. Use of this curriculum subject to the terms of our License Agreement."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:bdda753f-2736-47ac-bce6-2f3232dce861>","<urn:uuid:d15df403-5026-41ac-a669-ff10b1d39f6b>"],"error":null}
{"question":"I'm designing an outdoor electronics enclosure. ¿Cuáles son las ventajas del molding para los EMI gaskets, y how does heat resistance affect gasket material selection?","answer":"For EMI gaskets, molding offers several key advantages: it creates resilient, durable gaskets that can retain their shape under extreme force and hold very precise tolerances (within thousandths of an inch). Molded gaskets also allow for complex designs including joints and cross-sections, and can reduce manual labor compared to splicing. Regarding heat resistance, the material selection is crucial - silicone and fluorocarbon gaskets can withstand temperatures up to 400 degrees Fahrenheit, while materials like neoprene, nitrile, and EPDM typically cannot maintain their structural integrity beyond 400 degrees. However, modifications can be made to gaskets to enable them to withstand higher temperatures and fire.","context":["Shielding technology is essential to protect electronics from electromagnetic interference. Many application-specific factors contribute to the impact that electromagnetic interference will have on a device, so it’s important to consider which shielding option is best for your specific case.\nShielding gaskets can be made from a wide range of materials to be shaped and sized to fit nearly any application. One of the more common gasket types is a conductive elastomer gasket. Two of the key manufacturing processes to produce elastomeric EMI gaskets are splicing and molding.\nThese two techniques offer different manufacturing and application advantages. Some applications can best be served by the use of both techniques for different areas of shielding within a device.\nWhile splicing uses an adhesive or vulcanizing process to bond two conductive elastomer gaskets together, molding involves placing uncured conductive elastomers into a custom-designed mold, allowing the elastomers to form into a unique, consistent product. Both techniques have their advantages and disadvantages.\nHave splicing needs?\nThe splicing process begins when conductive elastomer gaskets are created in long strips that need to be manipulated in order to form a proper O-ring. To create that O-ring, the elastomer gaskets are cut to the desired length and then fused together with a strong, proprietary adhesive glue at the ends. This process of cutting and fusing elastomer gaskets is called splicing.\nAdvantages of Splicing\nThe manufacturing process by which the shielding material is extruded is rapid and efficient. Materials can even be spliced in bulk. Similarly, these extrusion profiles can be designed to match the dimension requirements for almost any application. A hollow cross-section profile enables the manufacturing of parts that can succeed in low compression force enclosures.\nThe efficiencies within this process including low tooling cost for high production volumes, make it one of the most cost-effective EMI shield manufacturing techniques used today. Complex cross-sections can be designed using FEA (Finite Elemental Analysis) to meet the required closure force EMI Shielding and environmental sealing requirements.\nDisadvantages of Splicing\nWhile extruded gaskets can be designed with complex cross-sections, for complex gasket patterns, Form-In-Place (FIP) gaskets or molded gaskets are often the better.\nSome have found that O-rings do not retain their shape as well as if they were to have been molded. Part of this might be due to the lack of complexity that the splicing process is able to achieve due to its loop-like way of operating.\nThe molded O-rings can be crafted to fit near any desired shape or size. The splicing process is also known for its low tolerance range. Typically, splicing will not hold tight tolerances. Of course, there are tight regulations for how small gaskets should be when spliced.\nTo mold a gasket, uncured elastomers are compressed into a pre-designed mold. Once the material takes the form of the mold, the gasket is complete. This allows for very precise size and shape and warrants unparalleled consistency throughout the manufacturing process.\nAdvantages of Molding\nOnce a mold is created, it makes for a resilient, durable gasket time and time again. A molded gasket allows for complex designs including joints, cross-sections, and more. Molded gaskets can retain their shape under extreme force and hold tolerances within thousandths of an inch. The nature of the molding process eliminates some of the manual labor present in the splicing process, making it even more cost-effective in some instances.\nDisadvantages of Molding\nThe molding process requires a mold for each gasket pattern and design so it does not offer the flexibility and speed for design change offered by other techniques. The molding process can be less efficient when each new gasket design requires a new mold. Unlike splicing, the molding method is not conducive to creating hollow gaskets.\nVulcanizing is a commonly-known bonding technique used for splicing where heat, pressure and a splice are used to fuse the ends of a gasket together. At KRA Fab, our expert manufacturers perform vulcanizations daily. This process is a dependable, cost-efficient way to join profiles of countless types of materials in a variety of different shapes and sizes.\nTypically, this tactic is used when there is a space for the free-flowing passage of air to the gasket. All things considered, the vulcanizing technique should be used for low-volume applications as opposed to high-volume production rates.\nSplicing vs. Vulcanizing O-Rings\nSpliced and vulcanized O-rings have different purposes that should be taken into consideration well before the gasket manufacturing process. First, ask yourself whether or not the project at hand is a dynamic application or not.\nIf there are moving parts near the O-ring, a vulcanized O-ring likely isn’t the best fit. If the application is not dynamic, a vulcanizing ring might be a good solution. The vulcanized O-ring is suitable for a wide range of industries and applications, so long as there is a lid that needs to be sealed.\nMolding and splicing are both industry-leading techniques for manufacturing durable EMI shielding gaskets of various shapes and sizes.\nThe best EMI gasket manufacturing process, material selection, and gasket design depend on variables including your need for rapid design change, closure force, operating environment, shielding attenuation needed at specific frequencies, and quantity required.","3 Factors to Consider When Choosing a Gasket For Your Enclosure\nNo matter how well your container is designed, it can be rendered virtually useless if it isn’t properly sealed. The contents of your enclosure can be compromised if exposed to outside elements, and the gasketing you choose can be the difference between a high-quality design and an ineffective design. The right gasket will not only function as expected, but also continue to provide optimal performance even when challenged by factors such as physical stress and extreme environments.\nWhen selecting gasketing for your enclosure, pay special consideration to the following qualities to ensure that your design will be able to perform as expected:\nEven if the materials that make up your enclosure are designed to avoid expansion and contraction, the seal you choose should be flexible enough to compensate for minor fluctuations. Without a flexible gasket, you risk allowing elements such as moisture and temperature to damage the product inside your enclosure. Softer materials, such as those that are rubber-based, will be more flexible and resistant to abuse.\nHEAT AND SUNLIGHT RESISTANCE\nHeat and direct sunlight are common causes of failure for improperly specced gaskets. While materials such as nitrile can break down after consistent exposure to the sun, EPDM is resistant to the damage that sunlight can cause. If your design is likely to be used outdoors, considering this factor is crucial to protecting the integrity of your enclosure.\nHigh-heat environments can completely destroy a gasket (and the contents it’s protecting) if the wrong material is used. Silicone and fluorocarbon gasketing can generally withstand heat up to 400 degrees Fahrenheit, but neoprene, nitrile, and EPDM materials are usually unable to maintain their structural integrity past 400 degrees. However, modifications can be made to gaskets to enable them to withstand higher temperatures and fire.\nThe material inside a container can ultimately be what renders a gasket ineffective. Consider what media may come into contact with your gasket, and spec the proper gasketing into your design accordingly.\nNitrile, for example, is usually a good gasketing material for enclosures likely to come into contact with oil. However, natural rubber generally isn’t compatible with oil. Steam is also a common media that should be considered when speccing a gasket into a design, especially when you factor in how temperature can determine in the effectiveness of a gasket. For instance, EPDM is a popular and reliable gasketing material and can safely come into contact with most steam, but it can be compromised when it comes into contact with steam that’s hotter than 400 degrees. If the gasket you choose doesn’t work well with the material it comes into contact with, both the gasket and the design as a whole can fail.\nA SEAL YOU CAN COUNT ON\nThere are many factors to consider when choosing the right gasket for your enclosure, and failure to account for them can yield unfavorable and even disastrous results. When selecting the right gasket to spec into your design, pay special attention to its required flexibility, temperature and sunlight resistance, and media compatibility to ensure the quality and durability of your enclosure.\nContact us today to learn more about choosing the right gasket for a worry-free seal on your design."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b76dd7f6-d093-46a6-85e8-9ed4fb0966e1>","<urn:uuid:28516565-9663-44b0-9511-29210705c0f3>"],"error":null}
{"question":"Are there protected jaguar habitats in both the Maya Biosphere Reserve and Costa Rica's national parks?","answer":"While the Maya Biosphere Reserve is described as a biodiversity hotspot and important wildlife corridor, the documents don't specifically mention jaguars there. In Costa Rica, Piedras Blancas National Park is explicitly mentioned as one of the few jaguar habitats in the country. Without specific confirmation of jaguar presence in the Maya Biosphere Reserve, we can only definitively say that Costa Rica's Piedras Blancas National Park is a protected jaguar habitat.","context":["The government of Guatemala, through the Honorable National Council of Protected Areas (CONAP), has granted a 25-year extension to the community-managed forestry concession of Carmelita, a community located in the Multiple Use Zone (MUZ) of the Maya Biosphere Reserve (MBR), in the department of Petén. First established in 1997 as a consequence of the peace treaties following the Guatemalan civil war, Carmelita is the first of nine active community forest concessions in the MBR to have its contract renewed.\nThe community concessions have provided jobs to thousands of workers, creating a robust local economy based on sustainable use of the forest. Together the nine active concessions generate more than $6 million USD in annual revenue, helping to make poverty rates in these communities significantly lower than the rest of Guatemala. Outmigration from the concessions is among the lowest in the country.\nIn addition, the concessions have maintained a near-zero deforestation rate for more than 15 years. In contrast, deforestation rates are twenty times higher in areas directly outside the community-managed concessions. This remarkable achievement is all the more noteworthy given the region’s considerable pressures from cattle ranching, narcotrafficking, and development.\nThe MBR is a biodiversity hotspot, an important forest corridor for wildlife, and a critical carbon sink.\nThe Rainforest Alliance enthusiastically applauds the decision to renew the Carmelita concession, and looks forward to the renewal of the eight other concessions for the benefits they provide to people and nature. Since 1997, the Rainforest Alliance has worked with local communities and organizations, such as the Association of Forest Communities of Petén (ACOFOP), to provide technical advice, build capacity, and to enhance access to markets for the communities’ forest products. With the renewal of Carmelita and others on the horizon, we look forward to many more years of collaboration.\nJust as the current leadership within CONAP has shown their support, we trust that the incoming administration of Dr. Giammattei – who has already demonstrated his commitment to sustainable development and the environment, will support the implementation of Roadmap of the Regulations for the extension of the remaining eight active concessions in the Mayan Biosphere.\n“We congratulate Carmelita on this achievement, as this process will allow community forest concessions to continue the generation of jobs, revenue from sales of timber and non-wood products that allows the use of renewable natural resources in a more sustainable way, so that communities can improve their living conditions in addition to guaranteeing them a place to live.” –Oscar Rojas, Director for Guatemala and Central America of the Rainforest Alliance\n“We thank the intention of all collaborators for this achievement; we have made history. We are the second generation to get this extension and now we are leaving a legacy to our children for another 25 years. Since we settled here, we knew we had to preserve the forest and until now we continue in this process of innovation. We are benefiting from the forest and we are protecting it. We thank the Rainforest Alliance who have been closely supporting us and CONAP as together we have been allies for conservation.” –Byron Hernández, president of Carmelita\n“This achievement proves that this conservation model based on natural renewable resources protection and generation of benefits for communities, does preserve the natural resources of the Maya Biosphere Reserve.” –Erick Cuellar, technical director of ACOFOP\nFor more information\nMaps, photos, and interviews available upon request.\nFor more background on Carmelita and the community forestry concessions:\nFor more information about Cooperativa Carmelita:","Costa Rica is abundant with national parks and preserves\nCosta Rica is the proud home of twenty-seven national parks, 58 wildlife refuges, 32 protected zones, 15 wetland areas/mangroves, 11 forest reserves and 8 biological reserves, plus 12 other conservation regions protect the distinctive and diverse natural habitats. Nearly 5% of global flora and fauna, representing 12 life zones, live in an area about the size of West Virginia. The six national parks below capture some of Costa Rica’s amazing biodiversity and all are within a few hours’ drive of Los Sueños Resort and Marina.\nMonteverde Cloud Forest Reserve in northern Puntarenas province sits on the Continental Divide, where mist produced by the high humidity at elevations of up to 5,200 feet collected on the branches of the tallest trees, supports a stunning range of biodiversity. Extending across eight distinct biological zones, the reserve is home to thousands of species of plants and animals. There are several ways to explore the reserve, on foot or exploring the canopy via skywalk or zipline. Well-maintained trails vary in difficulty from easy to moderate, and with thick moss covering nearly all surfaces, sturdy hiking boots are necessary.\nCorcovado National Park, on the Osa Peninsula, preserves the largest primary tropical rainforest on the American Pacific coast. Its remote location has helped preserve it as habitat for many endangered plant and animal species, even as the park’s popularity as an ecotourism destination grows. Scarlet macaws, resplendent quetzals, red-eyed tree frogs and tapirs are among the hundreds of species that inhabit the park’s 13 ecosystems. A certified professional guide must accompany all visitors.\nArenal Volcano within Arenal Volcano National Park is one of the most recognized in Costa Rica. The conservation area (290 square miles) encompasses eight of the 12 life zones in Costa Rica. Hiking, boating, cycling, bird watching and natural hot springs add to the volcano’s steady geothermal display. Birdwatchers especially should pay a visit; most of Costa Rica’s 850 bird species can be found here.\nPiedras Blancas National Park, formerly part of Corcovado, helps protect the last remaining lowland tropical rainforest on the Golfo Dulce. In addition, Piedras Blancas is one of the few jaguar habitats in Costa Rica. Bordering Golfito National Wildlife Refuge, Piedras Blancas and Corcovado create and protect an important ecologically diverse biological corridor in the Golfo Dulce. Many rate Piedras Blancas as a premier bird watching park because many birds from throughout the Americas gather.\nLos Quetzales National Park encompasses 3 rainforests and 14 ecosystems along the Cordillera de Talamanca. Reaching altitudes of nearly 10,000 feet, the mountains provide a stunning backdrop for the park to rival its colorful namesakes. Here, brightly colored, long-tailed green and red quetzals are plentiful, belying their nearly mythological rarity. Other avian residents include the colibri, with its cone-shaped nests and hummingbirds. Squirrel monkeys, sloths, jaguars and pumas are also plentiful within the park.\nSanta Rosa National Park, founded in 1972, is part of the Guanacaste Conservation area that protects a great deal of the last tropical dry forest in the world. Ten distinctive habitats, deciduous forests, littoral woodlands, evergreen forests, mangroves, marshlands and savannahs, can be found within park boundaries. Lush in the green season, the park becomes a topical dry forest in the dry season with well-marked trails. Playa Naranjo and Playa Nancite are two of several stunning beaches. Playa Nancite is an olive ridley turtle nesting site, while Witches Rock at Playa Naranjo offers some of the best surfing in the world.\nManuel Antonio National Park, on Costa Rica’s central Pacific coast, encompasses rugged rainforest, white-sand beaches and coral reefs. It’s renowned for its vast diversity of tropical plants and wildlife, from three-toed sloths and endangered white-faced capuchin monkeys to hundreds of bird species. The park’s roughly 680 hectares are crossed with hiking trails, which meander from the coast up into the mountains.\nFor such a small country, Costa Rica leads the world in environmental conservation, in land, species and government programs. HRG’s dedicated concierge team can help you arrange a visit to any of the parks on our list or help you choose from the many choices available to you. They’ll take care of all the details, from door-to-door."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c89d77f8-ea23-4c83-8f16-996169b799ae>","<urn:uuid:e5babdb5-6956-489f-9e37-2aafbaa6c4e4>"],"error":null}
{"question":"How does psychological depth compare as an analytical approach in PR ethics versus moral disengagement theory?","answer":"Psychological depth is approached differently in PR ethics and moral disengagement theory. PR ethics incorporates Jungian psychology, introducing concepts like the 'fragmented self' and the 'shadow' to move beyond traditional Western good/bad ethical dichotomies toward a more holistic understanding. In contrast, moral disengagement theory focuses on cognitive mechanisms and self-regulation, examining how people actively disconnect from their moral conduct through specific psychological processes like moral justification and displacement of responsibility. While both approaches deal with ethical behavior, PR ethics takes a deeper psychoanalytical perspective, while moral disengagement theory emphasizes the cognitive processes that enable unethical conduct.","context":["Public Relations Ethics and Professionalism\nThe Shadow of Excellence\nRoutledge – 2014 – 244 pages\nRoutledge – 2014 – 244 pages\nDo professions really place duty to society above clients' or their own interests? If not, how can they be trusted? While some public relations (PR) scholars claim that PR serves society and enhances the democratic process, others suggest that it is little more than propaganda, serving the interests of global corporations. This is not an argument about definitions, but about ethics - yet this topic is barely explored in texts and theories that seek to explain PR and its function in society.\nThis book places PR ethics in the wider context of professional ethics and the sociology of professions. By bringing together literature from fields beyond public relations - sociology, professional and philosophical ethics, and Jungian psychology - it integrates a new body of ideas into the debate. The unprecedented introduction of Jungian psychology to public relations scholarship shifts the debate beyond a traditional Western 'Good/Bad' ethical dichotomy towards a new holistic approach, with dynamic implications for theory and practice.\nThis thought-provoking book will be essential reading for students, academics and professionals with an interest in public relations, ethics and professionalism.\n'An unusual and original book which will become a classic point of reference; Jo Fawkes brings a new dimension to public relations literature. Not only does she present a critical and comprehensive overview of key issues and debates, but also a reflexive piece of writing that draws in compelling autobiographical insights. This highly readable book explores public relations through a rich combination of philosophical, sociological, and psychological and psychoanalytical literature to provide a multi-level layered analysis.'\nJacquie L’Etang, Professor, Queen Margaret University, Scotland\n'Johanna Fawkes’s application of Jungian ideas relating to the ‘fragmented self’, the ‘contradictory messiness of being’ and the ‘shadow’ to an exploration of professionalism – and Public Relations ethics in particular – is dazzlingly original. Moreover, her presence throughout as the overtly subjective, deeply questioning, fallible, researcher makes this text both intellectually enriching – and profoundly moving.'\nRichard Lance Keeble, Professor, University of Lincoln, UK\n'This volume is required reading, explicating ethics and performativity. Johanna Fawkes situates responsibility within a being that is embodied, thoughtful, and informed by a textured Jungian perspective.'\nRonald C. Arnett, Chair and Professor, Department of Communication & Rhetorical Studies, Duquesne University, USA\n'Dr. Fawkes will have none of the ethical prancing of public relations and delivers a powerful and personal narrative that takes us to the wild side of Jungian psychology and shadowy public relations. It fruitfully helps us deal with duality, complexity and contradiction.'\nØyvind Ihlen, Professor, University of Oslo, Norway.\n'You learn most about ethics not through smooth presentations that try to package the latest theory, but through the juxtaposition of perspectives that make you stop and think. This book makes you stop and think. Johanna Fawkes does that by bringing together a profession often characterised by surface image with the writings of Jung, who invites us to go deep, to look behind the image, at narratives most often not examined. She does this brilliantly, enabling the reader to interrogate and integrate theory and practice. This forms a challenge to the Public Relations profession, but goes beyond that, raising questions about professional ethics in general, not least about the relationship between integrity, image and identity. It is a book both intellectually stimulating and practically honest and clear, and should be on the book shelves of all researchers, teachers, consultants and practitioners in this area.'\nSimon Robinson, Professor, Leeds Metropolitan University, UK\n1. Messy Ethics: Introduction 2. What’s Wrong with PR Ethics? 3. Trust Me, I’m a Professional 4. To Serve Society 5. Does Public Relations have Professional Ethics? 6. Into the Psyche 7. Depth Ethics 8. Re-Imagining Professional Ethics 9. The Shadow of Excellence 10. What Next? Reflections and Directions\nJohanna Fawkes is Senior Lecturer in Public Relations at Charles Sturt University, NSW Australia and Course Director for the Doctor of Communication. She has led PR degrees in the UK since 1990, advised the CIPR, and published in international journals and leading text books.","Moral disengagement in relation to (Un-)Sustainability\n- Prof. Dr. Wendelin Küpers\nAmong the many irritating phenomena that characterise our times of multiple crises is the moral disengagement, especially related to (un-)sustainability.\nWhen people morally disengage, they often convince themselves that ethical standards or principles do not apply to them in a particular context or related to a specific issue. In moral disengagement problematic or destructive behavior is reframed as being morally acceptable or justifiable without changing the behavior or the moral standards.\nAccording to Bandura (1999), selective moral disengagement occurs when a person actively disengages or consciously disconnectstheir self-regulating efficacy for moral conduct. The person is then separating what one is doing in terms of moral acting or reactions from unacceptable conduct which serves also to disable mechanisms of self-condemnation. And as he or she has shown selective activation of self-sanctions and internal moral control or disengagement allows for a wide range of behaviour, given the same moral standard.\nAs Bandura and his colleagues (1996) found out moral disengagement functions in the perpetration of inhumanities and unethical behaviour through moral justification, euphemistic labelling, advantageous comparison, displacing or diffusing responsibility, disregarding or misrepresenting injurious consequences, and dehumanising the victim, all of them. Importantly all of these cognitive mechanisms are interrelated within a (materio-)socio structural context to promote unethical and unsustainable conduct in people's daily lives. Both are important as individual orientation and behaviour cannot be separated from (materio-)social situatedness, realised in everyday life. We know from various radicalist of fundamentalist groups how religious principles, nationalistic or other imperatives as well as righteous ideologies have and are used as means to justify all kinds of reprehensible and destructive conducts. But besides extreme groups and context moral justifications is increasingly used in everyday life and to make unsustainable und unjust behaviour socially acceptable.\nOne often used form of moral disengagement is realised by using euphemistic language to describe reprehensible acts or conduct is another way that individuals or groups can morally disengage from their moral standards. By using innocuous word or expression in place of one that is deemed to suggests something problematic or unpleasant euphemism is a powerful way of influencing. For example, euphemisms are used to make unsustainable, injurious, and harmful behavior or implications acceptable or even respectable while reducing esponsibility for it from the person. With the help of intricate rephrasing detrimental unsustainable behaviour is made innocuous and acceptable, and people who are part of it are liberated from feeling sense of guilty. There exists different varieties of euphemisms including sanitising language or sanitise labelling, showing how disinfection can be infectious.\nHow sanitising it is to refer to the dangerous heating up and destructive dynamics on the planet Earth in and through the Anthropocene as ‘climate change’. How neutralising is calling for ‘climate neutrality’: A climate can never become neutral, but what is necessary is to reduce CO2 emissions that requires more radical transformation on all levels, of individual and collective mindsets, practices and institutions.\nBy disguising the (actually) problematic and deleterious with seemingly innocent wordings, the detrimental behaviour causing it itself becomes more acceptable, or respectable, wrongly normalised. This is often done for the sake of public interests or utilities, and sometimes express like in the language or military and terror/ language of the military and terrorism with terms like \"collateral damages\"; long term negative effects for future generations of species are vindicated. Sanitising euphemism by rephrasing have a long history. For example, the acid rain, which imposed destroying effects on lakes and forests has its own euphemistic label of \"atmospheric deposition of anthropogenically derived acidic substances\" as described by Hechinger (1995) is his analysis of distorting Orwellian double-speak. Deliberately obscuring, disguising, or distorting the meaning of words and sense-making double-speaking tries to alter the truth sound more palatable or producing intentional ambiguity in cloudy vague language or to actual inversions of meaning.\nExploiting the contrast principle, according to which perception of human conduct is influenced by what it is compared againstadvantageous comparison are deployed to make harmful behavior seem morally acceptable. Here, individuals and groups contrast their conduct with other examples of more immoral behavior , and in doing this comparison their own behavior is trivialised or relativised.\nThese exonerating comparisons are based on moral justification by utilitarian logic. Accordingly non-violent alternatives are judged to be ineffective to achieve desired changes and consequently eliminated as options. Furthermore, injurious actions are affirmatively justified as they will prevent more human suffering than they cause. However, the calculation process of estimating the significance of these potential threats remains subjective.\nDisengagement of moral may also causes, happens through or results in forms of displacement or diffusion of responsibility, as well as disregarding or misrepresenting injurious consequences. Furthermore, consequences of moral disengagement imply increased unethical decision making and deceptive behavior.\nMoral disengagement & unsustainable practices activities\nWhile conventionally, moral disengagement has been looked at as occurring/ examined in extreme cases or moral intensity scenarios and behaviors. But the, its significance of moral disengagement and self-serving practices at unsustainable and unjust expense of others in everyday situations, especially related to (un-)sustainability is becoming more important.\nThe disengagement of moral self-sanctions enables people to pursue detrimental practices freed from the restraint of self-censure also to various forms of unsustainable behaviour.\nFor example, people are investing ecologically harmful practices with seemingly worthy purposes through social, national, and economic justifications. This can be done by enlisting exonerative comparisons that render the practices righteous use of sanitising and convoluting language that disguises what is being done; reducing accountability by displacement and diffusion of responsibility; ignoring, minimising, and disputing harmful effects; and dehumanising and blaming the victims and derogating the messengers of ecologically bad news. Importantly, these psychosocial mechanisms operate at both the individual and social systems levels.\nAs empirically shown by Kilian and Mann (2020), people are likely to engage in self-serving moral reasoning (i.e., moral disengagement) when a presented consumption option with poor socio-ecological performance was perceived as desirable and when a suitable argument (i.e., moral disengagement cue) was available. They demonstrated that moral disengagement reduces moral feelings to forgo a consumption option with poor socio-ecological performance and fostered behavioural intentions towards it. For instance, consumers tend to willfully ignore and are disproportionately more likely to forget about socio-ecological product attributes than about “traditional” attributes such as quality or price for avoiding negative and potentially conflicting emotions in the presence of ethical issues (Reczek et al., 2018). Thus, desire for a specific even unsustainable consumption and possibilities for moral disengagement (i.e., a suitable argument) motivate moral disengagement that induce lower moral feelings to forgo a product with poor socio-ecological performance (e.g. shopping sweatshop-produced clothes) and in turn facilitated favorable purchase intentions, word-of-mouth intentions, as well as hampered willingness to pay for better socio-ecological performance (i.e., radical CO2 reduction). As Kilian & Mann stated while calling for specially tailored communication strategies and message frames that mitigate the potential for moral disengagement related to e.g., fair trade, organic products etc. “most persuasive and visible campaigns are worthless if consumers will finally overwrite their moral feelings with disputable reasons” (ibid 127).\nAnother form of moral distorted communication and decoupling is the practice of so-called brown-washing. Adding to the fraudulent and deceptive color games of white washing and green washing, brown-washing organisation are either do not reveal, silence or underreport their environmental, social, or governance activities or progress. These forms of non-communication or understating, thus hiding their light (Montgomery & Robertson, 2022) are strategically done also to avoid the risk of getting caught of unwarranted promises that could result in potential further stakeholder scrutiny and negative reactions, such as activist or NGO attacks. Or organisations might fear of being seen as hypocritical or attacked or punished in one way or another when perceived as greenhushing (Font et al., 2017). Both forms, exaggeration, also via covering up or divert attention from amoral, illicit, or criminal activities by establishing partnerships or connections with legitimate, value-based organizations, or lack of communication and undue modesty in corporate sustainability disclosure (Kim & Lyon 2015) are part of inventions and fraught uses of miscommunication. There seems to be no end in sight of such deceitful practices, while at the same time, we are no closer to solving the pressing environmental and social issues of our time (Montgomery et al., 2023).\nWe are living in a world where all these green claims are multiplying with much investing in using CSR and environmental, social, and governance (ESG) criteria and all kinds of commitments, while it seems that corrupting, delaying, and weakening of genuine moral engagement takes place.\nWhat we need is a moral re-engagement, including ‘going public’ while knowing that it takes extra time, energy, and conviction to forge a path that diverts from the norm. This is especially true in a scholarly community, as it involves additional, counter-normative work to engage and communicate with other audiences (Etzion & Gehman, 2019).\nOverall, in the near future, we will probably see many more forms of disengagement as the decoupling of the ‘talk’ and the ‘walk’ of humans and organisations also regarding claimed goodness and real actions will increase. Therefore, investigating the rhetorical modes of ethos, pathos, and logos as a form of communication to handle ethical issues may become even more important (Küpers & Minisri, 2023). Considering the role of digitally mediated communication, and current crisis situation, what will be important is a critique of the instrumentalising, and irresponsible approaches in communication, and developing more balanced, proto-wise forms for rendering enlivening practices. These may then contribute to mediating and fostering a personal, interpersonal, and institutional integrity through which a more sustainable world can be enacted now, for a forthcoming future to come.\nBandura, A. (1999). Moral Disengagement in the Perpetration of Inhumanities, Personality and Social Psychology Review. 3 (3): 193–209\nBandura, A. (2007). Impeding ecological sustainability through selective moral disengagement, International Journal of Innovation and Sustainable Development. 2 (1): 193–209.\nBandura, A.; Barbaranelli, C.; Caprara, Gian V.; Pastorelli, C. (1996). Mechanisms of moral disengagement in the exercise of moral agency\". Journal of Personality and Social Psychology. 71 (2): 364–374.\nEtzion, D., & Gehman, J. (2019). Going public: Debating matters of concern as an imperative for management scholars. Academy of Management Review, 44(2), 480–492.\nFont, X., Elgammal, I., & Lamond, I. (2017). Greenhushing: the deliberate under communicating of sustainability practices by tourism businesses. Journal of Sustainable Tourism, 25(7), 1007-1023.\nHechinger, F. M. (1985). Down with doublespeak. San Francisco Chronicle, This World section.\nKilian, S., & Mann, A. (2020). When the Damage is Done: Effects of Moral Disengagement on Sustainable Consumption. Journal of Organizational Psychology, 20(1). https://doi.org/10.33423/jop.v20i1.2764.\nKim, E.-H., & Lyon, T. (2015). Greenwash vs. brownwash: Exaggeration and undue modesty in corporate sustainability disclosure. Organization Science, 26(3), 705–723.\nKüpers, W. n & Minsri, K. (2023). The Role of Embodied, Living Ethos in relation to Pathos and Logos for Integral ‘Aesth-Ethical’ and Wise Practices’ (unpublished paper under review).\nMontgomery, A. W., & Robertson, J. L. (2022). Why Firms Hide Their Light: Brownwash, Silence, and Bifurcated Stakeholder Communication. In Academy of Management Proceedings Vol. 2022, No. 1, p. 12138.\nMontgomery, A. W., Lyon, T. P., & Barg, J. (2023). No End in Sight? A Greenwash Review and Research Agenda. Organization & Environment, https://doi.org/10.1177/10860266231168905\nReczek, R. W., Irwin, J. R., Zane, D. M., & Ehrich, K. R. (2018). “That’s not how i remember it: Willfully ignorant memory for ethical product attribute information”. Journal of Consumer Research, 45(1), 185-207.\nThis is the blog of Karlshochschule International University. Here you will find articles (in German and English) and debates on society, economy, culture or science. Students, staff and lecturers are writing here. You will find some news and stories from our study program, student life, our research or didactics."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:5a1a804f-1582-41b5-b72f-a149adc0ba55>","<urn:uuid:72cf76b5-98a9-4026-a6ac-4fa18176a7a5>"],"error":null}
{"question":"How does Roger Federer's ATP Finals winning record compare to Novak Djokovic's?","answer":"Roger Federer and Novak Djokovic are tied for the most ATP Finals titles, with six each. Federer won his titles in 2003, 2004, 2006, 2007, 2010, and 2011, while Djokovic won his in 2008, 2012, 2014, 2015, 2018, and 2021. Federer has appeared in more finals overall, having been runner-up four times (2005, 2012, 2014, 2015) compared to Djokovic's two runner-up finishes (2016, 2018).","context":["|Surface||Hard – indoors|\n|Draw||8S / 8D|\n|Prize money||US$5,700,000 (2020)|\n|Current champions (2020)|\n|Men's singles||Daniil Medvedev|\n|Men's doubles|| Wesley Koolhof|\nThe ATP Finals is the second highest tier of annual men's tennis tournaments after the four Grand Slam tournaments. A week-long event, the tournament is held annually each November at the Pala Alpitour in Turin, Italy. The ATP Finals are the season-ending championships of the ATP Tour and feature the top eight singles players and doubles teams of the ATP Rankings. The tournament has been one of the popular candidates for the monicker of \"the fifth grand slam\". The tournament was first held in 1970, although it was known under a different name. Roger Federer holds the record for the most singles titles with six, while Peter Fleming and John McEnroe hold the record for the most doubles titles with seven. In the current tournament, winners are awarded up to 1500 ranking points; with each round-robin loss, 200 points are deducted from that amount.\nThe event is the fourth evolution of a championship which began in 1970. It was originally known as the Masters Grand Prix and was part of the Grand Prix tennis circuit. It was organised by the International Lawn Tennis Federation (ILTF). It ran alongside the competing WCT Finals. The Masters was a year-end showpiece event between the best players on the men's tour, but did not count for any world ranking points.\nIn 1990, the Association of Tennis Professionals (ATP) took over the running of the men's tour and replaced the Masters with the ATP Tour World Championship. World ranking points were now at stake, with an undefeated champion earning the same number of points they would for winning one of the four Grand Slam events. The ITF, who continued to run the Grand Slam tournaments, created a rival year-end event known as the Grand Slam Cup, which was contested by the 16 players with the best records in Grand Slam competitions that year.\nIn December 1999, the ATP and ITF agreed to discontinue the two separate events and create a new jointly-owned event called the Tennis Masters Cup. As with the Masters Grand Prix and the ATP Tour World Championships, the Tennis Masters Cup was contested by eight players. However, player who is ranked number eight in the ATP Champion's Race world rankings does not have a guaranteed spot. If a player who wins one of the year's Grand Slam events finishes the year ranked outside the top eight but still within the top 20, he is included in the Tennis Masters Cup instead of the eighth-ranked player. If two players outside the top eight win Grand Slam events, the higher placed player in the world rankings takes the final spot in the Tennis Masters Cup.\nIn 2009, the Masters was renamed the ATP World Tour Finals and was held at The O2 in London. The contract ran through 2013, but was extended up to 2015 in 2012, and another time until 2018 in 2015. In 2017 the event was renamed the ATP Finals and the contract with the O2 Arena was extended to 2020. In December 2018 it was announced that London, along with Manchester, Singapore, Tokyo and Turin were on a shortlist of five cities which made the cut from an initial list of 40 to host the event from 2021. In April 2019 the ATP announced that Turin is going to host the ATP finals from 2021 to 2025.\nFor many years, the doubles event was held as a separate tournament the week after the singles competition, but more recently they have been held together in the same week and venue.\nFor most of its history, the event has been considered as the most important indoor tennis tournament on the world tour (there were a few exceptions, when the event was organized outdoors: 1974 Melbourne & 2003–2004 Houston), allowing for controlled conditions of play, regarding both surface type and illumination system.\nIn recent years it has been played on indoor hard courts, however, indoor carpet has featured for many editions previously. Once when Melbourne hosted it in 1974 the grass courts of Kooyong Stadium were used and occurred a few weeks before the 1974 Australian Open, which were also played on grass. Apart from 1974, all tournaments have been on a hard court variant, which has prompted calls, primarily from Rafael Nadal to feature a mix of surfaces and include clay courts. However, this has drawn criticism as well as suggestions to reduce the number of clay court tournaments in the season and the ATP are not keen to change this aspect of the tournament.\nThere are eight players or teams, and playing is mandatory except for injury or other good cause.\nQualification is as follows:\n(a) the top seven players in the ATP rankings (b) up to two grand slam winners ranked between 8 and 20 (in order of ATP ranking, if any such players exist) (c) the next players in the ATP rankings, until the quota of eight is reached.\nPoints, prize money and trophies\nThe ATP Finals currently (2020) rewards the following points and prize money, per victory:\n|Round Robin (each of 3 matches)||$153,000||$30,000||200|\n- 1 Prize money for doubles is per team.\nThere is also an appearance fee of $153,000 singles, and $68,500 per doubles team. The two alternates are paid $73,000 (singles) and $25,000 (doubles teams).\nAn undefeated champion would earn the maximum 1,500 points, and $2,114,000 in singles or $354,500 in doubles.\nUnlike all other singles events on the men's tour, the ATP Finals is not a straightforward knock-out tournament. Eight players are divided into two groups of four and play three round-robin matches each against the other players in their group. The two players with the best records in each group progress to the semifinals, with the winners meeting in the final to determine the champion. Though it is theoretically possible to advance to the semi-finals of the tournament with two round-robin losses no player in the history of the singles tournament has won the title after losing more than one round-robin match.\nThe current round robin format of two groups of four players progressing to a semifinal and final, has been in place for all editions of the tournament except the following years:\n- 1970, 1971 – Round robin with no semifinals or finals, winner decided on best performed player\n- 1982, 1983, 1984 – 12 player knock-out tournament with no round robin. The top four seeds in the event received a bye in the first round.\n- 1985 – 16 player knock-out tournament with no round robin\nAs of 2019, the top two players from each group advance to the semi-finals. Round-robin standings are determined by: 1) Number of wins; 2) Number of matches; 3) In two-players-ties, head-to-head results; 4) In three-players-ties, percentage of sets won, then head-to-head result (if two players tied in percentage of sets won and third one is \"different\") or percentage of games won if all three players have same percentage of sets won, then head-to-head results; 5) ATP rankings.\nThe tournament has traditionally been sponsored by the title sponsor of the tour; however, in 1990–2008 the competition was non-sponsored, even though the singles portion of the event as part of the ATP tour was sponsored by IBM. In 2009, the tournament gained Barclays PLC as title sponsor. Barclays confirmed in 2015 that they would not renew their sponsorship deal once it expires in 2016.\nOn 10 September 2020, NItto Denko announced it will extend its title partnership of the ATP Finals for another 5 years, until 2025.\n|Tokyo, Japan||1970||Carpet (i)||Tokyo Metropolitan Gymnasium||6,500|\n|Paris, France||1971||Stade Pierre de Coubertin||5,000|\n|Barcelona, Spain||1972||Hard (i)||Palau Blaugrana||5,700|\n|Boston, United States||1973||Carpet (i)||Boston Garden||14,900|\n|Melbourne, Australia||1974||Grass||Kooyong Stadium||8,500|\n|Stockholm, Sweden||1975||Carpet (i)||Kungliga tennishallen||6,000|\n|Houston, United States||1976||The Summit||16,300|\n|New York City, United States||1977–1989||Madison Square Garden||18,000|\n|Frankfurt, Germany||1990–1995||Festhalle Frankfurt||12,000|\n|Hanover, Germany||1996–1999||Carpet (i)||Hanover Fairground||15,000|\n|Hard (i) (1997)|\n|Lisbon, Portugal||2000||Hard (i)||Pavilhão Atlântico||12,000|\n|Sydney, Australia||2001||Acer Arena||17,500|\n|Houston, United States||2003–2004||Hard||Westside Tennis Club||5,240|\n|Shanghai, China||2005–2008||Carpet (i)||Qizhong Forest Sports City Arena||15,000|\n|Hard (i) (2006–2008)|\n|London, United Kingdom||2009–2020||Hard (i)||O2 Arena||20,000|\n|Turin, Italy||2021–2025||Pala Alpitour||16,600|\n|Titles||Player||Years Won||Years Runner-up|\n|6||Roger Federer||2003, 2004, 2006, 2007, 2010, 2011||2005, 2012, 2014, 2015|\n|5||Ivan Lendl||1981, 1982, 1985, 1986, 1987||1980, 1983, 1984, 1988|\n|Novak Djokovic||2008, 2012, 2013, 2014, 2015||2016, 2018|\n|Pete Sampras||1991, 1994, 1996, 1997, 1999||1993|\n|4||Ilie Năstase||1971, 1972, 1973, 1975||1974|\n|3||Boris Becker||1988, 1992, 1995||1985, 1986, 1989, 1994, 1996|\n|John McEnroe||1978, 1983, 1984||1982|\n|2||Björn Borg||1979, 1980||1975, 1977|\n|Lleyton Hewitt||2001, 2002||2004|\n|1||Andre Agassi||1990||1999, 2000, 2003|\n|Stan Smith||1970||1971, 1972|\n|0||Vitas Gerulaitis||1979, 1981|\n|Jim Courier||1991, 1992|\n|Rafael Nadal||2010, 2013|\n|Dominic Thiem||2019, 2020|\n|Juan Carlos Ferrero||2002|\n|Juan Martín del Potro||2009|\n- Active players marked in bold.\n|Titles||Player||Years Won||Years Runner-up|\n|7||Peter Fleming||1978, 1979, 1980, 1981, 1982, 1983, 1984|\n|John McEnroe||1978, 1979, 1980, 1981, 1982, 1983, 1984|\n|5||Mike Bryan||2003, 2004, 2009, 2014, 2018||2008, 2013|\n|4||Bob Bryan||2003, 2004, 2009, 2014||2008, 2013|\n|Daniel Nestor||2007, 2008, 2010, 2011||1998, 2006|\n|3||Anders Järryd||1985, 1986, 1991||1989, 1992|\n|Rick Leach||1988, 1997, 2001|\n|2||Todd Woodbridge||1992, 1996||1993, 1994|\n|Mark Woodforde||1992, 1996||1993, 1994|\n|Max Mirnyi||2006, 2011||2009, 2010|\n|Jacco Eltingh||1993, 1998||1995|\n|Paul Haarhuis||1993, 1998||1995|\n|Nenad Zimonjić||2008, 2010||2005|\n|Stefan Edberg||1985, 1986|\n|Jonas Björkman||1994, 2006|\n|Henri Kontinen||2016, 2017|\n|John Peers||2016, 2017|\n|1||Sherwood Stewart||1976||1982, 1984|\n|John Fitzgerald||1991||1989, 1992|\n|Mark Knowles||2007||1998, 2006|\nMost singles titles:\nMost consecutive singles titles:\n- Novak Djokovic – 4 (2012–2015)\n- Ivan Lendl – 3 (1985–1987)\nIlie Năstase – 3 (1971–1973)\n- Roger Federer – 2 (2003–2004), (2006–2007), (2010–2011)\nPete Sampras – 2 (1996–1997)\nJohn McEnroe – 2 (1983–1984)\nBjörn Borg – 2 (1979–1980)\nLleyton Hewitt – 2 (2001–2002)\nIvan Lendl – 2 (1981–1982)\nMost singles match wins:\nMost singles match wins % (minimum 10 matches played):\n- Ilie Năstase – 88.0% (22–3)\n- Ivan Lendl – 79.6% (39–10)\n- Roger Federer – 77.6% (59–17)\n- Boris Becker – 73.5% (36–13)\n- Björn Borg – 72.7% (16–6)\nMost singles appearances:\n- Roger Federer – 17 (2002–2015, 2017–2019)\n- Andre Agassi – 13 (1988–1991, 1994, 1996, 1998–2003, 2005)\nNovak Djokovic – 13 (2007–2016, 2018–2020)\n- Ivan Lendl – 12 (1980–1991)\n- Boris Becker – 11 (1985–1992, 1994–1996)\nJimmy Connors – 11 (1972–1973, 1977–1984, 1987)\nPete Sampras – 11 (1990–2000)\nYoungest singles champion:\n- John McEnroe – 19 years, 11 months (1978)\nOldest singles champion:\n- Roger Federer – 30 years, 3 months (2011)\nMost doubles titles:\nMost consecutive doubles titles:\n- Peter Fleming – 7 (1978–1984)\nJohn McEnroe – 7 (1978–1984)\n- Stefan Edberg – 2 (1985–1986)\nAnders Järryd – 2 (1985–1986)\nBob Bryan – 2 (2003–2004)\nMike Bryan – 2 (2003–2004)\nDaniel Nestor – 2 (2007–2008), (2010–2011)\nHenri Kontinen – 2 (2016–2017)\nJohn Peers – 2 (2016–2017)\nMost doubles match wins:\n- Mike Bryan – 42\n- Bob Bryan – 38\n- Daniel Nestor – 34\n- Todd Woodbridge – 29\n- Anders Järryd – 25\nMark Woodforde – 25\nMost doubles match wins % (minimum 10 matches played):\n- John McEnroe – 100% (14–0)\n- Peter Fleming – 88.9% (16–2)\n- Anders Järryd – 78.1% (25–7)\n- Jacco Eltingh – 76.9% (20–6)\n- Stefan Edberg – 75% (9–3)\nMost doubles appearances:\n- Mike Bryan – 16 (2001, 2003–2006, 2008–2018)\n- Bob Bryan – 15 (2001, 2003–2006, 2008–2017)\n- Daniel Nestor – 15 (1995–1998, 2003–2012, 2014)\n- Leander Paes – 14 (1997–2001, 2005–2013)\n- Mahesh Bhupathi – 12 (1997–2001, 2003–2004, 2008–2012)\nMark Knowles – 12 (1995–1998, 2001, 2003–2009)\nYoungest doubles champion:\n- John McEnroe – 19 years, 11 months (1978)\nOldest doubles champion:\n- Mike Bryan – 40 years, 7 months (2018)\nYear-end championships triple\nWinning the Masters Cup, the WCT Finals and the Grand Slam Cup during his career.\n- The event at which the year-end championships triple was achieved indicated in bold below:\n|#||Player||Grand Prix/ATP Masters Cup||WCT Finals||Grand Slam Cup|\nYear-end championships doubles\nWinning the Masters Cup and the WCT Finals, or the Masters Cup and the Grand Slam Cup, or the WCT Finals and the Grand Slam Cup.\n- The event at which the year-end championships double was achieved indicated in bold below:\nMasters Cup – WCT Finals\n|#||Player||Grand Prix/ATP Masters Cup||WCT Finals|\n* he later completed the Y-EC Triple\nMasters Cup – Grand Slam Cup\n|#||Player||Grand Prix/ATP Masters Cup||Grand Slam Cup|\n* with the 1996 Grand Slam Cup title he also completed the Y-EC Triple\nWCT Finals – Grand Slam Cup\n|#||Player||WCT Finals||Grand Slam Cup|\n* with the 1996 Grand Slam Cup title he also completed the Y-EC Triple\nYear-end championships generations double\nWinning the ATP Finals and the Next Gen ATP Finals during his career.\n- The event at which the year-end championships generations double was achieved indicated in bold below:\n|#||Player||ATP Finals||Next Gen ATP Finals|\n- \"Why Indian Wells Is Almost (But Not Quite) a Fifth Slam\".\n- John Barrett, ed. (1991). The International Tennis Federation : World of Tennis 1991. London: Collins Willow. pp. 116, 140. ISBN 9780002184038.\nBesides the prize money of $2,020,000, there were also ranking points at stake for the first time at a season ending play-off\n- Piers Newbery (3 July 2007). \"London to host World Tour Final\". BBC Sport. Archived from the original on 7 March 2016.\n- \"ATP finals to stay in London through 2015\". The Times Of India. Archived from the original on 2012-11-08. Retrieved 2012-07-11.\n- \"ATP World Tour Finals to be showcased in London till 2015\". Archived from the original on 2012-11-13. Retrieved 2012-09-11.\n- \"ATP Confirms London As Host City Through 2018 As 2015 Season Finale Is Officially Launched | ATP World Tour | Tennis\". ATP World Tour. Archived from the original on 2016-11-17. Retrieved 2016-11-16.\n- \"ATP World Tour Finals to stay in London till 2020 under new title sponsor\". The Guardian. 25 May 2017. Archived from the original on 25 May 2017.\n- \"ATP Extends Season-Ending Finale In London Through 2020 With New Title Partner Nitto Denko Corporation\". Association of Tennis Professionals (ATP). 25 May 2017.\n- \"ATP Finals: Manchester & London on five-city shortlist to host event from 2021\". BBC Sport. 2018-12-14.\n- \"Turin To Host ATP Finals From 2021 To 2025\". ATP. 2019-04-24. Retrieved 2019-11-15.\n- \"ITF Tennis - Pro Circuit - Masters Singles - 10 December - 15 December 1974\". www.itftennis.com. Retrieved 2018-12-17.\n- \"I never played ATP Finals on clay or outdoor, complains Rafael Nadal\". Tennis World USA. Retrieved 2018-12-17.\n- \"Darren Cahill calls for ATP to make surface change at ATP Finals\". Tennis World USA. Retrieved 2018-12-17.\n- \"ATP urged to change Finals surface to give Rafael Nadal a better chance\". Tennis365.com. 2018-12-15. Retrieved 2018-12-17.\n- Ubha, R. (5 November 2013). \"Nadal and Federer at loggerheads over ATP World Finals\". CNN. Retrieved 17 December 2018.\n- \"Does the clay-court season take up too much of the tennis calendar?\". ESPN.com. 2018-05-22. Retrieved 2018-12-17.\n- \"ATP Finals won't be played on clay, says Chris Kermode\". Tennis World USA. Retrieved 2018-12-17.\n- \"ATP sets Double Challenge Cup for Jan. 29-Feb. 2 in Bangalore\". Associated Press AP. 2002-01-16. Archived from the original on 2012-11-04. Retrieved 2008-11-03.\n- \"Points and Prize Money - Nitto ATP Finals\". nittoatpfinals.com. Archived from the original on 1 November 2017. Retrieved 9 May 2018.\n- \"Thomas Lyte lifts Webb Ellis Cup\". 2015-09-15. Archived from the original on 2015-11-17. Retrieved 2015-11-13. Thomas Lyte Lifts Webb Ellis Cup\n- \"In pictures: Sporting trophy workshop\". BBC News. 9 May 2018. Archived from the original on 27 November 2017. Retrieved 9 May 2018.\n- \"Semi-final Qualifying Procedure\". Nitto ATP Finals.\n- \"ATP agree $35 million deal for showpiece tournament\". Reuters. 2008-06-18. Archived from the original on 2010-11-07.\n- \"Barclays to end World Tour Finals sponsorship\". BBC News. 4 November 2015. Archived from the original on 7 November 2015. Retrieved 13 November 2015.\n- \"ATP extends season-finale in London through 2020 with new title partner Nitto Denko Corporation\". London: Nitto ATP Finals. 25 May 2017. Archived from the original on 27 September 2017. Retrieved 28 August 2017.\n- \"ATP & Nitto Denko Corporation Extend Partnership Until 2025\". ATP Tour. 10 September 2020. Retrieved 20 November 2020.\n- \"Two for Smith\". The Province. 6 December 1971. p. 17.\n- \"That Rumanian black magic\". Sports Illustrated. Retrieved 27 October 2019.\n- O2, The. \"Event space capacities, The O2\". www.theo2.co.uk. AEG, 2016. Archived from the original on 20 December 2016. Retrieved 8 December 2016.\n- Turin To Host ATP Finals From 2021 To 2025\n- \"Barcays ATP World Tour Finals – Historical Stats\". ATP Tour. Retrieved 6 December 2014.\n|Wikimedia Commons has media related to ATP Finals.|","Top five players with most titles won in ATP Finals\nNovak Djokovic is the only active player in the list.\nThe ATP Finals is the season-ending championship of the tennis tour. It is the most significant men’s tennis event after the four Grand Slams, and it features the top eight singles players and top doubles team based on their results throughout the season. The ATP competition was first held in 1970 and has been conducted every year.\n54th edition of the tournament, is scheduled to take place at Turin’s Pala Alpitour stadium in Turin, Italy, from 12 to 19 November 2023. Roger Federer and Novak Djokovic share the record for most ATP Finals titles, with six each. Federer won his titles in 2003, 2004, 2006, 2007, 2010, and 2011 with Djokovic in 2008, 2012, 2014, 2015, 2018, and 2021.\nIn this article we will take a look at the top five players with most titles won at the ATP Finals:\nRoger Federer (6)\nThe Swiss maestro, Roger Federer, is one of the most successful players in ATP Finals history. He has won the tournament a record six times, and has also reached the finals three other times. Federer is known for his elegant style of play and his incredible shot-making ability. He is also one of the toughest players mentally on the tour.\nHis balletic court presence and strategic brilliance have made him a perennial contender, securing victories in 2003, 2004, 2006, 2007, 2010, and 2011.\nNovak Djokovic (6)\nDjokovic, asserting his dominance in the recent tennis landscape, is another one of the most accomplished players in ATP Finals history. The 24-time grand slam winner has won the tournament a record six times, and has also reached the final three other times. The Serbian is known for his incredible athleticism and relentless game.\nIvan Lendl (5)\nLendl was one of the most dominant players in ATP Finals history. He won the tournament five times, and he reached the finals thirteen consecutive times. His reign in the tournament spanned from 1981 to 1987, showcasing a level of consistency and excellence that defined an era.\nPete Sampras (5)\nThe American legend Pete Sampras shares the spotlight with five ATP Finals titles, a testament to his prowess on the court during the 1990s. His victories came in 1991, 1994, 1996, 1997, and 1999.\nIlie Nastase (4)\nThe Romanian tennis legend Ilie Nastase was one of the most exciting players in ATP Finals history. He won four titles, showcasing his skill and dominance in the early 1970s. Nastase’s victories came in 1971, 1972, 1973, and 1975. Ilie Nastase was known for his aggressive style of play and his powerful serve.\nThese tennis legends, with their multiple ATP Finals titles, have not only left an indelible mark in the tournament but have also contributed to the rich tapestry of tennis history. Their strategic brilliance, unwavering consistency, and ability to shine on the grand stage have solidified their places as true legends of the sport.\nThe ATP Finals, through the years, has been witness to the brilliance of these players, each adding a unique chapter to the tournament’s illustrious legacy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:264083c5-bb62-4cd0-9564-83c7f8d25e95>","<urn:uuid:8703b5d8-8a36-4e58-b58f-735895a218fd>"],"error":null}
{"question":"How do enzymes function in the body?","answer":"Enzymes are complex proteins made by cells that act as catalysts for specific biochemical reactions without changing themselves. They control various bodily functions including digestion, muscle contraction, and many other aspects of metabolism.","context":["First discovered in human adenoids (an adenoid is a tissue at the back of the throat),\nthis group of 20-sided viruses causes respiratory and other diseases. Adenoviruses contain\nOne form of a gene: for example, the gene for eye color has alleles for brown, blue,\nblack, and green eyes. An allele may be dominant or recessive.\nAn over-reaction by the body to substances that usually do not bother the average\nperson. Reactions include sneezing, itching, runny eyes and nose, rashes, wheezing, and\nCompounds that link together to make proteins. Essential amino acids are released in\nthe intestines when food containing protein is digested -- the body cannot make them.\nNon-essential amino acids can be made by the body. Amino acids contain carbon, hydrogen,\noxygen, nitrogen, and sometimes sulfur.\nResults when the blood doesnt have enough red blood cells, hemoglobin, or total\nvolume. Causes include loss of blood, an iron deficiency (so that not enough blood is\nformed), or a bone marrow disfunction where the blood is made. Symptoms include being\ntired and bone pain.\nThe process of taking blood, cleansing it or adding\ncertain substances, and returning the blood to the patient.\nA blood vessel which carries blood rich in oxygen and nutrients from the heart to the\ncells in the body.\nA single-celled microscopic organism. Bacteria live in dirt and water, or in plants and\nanimals. Some bacteria aid digestion, while others can cause diseases like pneumonia,\ndiphtheria, and tuberculosis. They can be round, rod-shaped or spiral. Vaccinations help\nincrease immunity to some of the diseases bacteria carry while antibiotics can fight\nA person who has one recessive gene for a recessive genetic disease. Because the person\nhas only one recessive gene, s/he does not have the disease, but can pass the disease gene\non to her/his children.\nThe smallest structural unit of life capable of functioning by itself, a cell has a\nmembrane which allows some things to pass through. Inside the cell are one or more nuclei\nand other cellular parts. Cells contain DNA and make up our bodies.\nA steroid alcohol which regulates certain cell functions and helps build cell\nStrands of deoxyribonucleic acid contain all or most of the genes of an individual. The\nnumber of chromosomes is specific to an animal or plant -- humans have 46.\nMade of three consecutive nucleotides, codons specify a particular amino acid in a\nprotein or start or stop proteins from being made.\nMicroorganisms (like bacteria or viruses) or tissues from plants and animals which are\ngrown outside the body in a prepared medium in a laboratory.\nThe breakdown of a chemical compound into a less complex compound.\nDeoxyribonucleic acid (DNA)\nThe molecular basis for heredity, formed in a double helix and held together by\nhydrogen bonds between purine and pyrimidine bases. DNA is found in the cell nucleus.\nA contagious disease caused by a bacteria, which inflames\nthe heart and nervous system and causes the formation of false membranes,\nespecially in the throat and air passeages.\nSegments of DNA known to be linked with inheritable traits or diseases. They may not\ncause the condition but always appear with the genes that do. Markers are used to find the\nlocations of genes on chromosomes.\nOne of a pair of alleles that suppresses the expression of the other in heterozygous\n(having one dominant and one recessive allele) conditions.\nA blockage of a blood vessel caused by an abnormal particle -- like an air bubble --\ncirculating in the blood.\nA complex protein made by a cell which is a catalyst (something that starts a reaction\nor chain of events) for a specific biochemical reaction which does not change the enzyme.\nEnzymes control digestion, muscle contraction, and many other functions of metabolism.\nWhen the trait a gene codes for shows in an organisms phenotype, or body, in a\nWhen a disease occurs in more people in a family than could be expected by chance\nalone, the disease is familial or inherited in that family.\nRelating to or coming from an unborn animal, especially an animal in the later stages\nof pre-natal development.\nA specific sequence of nucleotides in DNA or RNA, it is the unit of inheritance. A gene\ncodes for the expression of a trait by specifying the structure of a certain protein.\nUsually found on a specific place on a chromosome, genes reproduce exactly during cell\ndivision, and usually occur in pairs, except for those genes on the sex chromosomes X and\nAll the genetic information that makes an organism.\nA protein in red blood cells that contains iron and transports oxygen from the lungs to\nthe rest of the body.\nWhen a gene has one dominant and one recessive allele.\nWhen a genes alleles are either both dominate or recessive.\nHuman growth hormone\nA protein hormone produced by the pituitary gland that promotes growth, fat\nmobilization, and inhibition of glucose utilization.\nA cellular response to injury or stimulation from physical, chemical or biological\nagents. When cells inflame, they turn red, heat up, are painful, and swell to get rid of\nnoxious agents and damaged tissue.\nTo receive a gene from a parent during reproduction; in humans, fetuses inherit half of\ntheir genes from their fathers and half from their mothers.\nA protein hormone made by the pancreas which is necessary for metabolizing\ncarbohydrates and sugars and which is used in the treatment and control of diabetes.\nA fatty droplet of spherical lipid in suspension in tissue.\nA cell made by stem cells (cells in the bone marrow which make many other kinds of\ncells) that goes to the lymphoid tissue in the thymus or bone marrow. They fight\ninfections and make up a large portion of cells in the blood.\nA contagious viral disease marked by a fever and rash on the neck and face. Once\nyouve had measles, you generally are immune to it.\nA skin tumor containing dark pigment which may be benign or malignant; when malignant\n(or cancerous) it spreads rapidly and widely.\nA fine-pointed tube used for measuring or dispensing very small amounts -- the fluid is\ndrawn by suction and kept in by closing the top end of the tube.\nThe smallest unit that something can be divided into without changing its properties.\nThe speed of molecular motion and space between them determine if somethings a\nliquid, solid, or gas. Molecules are made of atoms and differ from each other in size,\nweight, and structure. Chemical action affects molecules.\nA slippery wet substance secreted to moisten and protect tissues -- the most commonly\nknown is nasal mucus.\nA contagious viral disease with fever and swelling of the parotid gland and the\nHaving to do with the structure or function of the nervous system, which includes the\nbrain, spinal cord, nerves, ganglion; and/or having to do with the receptors or organs\nthat receive and interpret stimulations and send impulses to the brain and affector\nA basic structural unit of RNA and DNA, made of a ribose or deoxyribose sugar and a\npurine or pyrinidine base and a phosphate group.\nThe part of the cell which is enclosed in its own membrane within the cell and which\ncontains the materials to make the chromosomes -- important in reproduction and protein\nA living being or individual.\nA complex substance made of amino acids and which include many compounds necessary for\nlife like hormones, enzymes, and immunoglobins. Proteins are found in cells and tissue.\nThe type of protein depends on the kinds and number of amino acids that make it. Proteins\nare used in building cells, cell functions, muscle contraction, digestion, growth, etc.\nA cell or group of cells that responds to various stimuli, or a molecule on the surface\nof a cell or in the cell that responds to a certain chemical, molecule, or virus, and then\nmakes the cell do something in reaction such as produce something, stop making something,\nlet the stimulating material enter the cell, etc.\nAn allele which has little or no effect on the phenotype of an organism when paired\nwith a more dominant allele; it only has an effect when paired with another recessive\nMade by cutting up the DNA of one species (usually a bacterium) and inserting the genes\nof another species so that the bacterium produces the protein or hormone of the other\nA virus which contains RNA and which uses its RNA as a template to produce its own DNA\nwhen incorporated into the genes of infected cells.\nA blockage of a blood vessel due to a blood clot.\nKeeping alive and growing body tissue in the laboratory, outside the organism in a\nsterile culture medium.\nAn inherited characteristic which is found in the phenotype\n(the physical appearance) or genotype (the genetic material) of an organism.\nTraits include eye color, hair color, the shape of body parts and genetic diseases.\nA mass of tissue with no physiological function that arises from tissue for no obvious\ncause; it may be benign or malignant (cancerous).\nA cell isolated from a malignant tumor, cultured with interleukin-2, and injected back\ninto the patient as a tumor-killing cell that has greater toxicity.\nA blood vessel that returns blood from the body to the heart to pick\nup more oxygen.\nA disease-causing agent that is able to live and reproduce only inside\nliving cells. Viruses cause many infectious diseases.\nWhite blood cell(s)\nA colorless blood cell (because it doesnt have hemoglobin) that fights invaders\nto the body and includes lymphocytes, monocytes, neutrophils, eosinophils, and basophils.\nAn infectious disease caused by a bacteria that spawns a convulsive cough sometimes\nfollowed by a crowing breath. Also called \"pertussis.\"\nOne of two kinds of chromosomes which help determine the sex of an organism -- females\nhave two X-chromosomes, males have one (a Y-chromosome is the other chromosome which helps\nA genetic condition arising when an allele coding for a specific genetic trait or disease is only found on the\nX-chromosome. If such an allele is recessive, a female (who has two X- chromosomes)\ndoesnt get the trait, but a male (who has only one X- chromosome) does. X-linked\ntraits are carried by mothers and passed to their sons.\nOne of two kinds of chromosomes which help determine the sex of an organism -- males\nhave one Y-chromosome, females have none (an X-chromosome is the other chromosome which\nhelps determine sex)."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:0ff55483-5490-4251-9e85-2b6374ac2d69>"],"error":null}
{"question":"What limitations exist in current greenhouse gas measurement methods for businesses?","answer":"Current GHG measurement methods like the Climate Footprint Calculator, Global Reporting Initiative (GRI), and resources from the World Business Council for Sustainable Development and World Resources Institute lack a consistent set of indicators that includes carbon inputs and outputs from both direct and indirect business activities. This makes it difficult to evaluate business response to climate change.","context":["These four indicators will help your business measure greenhouse gases across the value chain.\nBusiness response to climate change has been difficult to evaluate, partly because emissions across the value chain are not easily measured. Currently, GHGs are measured using the Climate Footprint Calculator, the Global Reporting Initiative (GRI), and resources from the World Business Council for Sustainable Development and the World Resources Institute. Yet, none of these offer a consistent set of indicators that includes carbon inputs and outputs from both direct and indirect business activities.\nResearchers Volker H. Hoffmann, and Timo Busch present an approach to standardize the assessment of a company’s carbon performance indicators in their paper, “Corporate Carbon Performance Indicators: Carbon Intensity, Dependency, Exposure and Risk.“\nManagers can apply a standardized approach to evaluate their company’s carbon performance. The approach builds on four corporate carbon performance measures that describe both the physical effects (i.e. carbon intensity and dependency) and the financial impact (i.e. carbon exposure and risk) of carbon flow in the present and the future.\n4 Indicators to Evaluate a Company’s Carbon Performance\nPhysical indicators (intensity and dependency) help identify the steps in the value chain where companies can reduce their GHG emissions. Financial indicators (exposure and risk) reveal the carbon costs and risks underlying a company’s business activities.\nCarbon Intensity: Relates carbon usage to business performance. Calculated as the firm’s carbon usage for the year divided by a financial metric (e.g. sales) for the same time period.\nCarbon Dependency: The change in a company’s use of carbon (intensity) over a given time period, expressed as a percentage.\nCarbon Exposure: Financial implications of carbon use for a given time period. Relates a company’s carbon costs to another financial metric (e.g. sales).\nCarbon Risk: The change in monetary carbon performance over a given time period, expressed as a percentage.\nImplications for Managers, Financial Analysts, and Policy Makers\nManagers can use the proposed approach to assess their carbon-reducing strategies and efforts. They should adopt a consistent approach to measuring and reporting carbon inputs and outputs to provide a more complete perspective on their company’s carbon use. This approach includes a broader scope of upstream and downstream effects, allowing managers to identify underlying carbon costs throughout their business activities.\nFinancial analysts and policy makers can also apply this framework to evaluate investment strategies and government policies. Financial analysts can use physical indicators to evaluate management’s effectiveness, while financial indicators can help analysts optimize portfolios and determine risk premiums. Policy makers can use physical indicators to identify targets for carbon reduction policies across the value chain.\nImplications for Researchers\nBusch and Volker offer a conceptual analysis of carbon performance indicators after reviewing existing indicators and metrics. They define four performance indicators and highlight their implementation through two examples. The authors focus on scope 1-2 carbon usage, which includes direct carbon inputs-outputs for a firm as well as carbon usage from purchased energy. They note that LCA is still a more appropriate measure of product life-cycle-wide impacts.\nThe researchers propose extending current frameworks, i.e. GRI and the Carbon Disclosure Project, to include indirect impacts. Future research should facilitate measurement of scope 1-3 carbon usage to incorporate impacts across the supply chain; but, an improved understanding of industry-specific conventions would first be required.\nHoffmann, Volker H., & Busch, Timo. (2008). Corporate Carbon Performance Indicators: Carbon Intensity, Dependency, Exposure and Risk. Journal of Industrial Ecology. 12(4), 505-520."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:a5bd64d3-3d6a-4930-81f4-d78e8b6518c6>"],"error":null}
{"question":"I am parent of troublesome kid - how attachment theory explain parent role for preventing delinquency, and what parent training techniqes can help?","answer":"Attachment to parents plays a crucial role in preventing delinquent behavior, as children who are strongly attached to their parents are generally less likely to commit criminal acts. When parents act as role models and teach their children what is socially acceptable behavior, they help build strong affective ties. Research shows that children are much less likely to engage in risky behavior when their parents are easy to talk to and provide adequate supervision. Parent Management Training (PMT) offers specific techniques to help parents, including: providing positive reinforcement for appropriate behaviors through praise and attention, consistently responding to negative behaviors by removing rewards or applying appropriate consequences, and learning to observe and record behavior patterns. PMT has shown effectiveness in changing children's behavior and maintaining improved behavior over the long term.","context":["Travis Hirschi’s Social Control Theory\nMickey A. Syrquin\nTexas Christian University\nTravis Hirschi belly that human beings were inherently hedonistic by nature. He said that, “we are all animals and thus naturally capable of committing criminal acts” (Hirschi, 1969:31). He chose to approach criminology in a completely different way than most of his peers, and in doing so he came up with several ground-breaking theories that are still at the center of the criminological world today. The 1960s was a decade filled with societal non-conformity. Rock and Roll had taken the music world by storm causing drug use and risky sexual behavior to reach an all-time high. From masses protesting the Vietnam War to the civil rights movement led by Dr. Martin Luther King Jr. there was an opportunity to behave delinquently around every turn. In 1969 Hirschi released one of his most famous pieces of work, a book called Causes of Delinquency. This book has been a staple in the criminological world for years and is often referred to for theory construction and research in the delinquency field. The piece laid out Hirschi’s social control theory, (sometimes called social bond theory) which is what I will be reviewing in this paper.\nAccording to Wiatrowski (1981), contrary to popular belief Hirschi’s social control theory implied that since delinquency is actually intrinsic to human nature, it is conformity that must be explained. Hirschi explains conformity as being “achieved through socialization, the formation of a bond between individual and society comprised of four major elements: attachment, commitment, involvement and belief” (Wiatrowski, 1981, p. 525). All four of these elements are what make up the social bond, and the stronger each of these four elements are, the less likely the individual will be to partake in delinquent behavior. Hirschi’s theory proposes that the weaker the group to which an individual belongs, the less he depends on them. Which causes the individual to depend more on himself and he will eventually realize no other rules of conduct unless they benefit his own private interests (Hirschi, 1969).\nThe first element of the social bond is attachment. According to Hirschi, “Attachment corresponds to the affective ties which the youth forms to significant others” (Wiatrowski, 1981, p. 525). These significant ties are often found in the family environment when parents act as role models by teaching their children what is and is not socially acceptable behavior. When Hirschi explains attachment in Causes of Delinquency he uses psychopathy to really help the reader understand how the term should be interpreted. Hirschi mentions in his book that John Martin and Kenneth Fitzpatrick say that a psychopath has characteristics such as, “deficient attachment to or affection for others, a failure to respond to the ordinary motivations founded in respect or regard for one’s fellows” and “excessive aggressiveness” (Martin, 1964). Most people would say that the characteristics of the psychopath come as a result of his lack of attachment, or that his lack of attachment comes as a result of his psychopathy. Hirschi says to lack attachment to others is to be free from moral restraints and means that you have no conscience or superego. If you subscribe to that train of thought then lack of attachment to others is the same thing as lack of conscience. “In this view, lack of attachment to others is not merely a symptom of psychopathy, it is psychopathy” (Hirschi, 1969).\nThe second element of the social bond as explained by Travis Hirschi is commitment. According to Wiatrowski, “Commitment is related to the aspiration of going to college and attaining a high-status job” (1981, p.525). The commitment to studying and working hard to reach these goals is considered an investment and the individual puts their investment at risk if they partake in delinquent behavior. According to this theory, youths with well-defined goals are much less likely to engage in delinquent behavior than those who are not looking towards the future. Hirschi believed that commitment is directly correlated to conformity. He said that men have been known to obey the rules for no other reason than because they were afraid of facing the consequences. He said that we label the rational component of conformity as commitment and that people who follow the rules out of fear of the consequences are committed to conformity, thus much less likely to behave delinquently (1969). Hirschi often emphasized education when he talked about commitment much like he emphasized parents when talking about attachment. He said that commitment has to do with time and effort invested in conventional plans of action. These actions could be educational aspirations, school performance, long-term career goals or maintaining ones reputation (1969). Commitment is also associated with the cost factor that is involved when engaging in delinquent activities. We can assume that somebody that is committed has invested a lot of time and effort into whatever it is that they are committed to. Both time and effort can be considered cost factors. If someone has spent years of their life working their way up to the head of a company then there probably is not much time left over to perform deviant acts. Maybe they did not put too much time into their investment but it did require a lot of effort. They probably won’t do anything that could jeopardize their investment because they do not want all of their work to be for nothing (Krohn; Massey, 1980). It is important to keep in mind when thinking about these elements of the social bond that Hirschi’s theory explains why we will NOT commit crimes, rather than why we will.\nInvolvement is the third of the four elements in the social bond. Involvement is plain and simple; if somebody spends a great deal of time doing conventional things then they will not have time to engage in delinquent behavior. Hirschi believed that somebody that is extensively involved in conventional activities is, “tied to appointments, deadlines, working hours, plans, and the like, so the opportunity to commit deviant acts rarely arises” (Hirschi, 1969). This is when we start to see the connection between the elements of the social bond. For example, if you are committed as a high school student to getting in to a prestigious college so you can eventually graduate and get a high-paying job there is no way to fulfill your commitment without extensive involvement.\nThe final element is belief. Belief is the acceptance of the moral validity of the central social-value system. The more rule-bound people feel, the less likely they are to commit a crime or violate those rules (Hirschi, 1969) Due to socialization Hirschi says that everyone, including deviants, recognized the validity of one dominant set of values. The difference between those deviants and the rest of us is that even though they recognize the same set of values they do not feel bound by them due to their lack of belief or weakened social bond (Wiatrowski, 1981). This element brings an interesting question to light. If we all recognize the same set of values, how can someone believe it is wrong to commit a crime while he is committing a crime? Hirschi claims that according to control theory there are two ways deviants get around the rules. One way is by giving no real meaning to their beliefs and considering them to be nothing more than words. The other way is through neutralization. This entails the deviant essentially justifying the act to themselves before it occurs so they can violate the rule and maintain their belief in it at the same time (Hirschi, 1969).\nJust like any other theory, social control theory has several criticisms, but the most widely known criticism actually came from Travis Hirschi Himself. In the year 1990 Hirschi teamed up with another criminologist named Michael Gottfredson and together they came up with General Crime Theory, also known as self-control theory. Instead of explaining deviance through the strength of an individual’s bond with society through the elements of attachment, commitment, involvement and belief, Hirshci’s new theory set out to explain all criminal behavior using just one type of control; self-control. The main idea of the theory is that individuals with high self-control are less likely to behave criminally, and those who possess low levels of self-control are more likely to lead life’s of crime and generally deviant behavior (Gottfredson and Hirschi, 1990). Hirschi often emphasized the importance of attachment to peers, this brought several opposing views. Many people argue that attachment could actually lead to delinquent behavior rather than prevent it. They claim that being attached to peers who are already engaging in criminal behavior could influence the newly attached individual to begin behaving in the same manner. Sociologist Rand Conger (1976) also states that delinquency will be at its highest when an individual’s attachment to a deviant environment is strong. As mentioned above the elements of commitment and involvement can be codependent, because of this there are criminologists that believe that the two should just be combined into one element. Marvin Krohn and James Massey (1980) did just that because they could not see a scenario in which somebody could be involved in something without at least a slight commitment to the activity and vice versa. Other more general criticisms of social bond theory include the idea that an individual’s social bonds will change over the course of a lifetime causing them to keep building them back up again. Also the idea that the scope of social bond theory is restricted when compared to the general crime theory since GCT attempts to explain all types of criminality.\nThanks to a large number of empirical studies performed since Causes of Delinquency (1969) testing Hirschi’s social bond theory there is a significant body of research on the topic. The results of these studies show strong support for Hirschi’s claim that the stronger an individual’s four elements of the social bond, the less likely to engage in delinquent behavior. I will review several of these studies over the next few paragraphs but in summary the research shows us that youths that are strongly attached to their parents are generally less likely to commit criminal acts. Youths involved in and/or committed to conventional activities are also much less likely to engage in delinquent behavior than those who are involved in unconventional behaviors. Kids who maintained weaker relationships with their peers moved towards delinquent behavior and those who were attached to their peers often shunned unconventional acts.\nA study done by Guang-zhen Wang at the University of Texas – Pan American set out to test Hirschi’s social bond theory by exploring the sex, racial/ethnic differences in tobacco, alcohol, and marijuana/drug use among adolescents. The variables in this study were the four elements of the social bond, attachment (to parents), commitment (to school), Involvement (with their classmates) and belief (content about life). According to Guang-zhen there is extensive literature that demonstrates that children are much less likely to partake in risky behavior when their parents are easy to talk to, and provide adequate supervision (2006). Despite the importance of attachment the results of Guang-zhen’s study showed that commitment, to school in particular, had the most significant impact on the students tobacco, alcohol and drug use. Those with a strong commitment engaged in these conventional activities far less often than those with a weaker commitment. The results were consistent across all six groups of adolescents analyzed in the study (white boys, white girls, black boys, black girls, Hispanic boys, Hispanic girls). This is consistent with research done by Carol Goodenow who argues that doing well in school can positively affect a student’s motivation to perform academically at a higher level. This new motivation could lead them to participate in more positive activities which in turn will keep them away from drugs and alcohol or delinquent behavior (1993). Goodenow’s work also demonstrates how commitment can lead to involvement. Another study by Lamborn and Steinberg (1993) found that children with high levels of emotional autonomy often have healthy, supportive relationships with their parents (strong attachment). Children with unsupportive relationships with their parents (weak attachment) will typically score lower on emotional autonomy. They also found that low levels of emotional autonomy are associated with youth behavioral problems such as underage drinking and smoking. Matsueda and Anderson (1998) did an analysis examining the relationship between delinquent behavior and delinquent peer association, essentially peer attachment when looked at through the eyes of social bond theory. They concluded that there is in fact a reciprocal relationship between the two but they found that the effect of delinquent behavior on peer associations is much greater than the effect of peer associations on delinquent behavior. In other words, the individual is much more likely to be delinquent and find a group of peers that are similar to him, than being turned into a delinquent by his peers. This essentially flies in the face of the critics who claimed that attachment could be a causal factor for delinquent behavior if someone was to become attached to a group who is already engaging in delinquent behavior.\nPolicy implications for social control theory are a little bit tricky. We cannot force someone to believe something so there really can’t be any explicit policy implications for belief. Commitment and involvement also come with a certain amount of free will so there is not much that can be done about those either. It is typically agreed upon that the most effective policy implications when it comes to control theories in general, for our purposes social control theory, are those that focus on parenting or the element of attachment. Vito, Maahs and Holmes (2005) claim that parents can have a very strong influence on their child’s level of conformity by the way they raise the child. If parents adequately supervise and communicate with their children during their upbringing they are more likely to form a strong attachment which is proven to lead to less delinquent behavior. There is evidence that the implementation of parent training programs have had some success in reducing delinquent behavior of the children whose parents participated in the programs. The fact is that many parents simply do not know how to be a good parent, not because they don’t care but they just were never taught. Vito also say that, “The popularity of reality television programs such as Supernanny and Nanny 911 suggest that many parents are indeed open to helpful parenting techniques”(Vito; Maahs; Holmes; 2005, p. 191). There are also similar programs in place that train teachers to use teaching techniques that are specifically designed to strengthen children’s bonds with the school. This would increase the child’s level of commitment as well as involvement without forcing either of them on the child. Of course there are also implicit policies that can be implemented to reduce the likelihood of criminal involvement. Some would recommend marriage or quality employment as a policy. If you have these things then you probably have a stable relationship and steady income, both of which relieve many of the stresses of life that can lead to criminal behavior.\nWhile there are plenty of criticisms surrounding this theory including general crime theory essentially adding elements on top of the original four, there is plenty of research out there supporting the original elements of the social bond and the social bond theory. In my opinion the more you simplify social learning theory, the more sense it makes. If you have 10 adolescents with no strong attachment to parents or peers, no solid commitments, no consistent activities they are involved in, and limited to no belief in a set of values; then how many of the 10 do you think would engage in delinquent behavior? They would be blank slates, naturally hedonistic just Like Hirschi claimed, with nothing or nobody to tell them not to be delinquent. My guess is that the vast majority of them would behave delinquently if not all 10, thus rendering Hirschi’s social control theory still relevant despite the challenges it has faced in the last four decades.\nConger, R. (1976). Social Control and Social Learning Models of Delinquency: A\nSynthesis. Criminology, 14; 17-40.\nGoodenow, C. (1993), The Psychological Sense of School Membership Among Adolescents: Scale Development and Educational Correlates. Psychol. Schs., 30: 79–90. doi: 10.1002/1520-6807(199301)30:1<79::AID-PITS2310300113>3.0.CO;2-X\nGottfredson, M. R., and Hirschi, T. (1990). A General Theory of Crime. Stanford, CA:\nStanford University Press\nGuang-zhen, W. (2006). Tobacco, Alcohol, and Marijuana/Drug Use among School Children: TestinHirschi's Social Bonding Theory. Conference Papers -- American Sociological Association, 1.\nHirschi, T. (1969). Causes of Delinquency. University of California Press, Berkeley, CA.\nKrohn, M. D., Massey, J. L. (1980). Social Control and Delinquent Behavior: An\nExamination of the Elements of the Social Bond. The Sociological Quarterly, 21;\nLamborn, S.D. and Steinberg, L. (1993). Emotional Autonomy Radix: Revisiting Ryan and Lynch. Child Development, 64; 483-499\nMatsueda, R. L. and Anderson, K. (1998), The Dynamics of Delinquent Peers and Delinquent Behavior. Criminology, 36: 269–308. doi: 10.1111/j.1745-9125.1998.tb01249.x\nVito, G. F., Maahs, J. R., Holmes, R. M. (2006). Criminology: Theory, Research, and Policy. Burlington, MA: Jones & Bartlett Learning.\nWiatrowski, M. D., Griswold, D. B., & Roberts, M. K. (1981). SOCIAL CONTROL THEORY AND DELINQUENCY. American Sociological Review, 46(5), 525-541.","Parent management training\nParent management training (PMT) is an adjunct to treatment that involves educating and coaching parents to change their child's problem behaviors using principles of learning theory and behavior modification .\nThe aim of PMT is to decrease or eliminate a child's disruptive or inappropriate behaviors at home or school and to replace problematic ways of acting with positive interactions with peers, parents and such authority figures as teachers. In order to accomplish this goal, PMT focuses on enhancing parenting skills. The PMT therapist coaches parents in applying such strategies as rewarding positive behavior, and responding to negative behavior by removing rewards or enforcing undesirable consequences (punishments). Although PMT focuses on specific targeted behaviors rather than on the child's diagnosis as such, it has come to be associated with the treatment of certain disorders. PMT is used in treating oppositional defiant disorder , conduct disorder , intermittent explosive disorder (age-inappropriate tantrums), and attention deficit disorder with hyperactivity ( attention-deficit/hyperactivity disorder ). Such antisocial behaviors as firesetting and truancy can also be addressed through PMT.\nIn PMT, the therapist conducts initial teaching sessions with the parent(s), giving a short summary of foundational concepts in behavior modification; demonstrating interventions for the parents; and coaching parents in carrying out the techniques of PMT. Early meetings with the therapist focus on training in the principles of behavior modification, response-contingent learning, and ways to apply the techniques. Parents are instructed to define the behavior(s) to be changed concretely and specifically. In addition, they learn how to observe and identify relevant behavior and situational factors, and how to chart or otherwise record the child's behavior. Defining, observing and recording behavior are essential to the success of this method, because when such behaviors as fighting or tantrums are highlighted in concrete, specific ways, techniques of reinforcement and punishment can be put to use. Progress or its absence is easier to identify when the description of the behavior is defined with enough clarity to be measurable, and when responses to the PMT interventions are tracked on a chart. After the child's parents grasp the basic interventions as well as when and how to apply them, the techniques that the parents practiced with the therapist can be carried out at home.\nLearning theory, which is the conceptual foundation of PMT, deals with the ways in which organisms learn to respond to their environment, and the factors that affect the frequency of a specific behavior. The core of learning theory is the notion that actions increase or decrease in frequency in response to the consequences that occur immediately after the action. Research in parent-child interactions in families with disruptive, difficult or defiant children shows that parental responses are unintentionally reinforcing the unwanted behavior. PMT trains parents to become more careful in their reactions to a child's behavior. The parents learn to be more discerning: to provide attention, praise and increased affection in reaction to the child's behaving in desired ways; and to withdraw attention, to suspend displays of affection, or to withdraw privileges in instances of less desirable behavior.\nThe most critical element of PMT is offering positive reinforcement for socially appropriate (or at least nondeviant) behaviors. An additional component involves responding to any undesired behaviors by removing rewards or applying punishment. These two types of response to the child must be carried out with great consistency. Consistent responding is important because erratic responses to unwanted behavior can actually cause the behavior to increase in frequency. For instance, if a child consistently throws tantrums in stores, hoping to be given something to end the tantrum, inconsistent parent responses can worsen the situation. If a parent is occasionally determined not to give in, but provides a candy bar or a toy to end the tantrum on other occasions, the child learns either to have more tantrums, or to have more dramatic tantrums. The rise in the number or intensity of tantrums occurs because the child is trying to increase the number of opportunities to obtain that infrequent parental reward for the behavior. Planning responses ahead of time to predefined target behaviors by rewarding desired actions and by withdrawing rewards or applying punishment for undesirable behavior is a fundamental principle of PMT. Consistent consequences, which are contingent on (in response to) the child's behavior, result in behavior change. Parents practice therapeutic ways of responding to their child's behavior in the PMT sessions with the therapist.\nThrough PMT, parents learn that positive rewards for appropriate behaviors can be offered in a variety of ways. Giving praise, providing extra attention, earning points toward obtaining a reward desired by the child, earning stickers or other small indicators of positive behavior, earning additional privileges, hugging (and other affectionate gestures) are all forms of reward. The technical term for the rewarding of desired behavior is positive reinforcement . Positive reinforcement refers to consequences that cause the desired target behavior to increase.\nPMT instructs parents to cancel rewards or give punishments when the child behaves in undesirable ways. The removal of rewards usually entails time away from the circumstances and situations in which the child can do desired activities or receive attention. The concept of a \"time out\" is based on this notion of removal of rewards. Time out from rewards customarily means that the child is removed from people and stimulation for a certain period of time; it can also include deprivation of privileges.\nPunishment in PMT is not necessarily what parents typically refer to as punishment; it most emphatically is not the use of physical punishment. A punishment in PMT involves a response to the child's negative behavior by exposing the child to something he or she regards as unpleasant. Examples of punishments might include having to redo the correct behavior so many times that it becomes annoying; verbal reproaches; or the military standby—\"drop and give me fifty\"—having to do pushups or situps or laps around a playing field to the point of discomfort.\nThe least challenging problems, which have the greatest likelihood of successful change, are tackled first, in hope of giving the family a \"success experience.\" The success experience is a positive reinforcement for the family, increasing the likelihood that they will continue using PMT in efforts to bring about change. In addition, lower-level behavioral problems provide opportunities for parents to become skilled in intervening and to learn consistency in their responses. After the parents have practiced using the skills learned in PMT on the less important problems, more severe issues can be tackled.\nIn addition to face-to-face sessions with the parents, some PMT therapists make frequent telephone calls to the parents between sessions. The purposes of the calls are to remind parents to continue to be consistent in applying the techniques; to answer questions about the work at home; and to praise the parents' attempts to correct the child's behavior. In addition, ongoing support in sessions and on the telephone helps parents feel less isolated and thus more likely to continue trying to use learning principles in managing their child. Troubleshooting any problems that arise regarding the application of the behavioral techniques is handled over the telephone and in the office sessions.\nAn additional aspect of learning theory is that rewarding subunits of the ultimately desired behavior can lead to developing more complex new actions. The subunits are finally linked together by changing the ways in which the rewards are given. This process is called \"chaining.\" Sometimes, if the child shows no elements of the desired response, then the desired behavior is demonstrated for the child and subsequent \"near hits\" or approximations are rewarded. To refine \"close but not quite\" into the targeted response, rewards are given in a slightly \"pickier\" manner. Rewarding successive approximations of the desired behavior is also called \"shaping.\"\nThe best way to learn to alter parental responses to child behaviors is with the support and assistance of a behavioral health professional ( psychologist , psychiatrist , clinical social worker). As noted earlier, parents often inadvertently reinforce the problem behaviors, and it is difficult for a parent to see objectively the ways in which he or she is unintentionally supporting the defiant or difficult behavior. Furthermore, inappropriate application of such behavioral techniques as those used in PMT can actually make the problem situation worse. Families should seek therapists with valid credentials, skills, training and experience in PMT.\nTypically, the parents should notice a decrease in the unwanted behaviors after they implement the techniques learned in PMT at home. Of the various therapies used to treat childhood disorders, PMT is among those most frequently researched. PMT has shown effectiveness in changing children's behavior in very well-designed and rigorous studies. PMT has a greater effect on behavior than many other treatments, including family therapy or play therapy . Furthermore, the results— improved child behavior and reduction or elimination of undesirable behavior— are sustained over the long term. When a group of children whose families had used PMT were examined one to fourteen years later, they had maintained higher rates of positive behavior and lower levels of problem behavior.\nSee also: Pyromania\nHendren, R. L. Disruptive behavior disorders in children and adolescents. Review of Psychiatry Series, vol. 18, no. 2. Washington, DC: American Psychiatric Press, 1999.\nWebster-Stratton, C., and M. Herbert. Troubled families—problem children: Working with parents, a collaborative process. Chichester, England: Wiley, 1995.\nFeldman, Julie and Alan E. Kazdin. \"Parent management training for oppositional and conduct problem children.\" The Clinical Psychologist 48, no. 4 (1995): 3-5.\nGolding, Kim. \"Parent management training as an intervention to promote adequate parenting.\" Clinical Child\nPsychology and Psychiatry 5, no. 3 (2000): 357-372.\nKazdin, A. E. \"Parent management training: Evidence, outcome and issues.\" Journal of American Academic Child and Adolescent Psychiatry 36, no. 10 (October, 1997): 1349-1356.\nAmerican Academy of Child and Adolescent Psychiatry. 3615 Wisconsin Ave., NW, Washington, D.C. 20016-3007. Telephone: (202) 966-7300. Web site: <www.aacap.org/> .\nAssociation for the Advancement of Behavior Therapy. 305 Seventh Avenue, 16th Floor, New York, NY 10001-60008. Telephone: (212) 647-1890. Web site: <www.aabt.org> .\nNorth American Family Institute. 10 Harbor Street, Danvers, MA 01923. Telephone: (978) 774-0774. Web site: <www.nafi.com> .\nThe Explosive Child <www.explosivechild.com> .\nParents & Teachers of Explosive Kids. <www.explosivekids.org> .\nDeborah Rosch Eifert, Ph.D"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:146b3104-23c6-4f26-b931-9fc519688fe5>","<urn:uuid:8dcd4739-db6b-4858-9292-7a3472519946>"],"error":null}
{"question":"What makes faces appear human, and how does this relate to social challenges in autism spectrum disorders?","answer":"Faces appear human primarily due to their eyes-over-nose-over-mouth configuration, which helps people distinguish between individuals despite faces being objectively similar. When this arrangement is disrupted, even by simply turning a face upside down, the face loses some of its perceived humanity. For individuals with autism spectrum disorders, face processing presents unique challenges. Many experience prosopagnosia (face blindness), making it difficult to recognize faces, which can significantly impact social communication. Research indicates that autistic individuals may use different strategies for facial recognition, often avoiding eye contact and instead focusing on other features like the mouth, which can lead to social interactions being strained from the beginning.","context":["What makes a face seem human? The answer, just like the question, is simultaneously facile and complex, according to Miami University psychology professor Kurt Hugenberg.\nThe simplest definition of what makes a face human, Hugenberg says, is an eyes-over-nose-over-mouth arrangement. This regular configuration helps us identify individual people more easily.\n“If all houses were as similar as human faces,” Hugenberg says, “we’d never be able to tell our house apart from someone else’s. But because we are finely tuned to very small differences within faces, we can distinguish between them. Two people appear quite different, even though, objectively, they’re quite similar to one another.”\nHugenberg’s research team – including five graduate students and eleven undergraduate students – works to understand how people perceive humanity in others’ faces by asking participants to rate faces on certain characteristics.\nThe researchers show participants photographs of various faces and ask questions like, “How empathetic is this person?” and “How able is this person to solve complex logical problems?”\n“These are characteristics only humans have, so the answers to those questions tell us a lot about the perceived humanity of a face,” Hugenberg says.\nWhat Hugenberg and his team have found is that if the eyes-over-nose-over-mouth arrangement is altered in even the most basic way – by turning a face upside down, for instance – participants answer those questions in ways that reflect reduced perceptions of humanity.\n“Remarkably,” Hugenberg says, “when you turn anything else upside down, it doesn’t lose its essential characteristics. If you turn a chimpanzee face upside down, it still seems equivalently chimp-like to people, but when you turn that human face upside down it loses some of its experienced humanity.”\nBut humanity – like beauty – is more than skin deep, and so, supported by an award of more than $500,000 from the National Science Foundation (NSF), Hugenberg and his colleagues, Indiana University’s Robert Rydell and Baruch College’s Steven Young, are now setting out to discover what goes on in the brains of people who view faces that violate the eyes-over-nose-over-mouth configuration.\nThey will use electroencephalography, or EEG, a technique in which electrodes are placed on the scalp to measure the electrical activity of neurons in the brain. “EEG measures temporal activation,” Hugenberg says, “so we’ll be able to see roughly where and exactly when different processes occur in the brain.”\nLike with Hugenberg’s previous research this new work will involve a number of graduate and undergraduate students every step of the way, from planning and preparing experiments to collecting and then analyzing data, to preparing manuscripts. “This is work that never would, or even could, happen without them,” Hugenberg says.\nHugenberg gives special credit to his graduate students, Steven Almaraz, Jason Deska, Paige Lloyd, Kurt Schuepfer, and Taylor Tuscherer, who he says make the participation of his more junior students, Chris Culp, Saara Khalid, Zoebedeh Malakpa, Neema Mohammadi, Kelli Peterman, Bill Schauer, Monica Scicolone, Michaela Schukies, Evans Smalley, Kellen Smith, and Jason Weiss, possible. “Without the graduate students’ direct mentorship, we just couldn’t offer the undergraduates this type of hands-on involvement.”\nUltimately, Hugenberg hopes that his team’s work may help us better understand the processes by which certain individuals or groups of people become dehumanized by others.\n“Instead of asking what makes a face seem human,” Hugenberg says, “we could ask what makes a person seem no longer like a person. When that happens, social cognition is essentially turned off, and it stops us from being motivated to think of people as people.”\nInsofar as that mechanism for dehumanization helps explain slavery, genocide, and other atrocities, Hugenberg’s research may help lead us closer to a world in which all people are safe and valued.\nWritten by Heather Beattey Johnston, Associate Director & Information Coordinator, Office for the Advancement of Research & Scholarship, Miami University.","Parents with children on the spectrum often speak about their kids’ difficulty recognizing faces, even those of friends and relatives. Face blindness, or prosopagnosia, means a person cannot recognize that they’ve seen a face before; this condition, found in many on the spectrum, may have serious psychosocial consequences.\nWhat do you see when you look at someone’s face? Where do you look first? Where do your eyes linger? Even though we look at the faces of others many times in a single day, it may be hard to answer such questions.\nMost people look at, and recognize, faces effortlessly. Facial recognition seems trivial, until the possibility of not recognizing loved ones is contemplated. Facial recognition is an intricate, integral part of social communication; for autistic individuals already facing difficulty in social communication this may be especially relevant and taxing.\nFace blindness or prosopagnosia\nThis condition is not about memory. It’s not about forgetting someone and encountering an awkward social encounter. It’s a serious mental condition where the ability to recognize faces is impaired or never adequately developed. There are two types of prosopagnosia:\n- Developmental prosopagnosia: (sometimes referred to as congenital prosopagnosia) is a lifelong, neurodevelopmental condition which is not associated with brain injury or intellectual deficits. It runs in families (Duchaine et al., 2007), manifests in childhood, and often occurs alongside conditions like autism\n- Acquired prosopagnosia: this condition occurs when poor face recognition is the result of brain damage, most commonly after a stroke, brain disease or injury, illness or tumor\nAcquired prosopagnosia became a subject of interest when a report (Bodamer, 1947) described deficits in face recognition of wounded soldiers. Face blindness is often linked to the brain’s right fusiform face area (FFA), but a study (Cohen et al., 2019) suggests no single area is always affected in face blindness. Rather, prosopagnosia may involve an entire network, where impairment in communication between components or different brain regions may contribute to the deficit in face recognition.\nFindings from the study, published in the journal Brain, led the first author Cohen (2019) to suggest that face recognition involves two distinct brain networks. The author is not sure whether face blindness is caused by a disruption in both networks, or whether the condition occurs because of an imbalance between the two networks.\nFor those who realize their skill of recognizing faces is compromised after brain damage, the loss of this skill must be deeply troubling. But would a child with developed face blindness or congenital prosopagnosia be aware of their deficit and its severity in recognizing faces? As it runs in families, and the child may never experience typical face recognition, would they be aware of any difference or impairment? Probably, at least according to a study (Dalrymple et al., 2014) aptly titled: “A room full of strangers every day”: The psychosocial impact of developmental prosopagnosia on children and their families.\nClick here to find out more\nDalrymple et al. (2014) concluded that children are mostly aware of their difficulties with face recognition, they use many different coping strategies as compensation for the condition, and face blindness may have significant social implications for some children. Do these conclusions apply to children with face blindness who are also on the autism spectrum? More research may be needed to establish how autistic kids with atypical facial recognition feel about the condition.\nThe face blindness, autism link\nThe results from a recent study (Minio-Paluello et al., 2020) revealed prosopagnosia potentially occurs in more than 36% of autistic adults without intellectual disability, compared to around 2% in the general population. Another study (Lynn et al., 2018) found connectivity between the fusiform face area (FFA) of the brain and the left frontal cortex was reduced in those with autism, relative to the typically developing individuals.\nThe study concluded by stating underconnectivity and abnormal development of functional connectivity may bring about an inferior face-processing network, in the context of escalating cognitive impairments in autism spectrum disorders (ASD).\nResearch suggests neural networks involved in the visual processing of faces may be one of the most crucial neural substrates affected in ASD (Pereira et al., 2019). This study also mentions real-time functional magnetic resonance imaging as a potential tool to study the visual processing network in ASD. Persistent differences in facial processing neural networks were found between typically developing individuals and those on the spectrum (Pereira et al, 2019).\nInformation based on brain MRI data, particularly functional MRI based neurofeedback, could be used to study visual processing networks in ASD, giving clues to exactly how those on the spectrum recognize faces. Parents with kids on the spectrum may wonder if there is anything they can do to help accommodate atypical face processing, if the condition is present in their child.\nAutistic kids, eye contact and poor face processing abilities\nChildren on the spectrum often find eye contact uncomfortable. Research (Neumann, 2006) discussing mouth fixation offered a possible explanation, which includes abnormal, top-down strategy for allocating visual attention by autistic individuals. The “eye avoidance” hypothesis of autism face processing (Tanaka & Sung, 2016) may be a plausible explanation of atypical face recognition where those on the spectrum avoid looking at the eye region of the face because it is perceived as socially threatening.\nParents with kids on the spectrum often speak about the unfairness of their children being perceived as “rude” in social contexts, especially when there is little understanding of autism and communication deficits. Raising awareness of face blindness could be helpful in educating society; a child on the spectrum may be overwhelmed and insecure in social contexts where it takes considerable effort to recognize a face, a task their peers execute effortlessly.\nFinding eye contact challenging, the child may look at other facial features (often the mouth), or they may look at clothing or other contextual clues to work out who the person is. Kids with face blindness may feel especially intimidated when encountering someone they are familiar with in a new or different environment. My own children seem surprised when they see their teachers in a supermarket; familiar contexts or environments do facilitate face recognition even for neurotypical children.\nWhen face recognition is impaired, social interactions may be strained from the get-go. If autism is associated with face blindness, it is appropriate for researchers to examine whether poor face recognition contributes to social communication deficits, a core autism characteristic. For parents, an awkward social interaction where memory fails and you just can’t place the person, should shed some light on the experience. Could anyone interact in a socially appropriate way with raging doubts about who they’re actually communicating with?\nAddressing face processing\nCleary autistic children with face processing deficits would benefit from early intervention—if face recognition is improved it could facilitate improved social communication. We need more research to identify the underlying mechanisms of face recognition and how this is impaired specifically in those on the spectrum.\nUntil recently clinicians did not seem overly optimistic about a cure for face blindness, probably because the condition seemed almost resistant to treatment. Reviewing successes and failures over the last 50 years of research, a study (Degutis et al., 2014) found compensatory training showed some effectiveness in children with developed face blindness. In adults with congenital prosopagnosia, remedial-training and administering oxytocin demonstrated improvements. Other approaches like transcranial magnetic stimulation are also under investigation as possible intervention for face blindness in those on the spectrum.\nA recent study (Bate et al., 2020) may change the pessimism about treatment for face blindness. The study describes the first remedial face training program appropriate for children. The program, using a modified version of the popular game Guess Who?, led to improvements in face memory—gains which were maintained when evaluated at a one month follow up session.\nAccommodate, compensate, educate\nWith more research and better intervention approaches, atypical facial recognition could be addressed in children on the spectrum. For parents, advocating for their kids becomes even more important when autism is comorbid with a condition like face blindness. Many of the studies above mention the importance of informing relevant people (in the child’s life) about the condition. Teachers, caregivers, and peers need to be educated and informed to help autistic children with face blindness.\nA teacher introducing themselves in a supermarket when they’ve been in the child’s life for a while may seem strange, but it could set the stage for a much easier and more comfortable social interaction. Doing things differently to accommodate different minds should not be regarded as an inconvenience. Rather, changing the way we do things, to accommodate neurodivergent individuals could embolden them to add a fresh perspective, new ideas, and boundless ways of thinking. A win-win for a world increasingly stumped by novel problems.\nBate, S., Adams, A., & Bennetts, R. J. (2020). Guess who? Facial identity discrimination training improves face memory in typically developing children. Journal of Experimental Psychology: General, 149(5), 901–913. https://doi.org/10.1037/xge0000689\nBODAMER J. (1947). Die Prosop-Agnosie; die Agnosie des Physiognomieerkennens [Prosop’s agnosia; the agnosia of cognition]. Archiv fur Psychiatrie und Nervenkrankheiten, vereinigt mit Zeitschrift fur die gesamte Neurologie und Psychiatrie, 118(1-2), 6–53. https://doi.org/10.1007/BF00352849.\nCohen, A. L., Soussand, L., Corrow, S. L., Martinaud, O., Barton, J., & Fox, M. D. (2019). Looking beyond the face area: lesion network mapping of prosopagnosia. Brain : a journal of neurology, 142(12), 3975–3990. https://doi.org/10.1093/brain/awz332.\nDalrymple, K. A., Fletcher, K., Corrow, S., das Nair, R., Barton, J. J., Yonas, A., & Duchaine, B. (2014). “A room full of strangers every day”: the psychosocial impact of developmental prosopagnosia on children and their families. Journal of psychosomatic research, 77(2), 144–150. https://doi.org/10.1016/j.jpsychores.2014.06.001.\nDeGutis, J. M., Chiu, C., Grosso, M. E., & Cohan, S. (2014). Face processing improvements in prosopagnosia: successes and failures over the last 50 years. Frontiers in human neuroscience, 8, 561. https://doi.org/10.3389/fnhum.2014.00561\nDuchaine, B., Germine, L., & Nakayama, K. (2007). Family resemblance: ten family members with prosopagnosia and within-class object agnosia. Cognitive neuropsychology, 24(4), 419–430. https://doi.org/10.1080/02643290701380491.\nLynn, A. C., Padmanabhan, A., Simmonds, D., Foran, W., Hallquist, M. N., Luna, B., & O’Hearn, K. (2018). Functional connectivity differences in autism during face and car recognition: underconnectivity and atypical age-related changes. Developmental science, 21(1), 10.1111/desc.12508. https://doi.org/10.1111/desc.12508.\nMinio-Paluello, I., Porciello, G., Pascual-Leone, A., & Baron-Cohen, S. (2020). Face individual identity recognition: a potential endophenotype in autism. Molecular autism, 11(1), 81. https://doi.org/10.1186/s13229-020-00371-0.\nNeumann, D., Spezio, M. L., Piven, J., & Adolphs, R. (2006). Looking you in the mouth: abnormal gaze in autism resulting from impaired top-down modulation of visual attention. Social cognitive and affective neuroscience, 1(3), 194–202. https://doi.org/10.1093/scan/nsl030.\nPereira, J. A., Sepulveda, P., Rana, M., Montalba, C., Tejos, C., Torres, R., Sitaram, R., & Ruiz, S. (2019). Self-Regulation of the Fusiform Face Area in Autism Spectrum: A Feasibility Study With Real-Time fMRI Neurofeedback. Frontiers in human neuroscience, 13, 446. https://doi.org/10.3389/fnhum.2019.00446.\nTanaka, J. W., & Sung, A. (2016). The “Eye Avoidance” Hypothesis of Autism Face Processing. Journal of autism and developmental disorders, 46(5), 1538–1552. https://doi.org/10.1007/s10803-013-1976-7."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:cf64778f-4aad-4f4c-8e74-7672d0fb2a49>","<urn:uuid:aaf0b8d8-ad3e-4fa4-ba66-26ad8cb826b5>"],"error":null}
{"question":"How do the bass frequencies produced by a concert grand piano compare to those of a pipe organ with a 32' stop?","answer":"A concert grand piano's lowest note is A0 at 27.5 Hz, and this fundamental frequency is quite audible on a 9-foot concert grand. In comparison, a pipe organ with a 32' stop can produce even lower frequencies, going down to 32Hz on its lowest C. While the piano's lowest note falls below the typical 40Hz threshold of human hearing, its fundamental can still be heard. The pipe organ's 32' stop, however, produces notes in its bottom octave (32Hz-64Hz) that are more felt than heard, particularly the bottom four notes which fall below 40Hz.","context":["What frequencies are Bass?\nby Charlie Santmire\nMost people don’t equate music with frequency (frequency is cycles per second, now called Hertz “Hz”). We do. It’s essential that we equate musical instrument ranges to frequency to get the best result from speakers and woofers. So here is a look at the issues and my definitions of ranges.\nLOWER MIDRANGE (80-160 Hz octave/E2 to E3)\nMany instruments and voices (baritone and bass) have notes in this range. E2 is the lowest note on a guitar and on the Tympani (tuned drum).\nBASS AND UPPER BASS (40-80 Hz/Eto E2)\nActually, 41.02 Hz for E1 to 82.4 Hz for E2. For normally tuned bass guitar and upright double bass, note fundamentals extend down to E1, (open E). I would consider bass arbitrarily to be below C2 (65.41 Hz). That is two octaves below middle C. The fundamental of a note is its musical pitch. It is also called the first harmonic. The second harmonic is thus twice the pitch frequency and so forth. Harmonics are generally the fundamental times 2, 3, etc.\nDEEP BASS (Below 40 Hz)\nSome acoustic instruments and electronic instruments have fundamentals in this range. This range is very difficult to reproduce at a level you can hear with acceptably low distortion. An upright bass with extension goes down to C1 (32.7Hz). The lowest note on a piano is A0 (27.5 Hz). The fundamental of this note A0 is quite audible on a 9′ concert grand – not audible on a spinet (small upright piano).\nDouble bass with extension and a concert grand piano can produce reasonable levels of their lowest fundamental frequency. Contrabassoon can play C1 but the fundamental is essentially absent. You might ask if you can’t here the fundamental why bother to be able to play the note. The answer is that we place the note in our mind by the interval between the harmonics. So, we know a piano is playing A0 (27.5 Hz) because we hear the harmonic structure 55Hz, 82.5Hz etc. even if we cannot hear much of the fundamental.\nVERY DEEP BASS (C0 to C1/16.35Hz to 32.7Hz)\nOnly electronic instruments and pipe organ sound in this range. Big pipe organs will have more than one rank (set of 24 pipes) covering this range. C0 requires a 32′ pipe. There are a couple of organs with 64′ pipes. They can sound at 8Hz.\nAll of these frequencies assume normal current tuning where A4 is 440Hz.\nBass impact is a totally different issue. You need to move a lot of air very quickly to get impact. This is difficult and expensive to do. It is an important aspect of music with kick drum and concert bass drum. The speaker must react forcefully and quickly. The room also enters into the equation. We are investigating this area of reproduction. Stay tuned. More to come on this.\nModerate levels in the midrange where most music is centered are in the range of 80dB SPL (Sound Pressure Level) or so. Zero dB SPL is a defined arbitrary level. The lowest levels we can typically hear are in the 10-20dB SPL range. Equal loudness to that 80 dB SPL level in the midrange at very low frequencies (30Hz or so) is generally considered to be about 110 dB SPL. This level is very difficult and very expensive to produce in an audio system.\nMost larger floorstanding speaker systems should try for 40Hz with low distortion and fairly high levels. To effectively extend response another octave to 20Hz at appropriately high levels requires subwoofers. Our experience is that it is not possible to position the main speakers for maximum output in this octave, even if they can reproduce it, and at the same time position them for best overall musical performance above 40Hz. They would need to be quite close to the wall behind them to benefit from that reinforcement. We all know that placement sounds very bad as the whole musical tonal structure is influenced negatively by reflection from that wall.\nStereo subwoofers are a must, not single subwoofers. Stereo subwoofers extend the bass and “open up” the midrange, giving a sense of space, the space in which the music was performed. If an acoustic instrument recording is made in a concert hall you will then hear the hall, the space in which the music was performed. A single subwoofer will get you some sense of this space and may give you more bass than no subwoofer but that depends on the individual recording. To the extent that bass has different phase in the recording between left and right channels, the bass may partially cancel with a single subwoofer.\nTHE LISTENING ROOM IS ALSO INVOLVED IN BASS REPRODUCTION\nThe room modes (emphasizing or deemphasizing certain frequencies at different places in the room) depends on the size and stiffness of the room. Your sitting position will be at a place where the loudness of some frequencies (notes) will be greater or less than at others. You can do a lot of math and calculate these boosts and cuts. You can also make measurements.\nIn my view, the best you can do is listen to examples of the music you typically listen to and move your listening seat around a little bit, if you can, to see if you like the bass better in one position or another. Stereo woofers tend to smooth out the bass to some extent and I also find that sitting a bit off-center may also help. Often a couple will have two chairs together each being a bit off-center of the width of the room. I find this to be fine in all aspects of listening if you are a bit further from the plane of the speakers than the speakers are apart on centers. We can help you with all this fine detail.\nSetting up stereo woofers is as much an art as a science. You start with the science but what makes things really work is the art of setup developed through the experience of doing many setups.","Every individual controllable register, or rank of pipes, in a pipe organ sounds either a 1st harmonic (fundamental, or prime tone) or an upper partial tone in the harmonic series. The unison pitch of the manuals is described as 8-foot (8') because the lowest C key tuned to concert pitch (which matches the 2nd to lowest C key of the piano, or C2) is sounded by an open pipe 8 feet long. Using the 8-foot open pipe as a reference point for the 1st harmonic, the tone produced by an open pipe 1/2 that length produces the 2nd harmonic, or 1st upper partial tone of that fundamental, vibrates at double frequency, and sounds the octave (Octave) at 4' pitch. The tone produced by an open pipe 1/3 that length produces the 3rd harmonic, or 2nd upper partial tone, vibrates at 3 times the frequency, and sounds the Twelfth (Nazard) at 2-2/3' pitch. The tone produced by an open pipe 1/4 that length produces the 4th harmonic, or 3rd upper partial tone, vibrates at 4 times the frequency, and sounds the Fifteenth (Super Octave) at 2' pitch, and so on.\nThis means that the tones produced by open pipes 1/5, 1/6, 1/7, and 1/8 that long produce the 5th, 6th, 7th, and 8th harmonics, respectively. These represent the 4th, 5th, 6th, and 7th upper partial tones, respectively, vibrate at 5, 6, 7, and 8 times the frequency, respectively, and sound the Seventeenth (Tierce), Nineteenth (Larigot), Flat Twenty-First (Septieme), and Twenty-Second at 1-3/5', 1-1/3', 1-1/7', and 1' pitches, respectively. The 4 individual stops having fractional numbers which sound the 2nd, 4th, 5th, and 6th upper partial tones, respectively, are sometimes referred to as \"mutation stops.\" The series continues upward from there as incomplete ranks which are part of the compound harmonic-corroborating (mixture) stops of the organ.\nSimilarly, the unison pitch of the pedals is described as 16', an octave below that, because the lowest C pedal key (which matches the lowest C1 key of the piano keyboard) is sounded by an open pipe 16 feet long. Since the accepted lower limit of determinate musical sounds the human ear can detect is around 40Hz, and the lowest 16' octave of the organ sounds frequencies in a range of 32Hz-64Hz, this means that all frequencies generated by a musical instrument below low E in the 16' octave (which corresponds to the E1 of the double bass of the orchestra) are felt rather than heard -- that would include the bottom 4 notes (C, C#, D, D#) with a 16' stop drawn.\nA sub-octave pedal stop, or \"Double,\" which sounds a whole octave lower, generated by an open pipe 32' long generating a prime tone on low C1, is sometimes supplied in larger instruments to provide additional gravity. Such grave stops may be of principal, flute, reed, or non-imitative string tone, and an important organ can, and often does, have more than one. Sometimes also, we find instruments in which digital stops of 32' pitch supplement the sounds of real pipes in the Pedal division [See blog, \"Hybrid\" Pipe Organ, Parts I-II].\nIn cases where space or cost considerations will not permit the introduction of the largest pipes of the 32' octave, an independent stop of 16' pitch may be joined with another independent 16' stop wired to play a perfect 5th above it, at 10-2/3' pitch (sometimes the 10-2/3' is introduced as a separate stop called \"Quint,\" which makes possible its combination with any pedal stop of 16' pitch). This combination, which represents the 2nd and 3rd harmonics (1st and 2nd upper partials) of a 32' prime, generates a differential tone represented by the difference between the frequencies of both pipes and produces a \"synthetic 32-foot\" sound at that frequency. This differential or resultant tone is weaker than the sound produced by an independent pipe of 32' but its introduction can be a satisfactory alternative. Such a stop is properly formed of 2 separate ranks of pipes closely situated to each other in the same chamber and typically is controlled by a drawknob labeled \"Acoustic Bass\" or \"Resultant.\"\nFrom what has been said, the entire bottom octave of a 32' stop is outside the lower limit of the human ear to detect musical vibrations, thus there appears to be no musical reason to provide an organ with stops any lower in pitch than 32'. People nevertheless are fascinated by extremes. Monster pedal stops of 64' pitch have been inserted in the very largest pipe and electronic organs which produce real or differential tones having frequencies down to 8Hz on low C. These have been constructed either of single, individual pipes or 2 pipes wired to each other which sound a perfect 5th apart (at 32' and 21-1/3' pitches respectively) creating a resultant 64' -- so-named \"Vox Gravissima,\" or simply \"Gravissima.\" Since the bottom octave sounds frequencies in the range of 8Hz-16Hz and this must be spread over 12 chromatic keys, this means that only an 8 Hz difference separates all of those 12 notes. In that range of frequencies it's impossible for the ear to determine any difference between one note and another, as all notes are rattling at about the same rate. All that may be discerned, note to note, is an unmusical, rumbling sound -- something akin to a washing machine going through its spin cycle.\nThe longest full length organ pipe on record (the low pedal C1 of the 64' Contra Trombone stop installed in the Thomas Hill organ in the Town Hall at Sydney, New South Wales) has a wooden resonator as long as a 5-story building, weighs 1-1/2 tons, and has a metal reed made to vibrate against a gigantic shallot at a frequency of 8Hz which, to our ears, resembles the alternating banging of 2 wooden drumsticks on a metal washtub. One can only surmise what advantage, on scientific and musical grounds, the introduction of such a costly and space-consuming voice in an organ would provide. It simply cannot be, as some suggest, that the reason this builder inserted this stop in this organ was to create the effect, when drawn, of a rumbling bass drum when the organ is brought up to a fortissimo dynamic and beyond; if such an effect were truly desired, then a real bass drum could have been electrically connected to the Pedal division and controlled by means of its own two drawknobs (single stroke and reiterate) which would take up far less space at considerably less cost.\nIf the reason was more to prove to the world that an organ building firm could construct such a grave, costly, and independent voice and insert it in an organ, then the same line of thinking could be carried further: With today's technology, superior materials, skilled mitering, and increased wind pressures, the logical next step would be to construct an independent stop of 128' pitch generating frequencies in its bottom octave down to 4Hz. For that matter, once we have that in our organ, the next logical step would be to extend that rank downward by 12 more pipes, set those 12 pipes on an offset windchest, raise the wind pressure to 200 inches, and unify that rank down to 256' pitch with a bottom C pipe 85 yards long weighing 20 tons sounding at only 2 Hz, something no one else's pipe organ in history has ever had, or could ever do. After that, the next logical step is to wire that same rank to play at 256' and sub-quint 170-2/3' pitches, each note sounding 2 pipes a perfect 5th apart, which would generate a resultant differential tone of 512' pitch. A Vox Magna Sub-Gravissima Absurdus Profunda 512' stop like that, with bottom C producing a frequency of only 1 Hz, would have to be the admiration of every organ builder and musical connoisseur in Western civilization and the envy of every concert hall in the world.\nNot. Somewhere in this insane, unsound, deranged, utterly foolish craze to build the biggest and badest, latest and greatest, tallest and widest, heaviest, lowest-pitched, and loudest-sounding machine on the face of the earth, one has to call a halt on scientific, physiological, musical, and common sense grounds. When the maximum frequency range of human hearing is 20Hz-20,000Hz, and the lower limit of perceptible musical tone is agreed to end around 40Hz represented by the low E1 of the double bass of the orchestra or the low E1 key of the pedals of an organ with a 16' stop drawn (4 more chromatic notes, down to low C1, are provided below that by the 16' stop), one is left to wonder what can be said of those frequencies which descend into the sound tombs of the 64' octave. Such powerful vibrations are not perceptible as musical tones and, while they may succeed in shaking us bodily, they fail to impress our musical senses. Moving chromatically down the frequency spectrum from 16 Hz at 32' C to 8Hz at 64' C, we find a difference of only 8Hz over 12 notes. The ear interprets these 12 notes only as different versions of the same unmusical noise.\nBelieve it or not, experiments conducted on the organ at Hammerwood Park (a country house situated in Hammerwood near east Grinstead, east Sussex, England) have in fact involved frequencies descending into the 128' octave. Using a super subwoofer, a mega-monster electronic 128' stop actually has been created which makes a scale from 4Hz up, producing an enormous racket that probably only a whale can hear as a tone. Were that not enough, a \"Self-Destruct\" button which draws the 128' and 64' frequencies along with the 32' is also being contemplated for introduction on this console. When added to the full organ the effects of this combination of 3 pitches are curiously interesting from an academic point of view but nothing more than the ultimate in noise machines. A sense of humor is considered necessary equipment for conducting these kinds of mad sonic experiments, along with the premises being very substantially built to withstand, if necessary, the tumult of an earthquake.\nBesides stops of 64' pitch, another class of stops might be considered under the heading \"noisemakers,\" or \"noise machines.\" These are stops of 4-4/7' pitch in the pedal and 2-2/7' and 1-1/7' pitches in the manual divisions which corroborate the 6th upper partial tones of the 32', 16', and 8' harmonic series, respectively, and are represented in the organ by the so-called Septieme (pronounced \"set-yem'\"). This rank, sometimes labeled \"Sharp Twentieth\" or \"Flat Twenty-First,\" is formed of open metal, cylindrical pipes voiced to yield a soft principal tone. When sounded on the note C1 (bottom C in the manuals) the 6th upper partial tone lies between middle A#3 and Bb3 of the physical scale and is therefore slightly out-of-tune with the middle A#/Bb key of the chromatic keyboard tuned in equal temperament. When this stop has been correctly and scientifically voiced, regulated, and tuned it remains out of sync with the chromatic keyboard, and, being tonally unruly this way, when it is introduced as a complete stop in any organ, it should be the most subdued in tone of any mutation stop and never unduly assertive to keep it from becoming a problem child. As an element of a complete harmonic structure it is a desirable voice for the foundation work in an important organ, preferrably as part of a 4-rank compound (mixture) stop in which it can sound favorably along with the 4th, 5th, and 7th upper partials belonging to the same harmonic series (i.e. the 17th, 19th, and 22nd) and be correctly adjusted tonally. When such a stop is introduced by itself it is free to combine with any other grouping of stops, lending at times an interesting color when surrounded by other stops closely related in pitch. The potential is also there for it to become a noisemaker unless caution is exercised with its use.\nIf it should ever be discovered that certain notes in a Pedal stop of 32' pitch are dead or the pipes producing such notes are out-of-tune, poorly regulated, or late to speak, and the music at hand calls for drawing that stop but does not rely particularly heavily upon these same notes, this author is of the opinion that this stop should still be drawn so that the listener does not miss out on hearing some of its sound. To think of such a voice under these conditions as simply another class of \"noisemaker\" and keep it retired causes the listener to miss the opportunity of experiencing its sound at all. Opinions among organists will vary about this, and it can be argued either way, but this author is inclined to put up with a couple of misbehaving notes in such a stop, take what good can be taken, and just go from there. When the music is showing an appetite for such a voice, half a loaf of bread seems to be better than none. It's also true that the squeaky hinge gets the oil; any misbehavior of a few notes is inclined to raise awareness with the right people who are in a position to take action to correct it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b4600a4c-e7e5-4c0f-a929-7cf5ad22fb2c>","<urn:uuid:e4602cfd-df12-4276-914d-bc9a3468f3c5>"],"error":null}
{"question":"How do supervision experiences differ between PhD candidates experiencing mental health issues and general depression patients seeking professional help?","answer":"PhD candidates receive limited supervision with 63% seeing their supervisor for less than one hour per week, while depression patients have access to more structured professional support through various channels including CBT, counseling available 7 days a week from 8am to 8pm, and regular treatment from doctors who use standardized assessment tools like PHQ9 and GADD7 to monitor their condition.","context":["Doing a PhD is frequently tough. But why? And to what extent should universities feel responsible for doing something about it?\nThe latest report for Hepi, PhD life: the UK student experience, authored by PhD candidate and Hepi intern Bethan Cornell, brings together UK data from Nature’s most recent annual global survey of PhD candidates with data from Wellcome’s recent survey on research culture to examine some aspects of the experience of 1,069 UK PhD candidates.\nThe headline results are concerning, if not necessarily surprising. The average PhD candidate works 47 hours per week – which at a standard Research Council stipend works out at less than minimum wage. Over a third (37 per cent) have sought help for anxiety and depression attributed to PhD study.\n63 per cent see their supervisor for less than one hour per week – which seems fantastically frequent to this English PhD graduate, but given that 78 per cent are satisfied with their degree of independence I suspect the threshold for candidates experiencing a problematic level of supervisorial inattention is rather lower than one hour a week.\nAnd while 68 per cent report they feel safe at work, a quarter have been bullied and 47 per cent have witnessed bullying. One fifth have faced discrimination and only 26 per cent believe complaints regarding bullying would be acted upon.\nA gloomy picture indeed. But one that is distinct from that painted by the annual postgraduate research experience survey (PRES), administered by Advance HE. PRES explores different aspects of the researcher experience – and, for what it’s worth, includes those undertaking research Masters as well as PhD candidates.\nPRES 2019 found that 81 per cent of PGRs definitely or mostly agree that they are satisfied with their experience. Supervision and support for development of research skills consistently score particularly well.\nResearch culture is the area where PRES respondents are least satisfied, at an average of 63 per cent – the question structure was changed for 2019, but the focus is on the quality of the research community – opportunities to discuss your research, a departmental seminar programme, and the important, but less tangible sense that the research environment and community “stimulates your work” which I’d interpret as being a source of positive motivation, rather than a drain on your energies.\nNone of this directly captures the aspects of research culture that the Nature/Wellcome data picks up: the long working hours, the competitiveness, and the apparent tolerance in some areas for abusive behaviours. Though PRES 2019 did find gaps in satisfaction between white and BAME ethnic groups, and notes that PGR wellbeing, especially in the area of anxiety scores significantly lower than in the general UK population, albeit on average higher than for undergraduate students.\nAll this points to the need to think carefully about how “experience” is defined and measured – and being prepared to adapt thinking when new evidence comes to light.\nAspects of experience\nWithin any PhD there’s a bunch of potential factors in play. There’s the tangibles of the research culture in terms of opportunities for learning and development: whether you’re able to access training that’s suitable for your research area, how frequently you should expect to see your supervisors and how frequently they’re supposed to appraise you and your work, and the resources that are available to support your research. These are the sorts of things that used to be set out in Chapter B12 of the Quality Code, which now just says, “Where the provider offers research degrees, it delivers these in appropriate and supportive research environments.”\nThere’s also the tangible aspects of the research culture in the form of terms and conditions – for example, whether it’s technically possible to take sick leave, whether you have a stipend and how comfortable it makes you, and your employment conditions if you are asked to teach, all fall into this category. These issues are not always considered as part of the academic quality picture, and can leave PhD candidates disappearing into the gap between student and employee rights.\nThen there are the intangible aspects of research cultures – things that aren’t written down anywhere but that you quickly learn are generally considered essential to being successful. In this basket probably goes long hours working cultures, isolation, what actually happens when you meet your supervisors, the punishments that might be meted out if you somehow aren’t perceived as fitting in – and the personal penalties and compromises that might be the cost of fitting in.\nThere’s also a question about your personal motivations and longer term aspirations and the likelihood that you’ll be able to achieve your goals. What’s frequently heartbreaking about achieving a PhD is that the level of competition for academic jobs is such that it’s not nearly enough to “only” have a PhD – and if academia is what you’ve set your heart on, the pressure, anxiety and disappointment as you try and fail to gain a spot in your field can be very tough indeed. Again, there can also be penalties that come with success – enforced mobility, delays to starting a family, and job insecurity, as well as those toxic cultures that some report.\nThough universities have done a lot to develop pathways beyond academic careers (and manage candidates’ expectations), it’s a bit of an issue that in the Nature/Wellcome data only 54 per cent agreed that their supervisor is open to them pursuing a career outside academia, and only 25 per cent felt their supervisor had useful things to say about careers outside academia.\nAnd once you’ve wrapped up all these elements, you then have to filter them through people’s personal circumstances, disposition, social capital, professional competencies, and general resilience. In my subject, taking on a PhD required a degree of independence and ability to manage my time that I’m not convinced I was really ready for – and if actually doing a PhD is difficult, not really doing it properly and then realising you have a lot of ground to make up is pretty anxiety-inducing.\nThough my experience is hardly universal, it’s clear I’m not alone in having suffered mental ill-health during my PhD. But should it have been that way? Was I the problem, or was the PhD? I’m not sure it’s that cut and dried.\nAppropriate and supportive\nGoing back to the Hepi report, the Nature data usefully ranks PhD candidates top concerns. The five most frequently cited are:\n- The difficulty of maintaining work/life balance – 37 per cent\n- Imposter syndrome – 32 per cent\n- Uncertainty about my job/career prospects – 30 per cent\n- The number of available faculty research jobs beyond postdoc – 27 per cent\n- Concern about my mental health as a result of PhD study – 26 per cent\nGranted, this question didn’t ask about really concrete issues like “I can’t get access to the resources I need”. But it’s striking that these concerns represent such a mix of the personal, and environmental. And there’s not an obvious response – offer a training session on work/life balance? This assumes the candidate is the problem, not the culture. Set a standard limiting working hours and force everyone to comply? But what if you do need to run an experiment for 24 hours at a time? And how would you force compliance in any case? And wouldn’t candidates resent having their time policed like that?\nWhat this indicates, to me, is the necessity of active stewardship of research communities – with shared (and well-enforced) codes of practice, clarity about the competencies required to be a full and active participant in that community and the rights and responsibilities of different members of the community, and, vitally, forums for debate and challenge about the evolution of the community, in which diverse voices and perspectives are actively sought, and actions taken in response.\nWhen you’re an undergraduate student, there’s a bunch of things you can reasonably expect from your institution, and if you don’t get them, you can be rude in the NSS, or raise a complaint, or start a campaign with your SU. When you’re a PhD candidate, variations of these things are available, but the lines are blurrier, the issues frequently less tangible and more personal, and the consequences more impactful. Both the Nature/Wellcome data and PRES create a way in to talking about these issues, but neither can really capture and encapsulate the experience of doing a PhD.","If you are living with depression yourself, or know someone who is, then you are not alone. In the UK in 2014 19.7% of people aged 16 and over showed symptoms of anxiety or depression.\nThis leads to unhappiness, workplace absenteeism, and general feelings of sadness and not being able to go about life as normal. At its extremes depression can be completely debilitating.\nThe team at Citizen Counselling can provide one to one support in person for Beating Stress, from our Counselling Centres in central Birmingham and Jewellery Quarter or via secure phone or video link. We use a secure web based service similar and as easy to use as Skype but with additional security and encryption features. We don’t provide a crisis service but this is provided by others the contacts of which you will find in the questions below.\nWhat is depression?\nDepression affects people in different ways and can cause a wide variety of symptoms including loss of appetite, poor sleep and feelings of impending doom, feelings of worthlessness. Others range from lasting feelings of unhappiness and hopelessness, to losing interest in the things you used to enjoy and feeling very tearful. Many people with depression also have symptoms of anxiety.\nIs depression different from anxiety?\nDepression and anxiety often are experience together but they are different conditions.\nDepression is more to do with persistent feelings of low mood or sadness. A lot of people with depression also experience anxiety at times and worry about the future.\nAnxiety can be a reaction to stress and worries, phobia related or even be a panic disorder. You can be anxious without having depression.\nWhat are the symptoms of depression?\nSymptoms can vary but generally can include a loss of appetite, sleeping poorly(feeling tired as a result), feelings of impending doom, lasting feelings of unhappiness and hopelessness, osing interest in the things you used to enjoy and feeling very tearful.\nIs there a test to see if I have depression?\nDoctors use a couple of tests; the PHQ9 (patient health questionnaire 9) and the GADD7 (general anxiety disorder) both of these allow a scoring of general mood and ability, or lack of ability. Score in double figures on both can indicate a degree of depression and anxiety.\nHow can depression be treated?\nDepression tends to be treated in two ways; psychological therapies inciuding counselling and CBT – cognitive behaviour therapy or by prescription medicines known as anti-depressants. The most prescribed anti-depressant drugs are citalopram and fluoxetine.\nPsychological therapies and antidepressants have both been found useful for some patients with mild symptoms been effectively treated through CBT.\nWhat can Citizen do to help with depression?\nCitizen provides CBT and Counselling to adults and young people over 14 years of age. We have over 30 counsellors who can help. They are available for appointments 7 days a week 8am to 8pm. Simply call the office on 0121 314 7075 to find the right counsellor for you.\nThose 14-25 who are registered with a GP in Birmingham and live in the area can access free counselling form Citizen in Birmingham from our Jewellery Quarter or Digbeth locations. This is due to our relationship with Living Well UK and Forward Thinking Birmingham.\nOver 25’s, or those out of the area, or under 14 can get free support though their GP. If you are employed your company might have a free counselling programme via BUPA, AXA or another provider\nWho can I talk to if I feel like self-harming or acting on suicidal thoughts?\nCitizen isn’t a crisis service but the following services may be able to help.\nSamaritans – 116 123\nCALM men’s helpline – 0800 58 58 58\nPapyrus Hopeline – preventing young suicide – 0800 068 41 41\nYou can also get help form your GP or local Crisis mental health teams at A&E.\nDepression statistics in paragraph one from Evans, J., Macrory, I., & Randall, C. (2016). Measuring national wellbeing: Life in the UK, 2016. Antidepressant information from nhs.uk/news/medication/big-new-study-confirms-antidepressants-work-better-placebo/"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5e82c6f4-0623-4be0-98d1-8e93973a85c8>","<urn:uuid:d8f4e83a-d4f3-48ed-9ea3-929b441e7d6f>"],"error":null}
{"question":"How does Korean Natural Farming method work in piggeries, and what are the main techniques used in vertical farming?","answer":"The Korean Natural Farming method in piggeries uses indigenous microorganisms (IMOs) that break down animal waste naturally. There are four types of IMOs, which are added once for the life of the practice. This system results in no smell, no flies, and no need for waste removal - some piggeries have operated for 12 years without waste removal. As for vertical farming techniques, there are three main approaches: hydroponics (growing food in water using mineral fertilizer solutions), aeroponics (using mist or nutrient solutions without any growing medium), and aquaponics (combining fish farming with hydroponics to create a symbiotic relationship where fish waste feeds the plants and plants clean the water for the fish).","context":["A Korean-based method of managing animal waste is improving hog farming conditions and garnering support on Hawaii Island.\n“There seems to be a growing interest in natural farming,” Donn Mende, Hawaii County research and development deputy director, said.\nSim Mook Kang, owner of Kang Farms in Mountain View, adopted the practice for his piggery outside of Kurtistown in 2009. It was the first of its kind in the United States to use innovative waste management technology that, according to Kang, leaves most visitors surprised.\n“It’s a pretty good system because there’s no smell,” Kang said.\nMichael DuPonte, Hawaii County extension livestock agent on the East side of the island, said he “couldn’t believe it” when Korean Natural Farming creator Master Cho Han demonstrated the method’s success during a tour of his Korean-based piggeries in 2008.\n“When I walked in I couldn’t believe what I saw — no smell; no flies,” he said.\nThe lack of smell that’s typically associated with pig waste can be attributed to the use of indigenous microorganisms or “IMOs.” Farmers who follow the Korean Natural Farming method incorporate IMOs into the soil that end up breaking down the animal waste naturally.\nAccording to a document written by DuPonte, the process of Korean Natural Farming centers around the use of self-collected IMOs, a “deep green waste litter system” and using “strategic solar positioning and natural ventilation for drying and cooling.”\nAccording to a U.S. Dept. of Agriculture document, there are four types of IMOs. IMO 1 are organisms that have been gathered by exposing sterile, fresh-cooked rice substrate to good fertile soil from the local garden, farm field, and humus from under trees and bushes.\nMature IMO 1 is bound together with fungal mycelia and also contains a variety of bacteria and other microorganisms. IMO 2 involves IMO 1 organisms along with the rice substrate, which has been mixed with an equal weight of brown sugar, mashed to a paste and allowed to ferment under a porous cover for at least a week. IMO 3 involves completed IMO 2 that has been diluted with water and optional nutrients and used to moisten rice or wheat bran, followed by fermentation to multiply the IMO 2 organisms.\nThe fermentation lasts a week or more depending on conditions. IMO 4 is IMO 3 that has been mixed with an equal weight of good rock-free native topsoil and allowed to continue fermentation until activity subsides.\nDuPonte said IMOs are added only once for the life of the practice, and at Kang’s farm there hasn’t been a need to clean the pigs’ area and no runoff has been reported since the system was adopted five years ago.\nAlthough the Big Island piggery has less than a dozen porkers, DuPonte said he’s seen the system work in piggeries that held more than 300 swine and that some of the piggeries in Korea had been in operation for 12 years without waste removal.\nThe state document also suggests that by using the natural method, animals are “less susceptible to disease and personal injury, as well as higher average-daily weight gain.”\nKang said thanks to the new method, the industry has a the potential to “make $200,000 a year.”\nDuPonte said the pigs are sold at six to eight weeks old, usually to markets in Micronesia, China and the Philippines.\nThe use of Korean Natural Farming at Kang’s farm is being monitored and studied by Hawaii County Research and Development, Hawaii Department of Health-Clean Water Branch, and USDA Natural Resource Conservation Service and works in collaboration with Cho Global Natural Farming in South Korea.\nThere are more than 40 piggeries in the state that now use the Korean Natural Farming method.\nEmail Megan Moseley at email@example.com","As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:41db0b43-4cf2-405c-9f7b-e80a02c0fbfd>","<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>"],"error":null}
{"question":"How can one reach the top of Daulatabad Fort in Maharashtra, and what obstacles might visitors encounter along the way?","answer":"To reach the top of Daulatabad Fort, visitors must climb 200 meters (656 feet) up uneven stone steps and steep ramps. The route includes twisty passages, some of which are very dark, with deliberate drop-offs and false paths (now roped off for safety). Visitors must cross a bridge over a moat that contains water with algae. The climb leads through multiple fortified gates and watchtowers before reaching the summit, which offers spectacular views of the surrounding countryside.","context":["Now that I’ve got your attention, this weekend past, during a trip to see the sights around Aurangabad in Maharastra, we climbed up to the top of Daulatabad Fort (aka Devagiri or Deogiri). It’s a vigorous hike up 200 meters (656 feet) of uneven stone steps and steep ramps, winding through dark passages, up to spectacular views of the surrounding countryside.\nDaulatabad and its fort have a long history as a key military and economic stronghold, dating back to the late 1100s. Devagiri was a fortress of the Hindu Yadava dynasty kings from the late 1100s before being seized by Moslem rulers around 1300, who saw its advantages as a military headquarters, and moved their capital to it in 1328. Water and resource shortages later led them to relocate back to Delhi. However it continued to be a center of power and contention for the next several hundred years, passing back and forth among various earlier Hindu, Mughal, Maratha, and later Maratha Peshwa rulers. These struggles, alliances, and inter-ethnic détentes are the foundation of Maharashtra province’s cultural mix to this day.\nDevagiri’s defenses include many fortified gates, watchtowers, and multiple moats, one of which used to be spanned by a retractable leather bridge, traversable only on foot. Today’s bridge is sturdy iron and wood, a good thing because the moat still retains water, green with algae in this drought.\nThere is a twisty, steep and in places – a very dark passage on the main entry route, with deliberate drop-offs and false paths to lure invaders to their doom (thankfully roped off to protect clueless tourists).\nThere are sluice gates through which rocks, scalding water or burning oil could be dropped on the unwary; archer holes, hidden sally ports, and cliffs hand cut and polished to make climbing impossible. Later defenses included gigantic brass cannon (as long as a van) mounted on towers. These are cleverly balanced on center pivots so they could batter besieging armies coming from any direction.\nThe ramparts are massive, cut into or assembled from the hard basalt stone of the region. Building across the entire site clearly shows multiple periods of construction, ranging from the mighty initial citadel, to later Moghul palaces of graceful stonework, decorated with imported Chinese porcelain tiles and painted stucco.\nRe-use of older fragments in newer construction is common:\nNow why did I invoke Minas Tirith for this post? I’m no Tolkien scholar, but to my novice eyes Devagiri presents some strong parallels to his descriptions. For one, Devagiri is chiseled into a butte-shaped rock outcropping, a freestanding mesa overlooking the surrounding lands. The fort itself is built into, on top of, and around this massive prow-shaped stone:\nThe fortifications are nested in winding layers, with gates widely offset to delay attackers, and to lead them past ambush points. There are quarters, cisterns, and storehouses all the way to the summit. The lower circles of defense sport wide avenues that circle the hill, passing through defiles that could shelter large numbers of defenders ready to pounce on any incoming troops. Many of the towers, shrines, palaces, and walls (with the exception of a later brick-red minaret) were once covered with white stucco, and must have been an imposing presence, a multi-tiered, gleaming set of ramparts on top of the hill’s sheer, black, hand-chiseled cliffs.\nOf course there are many differences, too. Tolkien describes a much larger city, with more circles of defense, and an entire population living inside. There was a town to support Devagiri, inside the outermost circle of fortifications, but it was arrayed at the base of the hill, rather than on the hill itself.\nDautalabad itself is far inland, up on the Deccan Plateau – on an arid high plain punctuated with other hills of similar configuration. There is no navigable river or port for ships, be they from Dol Amroth or Umbar. And while there are far-outer rings of defense enclosing what may have been support lands for agriculture or fodder-lands for war horses and elephants, there is no great wall of Rammas Echor enclosing a Pelennor-sized expanse before the gate.\nDevagiri was well-known to the British, and was widely described and depicted as early as the late 1820s, although by the time they arrived it had past its glory, and was no longer a military stronghold. I would not be surprised to learn that the professor had read about it, perhaps as a boy learning the history of Marathi Empire and the three Anglo-Maratha wars that culminated in the British consolidating power over the majority of the Indian subcontinent. Perhaps some student of literature will take this idea as thesis fodder and delve more deeply into it…\nNo longer the contended redoubt of fierce warrior poets, today Devagiri is the slowly crumbling home of tour guides, elderly temple wardens, souvenir hawkers, gray langur monkeys, stray dogs, and clouds of bats. Romance and idle speculation aside, visiting the fort has taught me much about the area’s history and onion-like culture, with layers of influences laid one on top of the other.\nI marvel at the scope of effort and breadth of power a monument of this size still represents."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:348cebc7-4c49-4dbe-8dbe-5e1fec11a5a4>"],"error":null}
{"question":"How do electromagnetic shielding materials and encapsulants compare in terms of their primary protective functions for electronic components?","answer":"Electromagnetic shielding materials and encapsulants serve different protective functions for electronic components. Electromagnetic shielding materials primarily reduce electromagnetic fields by blocking them with conductive or magnetic materials, protecting against radio waves, electromagnetic fields, and electrostatic fields. They typically use materials like sheet metal, metal screen, and metal foam to create barriers. In contrast, encapsulants are polymeric materials designed to provide environmental protection against mechanical, chemical, electrical, and thermal stress. They insulate and protect circuit boards and semiconductors by forming a protective layer that offers mechanical strength, chemical resistance, dielectric insulation, and thermal conductivity within a broad temperature range of -65 to 200° C.","context":["|This article needs additional citations for verification. (March 2010) (Learn how and when to remove this template message)|\nElectromagnetic shielding is the practice of reducing the electromagnetic field in a space by blocking the field with barriers made of conductive or magnetic materials. Shielding is typically applied to enclosures to isolate electrical devices from the 'outside world', and to cables to isolate wires from the environment through which the cable runs. Electromagnetic shielding that blocks radio frequency electromagnetic radiation is also known as RF shielding.\nThe shielding can reduce the coupling of radio waves, electromagnetic fields and electrostatic fields. A conductive enclosure used to block electrostatic fields is also known as a Faraday cage. The amount of reduction depends very much upon the material used, its thickness, the size of the shielded volume and the frequency of the fields of interest and the size, shape and orientation of apertures in a shield to an incident electromagnetic field.\nTypical materials used for electromagnetic shielding include sheet metal, metal screen, and metal foam. Any holes in the shield or mesh must be significantly smaller than the wavelength of the radiation that is being kept out, or the enclosure will not effectively approximate an unbroken conducting surface.\nAnother commonly used shielding method, especially with electronic goods housed in plastic enclosures, is to coat the inside of the enclosure with a metallic ink or similar material. The ink consists of a carrier material loaded with a suitable metal, typically copper or nickel, in the form of very small particulates. It is sprayed on to the enclosure and, once dry, produces a continuous conductive layer of metal, which can be electrically connected to the chassis ground of the equipment, thus providing effective shielding.\nRF shielding enclosures filter a range of frequencies for specific conditions. Copper is used for radio frequency (RF) shielding because it absorbs radio and magnetic waves. Properly designed and constructed copper RF shielding enclosures satisfy most RF shielding needs, from computer and electrical switching rooms to hospital CAT-scan and MRI facilities.\nOne example is a shielded cable, which has electromagnetic shielding in the form of a wire mesh surrounding an inner core conductor. The shielding impedes the escape of any signal from the core conductor, and also prevents signals from being added to the core conductor. Some cables have two separate coaxial screens, one connected at both ends, the other at one end only, to maximize shielding of both electromagnetic and electrostatic fields.\nThe door of a microwave oven has a screen built into the window. From the perspective of microwaves (with wavelengths of 12 cm) this screen finishes a Faraday cage formed by the oven's metal housing. Visible light, with wavelengths ranging between 400 nm and 700 nm, passes easily through the screen holes.\nNATO specifies electromagnetic shielding for computers and keyboards to prevent passive monitoring of keyboard emissions that would allow passwords to be captured; consumer keyboards do not offer this protection primarily because of the prohibitive cost.\nRF shielding is also used to protect medical and laboratory equipment to provide protection against interfering signals, including AM, FM, TV, emergency services, dispatch, pagers, ESMR, cellular, and PCS. It can also be used to protect the equipment at the AM, FM or TV broadcast facilities.\nHow electromagnetic shielding works\nElectromagnetic radiation consists of coupled electric and magnetic fields. The electric field produces forces on the charge carriers (i.e., electrons) within the conductor. As soon as an electric field is applied to the surface of an ideal conductor, it induces a current that causes displacement of charge inside the conductor that cancels the applied field inside, at which point the current stops.\nSimilarly, varying magnetic fields generate eddy currents that act to cancel the applied magnetic field. (The conductor does not respond to static magnetic fields unless the conductor is moving relative to the magnetic field.) The result is that electromagnetic radiation is reflected from the surface of the conductor: internal fields stay inside, and external fields stay outside.\nSeveral factors serve to limit the shielding capability of real RF shields. One is that, due to the electrical resistance of the conductor, the excited field does not completely cancel the incident field. Also, most conductors exhibit a ferromagnetic response to low-frequency magnetic fields, so that such fields are not fully attenuated by the conductor. Any holes in the shield force current to flow around them, so that fields passing through the holes do not excite opposing electromagnetic fields. These effects reduce the field-reflecting capability of the shield.\nIn the case of high-frequency electromagnetic radiation, the above-mentioned adjustments take a non-negligible amount of time, yet any such radiation energy, as far as it is not reflected, is absorbed by the skin (unless it is extremely thin), so in this case there is no electromagnetic field inside either. This is one aspect of a greater phenomenon called the skin effect. A measure of the depth to which radiation can penetrate the shield is the so-called skin depth.\nEquipment sometimes requires isolation from external magnetic fields. For static or slowly varying magnetic fields (below about 100 kHz) the Faraday shielding described above is ineffective. In these cases shields made of high magnetic permeability metal alloys can be used, such as sheets of Permalloy and Mu-Metal (trademark of Magnetic Shield Corporation, Illinois), or with nanocrystalline grain structure ferromagnetic metal coatings. These materials don't block the magnetic field, as with electric shielding, but rather draw the field into themselves, providing a path for the magnetic field lines around the shielded volume. The best shape for magnetic shields is thus a closed container surrounding the shielded volume. The effectiveness of this type of shielding depends on the material's permeability, which generally drops off at both very low magnetic field strengths and at high field strengths where the material becomes saturated. So to achieve low residual fields, magnetic shields often consist of several enclosures one inside the other, each of which successively reduces the field inside it.\nBecause of the above limitations of passive shielding, an alternative used with static or low-frequency fields is active shielding; using a field created by electromagnets to cancel the ambient field within a volume. Solenoids and Helmholtz coils are types of coils that can be used for this purpose.\nSuppose that we have a spherical shell of a (linear and isotropic) diamagnetic material with permeability , with inner radius and outer radius . We then put this object in a constant magnetic field:\nSince there are no currents in this problem except for possible bound currents on the boundaries of the diamagnetic material, then we can define a magnetic scalar potential that satisfies Laplace's equation:\nIn this particular problem there is azimuthal symmetry so we can write down that the solution to Laplace's equation in spherical coordinates is:\nAfter matching the boundary conditions\nat the boundaries (where is a unit vector that is normal to the surface pointing from side 1 to side 2), then we find that the magnetic field inside the cavity in the spherical shell is:\nwhere is an attenuation coefficient that depends on the thickness of the diamagnetic material and the magnetic permeability of the material:\nThis coefficient describes the effectiveness of this material in shielding the external magnetic field from the cavity that it surrounds. Notice that this coefficient appropriately goes to 1 (no shielding) in the limit that . In the limit that this coefficient goes to 0 (perfect shielding), then the attenuation coefficient takes on the simpler form:\nwhich shows that the magnetic field decreases like .\nNOTE: In the above relations, is relative permeability µr, which is the ratio of the permeability of a specific medium to the permeability of free space µ0:\nwhere µ0 = 4π × 10−7 N A−2.\n- Electromagnetic interference\n- Electromagnetic radiation and health\n- Ionising radiation protection\n- MRI RF shielding\n- Electric field screening\n- Faraday cage\n- Anechoic chamber\n- Plasma window\n- Seale, Wayne (2007). The role of copper, brass, and bronze in architecture and design; ‘‘Metal Architecture,’’ May 2007\n- Radio frequency shielding, Copper in Architecture Design Handbook, Copper Development Association Inc., http://www.copper.org/applications/architecture/arch_dhb/fundamentals/radio_shielding.html\n- \"Metal shields and encryption for US passports\". Newscientist.com. Retrieved 18 November 2012.\n- Martin Vuagnoux and Sylvain Pasini (2009-06-01). \"Compromising Electromagnetic Emanations of Wired and Wireless Keyboards\". Lausanne: Security and Cryptography Laboratory (LASEC).\n- \"MuMETAL\" (PDF). Magnetic Shield Corp. 2012. Catalog MU-2. Retrieved 26 June 2016.\n- \"Trademark Status & Document Retrieval\". tsdr.uspto.gov. Retrieved 2017-08-02.\n- \"Interference Technology Magazine Whitepaper on Ferromagnetic Nanocrystalline Metal Magnetic Shield Coatings\". Archived from the original on March 15, 2010.\n- \"NMR Magnet Shielding: The seat of the pants guide to understanding the problems of shielding NMR magnets\". Acorn NMR. 22 January 2003. Retrieved 27 June 2016.\n- Jackson, John David (10 August 1998). Classical Electrodynamics (third ed.). Section 5.12. ISBN 978-0471309321.\n- All about Mu Metal Permalloy material\n- Mu Metal Shieldings Frequently asked questions (FAQ by MARCHANDISE, Germany) magnetic permeability\n- Clemson Vehicular Electronics Laboratory: Shielding Effectiveness Calculator\n- Shielding Issues for Medical Products (PDF) — ETS-Lindgren Paper\n- Practical Electromagnetic Shielding Tutorial\n- Simulation of Electromagnetic Shielding in the COMSOL Multiphysics Environment","Encapsulants and Potting Compounds Information\nEncapsulants and potting compounds are designed to insulate and protect electrical and electronic components. They typically provide environmental protection, electrical insulation, and other specialized characteristics.\nEncapsulants protect circuit boards, semiconductors, and other electronic components from environmental hazards such as mechanical, chemical, electrical, and thermal stress. Nearly all encapsulants consist of polymeric material, which must fdeliver an adequate balance of mechanical strength, chemical resistance, dielectric insulation, and thermal conductivity within a broad temperature range (generally from -65 to 200° C [-85 to 390° F]).\nEncapsulant materials are typically chosen based on the rigors of the intended application. Polyurethanes, polyesters, and polyamides are employed when circuits will be exposed to mild environments and consistent thermal conditions. Thermoplastics, which offer the advantage of easy recycling and reuse, are occasionally used for low-reliability applications. Silicone, having several qualities which are desirable for encapsulating circuits, is commonly used as a pottant or encapsulant.\nInternal stress, an important parameter to consider when selecting encapsulants, refers to the stress of the hardened encapsulant on encapsulated components. Internal stress causes die warping, the cracking of encapsulants or chips, and wire shearing.\nSome common electronics packages, along with their ideal encapsulant characteristics, are listed below.\nSurface-mounted (SMT) packages: Small amounts of encapsulant with high mechanical strength, adhesion, hydrophobicity, and excellent high temperature resistance.\nBeam-bonded components: Low maximum processing temperatures.\nLarge chips: Low internal stress; very low thermal shrinkage.\nMemory devices and microprocessors: Uranium- and thorium-free to eliminate radiation-induced errors.\nHigh-power devices: High thermal conductivity; low internal stress.\nEncapsulant Systems and Curing\nEncapsulants change from a viscous, liquid substance, which can be poured into a circuit, to a solid, protective substance; this process is known as curing. Curing occurs when cross-linkers within encapsulant compounds form solid polymeric or elastomeric bonds. Encapsulants and potting compounds may cure when exposed to UV or visible light, or simply room temperature air (the latter process is known as room temperature vulcanizing, or RTV). Condensation curing involves the use of moisture present in the atmosphere, which is possible only for open-air curing.\nThe image above and at right shows the curing process of a UV adhesive. When exposed to ultraviolet radiation, photoinitiators within the resin—represented here by small, red circles—trigger cross-linking and subsequent hardening.\nThe application of various hot-melt encapsulants.\nVideo credit: PowerAdhesives\nSome encapsulants require an additive to cure; these are known as two-part or two-component systems consisting of the resin itself as well as a catalyst such as a hardener or accelerator. Some resins and catalysts are available premixed and frozen for convenience. Two-component resins must be carefully considered, as the curing reaction often produces byproducts which may prove harmful to certain circuit types.\nEncapsulants can be generally classified as potting compounds, glob-top encapsulants, and molding compounds.\nPotting compounds (or pottants) are designed to be poured into potted circuit boards. Potted PCBs have relatively high-walled sides, with components mounted within the enclosure. Potting compounds are typically applied in a thicker layer than other encapsulants as a result. The image at right shows pottant being poured into a potted PCB.\nMost potting compounds are two-component resins and share these common characteristics:\nLow viscosity and high thermal expansion coefficients\nModerate cure temperatures\nLow internal stress\nLow levels of ionic contaminants\nGood thermal stability and insulating properties\nGlob-top encapsulants are applied directly to microelectronic components which are mounted to a PCB. They have historically been low-precision thermoset resins used to encapsulate inexpensive components, although improved-quality glob tops are becoming more common. All glob tops share some common attributes, such as higher viscosity, quick cure times, mechanical strength, adhesive compatibility, and good electrical insulation properties.\nA bare component (left) and the same component encapsulated using a glob top.\nImage credit: DIMA Group\nMolding compounds take the form of granulated powder tablets which must be mixed with a fluid and heated to form a soft dough. This material then efficiently fills electronics packages using a molding press. The compound itself is typically a one-component formula consisting of finely ground resins, catalysts, and modifiers such as flame retardants.\nEncapsulants and potting compounds may be produced, used, and tested according to various published standards and specifications. Examples include:\nASTM F641 Standard specification for implantable epoxy electronic encapsulants\nBS EN 60664-3 Insulation coordination for equipment within low-voltage systems: Use of coating, potting, or moulding for protection against pollution\nQPL-24041 Polyurethane molding and potting compound\nMaster Bond / AZOM - Potting and Encapsulants for Electronics\nRelated Products & Services\nAnaerobic Adhesives and Anaerobic Sealants\nAnaerobic adhesives and anaerobic sealants cure in the absence of air or oxygen.\nElectrical and Electronic Resins\nElectrical and electronic resins includes adhesives, greases, pads, stock shapes, tapes, encapsulants, potting compounds, thermal interface materials, and electrically conductive substances used in electrical, electronics, and semiconductor applications.\nHot Melt Adhesives\nHot melt adhesives are solvent-free chemical compounds that are used to join materials. They can be repeatedly softened by heat and hardened or set by cooling, allowing parts to be removed or repositioned during assembly.\nPlastic and Rubber Balls\nPlastic and rubber balls are rolling, spherical elements that have low friction values and require little or no lubrication. They are lighter than metal balls and resistant corrosion and abrasion. Some plastic balls resist high temperatures, but others do not. Rubber balls are characterized by a high degree of flexibility and elasticity.\nPolymers and Plastic Resins\nPolymers and plastic materials are organic, synthetic or processed polymers that are supplied as raw materials. They typically consist of thermoplastic or thermosetting resins in the form of pellets, powders or liquid resins. These materials can then be molded into a variety of shapes for a wide range of uses.\nRubber Adhesives and Sealants\nRubber adhesives and sealants are highly flexible, natural or synthetic materials that are used to join components or fill gaps between seams or on surfaces.\nThermal Compounds and Thermal Interface Materials\nThermal compounds and thermal interface materials form a thermally conductive layer on a substrate, between components or within a finished product."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:af76c508-798c-4f9c-b84b-99171ab7585d>","<urn:uuid:3f5a7e03-1685-4809-92b1-44cb4d80725a>"],"error":null}
{"question":"What languages are found in both the Cairo Geniza and Dead Sea Scrolls collections?","answer":"Both collections contain texts in Hebrew, Aramaic, and Greek. The Cairo Geniza additionally includes documents in Judeo-Arabic, Judeo-Greek, Judeo-Spanish, Judeo-Persian, Judeo-German (Yiddish), and Arabic, while the Dead Sea Scrolls are primarily written in Hebrew and Aramaic, with only a few texts in Greek.","context":["Text Treasures: Cairo Geniza\nThe Cairo Geniza refers to the cache of about 300,000 documents found in the attic storeroom of the Ben Ezra Synagogue, located in Fustat (in Old Cairo), the capital city of Egypt during the seventh–tenth centuries C.E. The creation and preservation of the Cairo Geniza owes to the long-lived Jewish habit of consigning disused texts in Hebrew script to a slow decay in dignified limbo, safe from profanation, rather than casually destroying them through dumping. Not a curated collection or archive arranged for storage and retrieval, the Cairo Geniza is thus an accidental mass of dead writings piled up much like archaeological strata. The Hebrew word geniza signifies “hiding place.”\nThe storeroom—accessible only by ladder from the women’s balcony of the synagogue—was never really forgotten, so we are perhaps unjustified in talking about its discovery. Starting in the 1880s, however, scholars from Jerusalem, England, and elsewhere learned of the existence of the documents and thus began to empty the storeroom of some of its contents. Among the early visitors were the Scottish twin sisters Agnes Smith Lewis and Margaret Dunlop Gibson, who upon their return to Cambridge, in 1896, showed their documents to the great scholar of Jewish studies, Solomon Schechter. The documents included a page of the Hebrew original of the book of Ben Sira and inspired Schechter to travel to Cairo. With permission of the rabbi of the synagogue, Refael Aharon Ben-Shimon, himself an important scholar, Schechter was able to remove the remaining contents of the Cairo Geniza.a\nBuilt between 1025 and 1041, the Ben Ezra Synagogue served the local community for more than 900 years—thus explaining the enormous volume of accumulated writings—until it was decommissioned during the 1960s.b The survival of the documents owes to the fortunate confluence of multiple factors, including favorable climate with stable humidity levels; the omnipresent dust from local limestone containing high levels of calcium carbonate, which naturally aids the preservation of paper, parchment, and ink; and the fact that the storeroom was apparently large enough that it never needed to be emptied.\nThe text treasures of the Cairo Geniza cover more than a millennium of history. While the latest documents date from the 19th century, the earliest recovered writings predate the founding of the synagogue by centuries. This is due to the practice of repurposing old documents (often as palimpsests) and because sturdy parchment Torah scrolls may be used for centuries and, in the present instance, clearly were transferred from an older synagogue to the then newly built Ben Ezra. Dating as far back as the fifth or sixth century, the earliest writings in the geniza survived because they were reused as scrap paper to record new texts.\nNo other geniza or archive (ancient or medieval) can compare in size and diversity to the Cairo Geniza. Only the Oxyrhynchus Papyri—a monumental collection of papyri dated to the Greco-Roman period from the Egyptian city of Oxyrhynchus—comes close, 065 with 86 volumes of edited texts published so far. The largest single grouping of Geniza documents is now in the Cambridge University Library. With its approximately 200,000 documents, it is also the world’s largest collection of medieval Jewish manuscripts. The remaining documents are held in about 70 different museums, libraries, and private collections around the world.\nThe Cairo Geniza is unique also in that it contains much more than the usual sacred Hebrew books, such as biblical and Talmudic texts, prayer books, and compendia of Jewish law. For some reason, the Jews of Cairo also kept writings from many secular genres (grammar, lexicography, poetry, philosophy, etc.) as well as quotidian and ephemeral documents, such as receipts, medical prescriptions, recipes, marriage contracts, leases, private letters, and school writing exercises. Besides texts in Hebrew, the cache contains writings in Aramaic, Judeo-Arabic, Judeo-Greek, Judeo-Spanish, Judeo-Persian, Judeo-German (Yiddish), Greek, and Arabic. The polyglot documentation indicates the Cairo community’s contacts with other Jewish communities around the world.\nWritings recovered from the Cairo Geniza offer a window on the Jewish community of medieval and early modern Cairo but also the wider economic, social, and cultural history of the Mediterranean world. They bring to life Jewish customs and daily concerns of a minority living under the rule of Muslim dynasties. A sizable number of the palimpsests astonishingly include Arabic documents that emanated from the state administration of the Fatimid rulers of Egypt (969–1171) and allow an unparalleled view of the early Islamic state. The example on the opposite page shows 13 lines of an Arabic petition concerning land, from c. 1150 C.E., which was later reused to write down Jewish liturgical poems called piyyutim, in Hebrew.\nThere is no single publication series working toward a complete edition or translation of the Geniza documents. The sheer magnitude and diversity of the corpus favor discrete linguistically and subject-focused projects. Several digital initiatives provide access to the myriad of dispersed fragments and facilitate their identification, cataloging, transcription, and translation. They include the Friedberg Genizah Project, the Taylor-Schechter Genizah Research Unit at Cambridge University Library, Princeton Geniza Project, and Penn/Cambridge Genizah Fragment Project. Stefan Reif’s A Jewish Archive from Old Cairo: The History of Cambridge University’s Genizah Collection (Routledge, 2000) and Sacred Trash: The Lost and Found World of the Cairo Geniza, by Adina Hoffman and Peter Cole (Schocken, 2011), provide accessible introductions. You can also watch Michelle Paymar’s 2018 documentary From Cairo to the Cloud: The World of the Cairo Geniza.—M.D.\nThe Cairo Geniza refers to the cache of about 300,000 documents found in the attic storeroom of the Ben Ezra Synagogue, located in Fustat (in Old Cairo), the capital city of Egypt during the seventh–tenth centuries C.E. The creation and preservation of the Cairo Geniza owes to the long-lived Jewish habit of consigning disused texts in Hebrew script to a slow decay in dignified limbo, safe from profanation, rather than casually destroying them through dumping. Not a curated collection or archive arranged for storage and retrieval, the Cairo Geniza is thus an accidental mass of dead writings piled up […]","1.The Dead Sea Scrolls were discovered in eleven caves along the northwest shore of the Dead Sea between the years 1947 and 1956. The area is 13 miles east of Jerusalem and is 1300 feet below sea level. The mostly fragmented texts, are numbered according to the cave that they came out of. They have been called the greatest manuscript discovery of modern times. See a Dead Sea Scroll Jar. 2. Only Caves 1 and 11 have produced relatively intact manuscripts. Discovered in 1952, Cave 4 produced the largest find. About 15,000 fragments from more than 500 manuscripts were found.\n3. In all, scholars have identified the remains of about 825 to 870 separate scrolls.\n4. The Scrolls can be divided into two categories—biblical and non-biblical. Fragments of every book of the Hebrew canon (Old Testament) have been discovered except for the book of Esther.\n5. There are now identified among the scrolls, 19 copies of the Book of Isaiah, 25 copies of Deuteronomy and 30 copies of the Psalms .\n6. Prophecies by Ezekiel, Jeremiah and Daniel not found in the Bible are written in the Scrolls.\n7. The Isaiah Scroll, found relatively intact, is 1000 years older than any previously known copy of Isaiah. In fact, the scrolls are the oldest group of Old Testament manuscripts ever found.\n8. In the Scrolls are found never before seen psalms attributed to King David and Joshua. 9.There are nonbiblical writings along the order of commentaries on the OT, paraphrases that expand on the Law, rule books of the community, war conduct, thanksgiving psalms, hymnic compositions, benedictions, liturgical texts, and sapiential (wisdom) writings.\n10. The Scrolls are for the most part, written in Hebrew, but there are many written in Aramaic. Aramaic was the common language of the Jews of Palestine for the last two centuries B.C. and of the first two centuries A.D. The discovery of the Scrolls has greatly enhanced our knowledge of these two languages. In addition, there are a few texts written in Greek.\n11. The Scrolls appear to be the library of a Jewish sect. The library was hidden away in caves around the outbreak of the First Jewish Revolt (A.D. 66-70) as the Roman army advanced against the rebel Jews.\n12. Near the caves are the ancient ruins of Qumran. They were excavated in the early 1950's and appear to be connected with the scrolls.\n13. The Dead Sea Scrolls were most likely written by the Essenes during the period from about 200 B.C. to 68 C.E./A.D. The Essenes are mentioned by Josephus and in a few other sources, but not in the New testament. The Essenes were a strict Torah observant, Messianic, apocalyptic, baptist, wilderness, new covenant Jewish sect. They were led by a priest they called the \"Teacher of Righteousness,\" who was opposed and possibly killed by the establishment priesthood in Jerusalem.\n14. The enemies of the Qumran community were called the \"Sons of Darkness\"; they called themselves the \"Sons of Light,\" \"the poor,\" and members of \"the Way.\" They thought of themselves as \"the holy ones,\" who lived in \"the house of holiness,\" because \"the Holy Spirit\" dwelt with them.\n15. The last words of Joseph, Judah, Levi, Naphtali, and Amram (the father of Moses) are written down in the Scrolls. 16. One of the most curious scrolls is the Copper Scroll. Discovered in Cave 3, this scroll records a list of 64 underground hiding places throughout the land of Israel. The deposits are to contain certain amounts of gold, silver, aromatics, and manuscripts. These are believed to be treasures from the Temple at Jerusalem, that were hidden away for safekeeping.\n17. The Temple Scroll, found in Cave 11, is the longest scroll. Its present total length is 26.7 feet (8.148 meters). The overall length of the scroll must have been over 28 feet (8.75m).\n18. The scrolls contain previously unknown stories about biblical figures such as Enoch, Abraham, and Noah. The story of Abraham includes an explanation why God asked Abraham to sacrifice his only son Isaac.\n19. The scrolls are most commonly made of animal skins, but also papyrus and one of copper. They are written with a carbon-based ink, from right to left, using no punctuation except for an occasional paragraph indentation. In fact, in some cases, there are not even spaces between the words.\n20. The Scrolls have revolutionized textual criticism of the Old Testament. Interestingly, now with manuscripts predating the medieval period, we find these texts in substantial agreement with the Masoretic text as well as widely variant forms. 21. Some of the Dead Sea Scrolls actually appeared for sale on June 1, 1954 in the Wall Street Journal. The advertisement read — \"The Four Dead Sea Scrolls: Biblical manuscripts dating back to at least 200 BC are for sale. This would be an ideal gift to an educational or religious institution by an individual or group. Box F206.\"\n22. Although the Qumran community existed during the time of the ministry of Jesus, none of the Scrolls refer to Him, nor do they mention any of His follower's described in the New Testament.\n23. The major intact texts, from Caves 1 & 11, were published by the late fifties and are now housed in the Shrine of the Book museum in Jerusalem.\n24. Since the late fifties, about 40% of the Scrolls, mostly fragments from Cave 4, remained unpublished and were unaccessible. It wasn't until 1991, 44 years after the discovery of the first Scroll, after the pressure for publication mounted, that general access was made available to photographs of the Scrolls. In November of 1991 the photos were published by the Biblical Archaeological Society in a nonofficial edition; a computer reconstruction, based on a concordance, was announced; the Huntington Library pledged to open their microfilm files of all the scroll photographs.\n25. The Dead Sea Scrolls enhance our knowledge of both Judaism and Christianity. They represent a non-rabbinic form of Judaism and provide a wealth of comparative material for New Testament scholars, including many important parallels to the Jesus movement. They show Christianity to be rooted in Judaism and have been called the evolutionary link between the two. The rugged terrain of the Qumran area. Recommended For Further Study:The Dead Sea Scrolls: A New TranslationThe Dead Sea Scrolls BibleUnderstanding the Dead Sea ScrollsListing of Dead Sea Scroll Books and Links to Other Sites on the Web\nMain Page • What's New? • Specials • Title Index: A-J • K-Z\nArchaeology • Biblical Studies • Biblical Personalities • Dead Sea Scrolls • Games • History • Bible SoftwareMultimedia • Reference • Religion • Travel\nFeedback • Shipping Information • Order Form • CenturyOne FoundationCopyright ©1996-2005 CenturyOne Bookstore. All Rights"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:88162921-95f6-444a-9a28-48f9585cc902>","<urn:uuid:3c16e521-7e45-4cb4-94a1-ba65953fe21b>"],"error":null}
{"question":"What are the practical applications of visualization in mapping systems, and how do theoretical models guide their development?","answer":"Visualization in mapping systems, exemplified by the New York City subway map redesign, requires understanding geographical relationships and complex tools for better representation. The focus is on creating practical tools where size is just one factor, and attention must be paid to small details to achieve effectiveness. From a theoretical standpoint, visualization development is guided by systematic approaches that consider multiple representations and parameters. Developers need to explore various visualization states, compare different techniques, and evaluate their impact on visual features. This process requires consideration of both the practical needs of users and theoretical frameworks that help determine the most effective visual representation for a given dataset and task.","context":["Visualization is the graphic presentation of data -- portrayals meant to reveal complex information at a glance. Think of the familiar map of the New York City subway system, or a diagram of the human brain. Successful visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail that efficiently generate insight and new understanding.\nThis book examines the methods of two dozen visualization experts who approach their projects from a variety of perspectives -- as artists, designers, commentators, scientists, analysts, statisticians, and more. Together they demonstrate how visualization can help us make sense of the world.\n- Explore the importance of storytelling with a simple visualization exercise\n- Learn how color conveys information that our brains recognize before we're fully aware of it\n- Discover how the books we buy and the people we associate with reveal clues to our deeper selves\n- Recognize a method to the madness of air travel with a visualization of civilian air traffic\n- Find out how researchers investigate unknown phenomena, from initial sketches to published papers\nNick Bilton,Michael E. Driscoll,Jonathan Feinberg,Danyel Fisher,Jessica Hagy,Gregor Hochmuth,Todd Holloway,Noah Iliinsky,Eddie Jabbour,Valdean Klump,Aaron Koblin,Robert Kosara,Valdis Krebs,JoAnn Kuchera-Morin et al.,Andrew Odewahn,Adam Perer,Anders Persson,Maximilian Schich,Matthias Shapiro,Julie Steele,Moritz Stefaner,Jer Thorp,Fernanda Viegas,Martin Wattenberg,and Michael Young.\nTable of contents\n- Beautiful Visualization\n1. On Beauty\n- What Is Beauty?\n- Learning from the Classics\n- How Do We Achieve Beauty?\n- Putting It Into Practice\n2. Once Upon a Stacked Time Series\n- Question + Visual Data + Context = Story\n- Steps for Creating an Effective Visualization\n- Hands-on Visualization Creation\n- Wordle's Origins\n- How Wordle Works\n- Is Wordle Good Information Visualization?\n- How Wordle Is Actually Used\n4. Color: The Cinderella of Data Visualization\n- Why Use Color in Data Graphics?\n- Luminosity As a Means of Recovering Local Density\n- Looking Forward: What About Animation?\n- References and Further Reading\n5. Mapping Information: Redesigning the New York City Subway Map\n- The Need for a Better Tool\n- London Calling\n- New York Blues\n- Better Tools Allow for Better Tools\n- Size Is Only One Factor\n- Looking Back to Look Forward\n- New York's Unique Complexity\n- Geography Is About Relationships\n- Sweat the Small Stuff\n- 6. Flight Patterns: A Deep Dive\n- 7. Your Choices Reveal Who You Are: Mining and Visualizing Social Patterns\n- 8. Visualizing the U.S. Senate Social Graph (1991–2009)\n- 9. The Big Picture: Search and Discovery\n- 10. Finding Beautiful Insights in the Chaos of Social Network Visualizations\n- 11. Beautiful History: Visualizing Wikipedia\n- 12. Turning a Table into a Tree: Growing Parallel Sets into a Purposeful Project\n- 13. The Design of \"X by Y\"\n- 14. Revealing Matrices\n- 15. This Was 1994: Data Exploration with the NYTimes Article Search API\n16. A Day in the Life of the New York Times\n- Collecting Some Data\n- Let's Clean 'Em First\n- Python, Map/Reduce, and Hadoop\n- The First Pass at the Visualization\n- Scene 1, Take 1\n- Scene 1, Take 2\n- The Second Pass at the Visualization\n- Visual Scale and Other Visualization Optimizations\n- Getting the Time Lapse Working\n- So, What Do We Do with This Thing?\n- 17. Immersed in Unfolding Complex Systems\n18. Postmortem Visualization: The Real Gold Standard\n- Impact on Forensic Work\n- The Virtual Autopsy Procedure\n- The Future for Virtual Autopsies\n- References and Suggested Reading\n19. Animation for Visualization: Opportunities and Drawbacks\n- Principles of Animation\n- Animation in Scientific Visualization\n- Learning from Cartooning\n- Presentation Is Not Exploration\n- Types of Animation\n- Staging Animations with DynaVis\n- Principles of Animation\n- Conclusion: Animate or Not?\n- Further Reading\n20. Visualization: Indexed.\n- Visualization: It's an Elephant.\n- Visualization: It's Art.\n- Visualization: It's Business.\n- Visualization: It's Timeless.\n- Visualization: It's Right Now.\n- Visualization: It's Coded.\n- Visualization: It's Clear.\n- Visualization: It's Learnable.\n- Visualization: It's a Buzzword.\n- Visualization: It's an Opportunity.\n- A. Contributors\n- B. Colophon\n- About the Authors\n- Title: Beautiful Visualization\n- Release date: June 2010\n- Publisher(s): O'Reilly Media, Inc.\n- ISBN: 9781449379865\nYou might also like\nThe Big Book of Dashboards\nThe definitive reference book with real-world solutions you won't find anywhere else The Big Book of …\nStorytelling with Data\nInfluence action through data! This is not a book. It is a one-of-a-kind immersive learning experience …\nVisualize This: The FlowingData Guide to Design, Visualization, and Statistics\nPractical data design tips from a data visualization expert of the modern age Data doesn't decrease; …\nStorytelling with Data: A Data Visualization Guide for Business Professionals\nDon't simply show your data—tell a story with it! Storytelling with Data teaches you the fundamentals …","Visualization Space Exploration : Theoretical and Practical Viewpoints\nMetadataShow full item record\nVisualizations are graphical representations of data that have been used in a wide-ranging field of applications to provide a quick overview over data-inherent information. By taking advantage of human perceptual capabilities, visualizations help users understand features and phenomena in data, gather meaningful insights, and drive decision making processes. One of the main motivations in visualization research is to find the best visual representation for a given dataset, user, and task. This challenge is often solved in a subjective manner, where a visualization designer chooses graphical representations, visual channels, and encodings that they believe are best suited for the task at hand. Therefore, the effectiveness and reliability of the result largely varies with the designer's expertise. To make an objectively good design decision, the designer needs to consider all possible visualization methods, or in our words: explore the visualization space. For that purpose, the advantages and disadvantages of individual techniques can be highlighted through comparison methods based on quality metrics, user studies, or theoretical models. Each of these methods can additionally target the visual perception of representations, task-oriented and application-specific measures, structure-oriented matters, or meta-perceptual processes. In this thesis, we aim to establish a greater understanding of the interconnections between independently studied approaches for visualization evaluation by exploring the visualization space from several different viewpoints. First, we take a theoretical approach to identify and classify previous work on the evaluation of visualization methods. We analyze theoretical models, user studies, and quality metrics, and combine them in a unified structure to distinguish classes of task-oriented, perceptual, meta-perceptual, and structure-oriented measures. We then describe the individual class strengths and shortcomings and propose a direction to combine the separate efforts into a bigger picture to advance the field of visualization research as a whole. One instance where visualization exploration takes place in practice is during the development of visualization algorithms. By writing code, adding features, and changing parameters, visualization developers expose a large number of representations in visualization space. We developed a system that explicitly displays these individual states to the user and allows for their exploration and comparison. Parameter changes and their effect on all developed visualization states can be inspected to investigate their impact on visual features. The system not only encourages visualization developers to consider multiple representations when creating a visualization, but further allows for comparisons on a more general level. The simultaneous display of source code changes and visual changes in a meta visualization opens up a large branch of possible future research. We made a first step towards a practical development environment that encourages visualization comparison during the development process and reasoning about correlations of source code changes and their impact on the visual result. In our implementation, we display source code states via node-link diagrams of their abstract syntax trees. Although this representation provides a clear outline of individual hierarchical structures, its juxtaposed nature impairs the comparison of many states. To overcome this issue, we analyzed existing methods for the visualization of dynamic hierarchies and combined the benefits of treemaps and stream-based approaches to display both the individual hierarchies and their evolution over time. We conducted a user study to evaluate the differences in effectiveness on low-level tasks and captured perceptual characteristics in hierarchical visualizations over time. The results suggest that our visualization can be applied as a general-purpose method to replace previous representations for static hierarchies and hierarchical changes over time. All compared visualization types and the effects of mutual parameters can be explored through our open-source implementation. Finally, we explored aesthetic characteristics of artistic diagrammatic paintings and aimed to apply their visual appeal to storyline visualizations. We developed an interactive application that utilizes techniques for automatic layouting and image processing to create visual results similar to hand-drawn diagrams. Our application can further help artists create an initial layout by interactively adding data to the representation and focus their efforts on artistic aspects that are difficult for machines to imitate. In the combination of our work, we explore the visualization field from several different viewpoints, move from visualization theory to practice, and show how individual components of visualization comparison can be combined for greater knowledge gain. We hope to encourage visualization researchers to merge their efforts into a larger theory and understanding of how visualizations work and to create objectively effective visualization solutions.\nHas partsPaper A: Fabian Bolte and Stefan Bruckner. Measures in Visualization Space. In Chen, M., Hauser, H., Rheingans, P. and Scheuermann, G. (eds), Foundations of Data Visualization (2020), p. 39-59. The chapter is not available in BORA due to publisher restrictions. The published version is available at: https://doi.org/10.1007/978-3-030-34444-3_3\nPaper B: Fabian Bolte and Stefan Bruckner. Vis-a-Vis: Visual Exploration of Visualization Source Code Evolution. In IEEE Transactions on Visualization and Computer Graphics (2020). The article is available at: http://hdl.handle.net/1956/21799\nPaper C: Fabian Bolte, Mahsan Nourani, Eric D. Ragan, and Stefan Bruckner. Split- Streams: A Visual Metaphor for Evolving Hierarchies. In IEEE Transactions on Visualization and Computer Graphics (2020). The article is available at: http://hdl.handle.net/1956/22073\nPaper D: Fabian Bolte and Stefan Bruckner. Organic Narrative Charts. In Eurographics 2020 - Short Papers (2020). The article is available in the thesis file. The article is also available at: https://doi.org/10.2312/egs.20201026"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:28f63afd-9b72-4ccb-bfb9-b2150554ef0d>","<urn:uuid:cf728eb2-8e52-4223-b875-cad0c447323d>"],"error":null}
{"question":"For my research on mineral formations, how do the geological occurrence patterns differ between goethite and cinnabar in terms of their formation conditions?","answer":"Goethite and cinnabar form under quite different geological conditions. Goethite forms through weathering of pre-existing iron minerals and typically occurs in massive, stalactitic, fibrous, or botryoidal forms with radial growth patterns. In places like western Kentucky, it forms in porous earthy masses called bog iron ore. In contrast, cinnabar is a hydrothermal mineral that precipitates from ascending hot waters and vapors moving through fractured rocks. It forms at relatively shallow depths where temperatures are below 200 degrees Celsius, typically near recent volcanic activity, hot springs, and fumaroles. Cinnabar usually appears as coatings on rock surfaces and fracture fillings, and occasionally in sediment pore spaces.","context":["Oxides are elements combined with oxygen. Oxide minerals are:\nCrystal system: orthorhombic. Color: yellowish brown to dark brown. Hardness: 5.0-5.5. Streak: brownish-yellow. Luster: dull to adamantine. Specific gravity: 4.37.\nGoethite usually occurs in massive, stalactitic, fibrous, or botryoidal (grape-like) forms, and exhibits radial growth. Goethite weathers from pre-existing iron minerals. In western Kentucky, goethite is associated with limonite in porous earthy masses called bog iron ore. Goethite is an ore of iron in some areas; it is noncommercial in Kentucky.Hematite\nCrystal system: hexagonal. Cleavage: indistinct. Color: brownish red, steel gray, or black. Hardness: 5.5-6.0. Streak: cherry red. Luster: dull to metallic, opaque. Specific gravity: 5.2.\nHematite occurs in platy, compact, granular, or earthy masses. When individual grains are red and about the size and roundness of fish eggs, the mineral is called oolitic hematite; when it occurs in plates or scales as mica does and is steel gray it is called micaceous or specular hematite; and when it occurs as a fine powder that can be mixed with oil and used as a paint pigment, it is called red ocher. Hematite is often the red cementing material in sandstones, and is found to a lesser extent in red clays and shales. It is readily distinguished from all other minerals by its cherry red streak.\nHematite is the most important ore of iron, and also the most important metallic mineral in the world. The value of the pig iron and steel manufactured from it every year exceeds the value of any other metal.\nDuring the late nineteenth century, oolitic hematite was mined in Bath County in northeastern Kentucky. In this area, hematite is found in the Brassfield Dolomite of Silurian age and the Boyle Limestone of Devonian age. The average thickness of the iron-bearing beds is 3 feet, and the higher grade ore ranges in iron content from 46 to 57 percent. Iron ore of this type occurs in Silurian rocks from New York to Alabama. Because of its limited thickness in Kentucky, and because of vast supplies of hematite ore in the Great Lakes region, it became unprofitable to mine in Kentucky.\nHematite crystals have been observed lining the cavities of geodes from Estill County. These crystals are dark brown, bladed, and approximately ∓frac12; inch long.\nHematite can also occur in association with fossils in eastern Kentucky. These productid brachiopods were replaced by siderite, which oxidized to hematite, giving the brachiopods a silver-gray color. The yellow mineral on the right hand brachiopod is the mineral limonite, another iron oxide.\nCrystal system: hexagonal. Color: black to brownish black. Hardness: 5.0-6.0. Streak: black to brownish red. Specific gravity: 4.3-5.5. Magnetism: slightly magnetic; more magnetic when heated. Uses: pigment for white paints.\nIlmenite is considered a heavy mineral and commonly occurs with rutile in the McNairy Sands in western Kentucky and in peridotite dikes in eastern and western Kentucky. It commonly occurs in ancient, near-shore sandstone environments. Although the mineral is very noticeable in beach sands as black specks, it is used as a pigment for white paints; the refining process removes traces of iron and other deleterious materials to create a white refined product.\nCrystal system: tetragonal. Color: black. Hardness: 2-6.\nPyrolusite is commonly found as a secondary mineral and forms in a black, dendritic pattern in sedimentary rocks. It is commonly called wad. The black, soft, sooty mineral is associated with hematite, siderite, and limonite, and is found in the Givens Vein in the Western Kentucky Fluorspar District and in sandstones in eastern Kentucky.\nCrystal system: tetragonal. Color: red to black. Hardness: 6. Streak: yellow or brown. Specific gravity: 4.3. Uses: source of titanium.\nRutile generally occurs with ilmenite and has been noted in the peridotite dikes in eastern and western Kentucky. Rutile also occurs in the McNairy Sands in the Jackson Purchase Region and has been mined in Tennessee, Florida, and parts of the East Coast.\nCrystal system: isometric. Color: black to dark gray. Hardness: 5-6. Specific gravity: 9-10.\nSome radioactive minerals have been reported in the black shale that crops out around the Blue Grass Region. Other occurrences of uranium minerals have been reported in eastern Kentucky, including one at Bell Falls in Menifee County. These radioactive minerals are detected by Geiger counter or scintillometer, since usually the minerals themselves are too small to see with the naked eye. Uraninite generally occurs as massive, botryoidal, or banded coatings. The microcrystalline variety of uraninite is called pitchblende. Because of the rapid oxidation, radioactive decay process, and numerous hydrous phases, uraninite can have many chemical variations.","A toxic mercury sulfide mineral. The primary ore of mercury, once used as a pigment.\nWhat is Cinnabar?\nCinnabar is a toxic mercury sulfide mineral with a chemical composition of HgS. It is the only important ore of mercury. It has a bright red color that has caused people to use it as a pigment and carve it into jewelry and ornaments for thousands of years in many parts of the world. Because it is toxic, its pigment and jewelry uses have almost been discontinued.\nGeologic Occurrence of Cinnabar\nCinnabar is a hydrothermal mineral that precipitates from ascending hot waters and vapors as they move through fractured rocks. It forms at shallow depths where temperatures are less than about 200 degrees Celsius. It usually forms in rocks surrounding geologically recent volcanic activity but can also form near hot springs and fumaroles.\nCinnabar precipitates as coatings on rock surfaces and as fracture fillings. Less often, cinnabar can be deposited in the pore spaces of sediments. It is usually massive in habit and is rarely found as well-formed crystals. Other sulfide minerals are generally found associated with cinnabar. These can include pyrite, marcasite, realgar, and stibnite. Gangue minerals associated with cinnabar include quartz, dolomite, calcite, and barite. Small droplets of liquid mercury are sometimes present on or near cinnabar.\nProperties of Cinnabar\nThe most striking property of cinnabar is its red color. Its bright color makes it easy to spot in the field and is a fascination for those who discover it. It has a Mohs hardness of 2 to 2.5 and is very easily ground into a very fine powder. It has a specific gravity of 8.1, which is extremely high for a nonmetallic mineral.\nThe luster of cinnabar ranges from dull to adamantine. Specimens with a dull luster are usually massive, contain abundant impurities and do not have the brilliant red color of pure cinnabar. Adamantine specimens are usually the rarely-found crystals.\nPhysical Properties of Cinnabar\n|Color||Bright red to brownish red, sometimes gray|\n|Luster||Adamantine to dull|\n|Diaphaneity||Transparent, translucent, or opaque|\n|Mohs Hardness||2 to 2.5|\n|Specific Gravity||8 to 8.2|\n|Diagnostic Properties||Specific gravity, color, streak, cleavage, association with volcanic activity.|\n|Chemical Composition||Mercury sulfide, HgS|\n|Uses||The only important ore of mercury. Its use as a pigment, gem, and ornamental carving material has declined due to toxicity.|\nMetacinnabar is a polymorph of cinnabar. It has the same chemical composition (HgS) as cinnabar but a different crystal structure. Cinnabar is trigonal, while metacinnabar is isometric. The two minerals should not be confused with one another because metacinnabar has a metallic gray color, a gray-to-black streak and a metallic-to-submetallic luster.\nUses of Cinnabar\nCinnabar is the only important ore of mercury. For thousands of years, cinnabar has been mined and heated in a furnace. The mercury escapes as a vapor that can be condensed into liquid mercury.\nPeople began using cinnabar for pigments thousands of years ago in Italy, Greece, Spain, China, Turkey, and the Mayan countries of South America. Through time, people in almost every country where volcanoes are present discovered cinnabar and realized its utility as a pigment. Cinnabar is one of a very small number of minerals that was independently discovered, processed and utilized by ancient people in many parts of the world.\nCinnabar was mined at the volcano, ground into a very fine powder and then mixed with liquids to produce many types of paint. The bright red pigments known as \"vermilion\" and \"Chinese red\" were originally made from cinnabar.\nCinnabar has been especially popular for making red lacquer in China. Its use in lacquer has declined because of its toxicity, but some use of cinnabar in lacquer continues. Cinnabar has also been used in powdered form for ritual blessings and burials. Powdered cinnabar was used as a cosmetic in many parts of the world for thousands of years. Eventually it was discovered that cinnabar is toxic, and its use in pigments, paints, and cosmetics began to decline.\nToday most, but not all, items made and sold under the name \"cinnabar\" have been made with less toxic and nontoxic imitation materials. Antique items made with toxic mineral cinnabar are still found in the marketplace.\nUses of Mercury\nBecause cinnabar is the only important ore of mercury, the demand for mercury has driven mining activity. Mercury has many uses, but its toxicity has reduced its use in any application where reasonable substitutes can be found. Large amounts of mercury are currently used in the chemical industry in the production of chlorine and caustic soda during the electrolysis of brine.\nMercury was widely used in temperature- and pressure-measuring instruments such as thermometers and barometers. It was often used in gravity switches because it flowed easily as a liquid and conducted electricity. Most of these uses have been discontinued.\nMercury is currently used in some batteries and light bulbs, but their disposal is often regulated. Because it is toxic, it was once widely used to protect seed corn from fungus and to deworm materials used to make felt. It was used in dental amalgam but is being replaced by polymer resins and other materials. In almost all of its use, mercury is being replaced with less toxic and nontoxic substitutes.\nMercury has been widely used in mining to separate gold and silver from ores and stream sediments. Large amounts of mercury were spilled during these operations, and today, mercury used in the 1800s is still being recovered from streams.\nContributor: Hobart King\n|Rock, Mineral and Fossil Collections.|\n|Gold Pans and Panning Kits|\nMore From Geology.com:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b9b46e7d-f827-4d47-94fb-b143c2860bda>","<urn:uuid:92284b5d-eb6a-4e11-9428-77be6a70ca3b>"],"error":null}
{"question":"How do therapeutic communities (TCs) as Level 4 recovery residences help individuals with criminal justice involvement?","answer":"Therapeutic communities as Level 4 recovery residences help individuals with criminal justice involvement through structured programs that show improved outcomes. Studies have found that TC clients make longitudinal improvements in substance use, arrests, illegal behaviors and employment. Research shows that clients referred from the criminal justice system stayed in treatment as long as voluntary clients and achieved similar improvements. One 5-year study of 715 prisoners found that those in TC programs had significantly lower rates of reincarceration compared to no-treatment groups. Additionally, individuals who attended community-based TCs or both prison and community TCs showed the best outcomes in terms of substance use and re-arrest rates.","context":["SUPPORT LEVELS I - IV\nBecome part of a movement that makes a difference. CCORR is a state-wide initiative providing standards and inspections of sober homes that covers safety, cleanliness and the effectiveness of house programming. It is there to help you choose the level of care that will make your time at the home successful when you are struggling to maintain your recovery. CCORR goes the extra mile to share the community resources for counseling, interventions, and the community groups for recovery. Through a voluntary process, Certified Residences provide CCORR unrestricted access to interview management, staff and residents to verify implementation of policies, procedures and protocols previously documented by the house. Certification to the CCORR Standard ensures, for all stakeholders, provider compliance with measures demonstrated to enhance the quality of recovery support in a community-based, residential setting.\nWe recommend careful consideration of CCORR support levels before commencing your selection process. One level is not better than another; rather it offers distinctly different support characteristics and intensity of service. The effort you invest to familiarize yourself with these options helps to ensure your selection of the support level that is most appropriate for your needs given the unique DNA of every home.\nThe role that service needs and problem severity play in admissions decisions varies widely within and across levels of recovery residences (See illustration below). There are also recovery residences designed specifically for individuals with certain needs (e.g., co-occurring addiction and severe mental illness, veterans, mothers with children); however, some recovery residences may not be equipped to adequately meet these residents’ needs. Individuals with specific service needs seeking RRs should ask the provider about how these needs can (or cannot) be addressed within a particular residence.\nCharacteristics of Level I residences coincide most closely with Oxford Houses, which have been studied extensively by Jason et al. over the past 20 years. It is a self-governing house without paid staff, but proven effective.\nSober living houses (SLHs) similar to those that are members of the Sober Living Network (SLN) in Southern California and some houses affiliated with the California Association of Addiction and Recovery Resources (CAARR) are good examples of Level II residences. Like Level I residences, studies on these types of facilities have been limited. One of the few studies on Level II residences was a recent study of houses in Northern California (Polcin, Korcha, Bond, & Galloway, 2010). Researchers recruited 245 individuals entering Clean and Sober Transitional Living in Sacramento County, which includes 16 recovery homes. The houses were located in a very high methamphetamine (MA) use area and 53% of the participants entered the houses with dependence on MA during the past year. Participants were interviewed within 2 weeks of entering the houses and then at 6-, 12- and 18-month follow-up. Primary outcomes\nA Primer on Recovery Residences – FAQs: NARR September 20, 2012 22\nincluded measures of alcohol and drug use and Addiction Severity Index (ASI) alcohol and drug scales. Secondary measures consisted of other ASI scales and a variety instruments assessing criminal justice involvement, employment, and psychiatric problems. Longitudinal analyses revealed two patterns for primary and some secondary outcomes over time. One pattern involved residents entering the SLHs with moderate to high severity of problems, making significant improvements by 6 months, and then maintaining those improvements at 12 and 18 months. Results from measures that assessed alcohol and drug use over a 6-month time period showed this pattern. For example, alcohol and drug abstinence over a 6-month time period increased from 20% at entry into the SLH to 40% at 6-month follow up. Abstinence improved even more at 12month follow-up (45%) and declined only a bit at 18 months (42%). The other outcome pattern showed residents entering the SLHs with low severity of problems at baseline and then maintaining low severity at 6-, 12-, and 18-month follow up. Findings from the ASI alcohol and drug scales were good examples of this pattern. The average score on the ASI alcohol severity at baseline was 0.16 (se=0.02), and for drug severity at baseline, the average was 0.08 (se=0.01). Because ASI values range from 0 to 1, these scores are very low. There was therefore limited room to improve on these measures. Nevertheless, there were significant improvements at 6 months for both alcohol (mean=0.10, se=0.02) and drug (mean=0.05, se=0.01) scales. Those improvements were maintained at 12 and 18 months. Alcohol severity remained at 0.10 at 12 and 18 months, and drug severity also remained essentially unchanged, 0.06 at 12 and 18 months. It should be noted that improvements were maintained at 12 and 18 months despite the fact that most residents had left the SLHs. By 18 months, about 90% of the residents had left, yet there was little regression of the earlier improvements. Thus, the improvements noted were therefore not simply a function of residents being housed in a controlled environment. The study also examined a variety factors that predicted outcome. These included demographic characteristics and factors related to the philosophy of recovery in SLHs, such as involvement in 12-step groups and developing a social network supportive of abstinence. Generalized Estimating Equations showed that involvement in 12-step groups was the strongest and most consistent predictor of good outcome. As expected, drinking and drug use in the participant’s social network predicted worse outcome. Overall, a wide variety of demographic groups made improvement in the SLHs and only a few demographic characteristics predicted outcome. The most notable exception was the relationship between age and abstinence. Older age categories were over twice as likely to be abstinent than those aged 18-28. Relative to residents who had not completed high school, those with at least a high school diploma were nearly twice as likely to be abstinent over the past 6 months and about half as likely to be arrested.\nA good example of level III residences are “social model” recovery programs, which emphasize experiential learning, peer support, and 12-step recovery principles within a semi-structured group living environment. These programs are more structured than level II residences and include paid counselors who assist residents with case management services and the development of a formal recovery or treatment plan.\nfrom: A Primer on Recovery Residences – FAQs: NARR September 20, 2012 23\nTypically, there are various life skills and other types of groups offered at the facility. In many states, such as California, they are licensed by the state to provide treatment services. As with Phase I and Phase II residences, the outcome studies on Phase III residences have been fairly limited. Studies that have been conducted include the California Drug and Alcohol Treatment Assessment (CALDATA; Gerstein et al., 1994) and studies on social model recovery programs conducted by Kaskutas et al. (2003-2004, 2008) at the Alcohol Research Group. The CALDATA study examined treatment outcomes among 1,858 clients in California who received methadone treatment, non-methadone outpatient, clinically oriented residential treatment (21 providers), or social model recovery programs (23 providers). The study consisted of follow-up interviews approximately 15 months after leaving treatment. Clinically oriented residential programs included procedures such as psychiatric assessments, individual counseling, and treatment groups (e.g., therapeutic communities). Social model recovery houses were oriented toward peer support, communal living, and practicing 12-step recovery principles. Borkman, Kaskutas, Room, Bryan, & Barrows (1998) compared the two types of residential programs and reported that residents in social model programs had longer stays and incurred lower costs. Costs per treatment episode in the social model programs averaged $2,712, while costs per treatment episode in the clinical residential program averaged $4,405. Overall length of stay was associated with better outcome. Comparison of residents in the two types of residential programs who had comparable lengths of stay showed slightly better outcomes for the clinically oriented programs. For example, residents who remained in treatment 4+ months in the clinically oriented program reduced the number of months they used substances by 63% while social model residents reduced the number of months of substance use by 52%. Reductions in reports of criminal activity were slightly higher in social model programs (80%) than clinically oriented programs (74%). Studies conducted by Kaskutas et al. (2003-2004, 2008) were stronger designs because they included longitudinal designs that compared measures collected at treatment entry with follow-up measures. The 2003-2004 study consisted of a naturalistic comparison of outcomes for individuals in social model residential programs (N=164) with those in clinically oriented programs N=558). The social model programs were detoxification and residential facilities, and the clinically oriented programs were a mix of inpatient and outpatient. Individuals in the social model programs were more involved in 12-step meetings and reported fewer alcohol and drug problems at one-year follow up, but not problems between the two study conditions in other areas (e.g., family, medical, legal, and psychiatric). The 2008 study randomly assigned clients to receive day hospital program treatment (n=154) or services in social model residential programs (n=139). Although significant improvements were noted at 12 months for both groups, between group comparisons did not reveal significant differences. Overall, clients tended to remain in the residential programs longer and costs were higher.\nRelative to other levels, Level IV residences include more structure, paid professional staff and on-site treatment services. Residential therapeutic communities (TCs) for drug treatment are a good example of Level four residences. Large proportions (A Primer on Recovery Residences – FAQs: NARR September 20, 2012 24) of residents in TCs are referred from the criminal justice system, and some are actually located in prisons. TCs have a long history of participating in research, including large national studies assessing drug treatment outcome. These studies include the Drug Abuse Treatment Outcome Study [DATOS] (Hubbard, Craddock, Flynn, Anderson, & Etheridge, 1997), National Treatment Improvement Evaluation Study [NTIES] (Center for Substance Abuse Treatment, 1996), Treatment Outcome Prospective Study [TOPS] (Hubbard et al., 1984), and Drug Abuse Reporting Problems [DARP] (Simpson & Friend, 1988). Overall, these and other studies on TCs (e.g., Martin, O’Commel, Paternoster, & Bachman, 2011) show that clients make longitudinal improvements on substance use measures, arrests, illegal behaviors and employment. When TCs have been compared to voluntary, control, or alternative treatment groups, the findings have been encouraging. For example, DeLeon (1988) found that clients referred from the criminal justice system stayed in treatment longer than voluntary clients and had levels of improvement that were similar. Prendergast, Hall, Wexler, Melnick, & Cao (2004) conducted a randomized trial of 715 prisoners randomly assigned to either a therapeutic community program or to a no treatment group. At 5-year followup, the TC group had significantly lower rates of reincarceration, but not shorter times to first reincarceration. As in most studies of TCs, longer lengths of treatment were associated with better outcome. Martin, Butzin, & Inciardi (1995) studied 457 individuals participating in either an in-prison TC, a TC in the community, both types of TCs, or a no treatment comparison group. Those attending the community-based TC or both types of TC had the best outcome (substance use and re-arrest). The in-prison TC had modestly better outcomes than the no treatment comparison group.\nSummary of Outcomes There is obviously a significant need for additional research on residential recovery homes, especially those characterized by levels 1-3. Studies on level 4 residences are more numerous because of the large number of studies examining outcomes within therapeutic communities. Overall, the available studies across the different levels are encouraging. Longitudinal studies of residents housed within each of the levels show improvements in a range of areas. When comparisons have been made between recovery residences and appropriate alternatives, the results have shown recovery homes yield comparable or better outcomes. Cost and cost-benefit analyses have been limited and to have yielded mixed findings.\nBorkman, T. J., Kaskutas, L. A., Room, J., Bryan, K., & Barrows, D. (1998). An historical and developmental analysis of social model programs. Journal of Substance Abuse Treatment, 15(1), 7-17. doi:10.1016/S0740-5472(97)00244-4 Center for Substance Abuse Treatment. (1996). National Treatment Improvement Evaluation Study (NTIES). Rockville, MD: U.S. Department of Health and Human Services"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:0c2b5982-ee37-4a44-a270-0370b6c8c0f3>"],"error":null}
{"question":"How do the dispute resolution processes compare between Regulation Z billing disputes and Fair Housing violations?","answer":"Regulation Z and Fair Housing violations have distinct dispute resolution processes. For Regulation Z billing disputes, consumers must report errors within 60 days, after which lenders must acknowledge the dispute within 30 billing days and resolve it within two billing cycles or 90 days, including correcting errors and refunding disputed amounts. For Fair Housing violations, victims can file complaints with government agencies, and the law protects individuals from retaliation when asserting their rights. The law also permits legal action, allowing victims to consult attorneys to settle matters directly with housing providers or through court proceedings. Additionally, Fair Housing violations can involve neighbor-on-neighbor harassment cases, where housing providers may be held liable if they fail to address discriminatory harassment after being notified.","context":["In this article:\nRegulation Z is a federal law that standardizes how lenders convey the cost of borrowing to consumers. It also restricts certain lending practices and protects consumers from misleading lending practices.\nThe regulation is designed to make sure borrowers are able to view all the details they need to before entering into a lending agreement. This includes the requirement that lenders clearly disclose and define important terms, rates and fees so the borrower can make a more informed decision.\nHow Regulation Z Works\nRegulation Z is part of the Truth in Lending Act of 1968 and applies to home mortgages, home equity lines of credit, reverse mortgages, credit cards, installment loans and certain student loans.\nUnder the regulation, lenders are required to provide borrowers with access to interest rates, fees and finance charges in writing. Other aspects of the law include:\n- Lenders must provide monthly billing statements to borrowers.\n- Creditors must notify borrowers when there's a change in the interest rate on a variable-rate loan.\n- Consumers will receive fair and timely responses to billing disputes.\n- Mortgage lenders are prohibited from using unfair practices that give rise to a conflict of interest between the lender and a mortgage broker.\nHow Regulation Z Protects You With Mortgages\nThe primary way the regulation protects consumers during the mortgage process is by eliminating a conflict of interest for mortgage brokers.\nMore specifically, mortgage lenders aren't allowed to change a broker's fee based on the terms of the loan—which means brokers can't increase their commission check by pushing homebuyers to borrow more money or take on a loan with unfavorable terms.\nAs a result, borrowers can work with a broker they know won't get a kickback and will work with the homebuyer's best interests in mind.\nRegulation Z also requires mortgage lenders to provide borrowers with a written disclosure of rates, fees and other finance charges. Plus, if you have an adjustable-rate mortgage, they're required to let you know in advance if your rate will be changing.\nHow Regulation Z Protections You With Credit Cards\nSince the enactment of the Credit CARD Act of 2009, Regulation Z has provided expanded protections and rights for credit card holders including:\n- Liability for unauthorized use: Your maximum liability for credit card fraud is just $50, and a lender must meet certain requirements before it can hold you liable for any amount of an unauthorized charge.\n- Promotional rates: If a credit card offers a promotional interest rate, the card issuer must state when the promotional period will expire, what the ongoing APR will be after the promotion ends and whether a promotional fee applies and how much. It also requires lenders to note that a deferred interest promotion may incur interest retroactively if you don't pay the balance in full by the end of the promotional period.\n- Marketing to college students: Regulation Z limits how credit card issuers can market their products to college students. For example, they can't offer goods—such as a gift card or T-shirt—as an incentive, and cannot advertise to students within 1,000 feet of college campuses.\n- Disclosures: When you open an account, credit card issuers must provide clear information about a card's interest rate, fees and other finance charges and make certain additional disclosures. You're also entitled to a notification when your card issuer changes your variable interest rate.\n- Penalty fees: If you miss payments on your account, some card issuers may choose to assess a penalty fee. According to Regulation Z, this penalty must be reasonable and proportionate to the violation, and there are hard limits on such fees.\nCredit cards and other types of open-ended credit, including home equity lines of credit, are also covered by a billing dispute process. If you provide information about a billing error within the past 60 days, the lender must send written acknowledgment of the dispute within 30 billing days.\nIf the creditor confirms the billing error—which must happen within two billing cycles and no more than 90 days later—it must correct the error, refund the disputed amount, update fees and other charges associated with the error, and provide the customer with a correction notice.\nHow Regulation Z Protects You With Other Loans\nRegulation Z also applies to installment loans, including but not limited to personal loans, auto loans and short-term installment loans. With student loans, however, it applies to private student loans.\nAcross all types of installment loans, you'll receive all the basic protections other borrowers receive. That includes the right to a monthly billing statement, access to fair and timely responses to billing disputes and clear details about a loan's interest rate and fees.\nWhat to Do if Your Regulation Z Rights Are Violated?\nIf you believe your bank, credit card issuer or loan provider isn't following the rules of Regulation Z, and that's resulted in your rights being violated, start by calling their customer service line and requesting to speak with a supervisor or manager about the issue. The violation may have been a result of a mistake or a misunderstanding.\nIf the lender refuses to make the situation right, you can file a complaint with the Consumer Financial Protection Bureau, which has rule-making authority for the Truth in Lending Act. You can also submit a complaint to the Federal Trade Commission.\nAs a last resort, you may also consult an attorney, who can help you settle the matter directly with the creditor or in a court of law.\nMake Your Credit a Top Priority\nRegulation Z provides some excellent protections for consumers, but it's still your responsibility to read the fine print for every credit card or loan you apply for.\nAlso, keep in mind that billing disputes are valid only if you report them within 60 days of the lender sending the statement that reflects the error. As such, it's important to stay on top of your billing statements and review transactions to make sure everything is accurate.\nFinally, take the time to keep track of your credit score. With Experian's credit monitoring service, you'll get free access to your FICO® Score☉ plus updates when new inquiries and credit accounts are added to your Experian credit file.\nMonitoring your credit and developing good credit habits can help you improve your chances of qualifying for credit with favorable terms.","WHAT IS FAIR HOUSING?\nIn April of 1968, just a week after the assassination of Martin Luther King, Jr., Congress passed the federal Fair Housing Act (FHA), which made it illegal to discriminate against people in the sale or rental of housing on the basis of race, color, religion, sex and national origin. In 1988, the Fair Housing Amendments Act became law, adding protections based on disability and familial status.\nSince 1963, California has also prohibited certain types of discrimination in housing. Today, these laws are known as the Fair Employment and Housing Act (FEHA) and the Unruh Civil Rights Act. In addition to those characteristics listed under federal law, California bans housing discrimination based on marital status, sexual orientation, gender identity, ancestry, age, source of income and arbitrary characteristics.\nThe important thing to note about federal and state Fair Housing laws is that they only prohibit discrimination based on a protected characteristic. A bank can deny you a mortgage because you have poor credit, or a housing provider can refuse to rent to you because you are low-income, or have an eviction on your record. These things may not be fair, but they aren’t illegal. To violate the law, a housing provider must treat you differently because of a protected characteristic. For example, it would be a violation of the law if you were denied a mortgage because of race, or denied a rental unit because you have children. There must be a connection between the negative treatment and one of the listed characteristics.\nWho Does the Law Apply To?\nThe FHA covers most housing, with a few exceptions, such as owner-occupied buildings with four or less units and some single-family homes. California law, however, covers almost every type of housing and housing provider imaginable. Under FEHA, there are only four circumstances in which the law does not apply:\n- Owner-occupied units wherein the owner is leasing to only one boarder\n- Religious organizations providing certain types of housing may limit occupancy to people of the same religion\n- Private clubs providing certain types of housing may also give preference to members\n- An individual may state a preference for a roommate of a specific sex if sharing living spaces\nGenerally speaking, if a person or entity has control over your housing or any housing-related services, it’s likely that the Fair Housing laws apply. Housing discrimination laws have been applied to property owners, managers, maintenance staff, real estate brokers and agents, homeowners associations, mobile home parks, local housing authorities and governments. The courts have also interpreted the laws to apply to some hotels, motels, vacation rentals and homeless shelters.\nThe law also covers neighbor-on-neighbor harassment based on a protected characteristic. Homeowners or tenants who engage in a campaign of harassment against their neighbors on the basis of race, for example, may be held liable under the FHA. A housing provider may also be held liable for the action of its tenants if it fails to address this type of harassment once notified.\nWhat Does the Law Prohibit?\nIt is illegal to do any of the following because of someone’s race, color, religion, sex, national origin, disability, familial status, marital status, sexual orientation, gender identity, ancestry, age, source of income, or because of an arbitrary characteristic:\n- Refuse to rent or sell housing\n- Refuse to negotiate for housing\n- Make housing unavailable\n- Apply different terms, conditions, or privileges with respect to the sale or rental of housing, either before or after an individual purchases or rents a dwelling\n- Provide different housing services or facilities\n- Falsely deny that housing is available for inspection, sale, or rental\n- Deny anyone access to or membership in a facility or service (such as a listing service) related to the sale or rental of housing\n- Refuse to grant a reasonable accommodation or modification so that a person with disabilities has equal access to the housing of his/her choice\n- In mortgage lending:\n- Refuse to make a mortgage loan\n- Refuse to provide information regarding loans\n- Impose different terms or conditions on a loan, such as different interest rates, points, or fees\n- Discriminate in appraising property\n- Refuse to purchase a loan\n- Set different terms or conditions for purchasing a loan\n- Threaten, coerce, intimidate, or interfere with anyone exercising a fair housing right or assisting others who exercise that right\n- Advertise or make any verbal or written statement that indicates a limitation or preference on the basis of a protected characteristic (This prohibition against discriminatory advertising applies to all housing, even if exempted from other portions of the FHA and FEHA)\nIt is also illegal to implement a neutral policy that has the effect of discriminating on the basis of a protected characteristic, or perpetuating segregation based on a protected characteristic. This is called disparate impact discrimination.\nIndividuals who assert their rights under the Fair Housing laws, or assist someone in asserting their rights, are protected from retaliation.\nThough these prohibited actions may seem straightforward, they are anything but. Did you know that sex discrimination includes sexual harassment? Or that familial status discrimination includes banning children from playing outside? To learn about the specific types of behaviors the Fair Housing laws prohibit, click on each of the protected characteristics in the section below.\nThe protected characteristics as defined in the federal and state fair housing laws are listed below. Each page includes information and resources about the types of complaints and prohibited behaviors associated with each characteristic.Federally Protected Characteristics"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:96132eef-416d-40ce-93d8-e19cb92b3ddc>","<urn:uuid:ce93aa69-ba22-411c-8825-705ab24f4b8b>"],"error":null}
{"question":"Why is forest certification becoming increasingly important for landowners in the timber market?","answer":"Forest certification is becoming imperative as sustainability certification becomes more prevalent globally for products derived from forest raw materials. European wood pellet customers actively ensure certification of their supplies, and large consumer products companies pursue similar arrangements for their packaging needs. This certification helps landowners ensure the viability of their timber in a dynamic market.","context":["A new collaboration between members of the forest products industry and beyond aims to engage North and South Carolina land owners in responsible forestry practices and certification regimes. American Forest Foundation (AFF), International Paper, The Procter & Gamble Company (P&G) and 3M Company recently announced the formation of the Carolinas Working Forest Conservation Collaborative, which is a joint initiative in the Coastal Carolinas Plain to educate and engage family woodland owners in sustainable forestry, forest certification, the enhancement of habitat for at-risk species and the conservation of bottomland hardwood forests.International Paper, P&G and 3M are together providing a total of $285,000 over a three-year period to AFF in support of their work to implement sustained and strategic landowner engagement efforts in North Carolina (Columbus, Bladen, Robeson, Cumberland, Pender, Johnston counties) and South Carolina (Horry, Florence, and surrounding counties).\nThe Carolinas Working Forests Conservation Collaborative aims to:\n- Increase awareness and understanding of the importance of sustainable forestry and active management among 30,000 woodland owners, who collectively own 2.4 million acres of forestland in the Carolina Coastal Plains region.\n- Work with at least 450 landowners across 36,500 acres in the project counties to connect them with technical assistance and resources to get them started in forest management.\n- Engage at least 160 woodland owners within the project area to enhance, restore or expand bottomland hardwood forests and quality habitat for at-risk species on at least 13,000 acres.\n- Certify at least 120 landowners in a recognized forest certification standard.\n“P&G is committed to sourcing 100% of our raw paper materials from responsibly managed forests,” says Tonia Elrod, director of sustainability & product stewardship for P&G Family Care brands. “To help us meet this goal, it is important for us to invest in partnerships with likeminded companies to increase responsible forestry practices and landowner certification while allowing us to still deliver the same superior product experience our consumers expect.”\n“At 3M, we drive both business growth and positive societal impact through purposeful partnerships,\" says Jean Bennington Sweeney, 3M’s Chief Sustainability Officer. “We depend on committed landowners to provide sustainable, responsibly harvested wood fiber to our suppliers for products and packaging used in our brands like Post-it® Notes, Scotch® Tapes, FiltreteTM Filters, Scotch-Brite® Scrubbers and others. The sustainable practices this project enables will provide lasting benefits to landowners, forests, and our business.”\nFamilies and individuals own more than one-third of the forests across the US, which is more than corporations or the government. In the US South, 87 percent of family forest owners cite wildlife as a top reason they own land, yet most lack the technical skills and resources to manage it. Across the forest products supply chain, more than 50 percent of the wood supply comes from family-owned land. As sustainability certification becomes more prevalent on a global basis—covering a broad swath of products derived from forest raw materials—it is imperative that landowners gain the knowledge and skills to ensure the viability of the timber on their land in such a dynamic market. Many European wood pellet customers actively ensure certification of their supplies, and large consumer products companies (such as P&G and 3M) pursue similar arrangements due to their extensive packaging needs.\n“There is a misconception that you cannot have both paper products and habitat from the same forest, however we have found that wood and wildlife can go hand-in-hand when family woodland owners are sustainably managing, said Paul DeLong, Vice President of Conservation for AFF. “International Paper, P&G and 3M have all shown a great commitment to responsible forestry, making them strong partners in our efforts to increase the number of landowners actively managing their land for wildlife, while providing sustainably produced wood supplies.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:eed8cd7f-7ac7-435f-afc7-34e14405b917>"],"error":null}
{"question":"What are the technical requirements for Facebook business images, and how can their effectiveness be measured through website analytics?","answer":"Facebook business images require specific dimensions: profile pictures should be 170x170px for desktop and 128x128px for mobile in JPEG or PNG format, cover photos need to be 820x312px (preferably RGB JPG under 100KB), shared images should be 1200x630px, and carousel ads should be 1080x1080px. To measure their effectiveness, businesses can use Google Analytics by setting up specific goals like quote submissions or downloads, assigning monetary values to these goals, and then analyzing social media conversion data over 30-90 days. The Goal Flow feature shows how visitors from Facebook navigate to conversion points, while Multi-Channel Funnels provide a complete view of how social media contributes to revenue generation.","context":["The ultimate guide to Social Media sizes\nA picture tells a thousand words; to maximise impact on every platform and for every device, it’s essential to nail the perfect visual.\nEvery post needs an explosive image to captivate viewers and boost engagement; we’ll cover the size guide, you get the creative juices flowing.\nWe’ve created the ultimate guide to Social Media images sizes to equip you with a fast and simple reference to ensure you won’t disappoint your audience:\nTwitter is an essential tool, for promotion, insight and brand loyalty and is the #1 platform for discovery. Hashtags build momentum and encourage your posts to go viral.\nIf you’re a brand, your profile image should be a clear pic of your company’s logo:\n- Profile picture size: 400px x 400px (JPG, PNG or GIF)\nThe Header photo is pretty self-explanatory, it’s the image which sits at the top of your profile:\n- Header: 1500px x 500px (JPG, PNG)\nEvery tweet should include an image. Tweets with an image are 34% more likely to get a retweet:\n- Shared image sizes: 600px x 335px (JPG, PNG or GIF)\n- Shared link graphic sizes: 800px x 418px (1:91:1 aspect ratio) (JPG, PNG or GIF)\nCards should have rich, eye-catching imagery:\n- Card images: 144px x 144px (JPG, PNG, WEBP and GIF)\nFacebook boasts over 1.6 billion users and is able to target an exact demographic – it’s a powerful marketing tool for businesses of all sizes; no wonder it’s the leading social network platform.\nYour profile image should be a clear on-brand photo or logo:\n- Profile picture size: 170px x 170px (desktop) and 128px x 128px (smartphones) (JPEG, PNG)\nUse a little imagination when adding a cover photo. It’s a great way of humanising your brand with a picture of your team. Alternatively, reiterate your brand image with some relevant creative:\n- Cover photo size: 820px x 312px (An RGB JPG file less than 100KB gives best results and images with logo and/or text are best as a PNG file)\nThe more engagement you get with a shared image, the more likely it is to have a greater reach:\n- Shared image sizes: 1200px x 630px\nThere are two options with image links. They can either feature as a square with text beside it or alternatively as a large rectangle with text below. Image links allow more info to be shared, this includes a headline, display link and meta-description:\n- Shared link graphic sizes: 1200px x 630px\nCarousel ads should feature between 3-5 visually captivating images:\n- Feed carousel: 1080px x 1080 px\nWith 25 million business profiles on Instagram and 200 million users visiting at least one business profile every day, it’s vital to put a face to your brand.\n- Profile picture size: 110px x 110px (min.)\n- Shared image sizes and videos: 1080px x 1080px\n- Shared link graphic sizes – photo thumbnails: 110px x 110px\n- Instagram Stories: 1080px x 1920px\nWith 4 billion Pinterest boards, it’s no surprise it’s the social network where people can find inspiration and ideas for their interests and hobbies. It’s great for driving referral traffic and 90% of Pinterest pages are external links. There’s a higher chance of getting ‘referred’ if your image sizes are up to scratch:\n- Profile picture size: 165px x 165px\n- Board cover image: 222px x 150px\n- Shared image size: Expanded Pin: 236px x scaled height\n- Shared link graphic sizes – 735px x 1102px (pin preview); 600px x 600px (board display)\nLinkedIn; the ‘social networking site for business’, is one of the fastest-growing social platforms. On average, two people create an account on this platform every second. The platform sees 172,800 new users every day and about 62 million new users every year.\n- Profile picture size: 400px x 400px, 300px x 300px (company logo), 60px x 60px (square logo)\n- Background/cover image: 1584px x 396px\n- Business cover image: 1536px x 768px\n- Banner image for company pages: 646px x 220px\n- Hero Image: 1128px x 376px\n- Shared Image Sizes: LinkedIn personal profile / company page: 1104x x 736px\n- Shared link graphic sizes: 1104px x 736px\nWith 97% of marketers claiming that video helps customers understand the product and increase audience engagement, it’s no surprise that YouTube is an essential business tool.\n- Profile picture size: 800px x 800px\n- YouTube cover photo: Display varies by device; 2560px x 1440px (desktop), 1546px x 423px (safe area for mobile and web, without text and logo cropping\n- Shared image sizes: 16:9 aspect ratio (video); to qualify for HD: 3840px x 2160px\n- Shared link graphic sizes: Custom video thumbnail: 1280px x 720px\nThe power of infographic: Tumblr has been ranked #1 in social sentiment towards brands. Without sentiment, data can be misleading. It is an essential tool to measure content and brand success and to narrow down your target audience.\n- Profile picture size: 128px x 128px\n- Header: 3000px x 1055px\n- Shared Image Sizes: 500px x 750px\nMedium is a foundation for free-access high-quality content and enables you to reach beyond the existing brand audience and syndicates your popular content.\n- 60px x 60px (publication avatar), 1000px x 500px (max.) (logo image)\n- Background image: 1500px x 750px\n- Shared image sizes: 1400px wide (full column-width), 2040px wide (out-set), 500px wide (screen-width)\n- Shared link graphic sizes: Will pull story header image or profile image\nTikTok has grown exponentially in the short time it has been around by focusing on user-generated and authentic content which is on-trend, by building communities and driving engagement.\n- Profile picture size: 200px x 200px\n- Header image: TikTok 900px x 300px\n- Shared image sizes: 1:1 or 4:5 aspect ratio (video); 1080px x 1920px","You don’t sell a product directly on your website. There is no shopping cart. How do you know if your website is actually helping people buy your product or service? Now let’s complicate it further by asking what role social media played in bringing buyers to your website?\nThese are complicated questions and you would think you need a complex solution to answer them. Do you need to purchase special 3rd party software? Would it be better to pay a developer to write a custom program for your unique business situation?\nThere is a simple solution to this complex problem.\nDid we mention it is free?\nIt is Google Analytics.\nIt is likely that you already have Google Analytics on your website. If not, it is free and is simple to install. You do not need any special coding skills. Follow these simple steps to install Google Analytics. Once Google Analytics is installed you are ready to measure the impact of social media in the buying process.\nHow To Calculate The ROI From Social Media\nSet a Goal in Google Analytics\nChoosing a goal in Google Analytics is simple. Choose events that lead to revenue generation. It can be as simple as Quotes Submitted, particular Downloads or number of Downloads, Specific Time On Page or Watching a Video.\nClick here to see what Google says about setting goals.\nHere are some examples of goals that lead to revenue generation.\nIt takes an understanding of your business processes to select the correct goals. When selecting goals it is best practice to involve all those involved in the sales process so proper attribution can be assigned.\nProTip- Create a Thank You page for all completed activities that are related to revenue generation. Make the Thank You page the goal.\nAssign A Dollar Value To The Goal\nYour gut reaction might be to assign Total Sale Value as the monetary value of the goal. The problem is you have not accounted for all costs. We recommend that you download our ebook Marketing Metrics Your Boss Really Cares About for the methodology of calculating ROI on marketing. Consider using some percentage of the profit after accounting for all costs.\nYou are almost there. Here are the next steps:\n1. Collect Data over a Specified Time Range — 30 days is OK, but 90 days is better.\n2. Reevaluate your goals to make sure you did not miss any “key” revenue generating activity.\n3. Determine the monetary value from website generated revenue based on goal completions.\n4. Assign Monetary value in Google Analytics.\nMeasure the impact from Social Media\nIn Google Analytics, click Conversions.\nYou can now drill down to see what social media channels were involved on goal conversion.\nIn Overview, you can simply click Source/Medium and you will see which social media platforms contributed to conversions.\nYou can also click Goal Flow and see how visitors entered from social media and what path they took to arrive at the goal.\nNow take a look at Multi-Channel Funnels.\nThis provides a holistic overview of how they reached the goal.\nYou can then look at Assisted Conversions and see the monetary impact of social media .\nYou can now calculate the ROI from social media. This is also a gut check. Google Analytics has no emotions so you are going to see what is really going on with your marketing. The winners and losers. If your social media is not having the revenue generation impact you had hoped, maybe we should talk."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:8d155402-cbb7-499a-bf2f-76e4356a5ec7>","<urn:uuid:47a9229a-e007-4e76-b602-17e03cef4dcf>"],"error":null}
{"question":"Can you provide a detailed comparison of the compositional techniques used in Babbitt's All Set and Sacchini's Œdipe a Colone, focusing on their respective approaches to musical structure and dramatic expression?","answer":"In All Set, Babbitt employs a highly structured 12-tone system combined with time point sets, where he applies serial technique to attack points corresponding to the series. He creates contrast by applying operations to different groups of instruments while maintaining unified composition. The work features extremely complex rhythms and angular melodic lines that require great concentration from performers. In contrast, Sacchini's Œdipe employs a more flexible compositional style that moves seamlessly between recitative and aria forms. His approach emphasizes dramatic integrity and power in the accompanied recitatives, with the orchestra playing an active role rather than just accompanying. While Sacchini's work includes some decorative elements like choruses and dances, it maintains dramatic cohesion through expressive writing that creates unity and flows naturally between different sections.","context":["All Set (1957)\nComposed in 1957 for the Brandeis University Arts Festival, which in that year was a jazz festival, All Set is scored for a small jazz ensemble consisting of alto and tenor sax, trumpet and trombone, bass, vibes, piano and drums. While written in the jazz idiom, the work utilizes an all-combinatorial 12-tone row as its material. Characteristic of the \"Chicago style\", solo and ensemble juxtapositions recall \"certain characteristics of group improvisation (Barkin), while the sections correspond to serial technique. While the available literature concerning the work is quite limited, Milton Babbitt has this to say on his work:\n\"Whether All Set is really jazz I leave to the judgment of those who are concerned to determine what things really are, and if such probably superficial aspects of the works as its very instrumentation, its use of the 'rhythm section,' the instrumentally delineated sections which may appear analogous to successive instrumental 'choruses,' and even specific thematic or motivic materials, may justify that aspect of the title which suggests the spirit of a 'jazz instrumental,' then the surface and the deeper structure of the pitch, temporal, and other dimensions of the work surely reflect those senses of the title, the letter of which brings the work closer to other of my compositions, which really are not jazz.\"\nAlthough remaining faithful to his 12-tone system of composition, Babbitt's effective use of jazz inflection proves that 12-tone music not only can be extremely flexible, but also can indeed be fun! Salzman (1988) remarks that while Babbitt remains \"... faithful to a vision of total rationality and control...\" the work itself relates \"... to [the] character of the live performance, situation and virtuosity of the performers.\" In this sense, the virtuosity reflects the extremely complex rhythms of the instrumental parts, often polyphonic, which are sounded against a more regular pulse of the drums. Not only are these instrumental parts rhythmically complex, but the melodic lines are extremely angular, requiring a great deal of concentration and control on the part of the performers.\nThe work is experimental, in that it is the first one in which Babbitt used the idea of ‘time point sets.' Glen Watkins (1988) asserts that Babbitt was \"... dissatisfied with the incompatibilities of serial procedures used for pitch and rhythm...\" which resulted in Babbitt's creation of a system that could be applied in a more flexible way. Watkins: \"Here the obvious need for a clear and audible metric organization is acute if such an organization is to have any meaning for the listener.\" This statement obviously refers to Babbitt's famous and much misunderstood article \"Who Cares if You Listen?\" in which Babbitt places the responsibility of understanding and recognizing serial procedures upon performers and listeners.\nThe problem, as approached encountered by European serialists such as Pierre Boulez and Karlheinz Stockhausen, was a matter of whether the serial technique was perceptible when applied to a rhythmic system. Boulez and Stockhausen, among others, attempted to solve this by applying serial technique to duration. However, when Babbitt understood that the perception of duration was a subjective phenomenon, he successfully solved the issue by applying serial technique to a system of ‘attack points’ corresponding to the series. Simply put, the human brain understands the beginning of an event, but has trouble perceiving ‘how long’ the event has lasted without an external reference of measurement. Therefore, a variety of durational values could be assigned to notes in the series, creating a fluid and flexible system while at the same time adhering to a ‘strict’ rhythmic system that is perceptible to the listener.\nWatkins describes the concept of time point sets as one in which \"... various note-values are identified by their position at the point of attack within the bar\". However, Charles Wuorinen is much more specific in elaborating on Babbitt's concept. Wuornin (1979) defines a time point as \"... simply a location in the flow of time.\" In describing the time point system, he informs us that the concept is based upon two principles: \"1) The relationships of the pitch system are transferred in their totality to the sphere of time relations; 2) This transfer is accomplished through the linkage of one simple equivalence - that of time interval corresponding to pitch interval.\" In this usage both time and pitch continuums are applied to modules which correspond to respective intervals, thus arriving at a flexible system in which time and pitch intervals can be varied from work to work. Wuorinen: \"... twelve interval divisions of the time modules will therefore make up twelve time-point classes...\" In this system, the time points may be identified by locating their points of attack, which have nothing to do with individual event duration. While in All Set Babbitt applies this use to the traditional 12-note series, this system is flexible in that it can be used in series containing other than 12 notes. Additional flexibility can be obtained by varying the lengths of time interval divisions.\nWhile Babbitt obtains contrast by applying operations to different groups of instruments, the overall sonority and way in which he applies the time-point set theory in All Set creates an extremely unified composition.\nBabbitt, Milton. Spectrum: New American Music, Volume V. Record H-71303, Nonesuch Records, 1974.\nBarkin, Elaine. \"Milton Babbitt,\" The New Grove Dictionary of Music and Musicians, ed. by Stanley Sadie. London: Macmillan Publishers Limited, 1980.\nSalzman, Eric. Twentieth-Century Music: An Introduction, 3rd edition. Englewood Cliffs: Prentice Hall, 1988.\nWatkins, Glenn. Soundings: Music in the Twentieth Century. New York: Schirmer, 1988.\nWuorinen, Charles. \"Rhythmic Organization: The Time Point System.\" Chap. in Simple Composition. New York: Longman, 1979.","Notes and Editorial Reviews\nAntonio Sacchini is one of those highly talented musicians who hover at the periphery of music history but were greatly successful in their lifetime. In the case of Sacchini an echo of his fame can be heard through the present work which has some claims to be his masterpiece. It was performed regularly at the Paris Opéra between 1787 and 1830, which is remarkable indeed and then was revived in 1843.\nHe was born in Florence but was taken to Naples at the age of four where he was admitted to the Conservatorio when he was ten. His teacher was Francesco Durante, who is probably more well-known today. He obviously moved about within Italy and gained recognition both as opera composer and singing teacher. One of his pupils\nwas Nancy Storace, who among other things was Mozart’s first Susanna in Le nozze di Figaro - “The Julie Andrews of the 18th Century” as one source nicely puts it.\nHe then went to Stuttgart and Munich and came to London in 1772 where he remained for ten years. At first successful, he later ran into financial trouble and moved to Paris in 1781. There he became a favourite with the Queen but met opposition from parts of the musical establishment. His opera Dardanus was staged at Fontainebleau in 1785 but to his grief Œdipe lay unperformed during his lifetime. The disappointment is said to have contributed to his death. In 1787 Œdipe reached the Opéra; too late for the composer.\nListening to this recording it is easy to understand the longevity of the work. It is a highly accomplished piece of music drama, pointing forward beyond Gluck, who is the closest contemporary comparison. In fact there is a Gluckian nobility in the more reflective moments. Sacchini also has a dramatic integrity and power in the long and often intense accompanied recitatives. At his best, as in the long scene with Œdipe and Antigone in act two (CD1 tr. 14-16), he tends to overshadow even Mozart for dramatic acuity, though he can’t compete with the Salzburg master when it comes to musical invention and melodic memorability. Still he writes expressive and grateful music, as for example the singing part for Polynice in the first scene (CD1 tr. 3) and at the beginning of scene 4 (CD1 tr. 10). Antigone’s aria in act three (CD2 tr. 2), is heroic and tragic to match the text. This is a fairly long aria; mostly they are very short but his flexible style allows him to move more or less imperceptibly from recitative to aria with the orchestra a very active part, not just accompanying. In this respect he might almost be likened to late period Verdi. The writing creates a feeling of unity and cohesion, underlined here by Ryan Brown’s eager conducting. Just as in his recording of Gluck’s Orphée et Euridice (see review) he opts for swift tempos and had at least this reviewer sitting on the edge of his chair. There is such vitality and thrust in his reading that the work stands out as perhaps better than it actually is, but for my money this is an opera to set beside Gluck, Haydn and Mozart as a superb example of late 18th century music theatre. Readers should be warned though that, this being a French opera, there are some decorative elements, like scene 3 of the first act with choruses and dances. The whole opera ends in a kind of anti-climax with an eight-minute ballet sequence. All of this is superbly performed; good music but more or less superfluous.\nThe Opera Lafayette perform with enthusiasm and flair and Brown and producer Max Wilcox have gathered a fine line-up of soloists. Some of the smaller parts are taken by members of the chorus and among the main characters the experienced François Loup is a deeply involved Œdipe, expressive and with a rich pallet of vocal colours. His daughter Antigone is the dramatically vibrant Nathalie Paulin who is also able to express the nobility of her character. The two tenors, Tony Boutté and Robert Getchell, are excellent; especially the latter who is a model of lyric tenor singing of music from this period. He should be a likewise excellent Don Ottavio or Tamino.\nThe booklet gives, in the usual Naxos manner, all the information one could possibly expect within the space available and besides a good track-related synopsis we also get the French libretto. The English translation can be downloaded.\nThis is one of the more thrilling “finds” within the operatic genre.\n-- Göran Forsling, MusicWeb International\nWorks on This Recording\nOedipe a Colone by Antonio Sacchini\nNathalie Paulin (Soprano),\nPhilip Cave (Tenor),\nRobert Getchell (Countertenor),\nAnthony Boutté (Tenor),\nKirsten Blaise (Soprano),\nKara Morgan (Soprano),\nJonathan Kimple (Bass),\nFrançois Loup (Baritone),\nJason Kaminski (Baritone)\nOpera Lafayette Orchestra,\nOpera Lafayette Chorus\nWritten: 1785; France\nVenue: Smith Performing Arts Center, University\nLength: 112 Minutes 47 Secs.\nNotes: Smith Performing Arts Center, University of Maryland (05/13/2005 - 05/15/2005)\nOedipe a Colone: Act I: Overture\nOedipe a Colone: Act I Scene 1: Recitative: En vain un frere ingrat - Aria: Ma fille est le precieux gage (Thesee)\nOedipe a Colone: Act I Scene 1: Recitative: Ah! le trone ou j'aspire - Aria: Le fils des dieux (Polynice)\nOedipe a Colone: Act I Scene 2: Recitative: Habitants de Colone (Thesee) - Chorus: Nous braverons pour lui (Soldiers) - Recitative: Vous avez... (Herault)\nOedipe a Colone: Act I Scene 3: Chorus: Allez regner (Women's Choir)\nOedipe a Colone: Act I Scene 3: Dance: Andantino - Allegro\nOedipe a Colone: Act I Scene 3: Aria: Vous quittez notre aimable Athenes (An Athenian)\nOedipe a Colone: Act I Scene 3: Dance: Gavotte\nOedipe a Colone: Act I Scene 3: Aria: Je ne vous quitte point (Eriphile)\nOedipe a Colone: Act I Scene 4: Recitative: Allons au temple (Thesee, Polynice, Eriphile) - Aria: Votre cour devint mon asile (Polynice)\nOedipe a Colone: Act I Scene 4: Recitative: Cher prince, calmez vous - Trio: Implorons les bienfaits (Thesee, Eriphile, Polynice)\nOedipe a Colone: Act I Scene 5: Chorus: O vous que l'innocence meme - Recitative: Divinites d'Athenes protectrices... (Chorus, High Priest, Polynice, Eriphile)\nOedipe a Colone: Act II Scene 1: Recitative: Ou vais-je, malheureux - Aria: Helas, d'une si pure flamme (Polynice)\nOedipe a Colone: Act II Scene 2: Recitative: Ah, n'avancons pas d'avantage - Duet: Ma fille, helas (Oedipe, Antigone) - Aria: Tout mon bonheur... (Antigone)\nOedipe a Colone: Act II Scene 2: Recitative: Ta consolante voix - Duet: Filles du Styx (Oedipe, Antigone)\nOedipe a Colone: Act II Scene 3: Chorus: Quel mortel temeraire - Recitative: Audacieux vieillard (A Coryphee, Antigone, Oedipe) - Chorus: Oedipe est l'ennemi\nOedipe a Colone: Act II Scene 4: Recitative: Barbares! Arretez! (Thesee, Antigone, Chorus) - Aria: Du malheur auguste victime (Thesee)\nOedipe a Colone: Act III Scene 1: Recitative: Oedipe et le Roi sont ensemble - Duet: Vous le savez, grands dieux! (Polynice, Antigone)\nOedipe a Colone: Act III Scene 1: Recitative: Appesanti par l'age - Aria: Dieux! Ce n'est par pour moi (Antigone)\nOedipe a Colone: Act III Scene 1: Recitative: Dieux! Que tant de vertu - Duet: En ma faveur (Polynice, Antigone) - Recitative: On vient (Polynice)\nOedipe a Colone: Act III Scene 2: Recitative: Auguste malheureux (Thesee)\nOedipe a Colone: Act III Scene 3: Recitative: Ma fille, que veut-il (Oedipe, Antigone, Polynice) - Aria: Daignez rendre (Polynice)\nOedipe a Colone: Act III Scene 3: Recitative: O dieux! Toi, scelerat! (Polynice, Oedipe, Antigone) - Aria: Delivrez-vous (Polynice)\nOedipe a Colone: Act III Scene 3: Aria: Ou suis-je? (Oedipe, Polynice, Antigone)\nOedipe a Colone: Act III Scene 3: Trio: O doux moment (Oedipe, Polynice, Antigone)\nOedipe a Colone: Act III Scene 4: Recitative: Le ciel est desarme (High Priest, Polynice, Thesee, Eriphile, Oedipe) - Chorus: Le calme succede aux tempetes\nOedipe a Colone: Act III Scene 4: Dance: (Chaconne) - Gavotte - (Chaconne)\nBe the first to review this title"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:83b89b17-b5b1-4848-9547-dbb7723d8a61>","<urn:uuid:91b75b3b-355e-44a2-84a9-5b8b6c915237>"],"error":null}
{"question":"How do emergency response capabilities compare between APC Filtration and Austin Air in terms of their contributions during major crises?","answer":"Both companies have responded to significant crises but in different ways. APC Filtration responded to specific disasters by designing HEPA and Carbon filters for firefighting tractors during the Fort McMurray wildfire, and providing HEPA filters for air scrubbers during Hurricane Patricia. Austin Air's emergency response contribution was demonstrated when they were chosen by the Red Cross and FEMA to address air quality concerns in post 9/11 New York.","context":["As second generation family business, we have engaged with many diversified business sectors and industries over the last 40 + years. As a result of this history, our business model has evolved as well. Instead of being single industry focused, our network of clients are extremely diversified. A global crisis may force one industry sector down while others flourish. The broad range of industries we serve today has provided business stability and growth over each year.\nFAA mandates HEPA Filters on Aircrafts\nThe Federal Aviation Administration already had fairly strict health and safety measures in place to protect passengers and cabin crew on U.S. aircrafts. Today, strict cabin air quality standards are in place for ventilation. Studies have shown cabin air as good as, or better than offices and homes.\nResponse to COVID-19\nAPC increases production of medical-grade ventilator filters and HEPA filters for Powered Air Purifying Respirators (PAPRs) in the fight against COVID-19.\nBAE Systems Honors APC Filtration Inc. with Gold Tier Supplier Award\nAPC was presented the Gold Tier Award for the second consecutive year for BAE Systems’ Electronic Systems sector. This award honors suppliers for exceptional performance and contributions. Over 2,200 suppliers worked within this sector with APC providing 100% compliance in ISO product quality management, on time deliveries, critical filter design, engineering and manufacturing to ISO 9001:2015 standards.\nHEPA Filters for Animal Caging Systems\nAPC works with global OEM’s to design custom air filters for animal caging systems and the caging industry. There are a variety of considerations to make when designing custom air filters and HEPA filters for animal caging systems, and APC has proudly stood up to the challenge for years.\nAutomated HEPA Filter Panel Testing\nAPC expanded its critical air filter testing capabilities with two new test chambers designed and built to accommodate maximum filter sizes of 72” x 24” x 12” and 48” x 48” x 12”. Designed and engineered by APC, they perform filter leak testing and fractional efficiency filter testing to American (IEST), European (EN) and ISO test standards. ISO 29463-1:2017 test standards are in the final review stages before global filter industry implementation. ISO 29463-1:2017\nFort McMurray Wildfire\nIn response to this historic and devastating wildfire, APC designed HEPA and Carbon filters for roof-mounted cabin air systems used in firefighting tractors. Fire fighting vehicles have strict requirements that need to be adhered to. They must be robust enough to filter out all harmful pollutants and dangerous odors to protect operators while fighting wildfires.\nAPC Responds to Hurricane Patricia\nHurricane Patricia became the strongest hurricane on record on Oct. 23, 2015. APC responded quickly to manufactures of disaster recovery equipment requiring HEPA filters for air scrubbers, fire/smoke remediation and air purification equipment.\nULPA Filters – Chemical & Biological Shelters\nWithin our world there are places where homeland security consists of public and private shelters for the protection of its citizens against bomb, chemical and biological fallout. These fallout shelters come complete with modern safe room filtration technology that allows for safeguarding against chemical and biological agents. APC Filtration Inc., was commissioned by a global manufacturer to design, manufacturer and test HEPA and ULPA filters for a series of chemical and biological shelters. These custom filters needed to capture and filter out various gases and particle sizes while doing so in a variety of temperature and humidity environments.\nBringing back Manufacturing to North America\nAPC is focused on several growth initiatives to sustain efficient manufacturing and production. This includes, transferring production from APC China to APC Canada, automation and installation of robotic filter assembly operations and expanding filter testing and manufacturing capabilities.","Austin Air's Allergy Jr. unit is designed for individuals with allergies and asthma. The Allergy Machine Jr. provides identical particle removal capacity and efficiency of the Healthmate. Since the carbon cloth does not release carbon dust when air passes through, the design of the filter places the True Medical HEPA paper on the outside. This allows the air to pass through the paper before it passes through the carbon. Due to this design the airborne particles are removed before the air contacts the carbon. Total surface area of the carbon is available for gas and odor removal only which increases the useful life.\nBenefits of Clean Air\n- Sounder Sleep\n- Reduced Snoring\n- Stop Sneezing\n- Reduce nighttime allergies & asthma attacks\n- Strengthen your immune system\n- Reduced coughing and wheezing\n- Eliminate dry mouth and runny nose\nWho should consider the Allergy Machine Jr. ?\n- Allergy and asthma sufferers\n- Those highly reactive to dust, pollen and other allergens\n- People close to trees and plants, populated areas, farms, cities and roads\nWhy is the Allergy Air Jr. Purifier the best choice?\n- Designed specifically for allergy and asthma relief\n- Special filtration for removing allergens, chemicals and gases from the air\n- Removes viruses and bacteria\n- Quickly stops sneezing, coughing and stuffy noses related to seasonal and annual air problems\n- Will cleanse the air for areas up to 700 sq ft\nChosen by the Red Cross and FEMA to address the air quality concerns in post 9/11 New York.\n360° Progressive Filtering System\nThe HealthMate Plus™ addresses the complete spectrum of air cleaning, removing sub-micron particles, noxious gases and chemicals. Austin Air’s 360-degree intake system draws air into all sides of the HealthMate Plus™, passing it through a 4-stage filter.\nThe result, more clean air delivered faster and more efficiently than any other air cleaner on the market.\nSTAGE 1 – Large Particle Pre-filter Removes particles easily seen by the naked eye (e.g.dust, hair and pet dander)\nSTAGE 2 – Medium Particle Pre-filter Removes small to medium size particles (e.g.molds, spores and pollen)\nSTAGE 3 – About 15 lbs. of Impregnated Activated Carbon and Zeolite Removes chemicals, gases and odors.\nSTAGE 4 – 60 sq.ft.of Certified HEPA Removes 99.97% of all particles larger than 0.3 microns\n- 700 sq/ft\n- Height: 16.5’’\n- Width: 11’’ x 11’’\n- Weight: 18 lbs\n- 22 gauge steel housing\n- 22 gauge perforated steel intake\n- Baked on powder coat finish (Available in white, black, and sandstone)\n- 360 Degree Intake\n- 9.75'' diameter\n- 11.5'' height\n- 6.5 lbs activated carbon impregnated with potassium iodide and zeolite (ten to one ratio)\n- 30 sq/ft True Medical HEPA grade filter medium\n- Permafilt Prefilter traps large dust particles and is designed to be vacuumed from outside and eliminate costly 3 month filter changes\n- Metal end caps with foam sealing gaskets top and bottom\n- Total weight of 8 lbs.\n- 120 volt\n- 80 watt, 1.0 amp power consumption on high setting\n- 41 watt, 0.49 amp power consumption on medium setting\n- 25 watt, 0.3 amp power consumption on low setting\n- Plastic centrifugal fan\n- Three speed control\n- 200 CFM on high setting\n- 100 CFM on medium setting\n- 50 CFM on low setting\n- Permanent split capacitor motor, rated for continuous high RPM, long life duty\n- Shock absorbing motor mounts\n- CSA & UL approved"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b7f18892-7079-4be0-a537-f22539a49ae3>","<urn:uuid:a393ad30-0285-4551-a7c9-0af1091ca095>"],"error":null}
{"question":"Do lifetime hunting licenses remain valid after moving out of California?","answer":"Yes, lifetime hunting licenses remain valid even after moving out of California. You can still use the license when you return to visit the state, though you would be required to buy non-resident tags for big game species. The license privileges continue regardless of your state of residence.","context":["Question: Is there any kind of domoic acid test kit available that a consumer can use to test his/her own crabs? I would think there would be a lot of interest in this. I love to catch and eat crabs but also hate risking getting sick! (Bob W.)\nAnswer: If you do a google search you will find some kits that state they will detect domoic acid toxins in shellfish, marine algae and water samples. However, the California Department of Fish and Wildlife (CDFW) cannot comment on the suitability of these products to address your concerns. You can minimize your risk though by following California Department of Public Health (CDPH) advisories. In their recent news release, CDPH advised that meat from Dungeness crabs caught in areas where the advisory has been lifted is safe to consume. However, consumers are advised to not eat the viscera (internal organs, also known as “butter” or “guts”) of crabs.\nCrab viscera usually contain much higher levels of domoic acid than crab body meat. When whole crabs are cooked in liquid, domoic acid may leach from the viscera into the cooking liquid. Water or broth used to cook whole crabs should be discarded and not used to prepare dishes such as sauces, broths, soups or stews (i.e. cioppino or gumbo), stocks, roux, dressings or dips.\nTo check for current health warnings on the consumption of crabs and other shellfish, I suggest you call CDPH’s shellfish hotline at (800) 553-4133 or visit CDPH’s Domoic Acid health information Web page. This information is always up to date and available via a recorded message 24/7.\nIf I see a mountain lion, who do I call?\nQuestion: What do I do when I see a mountain lion come on my property? Who do I call? (Darren M.)\nAnswer: If you see a mountain lion come onto your property, you don’t need to call anyone unless the animal is acting aggressively toward you or your family, or if it appears to be sick or diseased. If you feel it is an immediate threat to you, call 911. But mountain lions are usually just looking for deer or other prey animals.\nIf you do know you have a mountain lion around your home, I suggest you keep small children, pets and other animals in a protective area, especially from early evening through mornings when mountain lions are most active. If the animal is just passing through, as they typically do, you might just watch it and enjoy the unique opportunity you’re being given to actually see one. Most people will never have the chance to see one in their lifetimes.\nFor more information, please check out our living with mountain lions webpage.\nDo blue catfish reproduce in California?\nQuestion: Do blue catfish reproduce in California lakes? If not, why? (Mike M., Anaheim)\nAnswer: Blue catfish can reproduce if they are mature (which can take 4-7 years) and the right temperatures and other environmental conditions exist. In the wild, they typically prefer a cave habitat where they can construct a nest (eg: under rock ledges, logs, or undercut banks) and it is the male that guards and protects the eggs and young fry. Catfish farmers often place into their ponds containers like old milk cans to help the spawning catfish establish nests so that the eggs are easily retrieved and further nurtured in the hatchery. In California, spawning season is late spring/early summer, as temperatures are warming.\nLifetime license still valid if I move out of state?\nQuestion: I was looking into the Lifetime Hunting/Fishing license. This may be a stupid question but if I leave the state of California and change my residency, do I forfeit the whole lifetime license? I assume I must live in California in order to qualify. (Bill)\nAnswer: No, you won’t forfeit it. Under the provisions of a Lifetime Hunting or Fishing License, your license is valid for hunting/fishing when you return to visit even if you move out of state. You would be required to buy non-resident tags for big game species but the license is still valid. For more on the benefits and privileges of hunting and fishing lifetime licenses, please visit the department website.\nCan you harvest abalone for a handicapped individual?\nQuestion: Just curious if there are any provisions in the abalone regulations to allow someone to assist a handicapped person. For example, if the person is unable to dive for abalone, can someone else harvest the catch for them? (Todd J., Milbrae)\nAnswer: No, an individual may only take or possess one daily limit of abalone (which is three). A diver could take three abalone one day, record and tag them with their abalone report punch card and tags, and then give them to a disabled person who is not able to dive. Then the following day, the diver may go out to get three more abalone for themselves, and again, report them on the punch card and tag them in accordance with the regulations.\n# # #\nCarrie Wilson is a marine environmental scientist with the California Department of Fish and Wildlife. While she cannot personally answer everyone’s questions, she will select a few to answer each week in this column. Please contact her at CalOutdoors@wildlife.ca.gov."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:769e80e6-a831-4154-bb14-4aea8d290853>"],"error":null}
{"question":"Working in healthcare policy, I need to understand the distinction between individual-focused care and population health approaches. How do pharmaceutical companies and general practitioners differ in balancing these two aspects?","answer":"Pharmaceutical companies and GPs take different approaches to balancing individual and population health. Pharmaceutical companies primarily focus on population-level transparency and ethical standards through industry-wide regulations and reporting requirements for healthcare professional interactions, while maintaining relationships with individual healthcare providers. GPs traditionally concentrate on individual patient care through a biomedical approach, but are increasingly incorporating population health perspectives by addressing social determinants of health, participating in community-level health initiatives, and implementing preventive health measures. While GPs may be limited by fee-for-service funding models in pursuing population health initiatives, they can contribute to public health through both individual consultations and broader community engagement.","context":["Improving transparency in the pharmaceutical industry\n- Deborah Monk\n- Aust Prescr 2016;39:110-1\n- 1 August 2016\n- DOI: 10.18773/austprescr.2016.049\nMedicines Australia represents many of the companies that develop new prescription medicines. It believes that companies’ interactions with Australian healthcare professionals have a high degree of ethical integrity. This stems from both the Medicines Australia Code of Conduct1 and complementary ethical standards developed and adopted by the professions. Based on these ethical standards, there should be a high level of community trust in the industry and healthcare professionals. However, interactions between pharmaceutical companies and healthcare professionals have been subject to negative perceptions. This is despite changes to the Code of Conduct over the last decade.\nAs an industry, we need to take some responsibility for these negative perceptions and any resulting erosion of trust. We have not done as good a job as we might have in explaining why companies and healthcare professionals interact, how they interact, how our interactions contribute to better patient care and the standards industry adheres to when engaging with healthcare professionals. Medicines Australia seeks to uphold and strengthen community trust in the pharmaceutical industry and our engagement with healthcare professionals. One way Medicines Australia is doing this is by being more transparent about these interactions.\nWith the introduction of the latest edition of the Code of Conduct in May 2015,1 the industry is striving to be more open, more transparent and more communicative about interactions between companies and healthcare professionals. The new Code requires Medicines Australia’s member companies to publish information about individual healthcare professionals (including doctors, pharmacists, nurses, dentists and dietitians) who receive a ‘reportable payment’. Reportable payments are fees for a healthcare professional’s advice or service, such as an honorarium, consulting or sitting fee, and the provision of airfares, accommodation or registration fees to enable a healthcare professional to engage in education. The cost of food and beverages provided during educational meetings is not reportable for individual healthcare professionals. Reportable payments will be published every six months on companies’ websites while Medicines Australia investigates establishing a central reporting system for all companies’ reports.\nCompanies are responsible for reporting payments associated with company-initiated activities and meetings, whether these are organised directly or by another company or agency. If a company provides sponsorship to a college or society to hold its own educational meeting, these sponsorships will be separately reported, by event, in reports published on Medicines Australia’s website.\nWhen Medicines Australia developed the new Code of Conduct, there was considerable debate about whether healthcare professionals should be able to ‘opt out’ of their payments being reported. However, the Royal Australian College of General Practitioners, the Royal Australasian College of Physicians, the Royal Australian and New Zealand College of Psychiatrists, the Society of Hospital Pharmacists of Australia and consumer organisations such as the Consumers Health Forum of Australia had strong views against an ‘opt-out’ clause. Disclosing reportable payments will therefore become mandatory.\nFrom 1 October 2015, Medicines Australia’s member companies started to collect information about reportable payments to individual healthcare professionals. At first this information will only be publicly reported for each healthcare professional with their consent. The first reports will be published on companies’ websites by 31 August 2016. After a year’s transition, payments will be reported without seeking consent. Healthcare professionals will be notified of the reporting requirement when they receive a reportable payment. From 1 October 2016, healthcare professionals’ details will be disclosed whenever a reportable payment is made.\nThe new measures are an important step forward for industry, healthcare professionals and importantly Australian patients. They are part of a movement towards more transparency which is also underway in the USA, across Europe and in Japan.\nMedicines Australia supports greater transparency in companies’ relationships with healthcare professionals. Our ultimate goal is for the degree of transparency required by the Medicines Australia Code of Conduct to be normal, expected business practice for the entire Australian medicines and medical devices industry. Medicines Australia would encourage and support other industry sectors, such as the medical devices, generic medicines, over-the- counter medicines and complementary medicines, to follow this lead towards greater transparency about their interactions with healthcare professionals.\nConflict of interest: Deborah Monk is Director, Compliance at Medicines Australia.\nDirector, Compliance, Medicines Australia, Canberra","General practitioners (GPs) tend to focus most of their energies on providing primary healthcare to individuals, with less attention to the overall population health issues in their community. In contrast, public health practitioners tend to focus on the health needs of entire populations, by addressing the social determinants of health, with less attention to individual patient care.\nThis article seeks to provide a practical approach for GPs to incorporate a public health perspective in their everyday work.\nGPs have an important role in public health both through individual patient care and by engaging with public health issues at local, community and global levels. Adopting a population perspective to healthcare is an important part of modern general practice.\n‘Our patients can reasonably expect to have their immediate medical needs attended to, along with an examination of broader issues which may have led to their presentation. It follows logically that family physicians must have a role in looking beyond illness, and trying to shape behavioural, societal and environmental influences on ill health’.1\nGeneral practitioners (GPs) have traditionally focused on providing primary care to individuals who present to their medical practice with various illnesses. This patient-centric approach has its philosophical underpinnings in a biomedical view of disease that emphasises an individual focus. Public health on the other hand focuses on the health of the entire population and is underpinned by addressing the social determinants of health. Such a model tends to privilege the needs of the population over the needs of the individual.\nOf course, focusing on individual care does not preclude also being committed to a public health mindset; in fact, they are usually congruent objectives. In this paper, we argue that now more than ever, adopting a population perspective to healthcare is an important part of modern general practice and we seek to provide a practical approach for GPs to incorporate a public health perspective in their day-to-day work. We suggest that encouraging GPs to be more involved in the population aspects of public health will require a shift in the culture of general practice in addition to the re-orientation of the incentives.\nThe traditional model of general practice (individual)\nAccording to The Royal Australian College of General Practitioners (RACGP), general practice is ‘the provision of patient-centered, continuing comprehensive, coordinated primary care to individuals, families and communities’2 (emphasis own). Yet, it is probably true that despite the inclusion of ‘communities’ in this definition, in reality most GPs focus most of their energies on providing direct clinical care to individuals who seek medical advice.3 In countries such as Australia, the fee-for-service funding model has encouraged this focus, with GPs having limited funded time available to participate in any work outside of direct individual patient care.4 Involvement in community level activities has traditionally been a voluntary contribution by altruistic GPs or perhaps, more recently, facilitated through organisations such as the Medicare Locals. In recent years there have been some exceptions to this rule, with Medicare practice incentive payments encouraging doctors to utilise individual patient interactions to promote public health aims. For example, in return for incentive payments, GPs have helped increase child immunisation rates, thus contributing to the population level protection against infectious diseases.5 Furthermore, chronic disease management plans, cancer screening programs and mandatory notification of infectious diseases all represent opportunities for GPs to contribute to public health through their regular practice. Yet, the vast majority of funding in general practice remains dependent on providing curative individual services; thus, there is little incentive for population-level public health strategies to be implemented by GPs. Training models have also reflected the primacy of individual curative care, with examinations testing a candidate’s understanding of the biomedical determinants of disease and their ability to assess, diagnose and manage individual patients6 rather than assess the implementation of population health interventions.\nAs a result, it is not surprising that GPs dedicate most of their time to providing one-on-one curative healthcare and tend to give little attention to the fact that the health of an individual patient is usually dependent on the overall health of their community. Often this is due to the opportunity cost of undertaking unpaid community level health initiatives at the expense of seeing individual patients. Regrettably, it is often in the areas where public health interventions are most needed (rural and remote regions or lower SES regions) that GPs are most overloaded with individual patient care.\nThe reality of modern general practice in Australia, where patients come to a GP’s rooms for treatment, and house calls are diminishing, means that GPs rarely observe their patients’ life circumstances. The social and environmental determinants underlying a patient’s condition, which are well established to be the drivers of a population’s health status, may go unnoticed by GPs who are confined to the consultation room. Furthermore, the culture of general practice has become one of strong advocacy for the patient sitting in the examination room over and above thinking about the population health implications of the treatment plan commenced. For example, most GPs will fervently advocate for the earliest possible specialist appointment in the public health system without giving much thought to public health resource or triage implications.\nWhat is a public health perspective?\nPopulation or public health focuses on the health needs of an entire population as outlined by the World Health Organization:\n‘Public health refers to all organized measures (whether public or private) to prevent disease, promote health and prolong life among the population as a whole. Its activities aim to provide conditions in which people can be healthy and focus on entire populations, not on individual patients or diseases. Thus, public health is concerned with the total system …’7\nPublic health then is important in promoting healthy communities and, by definition, focuses on the health of the population rather than individuals. Historically, it has included advances ranging from sanitation and housing through to infection control and immunisation programs.\nAs defined, public health has a particular concern with improving ‘the conditions in which people can be healthy’ or what are known as the social determinants of health.4 These approaches are likely to benefit the population as a whole.8 The underlying social conditions, considered throughout the lifespan, determine the health of individuals and communities. They include a combination of economic stability, education, social and community context, accessibility of healthcare services and environmental factors. When medical practitioners do not consider and address these root-causes of ill health, they simply put out spot fires and ignore the real firestorm. Importantly, communities need to be empowered to take control of their own environments and alter them in accordance with what is best for them, moving the locus of control away from distant external bodies (eg. government and policymakers) and closer to communities.\nPublic health issues in general practice\nIn Australia, public health measures have been left largely to the policymakers, for example, town planners and architects, usually with little, if any, input from GPs. While it is easy to propose that GPs should be more involved in public health, it is fair to ask where does the role of the GP stop? Many public health issues may not require the expertise of a GP. For example, water sanitation may well be better left to water engineers, and health promotion advertisements left to marketing experts. However, GPs do bring a wealth of experience and intimate knowledge of their patients and do have a valuable role in informing those public health initiatives that will have bearing on the health of their patients.\nWhile not necessarily understood as population health measures, many activities of GPs already contribute to public health. For example, activities within individual consultations may include one-on-one prevention and health promotion and GPs are often at the forefront of risk factor screening and management.9 There are also various funded GP items (eg. care plans, mental health plans, check-ups for people aged 45–49 years) that incorporate significant preventive elements.9 Therefore, one approach for GPs to promote public health and contribute to the health of their local population more widely is to incorporate a public health perspective in each individual patient consultation. These individual focused activities, when taken across the entire health system, are critical in supporting a system that promotes population health.\nHowever, there are also areas where GPs can more directly engage with communities to influence population health. The World Health Organization’s Alma-Ata Declaration on primary healthcare provides a helpful model for primary healthcare services to achieve universal access to health. Primary healthcare is seen as central to integrating all levels of the health sector as well as encouraging an inter-sectorial approach, social justice, community participation and empowerment. On the basis of the Alma-Ata Declaration, Neuwelt identifies three population health principles that primary healthcare services can put into practice: equitable access, involvement of communities in decision-making and understanding of social determinants of health.10,11\nOne of the aims of primary healthcare is health equity.11 GPs can work towards health equity in their community by allowing those who need services to be prioritised, seeking ways to provide preventive services to those who may not usually be able to access them and utilising recommended treatment guidelines appropriate to individual patients to steward scarce resources. There may also be room for GPs to work with their local community to develop systems that allow those in most need to access health services more easily, for example, through a graduated fee structure.\nInvolvement of communities in decision-making\nThe Alma-Ata declaration seeks full community participation in primary health services. GPs can move towards this through ongoing discussions with local community leaders and members around strategies to promote health and improve services in the local community. One innovative approach has been the Aboriginal Community Controlled Health Services in Australia. These health services, which have local leadership and ownership, seek to holistically transform the health of their communities through providing a wide range of integrated services to and for their own communities.12 There may well be lessons from this model for mainstream health services on how to more fully incorporate the perspectives and preferences of health consumers.\nUnderstanding social determinants of health\nThe most achievable step forward for GPs in improving their public health practice is to seek to understand the background context to an individual’s life when approaching their healthcare.10 Referrals can be made not just to specialist medical practitioners but to community services that may help to address the underlying social determinants of their ill health. Examples may include referral to financial counselling, social workers or public housing officers.\nMany patient presentations to GPs involve issues that are influenced by the upstream social determinants of health for that particular patient. For example, smoking, obesity, inactivity, and drug and alcohol abuse often have underlying environmental, social or physical factors that result in the ongoing risky health behaviour by an individual patient. As GPs are the respected guardians of health in our community, their input can have a significant impact on policies and activities that affect the social determinants of health. GPs are in a prime position to undertake public health advocacy, conduct community education activities, influence policymaking dialogues at a local level, and provide representation at local stakeholder meetings about issues that affect health. In effect, GPs need to be leaders in empowering their communities to take an active control over their health and the environmental factors that influence their health. For example, GPs might challenge and support their community to consider finding new ways to access healthy food sources (eg. community gardens) or advocate with their community for accessible exercise options (eg. bike paths and running tracks). Table 1 outlines some further suggestions for ways in which public health could be incorporated into general practice.\nTable 1. Practical recommendations for further GP engagement with public health\n|1) In routine care|\n|Include the individual, patient-centred activities that have an impact on the health of the community more broadly (eg. reporting notifiable diseases, immunisation, undertaking health promotion and health education and addressing relevant social determinants).\n|2) Engage with local community health promotion|\n|At the local level, this might involve speaking at community forums, initiating preventve health campaigns or engaging with schools and community groups around relevant health issues.\n|3) Contribute to public health debate in the media|\n|GPs can raise public health issues in the mainstream media, medical journals and other professional literature.\n|4) Publish patient education health materials that are locally relevant|\n|GPs can develop locally relevant patient education materials that address key local health issues (eg. smoking, inactivity, air pollution or STI testing).\n|5) Enrol in membership/advocacy organisations that have an impact on public health|\n|For example, the RACGP, the Public Health Association of Australia (PHAA) and the Australian Medical Association (AMA) all have powerful voices in the public health arena.\n|6) Broaden GP training|\n|We need to train GPs to understand, value and feel capable of being involved in public health practice. This might include training on the basic principles of public health as well as training in media, presentation, community engagement, and policy advocacy. Such skills would better equip GPs to engage with their local communities in leadership roles, health promotion and advocacy.\n|7) Support incentives for GPs to promote public health|\n|If funding were available for public health interventions, then it would encourage GPs to be involved in such activities alongside their face-to-face clinical work.\n|8) Simplify Medicare preventive health incentive programs|\n|Maximise GP uptake of preventative programs with minimal red tape.\n|9) Special interest groups|\n|Encourage GPs to consider population health as a possible special interest group, for example, in the professional colleges.\nGPs should be aware that they have an important role in public health, not only through individual patient care but also through engaging with public health issues at a community and global level. While system-wide change both culturally and organisationally will be required to support increased GP engagement in public health in the future, there are many ways that GPs can incorporate the principles of public health within our existing Australian context.\nCompeting interests: None.\nProvenance and peer review: Not commissioned; externally peer reviewed.\n- Weller, D. Family medicine should focus on the sick: Negative position. Ideological debates in family medicine. New York: Nova Biomedical Books, 2007.\n- The Royal Australian College of General Practitioners. Standards for general practices. 4th edn. Melbourne: RACGP, 2010.\n- McWhinney IR. Primary care: core values. Core values in a changing world. BMJ 1998;316:1807–09.\n- Fraser J. Population and public health in Australian general practice – changes, challenges and opportunities. Aust Fam Physician 2005;34:177–79.\n- Australian Government. Practice Incentives Program. Medicare 2012. Available at www.medicareaustralia.gov.au/ provider/incentives/ pip/index.jsp [Accessed 6 May 2013].\n- Achhra A. Health promotion in Australian general practice – a gap in GP training. Aust Fam Physician 2009;38:605–08.\n- World Health Organization. Public Health. World Health Organisation, 2013. Available at www.who.int/trade/ glossary/story076/en [Accessed 6 May 2013].\n- Wilkinson RG, Pickett K. The spirit level: why greater equality makes societies stronger. New York: Bloomsbury Press, 2010.\n- Fry D, Furler J. General practice, primary health care and population health interface. In: Pegram R ed. General Practice in Australia Canberra: General Practice Branch, Department of Health and Aged Care; 2000;385–427.\n- Neuwelt P, Matheson D, Arroll B, et al. Putting population health into practice through primary health care. N Z Med J 2009;122:98–104.\n- World Health Organization. Declaration of Alma-Ata. Alma Ata. USSR: WHO; 1978. Available at www.euro.who.int/_ _data/assets/pdf_file/0009/113877/ E93944.pdf [Accessed 12 May 2014].\n- NACCHO. National Aboriginal Controlled Community Health Organisation: Vision and Principles. National Aboriginal Controlled Community Health Organisation, 2012. Available at: www.naccho.org.au/ about-us/vision-and-principle [Accessed 22 January 2014]."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5d03355c-b025-488b-aad1-f7f222ac0cbf>","<urn:uuid:32cd8f52-c90c-4304-9e9f-013aface0584>"],"error":null}
{"question":"¿Por qué Bob the Builder casi se llama Bill the Builder or Bob the Construction Worker?","answer":"Bob the Builder was almost named Bill the Builder because the creators were choosing between these two names, but 'Bill' didn't sound quite right. As for 'Bob the Construction Worker', this alternative was suggested because the term 'builder' isn't commonly used in America, but they ultimately decided to keep the original name 'Bob the Builder'.","context":["Who is the most popular builder in the world? Easy. He’s eight inches tall and made out of foam and silicone. His best friend is a dipper called Scoop. And his fans have no doubt at all over whether he can repair things satisfactorily.\nCan he fix it? Yes he can!\nYet Bob the Builder – a children’s show created in Britain that went on to conquer the planet – was very nearly “Bill the Builder”. Stranger still, there was serious talk he should be “Bob the Construction Worker”.\nBob’s creator, Keith Chapman, explains: “It was a toss-up between Bill the Builder and Bob the Builder. But Bill didn’t sound quite right.”\nAnd “Bob the Construction Worker”? Well, Chapman was advised that the term “builder” isn’t often used in America, and if he was considering selling the programme overseas... “but, thankfully, in the end they let us keep the name.”\nBeing a bog-standard builder doesn’t seem to have held Bob back. He’s been translated into 45 languages and shown in almost every country on the planet. Chapman says that Bob has raked in worldwide revenues – from TV sales, merchandising and so on – of more than $5 billion. Incredibly, that’s nearly twice the amount earned by Avatar, the biggest movie of all time.\nOf course, when we remember the children’s characters that mean the most to us, we’re not interested in how much money they made. (For a start, when the likes of Muffin the Mule or Noggin the Nog were doing their stuff on screen, the merchandising industry was barely out of nappies.)\nWhether we’re from the generation that harks back to Andy Pandy, The Magic Roundabout or The Tweenies, we know the emotional power of these shows. Why? Because they were what we watched when we first started watching TV.\nHow can you not feel warm when you think of Mr Benn, Festive Road and “as if by magic, the shoekeeper appeared”?\nIs there anything more enchanting than Basil Brush’s “Boom! Boom!”, the “underground, overground” Wombles of Wimbledon or Bagpuss’s satisfied yawn?\nWhich is probably why it’s so rewarding to watch our own children watching their own programmes. Because when they’re in front of Iggle Piggle, Fireman Sam or Charlie and Lola, they’re investing in their own future nostalgia.\nBut for every show that hits the spot, there are dozens that rightly drift into obscurity. So what’s the secret of a successful children’s TV character?\nWhen RT consulted experts in the field, they all agreed on one thing: getting the name right is crucial. It has to be easy for a child to pronounce, it has to have the correct rhythm and, if you’re looking to sell lots of games and toys (and everyone is), it has to look good when printed on a box (another reason why “Bob the Builder” triumphs over “Bob the Construction Worker”).\nAnna Wood, co-creator of Teletubbies, says the name Tinky Winky “just came to me. Then we sat down and worked out the other names so it went in a nice rhythm: Tinky Winky, Dipsy, Laa-Laa and Po. It took about half an hour.”\nLauren Child, who came up\nwith Charlie and Lola, agrees\nthat the cadence was important:\n“I liked the sing-song sound of\nCharlie and Lola, they work well together and are easy to say.” Back in 1999 when she created the lovable brother and sister, few little girls were called Lola, “but\nI sensed that there was something in the air and that this name would become popular. This has proved to be the case – I now meet countless Lolas.” (Whether this would have been the case without Child’s own characters is difficult to say, of course.)\nTiming can be important, too. The Teletubbies – all of whom have hi-tech screens in their bellies – came at the start of the digital revolution.\nBob the Builder launched when DIY was becoming intensely fashionable (remember Changing Rooms and the 101 variations that followed?). Quite why this should affect an audience that wouldn’t be allowed to pick up a power drill, one doesn’t know. But Keith Chapman is convinced it helps explain his success: “The show hit at the right time.” Humour is the next key ingredient. On this, everyone is pretty much agreed. Physical comedy will never go out of fashion.\n“Falling over is the best joke of all time,” says Anne Wood, the woman behind Iggle Piggle and Roland Rat as well as Teletubbies. “It’s funny to a pre-school audience because you’re at the age where you fall over and you’ve just learnt how to get up again.” If you fall over more than once, it’s even funnier.\nRichard Starzak who devised Shaun the Sheep, agrees. “Slapstick is a universal language.” This can, however, pose a problem when you’re making a show aimed at very young people. “There’s a lot of comedy in other people’s pain, of course. But there are things you can’t do on children’s shows. We can’t really have characters being hit on the head.\n“I understand why. We don’t want to be responsible for kids hitting each other with frying pans. But I sometimes think children’s TV can be overly safe. I don’t think cartoon-style violence affects kids. I think it’s a bit of escapism.”\nHe recalls with fondness an edition of ITV’s Tiswas from his own childhood. “They used to lift children out of tea chests by their ears. Then, one Saturday, one of the kids cut himself on the edge of the chest, and was bleeding on live TV. Chris Tarrant lifted him out, put him to one side and said: ‘Could you pass me another one that’s not bleeding?’ And they carried on with the show. Can you imagine the front pages of the newspapers if that happened today?” (Tiswas also featured a man repeatedly hitting himself over the head with a metal tray while singing Mule Train.)\nCharlie and Lola, too, have been reined in. Lauren Child says that a proposed episode in which Lola caught head-lice was rejected by a squeamish Disney corporation, which had partially financed the show. Meanwhile, in the original books, Lola eats biscuits and crisps and sometimes stands on chairs. Not in the BBC version – where she snacks on fruit and never, ever climbs on furniture. (Though there will be plenty of parents who applaud the show’s anti-crisp stance.)\n“And we almost had a problem with Lola doing forward rolls,” adds Child. “The BBC got terribly worried she could have a dreadful accident and break her neck. It doesn’t matter that she and Charlie are fictional, and they’re made of paper. The designers had to draw a very squishy mattress for her to do her forward roll on.”\nThen there are the things that don’t matter. Look at an episode of Clangers or Trumpton and the quality of animation looks quite amateur by today’s standards – whether you compare it with the smooth stop-motion animation of Shaun the Sheep or the computer-generated Mike the Knight. Yet what difference did it make that the action was a little juddery?\nNick Park, creator of Wallace & Gromit, says his favourite show as a child was Clangers. “There was always something a little quirky about it, unpredictable.\n“I liked the crude way it was done. It made you think you could do it at home. I started animating as a 13-year-old. I was emulating Clangers.”\nHe is currently part of the team making a new Shaun the Sheep movie. A prequel to the TV series, it’s due for release early next year and will feature all the characters from the TV show and explain how they got to know each other.\nAs editor of Blue Peter from 1965 to 1988, Biddy Baxter had her finger on the pulse of children’s TV – and was another Clangers fan: “It was wonderful... the animation and the groans and the squeaks and the grunts. It was so surreal. You could use your imagination.”\nBut she doesn’t envy readers having to choose a favourite in our children’s character poll. “How awful to have to choose between Basil Brush, The Magic Roundabout and Clangers!\n“And then, of course, there was Paddington. I had a huge affection for Paddington because it was created by Michael Bond, a lovely man who was a cameraman on Blue Peter. Paddington was such a sympathetic character – and children, on the whole, are pretty keen on bears.”\nLauren Child is another Paddington advocate: “I was watching it on DVD with my four-year- old daughter yesterday, and I don’t think it’s dated: it’s got a snappy pace, it’s very funny and the jokes are very sophisticated.”\nThe point is you need to watch the programmes with the eye of a child and leave your adult preconceptions at home.\nNever was this more apparent than in 1999 when the Teletubbies found themselves at the centre of a bizarre international furore. American TV evangelist Jerry Falwell denounced the BBC show for secretly “role modelling a gay lifestyle” – which, he said, was “damaging to the moral lives of children”. His evidence? Tinky Winky – “whose voice is obviously that of a boy” – carried a handbag, and “he is purple – the gay pride colour; and his antenna is shaped like a triangle – the gay pride symbol.” Even after all these years, Anne Wood is irked. “Jerry Falwell had his own agenda and before he died he admitted to that. But his comments stopped all the sales of Teletubbies merchandise in the southern states of America because everybody thought Tinky Winky was a homosexual.\n“It was so insulting. The fact is that children’s television is about love, it’s got nothing to do with sex at all. And all children – boys and girls – love to look in your handbag. I haven’t known a child who didn’t.”\nMeanwhile, Bob the Builder seems to have been sending out strange messages, too, albeit of a different kind. For he is rather more philanthropic than your average odd-job man.\n“You’ll notice,” says Keith Chapman, “that he never seems to take money off anyone. Not even a deposit. So he’s doing all this work for nothing!”\nChoose your favourite and vote in our poll right here"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:542c8654-a9c1-4035-9da0-5a97508674d5>"],"error":null}
{"question":"What feeding precautions are recommended for babies and puppies during their transition to solid food?","answer":"For babies, key precautions include offering one food at a time with several days' wait between new foods to monitor reactions, ensuring smooth texture similar to milk, and only serving boiled, steamed, or mashed fruits and vegetables. For puppies, precautions include making the transition gradually over 2-3 weeks to minimize gastric upset, using high-quality puppy food that's a good source of protein and calcium, and avoiding foods that list corn or meat by-products as the first ingredient. Both babies and puppies should have their solid foods mixed with their primary liquid nutrition (breast milk/formula for babies, milk replacer for puppies) during the transition.","context":["Paediatricians recommend breast milk as the primary source of nutrition for the first six months of a baby’s life. By the time your infant is 4 to 6 months old, you've probably mastered the art of breastfeeding or formula feeding. But once your baby is six months old, breast milk is still healthy and beneficial, and it should be your child's primary source of nutrition. But your baby needs additional nutrients from food, especially iron, protein, and zinc. At this point, your baby is ready for baby food or solid foods. Your baby's paediatrician will advise you when it's time to begin introducing your baby to solids. Though it is recommended that you should start your child on solids between 4 and 6 months, but the answer depends on your baby. Remember, there's no need to rush this milestone. But how do you know if your baby is ready for foods other than breast milk or infant formula? You can look for these signs that your child is developmentally ready:\n- Your baby can sit up with little or no support\n- Your baby has good head control\n- Your baby opens his or her mouth and leans forward when food is offered\n- Your baby can turn his or her head away to let you know he or she is full\nThe ideal way to make eating solids for the ﬁrst time easier is to give your baby a little breast milk, formula, or both ﬁrst; then switch to very small half-spoonfuls of food, and ﬁnish with more breast milk or formula. At ﬁrst, some babies refuse solids or have trouble eating new foods. Don’t panic, it is a general phenomenon, keep trying. By tradition, single-grain cereals are usually introduced ﬁrst. Many pediatricians will recommend starting vegetables before fruits and would also tell you that babies are born with a preference for sweets. While introducing your baby to solid foods, use these tips as they would come in handy:\n- Veggies and fruits in their raw form would be quite hard to chew and swallow for the baby, so they should only be boiled, steamed, and mashed as a smoothie. It’s easier for your child to eat foods that are mashed, pureed, or strained and very smooth in texture. Oﬀer foods one at a time and wait a few days between starting new foods so you can tell if your child reacts to something new.\n- Try and keep the texture of the food as smooth as milk. Till now the baby is familiar with that texture only and understands it quite well, so acceptability of the new diet would not be a problem. Mix cereals and mashed cooked grains with breast milk, formula, or water to make it smooth and easy for your baby to swallow.\nThe baby’s appetite would not be big enough for them to start gorging on food right from day one. It would be minuscule initially and would increase gradually. Therefore, start with a single serving in a day of this new meal initially. At the beginning of this new diet plan, serve food in small quantities. As the baby shows signs of an increases appetite over a period of time, add one more meal in daily intake, and gradually increase the quantity of each serving.\n- Burping is an important part of the baby’s feeding process for their wellness and comfort. When babies are feeding, they take in air, due to which they tend to sit up and feel uncomfortable, making them fussy and cranky. Burping helps to get rid of some of the air that babies tend to swallow during feeding.","Puppy feeding guidelines\nHello, I am rescuing my first puppy next Monday. She'll be 11.5 weeks when we get her, and I've never had a dog that young. The people fostering her are feeding her litter 3/4 cup twice a day.\nShe's an Australian shepherd/great pyrenees/golden retriever mix. I keep reading mixed things about feeding them twice or three times a day. Which do you recommend?\nAnd if it is three times a day, given that she's almost three months old, how much would you feed her each time? I have an appointment with a vet on that Friday, but wanted to ask you what you thought, hoping to get an answer before I pick her up.\nThanks so much in advance! I'm really excited to take on this challenge, But it's also a little nerve-wrecking to not know how much to feed her exactly?\nRead Dr. Kristy Conn's advice:\nCongratulations on your newest four-legged addition to your family. I understand the excitement and trepidation that comes with bringing home a furry ball of joy. You want to do right by her and that includes making sure she is getting the correct amount of nutrition. This is a popular time of year for bringing home puppies so I’d like to take the opportunity to review basic puppy feeding guidelines for the first year of life while covering your question which may also answer any future questions you or other readers may have.\nFirst 6 to 8 weeks\nDuring the first six to eight weeks of life the puppy should stay with the mother and be allowed to nurse ad-lib. It is especially important they nurse from the mother. The mother’s milk provides the best nutrition and provides antibodies to help protect your puppy from disease. Sometimes it is not possible to keep a puppy with the mother for the first eight weeks such as when the mother develops eclampsia or mastitis. In these situations milk replacers and bottles especially designed for puppies can be found at any major pet store.\nWeaning your puppy to solid food\nWeaning your puppy to solid food should not be an overnight endeavor but should ideally take place over the course of two to three weeks. First select the brand of puppy food you intend to feed. Puppies have high caloric and nutritional needs and so the food selected should be a high quality brand of puppy food. Talk to your veterinarian for specific recommendations but generally the best puppy foods be a good source of protein, calcium and calories.\nStarting around four to six weeks of age begin introducing your puppy to puppy food by making a gruel by blending the puppy food with milk replacer. Offer the gruel three to four times a day gradually reducing the amount of milk replacee used to make the gruel. This way your puppy gradually learns to adapt to solid food and gastric upset is minimized. By around eight weeks of age your puppy should be eating solid food.\nHow often to feed your pup\nPuppies should be fed three to four times a day therefore if you are currently feeding ¾ a cup of puppy food twice a day you should consider spacing it out by feeding ½ cup three times a day.\nSmaller meals are easier to digest for the puppy and energy levels don’t peak and fall so much with frequent meals. At around six months you may start feeding twice a day for convenience but because your dog is a mixed large breed dog I would recommend sticking with a 3-4 times a day feeding schedule if possible to minimize the risk of gastric dilatation volvulus.\nMove on from puppy food\nPuppy food is very high in calories and nutritional supplements so you want to switch to adult food once your puppy begins to approach maturity. There is no set age when the switch should be made because it will vary with the breed and individual dog.\nIn general, the smaller the dog the faster they reach maturity. Small breeds up to 30 pounds mature around ten to twelve months of age although some toy breeds reach maturity even sooner. Medium breed dogs up to 80 pounds will reach maturity between twelve to sixteen months and I believe your puppy may fall into this category. It really depends how much Great Pyrenees she has in her. Large breed dogs weighing more than 80 pounds can take up to two years to reach maturity.\nWhen making the switch to adult food, do it slowly over the course of one to two weeks by gradually mixing in increasing amounts of the adult food with decreasing amounts of her puppy food to minimize gastric upset.\nFoods to avoid\nTry to avoid foods that list corn or meat by-products as the first ingredient because meat should be the first ingredient.\nAvoid feeding your dog puppy food longer than you have to. Feeding puppy food too long can result in obesity and orthopedic problems. You will know when it is time to make the switch when you notice your dog eating less of the puppy food or if she starts to put on too much weight.\nDon't let your dog get chubby\nIt is important to be able to judge your dog’s body condition score in order to determine how much food you should give.The recommendations on the labels are guidelines only and will not apply to every dog. You may need to feed a little more or less depending on your dog’s condition score.\nIdeally you should be able to feel but not see the ribs and by looking straight down at your dog while she is standing there should be a recognizable waist. Ask your veterinarian if your dog is at a healthy weight during your next visit and ask for examples of body condition scores.\nHow’s your puppy’s diet like? Tell us all about it in the comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:adf52351-3e5b-4b1c-97fd-2e8447b2a71e>","<urn:uuid:f4d02fd9-82ba-4116-b2db-9313b071fb0b>"],"error":null}
{"question":"What is the relationship between handling precious items and their market value, considering both auction prices and preservation methods?","answer":"Valuable items can fetch high prices at auction, as demonstrated by record-breaking sales like the tetradrachm from Catane that sold for 200,000 Euros. However, proper handling is crucial to maintain such value - oils from human hands can cause permanent damage, particularly to silver which tarnishes from hand oils and hydrogen sulphide compounds. Museums and collectors should minimize handling of items and when necessary, handle with great care to preserve their condition and value.","context":["01-10-2015 – 01-01-1970\nRecord prices for Greek silver and Roman gold\nSix auctions conducted over the course of six days is remarkable in itself, but a turnover of 13.8 million euros achieved over the course of only six days truly marks a new record. With the final result Auction House Künker happily records the highest-grossing auction week in the history of the company.\nNot only the large number of special collections accounted for this impressive result, but also the broad range of world and ancient coins as well as orders and decorations. With estimates starting at 20 euros, many collectors know that at Künker, they will find the right coin.\nLot 8117: GREEKS. Sicily. Catane. Tetradrachm, c. 405/402, signed by Herakleidas. Ex Jameson Coll. 547. Very rare. Extremely fine. Estimate: 100,000,- euros. Hammer price: 200,000,- euros.\nAuction 270: Coins from the Ancient World\nNo less than 200,000 Euros was the final bid for an extremely fine tetradrachm from Catane. The coin exhibits one of the most coveted facing portraits ever to be found in monetary history when at the same time it also offers a noteworthy pedigree. Not only stretches it back to 1913 when the collector R. Jameson itemized this specimen in his catalog, but it goes even further. In times when the community of German collectors is engaged in an ongoing debate about modifications of the stipulations regarding the Protection of Cultural Heritage, such a solid provenance is literally worth a lot of money.\nLot 8135: GREEKS. Sicily. Syracuse. Decadrachm, after 405, signed by Euainetos. Virzi Coll., ex Hirsch Auction 32 (1912), 322. Very rare. Extremely fine. Estimate: 50,000,- euros. Hammer price: 100,000,- euros.\nThe same goes for a marvelous decadrachm, signed by Euainetos. Stemming from the Virzi Collection, which was auctioned off by Jakob Hirsch in 1912, bidding started at 50,000 euros, only to reach a hammer price of 100,000 euros.\nLot 8317: GREEKS. Ionia. Herakleia. Tetradrachm, 2nd cent. BC. Extremely fine. Estimate: 750,- euros. Hammer price: 3,600,- euros.\nAmong the coins from the Greek department, any number of specimens really obtained spectacular prices. A coin that needs special mentioning is a wonderful Archaic stater from the island of Naxos which started at 5,000 and arrived at a final bid of 12,000 euros. A tetradrachm from Herakleia, on the other hand, vouched for the fact that perfection has a major effect on an item’s price. Not a rare, but a perfectly struck coin in the best grade possible and of the finest style, it obtained 3,600 euros based on an estimate of just 750 euros.\nLot 8642: ROMAN IMPERIAL TIMES. Vitellius. Aureus, Rome. Lanz 135 (2007), 569. Very rare. Tiny scratches, expertly restored. Extremely fine. Estimate: 50,000,- euros. Hammer price: 105,000,- euros.\nThe only coins able to compete with the Greek specimens were Roman aurei and Künker was able to offer a great many at this auction. All together the aurei stemmed from a period between the late Roman Republic, with an aureus of Marc Antony together with Octavian (estimate: 35,000 / hammer price: 36,000 euros), and the era of the barracks emperors, of which we would like to single out an aureus of Geta (estimate: 12,500 / hammer price: 52,500 euros). Any item in a grade above average achieved an exceptional price. Two aurei even ranged in the six-digit region: an aureus of Vitellius (estimate: 50,000 euros / hammer price: 105,000 euros) …\nLot 8836: ROMAN IMPERIAL TIMES. Didius Julianus. Aureus. Ex Montagu Collection, Rollin & Feuardent Auction 1896, lot 461. Very rare. Extremely fine / nearly extremely fine. Estimate: 35,000,- euros. Hammer price: 150,000,- euros.\n… and an aureus of Didius Julianus (estimate: 35,000 euros / hammer price: 150,000 euros).\nLot 8549: ROMAN IMPERIAL TIMES. Augustus, 30 BC – AD 14. Denarius, 28 BC, Italian mint. Ex Tkalec Auction (2011), 134. Very rare. Extremely fine to FDC. Estimate: 35,000,- euros. Hammer price: 55,000,- euros.\nBut not only gold attained record prices. When turning to silver in the finest grade, an AEGYPTO CAPTA-denarius of Augustus also achieved an amazing price. Originally estimated at an impressive 35,000 euros, it was auctioned off for an even more impressive sum of 55,000 euros. A cistophorus of Nero is another great example: modestly estimated at 2,500 euros, it reached a hammer price of 17,000 euros.\nLot 8828: ROMAN IMPERIAL TIMES. Lucius Verus. AE-medallion, 164/5, Rome. Ex Kastner Auction 4 (1973), 251. Extremely rare. Extremely fine. Estimate: 25,000,- euros. Hammer price: 44,000,- euros.\nLastly, bronze coins deserve mentioning as well: to conclude this overview, we would like to single out two medallions: one of Lucius Verus (estimate: 25,000 euros / hammer price: 44,000 euros), and the other of Constantine the Great for Urbs Roma (estimate: 6,000 Euro / hammer price: 15,000 euros).\nPlease find all results online here.","Across the country, many people who attend my antiques appraisal events are shocked to hear about some of the little-known methods used in major museums to preserve and protect precious art and antiques.\nWhile museums make a long-term commitment to preserving and protecting objects in their care to educate the public, most of us are equally committed to keeping our family heirlooms and keepsakes in good condition in order to retain their value.\nSome of the most common ways an object can be harmed include: pests and other insects, pollutants (dust, mold, etc.), temperature and humidity fluctuations, lights or sunshine, and oils from the human touch.\nFor instance, the oils on your hands and the hydrogen sulphide compounds in the air cause silver to tarnish and will leave a permanent mark on your valuable silver pieces.\nSigns that read \"Do not touch\" seem extreme but necessary when objects are on display in museums. When it comes to collectibles that we live with on a daily basis, it is a good idea to handle with care and handle only occasionally.\nSo, if you must handle an object, don't handle it too often. Remember, the oils and small dust particles on your hands can cause permanent damage to your heirlooms and aging treasures.\nIt is best to store your private collections in an area of your home where it is cool and dry.\nAttics (too hot with poor ventilation), basements (too damp), foyers (where the front door opens and closes often are bad because temperature changes are frequent), kitchens (too many cooking odors and too much heat), bathrooms and laundry rooms (too much moisture and possible mold) are not the best places for art or antiques.\nImproper climate conditions can stimulate mold growth and cause objects to mildew, dry out and crack. Never use harsh chemicals or abrasive pads to clean antique objects.\nHanging a framed print in a sunny window, storing objects in acidic cardboard boxes and over-cleaning your antiques can damage your pieces forever.\nSunlight is the first culprit that damages most works of art. Heat is a close second.\nPainted objects, prints and textiles should not be placed in sunny areas of your home as they are sensitive to light and will be damaged in a few short months.\nThere are few options to repair sun damage and fading once it happens. However, you can prevent heat from damaging your antiques. One of the hottest places where you display your collectibles is your china cabinet.\nThe glass doors act like a greenhouse and your objects are baking inside. Be sure to open those doors and let your objects get some good air flow every three months or so.\nSpray the rag, not the Renoir\nCleaning a framed work of art, such as a print, seems straightforward.\nHowever, there is a right way and a wrong way to clean it. Spray the rag first. Do not spray the cleaner directly onto the glass as the chemical could drip in between the glass and the work of art and damage it.\nBeware of bugs\nInsects are monsters, killers. They carry bacteria and they will eat and not stop eating until they have damaged your antique - particularly wooden ones - beyond recognition.\nYou may stop an infestation by wrapping a small wooden object in acid free tissue paper and placing the object in a freezer. The bugs will die off in the cold.\nAlso, bugs love dark spaces and close quarters.\nAn easy way to protect your antiques from insects is to clean around your objects regularly, don't eat food near your collectibles and use insect traps when necessary.\nCertain types of art and antiques need special types of care.\nBe diligent and handle your antiques carefully and you'll enjoy them for years to come.\nPh.D. antiques appraiser, author, and award-winning TV personality, Dr. Lori presents appraisal events to audiences worldwide."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:1c27f4ed-59dc-41da-a527-7219695f687f>","<urn:uuid:3eeb8572-d410-42c9-afd1-c1b6a8e44a3f>"],"error":null}
{"question":"How does the chemical composition of steel affect its properties, and what makes ship steel particularly valuable for recycling?","answer":"Steel's properties are controlled by combining iron with elements like carbon (0.2-2.1% by weight) and others such as manganese, chromium, vanadium, and tungsten. These elements act as hardening agents by preventing dislocations in the iron atom crystal lattice, affecting the steel's hardness, ductility, and tensile strength. Higher carbon content makes steel harder and stronger than iron, though less ductile. In the context of ship recycling, ship steel is particularly valuable because its chemical composition is controlled by classification society rules and surveys, resulting in good yield strength, ductility, and impact strength, making it attractive for steel making.","context":["Steel is an alloy made by combining iron and other elements, the most common of these being carbon. When carbon is used, its content in the steel is between 0.2% and 2.1% by weight, depending on the grade. Other alloying elements sometimes used are manganese, chromium, vanadium and tungsten.\nCarbon and other elements act as a hardening agent, preventing dislocations in the iron atom crystal lattice from sliding past one another. Varying the amount of alloying elements and the form of their presence in the steel (solute elements, precipitated phase) controls qualities such as the hardness, ductility, and tensile strength of the resulting steel. Steel with increased carbon content can be made harder and stronger than iron, but such steel is also less ductile than iron.\nAlloys with a higher than 2.1% carbon content are known as cast iron because of their lower melting point and good castability. Steel is also distinguishable from wrought iron, which can contain a small amount of carbon, but it is included in the form of slag inclusions. Two distinguishing factors are steel's increased rust resistance and better weldability.\nThough steel had been produced by various inefficient methods long before the Renaissance, its use became more common after more efficient production methods were devised in the 17th century. With the invention of the Bessemer process in the mid-19th century, steel became an inexpensive mass-produced material. Further refinements in the process, such as basic oxygen steelmaking (BOS), lowered the cost of production while increasing the quality of the metal. Today, steel is one of the most common materials in the world, with more than 1.3 billion tons produced annually. It is a major component in buildings, infrastructure, tools, ships, automobiles, machines, appliances, and weapons. Modern steel is generally identified by various grades defined by assorted standards organizations.\nOther articles related to \"steel\":\n... National Steel has several meanings National Steel (1929), a defunct steel production company in the United States National Steel Company (1899), part ... Steel National Steel and Shipbuilding Company, a shipyard in San Diego, California National Steel (album), a blues album by Colin James National Steel, a type of ...\n... Galvanized steel potable water supply and distribution pipes are commonly found with nominal pipe sizes from 3/8 to 2 ... Steel pipe has National Pipe Thread (NPT) standard tapered male threads, which connect with female tapered threads on elbows, tees, couplers, valves, and ... Galvanized steel (often known simply as \"galv\" or \"iron\" in the plumbing trade) is relatively expensive, and difficult to work with due to weight and requirement of a pipe ...\n... Main article Low-background steel Steel manufactured after World War II became contaminated with radionuclides due to nuclear weapons testing ... Low-background steel, steel manufactured prior to 1945, is used for certain radiation-sensitive applications such as Geiger counters and radiation shielding ...\n... The windmill was produced by the Steel Wings Company, in North Sydney between 1907 and 1911 with only six models ever erected ... The windmills comprise a steel frame and fan which turns to the wind between a bearing at the bottom and a swivel at the top, all supported by guy-wires ... The Jerilderie Steel Wings windmill, built in 1910, was transported by rail from Sydney and then taken by bullock wagon to Goolgumbula Station for Sir Samuel McCaughey ...\n... In 1933, the site was expanded to include steel production and the company was renamed Showa Steel Works ... industries developed around the iron and steel mills ... had become one of the largest producers of iron and steel in Asia if not the world ...\nFamous quotes containing the word steel:\n“For every man that Bolingbroke hath pressed\nTo lift shrewd steel against our golden crown,\nGod for his Richard hath in heavenly pay\nA glorious angel. Then if angels fight,\nWeak men must fall; for heaven still guards the right.”\n—William Shakespeare (15641616)\n“The finest workers in stone are not copper or steel tools, but the gentle touches of air and water working at their leisure with a liberal allowance of time.”\n—Henry David Thoreau (18171862)\n“For when they meet, the tensile air\nLike fine steel strains under the weight\nOf messages that both hearts bear\nPure passion once, now purest hate....”\n—Allen Tate (18991979)","This is a paper about recycling of ships, but it shows how much energy could be saved by recycling steel rather than making it from scratch with iron ore in blast furnaces (mainly powered by coal).\nShip Recycling markets and the impact of. April 2013. International Conference on Ship Recycling.\nConclusion: The annual average of 3.6 million tonnes of melting steel scrap from the global ship recycling industry is about 1.5% of the needs of the global steel making industry for old steel scrap, so the impact of ship recycling to the steel making industry is low and therefore can’t dictate pricing.\nThere are two main processes in modern steel making:\n- production from pig iron ore in a blast furnace, refined into steel in a Basic Oxygen Furnace (BOF). Some steel scrap is also added in the refining process. Around 70% of the world’s steel is produced this way\n- production from steel scrap in an Electric Arc Furnace (EAF), around 30% of the world’s steel production. The usage of steel scrap in steel making makes absolute sense, both from the economic and the environmental points of view. The energy requirements for making 1 tonne of steel from iron ore is 23 GJ as opposed to 7 GJ when using steel scrap. Also, recycling of steel saves natural resources. Every tonne of recycled steel saves around 1.1 tonnes of iron ore, 0.6 tonnes of coal, and reduces pollution: 86% less air pollution, 76% less water pollution, a 40% reduction in water usage, and avoidance of generation of about 1.3 tonnes of solid waste. Nevertheless, reliance on iron ore is unavoidable as steel scrap is available in relatively limited quantities.\nUsage of steel scrap in steel production. Contrasted to the world’s 70/30 mix (70% v 30%) of BOF and EAF in 2011, China’s mix was 90/10, India’s 40/60, and Turkey’s 25/75.\nThere are three sources of steel scrap for steel making :\n- 35% “own arisings” (a.k.a. “circulating scrap”, or “home scrap”) which arise internally in steel mills as rejects from melting, casting, rolling, etc;\n- (ii) 21% “new steel scrap” (or “process scrap”) which is generated when steel is fabricated into finished products; and\n- (iii) 44% “old steel scrap” (or “capital scarp”) which is steel scrap from obsolete products and which is collected, traded and sold to steel plants for remelting. Ship steel scrap obviously falls in the third category of sources of steel scrap. In recent times the market of old steel scrap is around 225 million tonnes annually\nShips are recycled primarily to recover their steel, which forms approximately 75% to 85% of a ship’s lightweight, or light ship weight (the mass of the ship’s structure, propulsion machinery, other machinery, outfit and constants).\nIn addition to steel, the recycling process recovers non-ferrous metals (i.e. copper), machinery, equipment, and fittings. Non-ferrous metals are particularly valuable and although just 1% of a ship’s LDT, they can recover for the recycler up to 10% to 15% of the price paid for the ship. Machinery from recycled ships is often reconditioned and sold for further use in maritime or land industries, or when it is beyond repair , it is cut and sold as steel scrap. Because the chemical composition of the steel used in shipbuilding is controlled by classification society rules and surveys, ship steel has good yield strength, ductility and impact strength. Ship steel scrap is therefore attractive for steel making."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:4a2d4b16-bc00-444e-aff8-229b926944ed>","<urn:uuid:08b758f9-78cb-451c-a06c-5efa33a42c80>"],"error":null}
{"question":"What's the proper way to arrange sprinkler heads for lawn coverage without wasting water on the street?","answer":"The best approach is to place sprinkler heads in an alternated pattern for proper head-to-head coverage. This is better than placing a single row of half-pattern heads along the sidewalk, which often results in overspraying onto the street and wasting water.","context":["Troubleshooting a Sprinkler System\nTips for fixing residential sprinkler systems\nBy Scott Cohen, garden artisan and LandscapingNetwork.com columnist\nThe heads overspray and fog up. What is wrong? If water pressure is too high, the sprinkler system actually becomes a mist system and fogs the plantings. The slightest breeze will blow the water away from its intended area. All sprinkler heads are designed to work best when operating under an optimal water pressure. Check the sprinkler head instructions and install a \"pressure regulator\" (a plumbing device that reduces water pressure to a lower consistent pressure) if one is required to meet the optimal numbers.\nPlanter Coverage Tip:\nUse an alternated pattern for watering planter areas as growing plants often block sprinkler coverage. Called \"front to back coverage\" this triangular sprinkler pattern helps saturate the entire flower bed during irrigation sessions.\nIs the sprinkler system leaking or is the sprinkler valve not shutting off properly?When a sprinkler line is broken you can often see a \"blow hole\" like a whale's spout in the general area of the break. Inspecting the line will show a spot where soil is being blown out as water bubbles up from below.\nIf an automatic sprinkler valve is not seating properly, the actuator can't seal because of some grit or sediment on the membranes, then water can slowly leak down the lines. Typically this water will secrete from the lowest sprinkler head on the line. If you find a constant wet spot at the lowest head on the line it is most likely a valve that needs repair and not a broken sprinkler line.\nIrrigation controller location dilemma: Should I place my irrigation controller inside or out?Here's the scenario - You are a couple that travels a lot and you would like the maintenance gardener or a neighbor to be able to change the timer without access into the house, so the best place to put the timer is outdoors, right? Not so fast. If it's raining you need to go outside in the rainstorm to shut down the timer so you don't waste water. It sure would have been easier to turn off the timer if it had been installed in the garage.\nChoose timer location based on easiest access for the person that will need to set it most, and don't forget about rainy weather.\nDon't do this: To get adequate lawn coverage, this sprinkler pattern over-sprays adjacent cars and wastes water down the street.\nThe Green Scene in Northridge, CA\nAre you providing a free car wash when you water the parkway lawn? I hate it when I have just had my truck washed and I park next to parkway lawn strip and the sprinklers are installed wrong. The sprinkler installer placed one row of heads along the sidewalk side of the street using half pattern heads. In order to get adequate water on the lawn the heads all overspray the street. My timing is always perfect and when the lawn sprinklers turn on I get a free car wash I didn't want.\nThis is one the most common mistakes I see in residential sprinkler design. It's also a terrible waste of precious water and it is easy to avoid.\nPlace sprinkler heads in an alternated pattern for proper head-to-head coverage and reduced water waste. Best of all, no free car washes!\nScott Cohen is a nationally acclaimed garden artisan, author, speaker, and HGTV television personality.\nLearn how to save water and have a thriving lawn\nGet an overview of the sprinkler installation process\nGet seasonal sprinkler care tips from the pros\nReturn to Sprinkler Systems"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:73bdd180-8147-4367-bcca-cfbccfce06ca>"],"error":null}
{"question":"Do both the RACE Framework and Seven Ss Framework equally emphasize soft and hard elements in organizational success?","answer":"No, the frameworks differ in their emphasis on soft and hard elements. The RACE Framework primarily focuses on hard, measurable metrics like revenue per visitor, conversion rates, and KPIs tracked through Google Analytics. In contrast, the Seven Ss Framework explicitly balances both hard elements (strategy, structure, and systems) and soft elements (skills, shared values, staff, and style). The Seven Ss Framework specifically notes that Japanese companies' success was attributed to their ability to combine both hard and soft elements effectively, while Western companies typically excelled only at the hard elements.","context":["RACE Framework Explained\nThe RACE Framework is a proven multichannel marketing method that helps you define your business objectives and target your audience. This framework helps you determine the best business activities, including the creation of Facebook ads that target a specific audience. It also helps you measure your success in each step, including customer journeys, brand advocacy, and repeat purchases. Learn more about the RACE Framework below. Let’s look at some of the key components of a successful marketing plan.\nFirst, we’ll look at the Reach stage. This is where you build brand awareness and introduce your brand to your target audience. This stage is essential when your product or service is new, and your goal is to maximize exposure in order to get the best return on investment. You can use paid, owned, and earned media to accomplish this. In addition, you can integrate your campaign with other activities to drive more traffic to your web properties.\nThe RACE Framework also defines what constitutes an effective marketing strategy. Essentially, each stage has a common goal – conversion. In other words, the ultimate goal is to make a customer pay for your product or service. The REAN framework includes four main steps, based on which you can measure success and make adjustments. The RACE Framework helps you understand how your marketing campaigns are going and see how they’re doing.\nTo measure your ROI, define KPIs and objectives. Each stage of the RACE Framework has specific KPIs. Ideally, each KPI should be tied to revenue per visitor. For example, you should measure revenue per click from media outlets, search engines, and social media. In addition, you should tie your website KPIs to revenue per visitor. This way, you’ll be able to see how your website progresses over time and how much value you’re generating from each visitor.\nThe RACE Framework outlines the stages of a customer’s journey. For instance, RACE planning allows healthcare providers to use search engine insights to determine which strategies are most effective. By understanding each step, healthcare brands can use this framework to engage with their audience and foster long-term relationships with their patients. By implementing an effective digital marketing strategy, doctors can attract new patients and retain existing ones. If you follow these steps, your practice will grow and remain profitable.\nThe RACE Framework outlines four phases of online marketing activities that help brands engage with customers throughout the customer lifecycle. The initial phase of the Plan involves defining an integrated digital strategy, establishing governance, and resourcing for the transformation. Following these steps, the marketing plan can be evaluated to determine if it is delivering the desired results. When done properly, it will increase your brand’s conversion rates, drive website traffic, and improve your bottom line.\nThe RACE Framework uses Google Analytics to create KPIs. Google Analytics requires customization to meet the needs of each business, but many of them can be calculated using the RACE Framework’s online dashboard. The monthly reporting dashboard is interactive and uses Google Analytics’ API to create Google Docs Sheets reports. Once created, it will be easy to track the performance of each of these KPIs. You can even automate your reporting process with Google Analytics by using the RACE framework.\nA successful marketing strategy combines the various channels to reach your target audience. For example, using Facebook advertising to promote your brand is essential. This strategy can help you measure the effectiveness of your advertising and reach goals in a more efficient manner. For the most effective marketing campaign, you should also integrate your digital channels with traditional offline media, such as print ads. The RACE framework can help you plan your marketing efforts using all of these channels.\nA better marketing strategy should incorporate data-driven metrics. Using data-driven metrics, you can measure the success of a campaign and monitor your competitors’ online marketing efforts. In fact, the RACE Framework is an indispensable guide for businesses of all sizes, and even small businesses can benefit from using it. In fact, it’s a universal framework for all businesses, regardless of size. It can be used to optimize your marketing efforts for any industry or type of business.","The Seven Ss is a framework developed in the late 1970s and early 1980s for analyzing organizations and looking at the various elements that make them successful (or not). The framework has seven aspects, each of them beginning with the letter S, hence the mnemonic:\n1. Strategy: the route that the organization has chosen for its future growth.\n2. Structure: the way in which the organization is put together; how its different bits relate to each other.\n3. Systems: the formal and informal procedures that govern everyday activity; today this increasingly involves the implementation of information technology.\n4. Skills: the distinctive capabilities of the people who work for the organization.\n5. Shared values: originally called superordinate goals, the things that influence a group to work together for a common aim.\n6. Staff: the organization’s human resources.\n7. Style: the way in which the organization’s employees present themselves to the outside world, to suppliers and customers in particular.\nThe Seven Ss helped change managers’ thinking about how companies could be improved. The theory told them that it was not just a matter of devising a new strategy and following it through (as they might have thought before). Nor was it a matter of setting up new systems and letting them generate improvements. To improve, companies had to pay attention to all seven of the Ss at the same time.\nThe seven were often subdivided into the first three (strategy, structure and systems), referred to as the hard Ss, and the last four, called the soft Ss. All seven are interrelated, so a change in one has a ripple effect on all the others. Hence it is impossible to make progress on one without making progress on all of them.\nThe theory was developed in the context of the astoundingly rapid progress of Japanese manufacturing companies in the 1960s and 1970s. Western companies, it was said, were better at the hard Ss. But it was because the Japanese combined both hard and soft that they were so much more successful.\nDiagrammatically, the seven are usually represented in a circle to convey the idea that they are all of equal significance. No one of them is more important than any other, although Richard Pascal (see article ), the theory’s champion, subsequently gave a special status to super ordinate goals (also known as shared values). These, he said, “provide the glue that holds the other six together”. This positioning of super ordinate goals at the centre of the circle stimulated some of the subsequent work on corporate culture, since culture is to some extent a combination of an organization’s super ordinate goals and its style.\nJust as the growth share matrix is powerfully associated with one of the leading strategy consultancies (the Boston Consulting Group), so the Seven Ss is linked with another (McKinsey & Company). It was the seed corn from which grew the idea of excellence and one of the most popular business books ever written (”In Search of Excellence”). Excellent companies were those that excelled in all of the Seven Ss. Pascal subsequently expanded the idea in his book “The Art of Japanese Management”, in which he compared a Japanese company, Matsushita, with an American company, ITT, greatly to the credit of the former."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:39d31e34-5b33-412f-a7a5-48f95e0ce8e6>","<urn:uuid:d626cf6f-1282-4e50-937a-b66a629ad096>"],"error":null}
{"question":"What specific factors does the Third Circuit court consider when evaluating trademark infringement cases?","answer":"The Third Circuit, as established in Scott Paper Co. v. Scott's Liquid Gold, considers ten factors: (1) similarity between owner's mark and alleged infringing mark; (2) strength of owner's mark; (3) price and factors indicating consumer care in purchases; (4) length of defendant's mark usage without actual confusion; (5) defendant's intent in adopting the mark; (6) evidence of actual confusion; (7) whether goods are marketed through same channels and media; (8) similarity of sales targets; (9) relationship of goods in public mind due to similar function; (10) facts suggesting public might expect prior owner to manufacture in defendant's market.","context":["U.S. Trademark Law Edit\nTrademark registration Edit\nLikelihood of confusion is a statutory basis for refusing registration of a trademark or service mark because it is likely to conflict with a mark or marks already registered or pending before the U.S. Patent and Trademark Office. After an application is filed, the assigned examining attorney will search the USPTO records to determine if such a conflict exists between the mark in the application and another mark that is registered or pending before the USPTO. The USPTO will not conduct any preliminary searches for conflicting marks before an applicant files an application and cannot provide legal advice on whether a particular mark can be registered.\nThe principal factors considered by the examining attorney in determining whether there is a likelihood of confusion are: (1) the similarity of the marks; and (2) the commercial relationship between the goods and/or services listed in the application.\nTo find a conflict, the marks do not have to be identical, and the goods and/or services do not have to be the same. It may be enough that the marks are similar and the goods and/or services related. If a conflict exists between your mark and a registered mark, the examining attorney will refuse registration on the ground of likelihood of confusion.\nTrademark infringement Edit\nLikelihood of confusion is one of the elements a plaintiff must plead and prove to establish trademark infringement. Each federal circuit court of appeals has its own multi-factor test for evaluating likelihood of confusion necessary to ground a trademark infringement claim. While the articulation of the factors varies somewhat, all of the tests address the same basic types of factors. These factors include the similarity of the marks, the similarity or relationship of the respective goods and/or services, the strength (inherent and marketplace) of the asserted mark, the commonality of trade channels and advertising methods, the sophistication of purchasers, whether the accused mark was adopted in bad faith, and the existence of actual confusion.\nAlthough no one factor is necessarily controlling, two key factors are the similarity between the marks and the proximity of the goods and/or services. Average purchasers retain only a general, rather than specific, impression of trademarks. Thus, to qualify as “similar,” marks need not be identical. Rather, the marks need only be sufficiently similar in the overall commercial impression they convey (e.g., they share sufficient similarities in one or more of the following factors: appearance, sound, or meaning). Likewise, the respective goods/services do not have to be identical or even competitive, and need only be related (e.g., they are of the same type, in the same field, used together, or marketed through the same channels of trade). Generally speaking, the more similar the marks, the less related the goods and/or services need to be to find a likelihood of confusion and the less similar the marks, the more related the goods and/or services need to be to find a likelihood of confusion.\nThird Circuit Edit\nIn Scott Paper Co. v. Scott's Liquid Gold, the Third Circuit set forth the following factors to be considered:\n|“||(1) the degree of similarity between the owner's mark and the alleged infringing mark; (2) the strength of owner's mark; (3) the price of the goods and other factors indicative of the care and attention expected of consumers when making a purchase; (4) the length of time the defendant has used the mark without evidence of actual confusion arising; (5) the intent of the defendant in adopting the mark; (6) the evidence of actual confusion; (7) whether the goods, though not competing, are marketed through the same channels of trade and advertised through the same media; (8) the extent to which the targets of the parties' sales efforts are the same; (9) the relationship of the goods in the minds of the public because of the similarity of function; (10) other facts suggesting that the consuming public might expect the prior owner to manufacture a product in the defendant's market.||”|\nSecond Circuit Edit\nThe Second Circuit, in Polaroid Corp. v. Polarad Elecs. Corp., set forth the following factors:\n|“||the strength of his mark, the degree of similarity between the two marks, the proximity of the products, the likelihood that the prior owner will bridge the gap, actual confusion, and the reciprocal of defendant's good faith in adopting its own mark, the quality of defendant's product, and the sophistication of the buyers.||”|\nHowever, while a trial court considering the likelihood of confusion must evaluate the Polaroid factors, the Second Circuit has cautioned that the Polaroid factors are not always dispositive. Moreover, courts may consider other variables in evaluating the likelihood of confusion, and irrelevant factors may be abandoned. The unique facts of each case must be considered in evaluating the likelihood of confusion.\n- ↑ 15 U.S.C. §1052(d).\n- ↑ \"In order to succeed on the merits, a plaintiff must establish that: (1) the marks are valid and legally protectible; (2) the marks are owned by the plaintiff; and (3) the defendants' use of the marks to identify goods or services is likely to create confusion concerning the origin of the goods and services.\" Opticians Ass'n v. Independent Opticians, 920 F.2d 187, 192 (3d Cir. 1990) (full-text).\n- ↑ See 4 J. Thomas McCarthy, McCarthy on Trademarks and Unfair Competition §§ 24:30-24:43 (4th ed. 2010) (listing factors by circuit).\n- ↑ 589 F.2d 1225 (3d Cir. 1978) (full-text).\n- ↑ Id. at 1229.\n- ↑ 287 F.2d 492 (2d Cir. 1961) (full-text).\n- ↑ Id. at 495.\n- ↑ Streetwise Maps, Inc. v. VanDam, Inc., 159 F.3d 739 (2d Cir. 1998) (full-text); Estee Lauder Inc. v. The Gap. Inc., 108 F.3d 1503 (2d Cir. 1997) (full-text).\n- ↑ See Gruner + Jahr USA Publishing v. Meredith Corp., 991 F.2d 1072, 1077 (2d Cir. 1993) (full-text).\n- ↑ W.W.W. Pharm. Co., Inc. v. Gillette Co., 984 F.2d 567, 572 (2d Cir. 1993) (full-text); Thompson Med. Co. v. Pfizer Inc., 753 F.2d 208, 214 (2d Cir. 1985) (full-text) (\"[T]he complexities attendant to an accurate assessment of likelihood of confusion require that the entire panoply of elements constituting the relevant factual landscape be comprehensively examined. No single Polaroid factor is pre-eminent, nor can the presence or absence of one without analysis of the others, determine the outcome of an infringement suit.\")"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:3975d554-8e92-41f7-bc60-e61da55412a0>"],"error":null}
{"question":"As medical student learning about cardiac procedures, what is main difference in how effectiveness was measured between PCI study and valvulotomized vein graft study?","answer":"The two studies measured effectiveness through different primary metrics. The PCI study primarily measured effectiveness through a composite outcome of all-cause death or heart failure hospitalization, along with changes in left ventricular ejection fraction and quality of life scores. Meanwhile, the valvulotomized vein graft study primarily measured effectiveness through graft patency rates (assessed via multislice computed tomography and coronary angiography) and clinical outcomes like survival and MACCE-free rates.","context":["Percutaneous coronary intervention (PCI) with optimum medical remedy (OMT) doesn’t extend survival or enhance ventricular perform in contrast with OMT alone in sufferers with extreme ischemic cardiomyopathy, in line with outcomes from the REVIVED-BCIS2 trial.\nThe first composite final result of all-cause loss of life or heart failure hospitalization occurred in 37.2% of the PCI group and 38% of the OMT group (hazard ratio [HR], 0.99; P = .96) over a median of three.Four years follow-up. The remedy impact was constant throughout all subgroups.\nThere have been no important variations in left ventricular ejection fraction (LVEF) at 6 and 12 months.\nHigh quality of life scores favored PCI early on however there was catch-up over time with medical remedy, and this benefit disappeared by 2 years, principal investigator Divaka Perera, MD, King’s School London, United Kingdom, reported in the present day on the European Society of Cardiology (ESC) 2022 Congress.\n“The takeaway is that we shouldn’t be providing PCI to sufferers who’ve steady, well-medicated left ventricular dysfunction,” Perera instructed theheart.org | Medscape Cardiology. “However we must always nonetheless contemplate revascularization in sufferers presenting with acute coronary syndromes or who’ve a number of angina, as a result of they weren’t included within the trial.”\nThe examine, published simultaneously within the New England Journal of Drugs, supplies the primary randomized proof on PCI for ischemic cardiomyopathy.\nRevascularization pointers in america make no advice for PCI, whereas these in Europe suggest coronary artery bypass grafting (CABG) first for sufferers with multivessel illness (class 1) and have a category 2a, stage of proof C indication for PCI in choose sufferers. US and European coronary heart failure pointers additionally help guideline directed remedy and CABG in choose sufferers with ejection fractions of 35% or much less.\nThis steering is predicated on consensus opinion and the STICH trial, during which CABG plus OMT failed to supply a mortality profit over OMT alone at 5 years however improved survival at 10 years within the extension STICHES study.\n“Medical remedy for coronary heart failure works and this trial’s outcomes are one other essential reminder of that,” stated Eric Velazquez, MD, who led STICH and was invited to touch upon the findings.\nMortality will solely get higher with the usage of SGLT2 inhibitors, he famous, which weren’t included within the trial. Utilization of ACE inhibitors/ARBs/ARNIs and beta blockers was much like STICH and glorious in REVIVED. “They did do a greater job in utilization of ICD and CRTs than the STICH trial and I believe that must be explored additional concerning the affect of these modifications.”\nNonetheless, ischemic cardiomyopathy sufferers have “unacceptably excessive mortality,” with the noticed mortality about 20% at Three years and about 35% at 5 years, stated Velazquez, with Yale College in New Haven, Connecticut.\nIn most coronary heart failure trials, HF hospitalization drives the first composite endpoint however the reverse was true right here and in STICH, he noticed. “You had twice the chance of dying through the 3.Four years than you probably did of being hospitalized for coronary heart failure, and it is essential that that could be a distinction we should notice is clear in our ischemic cardiomyopathy sufferers.”\nThe findings will doubtless not result in a change within the pointers, he added. “I believe we proceed as establishment for now and get extra information.”\nRegardless of the shortage of randomized proof, he cautioned that PCI is more and more carried out in sufferers with ischemic cardiomyopathy, with registry information suggesting practically 60% of sufferers acquired the process.\nReached for remark, Clyde Yancy, MD, chief of cardiology and vice dean of variety & inclusion at Northwestern College Feinberg Faculty of Drugs in Chicago, stated, “For now, the present pointers are right. Greatest software of guideline-directed medical and machine remedy is the gold commonplace for coronary heart failure and that features coronary heart failure because of ischemic etiologies.\n“Do these information resolve the query of revascularization within the setting of coronary illness and lowered EF coronary heart failure? Hardly,” he added. “Medical judgment should prevail and the place acceptable, coronary revascularization stays a consideration. However it’s not a panacea.”\nBetween August 2013 and March 2020, REVIVED-BCIS2 enrolled 700 sufferers at 40 UK facilities who had an LVEF of 35% or much less, intensive CAD (outlined by a British Cardiovascular Intervention Society myocardial Jeopardy Rating [BCIS-JS] of not less than 6), and viability in not less than 4 myocardial segments amenable to PCI. Sufferers had been evenly randomly assigned to individually adjusted pharmacologic and machine remedy for coronary heart failure alone or with PCI.\nThe typical age was about 70, solely 12.3% girls, 344 sufferers had 2-vessel CAD, and 281 had 3-vessel CAD. The imply LVEF was 27% and median BCIS-JS rating 10.\nThroughout follow-up, which reached 8.5 years in some sufferers as a result of lengthy enrollment, 31.7% of sufferers within the PCI group and 32.6% sufferers within the OMT group died from any trigger and 14.7% and 15.3%, respectively, had been admitted for coronary heart failure.\nLVEF improved by 1.8% at 6 months and a pair of% at 12 months within the PCI group and by 3.4% and 1.1%, respectively, within the OMT group. The imply between-group distinction was -1.6% at 6 months and 0.9% at 12 months.\nWith regard to high quality of life, the Kansas Metropolis Cardiomyopathy Questionnaire general abstract rating favored the PCI group by 6.5 factors at 6 months and by 4.5 factors at 12 months, however by 24 months the between-group distinction was 2.6 factors (95% CI, -0.7 to five.8). Scores on the EuroQol Group 5-Dimensions 5-Stage Questionnaire adopted an identical sample.\nUnplanned revascularization was extra widespread within the OMT group (HR, 0.27; 95% CI, 0.13 – 0.53). Acute myocardial infarction charges had been related within the two teams (HR, 1.01, 95% CI, 0.64 – 1.60), with the PCI group having extra periprocedural infarcts and barely fewer spontaneous infarcts.\nDoable causes for the discordant outcomes between STICH and REVIVED are the threefold extra mortality inside 30 days of CABG, whereas no such early hit occurred with PCI, lead investigator Perera stated in an interview. Medical remedy has additionally developed over time and REVIVED enrolled a extra “real-world” inhabitants, with a median age near 70 years vs 59 in STICH.\n“Modest” Diploma of CAD?\nAn accompanying editorial, nonetheless, factors out that regardless of appreciable ventricular dysfunction, about half the sufferers in REVIVED had solely 2-vessel illness and a median of two lesions handled.\n“This comparatively modest diploma of coronary artery disease appears uncommon for sufferers chosen to bear revascularization with the hope of restoring or normalizing ventricular perform,” writes Ajay Kirtane, from Columbia College Irving Medical Heart, NewYork-Presbyterian Hospital, New York Metropolis.\nHe stated extra particulars are wanted on completeness of the revascularization, severity of stenosis, physiologic evaluation of the lesion and, “most significantly, the correlation of stenosis with earlier ischemic or viability testing.”\nRequested concerning the editorial, Perera agreed that info on the kind of revascularization and myocardial viability are essential and stated they hope to share analyses of the one not too long ago unblinded information on the American School of Cardiology assembly subsequent spring. Importantly, about 71% of viability testing was finished by cardiac MR and the remainder largely by dobutamine stress echocardiogram.\nHe disagreed, nonetheless, that individuals had comparatively modest CAD based mostly on the 2- or 3-vessel classification and stated the median rating on the extra granular BCIS-JS was 10, with most 12 indicating your complete myocardium is equipped by diseased vessels.\nThe trial additionally included virtually 100 sufferers with left important illness, a bunch not included in earlier medical remedy trials together with STICH and ISCHEMIA, Perera famous. “So, I believe it was fairly, fairly extreme coronary illness however a cohort that was higher handled medically.”\nGeorge Dangas, MD, PhD, a professor of medication at Icahn Faculty of Drugs at Mount Sinai in New York Metropolis, stated the examine supplies precious info but additionally expressed considerations that the power coronary heart failure within the trial was way more superior than the CAD.\n“Signs are low stage and that is predominantly associated to CHF, and when you handle the CHF one of the simplest ways with superior therapies, help machine or transplant or another means, that may take precedence over the CAD lesions,” stated Dangas, who was not related to REVIVED. “I’d count on CAD lesions would have extra significance if we transfer into the category Three or larger of symptomatology, and, once more on this examine, that was not [present] in over 70% of the sufferers.”\nThe examine was funded by the Nationwide Institute for Well being and Care Analysis’s Well being Know-how Evaluation Program. Perera, Velazquez and Dangas report no related monetary relationships.\nKirtane studies grants, nonfinancial help and different from Medtronic, Abbott Vascular, Boston Scientific, Abiomed, CathWorks, Siemens, Philips, ReCor Medical, Cardiovascular Methods Included, Amgen, and Chiesi. He studies grants and different from Neurotronic, Magental Medical, Canon, SoniVie, Shockwave Medical, and Merck. He additionally studies nonfinancial help from Opsens, Zoll, Regeneron, Biotronik, and Bolt Medical, and private charges from IMDS.\nEuropean Society of Cardiology Congress 2022. Introduced August 27, 2022.\nN Engl J Med. Revealed August 27, 2022. Abstract, Editorial\nComply with Patrice Wendling on Twitter: @pwendl. For extra from theheart.org | Medscape Cardiology, be a part of us on Twitter and Facebook.","Mid-Term Clinical Outcome of Patients Undergoing Coronary Artery Bypass Grafting with Valvulotomized Vein Grafts\nBackground: The lower patency rate of vein grafts (VG) in comparison to arterial grafts may be related to vein valves, which favor turbulences and thrombosis that lead to graft failure. The aim of this study was to determine the outcome of patients with valvulotomized VG after coronary artery bypass grafting (CABG) procedure.\nMethods: From 2007 to 2014, 233 patients with a mean age of 67 ± 9 years had CABG or combined CABG and valve procedures. Valvulotomized saphenous VG and arterial grafts were used. Clinical follow-up and outcome were evaluated after 6.3 ± 2 years. The graft patency was rated with multislice computed tomography in 57 patients and coronary angiography in 29 patients 3.1 ± 2 years postoperatively.\nResults: Overall, 168 patients had segregated CABG surgery, and 65 patients received additional procedures, with mean 2.7 ± 1 arterial and 1.5 ± 0.7 venous anastomoses. The 30-day-mortality in isolated CABG patients was 2%. Survival at five years was 80%. Major adverse cardiac and cerebrovascular events (MACCE) free rate at five years was 80%. At the last follow up (mean 6.3 years), 94% of the patients were in Canadian Cardiovascular Society (CCS) class 0. The quote of patent valvulotomized VG was 96.1% compared to a patency rate of 96.7% for the arterial grafts in the subgroup undergoing angiography or computed tomography of the heart.\nConclusion: Our data demonstrate good mid-term results of graft patency, and comparable clinical results in patients undergoing CABG with valvulotomized VG. A longer follow-up period and a higher number of bypass graft imaging examinations are necessary to affirm our results.\nAthanasiou T, Saso S, Rao C, et al. 2011. Radial artery versus saphenous vein conduits for coronary artery bypass surgery: forty years of competition – which conduit offers better patency? A systematic review and meta-analysis. Eur J Cardiothorac Surg 40:208-20.\nButtar SN, Yan TD, Taggart DP, et al. 2017. Long-term and short-term outcomes of using bilateral internal mammary artery grafting versus left internal mammary artery grafting: a meta-analysis. Heart 103:1419-1426.\nCalafiore AM, Contini M, Vitolla G, et al. 2000. Bilateral internal thoracic artery grafting: long-term clinical and angiographic results of in situ versus Y grafts. J Thorac Cardiovasc Surg 120:990-6.\nCalafiore AM, Teodori G, Bosco G, et al. 1996. Intermittent antegrade warm blood cardioplegia in aortic valve replacement. J Card Surg 11:348-354.\nGlineur D, D´hoore W, Price J, et al. 2012. Survival benefit of multiple arterial grafting in a 25-year single institutional experience: the importance of the third arterial graft. Eur J Cardiothorac Surg 42:284-90\nGlineur D, Hanet C, D´hoore W, et al. 2009. Causes of non-functioning right internal mammary used in a Y-graft configuration: insight from a 6-month systematic angiographic trial. Eur J Cardiothorac Surg 36:129-136.\nHayward PA, Gordon IR, Hare DL, et al. 2010. Comparable patencies of the radial artery and right internal thoracic artery or saphenous vein beyond 5 years: results from the Radial Artery Patency and Clinical Outcomes trial. J Thorac Cardiovasc Surg 139:60-5.\nJoannides R, Haefeli W, Linder L, et al. 1995. Nitric oxide is responsible for flow-dependent dilatation of human peripheral conduit arteries in vivo. Circulation 91:1314-9.\nKhot UN, Friedman DT, Pettersson G, et al. 2004. Radial artery bypass grafts have an increased occurrence of angiographically severe stenosis and occlusion compared with left internal mammary arteries and saphenous vein grafts. Circulation 109:2086-91.\nLajos TZ, Robicsek F, Thubrikar M, et al. 2007. Improving patency of coronary conduits “valveless” veins and/or arterial grafts. J Card Surg 22:170-7.\nLytle BW, Blackstone EH, Loop FD, et al. 1999. Two internal thoracic artery grafts are better than one. J Thorac Cardiovasc Surg 117:855-72.\nMills NL. 1989. Saphenous vein graft valves: “The bad guys”. Ann Thorac Surg 48:613-4.\nMohr FW, Morice MC, Kappetein AP, et al. 2013. Coronary artery bypass graft surgery versus percutaneous coronary intervention in patients with three-vessel disease and left main coronary disease: 5-year follow up of the randomized, clinical SYNTAX trial. Lancet 381:629-38.\nMolina JE. 1989. Nonreversed saphenous vein grafts for coronary artery bypass grafting. Ann Thorac Surg 48:624-7.\nMonsefi N, Zierer A, Honarpisheh G, et al. 2016. One-year patency of valvulotomized vein grafts is similar to that of arterial grafts. Thorac Cardiovasc Surg 64:204-10.\nMukherjee D, Cheriyan J, Kourliouros A, et al. 2012. Does the right internal thoracic artery or saphenous vein graft offer superior revascularization of the right coronary artery? Interact Cardiovasc Thorac Surg 15:244-7.\nNaik MJ, Abu-Omar Y, Alvi A, et al. 2003. Total arterial revascularisation as a primary strategy for coronary artery bypass grafting. Postgrad Med J 79:43-8.\nRisteski PS, Akbulut B, Moritz A, et al. 2006. The radial artery as a conduit for coronary artery bypass grafting: review of current knowledge. Anadolu Kardiyol Derg 6:153-62.\nShah PJ, Bui K, Blackmore S, et al. 2005. Has the in situ right internal thoracic artery been overlooked? An angiographic study of the radial artery, internal thoracic arteries and saphenous vein graft patencies in symptomatic patients. Eur J Cardiothorac Surg 27:870-5.\nTaggart DP, D`Amico R, Altman DG. 2001. Effect of arterial revascularisation on survival: a systematic review of studies comparing bilateral and single internal mammary arteries. Lancet 358:870-5.\nThubrikar MJ, Robicsek F, Fowler BL, et al. 1994. Pressure trap created by vein valve closure and its role in graft stenosis. J Thorac Cardiovasc Surg 107:707-16.\nWimmer-Greinecker G, Yosseef-Hakimi M, Rinne T, et al. 1999. Effect of internal thoracic artery preparation on blood loss, lung function, and pain. Ann Thorac Surg 6:153-162.\nAuthor Disclosure & Copyright Transfer Agreement\nIn order to publish the original work of another person(s), The Heart Surgery Forum® must receive an acknowledgment of the Author Agreement and Copyright Transfer Statement transferring to Forum Multimedia Publishing, L.L.C., a subsidiary of Carden Jennings Publishing Co., Ltd. the exclusive rights to print and distribute the author(s) work in all media forms. Failure to check Copyright Transfer agreement box below will delay publication of the manuscript.\nA current form follows:\nThe author(s) hereby transfer(s), assign(s), or otherwise convey(s) all copyright ownership of the manuscript submitted to Forum Multimedia Publishing, LLC (Publisher). The copyright transfer covers the exclusive rights to reproduce and distribute the article and the material contained therein throughout the world in all languages and in all media of expression now known or later developed, including but not limited to reprints, photographic reproduction, microfilm, electronic data processing (including programming, storage, and transmission to other electronic data record(s), or any other reproductions of similar nature), and translations.\nHowever, Publisher grants back to the author(s) the following:\n- The right to make and distribute copies of all or part of this work for use of the author(s) in teaching;\n- The right to use, after publication in The Heart Surgery Forum, all or part of the material from this work in a book by the author(s), or in a collection of work by the author(s);\n- The royalty-free right to make copies of this work for internal distribution within the institution/company that employs the author(s) subject to the provisions below for a work-made-for-hire;\n- The right to use figures and tables from this work, and up to 250 words of text, for any purpose;\n- The right to make oral presentations of material from this work.\nPublisher reserves the right to grant or refuse permission to third parties to republish all or part of the article or translations thereof. To republish, such third parties must obtain written permission from the Publisher. (This is in accordance with the Copyright Statute, United States Code, Title 17. Exception: If all authors were bona fide officers or employees of the U.S. Government at the time the paper was prepared, the work is a “work of the US Government” (prepared by an officer or employee of the US Government as part of official duties), and therefore is not subject to US copyright; such exception should be indicated on signature lines. If this work was prepared under US Government contract or grant, the US Government may reproduce, royalty-free, all or portions of this work and may authorize others to do so, for official US Government purposes only, if the US Government contract or grant so requires.\nI have participated in the conception and design of this work and in the writing of the manuscript and take public responsibility for it. Neither this manuscript nor one with substantially similar content under my authorship has been published, has been submitted for publication elsewhere, or will be submitted for publication elsewhere while under consideration by The Heart Surgery Forum, except as described in an attachment. I have reviewed this manuscript (original version) and approve its submission. If I am listed above as corresponding author, I will provide all authors with information regarding this manuscript and will obtain their approval before submitting any revision. I attest to the validity, accuracy, and legitimacy of the content of the manuscript and understand that Publisher assumes no responsibility for the validity, accuracy, and legitimacy of its content. I warrant that this manuscript is original with me and that I have full power to make this Agreement. I warrant that it contains no matter that is libelous or otherwise unlawful or that invades individual privacy or infringes any copyright or other proprietary right. I agree to indemnify and hold Publisher harmless of and from any claim made against Publisher that relates to or arises out of the publication of the manuscript and agree that this indemnification shall include payment of all costs and expenses relating to the defense of any such claim, including all reasonable attorney’s fees.\nI warrant that I have no financial interest in the drugs, devices, or procedures described in the manuscript (except as disclosed in the attached statement).\nI state that the institutional Human Subjects Committee and/or the Ethics Committee approved the clinical protocol reported in this manuscript for the use of experimental techniques, drugs, or devices in human subjects and appropriate informed consent documents were utilized.\nFurthermore, I state that any and all animals used for experimental purposes received humane care in USDA registered facilities in compliance with the “Principles of Laboratory Animal Care” formulated by the National Society for Medical Research and the “Guide for the Care and Use of Laboratory Animals” prepared by the Institute of Laboratory Animal Resources and published by the National Institutes of Health (NIH Publication No. 85-23, revised 1985)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c5534dbc-372a-4d61-8b2f-4b9ec2dd0341>","<urn:uuid:8f3b9aab-0adb-4381-9280-b7d9727d0412>"],"error":null}
{"question":"What are the main treatment approaches for type 2 diabetes mellitus, and how do ACE inhibitors interact with DPP-4 inhibitors in diabetic patients with cardiovascular conditions?","answer":"The main treatments for type 2 diabetes mellitus include diet corresponding to table 9 for Pevzner, exercise, and medications. For most obese patients with mild type 2 diabetes, metformin is the first-line agent, and if inadequate, a second agent should be added. Regarding ACE inhibitors and DPP-4 inhibitors, while there were initial concerns about potential adverse hemodynamic effects when these medications are used together, research from the EXAMINE trial showed no increased rate of cardiovascular events in patients taking both medications compared to those not taking ACE inhibitors. The study found comparable composite rates of major cardiovascular events between alogliptin (a DPP-4 inhibitor) and placebo groups, regardless of ACE inhibitor use. Additionally, no significant changes were observed in blood pressure and heart rate in patients taking both medications.","context":["Treatment of type 2 diabetes mellitus. For the treatment of type 2 diabetes mellitus, men use diet , exercise , special drugs in tablets and insulin preparations. The diet corresponds to the 9 table for Pevzner. The purpose of this study is to determine whether laparoscopic removal of the omentum (thin layer of fat inside the abdomen) will significantly improve insulin resistance in patients with non-insulin dependent type 2 diabetes mellitus. Diabetes mellitus type 2. Classification and external resources.A meta-analysis of randomized controlled trials by the Cochrane Collaboration found \"only a minor clinical benefit of treatment with long-acting insulin analogues for patients with diabetes mellitus type 2\". More recently, a Adding linagliptin to treatment in patients with type 2 diabetes mellitus that has been inadequately controlled with a metformin and sulfonylurea combination improves glycemic control. Diabetes Mellitus Treatment. July 07.2010 - Umesh Masharani, MB, BS, MRCP(UK).For most obese patients with mild type 2 diabetes, metformin is the first-line agent. If it proves to be inadequate, then a second agent should be added. Читать работу online по теме: Цветной атлас по фармакологии 2005. ВУЗ: ЧГУ. Предмет: [НЕСОРТИРОВАННОЕ]. Размер: 10.6 Mб. Type II diabetes mellitus is a major global health problem and there is ongoing research for new treatments to. manage the disease. The glucagon-like peptide-1 receptor (GLP-1R) controls the physiological response to the incretin peptide, GLP- 1, Sitglaptin, is a DPP Опыт применения ситаглиптина (первого ингибитора ДПП-4) в лечении сахарного диабета 2 типа в Российской Федерации: результаты наблюдательной программы диа-Да? Diabetes Mellitus Type 2. What is it? Diabetes is a common health problem in the U.S. and the world.Treatment and management.\nIn most people who have type 2 diabetes, treatment starts with weight. 7. Genes in type 2 Diabetes Mellitus. 7.1. Inherited DM: Maturity onset diabetes of the young or MODY. 7.2.\nMitochondrial diabetes: A monogenic Diabetes Mellitus.From the triumvirate to the ominous octet: a new paradigm for the treatment of type 2 diabetes mellitus. Type 2 Diabetes Mellitus. (5th Edition) 2015. Topic 1: Overview of CPG.10. Hyperglycaemia in Pregnancy (GDM T2DM). Algorithm 5: Treatment Algorithm for Newly Diagnosed T2DM. Rosiglitazone and glimepiride are effective antihyperglycemic agents approved for the treatment of Type 2 diabetes mellitus. Primarily, rosiglitazone increases insulin sensitivity, while glimepiride stimulates insulin release from -cells in the pancreas. The measures of the treatment of type 2 diabetes mellitus in which a difference is expected to be observed are glycosylated hemoglobin A1C (HbA1C), fasting glucose, and insulin resistance. Type 2 diabetes mellitus (formerly called noninsulin-dependent diabetes) causes abnormal carbohydrate, lipid and protein metabolismType 2 diabetes is a common and underdiagnosed condition that poses treatment challenges to family practitioners. The introduction of new oral agents Original Editors - Kara Casey and Josh Rose from Bellarmine Universitys Pathophysiology of Complex Patient Problems project. Top Contributors - Kara Casey, Deirion Sookram, Ross Munro, Elaine Lonnemann and Rachael Lowe. No high quality data on the efficacy of diet alone exists for treatment of type 2 diabetes mellitus. This systematic review assesses the effects of studies that examined dietary advice with or without the addition of exercise or behavioural approaches. Most are over 40 years but teenagers are increasingly getting type 2 Diabetes Mellitus.Report of course of treatment: After prescribing the remedy and insisting on her diabetic diet , her oral anti diabetic agents, Metformin and Glibenclamide were continued and not tapered, until her FBS Predicting type 2 diabetes mellitus: a comparison between the FINDRISC score and the metabolic syndrome.8. Expert Panel on Detection. Evaluation, and treatment of high blood cho-lesterol in adults. Assessment | Biopsychology | Comparative | Cognitive | Developmental | Language | Individual differences | Personality | Philosophy | Social | Methods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |. Type one Diabetes Mellitus needs to be managed using insulin injections. Diabetes type 2 requires further treatments (type-1 diabetes and type-2 diabetes treatments) involving medications either with or without insulin. Diabetes mellitus is common, costly, and increasingly prevalent. Despite innovations in therapy, little is known about patterns and costs of drug treatment.National trends in the use of different types of insulin to treat diabetes, 1994 to 2007. Overview: Types of Diabetes Mellitus. Diabetes mellitus (DM) is a common disease in which the blood sugar (glucose) is abnormally elevated.Pancreas transplants may be performed in patients when treatment with medications fails. Treatment of Type 2 Diabetes Mellitus. Introduction. gastrointestinal adverse effects have not occurred, The incidence of diabetes is growing and is the dose can be increased to 850 to 1,000 mg with. Type 2 diabetes mellitus is the most common form of the disease (affecting 90 to 95 of persons with diabetes), with a prevalence of approximately 29.1 million people in the United States (1). Thewww.annals.org. CLINICAL GUIDELINE. Oral Pharmacologic Treatment of Type 2 Diabetes Mellitus. Pathogenesis of Type 2 Diabetes Mellitus The appropriate treatment of any disease is based on an understanding of its pathophysiology (7). The mechanisms responsible for impaired glu-cose homeostasis in type 2 diabetes mellitus (Figure 1) Are there different types of diabetes mellitus?The reason for the change of name is that older people can develop type 1 diabetes and type 2 diabetics often require treatment with insulin, so the old names are no longer seen as appropriate. Received date: December 15, 2015 Accepted date: March 23, 2015 Published date: April 07, 2015. Citation: Zhao Y, Xu G, Wu W, Yi X (2015) Type 2 Diabetes Mellitus- Disease, Diagnosis and Treatment. LIXISENATIDE and iGlarLixi (insulin glargine/lixisenatide fixed-ratio combination). For the treatment of type 2 diabetes mellitus. National trends in treatment of type 2 diabetes mellitus, 1994-2007. Richter B, Bandeira-Echtler E, Bergerhoff K, Lerch CL. Dipeptidyl peptidase-4 (DPP-4) inhibitors for type 2 diabetes mellitus. There are three major types of diabetes: type 1 diabetes, type 2 diabetes, and gestational diabetes. All types of diabetes mellitus have something in common.Treatment for type 1 diabetes involves taking insulin, which needs to be injected through the skin into the fatty tissue below. Treatment of diabetes mellitus. Table 35.1 summarizes the pharmacokinetics of most commonly used insulin preparations.The use of premixed insulins (see Insulin treatment in type 2 diabetes) is not recommended for patients with type 1 diabetes. bChina University of Petroleum, Beijing, Box 260 Changping District, Beijing, 102249, China. Abstract: Metformin is an antihyperglycemic agent commonly used for the treatment of Type II diabetes mellitus. Type 2 diabetes is also termed as Non insulin dependent diabetes (NIDDM) or adult onset diabetes and it accounts for 80 of diabetes mellitus globally.Since the insulin is not needed for the treatment of type 2 diabetes mellitus so it is regarded as (Adult Onset Diabetes). Laparoscopic duodenal-jejunal exclusion in the treatment 53. Janssen I, Katzmarzyk PT, Ross R. Body mass index, of type 2 diabetes mellitus in patients with BMI <30 kg/m2 waist circumference, and health risk: evidence in support of (LBMI). diabetes mellitusAn overview of diabetes mellitus and advances in treatment.Type 2 diabetes is far more common than type 1 diabetes, accounting for about 90 percent of all cases. The frequency of type 2 diabetes varies greatly within and between countries and is increasing throughout the world. Diabetes Mellitus Type 2 - Продолжительность: 21:45 Ahmed Zaitoun 1 901 просмотр.Groundbreaking New Treatment for Type 2 Diabetes Patients - Продолжительность: 5:30 TheBalancingAct 504 просмотра. Confirming the diagnosis. If you are able to confirm that the patient has new onset type 2 diabetes mellitus, what treatment should be initiated? Are there some therapies that should be instituted immediately? Diabetes mellitus type 2 (also known as type 2 diabetes) is a long-term metabolic disorder that is characterized by high blood sugar, insulin resistance, and relative lack of insulin. Common symptoms include increased thirst, frequent urination, and unexplained weight loss. Treatment of type 2 diabetes mellitus.The diet -which generally must be low-calorie due to the frequency of associated obesity- and a program of regular exercise are the basis of the treatment of type 2 diabetes mellitus. Diabetes mellitus type 2 It is overweight is a leading cause of diabetes type 2, which accounts for more than 90 of all cases of this disease.Treatment of diabetes. Diabetes mellitus type 2 is one of the so-called civilization diseases whose incidence goes hand in hand with prosperity.Treatment consists of dietary measures to reduce weight and sufficient exercise. Objective: to assess pharmacoeconomic aspects of treatment substitution of metformin immediate release (IR) form for metformin extended release (XR) form in diabetes mellitus (DM) type 2 treatment in Russian Federation health-care system. Exercise. If you have diabetes mellitus type 2, you have too much glucose in your blood.Medical Treatment. For managing the symptoms of Diabetes Mellitus Type 2 the doctor often prescribes a good diet and proper exercise. Type 2 diabetes is also called type 2 diabetes mellitus and adult-onset diabetes. Thats because it used to start almost always in middle- and late-adulthood.The treatment of type 2 diabetes also can produce symptoms. Type 2 Diabetes Mellitus. Initial basal dose (detemir or glargine) 15 units or 0.\n25 units/kg (whichever is greater).The effect of intensive treatment of diabetes on the development and progression of long-term complications in insulin-dependent diabetes mellitus. Inhaled insulins have not been shown to be effective in reducing A1C levels to the goal of less than 7 percent that is often recommended. (See \"Patient education: Diabetes mellitus type 2: Insulin treatment (Beyond the Basics)\".) Conventional Treatment Methods of Diabetes Mellitus Type 2. The treatment of diabetes mellitus type 2 includes the injections of insulin as well as the use of antibiotics. The first step is to chalk out a weight loss and a glucose-level monitoring program. Type 2 diabetes is a serious disease, and following your diabetes treatment plan takes round-the-clock commitment.Type 2 diabetes mellitus. In: Tintinallis Emergency Medicine: A Comprehensive Study Guide. It revolutionized the treatment of diabetes and prevention of its complications, transforming Type 1 diabetes from a fatal disease to one in which long-term survival becameComplications associated with diabetes mellitus. Diabetic retinopathy is a leading cause of blindness and visual disability. . Pioglitazone hydrochloride in combination with metformin in the treatment of type 2 diabetes mellitus: a randomized, placebo-controlled study.","ACE Inhibitor Use and Major Cardiovascular Outcomes in Patients With Type 2 Diabetes Mellitus Taking a DPP-4 Inhibitor\nIn patients with type 2 diabetes mellitus and cardiovascular diseases, concomitant administration of the angiotensin converting enzyme (ACE) inhibitors and the dipeptidyl dipeptidase 4 (DPP-4) inhibitors is common. In the clinical pharmacology laboratory, adverse hemodynamic effects have been reported when high doses of ACE inhibitors are taken on a background of DPP-4 inhibitors.1,2 These effects may be due to increases in substance P, a stimulator or sympathetic tone and a substrate for DPP-4.2 The results of this interaction include short-term increases in heart rate and blood pressure.3 Thus, the potential for offsetting the beneficial effects of ACE inhibitors by a DPP-4 inhibitor in patients with heart disease became a concern.3 To address this issue, we had the opportunity to study this potential interaction in the EXAMINE (Exploring the Cardiovascular Safety of Therapies for Type 2 Diabetes) trial,4,5 a large cardiovascular outcomes study, that assessed the effects of the DPP-4 inhibitor alogliptin on major cardiovascular events and in a population that had a high proportion of study participants taking an ACE inhibitor.\nPatients were included in the EXAMINE trial if they had a diagnosis of type 2 diabetes requiring anti-hyperglycemic therapy and had had an acute coronary syndrome in the 15 to 90 days before being randomized to the DPP 4 inhibitor alogliptin or placebo. The majority of the 5380 patients in the study had an acute myocardial infarction (vs. unstable angina) and 62% were taking an ACE inhibitor at the time of randomization. We divided the population into those taking an ACE inhibitor (n = 3323) and those not taking an ACE inhibitor (n = 2057). Within those subgroups, there were similar characteristics for the patients randomized to aloglitpin or placebo. Of interest, patients taking ACE inhibitors were more likely to be male, had shorter durations of type 2 diabetes, had higher body mass indexes, and were less like to be Asian (the EXAMINE trial was conducted in nearly 50 different countries and 19% of the study patients came from Asia). Patients on ACE inhibitors at baseline were also more likely to have hypertension, heart failure, and preserved kidney function than those patients not taking an ACE inhibitor. Overall, nearly 85% of patients in the EXAMINE trial were taking beta-blockers, 90% were taking a statin, and 90% were taking antiplatelet therapy.\nFollowing randomization, vital signs including heart rates measured by electrocardiography and conventional measurements of blood pressure by observers in duplicate were obtained at 1, 3, 6, 9, and 12 months during the first study year and at 4 month intervals in subsequent years. All serious cardiovascular events were prospectively adjudicated by a formal committee blinded to treatment assignment to determine the following primary and secondary events: death from cardiovascular causes, non-fatal myocardial infarction, non-fatal stroke, urgent revascularization for unstable angina, and hospitalization for heart failure. The rates of these events were compared for alogliptin versus placebo stratified by baseline history of ACE inhibitor use at the time of randomization using Cox proportional hazard modeling and adjusted by kidney function and region of the world. In addition, specific subgroup analyses were done according to a history of heart failure and lower versus higher doses of ACE inhibitors. Lower doses of ACE inhibitor were defined as the initial dose for the treatment of hypertension and higher doses were defined as those doses that were greater than the initial approved dose for the treatment of hypertension.\nChanges in the use of antihypertensive therapies including ACE inhibitors over the course of the trial were similar in patients randomized to alogliptin and placebo. There were no significant changes from baseline in blood pressure and heart in patients taking alogliptin versus placebo when on a background of ACE inhibitor therapy. In patients not using an ACE inhibitor, there was a small, significant reduction in the systolic blood pressure on alogliptin versus placebo (-1.3 mmHg, p = 0.033).\nComposite rates of the primary MACE endpoint (death from cardiovascular causes, nonfatal myocardial infarction and nonfatal stroke) were comparable in the aloglitpin and placebo groups who also took an ACE inhibitor (11.4% and 11.8%, respectively with a hazard ratio of 0.97, 95% confidence interval of 0.79-1.19) as well as those not taking an ACE inhibitor (11.2% and 11.9% for alogliptin and placebo respectively; hazard ratio of 0.94, 95% CI of 0.72-1.21) (Figure 1A). Similar results were seen for the composite of cardiovascular death or hospitalized heart failure as well as for their components (Figure 1B). Additionally, the event rates for these composites were similar for alogliptin and placebo in patients taking ACE inhibitors who had a baseline history of heart failure. Finally, no differences for alogliptin versus placebo for any event category was different according to lower versus higher doses of the ACE inhibitors.\nTime to First Event for Cardiovascular Death, Nonfatal MI, and Nonfatal Stroke on Alogliptin versus Placebo According to ACE inhibitor Use\nTime to First Event for Cardiovascular Death and Hospitalized Heart Failure on Alogliptin versus Placebo According to ACE inhibitor Use\nA potential concern was raised for an untoward hemodynamic outcome when users of DPP-4 inhibitors also took an ACE inhibitor for the treatment of hypertension, heart failure or cardio-protection in the post-myocardial infarction period.2,3 The key concern was that patients on higher doses of ACE inhibitors would have an attenuation of efficacy (less blood pressure reduction) or an increase in the heart rate mediated by substance P or the neurotransmitter neuropeptide Y.4 For instance, when ACE is inhibited, substance P is cleaved at its amino terminus and inactivated by DPP-4. Therefore, inhibiting both ACE and DPP-4 could theoretically stimulate the effects of the substance P. If this led to sympathetic nervous system activation, it could counteract some of the antihypertensive efficacy of an ACE inhibitor. Our data do not support this notion based on long-term measurements of blood pressure and heart rate; however, there is a remote possibility that such a finding might have been missed due to the different methodologies of data collection (fastidious frequent measurements of blood pressure and heart rate in a short-term clinical pharmacology program vs. more sparse measurements made in a large clinical outcome trial). Nevertheless, any blood pressure changes would likely be very small and of limited or no clinical importance.\nIn conclusion, we found no signal of an increased rate of cardiovascular events on alogliptin compared to placebo in patients treated with ACE inhibitors versus those patients not taking an ACE inhibitor. Hence, in a population of high cardiovascular risk patients taking the DPP-4 inhibitor alogliptin and standard, evidence based secondary preventive therapies, no untoward effects on ACE inhibitor benefits occurred over a median follow-up period of 18-19 months.\n- Marney A, Kunchakarra S, Byrne L, Brown NJ. Interactive hemodynamic effects of dipeptidyl peptidase IV inhibition and angiotensin converting enzyme inhibition in humans. Hypertension 2010;56:728-33.\n- Devin JK, Pretorius M, Nian H, Yu C, Billings FT, Brown NJ. Substance P increases sympathetic activity during combined angiotensin-converting enzyme and dipeptidyl peptidase 4 inhibition. Hypertension 2014;63:951-7.\n- Brown NJ. Cardiovascular effects of antidiabetic agents: focus on blood pressure effects of incretin based therapies. J Am Soc Hypertens 2012;6:163-8.\n- White WB, Cannon CP, Heller SR, et al. Alogliptin after acute coronary syndromes in patients with type 2 diabetes. N Engl J Med 2013;369:1327-35.\n- White WB, Wilson CA, Bakris GL, et al. Angiotensin converting enzyme inhibitor use and major cardiovascular outcomes in type 2 diabetes treated with the dipeptidyl peptidase 4 inhibitor alogliptin. Hypertension 2016;68:606-13.\nClinical Topics: Acute Coronary Syndromes, Dyslipidemia, Heart Failure and Cardiomyopathies, Prevention, ACS and Cardiac Biomarkers, Nonstatins, Novel Agents, Statins, Acute Heart Failure, Heart Failure and Cardiac Biomarkers, Hypertension\nKeywords: Acute Coronary Syndrome, Angina, Unstable, Angiotensin-Converting Enzyme Inhibitors, Antihypertensive Agents, Blood Pressure, Cardiovascular Diseases, Diabetes Mellitus, Type 2, Dipeptidases, Electrocardiography, Heart Failure, Heart Rate, Hydroxymethylglutaryl-CoA Reductase Inhibitors, Hypertension, Myocardial Infarction, Neuropeptide Y, Neurotransmitter Agents, Peptidyl-Dipeptidase A, Pharmacology, Clinical, Risk Factors, Stroke, Uracil\n< Back to Listings"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:214ca805-4f8d-4529-a2ae-05d6b0a862ff>","<urn:uuid:d6283923-1e11-45e9-bad7-169b47285f52>"],"error":null}
{"question":"I need to assess a cloud provider's data policies. What are the main factors I should look at?","answer":"When assessing a cloud provider's data policies, you should primarily examine their terms of service, which will define how they view your data and what they can do with it. You need to consider the governing principles of your region, as these will restrict the terms of service. Additionally, check where your data will be stored, as different regions have different governing rules about data processing. Also verify if the provider outsources to other cloud providers, as this creates additional agreements and considerations. Be aware that reading all relevant user agreements thoroughly could take significant time - reportedly around 250 working hours per year for typical users.","context":["When trying to figure out who has rights to your data there are three things to consider: you, the cloud provider, and the region your data is held in. A lot of the issues become issues because of the varying laws; where your data is held might be in different country than the country you uploaded from. So, even after you figure out what your agreement is with a Cloud provider they can be subject to the particular laws of another country; fore instance America has a set of laws known as the Patriot Act which grants the US government access under certain conditions. So even after you figure out who owns the data, and what that means, you might not have control over who is accessing the data.\nWhen you decide on a Cloud provider there are a number of things that you want to look at. One of them being the terms of service that will, most likely, define how a provider views your data, and what they can do with it. The terms of service will be restricted by your regions governing principles. Fore-instance in England they have the ‘Copyright and Rights in Databases Regulations 1997’ to help clear up some of the vagaries of this new technological development. The law defines two types of data one that is protected by copyright law, and ones that aren’t but are still regulated in their way. The existence of the law is a step in the right direction towards clarifying ownership of the information that is being stored in the Cloud.\nAlthough to confuse this issue even further is the fact that some of your information may be stored in your own database but you are using a Cloud service to handle it from time to time. Or your Cloud provider is servicing out to another Cloud provider; so they may host your information in a storage unit that isn’t their own. Each of these situations has unique problems and each part of this chain of concerns depends on user agreements and the particular governing bodies. So there is no single solution to answer the question of who owns your data, and as this issue becomes generally understood hopefully we will see some best practices winning out. Although I wouldn’t necessarily say there is no way to find out. There are some things that can be done to better understand what is happening. Unfortunately one of those things is reading over all your relevant user agreements, and as one source claims it would take roughly 250 working hours to read all the user/privacy agreements most of us come across in one year. So you have to balance your need to know with your time, but be warned the details are important.\nUnderstanding governing rules of where your data is being held or processed is not insignificant either. Each region is going to have its own governing rules about what happens when data is processed and the processing of the data may influence who owns the data now that it has been changed. So each step and movement of your data becomes an important issue to consider when deciding on a Cloud provider.\nWho owns your data, then? It depends on the governing laws and user agreement made between you and the Cloud provider. It also depends upon the governing laws of where your data is being held, in addition to the agreements that your cloud provider may be making with their cloud provider. The Cloud has so much under the umbrella of Cloud services, that often one type of Cloud provider will outsource to another type of Cloud provider."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:1670378a-cd48-4531-9164-cdaa269023ee>"],"error":null}
{"question":"How do statutory deposit insurance and computer fraud coverage differ in terms of financial protection limits and what they cover?","answer":"Statutory deposit insurance and computer fraud coverage protect against different types of financial risks with distinct coverage limits. Statutory deposit insurance protects bank deposits up to 100,000 dollars per customer and institution when a bank becomes insolvent, with possible increases to 500,000 dollars under special conditions for six months. In contrast, computer fraud coverage protects businesses against losses resulting from malicious misuse of computer systems aimed at financial gain or causing loss of property, monies, or securities, without specifying a fixed coverage limit.","context":["Investors have many options for investing money; serious and dubious providers compete for their favor. There are several of red flags that can indicate that a provider or product is in doubt. Banking, financial services and insurance business may only be carried out with state permission. But, this does not automatically mean that the products offered are recommendable. Securities and investments may also be offered to the public if a prospectus. But that doesn’t necessarily mean that the prospectus or information sheet and the products are reputable.\nIn general, think about your investment goals before investing and check your financial options. And: don’t let yourself be rushed. Take a time to think about pros and cons of investing before investing your money. Even if you seek advice, you do not make up your mind immediately.\nWHICH OFFERS SHOULD YOU BE PARTICULARLY CAREFUL WITH?\nIs someone calling you unsolicited to offer you a deal? In no case do not go into it. Such unsolicited calls are prohibited. Investment services companies and other companies are expressly prohibited from cold calling .\nE- mail / fax\nHave you by a vendor stock recommendations by e – mail received, you do not know? Do you receive stock letters by fax that you did not order? Or are you being presented with an alleged insider tip that you are supposed to be aware of? Behind such offers are mostly dubious providers who want to convey shares of worthless companies to investors for their own benefit through an invented success story.\nAre you put under time pressure? Does the provider entice you with an exclusive business that you have to decide on very quickly? You shouldn’t go into that because it’s often just a trick. As I said: never let yourself be rushed! Serious offers are not only available today, but also tomorrow.\nHigh Returns Or Exceptional Development Potential\nAre you promised unusually high interest rates? Promises of returns that are far above the market standard can also be an indication of dubious offers. The higher the promised profit, the higher the risk that you will lose the capital invested. You can find out which returns are customary in the market. You should also critically examine recommendations on companies that are considered to have exceptional development potential. Penny stocks are particularly vulnerable to speculation and manipulation due to their low prices and trading volumes.\nDoes the provider have difficulties explaining their product? Never buy a pig in a poke – first inform, then decide! Only buy what you really understand! Basically, the more complicated a product is, the more experience you should have with financial transactions. Get to grips with the product yourself and do not be tempted by fantasy titles and embellished graphics.\nOpaque Exit Options\nClarify how and when you will get your investment back. You should be particularly careful with long-term contracts, if there is no possibility of early termination or if this would entail considerable financial disadvantages.\nYou should only conclude contracts that run for several years without the possibility of early exit with providers whose seriousness you have no doubt in any way. Remain critical even if you can cancel or terminate a deal at any time within a certain period of time. Such rights do not automatically protect you from financial loss either. Clarify which repayment you will actually receive if the worst comes to the worst. The following applies to securities transactions: Find out about options for parting with a security before the end of the term. It is often important to know whether there is a liquid market for the product.\nDo you want to transfer money to (outside of Europe) abroad? Be extra careful. Many investors have already lost their money in the process. You may no longer be able to see whether and how your money is being invested. There have been cases in which the company to which the money was transferred did not invest the amount as agreed or did not invest at all. Sometimes the company didn’t even exist.\nInvestment on trial\nAre you tempted to invest a smaller amount first on a trial basis? The reason why you cannot find any information about the company during your research is that it is a young company with promising business ideas? Then the supposed insider tip is probably a trap. After a short time, the provider will report on the great success of the system and ask you to invest larger amounts. Lured by the success of your trial investment, you should be tempted to invest more money.\nThey are persuaded to invest in supposedly lucrative investment businesses. In reality, however, the funds are not invested, but used to distribute or repay previous investors. Such a pyramid scheme is usually not recognizable for investors. The investment and its return are often simulated in glossy prospectuses. Sooner or later this system breaks down.\nWHICH OFFERS SHOULD YOU BE PARTICULARLY CAREFUL WITH?\nHigh costs and commissions\nGet an overview of what proportion of your investment amount should be used for costs, fees and commissions. Use the mandatory information provided by the provider! Investment services providers not only have to explain the total costs to investors, they also have to inform them of all costs incurred and their effects on returns. Donations are even to be shown separately. Since securities service providers are allowed to summarize the costs, they must provide you with a list broken down into individual items if you request this.\nParticular caution is required with forward transactions and the day trading that is often associated with them . High fees can be incurred for each transaction. The provider is therefore fundamentally interested in a large number of deals. The fees are often so high that the bottom line is that you can hardly make a profit. Often the costs even consume the capital invested in a short period of time.\nCan’t you see who your contract partner should be? Are there any warnings or other information? Check the names of the vendors and products using search engines on the internet. Information is often also available from the local consumer advice centers. Don’t do business with vendors who don’t provide you with meaningful information. Do not rely on good-sounding names or reputable websites. And if you don’t understand the contract, stay away from the offer!\nHow is your investment secured?\nIf your bank or securities trading company experiences payment difficulties, deposit insurance and investor compensation protect – to a certain extent – your balances and claims. Customer deposits are protected by the deposit insurance, customer claims from securities transactions by the investor compensation.\nStatutory deposit insurance\nWhen the bank becomes insolvent, the contractual deposit insurance covers the savings of the client. Account balances, fixed-term deposits and savings deposits of up to 100,000 dollars per customer and institute – and not per account – are legally protected. In the case of joint accounts, each individual account holder has a separate entitlement. Under special conditions, the maximum amount for a period of six months can increase to up to 500,000 dollars.\nVoluntary deposit insurance\nIn addition to the statutory compensation schemes, the banking associations have set up voluntary security schemes. However, they do not grant bank customers any legal right to compensation.\nMonies owed to you as an investor in connection with securities transactions (e.g. distributions, sales proceeds) are subject to statutory investor compensation. This protects your claims against your bank to surrender the securities held for you or money that it owes you from securities transactions. The compensation claim is limited to 90 percent of the liability and the equivalent of 20,000 dollars\nHowever, the Investor Compensation does not pay you any compensation, for example if your insolvent bank gave you wrong advice. You will therefore not be reimbursed for lost profits or losses that you have incurred due to a wrong investment strategy.\nBearer and order bonds or bonds are not secured. The security systems also do not work if you have acquired a share in a company with your investment, for example through the acquisition of a share or a silent partnership. Your investment will then participate in both the company’s profits and losses. Always check before investing whether and how your deposits are protected if the company should no longer be able to repay the money.\nSpecial case processing\nInstitutions that, due to their systemic relevance, should not go into bankruptcy are wound down in an orderly manner if they fail. In liquidation, too, as in bankruptcy, losses are shared between owners and creditors. One then speaks of a bail-in. In the event of a bail-in, you as a private investor can also participate in losses: as a shareholder, as the holder of relevant capital instruments, but also as a creditor. You are a shareholder, for example, if you have shares in this bank in your custody account . As the holder of relevant capital instruments, you are considered to be if you have invested in instruments of the institute’s additional tier 1 capital or tier 1 capital, such as subordinated bonds or subordinated loans. Among other things, you are a creditor if you have a credit account with the bank or if you own bonds from the institution – for example index certificates.\nThe deposits covered by the statutory deposit insurance are excluded from the bail-in. In addition, the liability in the resolution must not exceed the losses that you would have to bear if the bank had been led into regular insolvency proceedings.","Crime Insurance coverage is designed to protect your busines in the event of loss due to burglary, robbery, fraud, and/or employee dishonesty.\nListed below are available crime insurance coverages:\nCOMPUTER FRAUD: This coverage protects your business against loss resulting from malicious misuse of your computer system with the intention of financial gain by an individual or group, or with the intent to cause a loss of property, monies or securities.\nEMPLOYEE DISHONESTY: This coverage will protect your business in the event fraudulent or dishonest acts are committed by an employee which result in loss of money, securities or property. This coverage is also known as a \"fidelity bond\".\nEXTORTION: This coverage protects against loss due to the extortion (kidnap) of an insured employee or family member in exchange for ransom, whereas the individual(s) or property is subject to damage or harm.\nFORGERY OR ALTERATION: This insurance coverage protects your business in the event of forgery or alteration of your company's business checks, promissory notes, drafts, consignments, or similar documents. An attached endorsement can protect against loss due to incoming documents, if so requested.\nGUEST'S PROPERTY-PREMISES: This provides protection against burglary, robbery, loss or destruction of a guest's property, for which you are responsible for safekeeping.\nGUEST'S PROPERTY-SAFE DEPOSIT LIABILITYThis coverage provides protection against burglary, robbery, destruction or other damage of a guest's property while in a safe deposit box, for which you are responsible for safekeeping.\nLESSEES OF SAFE DEPOSIT BOXES: This provides protection of securities against loss caused by theft, disappearance or destruction and covers property other than money and securities against loss caused by burglary, robbery or vandalism. Coverage is limited to covered property in the insured's safe deposit box when inside a vault, as well as during the course of removal or deposit from that safe deposit box while inside the insured's premises.\nPREMISES BURGLARY: This coverage protects your business from loss or disappearance of stock or property as a result of a burglary, inside your premises. Coverage is also provided in the event of the robbery of a watchman.\nPREMISES THEFT AND ROBBERY OUTSIDE:This coverage protects your property other than monies, securities, and motor vehicles when inside your premises, in the event of an actual or alleged theft. Damage to the building structure is also included. This coverage provides protection when loss occurs while you are open for business. You are also covered for robbery of yourself, your partners or employees when outside your premises.\nROBBERY AND SAFE BURGLARY: This provides protection in the event of loss of stock or property due to a robbery in which you or your employees are forced to relinquish goods, or where a safe is burglarized and a similar loss results.\nSAFE DEPOSITORY DIRECT LOSS: This insures a customers' property while: inside the customers' safe deposit box within a fault on the depositorys' premises; stored in such vaults within the premises; temporarily elsewhere within the premises while in the course of deposit to or removal from such boxes and vaults.Coverage is limited to loss caused by robbery or burglary, as well as damage or destruction of the insured property.\nSAFE DEPOSITORY LIABILITY: This coverage insures the sums that an insured becomes legally obligated to pay as a result of damages or loss to a customers property while: inside the customers' safe deposit boxes inside the vaults within the depositorys' premises; stored inside such vaults within the depositorys' premises; temporarily elsewhere within the depositorys' premises while in the course of deposit into or removal from the safe deposit boxes or vaults.\nSECURITIES DEPOSITED WITH OTHERS: This coverage protects securities while being transported by a custodian, while inside a custodians premises, or while on deposit in a depository.Losses are covered when caused by theft, disappearance or destruction.\nSELLING PRICE: This added endorsement will provide a reimbursement on the actual profits lost when a loss of stock occurs due to an insured crime peril. Without this amendment, you would only receive payment equal to the cost of the stock, thus losing any profits that would have been realized if no loss had occurred.\nTHEFT, DISAPPEARANCE, DESTRUCTION: This coverage will protect your company's monies and securities in the event they are lost or damaged due to any peril, except those which are specifically excluded by the policy. This is the broadest form of crime coverage available for money and securities and includes losses on or off your premises."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:f3f5ca44-fb06-4596-a5aa-8459a416c1f9>","<urn:uuid:6adfcd32-ab81-4da9-b093-430f91a6c826>"],"error":null}
{"question":"How did both Jane Couch and Azumah Nelson deal with their post-boxing careers and retirement?","answer":"Jane Couch struggled significantly after retirement, admitting that for a couple of years she 'probably should have been assigned to a mental hospital.' She had difficulty adjusting to normal life after being told when to train and fight for 15 years, describing herself as 'like a robot' who had to relearn how to live a normal life. She eventually succeeded in adjusting by keeping her distance from combat sports and settling down with her partner Brian. In contrast, Azumah Nelson's post-boxing life appears to have taken a different direction, as he has remained involved in boxing by training his son, Azumah Nelson Junior, as a boxer, and even organized a fight night to celebrate his 60th birthday at the Bukom Boxing Arena in July 2018.","context":["Don't rewrite history, there were female boxers before Katie Taylor and Nicola Adams, says women's pioneer Jane Couch\nJane Couch the boxer was a multiple world champion, a pioneer in her sport and a woman unafraid to challenge convention.\nJane Couch the ex-boxer still doesn't pull any punches. Older and wiser, but still fiercely passionate about the sport that shaped who she is, she reveals today the reality of being a woman in a 'man's world', the battles she had outside the ring and her real concerns about the future of the sport she has fought so hard for.\nIn an exclusive interview with NewsChain, she talks about:\n- the financial struggles throughout her career - 'It cost me to fight. My biggest purse was $2,500 and Barry Hearn [promoter] once gave me £500 over breakfast when he found out I was boxing for free on one of Prince Naseem Hamed's cards'\n- the mental pressure of her fighting career -\"For a couple of years after I gave up I should probably have been in a mental hospital. I was like a robot. I had to relearn how to live a normal life'\n- the notion that the sport has improved for women - 'Get real. Tell the truth. Stop sugar-coating everything because it isn’t like that.'\n- transgender athletes entering women's boxing - 'I think that's going to be the next step, which will bury it, [women's boxing] the whole thing'\nJane Couch didn't become a boxer to make money. And, as she so candidly explains, that was a probably a good thing.\n“It cost me to fight,\" she says frankly. \"For the Sandra Geiger fight (her first world title victory), I was in the gym for 14 months. I never earned a penny and I wasn’t working.\n\"It was being promoted in Denmark and kept getting cancelled so I had to just keep going, keep going, keep going, living with nothing for a year.\n\"And then you get there, and I think I was given $1,000 and (after trainer, expenses etc were paid for) walked away with $150.\"\nThe Lancashire-born fighter estimates the highest purse of her career saw her walk away with $2,500, which came in a world title fight against Lucia 'The Most Dangerous Woman in the World' Rijker, on the undercard of Lennox Lewis vs Vitali Klitschko.\nElsewhere, she received just $500 for appearing on a show headlined by Roy Jones Jr, who at the time was widely considered the best boxer on the planet.\nAnd then there was the occasion when she was paid £500 during breakfast by promoter Barry Hearn after he found out she was scheduled to be boxing for free on one of Prince Naseem Hamed's cards.\nBut it is these chapters of Couch's career that paved the way for today's female boxers, that opened the doors of opportunity and that have, for example, led to Katie Taylor topping the bill at the Manchester Arena in a title fight against Christina Linardatou this weekend.\nAnd Taylor, one suspects, won't have to rely on a handout over her cornflakes.\n\"Mad, isn't it?,\" says Couch with a resigned chuckle.\n“I still question myself now. Why did I keep going? Why did I keep doing it? I just can’t come up with the answer.\"\nHer role as pioneer in her sport came via a two-year long court battle in the 1990s after the British Boxing Board of Control refused her a licence on the basis that she was a woman.\nThe legal action she took saw her claim sexual discrimination and eventually the decision was overturned in March 1998, despite the British Medical Association coming out afterwards with a statement saying it was 'a demented extension of equal opportunities'.\nAll these years - and an MBE in 2007 for services to sport - later, she remains sceptical that there is a majority body of support within the sport for women boxers, and recalls fighting her corner with promoters who she believed were against it throughout her career.\n“I used to argue with Frank Warren and Frank Maloney that women are playing rugby, doing judo and taekwondo in the Olympics, what’s so bad about boxing? It’s still a contact sport,\" she says.\n\"And they couldn’t answer it. They just didn’t want to see women boxing.\"\nHowever, Warren now promotes two-time Olympic gold medalist and newly-crowned WBO World Flyweight champion Nicola Adams and has gone on record saying he has changed his views on women's boxing.\nCouch, nevertheless, remains adamant: \"I really don't think things have changed,\" she says.\nNow 51, she was a latecomer to the sport. She didn't box until the age of 26 when she saw a TV documentary about women's boxing and thought she'd give it a go.\nInitially fighting in Muay Thai, combat sports gave her the focus and discipline which she had lacked earlier on in her life when she was expelled from her school in Blackpool.\nAnd when she packed in the sport years later, the impact of lacking a primary goal reared its head once more.\nShe admits that for a couple of years after giving up she \"probably should have been assigned to a mental hospital\".\n\"I just didn’t know what to do,\" she says. \"I’d been told when to get up, to train, to fight for 15 years and I was like a robot. I had to relearn how to live a normal life.\n“There has got to be a connection between getting punched in the head and then retiring and having nervous breakdowns, anxiety, getting depressed. There’s got to be some sort of link there.\"\nAfter dabbling in promoting and videography within boxing, Couch successfully 'relearned how to live a normal life' by keeping her distance from combat sports.\nShe settled down with her partner Brian, who she has now been with for nine years, and began to enjoy life again away from the crazy world of fighting.\nBut when one of the lawyers who fought her case back in the 90s, Sarah Leslie, died of breast cancer, Couch thought it fitting to honour her by releasing a new autobiography - 'The Final Round: The Autobiography of Jane Couch'.\nAnother reason she gives for doing the book is that she feels the history of women's boxing, and her part in it, is being airbrushed by TV coverage promoting the likes of Katie Taylor as the female face of the sport.\n“What about the history of women’s boxing? What about how it got started?,\" she says.\n\"That was another reason (for the book), because when Sarah died I thought it was disrespectful - they fought so hard, the time put in to get it recognised.\n“But maybe they (TV companies) don’t want to know. Maybe they want to build this model and say ‘that’s how we want it perceived and shown'.”\nTaylor's and Nicola Adams' success at London 2012 and Rio 2016 have allowed them to be the torchbearers for women's boxing.\nBut underneath those two sits a pool of talented fighters who continue to struggle for the financial rewards, says Couch.\nWorld championship bronze medalist and Olympic quarter-finalist Savannah Marshall rents out a flat in Manchester with 11-0 boxer Chantelle Cameron when the pair are training for bouts.\nAnd Couch cites predicaments like theirs as examples of how women's boxing has not changed whatsoever for even some of the very best in the country.\n“It’s criminal,\" she says. “Savannah has come out of the amateurs after being in an elite squad, boxing on Olympic teams against the best in the world, and she’s coming to rent a flat in Manchester.\n“Move (boxing) on now and get the sport better, to a point where two ex-Olympians are not sharing a flat in Manchester trying to get by, negotiating travel home every weekend, where girls are getting fights cancelled because they haven’t sold enough tickets.\n“Get real. Tell the truth. Stop sugar-coating everything because it isn’t like that.\"\nCouch admits she 'feels sick' when she then has to hear Anthony Joshua and Deontay Wilder failing to negotiate a fight for a reported $100 million.\nAnother major concern for the former lightweight champion is the threat posed by transgender athletes moving from men's to women's boxing.\nIt is a trend seen recently with Rachel McKinnon in cycling and Laurel Hubbard in weightlifting.\nBut Couch, who rubbished claims that she tried to organise a bout between her and a male fighter years ago, believes it would be a grave mistake for boxing to allow transgender athletes to cross over to women's competition.\n“It’s going to be an issue,\" she says. \"If you’re born a male and you compete in a women’s sport then you’re going to win.\n“Males are naturally stronger than women. They’ve got more testosterone, they’ve got what women haven’t got. It’s a dangerous time.\n“I used to spar with a few of the lads in Bristol and even the top amateurs used to hurt me with a jab - a jab! And then I’d stand and have wars with journeymen, stand and have proper gym wars with them.\"\nIrrespective of her concerns though, Couch believes there will come a time when a woman who has transitioned from being a man begins competing at an elite level in women's boxing.\n“I think that’s going to be the next step which will then bury it (women's boxing), the whole thing,\" she adds.\n\"All of that court case, everything Katie Taylor is doing, all what the girls are doing, if that happened it would put women’s boxing back 50 years.\n“It would be like putting the England women’s football team against the England men, it’s not even a competition. Skill, fitness and everything they work hard at, but it’s not the same. Men are built differently to women.”\nCouch now lives a quieter life in Bristol and admits that she watches very little boxing outside of the big fights.\nShe does not have kids, but insists if she had a daughter who wanted to follow in her footsteps then it would be a mother's responsibility to \"keep her away from the sharks\" in boxing.\nAfter a relatively short time in her company, she leaves you in little doubt that that's another fight she wouldn't shirk from, another battle she'd likely win.\nThe best videos delivered daily\nWatch the stories that matter, right from your inbox","Azumah Nelson facts for kids\nQuick facts for kidsAzumah Nelson\n|Height||5 ft 5 in|\n19 July 1958 |\n|Wins by KO||28|\nAzumah Nelson (born 19 July 1958) is a Ghanaian former professional boxer who competed from 1979 to 2008. He was a two-weight world champion, having held the WBC featherweight title from 1984 to 1987 and the WBC super-featherweight title twice between 1988 and 1997. He also challenged once for the unified WBC and IBF lightweight titles in 1990. At regional level he held the ABU, and Commonwealth featherweight titles between 1980 and 1982. Widely considered one of the greatest African boxers of all time, he is currently ranked as the 31st greatest pound for pound boxer of all time by BoxRec.\nNelson competed at the 1978 All-Africa Games and 1978 Commonwealth Games, winning gold medals in featherweight at both events. He was awarded Amateur Boxer of the year by the Sports Writers Association of Ghana (SWAG) that same year.\nDespite all his early achievements and being undefeated in 13 fights, Nelson was virtually unknown outside Ghana. Because of this, he was a decisive underdog when, on short notice, he challenged WBC featherweight champion Salvador Sánchez on 21 July 1982 at the Madison Square Garden in New York. Nelson lost the fight by fifteenth-round knockout.\nNelson won all four of his fights in 1983, and he began 1984 by beating Hector Cortez by decision on 9 March in Las Vegas. Then, on 8 December of that year, he became boxing royalty by knocking out Wilfredo Gómez in round 11 to win the WBC featherweight championship. Behind on the three judges' scorecards, Nelson rallied in that last round to become champion in Puerto Rico.\nNelson began 1988 by defeating Mario Martinez by a split decision over 12 rounds in Los Angeles to win the vacant WBC super featherweight title. Nelson was dropped in the 10th round of their encounter and the decision was not well received.\nOn 1 December 1995, defeated world champion Gabriel Ruelas in the fifth round to claim the title.\nHis first defense took place almost a year later, when he and Leija had their third bout. Nelson retained the title with a six-round knockout. As had become his common practice, that was the only time Nelson fought in 1996.\nIn 1997, Nelson lost the Lineal & WBC titles to Genaro Hernandez when beaten on points in twelve rounds.\nThe Azumah Nelson Sports Complex at Kaneshie in Accra was named after him.\nIn 2014 the biography of Azumah Nelson was published. Written by Ashley Morrison it was titled \"The Professor - The Life Story of Azumah Nelson\" (ISBN: 978-1628571059) was published by Strategic Book Publishing.\nNelson has a son, Azumah Nelson Junior, whom Nelson is training as a boxer.\nIn July 2018, Azumah organized a fight night to celebrate his 60th birthday at the Bukom Boxing Arena. This event brought together fighters from highly rated gyms in the country to fight contenders in their divisions. Some dignitaries including Nii Lante Vanderpuiye and Nii Amarkai Amarteifio who are two former sports ministers, Ian Walker the British High Commissioner to Ghana and Peter Zwennes the president of the Ghana Boxing Authority, graced the occasion. In all there were five bouts, three of which were won by knockout.\nProfessional boxing record\n|46 fights||37 wins||7 losses|\n|46||Loss||38–6–2||Jeff Fenech||MD||10||24 Jun 2008||Hisense Arena, Melbourne, Victoria, Australia|\n|45||Loss||38–5–2||Jesse James Leija||UD||12||11 Jul 1998||Alamodome, San Antonio, Texas, U.S.|\n|44||Loss||38–4–2||Genaro Hernández||SD||12||22 Mar 1997||Memorial Coliseum, Corpus Christi, Texas, U.S.||Lost WBC super-featherweight title|\n|43||Win||38–3–2||Jesse James Leija||TKO||6 (12), 1:58||1 Jun 1996||Boulder Station, Las Vegas, Nevada, U.S.||Retained WBC super-featherweight title|\n|42||Win||37–3–2||Gabriel Ruelas||TKO||5 (12), 1:12||1 Dec 1995||Fantasy Springs Resort Casino, Indio, California, U.S.||Won WBC super-featherweight title|\n|41||Loss||36–3–2||Jesse James Leija||UD||12||7 May 1994||MGM Grand, Las Vegas, Nevada, U.S.||Lost WBC super-featherweight title|\n|40||Draw||36–2–2||Jesse James Leija||SD||12||10 Sep 1993||Alamodome, San Antonio, Texas, U.S.||Retained WBC super-featherweight title|\n|39||Win||36–2–1||Gabriel Ruelas||MD||12||20 Feb 1993||Estadio Azteca, Mexico City, Distrito Federal, Mexico||Retained WBC super-featherweight title|\n|38||Win||35–2–1||Calvin Grove||UD||12||7 Nov 1992||Caesars Tahoe, Stateline, Nevada, U.S.||Retained WBC super-featherweight title|\n|37||Win||34–2–1||Jeff Fenech||TKO||8 (12), 2:20||1 Mar 1992||Princes Park Football Ground, Melbourne, Victoria, Australia||Retained WBC super-featherweight title|\n|36||Draw||33–2–1||Jeff Fenech||SD||12||28 Jun 1991||The Mirage, Las Vegas, Nevada, U.S.||Retained WBC super-featherweight title|\n|35||Win||33–2||Daniyal Mustapha Ennin||KO||4 (10)||16 Mar 1991||Polideportivo Principal Felipe, Zaragoza, Aragón, Spain|\n|34||Win||32–2||Juan Laporte||UD||12||13 Oct 1990||Sydney Entertainment Centre, Sydney, New South Wales, Australia||Retained WBC super-featherweight title|\n|33||Loss||31–2||Pernell Whitaker||UD||12||19 May 1990||Caesars Palace, Las Vegas, Nevada, U.S.||For WBC and IBF lightweight titles|\n|32||Win||31–1||Jim McDonnell||KO||12 (12), 1:40||5 Nov 1989||Royal Albert Hall, Kensington, London, England||Retained WBC super-featherweight title|\n|31||Win||30–1||Mario Martínez||TKO||12 (12), 1:18||25 Feb 1989||Hilton Hotel, Las Vegas, Nevada, U.S||Retained WBC super-featherweight title|\n|30||Win||29–1||Sidnei Dal Rovere||KO||3 (12), 2:04||10 Dec 1988||Accra Sports Stadium, Accra, Ghana||Retained WBC super-featherweight title|\n|29||Win||28–1||Lupe Suarez||TKO||9 (12), 0:27||25 Jun 1988||Trump Plaza Hotel and Casino, Atlantic City, New Jersey, U.S.||Retained WBC super-featherweight title|\n|28||Win||27–1||Mario Martínez||SD||12||29 Feb 1988||Great Western Forum, Inglewood, California, U.S.||Won vacant WBC super-featherweight title|\n|27||Win||26–1||Marcos Villasana||UD||12||29 Aug 1987||Olympic Auditorium, Los Angeles, California, U.S.||Retained WBC featherweight title|\n|26||Win||25–1||Mauro Gutierrez||KO||6 (12), 0:33||7 Mar 1987||Hilton Hotel, Las Vegas, Nevada, U.S.||Retained WBC featherweight title|\n|25||Win||24–1||Danilo Cabrera||TKO||10 (12), 2:31||22 Jun 1986||Hiram Bithorn Stadium, San Juan, Puerto Rico||Retained WBC featherweight title|\n|24||Win||23–1||Marcos Villasana||MD||12||25 Feb 1986||Inglewood Forum, Los Angeles, California, U.S.||Retained WBC featherweight title|\n|23||Win||22–1||Pat Cowdell||KO||1 (12), 2:24||12 Oct 1985||National Exhibition Centre, Birmingham, West Midlands, England||Retained WBC featherweight title|\n|22||Win||21–1||Juvenal Ordenes||TKO||5 (12), 2:45||6 Sep 1985||Tamiami Park, Miami, Florida, U.S.||Retained WBC featherweight title|\n|21||Win||20–1||Wilfredo Gómez||KO||11 (12), 2:58||8 Dec 1984||Hiram Bithorn Stadium, San Juan, Puerto Rico||Won WBC featherweight title|\n|20||Win||19–1||Hector Cortez||UD||10||9 Mar 1984||Las Vegas Convention Center, Las Vegas, Nevada, U.S.|\n|19||Win||18–1||Kabiru Akindele||KO||9 (15)||25 Nov 1983||National Stadium, Lagos, Nigeria||Retained Commonwealth featherweight title|\n|18||Win||17–1||Alberto Collazo||TKO||2 (10), 1:40||23 Sep 1983||Richfield Coliseum, Richfield, Ohio, U.S.|\n|17||Win||16–1||Alvin Fowler||TKO||2 (10), 0:41||17 Aug 1983||Showboat Hotel and Casino, Las Vegas, Nevada, U.S.|\n|16||Win||15–1||Ricky Wallace||UD||10||12 Feb 1983||Public Auditorium, Cleveland, Ohio, U.S.|\n|15||Win||14–1||Irving Mitchell||TKO||5 (10), 2:24||31 Oct 1982||Great Gorge Resort, McAfee, New Jersey, U.S.|\n|14||Loss||13–1||Salvador Sánchez||TKO||15 (15), 1:49||21 Jul 1982||Madison Square Garden, New York, New York, U.S.||For WBC and The Ring featherweight titles|\n|13||Win||13–0||Mukaila Bukare||TKO||6 (10)||26 Jun 1982||Kaneshie Sports Complex, Accra, Ghana|\n|12||Win||12–0||Charm Chiteule||TKO||10 (15)||28 Feb 1982||Woodlands Stadium, Lusaka, Zambia||Retained Commonwealth featherweight title|\n|11||Win||11–0||Kabiru Akindele||KO||6 (15)||4 Dec 1981||Siaka Stevens National Stadium, Freetown, Sierra Leone||Retained Commonwealth featherweight title|\n|10||Win||10–0||Brian Roberts||TKO||5 (15)||26 Sep 1981||Accra Sports Stadium, Accra, Ghana||Won vacant Commonwealth featherweight title|\n|9||Win||9–0||Miguel Ruiz||TKO||4 (10)||18 Aug 1981||Stadium, Bakersfield, California, U.S.|\n|8||Win||8–0||Don George||KO||5 (10), 0:54||2 May 1981||Kaneshie Sports Complex, Accra, Ghana|\n|7||Win||7–0||Aziza Bossou||PTS||8||6 Mar 1981||Lomé, Togo|\n|6||Win||6–0||Joe Skipper||TKO||10 (12)||13 Dec 1980||Kaneshie Sports Complex, Accra, Ghana||Won African featherweight title|\n|5||Win||5–0||David Capo||PTS||10||4 Oct 1980||Kaneshie Sports Complex, Accra, Ghana|\n|4||Win||4–0||Abdul Rahman Optoki||TKO||8 (12)||2 Aug 1980||Kaneshie Sports Complex, Accra, Ghana||Retained Ghanaian featherweight title|\n|3||Win||3–0||Henry Saddler||TKO||9 (12)||1 Mar 1980||Kaneshie Sports Complex, Accra, Ghana||Won Ghanaian featherweight title|\n|2||Win||2–0||Nii Nuer||TKO||3 (8)||2 Feb 1980||Kaneshie Sports Complex, Accra, Ghana|\n|1||Win||1–0||Billy Kwame||PTS||10||1 Dec 1979||Accra Sports Stadium, Accra, Ghana|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:96a406cb-148a-45d6-af27-adfb9beb528c>","<urn:uuid:d84bd1fd-9582-4290-94f7-3a910eb799a8>"],"error":null}
{"question":"As a baseball historian, I'm curious about Cy Young's career achievements and how they compare to the labor disputes in baseball - what were his most significant records, and how did the 1994 strike impact baseball's record-keeping?","answer":"Cy Young holds several unbreakable records, including most career wins (511), most career starts (815), and most complete games (749). He pitched 7,356 innings and holds the record for most career hits allowed (7,092). He also threw the first perfect game of the modern era in 1904. The 1994 baseball strike interrupted the season and affected baseball's record-keeping - for instance, it prevented some players from achieving statistical milestones, as evidenced by how it broke Cal Ripken Jr.'s streak of consecutive 140+ game seasons. The strike led to a shortened 1995 season of 144 games and caused a 20% drop in per-game attendance, from 31,632 in 1994 to 25,260 in 1995.","context":["By Glen Sparks\nDenton True Young threw so hard that people nicknamed him “Cy,” short for Cyclone. Born on the farm in Gilmore, Ohio, on March 29, 1867, Cy Young made his major league debut Aug. 6, 1890, for the Cleveland Spiders. He tossed a three-hit shutout.\nIn an age of strong-willed ironmen, no one was tougher than Young. He won more games than anyone in baseball history (511), and he lost more games than anyone (316) over his 22 seasons. Young is nearly 100 wins ahead of the No. 2 guy on the all-time wins list, Walter Johnson (417 wins). The 6-foot-2-inch right-hander completed nearly 92 percent of his career starts.\nYoung retired at the end of the 1911 season. He put together one of the most extraordinary careers in the game’s history. Baseball writers voted him into the Baseball Hall of Fame in 1937. The great pitcher died in Ohio on Nov. 4, 1955, at the age of 88. One year later, baseball introduced the Cy Young Award, given out each season to the game’s best pitcher. (In 1967, each league began giving out a Cy Young Award.)\nPoet Ogden Nash wrote this little ditty about Young for the January 1949 edition of Sport magazine.\nY is for Young\nThe magnificent Cy;\nPeople batted against him,\nBut I never knew why.\nRead more about Cy Young and his amazing time in baseball:\n- Young began his career with the Spiders (1890-98). He went from there to the St. Louis Perfectos/Cardinals (1899-90), the Boston Americans/Red Sox (1901-08), the Cleveland Naps/Indians (1909-11) and, finally, the Boston Rustlers/Braves (1911).\n- Young won at least 25 games in a season 12 times and at least 30 games five times. He won 93 games from 1901-03.\n- He started more games than anyone in baseball history (815) and completed more than anyone (749).\n- No one pitched more career innings than Young (7,356) or gave up as many hits (7,092). He topped the 400-inning mark five times and yielded 477 hits in 1896.\n- Young only made it through the sixth grade in school. That didn’t stop him from serving as pitching coach at Harvard University for a few months before the start of the 1902 campaign.\n- Baseball played the first modern World Series in 1903. The Boston Americans, winners of the American League pennant, went up against the National League champ Pittsburgh Pirates. Young, pitching in Game One for Boston, lost 7-3. He came back and won his next two games, posting a combined Series ERA of 1.85. The Americans won the best-of-nine match-up five games to three.\n- On May 5, 1904, Young tossed the first perfect game of the modern era (post-1900). Pitching for the Americans, he beat the Philadelphia A’s at Boston’s Huntington Avenue Grounds in front of 10, 267. Young, who threw three no-hitters in his career, struck out eight batters in a game that lasted one hour, 25 minutes. He beat the great Rube Waddell.\n- Young aged gracefully. He tossed his final no-hitter in 1908, three months after he turned 41. He was the oldest to throw a no-no until Nolan Ryan hurled one 82 years later at the age of 43.\n- The pitcher is tied with Roger Clemens for first on the all-time Red Sox wins list with 192. Clemens pitched 2,776 innings for Boston, and Young pitched 2,728.1 innings. Cy spent eight seasons spent the Red Sox; Clemens spent 13.\n- A control artist, Young topped his circuit in BB/9 innings 14 times and in K/BB ratio 11 times. He led the league in strikeouts twice.\n- This all-time great retired with a career WAR (Wins above Replacement, according to baseball-reference.com) of 168.4. He exceeded 10.0 in seven seasons and posted a career high of 14.1 in 1892 for Cleveland. He ranks second on the all-time list, just behind Babe Ruth (183.6).\n- In 1999, baseball named Young was named to its All-Century Team, a dream squad made up of great players from throughout the game’s history. Other pitchers on the team included Roger Clemens, Bob Gibson, Lefty Grove, Walter Johnson, Sandy Koufax, Christy Mathewson, Nolan Ryan, Warren Spahn and Cy Young.","This weekend we’ll present a retrospective on the 1994 baseball season. But first, a look back on the infamous strike of that season, and what led up to it.\nHow come there was a strike? I thought labor relations in baseball are pretty good.\nThey are now. They weren’t then. The Major League Baseball Players Association (MLBPA) was formed in 1953, but it was pretty toothless until Marvin Miller became its Executive Director in 1966, leading to the first collective bargaining agreement (CBA) in 1968. There was a short strike in April 1972. There was a much longer strike from June to August 1981, forcing a split season. There was a two-day strike in August 1985, but nobody remembers that one of because the missed games were rescheduled. There was a preseason lockout in 1990 that lasted 32 days and delayed the regular season by a week, but all games were played. So the 1994 strike was the fifth work stoppage in 22 years that affected the regular season schedule.\nWhat was the bone of contention between the sides?\nMoney, of course. The 1972 strike was before free agency and it was settled when the owners agreed to salary arbitration and a $500,000 increase in pension fund contributions. The subsequent stoppages all occurred after the 1975 arbitration ruling eliminating baseball’s Reserve Clause, which bound players to the same team in perpetuity. In its aftermath, players became free agents after six seasons, and salaries rose rapidly. The 1981 strike was over the owners’ demands for compensation for losing a free agent. They got compensation, but nowhere near as much as they wanted. The 1985 strike resulted in a $33 million increase in pension fund contributions and an increase in the minimum salary to $60,000. The 1990 lockout occurred after the Basic Agreement between owners and players expired at the end of 1989 and the owners wanted to replace it with a revenue-sharing model that would pay players 48% of gate receipts and broadcast revenues. It ended when owners raised their pension fund contribution by $55 million, expanded arbitration eligibility, and raised the minimum salary to $100,000.\nIt sounds like the owners didn’t do well.\nWell, they got some of the things they wanted, like limits on arbitration and free agent compensation, but in general, if you were to declare winners and losers, the players were on a winning streak. Then-Commissioner Fay Vincent’s efforts to end the 1990 lockout were a reason the owners booted him in 1992 (OK, he lost a no-confidence vote). In addition, the players filed grievances against the owners, alleging that ownership colluded to not sign free agents from ’85-’87, and an arbitrator ruled in the players’ favor, awarding them a total of $280 million. (There was a much smaller award, $12 million, in 2006, to settle allegations of collusion after ’02 and ’03 as well.) So the owners were looking to end their slump.\nWhat were the issues in 1994?\nAfter effectively losing four work stoppages and the collusion arbitration, the owners went for the equivalent of a six-run homer. They proposed:\n- Revenue sharing tied to a salary cap, which would result in more even compensation across the clubs (the welfare of small-market clubs was a stated ownership concern)\n- Elimination of salary arbitration\n- Free agent after four years instead of six, with clubs holding a right of first of first refusal (ability to retain the player by meeting the best offer) for years five and six\nThe owners claimed their proposal would raise total compensation (salary and benefits) to $1 billion and raise average salaries from $1.2 million in 1994 to $2.6 million in 2001.\nWhy didn’t the players go along?\nThe MLBPA has always been adamantly opposed to a salary cap, feeling that it would ultimately limit player compensation. MLB is currently the only North American sports league without some sort of salary cap.\nWhy was there a strike in the middle of the season? Had the Collective Bargaining Agreement expired?\nNo, the CBA was good through the end of the year. But in June, the owners withheld a $7.8 million payment owed to the players’ pension plan. That provoked a vote in July by the MLBPA Executive Board in authorize a strike starting August 12. The last games of the year were played on August 11.\nDid anyone try to end the strike?\nSure they did. The sides negotiated through August, but at the end of the month, federal mediators assigned to the negotiations said there was no progress, and talks broke off. On September 8, the players made a counter-proposal, calling for teams sharing 25% of gate receipts among all teams, and a 2% luxury tax on the payrolls of the 16 highest-payroll clubs, with the proceeds divided among the other 12 clubs, both aimed at helping smaller-market teams. The owners rejected it.\nWhen was it apparent the season was in jeopardy?\nAt the end of August, acting commissioner Bud Selig said that September 9 was the deadline for canceling the rest of the season. He pulled the plug on September 14.\nWhen was it settled?\nIt took a while. On December 23, with the CBA about to expire, the owners implemented a salary cap. In January, President Clinton ordered the two sides to the bargaining table. In early February, the owners withdrew the salary cap, following a National Labor Relations Board (NLRB) ruling that it had been imposed illegally, but the two sides failed to meet Clinton’s February 6 deadline to resolve the strike. On that date, the owners’ Player Relations Committee notified all 28 clubs that they could not sign free agents nor enter into salary arbitration. The owners began hiring replacement players for spring training and, if necessary, the regular season.\nWait, there were replacement players? How did that work out?\nOh, it was a crapshow. Orioles owner Peter Angelos, partly because he didn’t want a season with replacement players imperiling Cal Ripken Jr.‘s consecutive-games streak and partly because he was a union attorney, refused to field replacement players and canceled spring training. Tigers manager Sparky Anderson refused to manage replacement players and was put on an involuntary leave of absence. The Blue Jays assigned manager Cito Gaston and his coaches to the team’s minor league complex so they wouldn’t have to manage replacement players. Under Ontario law, replacement workers were not permitted to be used during a work stoppage, so the Jays made plans to play their home games at the Dunedin, Florida spring training facility.\nSo how did the strike end?\nOn March 27, the National Labor Relations Board filed an unfair labor practices complaint against the owners, alleging that the owners had illegally eliminated both free agency and salary arbitration while negotiations were ongoing. The following day, the players voted to return to work if a judge upheld the NLRB ruling. The owners responded by voting 26-2 in favor of using replacement players. On March 31, district judge Sonia Sotomayor–yes, the same Sonia Sotomayor who’s now on the Supreme Court–issued a preliminary injunction against the owners, and her judgment was affirmed by the U.S. Court of Appeals. With free agency and arbitration in place, the players voted to return to work. The strike ended on April 2, the day before the season was going to start with replacement players. The regular season started three weeks late and was limited to 144 games.\nWas a new Collective Bargaining Agreement part of the settlement?\nNope. Sotomayor’s injunction bound both sides to the terms of the expired CBA. Nothing changed until the two sides agreed on a new CBA in December 1996.\nHow about financially?\nEstimates are that the 1994 strike cost the owners $580 million and the players $230 million, but those should be taken with a shaker of salt, since it’s impossible to put a precise estimate on foregone revenues and rosters.\nIn 1994, ABC, NBC, and MLB formed a joint venture called The Baseball Network to air nationally televised games. The strike cost The Baseball Network an estimated $595 million in ad revenues. The joint venture, which was supposed to have run through 1999, ended after the 1995 season when both networks, stung by their losses, opted out. NBC vowed never to do business with MLB again, a vow which stuck for, well, several days–it struck a deal to split All-Star Game and postseason coverage with Fox and ESPN starting in 1996.\nThe strike was a public relations horror show. Early 1995 game accounts were full of stories of fans booing players, throwing objects onto the field (including a hubcap in Detroit–“Hey, guys, I got an idea, let’s bring a hubcap to the game”), and wearing shirts and holding up signs with various permutations on dollar signs and the word greed. Per-game attendance fell 20%, from 31,632 in 1994 to 25,260 to 1995. Attendance at major league games has exceeded 1994’s record per-game total only twice since, in 2007 (32,785) and 2008 (32,528), though that’s in large part due to new stadiums with less seating capacity. If one follows the trajectory of television contracts and MLB Advanced Media, it’s pretty hard to make a case that the game’s struggles in the immediate aftermath of the strike have persisted.\nOh, and the owners’ claim that under their salary cap proposal, average player compensation would rise to $2.6 million in 2001? Whether one believes that or not, the average opening day salary in 2001 was $2.26 million, according to the Associated Press.\nAny less-known impacts of the strike?\nA darker impact of the strike is performance-enhancing drugs. Nobody can ever prove this, of course, but there’s a school of thought that the owners, desperate to win back the fans lost in the strike, turned a blind eye to the growing use of PEDs in the majors, enticed by the draw of more homers and more scoring. Prior to 1994, the last players to hit 50 homers in a season were George Foster, with 52 in 1977, and Cecil Fielder, with 51 in 1990. In the eight years after the strike, there were 18 player-seasons of 50 or more homers.\nSo we don’t end this on a downer, let me add that the strike resulted in a great trivia question: Who are the four MLB players with 16 straight seasons playing 140 or more games? The two players everyone gets wrong are Lou Gehrig (not really close; ALS forced him out of the game at age 36 after 13 straight 140-game seasons) and Cal Ripken Jr. (the Orioles played only 112 games in 1994 due to the strike). The answer: Hank Aaron, Brooks Robinson, Pete Rose (whose streak was broken by the 1981 strike), and… Johnny Damon.Next post: ’94 Throwbacks, Pt 1: Year of the Strike\nPrevious post: Trailing 30 – Last Call for A-Rod Edition"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:c12f1f7f-cdc6-4096-a675-3bd0cf604dcc>","<urn:uuid:6e7fb04e-2229-4d90-8bf6-c1a0c1bcf69d>"],"error":null}
{"question":"What are the two main factors that cause global sea level rise?","answer":"The two main factors that cause global sea level rise are: 1) melting ice that is currently on land (glaciers and big ice sheets like Antarctica and Greenland), and 2) the expansion of water as it heats up.","context":["By Sarah Derouin, Ph.D.\nSea levels have been marching higher for the past century, and rates have accelerated in the past few decades. From 1993 to 2014, average sea level rose 2.6 in., (about an eighth of an inch per year), and it continues to rise at about that same pace.\nWaterfront cities are facing increasing threats from rising waters, including flooding and coastal erosion. By the year 2100, scientists estimate that about a billion people around the world will be at risk of hazards from sea level rise. And with the continued acceleration of sea level rise, current hazards could increase by two to three orders of magnitude.\nSmart adaptation of civil engineering can help mitigate the risk to coastal communities. But for these efforts to be effective, engineers and city managers have to know where sea level rise effects will happen and by how much. But having good information is just the start — communities also need robust plans that will take them through the next century and beyond.\nRecent papers in the journal Earth’s Future showcase new research on more accurate elevation measurements for coastal land at risk from rising sea levels, what some communities have planned for expected inundation, and what sort of land reclamation is taking place around the world as new land is created off existing coastlines.\nUnderstanding where impacts will happen\nThere are two main factors that affect a coastal community’s flood risk: the amount of sea level rise it experiences and the elevation of its coastal land.\n“When we think about sea level rise, globally, there are two main factors that cause sea level to rise. One is melting ice that is currently on land — so glaciers and big ice sheets like Antarctica and Greenland. The other thing that causes sea level rise is that when water heats up, it expands,” explains Andra Garner, Ph.D., a climate scientist at Rowan University in Glassboro, New Jersey.\nBut for individual cities, local sea level rise can be affected by a few other factors that are specific to their locations. “Here in New Jersey, during the last ice age, we were on the edge of a big ice sheet. That ice sheet sitting inland caused the land to rise up, and now that that big ice sheet is gone, we're sinking back down. So our sea levels are rising faster than average here,” explains Garner.\nMost cities and communities get their sea level rise numbers from an international consortium of scientists known as the Intergovernmental Panel on Climate Change. The IPCC just published its Sixth Assessment Report, a comprehensive look at climate change around the world. This includes sea level rise projections for 2100 and the centuries beyond.\nThe IPCC estimates sea level rise values as a range, depending on the uncertainties in natural and human behavior over the next century. “We have global projections for really low emissions, assuming we take drastic action to limit climate change, or really high emissions, where we do nothing,” explains Garner.\nEstimating how far the water will rise is one factor in mitigating coastal flooding; the other is land elevation. While the elevation of coastal areas may seem like a static measurement, the Earth is an ever-changing entity. For example, land can subside over time as groundwater or natural resources are removed, such as is happening in California’s Central Valley. The elevation of land relative to sea level can also change because of rebounding forces when enormous ice sheets elsewhere melt, such as Garner explains is happening in New Jersey, or during reclamation projects where new land is created.\nBecause of these changes, monitoring coastal areas over time is an important part of managing flooding risks. In the past, most coastal elevations were measured with radar images collected by Earth observation satellites, creating a global digital elevation model. However, radar data has limitations — it can’t see through thick vegetation canopies or differentiate built-up areas from natural ground surfaces. Radar measurements reflect off such surfaces, creating inflated ground elevations.\nTo alleviate this problem, researchers turned to new Earth observation satellites that carry lidar sensors. “Lidar is able to penetrate the canopy and improves surface elevation estimates at an accuracy of about 0.5 m at 1 km spatial resolution,” says Ronald Vernimmen, Ph.D., a researcher in the Netherlands and director at Data for Sustainability.\nIn a recent paper, “New LiDAR-Based Elevation Model Shows Greatest Increase in Global Coastal Exposure to Flooding to Be Caused by Early-Stage Sea-Level Rise,” Vernimmen and co-author Aljosja Hooijer used this newly available data and created a lowland elevation model for coastlines. Previous studies that used radar elevations noted that flooding rates would increase as sea levels rose. However, the new lidar models painted a very different picture of flooding.\n“We find that the greatest increase in coastal areas below mean sea level will occur in the early stages of sea level rise, contrary to earlier assessments,” Vernimmen says. Flooding not only occurs faster in the beginning stages of sea level rise, but the researchers also found that in one-third of countries, most of the increase will occur during the first meter of sea level rise.\n“Our findings, therefore, indicate that the time to adapt to rising seas in many areas is indeed shorter than would appear from previous elevation models,” he says.\nDo US cities have a plan for sea level rise?\n“The science of sea level rise is constantly evolving, and of course, one of the reasons that we want to understand sea level is so that we can plan for it in our coastal communities,” says Garner. “One of the questions we had is how well is the latest science making its way out of the scientific realm in our peer-reviewed journal articles and into local assessment reports that ideally are developed by and for local stakeholders, local decision-makers, and policymakers.”\nIn their study, “Evaluating Knowledge Gaps in Sea-Level Rise Assessments From the United States,” Garner and her co-authors, including colleagues and students, tracked down what sort of assessments coastal states or cities had for dealing with sea level rise. In particular, the researchers looked for the year reports were published, how far into the future they were planning, and if communities used a range of sea level rise to capture uncertainty.\nThey divided the country into three sections — U.S. Northeast, West, and South — and compared the local reports to the latest IPCC report. “We found that over half of the locations in the U.S. that we looked at actually are underestimating future sea level rise compared to the high-end projections from the IPCC report,” Garner says.\nGarner added that many places did not have long-term projections beyond the end of the century. “In the U.S. Northeast and the U.S. West, both at least had several reports that do plan for the year 2150 or 2200, but the U.S. South didn't have any long-term projections that go beyond the year 2100,” Garner says. “We view that as a potentially really big knowledge gap in that region to not have that longer-term look at what sea level rise might do.”\nIn addition to shorter sea level projections, many reports in the U.S. South only used one value for sea level rise. “Having just that one number, for us as human beings, is very appealing — we’d like to just have one number to plan for and be done with it,” says Garner. “The unfortunate reality is that sea level change just isn’t that certain.”\nShe explains that one number creates a simpler plan for building guidelines, but that approach might also be underestimating future flooding challenges. “We could have more sea level rise than they’re planning for, which then becomes really costly for trying to retrofit infrastructure.” In fact, recent research has suggested that recent sea level rise may be even higher than previously thought in the U.S. South.\nReviewing the latest IPCC sea level projections can be important for future building and infrastructure as well. For example, Garner notes that a park with lots of green space can likely handle some flooding, but “if it’s a power station or a new hospital, then it’s worth doing a broader search beyond the local projections and see what some of the higher projections from IPCC are looking like.”\nShe says if communities find they have underestimated or outdated sea level estimates, they can easily update their data with a new, free sea level portal tool from NASA. The interactive tool shows the latest sea level rise projections from the IPCC report projected for 2100 and far beyond.\nWhere is land reclamation taking place?\nFor areas affected by sea level rise, engineering can help bolster their coastlines and infrastructure. Activities such as seawall construction or infilling wetlands or shallow seas are reclamation efforts that can protect or even create coastal lands.\n“Land reclamation is not something new. Cities have been building out since the 19th century and even before — the Netherlands is the best example,” says Dhritiraj Sengupta, Ph.D., a geographer at the University of Southampton and co-author of a new study, “Mapping 21st Century Global Coastal Land Reclamation.”\nSengupta notes that over the past century, regions in the U.S. like the San Francisco Bay area and the New York metropolitan area have built out, as have numerous cities in the United Kingdom. As sea levels rise, more cities and infrastructure will be at risk.\nRecent technological advancements in engineering have allowed megacities in Southeast Asia and West Africa to expand through land reclamation, Sengupta says. “I was interested in the environmental impact of land reclamation,” he says. “To assess that correctly, I need to understand the way construction happens.”\nSengupta and his colleagues used NASA and European Space Agency satellite imagery and cloud computing technology to monitor more than 100 cities with populations of more than 1 million over a 20-year time span. The researchers used the imagery and algorithms to detect the changes from land reclamation activities that were happening.\n“There is this hot spot of Southeast Asia, where land reclamation in coastal cities is kicking in,” he says, adding that Western Africa is a second huge reclamation effort. On a global scale, the researchers found that 70% of reclaimed land was in areas facing potentially extreme sea level rise.\nSengupta points out that building these sites is one thing, but maintaining new land in coastal environments requires continual resources. “One common thing which connects all these construction sites is sand,” Sengupta says. “If you're planning to build land and maintain that, with future sea level rise and climate change, you need more sand.”\nHe points out that sand is a natural yet nonrenewable resource. And as sea level rises, more sand will need to be added to reclaimed land to fight off the rising waters. “The land will subside, but you need more and more sand to kind of fill in and then keep that level up. Exactly how sustainable is that? That's the question.”\nCommunication for mitigating sea level rise\nThe researchers agree that good communication — including the latest scientific research and mitigation ideas — is crucial for protecting communities from sea level rise. “I think that communication is super important” between scientists and stakeholders, says Sengupta. “It's important that we have this interaction and ask questions: Do we actually need this? Is it sustainable, keeping in mind the raw materials?”\nDuring these conversations, it’s important to look forward. “I think when you read the study, you might think, ‘Oh my gosh — we’re really failing here,’” Garner says. “I like to point out that I think that might be true in some places. But there are other places that were definitely doing a great job.” She notes that those success stories can provide a road map for communities nearby.\nThis article first appeared in Civil Engineering Online."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:bf9750b1-3828-432c-8663-b558f0824f83>"],"error":null}
{"question":"I'm researching visitor engagement in cultural institutions. How do museums balance maintaining their historical collections while evolving to meet modern visitor expectations, and what barriers prevent people from visiting?","answer":"Museums face a complex balance between preserving history and meeting contemporary needs. On the collection side, institutions like Beamish Museum focus on maintaining historical authenticity while incorporating practical modifications - for example, adding sealed-for-life bearings and electronic ignition to historical vehicles to ensure reliable operation for visitors. Regarding barriers to visitation, research shows multiple challenges: Cost is a major factor, with studies showing admission prices deter many potential visitors. Even more significantly, the perception that museums are 'not for someone like me' is the number one barrier to cultural participation. This is particularly evident in demographic data - only 49% of Toronto residents visited museums in 2016, with even lower rates among immigrants (47%) and Indigenous people (44%). Additionally, studies found that people of color are 65% more likely to avoid cultural events that don't reflect diverse backgrounds.","context":["The last week or so have been particularly busy at work, so the blog’s rate of updates has suffered accordingly! The planning and preparation for eventual reopening within what will be a very different world, a new normal as many call it, is becoming increasingly detailed and extensive.\nSo we will have to be clever, find innovative ways to ensure the appeal of Beamish is maintained and to remind ourselves of why we are here and who we are for. There will inevitably be evolution of our ideas, and an initial limited offer should expand in time as we apply ourselves to the many operational facets that make up the museum and visitor experience. We do, of course, also have to operate at a level that generates some income for the museum and therefore has to be an appealing visit for those who come.\nEarlier this week we carried out some socially distanced filming work (we really ought to be able to drop the endless prefix ‘social distance’ – it is now becoming so normal as to not really be needed) for a museum activity that you will see more of in due course. My part entailed digging the Model T Crewe Tractor out of storage and preparing it for a role. It was gratifying to note that after overnight charging of the battery and the usual walk around inspection, it fired on the first depression of the starter button.\nDriving around the site, I was very aware that sharing lifts, offering a ride on the seat (sometimes a nice way to indulge a visitor’s interest in the T) or showing someone around the vehicle are all going to be things that we cannot do or have to be very careful in managing, for some time to come. Hand washing will be (as it should always be!) second nature, and maintaining a 2 metre distance will also become a new normal.\nWe have tended to focus on the big-ticket events, such as the Great North Steam Fair (which remains an important part of our events programme) but we also know how much pleasure is derived from seeing one or two veteran motorcycles out on site, or examples of the cycle collection. I’m sure a delivery of stone using one of our two tipper lorries in conjunction with Glyder and the narrow gauge railway would enable visitors to enjoy something they can’t readily access elsewhere. I’m also keen to see R025 out rolling the events field too!\nWe will certainly be taking a look at our strategic transport plan and the overhaul/maintenance queue too. Matt and his engineering team have demonstrated what can be achieved in terms of quality through the rebuild of the centre engine, and moving towards that sort of thoroughness and quality is an intrinsic part of the plan. We may, in the future (and as often reported here), not expand the operating fleet as much as we have done in the last ten years, but we will focus on reliability and durability of what we do have. Some examples of this include:\nThe overhaul (rebuild) of Gateshead Tram No.10 – certainly one of the most comprehensive programmes of work that we will have carried out at Beamish on a tramcar, and which is also incorporating sensible modifications (discreetly) that will make maintenance easier and hopefully extend the future working life of this tram.\nThe overhaul of Steam Elephant – again incorporating some modifications in light of experience with this locomotive over the past decade and a half.\nThe reconstruction of the Model T Ton Truck – the work on the Crewe Tractor has shown the benefits of a comprehensive rebuild and incorporation of redesigned components where this is sensible – for instance the sealed-for-life wheel bearings and electronic ignition. We wouldn’t apply these to a curatorial restoration or overhaul, but for a vehicle that needs to start every morning, every time, they are are outwardly invisible and ensure that the visitors see these vehicles at work as much as possible (the Model T Ton Truck will be allocated to the engineering team once completed).\nKerr Stuart 721 – one of the apprentice projects and with the skills within the engineering team that we have, there is no need not to do a thorough job!\nAustin 10 – this has had a thorough mechanical overhaul and no end of body work replaced (all of the curvy tricky bits that seem to rust first!). It will be turned out in due course with new carpet trim and new paintwork – and also (importantly) restrictions on use to ensure it lasts and is available for when we need it…\nWe also have the external contracts, albeit now delayed due to COVID-19, namely Dunrobin and Crosville 716 – both of these are being done to the highest standard and hopefully will endure for decades to come, in operation at the museum.\nSo with this sort of ethos, we may look to reduce the number of period vehicles we have in the ‘common user pool’ (most of which are now out of use awaiting repairs) such as the Wolseley car and Bedford CA ambulance. We now know that period pool vehicles do not work for us, but allocating well maintained period vehicles to a single user works very well – the electricians extract a great deal of work from the Commer van (awaiting repairs needed due to age not use/abuse) and the Morris Commercial, due to taking care of them and restricting the operators of them.\nWe will also continue to have the nice-to-haves – the Dodge bus being an example of this and which, in due course, should become a useful and usable member of the bus fleet as well as an interesting restoration challenge for the Friends of Beamish engineering team.\nThese are mainly musings, for now, but they will help inform the future and the current strategic plan will be adapted to cope with the challenges of the future.\nThe motivation, however, remains the same. As do those objectives set for ourselves. The means of achieving this will inevitably have to adapt and it will be interesting to compare notes with the wider sector (both museums and heritage railways, the latter being that with which we have most in common in this team) as the challenges are faced and met. There are going to be lots of ways to support this and blog readers will start to see the museum’s social media revealing how this can be accomplished, shortly.\nPlease do support us – by engaging with the museum virtually or by visiting (and spending money with us!) when it is safe for us to reopen. 95% of what we spend comes from what we earn at the gate and through the tills – this is the reality of being an independent museum and we have certainly enjoyed this independence when it comes to creating exciting and vivid projects – looking back through the blog over the last 10 years or so reveals just how much can be done as an independent museum and this is something of which we are very proud.\nI will finish my monologue here as readers will no doubt want to see some photographs! So here are a few photos of the Crewe Tractor in the sunshine between ‘takes’ during the recent in-house filming work…\nGroverake Mine and Tramway\nWith restrictions easing a little, but still mindful of the need to socially distance from others, a quick trip was made to Groverake Mine, west of Rookhope (and to the north of Weardale) to scratch an itch regarding something read about and which I thought that I could see on Google Earth – namely the shape of a narrow gauge engine shed, long abandoned, on the mineral tramway once worked by 2 ft. 6 in. gauge Black Hawthorn 0-4-0ST locomotive.\nLead mining in Weardale has marked the landscape throughout the dale, but Groverake Mine is probably one of the most striking remnants of the industry, due to the operation of the mine until 1999 for the extraction of flourspar and the survival of one of the headstocks latterly in use on the site.\nW. B. Lead, owned by the Blackett-Beaumont family, was one of two principle lead mining businesses in the area (the other being the London Lead Company – operators of the Cornish Hush mine where the inspiration for our replica of 0-4-0WTG Samson worked), owning around a third of the mines from the seventeenth century until, in the case of W. B. Lead, the interests were sold and dispersed in the 1870s, with the Weardale Lead Company Ltd being formed to take over the W. B. Lead interests from 1883.\nGroverake was served by two railways – the standard gauge branch of the Weatherhill & Rookhope Railway (owned by the Weardale Iron Company) and the narrow gauge railway built by the Lead Company to link the mine at Groverake with a washing plant at Ripsey Mill around a mile to the east. Along the route it also served a mine at Wolfcleaugh, the latter closing in 1912. Steam haulage was used on this tramway between 1889 and 1909, though the tramway was in use after this date, presumably using horses as motive power.\nGroverake was to become the leading producer of flourspar in the ore field and was owned and developed by various operators until the last, Sherburn Minerals, closed the operation in 1999. Flourspar is a source of hydrogen flouride, used as a flux for lowering the melting point of raw materials in the manufacture of steel and also has manufacturing applications in the production of aluminium, glass, enamels and also for cooking utensils.","Most Torontonians aren't going to museums. To attract new visitors, the Art Gallery of Ontario is making admission free all year for some and cheaper for others\nMuseums are in the middle of an existential crisis. Built to house the collections of wealthy colonialists, they are evolving into places we expect to reflect our lives and the diversity of the world around us.\nBut before we can experience art, we need to be able to afford the admission. The cost of housing, internet access and entertainment are rising in Toronto. In response, one major art institution is changing its pricing to attract new visitors.\nStarting May 25, admission to the Art Gallery of Ontario (AGO) will be free for visitors age 25 and under. The museum is also introducing a new $35 annual pass that offers unlimited admission all year to people over 25 (without the perks members get, like previews).\nSingle-ticket prices are changing, too. Instead of $20 to view the permanent collection and $25 for special exhibitions, one visit will now cost $25 and include both. The gallery’s membership program remains the same, as do all of the current free entry initiatives such as free passes for Toronto Public Library cardholders and free Wednesday nights.\nThe idea behind the new pricing, which is launching as a year-long pilot, is to encourage visitors to make going to the AGO a habit – a space to come and go on a regular basis rather than on a special outing. But the goal is also to attract new visitors who can’t afford $25 a pop to visit.\n“They have to be new people. [Otherwise] it’s probably not going to work,” says the AGO’s director and CEO Stephan Jost. “Can we be affordable to somebody who is changing sheets at the Hilton, is making $18 an hour and has two kids? I think $35 a year is starting to be that.”\nDuring his first week on the job, Jost did a meet-and-greet with visitors lined up for Free Wednesday nights and immediately heard $25 was too high a price. In Culture Track: Canada, a 2017 survey by Nanos Research of more than 6,000 national culture-goers, cost was also cited as a reason people don’t go to museums.\nThe AGO expects to attract 125,000 annual pass-holders and around 165,000 free visitors under 25. If the model fails, the risk is about $4 million, says Jost, but the AGO has mitigated a potential revenue hit by fundraising $1.7 million from five big donors interested in supporting greater accessibility.\n“A large American company gave us $200,000 U.S. on the condition they be anonymous because they are big cultural funders throughout North America,” Jost says. “Like me, they are looking at the [admissions] model and wondering if we’ve had it wrong.\n“Once we get it working, the challenge is to keep it at $35,” he continues, adding that he does not expect the new pricing to increase revenue. “It’s easy to be like, ‘We could just make it $40’ and then in three years it’s $60 and you’re back at square one. The goal will be to control costs. The goal is to break even but increase access.”\nAdmission prices are just one barrier. People have to want to go in the first place. The AGO has been criticized as an exclusive realm for the corporate elite, with a collection that does not reflect the diversity of Ontario’s cities. Moreover, global movements among Indigenous people and former European colonies to reclaim pillaged artifacts are forcing big art institutions to rethink why they exist and for whom.\nIn the 2018-19 fiscal year, 950,841 people visited the AGO, down from the record high of more than a million in 2017-18. (The big draws that year were Guillermo del Toro and Georgia O’Keeffe.) Meanwhile, Royal Ontario Museum (ROM) had 1.44 million in 2017-2018 and 1.34 million in 2018-19, making it the country’s most-visited museum.\nBut how many of those visitors are among Toronto’s estimated 2.81 million residents?\nThe AGO doesn’t have a complete picture of who visits, which is part of the impetus for the annual pass. Of the 950,841 who visited last year, half are unique visitors and one-fifth of those are members, meaning four-fifths are anonymous. The more people who sign up for the new pass, the more people become known to the gallery and the more targeted marketing and programming will become.\nThe gallery does exit surveys, so it knows that roughly a third of visitors go to the AGO to be alone, 60 per cent are women and largest age group is 20-somethings. Race is trickier to track given most visitors self-identify as Canadian, says Jost, but underrepresented demographics include Asian visitors.\nTo attract more diverse audiences, the AGO restructured its Canadian art department to boost Indigenous art, mounted solo shows for Rebecca Belmore and Mickalene Thomas, and started acquiring more contemporary work by Indigenous and Black artists for the permanent collection, particularly pre-1970. “When you look at our collection, it’s extraordinarily off-kilter when it comes to Black artists,” says Jost.\nIn the Culture Track report, “it’s not for someone like me” was cited as the number one barrier stopping Canadians participating in cultural activities, with cost coming in second. It also noted that Indigenous people and people of colour are 65 per cent more likely to stay away from cultural events that do not reflect people from a range of backgrounds.\n“We live in economically challenging times and so the cost of museum visits is a factor in attendance. The lineups for free events show how important the issue of ‘free’ is. Museums should be public services like libraries.”\nAnother study shows the majority of Torontonians are not visiting museums.\nAccording to research based on 2016 census data presented by Kelly Hill, president of arts and culture research firm Hill Strategies, at the Ottawa Cultural Summit in January, only 49 per cent of Toronto residents age 15 or older attended a museum or art gallery at least once in 2016 – one per cent higher than the national average. Ottawa had the highest museum attendance rate of seven major Canadian cities at 63 per cent, followed by Calgary with 59 per cent and Vancouver with 56 per cent. Montreal ranked lowest at 46 per cent.\nMuseum attendance rates are even lower for immigrants (47 per cent) and non-English speakers in Ontario’s major cities (47 per cent), and Indigenous people in Canada (44 per cent).\n“Toronto attendance at museums is average,” says Toronto-based museum planner and consultant Gail Lord. “I don’t think average is good enough for Canada’s largest and most diverse city with some of the country’s largest museums. Average is a problem.\n“We live in economically challenging times and so the cost of museum visits is a factor in attendance,” she adds. “The lineups for free events show how important the issue of ‘free’ is. Museums should be public services like libraries.”\nLord notes there are other barriers to attendance, but admission charges make going to the AGO or the ROM “impossible for low-income families and a special event for working families.”\nIn 2001, the United Kingdom reintroduced free admission at 67 national museums, including 25 in London (though many, like the Tate Modern, rake in revenue by charging for special exhibitions). As Lord and co-author Ted Silberberg note in the 2015 book Balancing Mission And Money: Issues In Museum Economics, there was an overall increase in attendance of 151 per cent between 2001 and 2010.\nSo why aren’t museums in Toronto free?\n“There’s just too big of a financial risk,” says Jost. “We ran the numbers and somebody would have to give us $10 to $15 million a year to do that.”\nLord and Silberberg also point out the move to make British museums free was supported by £1 billion (CAD $1.76 billion) in public lottery spending plus £44 million (CAD $77.6 million) for operating costs by the Department for Culture, Media and Sport.\nLast fall, a House of Commons committee report entitled Moving Forward – Towards A Stronger Canadian Museum Sector noted that most federal museum funding in this country is for special projects, not operating costs. The report also recommended the Department of Canadian Heritage study admission fees and their impact on attendance, particularly among underrepresented groups.\nIn 2018, 10 per cent of the AGO’s revenue, or $7.4 million, was from admissions. Nearly a third came from government grants totalling $22.1 million. (Although Doug Ford’s Conservative government just slashed the Ministry of Tourism, Culture and Sport’s budget by $60 million, Jost says the AGO’s provincial base funding of just over $21 million was spared.)\nAs Lord told the committee, “The most inefficient way of raising money is through admission charges. Admission charges are costly to administer. They only bring in 10 per cent to 12 per cent of revenue, and they keep so many people away… The whole question of how we charge, and why we charge and what we charge for needs to be looked at.”\nFree or not, museums need to change the way they are perceived. Jost does not see other museums or galleries as the AGO’s competition, but rather streaming platforms like Netflix that give people an excuse to stay home.\n“Social isolation is something we have to deal with,” he says. “With some of our regular visitors, particularly our older visitors, you can see that they used to be socially isolated people. They come here because it’s part of their week.”\nIn tandem with the new pricing, the AGO is starting a new day-long block-party-style series called AGO All Hours. Timed with the launch of each new season and running from 10:30 am to 1 am, it will be all-ages and feature an array of programming, from kid-friendly activities to pop-up talks and DJs. The event is the evolution of the First Thursdays party, which wound down in February after six years.\nThe party started in order to increase attendance among 20-somethings. Goal achieved, Jost says First Thursdays had lost its “cool factor” and it was time to reinvent.\n“I don’t mind a great party, but we’re not a nightclub,” he says. “We’re doing really well so maybe now is the time to focus more on our mission.”\nEven with an emphasis on free, museums need to do more to ensure they are serving the public good.\nA 2009 study of UK museums by independent charity The Art Fund showed that free admission did not significantly change the profile of British visitors. People still felt barriers, such as “a lack of knowledge about the art on display” and “a feeling of intimidation about the buildings themselves [that] made people feel they were not qualified to appreciate the art owned by the nation.”\nLast year, the Montreal Museum of Fine Art teamed up with a doctors’ association to prescribe free trips to the museum as a way to alleviate stress and improve mental health. The ROM followed suit in January. It also became more visibly welcoming by reopening its Queen’s Park entrance and starting a Community Access Network that gives 100,000 free visits a year to communities with financial, social and cultural barriers to access.\nLord thinks museums could go further to create value for Torontonians by collaborating more.\n“Both the ROM and the University of Toronto galleries have exhibitions on India – at the same time and within walking distance,” says Lord. “However, they did not co-market. An Indian festival with food and performances and events on Philosopher’s Walk – joint symposiums and fashion shows – would have created excitement and a sense of a destination.\n“Toronto museums do not do that sort of thing,” she says. “They have much less impact than they could have.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:4400f42b-1dfe-4a3e-b06e-67a3d93a46e5>","<urn:uuid:b3a88892-693f-4643-be6e-18f9693127aa>"],"error":null}
{"question":"Is hand positioning equally important for safe operation of both hammer drills and angle grinders when working with concrete?","answer":"Yes, both tools require careful hand positioning for safe operation. A hammer drill must be held with both hands - one gripping it like a handgun and the other either using an auxiliary handle or bracing the back of the drill. Similarly, angle grinders require maintaining a firm grip and clear surroundings to prevent accidents during the high-speed operation.","context":["How To: Drill Into Concrete\nBore a hole through masonry or concrete in a minute or less when you use this special tool and the correct technique.\nWhile concrete’s cool, industrial aesthetic is a go-to design touch in modern homes, its density and strength are what have long made it an attractive building material. That durability can be quite the obstacle, however, for homeowners trying to mount an art installation on a concrete feature wall, drill a hole to set a fence post at the end of the driveway, or sink a hole through the back corner of a concrete countertop. Such activities run the very real risk of damaging a drill bit or accidentally marring the concrete surface in an overzealous, poorly executed attempt at how to drill into concrete.\nYou can, in fact, drill a hole in a concrete interior feature wall armed with nothing more than your trusty rotary drill and a masonry bit—so long as you take care to not burn out the motor of the drill or demolish the bits. Older concrete, however, is often much more dense than some of the cosmetic concrete used in modern finishes, so boring through a 50-year-old concrete foundation with your old-school rotary drill just won’t cut it. When you’re working with older concrete—or if you’re planning to drill multiple holes about two to four inches deep and up to ¾ inch wide—it’s best to upgrade to an electric hammer drill.\nSpecially designed for drilling into masonry or rock using a rapid hammer action, these drills and their carbide-tipped masonry bits are widely available at tool rental shops. A quality hammer drill (also known as a rotary hammer) can bore a two-inch-deep, ¼-inch-wide hole in less than a minute, which is much faster than a rotary drill and thus justifies its roughly $40 afternoon rental cost.\nWhen renting or buying a hammer drill, look for one with good power, ideally more than one speed setting, a stop function, and an auxiliary handle for your spare hand for enhanced comfort, control, and safety. Among others, DeWalt, SKIL, Black & Decker each offer a popular model suitable for most DIY applications. Note that although the Dewalt tool (about $100 on Amazon) costs more than twice as much, it offers more power than either the SKIL (about $50 on Amazon) or Black & Decker models (also about $50 on Amazon).\nMATERIALS AND TOOLS Available on Amazon\n– Hammer drill\n– Masking tape (optional)\n– Tungsten carbide masonry bits\n– Large masonry nail (at least 3 inches)\n– Canned compressed air\nMark the desired position of the holes in pencil on the concrete surface, and double-check their locations before you proceed. Also, during this planning and prep work, consider the drilling depth necessary for each hole. If your drill’s features include a stop bar, set it to the exact depth you want by following the drill manufacturer’s specifications. No stop bar feature? Instead, wrap a piece of masking tape around the drill bit to show you where to stop.\nDon your goggles, then insert the appropriate tungsten carbide masonry drill bit for your desired hole size into the hammer drill. Next, get into position to drill by planting your feet firmly on the ground, shoulder-width apart. Hold your drill securely with both hands: Grip it in one hand like a handgun, and, if there’s no auxiliary handle for your spare hand, use that hand to brace the back of the drill.\nIt’s critical to control the drill so it doesn’t run away once you begin work. When you lean in to bore the hole, the drill bit should be perfectly perpendicular to the concrete. Be prepared for some recoil from the drill’s hammer action.\nMake a guide hole first. Many hammer drills offer only two speeds, so turn your drill on at the slower speed for best control when making the guide hole. If your drill has only one speed, then work in short, controlled bursts of a few seconds each until you’ve established a hole. The guide hole needs to be just 1/8 to ¼ inch deep.\nWhen you start with a guide hole at least 1/8 inch deep, your drill will be easier to control, but all the same continue to operate the drill with a steady, light-but-firm touch so you’re never forcing it in. If you’re feeling confident, turn the speed to full, keeping a firm grip on the tool with both hands, and drill into the concrete until the hole is complete.\nBeware: Concrete can have air pockets and pebbles or stones that can make resistance unpredictable, with the result that it can be disturbingly easy to lose control of the tool for a moment.\nIf you hit obstructions, never force the drill farther into the concrete. This can damage the bits or drill, or cause you to lose control of the drill and mess up your hole, damage the concrete surface, or worse.\nWhenever you reach any too-tough-to-crack spots that impede progress, set the drill down and grab the masonry nail and hammer. Put the tip of the masonry nail at the problem spot and give it a few taps—not hard whacks—with the hammer to break up the obstruction. When you’re done, resume drilling the concrete at a slow speed until you’re sure you’ve passed the rough patch.\nPeriodically pull the drill out to brush away concrete dust. Considering you can bore a two-inch hole in under a minute using a hammer drill, pausing every 15 to 20 seconds should suffice.\nOnce you have drilled your hole to the necessary depth, blow all of the concrete dust out of the hole with a can of compressed air then vacuum up whatever has fallen to the ground. You should still be wearing your goggles throughout this process in order to protect against any concrete dust and shards that might fly in your face and scratch your eyes.\nRepeat this procedure for any other holes you need. Once through, a pass with a vacuum will make cleanup a breeze.","Maximizing Angle Grinders for Concrete Cutting in Melbourne\nEfficient and Precise Cuts\nIn the dynamic construction landscape of Melbourne, angle grinders are a popular choice for cutting concrete. Equipped with a diamond blade, they provide:\n- Clean, sharp cuts in concrete blocks.\n- The ability to create precise openings in walls.\n- While suitable for surface-level work, for deeper cuts beyond 10 cm, a masonry cut-off saw is recommended.\nSelecting the Appropriate Grinder\nThe depth and quality of the cut depend on the grinder’s size:\n- Small grinders (around 115mm) handle cuts up to 25-38mm deep.\n- Larger grinders (approximately 230mm) can manage cuts up to 76-89mm deep.\n- The right grinder ensures efficiency and accuracy.\nTechniques for Optimal Cutting\nA steady hand and a methodical approach are crucial:\n- Starting the Cut: Gently lower the spinning diamond blade onto the concrete to create an initial divot.\n- Deepening the Cut: Continue to deepen the cut, maintaining control and precision.\n- Reducing Dust: Employ a back-and-forth motion with moderate pressure to minimize dust and debris.\nGiven the high-speed operation and potential risks, safety is paramount:\n- Always wear protective gear: safety goggles, gloves, and dust masks.\n- Ensure a firm grip and clear surroundings to prevent accidents.\n- Utilize the correct blade type, such as a diamond blade, for safe and effective cutting.\nEnlisting Professional Expertise\n- Expert knowledge of local construction practices.\n- Access to the right tools and safety equipment.\n- Assurance of precise, clean, and safe cutting tailored to Melbourne’s unique architectural requirements.\nFrequently Asked Questions About Angle Grinders and Concrete Cutting\nCan angle grinders cut through concrete effectively?\nYes, angle grinders equipped with a diamond blade can effectively cut through concrete, making precise, sharp-edged cuts and even small square openings.\nWhat size angle grinder do I need to cut concrete?\nThe size needed depends on the depth of the cut. A 115mm grinder is suitable for up to 25-38mm deep cuts, while a 230mm grinder can handle cuts up to 76-89mm.\nIs it safe to cut concrete with an angle grinder?\nWhile it’s possible, it requires proper safety measures, including wearing protective gear like goggles, gloves, and dust masks, and ensuring a firm grip and clear surroundings.\nWhat type of blade should I use to cut concrete with an angle grinder?\nA diamond blade is the most suitable for cutting concrete due to its strength and ability to make clean, precise cuts.\nHow can I minimize dust when cutting concrete?\nUse a back-and-forth motion with moderate pressure, consider wet cutting methods, or attach a dust shroud and vacuum to the grinder.\nCan I cut reinforced concrete with an angle grinder?\nYes, but it’s more challenging due to the metal rebar. A high-quality diamond blade is required, and more time and effort will be needed to cut through both the concrete and rebar.\nWhat are the risks of cutting concrete with an angle grinder?\nRisks include potential injury from the high-speed operation, kickback, and exposure to dust. Using the tool improperly can also damage the grinder or blade.\nHow deep can I cut with an angle grinder?\nA 115mm grinder can cut 25-38mm deep, and a 230mm grinder can cut 76-89mm deep. The depth also depends on the blade size and type.\nShould I use a wet or dry blade for cutting concrete?\nWet blades are typically used for larger, stationary saws. For angle grinders, dry blades are commonly used, but for deeper cuts or to reduce dust, wet cutting with a suitable blade and water supply can be considered.\nCan angle grinders make curved cuts in concrete?\nYes, with the right technique and a smaller-diameter blade, angle grinders can make curved cuts, though it requires practice and precision.\nHow do I maintain my angle grinder when cutting concrete?\nRegularly check and replace worn-out blades, clean the tool after use, ensure all fittings are tight, and follow the manufacturer’s maintenance guidelines.\nWhat are the signs that the concrete cutting blade needs to be replaced?\nSigns include reduced cutting speed, excessive vibration, visible blade wear, or chipped and missing segments.\nAre there any specific techniques for cutting concrete with an angle grinder?\nStart with a shallow score to guide the cut, proceed slowly and steadily, and use a back-and-forth motion to prevent overheating and reduce dust.\nWhere can I seek professional help for concrete cutting in Melbourne?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c5083324-13d2-4ca7-80ae-d1648ed76fc8>","<urn:uuid:5affb516-7f0a-4bbb-96a9-f3f517e523e7>"],"error":null}
{"question":"What is significance of naval flags in military operations, and how do navy racks provide sleeping arrangements for sailors?","answer":"Naval flags serve multiple specific functions in military operations, including national ensigns, signal flags, rank flags and jacks that are flown exclusively by naval vessels and shore establishments. Meanwhile, navy racks are the primary sleeping accommodations for enlisted sailors aboard naval vessels, designed to maximize limited space. They typically measure 6.5 feet in length, 30 inches in width for single racks, and are often stacked in multiple tiers with narrow passageways between them. The racks include features like privacy curtains, storage compartments, and can be customized with mattress toppers and organizational accessories for improved comfort.","context":["- NAIANT (or NATANT)\n- The heraldic term used when a fish (or occasionally a water-fowl) is shown swimming per fess, usually towards the dexter – natant (see also\n‘per fess’ and\nArms and Flag of Nyon, Switzerland (Wikipedia\nFlag of Fortune, Canada (fotw)\n- NAIL (or NAILING) ONE’S COLOURS (COLORS or FLAG) TO THE MAST\n- In British RN and some other usage, the term (employed as a metaphor for taking an\nirrevocable action) which refers to a ship’s ensign being nailed to the mast before or during\nan engagement at sea, thus making it impossible to strike (or lower) it in surrender\n(see also ‘strike’).\n- NAILS (or NAILING)\n- 1) (n) One means by which a colour or parade flag is sometimes fixed – often by a precisely regulated number of nails -\nto its staff (see also ‘colour 2)’, ‘colours 2)’,\n‘parade flag’, ‘sleeve’, ‘tab’ and\n- 2) (adj) The act, sometimes ceremonial, of attaching a colour or parade flag to is its staff by nails.\nInfantry Colour, Germany 1936 (fotw)\n- A frequent misspelling of the heraldic term “naiant” – see ‘naiant’.\nArms and Flag of Kali, Croatia (fotw)\n- NAISSANT (or NASCENT)\n- The heraldic term for a charge or figure emerging from the side of a shield, banner of arms or a flag,\nor the centre of an ordinary – nascent – but see ‘issuant’ with following note (also\n‘banner of arms’ and ‘ordinary’).\nFlag of Kalbe upon Milde, Germany (fotw);\nArms of Ostrów Wielkopolski, Poland (fotw);\nFlag of Chavannes-sur-Moudon, Switzerland (fotw)\n- NAME PENNANT\n- The term for a flag or pennant that bears the name of a ship or possibly\na shipping company or an institution – an\nThe John Gibson, Barque, Nova Scotia, 1875 (fotw)\na) The (often unofficial) practice\nof flying such a flag or pennant aboard ship began in the 19th Century, and could\nnow be largely (but certainly not entirely) obsolete.\nb) The practice of displaying a name pennant is continued by some universities\nin the US - particularly at sporting venues (see also\n‘institutional flags (unofficial)’\nand ‘sports flag 2)’).\n- NARROW PENNANT\n- See ‘masthead pendant’.\nNarrow/Masthead Pennant of Norway (fotw)\n- NATIONAL ARMS (or COAT OF ARMS)\n- See ‘state arms’ under\nNational Arms of Venezuela (fotw); National Arms of\nSan Marino (fotw); National Arms of\nMontenegro (fotw); National Arms of\n- NATIONAL BANNER\n- See ‘banner 8)’.\n- NATIONAL COLOUR (or COLOR)\n- 1) See ‘colour 2)’.\n- 2) Where only one colour is selected (generally one appearing on the flag)\nto be the national colour, as in for example the Rhododendron Red of\nNational Colour (Army and Air Force), US (fotw)\n- NATIONAL COLOURS (or COLORS)\n- 1) A plural form of ‘colour 2)’\n(see also ‘colours 4)’).\n- 2) Usually the two or three principal colours of a national flag - for example,\nthe blue, white and red / red, white and blue of France and the US, or the red and white of Poland - landesfarben (see\n‘roundel 1)’, ‘sash\n‘state colours 3)’).\nColour of the Royal Navy 1939 – 1952, UK (fotw); National Colours of Italy, the US, France and Poland\nPlease note with regard to 2) that the colours concerned need not necessarily be those of the relevant national flag,\nas in for example, the green chosen by Slovenia or the Australian national sporting colours of green and gold.\n- NATIONAL EMBLEM\n- See ‘emblem, state, national or Royal’ under ‘emblem’.\nNational Emblem of the Georgian SSR 1922 – 1990 (fotw);\nNational Emblem of the Kazakhstan SSR 1937 – 1992 (fotw);\nNational /State Emblem of Romania (1948-1952) (fotw)\n- NATIONAL FLAG\n- 1) A flag that represents an independent state, especially a nation-state.\n- 2) A flag of a formerly independent state or of a non-independent national\ngroup (see also ‘tribal flag’)\n- 3) That flag which is generally thought of as representing a state when specific\ncircumstances of display are not considered – for example, some flag books might give\na plain tricolour with stars as the national flag of Venezuela, whilst others may\nshow the flag with arms as illustrated below (see also ‘tricolour 1)’).\nNational Flag of Andorra; National/Civil Flags, Venezuela\na) In some countries the national\nflag is available for use by all citizens. In other countries, however, the national\nflag is restricted to official use by law or custom, with a variant of it as a\ncivil flag for use by private citizens.\nb) The national flag\nmay have other variants with specific uses on land and/or sea, and that these\nare herein listed separately according to type (see also those flags listed under\n‘ensign’, as well as\n‘civil flag’, ‘jack’,\n‘state flag’ and\n- NATIONAL ORNAMENT\n- AA decorative strip (usually placed along he hoist of a flag) intended to represent\na folk or traditional element of national culture, and particularly prevalent\namong Eastern Slavic and Siberian states such as Belarus, Kazakhstan and Mariy\nEl – see ‘folklore ornament/a>’.\nNational Flag of Kazakhstan (fotw); National Flag of\nBelarus (fotw); Flag of\nMariy El, Russian Federations (fotw)\n- NATIONAL PENNANT (or WIMPEL)\n- See ‘wimpel 1)’.\nThe National Pennant/Wimpel of Norway (fotw & CS)\n- NATIONAL SERVICE FLAG (or BANNER)\n- See ‘service flag 4)’.\nNational Service/Sons in Service Flags US (fotw)\n- NATIONAL SYMBOLS\n- Those things, usually established by law, which have been adopted as being symbolic of a\ncountry, these may include the national flag, national coat of arms, emblem or seal, the\nnational colours, the national anthem and possibly a plant, fruit, animal and/or a bird etc.\n(see also ‘national colours 2)’ and\n‘state symbols 2)’).\nSome National Symbols of\nIndia: National Flag, National Emblem, National Animal – Tiger, National Bird - Peacock and National Flower - Lotus (fotw and Official Website)\n- NATIONALLY-CANTONED FLAG\n- See ‘canton flag’.\nJoint Service Flag, Jordan (fotw)\n- In heraldry see ‘proper’.\nArms and Flag of New York, US (fotw)\n- NAVAL CROWN\n- A crown generally (but not exclusively) formed from the sterns and square\nsails of ships placed upon a circle or fillet, and loosely based on an ancient\nRoman triumphal ornament of the same name (see also\n‘mural crown’ and\nFrom left: UK (Parker), Italy (free.vector)\n- NAVAL ENSIGN\n- See under ‘ensign’.\nNaval Ensigns: Croatia,\nJapan and the UK (fotw)\n- NAVAL ENSIGN OF HONOUR\n- See under ‘ensign of honour’.\nNaval Ensign of Honour, Yugoslavia 1949 – 1991 (fotw)\n- NAVAL FLAG\n- 1) Generically any flag or pennant used in a solely naval environment.\n- 2) Specifically flags such as jacks, masthead pennants, signal flags, rank flags and ensigns,\nexcept where the latter is also the national flag/war flag, flown exclusively by naval vessels and\nnaval shore establishments (see also\n‘naval ensign’ under\nand ‘suit of colours’).\nNaval Jack of Spain (fotw); Naval Ensign of\nSignal Flag Numeral 1 in the RN/USN/NATO Code (fotw);\nFlag of Naval Group Commander, Morocco (fotw)\nPlease note that the Editors consider the generic\ndefinition in 1) as too broad to be employed with any accuracy and that the various flags/pennants\nlisted in definition 2) and defined separately herein, should be preferred in description.\n- NAVAL JACK\n- See under ‘jack’).\nNaval Jack of Russia (fotw);\nNaval Jack of Croatia (fotw);\nNaval Jack of Norway (fotw);\nNaval Jack of Cuba (fotw)\n- NAVY FLAG\n- See ‘branch of service flag’\n(also ‘armed services flag’\nand ‘naval ensign’ under ‘ensign’).\nNavy Flag, Thailand (fotw);\nNavy Flag, The Philippines (fotw);\nNavy Flag, Venezuela (fotw);\nNavy Flag, Colombia (fotw)","A Navy rack is a unique and essential piece of furniture found on naval vessels that serve as sleeping quarters for the enlisted personnel. These racks are often a subject of curiosity for those who have never served in the Navy, as they differ significantly from the typical beds one would find on land.\nIn this article, we will explore what a Navy rack is, its dimensions, and how to make it more comfortable for the sailors who rely on it for rest during their demanding duties. We will also delve into the importance of Navy rack curtains and sheets, offering tips on how to choose the best bedding. By the end of this article, you'll have a better understanding of Navy racks and how to improve their comfort and functionality.\nWhat is a Navy Rack?\nA Navy rack, also known as a berthing or bunk, is the primary sleeping accommodation for enlisted sailors aboard naval vessels, including aircraft carriers, destroyers, submarines, and other ships. These racks are designed to maximize the use of limited space on naval vessels while providing sailors with a place to rest and sleep during their often long and demanding deployments at sea.\nNavy racks come in various sizes and configurations, depending on the type of ship and its specific layout. The most common rack sizes are \"single racks\" and \"double racks.\" Single racks are designed for individual use and measure approximately 6.5 feet in length, 30 inches in width, and 5.5 feet in height. On the other hand, double racks are designed to accommodate two sailors and are slightly larger, measuring around 6.5 feet in length, 42 inches in width, and 5.5 feet in height.\nThe design and construction of Navy racks prioritize space efficiency. They are typically stacked in multiple tiers, with narrow passageways between them for access. This arrangement allows naval vessels to maximize the use of available space, as ships must accommodate a large crew and various equipment.\nWhile Navy racks are functional and space-saving, they can be quite Spartan and uncomfortable, making it essential for sailors to find ways to improve their sleeping conditions.\nTips to Help Make Your Navy Rack More Comfortable And Efficient\nHere are some tips to help you to optimize the comfort and efficiency of your navy rack.\n- Personalizing Your Space: To make your Navy rack more comfortable, consider adding personal touches. While space is limited, you can bring small items like family photos, a favorite book, or a soft pillow to make your rack feel more like home.\n- Mattress Toppers: Navy rack mattresses are usually thin and firm to save space. Consider investing in a comfortable mattress topper or memory foam pad to add cushioning and make your sleeping surface more inviting.\n- Organizational Accessories: Navy racks have storage compartments for personal belongings, and using dividers and organizers can help keep your space tidy. This not only makes your area more comfortable but also more functional.\n- Earplugs and Eye Masks: Life aboard a naval vessel can be noisy and well-lit at all hours. Earplugs and eye masks can help you sleep better by blocking out unwanted noise and light.\n- Stay Hydrated and Well-Nourished: Proper nutrition and hydration are essential for restful sleep. Avoid consuming caffeine or heavy meals close to bedtime, and be mindful of your dietary choices.\nNavy Rack Curtains and Why They Are Important\nCustom navy rack curtains play a vital role in ensuring the privacy, comfort, and functionality of the sleeping quarters on a naval vessel. Here's why they are essential:\n- Privacy: Navy racks are often stacked in tight quarters, and maintaining privacy is challenging. Curtains offer sailors a degree of personal space and a sense of privacy during their off-duty hours. This privacy is crucial for unwinding, changing clothes, and personal grooming.\n- Light Control: Naval vessels operate around the clock, which means that sleeping quarters can be well-lit at any time. Navy rack curtains help block out ambient light, allowing sailors to sleep during daylight hours or when overhead lights are on.\n- Noise Dampening: Navy ships can be noisy places, with machinery, engines, and crew members working around the clock. Curtains can help dampen noise, creating a more conducive environment for rest.\n- Division of Space: On ships with open berthing areas, curtains can help separate sleeping spaces and provide a sense of individuality for each sailor.\n- Added Comfort: Beyond privacy and practicality, curtains also contribute to the overall comfort of the Navy rack. They add a personal touch to the otherwise spartan environment and help create a cozy sleeping nook.\nOrganizing your Navy Rack Using Custom Navy Rack Curtains\nCustom curtains not only add a touch of individuality but also serve a functional purpose in terms of privacy, light control, and organization. Here's how you can organize your rack space using custom Navy rack curtains:\n1. Measure Your Rack\nStart by measuring the dimensions of your Navy rack. Ensure you take accurate measurements of the width, height, and length of the space where you plan to hang the curtains. This will help you order or make curtains that fit perfectly.\n2. Choose the Right Fabric\nSelect a fabric that suits your personal style and preferences. Ideally, choose a fabric that is durable, easy to clean, and offers some degree of privacy. Cotton, polyester, or a blend of these materials can work well. Avoid fabrics that are too sheer, as they may not provide the desired privacy.\n3. Select a Color or Pattern\nDecide on the color or pattern for your custom curtains. This is where you can add a personal touch to your sleeping space. You can choose a color that matches your bedding or go with a pattern that reflects your personality or interests. Darker colors may help block out light more effectively.\n4. Hanging Mechanism\nThere are various ways to hang custom curtains in your Navy rack. You can use curtain rods, tension rods, or hooks. Make sure to choose a method that is secure and doesn't damage the ship's infrastructure. Check with your ship's regulations or guidelines to ensure you're using an approved hanging method.\n5. Curtain Length\nWhen measuring the length of your curtains, make sure they reach from the ceiling to the floor. This provides maximum privacy and light control. You can also choose to have the curtains reach only to the height of the mattress for a cozier feel.\n6. Securing the Curtains\nEnsure that your curtains are secured in a way that prevents them from swaying or falling during rough seas or sudden ship movements. You might need to use clips, ties, or Velcro straps to keep the curtains in place.\n7. Maintenance and Cleaning\nNavy ships can be dusty and exposed to various elements. Make sure your curtains are easy to remove for washing or cleaning. Choose a fabric that can be machine-washed or wiped down if necessary.\n8. Personal Touch\nConsider adding personal touches to your curtains. You can sew or attach pockets for storage, such as small shelves for books or personal items. This will help you keep your space organized and functional.\n9. Privacy Panels\nIf you're sharing a berthing space with other sailors, you can add separate privacy panels that can be pulled closed when you need personal space or opened when you want to interact with your neighbors.\n10. Safety and Compliance\nAlways ensure that your custom Navy rack curtains comply with safety regulations and guidelines on your ship. They should not obstruct emergency access, ventilation, or any safety equipment.\nBy customizing your Navy rack with curtains, you not only create a more comfortable and personalized sleeping space but also a more organized and functional one. Your custom curtains can offer privacy, block out light and noise, and serve as storage solutions, helping you make the most of your limited living space aboard a naval vessel. Remember to check with your ship's regulations and guidelines to ensure that your curtain setup is in compliance with safety and operational requirements.\nNavy Rack Sheets: Why You Need Them and How to Choose the Best\nSheets are an integral part of creating a comfortable and hygienic sleeping environment in a Navy rack. Here's why you need them and how to select the best ones:\n- Hygiene and Cleanliness: Navy rack mattresses are often made of vinyl or other easy-to-clean materials. However, to maintain a high level of hygiene, it's essential to use fitted sheets. These sheets can be easily removed, laundered, and replaced, ensuring a clean and fresh sleeping surface.\n- Comfort: Navy rack mattresses are not known for their plushness. Adding quality sheets can significantly improve the comfort of the sleeping surface. Look for sheets made from soft and breathable materials, such as cotton, that will provide a more comfortable night's sleep.\n- Size and Fit: Navy rack mattresses come in standard sizes, and it's crucial to choose sheets that fit snugly. Fitted sheets designed for Navy racks are widely available and ensure a proper fit without excess fabric.\n- Durability: Navy racks endure a fair amount of wear and tear. Choose sheets that are durable and can withstand frequent washing. Look for high thread count sheets for a more robust and long-lasting option.\n- Breathability: Given the limited ventilation in Navy racks, it's important to choose sheets that allow for adequate airflow to prevent moisture buildup and promote a more comfortable sleeping environment.\nNavy Rack - What Size Bed Is It?\nThe term \"Navy rack\" is a colloquial expression commonly used to describe the sleeping accommodations on board a ship in the United States Navy. Navy racks are essentially the equivalent of beds on land, but they are designed to optimize space and efficiency on a ship, where space is at a premium. To understand the size of a Navy rack, one must consider the dimensions and features that make it unique.\nNavy racks come in various sizes, and the specific size of a rack can vary from one class of ship to another. However, there are some standard dimensions and characteristics that are commonly associated with Navy racks. These beds are designed to accommodate the unique needs and constraints of life at sea, providing sailors with a place to rest and sleep during their time on the ship.\nTypically, a Navy rack is designed to be compact and space-efficient. The standard size of a Navy rack is about 77 inches in length, which is slightly shorter than a twin-sized bed commonly found on land. This reduced length allows for more racks to fit within the confined spaces of a ship's berthing compartments. The width of a Navy rack is typically around 27 inches, making it narrower than a standard twin bed. This width is just wide enough to comfortably accommodate one person.\nOne of the defining features of Navy racks is that they are stackable or \"hinged.\" Each rack is hinged at the head and foot, allowing it to be folded up during the day to create more open floor space in the berthing area. This foldable design is essential for maximizing space on a ship, as it enables sailors to move around more freely when they are not sleeping. When it's time to sleep, the rack can be folded down to create a secure and private sleeping space.\nThe height of a Navy rack can vary depending on the ship's class and the specific design of the berthing compartment. In some cases, the upper rack may be positioned relatively close to the ceiling of the compartment, while the lower rack is closer to the deck. This arrangement allows for a tiered sleeping configuration that further optimizes space. The upper rack is accessed using a small ladder or built-in steps.\nAdditionally, Navy racks are equipped with various features to enhance the comfort and privacy of the sailor using them. These features may include a privacy curtain that can be drawn closed, a reading light, and a small storage area for personal items. The privacy curtain is particularly important as it provides a sense of personal space and helps block out ambient light and noise, making it easier for sailors to rest and sleep.\nThe Navy pays careful attention to the design and maintenance of racks to ensure the well-being of its personnel. The mattresses used in Navy racks are specially designed to provide comfort and support. In addition, bedding and linens are provided to sailors, and these are regularly cleaned and replaced to maintain hygiene and comfort.\nWhile Navy racks are space-efficient and functional, they are not known for their luxurious comfort. Sailors often have to adapt to the unique challenges of sleeping at sea, where the ship's motion can lead to unpredictable and uneven sleeping conditions. Despite these challenges, the Navy rack serves its purpose, allowing sailors to get the rest they need while on duty, and it exemplifies the Navy's commitment to maximizing space and resources on its vessels.\nIn conclusion, the size of a Navy rack is typically about 77 inches in length and 27 inches in width, making it slightly shorter and narrower than a standard twin bed. These racks are designed to be space-efficient and are equipped with features to enhance the comfort and privacy of sailors. Their foldable design, combined with other space-saving measures, allows multiple racks to fit within the confined spaces of a ship's berthing compartment. While Navy racks may not offer the same level of comfort as a land-based bed, they are an essential part of life at sea in the United States Navy, providing sailors with a place to rest and sleep during their deployments."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:35e1baf2-7c3e-4204-ab2a-e514e7bef623>","<urn:uuid:b50d545d-479c-425e-b6a7-57edccd4a6a9>"],"error":null}
{"question":"What are the key differences between the 2009 Ultra High Relief Double Eagle and the 1920-S Saint-Gaudens Twenty in terms of their historical significance and rarity?","answer":"The 2009 Ultra High Relief Double Eagle and 1920-S Saint-Gaudens Twenty have distinct historical significance and rarity profiles. The 2009 Ultra High Relief was a special one-year issue struck in 24-karat .9999 fine gold, with the first coin sent to the Smithsonian Museum. It required two strikings at 65 metric tons pressure and quickly sold out upon release. In contrast, the 1920-S, with a mintage of 558,000 pieces, became rare due to the 1933 Gold Recall, with most coins melted. It is now the fifth rarest issue in its series, with only about 40-75 Mint State specimens surviving. The 1920-S is particularly scarce in high grades, with only two coins graded MS66 by PCGS.","context":["2009 Ultra High Relief Double Eagle: Invest in History\nThe U.S. Mint announced the plan for the Ultra High Relief in March 2008 and it immediately raised high interest in the gold investment community. The coin was released for public sale the following year and it quickly sold out at the Mint, as investors acquired every coin they could get their hands on. In the following years, demand for the coin has steadily increased, but the supply stays the same, which has driven a consistent increase in the price, making early buyers very happy in the process. Even with the sharp increase in value, there are very few of these coins available in perfect \"Mint State 70\" condition on the market. All 2009 Ultra High Relief coins are certified in perfect condition and are sealed in a tamper-proof holder to guarantee and preserve the condition.\nThe coin pays homage to famed mint engraver Augustus Saint Gaudens, who created the legendary Liberty Head Double Eagle, including the 1907 High Relief Double Eagle. The first 2009 Ultra High Relief coin that was made was sent immediately for safekeeping at the Smithsonian Museum. The second coin was preserved by the U.S. Mint to add to its heritage collection.\nThe special one-year issue coins are struck in solid 24-karat gold on 27 millimeter blanks that are 4 millimeters thick. The Ultra-High Relief coins contain one full ounce of .9999 fine gold and required two strikings of the coin press at a pressure of 65 metric tons to fully bring up the high-relief detail.\nMint State 70\nHIGH RELIEF GOLD COIN FACTS\nOf the top 100 highest-priced coins ever, 10 involve the 1907 Saint Gaudens High Relief gold coin:\n#3 PCGS PF-69 : sold for $2,990,000 in November of 2005\n#7 PCGS PF-68 : sold for $1,840,000 in January of 2007\n#20 PCGS PF-67 : sold for $1,210,000 in May of 1999\n#40 Proof (un-certfied) : sold for $825,000 in December of 1996\n#50 Proof (un-certfied) : sold for $690,000 in October of 2001\n#55 PF-67 : sold for $660,000 in January of 1997\n#69 PCGS MS-69 : sold for $575,000 in November of 2005\n#75 PCGS MS-69 : sold for $546,250 in January of 2007\n#77 NGC PF-69 : sold for $534,750 in November of 2005\n#89 Proof - EF : sold for $488,750 in March of 2005\nREALIZING SAINT GAUDENS' DREAM\nMany collectors believe that the 24 karat 2009 Ultra High Relief Double Eagle gold coin is the most beautiful coin that the U.S. Mint has ever produced.\nTo create the coin, the original etchings by Saint Gaudens were digitally mapped to make the die for pressing the coins.\nThe front (obverse) of the coin shows Lady Liberty walking forward, with her flowing robe conjuring images of ancient Rome and Greece. She holds a torch and olive branch in her hands.\nThe reverse shows a bald eagle flying across the sunrise. The inscription “In God We Trust” was added to the coin, since it was not present in the 1907.\nSculptor of over 200 works in marble and bronze, Augustus Saint-Gaudens had an international reputation and clientele for his portrait reliefs, decorative projects, and public monuments. His long career in New York, Paris, and Rome began as an apprentice to a cameo maker, and ended with a request from the president of the United States, Theodore Roosevelt, to design gold coins for the nation. In between these landmarks -- humble and exalted -- lay Saint-Gaudens' life as a sculptor of portraits, memorials, and architectural decorations. He was inspired by the golden age of Renaissance bronze statuary, committed to the overall relationships of architecture, design, and sculpture advocated by the Aesthetic Movement, and blessed by a personal genius for painstakingly researched yet astoundingly fluid imagery.","Premium Gem 1920-S Saint-Gaudens Twenty\n1920-S $20 MS66 PCGS Secure. CAC. Coins that appear on the\nnumismatic scene unexpectedly have a special cachet and mystery,\nmuch like buried treasure. Numismatists prize these \"fresh\"\nexamples far above the run-of-the-mill pieces that are offered at\none auction after another in quick succession. Like many coins in\nDr. Duckor's collection, this delightful Premium Gem has no recent\nauction appearances, and present-day collectors will be amazed at\nits technical quality and spectacular eye appeal. This piece was\ndiscovered in an old-time collection around 2006 and sold to Dr.\nDuckor in a private treaty transaction. We have been unable to\ntrace its last public offering, and series enthusiasts will be\nexcited to see it offered for the first time in many years.\nExtraordinary Rarity and Quality\nTied for Finest Certified\nThe present coin is a magnificent Premium Gem, with vivid orange-gold surfaces that display a few highlights of lilac and green. The mint luster is vibrant and frosty, creating extraordinary visual appeal. The design elements are sharply detailed throughout, with just a touch of flatness on the stars below the Capitol. The surfaces are free of significant distractions, but a shiny mint-made depression can be observed between two of the obverse rays, and a small luster graze on the sun could serve as a pedigree marker. This coin is tied with one other MS66 example at PCGS for the title of \"Finest Certified\" (10/11). Altogether, this piece is one of the most attractive and important coins in the remarkable Duckor collection.\nThe 1920-S is the first date in the series to owe its rarity to the great Gold Recall of 1933. The recorded mintage of 558,000 pieces should have ensured future generations of collectors an ample supply of high-grade coins to choose from, but such was not the case. The 1920-S is the fifth rarest issue of the 53-coin series in terms of number of specimens known, and it occupies the third spot in terms of high-grade rarity. The great majority of the coins were stored in Treasury vaults as backing for gold certificates, and few examples escaped the mass meltings of the mid-1930s.\nA small number of coins were undoubtedly released into circulation, as the 1920-S is one of the few double eagles from the era that is seen nearly as often in circulated grades as it is in Mint State. The June 1920 edition of The Numismatist records that 15,000 double eagles were struck in April 1920, the first double eagles struck at any mint since 1916. It would be logical to assume this small emission was in response to a need for coins in the local economy, while the larger deliveries later in the year would have served as currency reserves, but that is uncertain. Unfortunately, The Numismatist does not specify if these coins were from Philadelphia or San Francisco. New research by Roger W. Burdette reveals that no Philadelphia 1920 double eagles were released into circulation until March 26, 1926, so if the April delivery was intended for circulation, it must represent San Francisco coins. The remnants of this small delivery would then represent the circulated examples of the 1920-S we know about today. Further research is needed to confirm the theory.\nSome resourceful numismatists succeeded in acquiring specimens of the 1920-S directly from the Mint at the time of issue. Others relied on inside connections with Mint officials or Treasury Department personnel. Two collectors who followed this practice were Connecticut State Senator William Henry Hall and his friend George Seymour Godard, the Connecticut State Librarian. Godard and Hall often obtained their coins through Dr. Thomas Louis Comparette, the curator of the Mint Cabinet, and many of those pieces were acquired for the collection of the Connecticut State Library, where they can be seen today. It would not surprise us to learn that the present coin, with its unparalleled quality and appearance, was originally obtained through connections at the Mint or the Treasury Department.\nUnlike some dates of the series, only a few specimens of the 1920-S have turned up in European holdings over the years. Appearances have been few and widely scattered, with no significant number of pieces recovered from foreign sources at any time. The supply of high-grade specimens remains small today, with different experts estimating the surviving Mint State population in the range of 40-75 pieces. In higher Mint State grades, the 1920-S is even rarer. PCGS has certified only two coins in MS66, with another four in MS65, one in MS64+, and 11 examples in MS64 (10/11). We have compiled a roster of all specimens graded MS64 and above that we are aware of from a search of auction records over the last 15 years. We can account for only 16 pieces that grade MS64 or finer, and two of them are in institutional collections. Clearly, the chance to acquire a specimen of this quality will only occur rarely, and we doubt that any other example of this rare date could bestow the pride of ownership this specimen will provide some determined collector.\n1920-S Double Eagle Roster, MS64 and Finer Specimens.\n1. MS66 PCGS. The present specimen. A coin from an old-time collection, sold by Todd Imhof of Heritage Auctions to Dr. Steven Duckor in early 2006. Depicted on the PCGS Coin Facts website.\n2. MS66 PCGS. Louis Eliasberg, Sr.; United States Gold Coin Collection (Bowers and Ruddy, 10/1982), lot 1051, not certified at the time, graded Select Brilliant Uncirculated by the cataloger; Dr. Steven Duckor; Phillip H. Morse Collection (Heritage, 11/2005), lot 6641, realized $517,500.\n3. MS65 PCGS. Jeff Browning; Dallas Bank Collection (Sotheby's/Stack's, 10/2001), lot 185, not certified at the time, graded Gem Brilliant Uncirculated by the cataloger; Pittsburgh ANA (Heritage, 8/2004), lot 7782; \"Dr. EJC\" PCGS Registry Set Collection. The Akers and Bowers plate coin.\n4. MS65 PCGS. Milwaukee ANA (Heritage, 8/2007), lot 2074, realized $264,500.\n5. MS65 PCGS. FUN Signature (Heritage, 1/2011), lot 5314, realized $212,750.\n6. MS64 PCGS. Reed Hawn Collection (Stack's, 10/1993), lot 1118, not certified at the time, graded Choice Brilliant Uncirculated by the cataloger; Long Beach Signature (Heritage, 6/2000), lot 7702; Philadelphia ANA (Heritage, 8/2000), lot 7599; Benson Collection, Part II (Goldberg, 2/2002), lot 2271; Dallas Signature (Heritage, 10/2008), lot 2486; Los Angeles ANA (Heritage, 7/2009), lot 1128; Central States Signature (Heritage, 4/2010), lot 2352, realized $126,500; Central States Signature (Heritage, 4/2011), lot 5518; Chicago Signature (Heritage, 8/2011), lot 7700.\n7. MS64 PCGS. Dr. Thaine B. Price Collection (David Akers, 5/1998), lot 100, not certified at the time, graded Very Choice Uncirculated by the cataloger; Dr. Richard Ariagno Collection (Goldberg, 5/1999), lot 895; FUN Signature (Heritage, 1/2003), lot 9326; San Francisco ANA (Heritage, 7/2005), lot 10428; Long Beach Signature (Heritage, 9/2009), lot 1950, realized $132,250.\n8. MS64 PCGS. Phillip H. Morse Collection (Heritage, 11/2005), lot 6642; Pre-Long Beach Auction (Goldberg, 9/2007), lot 3523; Pre-Long Beach Auction (Goldberg, 1/2010), lot 2628.\n9. MS64 PCGS. FUN Signature (Heritage, 1/2007), lot 3287; Jay Brahin; FUN Signature (Heritage, 1/2010), lot 2314, realized $161,000.\n10. MS64 PCGS. Rarities Sale (Bowers and Merena, 10/2004), lot 940.\n11. Very Choice Uncirculated 64. Auction '90 (David Akers, 8/1990), lot 1988.\n12. MS64 PCGS. FUN Signature (Heritage, 1/2010), lot 2313, realized $133,975.\n13. MS64+ PCGS Secure. CAC. Simpson Collection; ANA Signature (Heritage, 8/2010), lot 3622, realized $161,000.\n14. MS64 Secure PCGS. Chicago Signature (Heritage, 8/2011), lot 7699.\n15. MS64 Uncertified. The example in the National Numismatic Collection at the Smithsonian Institution.\n16. A specimen in the ANS Collection, reported to be a \"superb gem\" by Jeff Garrett and Ron Guth.\nDavid Akers Comments:\nThis is the third most valuable collectible regular issue of the series in all grades, exceeded by only the 1927-D and 1921. Not surprisingly then, it is one of the most important coins by which the quality of any set of Saint-Gaudens double eagles is ultimately judged. The 1920-S is not the third rarest issue in the series in terms of total population rarity, however, since there are many more examples of this issue in existence than there are of the 1930-S and 1932 although most of them are EF, AU and the lowest uncirculated grades below MS63. While the half million plus mintage of the 1920-S is less than that of most of the other rarities of the 1920's, like the others this issue was not really intended for general circulation. However, the number of circulated examples around shows that at least some small quantities were released into public hands, and other modest quantities were also sent to Europe between 1926 and 1933, probably in mixed date, mostly circulated bags (same for the 1921). I doubt if any single date bags of uncirculated 1920-S double eagles were ever sent overseas since no large quantity or even small groups of uncirculated 1920-S Saints were ever returned to the U.S., just the occasional circulated or minimal uncirculated specimen. Paul Wittlin, the European buyer for James Kelly and later Paramount, searched more than 20 years for rare and scarce date U.S. gold coins in Paris and Swiss banks and acquired some amazing things yet managed to find only the occasional single AU or uncirculated 1920-S double eagle, never any quantities of uncirculated pieces and not a single one that was really nice, like MS63 or better. So it is my opinion that the known population of the 1920-S has not materially changed in decades and nearly all of the specimens in the hands of collectors and dealers today, and certainly all of the better grade ones, were most likely the ones known in the 1940's. That is probably why the 1920-S was considered only rare at that time but not really one of the major rarities of the day. Since then the 1920-S has risen to the top of the rarity pyramid of Saint-Gaudens double eagles, not because it has been found to be more rare than originally thought but rather because all of the others ahead of it at that time (except the 1921) were subsequently found in sufficient quantities to prove them less rare.\nIt is in the grades of choice uncirculated and above that the 1920-S really makes its case as a major rarity today. Relatively few grade as high as MS63 and MS64, perhaps only 25-35 pieces between the two grades combined and, in gem MS65 condition, no more than 6-8 are known, if that many when strict grading is applied, including the Jeff Browning (Dallas Bank Collection) specimen as perhaps the finest of that grade group. Only two MS66 examples have been graded and both have been owned by Dr. Steven Duckor. He and I attended the Eliasberg sale together in 1982 when he bought the first of the two coins. The Eliasberg coin was very conservatively graded MS63 but it was obviously a gem with great color, luster and eye appeal. It subsequently was graded MS66 by PCGS. Almost 20 years later, Dr. Duckor decided to sell the coin because he received an offer for it that was simply too good to turn down. He didn't expect to ever own another 1920-S again, but fortunately, some years later, he had the opportunity to purchase the 1920-S now offered here with his collection, also graded MS66 by PCGS. In my opinion, his current MS66 coin is clearly finer than the Eliasberg specimen he previously owned even though they are both in holders with the same technical grade. I believe that this coin is actually deserving of a (+) designation at the very least and that it is every bit as superb as any of the other MS66+ coins in his set, if not finer.\nFrom The Dr. and Mrs. Steven L. Duckor Collection.\nSeller is donating a portion of their proceeds, and Heritage is donating the same portion of the Buyer's Premium, from the sale of this lot to the National Numismatic Collection at the Smithsonian Institution. See page 3 for details.(Registry values: N1) (NGC ID# 26FZ, PCGS# 9171)\nView all of [The Dr. and Mrs. Steven L. Duckor Collection ]\nService and Handling Description: Coins & Currency (view shipping information)\nSales Tax information | PCGS Guarantee of Grade and Authenticity | Terms and Conditions\nBidding Guidelines and Bid Increments\nGlossary of Terms\nBuyer's Premium per Lot:\n15% of the successful bid (minimum $14) per lot.\nRevised Edition by James L. Halperin, Mark R. Borckardt, Mark Van Winkle, Jon Amato, and Gregory J. Rohan, with special contributor David W. Akers\nThe Coinage of Augustus Saint-Gaudens is an issue-by-issue examination of these two artistically inspired series of gold coins.\nEach date and mintmark is reviewed with up-to-date information, much of which has never been previously published. The book is based on\ntwo extraordinary collections: The Phillip H. Morse collection and the Dr. and Mrs. Steven L. Duckor collection.\nOrder Now! Just $95\nFloor auctionsOpen for bidding: (View All)\nComics & Comic Art\nEnds on 08/30/2015\nFine & Rare Wine\nEnds on 09/04/2015\nEnds on 09/12/2015\nEnds on 09/16/2015\nEnds on 09/17/2015\nWorld & Ancient Coins\nEnds on 09/17/2015\nEnds on 09/17/2015\nEnds on 09/19/2015\nEnds on 09/26/2015\nJewelry & Timepieces\nEnds on 09/28/2015\nEnds on 09/29/2015\nOpens about 09/03/2015\nNature & Science\nOpens about 09/08/2015\nLuxury Real Estate\nOpens about 08/07/2015\nOpens about 09/08/2015\nOpens about 09/18/2015\nNature & Science\nOpens about 09/08/2015\nOpens about 10/05/2015\nArms & Armor\nOpens about 10/06/2015\nOpens about 09/25/2015\nsold in the last year\n- Past Auction Values (prices, photos, full descriptions, etc.)\n- Bid online\n- Free Collector newsletter\n- Want List with instant e-mail notifications\n- Reduced auction commissions when you resell your"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:a1f09c95-72c1-4ec6-af72-fd204fe24afa>","<urn:uuid:b9c8b2bb-5020-4a00-99bc-8edc3eaa28d0>"],"error":null}
{"question":"As someone planning a luxury residential investment, could you compare the amenities offered at Pacific Gate with the emerging EV charging infrastructure considerations for high-end properties?","answer":"Pacific Gate offers luxury amenities including a swimming pool terrace, private lounge, fitness suite, meeting rooms, movie screening facilities, a private dog-walking area, and unique access to a 45-foot yacht. The building also features a double-height lobby with cherry wood panels and 215 upscale residences. Regarding EV infrastructure, modern luxury properties need to consider charging capabilities, as consumer interest in EVs is high with 50% of consumers interested in ownership. Properties must plan for charging infrastructure including network availability, charging point operators, and various billing plans based on customer types (individual, fleet, business). Utilities can provide charging ecosystems and special EV tariffs to enhance the resident experience.","context":["Global architecture firm Kohn Pedersen Fox has completed a high-rise condo building on the San Diego waterfront, with a curved glass form that stands out from the city’s boxy towers.\nPacific Gate by Bosa is 41 stories tall at the junction of Broadway and Pacific.\nWith plans to create a “gateway” into the city, developer Bosa aimed to capitalise on the revitalisation of the surrounding waterfront and continuing work in the area.\nThe company built a tower filled with luxury condos, along with public amenities like retail space, restaurants and a plaza at ground level.\nThe city initially wanted a mixed-use development for the prime site, but agreed on a mostly residential scheme on the grounds that an architectural “landmark” would be created.\nTherefore, Bosa enlisted KPF to design a building that would contrast with the others in the area – many of which comprise simple rectangular volumes.\nThe architecture firm chose the nearby ocean as a starting point for the tower’s form.\n“The general character of the building, we wanted to belong to San Diego, and still be a modern building,” said senior managing principal Richard Nemeth during a press tour. “The area we kept coming back to was the water. The waves, the dunes, rolling forms.”\n“The tower itself, you can see in plan, is two wave shapes nested together,” he continued. “In between, at the intersections of those nests, are the balconies that look out.”\nGlass panels are arranged to create curved facades that run the full height of the tower, which is angled at its peak.\n“The top of the building has a gentle slope to it, you can see from a distance,” said Nemeth. “That was to add something special to the skyline.”\nIt contains 215 two- and three-bedroom residences, ranging from approximately 1,276 to 2,450 square feet and priced at $1.1 million and beyond.\nThe retail area on the ground floor totals 15,000 square feet. Plans for the space include an Asian-American restaurant called Animae, run by a team that operates several restaurants in the city. A sculpture by artist Jaume Plensa also sits at the tower’s base.\nResidents and their visitors arrive at a forecourt around the back, which is partially shaded and boasts a large water feature. The canopy leads to a double-height lobby, decorated with cherry wood panels and circular light fixtures.\nInterior details like tiling, carpets are intended to pick up on characteristics of San Diego, such as the water and sunsets.\nPrivate amenities at Pacific Gate include a swimming pool terrace, with seating, grills and cabanas, a private lounge with kitchen facilities, and a fitness suite.\nRooms for meetings, conferences, movie screenings are also available to residents, as is a private dog-walking area, and access to a 45-foot yacht for gatherings or events.\nThe firm is currently working on several high-rise projects across the US. In New York City, its One Vanderbilt skyscraper is set to become the metropolitan’s second tallest and its Brooklyn Point tower is poised to take the same title for the borough.\nAnother record-holder is also underway in Miami, KPF’s One Bayfront Plaza is due to be the city’s joint-tallest building.","Utilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nUtilities in the age of electric vehicles\nThe impact of vehicles on the environment has driven regulatory mandates to adopt a more sustainable way of commuting. As a result, electric vehicles (EVs), and the necessary infrastructure to operate them, has changed the automobile and utility industries over the past decade.\nElectric vehicles are powered by a charged battery pack and can be separated into two categories:\n- Battery Electric Vehicles (BEVs): These EVs are purely electric with lithium ion batteries suitable for short to medium distances.\n- Plug-In Hybrid Electric Vehicles (PHEVs): Electric vehicles with an internal combustion engine (ICE) with support from a small electric motor.\nWhy Electric Vehicles?\nThe 2015 Paris Agreement has challenged countries to reduce their carbon emissions to “net zero” over the coming years. This international treaty has prompted governments around the world to phase out gas and diesel powered vehicles, shifting instead to EVs:\nSales of electric vehicles have grown steadily over the last decade. The following chart from the International Energy Agency shows China leading market share at 47%. Twenty other countries have reached a market share of above 1%: emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs.\nAccording to a study by IRENA (International Renewable Energy Agency) on EVs:\n- Electric passenger cars will reach 200 million by 2030\n- Electric two-wheeled and three-wheeled vehicles could outnumber four-wheeled vehicles, with as many as 900 million on roads by 2030\n- Electric buses and light-duty vehicles could surpass 10 million by 2030\nFactors Contributing to EV Adoption\n1. Consumer Interest:\nEco-friendly consumers who want to decrease their carbon footprint prefer to buy EVs. Transportation around the globe is one of the biggest contributors of carbon emissions. As a segment, the automobile industry can tout sustainability and the environmental benefits of emerging technologies to entice consumers to buy EVs. A 2019 international electric vehicle consumer survey (of 7,600 consumers in seven regions) shows consumer interest in electric vehicles is high. 50% of consumers say they’re interested in owning an EV and 28% say they’ll purchase one as their next vehicle.\nConsumer benefits to owning electric vehicles:\n- Reduced operating costs, lower charging prices and simpler maintenance\n- Quieter driving experience\n- Exemption in Clean Air Zones – areas that charge fees to vehicles that pollute the environment\n- Government subsidies that make EVs cheaper than ICE vehicles\n- Preferential parking permits in dense urban areas\nEV technology has vastly improved. Range limitations and charging times have been addressed, alleviating concerns and increasing purchase momentum. Consider these three, top selling EV models in the world in 2020:\n3. NetZero Target:\nIn support of the 2015 Paris Agreement, utility and automobile companies are working to achieve net-zero emissions. To do this, they’re offering customers low-carbon products such as renewable electricity and electric vehicles. By taking advantage of these offerings, individual consumers can reduce their carbon footprint. Businesses can reduce their overall cost of fleet ownership, and organisations can reduce fuel costs, reap tax benefits and take advantage of government incentives.\nChallenges for Utilities:\nEVs help combat climate change. However, barriers to adoption exist:\nCharging Pricing: An increase in the number of electric vehicles can lead to disorganised charging. This makes peak shaving difficult, creates incremental costs for generators, increases transmission and distribution pressures and reduces grid reliability and security. It also degrades power quality and increases the harmonics of the grid. Ultimately, an unreasonable pricing structure can lead to its failure. A dynamic pricing strategy can help utilities overcome the challenges of EVs and can reduce the burden of power on a grid.\nComplex Billing: EVs also present billing challenges for utilities:\n- Number of Stakeholders: Charging hosts, charging point operators, eMobility service providers, roaming network providers, etc. are all involved in the billing process. These stakeholders have to manage multiple plans – pre-paid, postpaid, ad hoc, group plans, etc.\n- Customer Type and Charging Location: Plans offered will vary based on customer type such as individual, fleet, business, public and private. They’ll also vary based on location, including home, office, fleet charging center, parking lot, multi-tenant unit, municipal location and more.\n- Price Per Charge: The customer can be charged based on charge point, price per kWh or by minute/ hour (flat fee). The charging session may include ancillary fees such as a connection fee or a waiting fee for staying connected after reaching a full charge.\nCharging Infrastructure: The mechanics of charging pose challenges to utility companies:\n- Network: Availability remains limited\n- Technology: Fast-charge still in its initial stage and widely unavailable in the network\n- Customer Experience: Unpredictable charging experience negatively affects customer opinion\nService and Maintenance: Electric vehicles require specialised mechanics who are still difficult to find. According to a study done by UK’s Institute of the Motor Industry (IMI), 97% of today’s mechanics aren’t qualified to work on electric vehicles. Of the 3% of mechanics who do qualify, many work directly for EV dealerships, limiting service options for general EV buyers.\nHigher Upfront Investment: Higher manufacturing costs vs. the cost to make a combustion engine vehicle make EVs more expensive to buy. This sticker shock feeds consumer doubt about the long-term economic benefits of an electric vehicle. Government subsidisations help alleviate that doubt, but total consumer buy-in will take time.\nThe Utility Opportunity\nAs more people switch to electric cars, the impact of EV charging loads on generation, transmission and distribution networks translates into more energy and more revenue opportunities for utility companies.\n- Charging Infrastructure: Utilities can play a vital role in modulating charging rates and shifting charging times to provide grid services that support supply and demand. Consider these energy giants already investing in charging infrastructure:\n- Shell recently announced the rollout of 500,000 electric charging stations over the next four years.1\n- Ecotricity a “Big Six” UK energy supplier, partners with Moto, RoadChef and Welcome Break to offer 45-minute fast-charge stations. They call the network “The Electric Highway.”2\n- In the UK, companies like Centrica are building out their EV charging capabilities by acquiring smaller independents. Centrica invested in Driivz, a software company that manages EV fleets and charging networks, to create Centrica Electric Vehicle Services (CEVS) 3\n- New EV Tariffs: Consumer tariff structures (e.g. time-of-use tariffs) reward consumers who slow-charge during off-peak hours. These tariffs, which reduce consumer bills and prevent overloads on the grid, help influence EV drivers to shift their charging behavior. By partnering with EV manufacturers, utility companies can create custom electricity tariffs that can be bundled into the purchase of an electric vehicles. Energy suppliers in the US, UK and other European countries have already begun offering EV energy tariffs.\n- Improved Customer Experience: Careful planning, phased execution and synergy with non-utility businesses can help electric utilities facilitate a smooth transition to EV adoption. With the right customer relationship management (CRM) platform in place, utilities can offer consumers “charging ecosystems” – chargers, charging plans, etc. - for their vehicle. This positive consumer experience, combined with the financial upside of aligning with non-utility companies, translates into increased revenue for the utility company.\n- Vehicle-to-Grid (V2G): While EV tariffs can prevent overloads by shifting charging behavior, they also present challenges. If too many EV drivers charge during off-peak times, it can spike load levels and lead to grid congestion. To counter this, vehicle-to-grid (V2G) enables energy to be pushed back to the power grid from the battery of an electric vehicle. This helps balance the variations in energy production and consumption. Furthermore, V2G can support the integration of renewable energy resources into the grid.\n- AI Driven EV Marketing: With electric vehicle purchases on the rise, utilities must position themselves at the forefront of energy innovation to ensure brand credibility. AI-driven marketing helps identify crucial digital touchpoints for targeted messaging.\n- Data Advantages: Utilities can use data analytics and data science tools to develop services, applications and hyper-personalised product offerings. They can also leverage the data to expand into non-core markets. This allows for:\n- Joint offerings with automotive companies\n- After sales services in conjunction with car dealerships\n- Installation of charging stations at locations where customers park electric vehicles for more than an hour\nGlobally the uptake of electric vehicles is leading to the transportation and electricity sectors becoming increasingly connected.\nEven though barriers to EV adoption exist they are phasing out due to technological advancements. Innovation is the key in identifying opportunities to minimise costs and reducing pain points.\nFor utilities the biggest challenges is to ensure grid reliability and resilience. Providing a successful infrastructure for EV adoption will require coordination among various parties - Vehicle & charging manufactures, Electricity service providers, Distribution network operators, and Regulatory authorities\nIn the long run - Utilities that invest in electric vehicle infrastructure and technology will be well-equipped to offer solutions that benefit future customers.\n2Ecotricity and Nissan install UK electric-car-charging network | Guardian sustainable business | The Guardian\nEXL Utilities Academy"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:822046fb-ffff-45e5-be6d-89933558c615>","<urn:uuid:dc7f0a0b-46cd-416c-b618-7a830371a8fd>"],"error":null}
{"question":"How do micro-annuli affect well integrity, and what changes occur in soil's physical properties when contaminated with oil?","answer":"Micro-annuli significantly impact well integrity by creating micro-separation between pipe and cement or cement and formation, leading to production reduction and expensive remedial costs. In many cases, wells must be shut off despite having years of production ahead. Regarding soil properties, oil contamination causes several changes: it decreases the soil's permeability, optimum water content, and Atterberg limits. Additionally, it leads to an increase in compression index and maximum dry density as oil content increases. These changes in soil properties are particularly important for coastal engineering and environmental remediation activities.","context":["Micro annuli in cement are a constant problem in oilfield well cementing. In this article, I will look closer at the most common reasons and discuss how to deal with them.\nIt is one numerous well integrity problem that often results in production reduction and expensive remedial costs.\nPhoto by Oliver Paaske\nWe all know the consequences of a well not properly sealed off. As we speak, there are thousands of leaking wells on the planet, and they are a challenge in many aspects.\nA significant number of these wells are shut off despite having years of production ahead of them.\nIn other words, instead of losing money, the owners of these wells would see a significant payback of their investment if they had a way to stop the leakage (other than shutting off the well completely).\nWhat are micro-annuli?\nMicro annuli are micro-separation between pipe and cement caused by a change in temperature or pressure during the cementing setting process, or after the cement is set.\nMicro-annulus between cement and formation can also occur. Although less common, it can result in a lack of bonding to the casing/formation and a possible communication channel.\nHow do micro-annuli occur?\nIt is during the setting phase of the cement job one needs to pay extra attention to the behavior of the cement. The slurry is constantly exposed to forces and obstructions of different character while being in the wet state. It is under these conditions that micro-annuli are most likely to occur.\nThese are the most common causes:\n1. Cracks in the cement\nCracking under stress. The forces that occur during setting can not only lead to de-bonding, it may also create micro fractures in the cement. Shrinkage of cement over time can do the same.\n2. Inadequate cleaning\nFailure to remove layers of mud or other materials on the wall of formation or the annulus, resulting in weak bonding.\n3. Expansion and contraction\nDynamics related to this can lead to debonding from the formation. It could be just gradual pressure reduction under construction of the well, following a decrease in reservoir pressure. It could also be a sudden change when displacing the well from mud to water or stresses seen during a frac job.\n4. The hydrostatic head\nWhen you set cement in a zone with less hydrostatic head on the formation, high gas pressure from the bottom may cause the gas to migrate during the setting process. The gas will find its way through the mud cake, which in turn will make a fluid column in the cement.\nYou may also like our blog article \"Materials for plug and abandonment of oil and gas wells\" and our case \"Sustained casing pressure\".\nDealing with micro-annuli\nThere are a few things you can do to prevent micro-annuli. But even the best preparations would not be enough to avoid the problem entirely. You must have a backup plan as well.\n- Good centralization of casing\n- Clearance all around the pipe to place the cement properly\n- Good hole-cleaning before cementing\n- Chemicals to break the mud cakes and to separate cement from mud – spacers\n- Correctly designed and executed displacement rates to replace the mud with cement efficiently.\nOnce the damage is done - what do you do?\n- Use small-particle cement\n- Try to squeeze ultra-fine cement in\n- Particles are very often too big to penetrate the annuli effectively\nIt happens all the time: On a given job a certain percentage of the gaps will be filled and sealed, but there will be quite a few that are not. Then you have an ongoing process in which you try to get to the problem areas and seal them off.\nIn many cases, it is simply impossible due to the particle size of the cement.\n- Use low-viscosity resin material\n- Particle free - you can get it in everywhere you can pump water. A more viable choice for sealing off micro-annuli\n- Less prone to bridging off. It' s not going to block the entrance or access path.\nReading tip: Resin curing process.\nMicro-annuli will be an ever-present issue in the cementing process. However, proper planning that involves process and material selections will minimize their occurrences. Proper contingencies should also be in place to deal with problems areas immediately if a micro-annuli is detected.\nDid you enjoy this article? Please add your comments below.\nYou may also download this free guide:","Every day, petrochemical activities, oil spills, and pipeline or reservoir leakage contaminate the ground. In addition to environmental concerns, such as groundwater pollution, the alteration of geotechnical properties of the contaminated soil is also cause for worry. There are many ways which could enhance the leakage of gas oil including; corroded storage tanks, processing plants and petroleum transportation. Contamination has been proven to alter the geotechnical properties of soil, and researchers have extensively studied the properties of contaminated granular soils (sand) and fine-grained soils (clay and silt).\nThis paper summarizes the results relating to the effects of crude oil and gas oil contamination on the geotechnical properties of soils such as Atterberg limits, compressibility, hydraulic conductivity, shear strength, bearing capacity and the cohesion of the soils. In addition, this investigation was undertaken to evaluate the effect of temperature on the strength, permeability, and compressibility of oil-contaminated sand. The samples were artificially contaminated by mixing the soils with crude oil and gas oil in the amount of 2%, 4%, 8%, 12%, and 16% by dry weight.\nThe results indicated a decrease in strength, permeability, optimum water content and Atterberg limits and an increase in compression index and maximum dry density occur as the oil content increases. Knowledge of these effects of oil contamination is important in coastal engineering and environmental remediation activities of the studied coastal plain.\nKeywords: Oil-contaminated soil, Geotechnical properties, Sand and clay, Granular soil, Fine-grained soils\nLeakage of crude oil and its derivatives into the surrounding environment can be proceeded by many means for example; damaged pipeline, petroleum transportation facilities, tanker accidents, oil drilling processes, corroded tanks and natural seepage. The leakage causes the contamination of soil and changes its physical, chemical and geotechnical properties. In connection with the cleanup works and for any possible applications, knowledge of the geotechnical properties and behavior of contaminated soils is required.\nThis information is also required when oil leakage from storage tanks and processing plants cause oil pollution in the surrounding soils. In this case, it is necessary to determine the effects of oil contamination on existing structures. The soil-bearing capacity, foundation settlement, shear resistance, compressibility, and plasticity are the factors that must be taken into consideration. The extent of the contamination depends on physical specification of soil (infiltration and retention properties) and the volatilization and viscosity of the contaminants (Fine et al. 1997).\nThe subject was the main interest of many researchers in recent years. For example, Meegoda and Ratnaweera (1994) concluded that the type, the amount and the viscosity of chemicals in pore fluids affect the compressibility of contaminated soils. Evgin and Das (1992) and Evgin et al. (1989) carried out triaxial tests on clean and oil-contaminated quartz sand. They found that full saturation with oil caused a significant reduction in the friction angle of both loose and dense sands and a drastic increase of volumetric strains.\nThey also showed by finite-element analysis that settlement of footing increased due to oil contamination. Al-sanad et al. (1995) reported a small reduction in permeability and an increase in compressibility due to oil-contaminated in Kuwaiti sand. Al-sanad and Ismael (1997) determined an increase in the strength and stiffness due to aging of contamination of sandy soil with oil. Temperature effects on engineering properties of contaminated sand studied by Aiban (1998). The result showed that the compressibility and permanent deformation of an oil-contaminated sand increase as the temperature increases above room temperature. Puri (2000) evaluated the geotechnical aspects of oil-contaminated soils through laboratory testing on sand samples.\nThe test results indicated that the compaction properties are influenced by oil contamination. The angle of internal friction of the sand based on the total stress condition was found to decrease with the presence of oil in the pores. Bearing capacity of unsaturated oil-contaminated sand examined by Shin and Das (2001). Results showed the ultimate bearing capacity of a surface strip foundation drastically reduced by oil contamination. Ratnaweera and Meegoda (2006) performed a series of unconfined compression tests on fine-grained soils contaminated with varying amounts of chemicals.\nGlycerol, propanol and acetone were used as contaminants. The results showed a decrease in shear strength and stress–strain behavior of the soils. Khamehchiyan et al. (2007) carried out a laboratory testing program to determine the influence of crude oil contamination on the geotechnical properties of clayey and sandy soils. Results indicated a decrease in strength, permeability, maximum dry density, optimum water content and Atterberg limits with increase in the crude oil content.\nOlchawa and Kumor (2007) investigated the effect of diesel oil on the compressibility of organic soils. The results showed that compressibility of the soils increased with increase in diesel oil content. Olgun and Yildiz (2010) examined the effect of organic fluids on the geotechnical behavior of a highly plastic clayey soil. This research revealed that liquid limit and consolidation parameters generally decreased while shear strength increased with increase of organic fluid/water ratio and decrease of dielectric constant of the pore fluid. Jia et al. (2011) carried out a laboratory and in- situ testing-program to determine the effect of crude oil contamination on the geotechnical properties of soils.\nThe results showed the clay fraction (< 0.005 mm) is higher in the heavily polluted samples while the Atterberg limits increased with increasing of oil contamination. In-situ penetration tests showed a decrease in strength with depth for the heavily polluted samples. Contamination effect on the geotechnical properties of an over-consolidated clay contaminated with motor oil examined by Nazir (2011).\nThe result showed a significant decrease in both liquid and plastic limit with the increase of the duration of oil contamination up to approximately 3 months. In addition, a significant reduction in the unconfined compressive strength and an increase in the compression and swell index with increasing duration of contamination up to 6 months observed. Khosravi et al. (2013)\ninvestigated the effect of gas oil contamination on the geotechnical properties of kaolinite. Results indicated an increase in the cohesion and a decrease in both the internal friction angle and compressibility of kaolinitic soils with increase of gas oil percent. This paper carried out a consideration on effects of oil-contamination on geotechnical parameters.\nScope of the problem\nSeveral oil-derivatives production sites, storage facilities, pipeline basements, launcher and receiver instruments, and sewage facilities will be constructed over contaminated soils in the vicinity of the existing structures. Moreover, engineers are willing to beneficially use the contaminated soil in civil engineering projects. Therefore, the results of this research can be used in the first phase of studies for the development program.\nSoil samples were taken from 30 to 50 centimeters below the ground surface to prevent upper organic soil layers from entering the sample soils. Particle size distribution of the studied soils is according to ASTM D422 (1999). Specific gravity determined based on ASTM D854 (1999). The standard Proctor compaction tests carried out on the soils samples based on ASTM D698 (1999).\nFor direct shear and unconfined compression tests, standard Proctor compaction test carried out to find density and optimum moisture content of the samples at different percentages of contamination.\nThe liquid limit (LL) and plastic limit (PL) experiments were conducted with the ASTM D4318 and ASTM D4318 standards, respectively, and plastic index was calculated as PI = LL − PL.\nThe measurement of the Specific Surface Area (SSA) of soils may be useful for ranking soils for their ability to sorb organic compounds such as pesticides and pollutants. The SSA of a soil sample is the total surface area contained in a unit mass of soil. Soils with high SSA have high water holding capacities, more absorption of contaminants, and greater swell potentials.\nAfter particle size classification, each sample was divided into five parts and then they were dried by oven at 105 °C. Then the samples were mixed with crude oil in the amount of 0, 4, 8, 12 and 16% by weight of the dry soil samples. The mixed samples were put into closed containers for 1 month for aging and equilibrium allowing possible reactions between soil and crude oil.\nResults and Discussion\nThe results of the Atterberg limits tests are shown in Figure 1. Atterberg limits are extensively used for identification, description and classification of cohesive soils and as a basis for preliminary assessment of their mechanical properties. The limits consist of liquid limit (LL), plastic limit (PL) and shrinkage limit (SL). Liquid and plastic limits control the consistency of the soils as wetting conditions change. The results indicated the liquid limit (LL) and the plastic limit (PL) increased in the clayey (CL) and silty (ML) soil samples with increasing the gas oil percent.\nThe increase can be explained by the theory of the diffuse double-layer. Water molecules are polar. As a result, a water molecule has a positive charge at one side and negative charge at the other side. It is known as a dipole. Dipole water is attracted both by the negatively charged surface of the clay particles and by the cations in the double layer. The other mechanism by which water attracted to clay particles is hydrogen bonding, where hydrogen atoms in the water molecules are share with oxygen atoms on the surface of the clay.\nAll of the water held to clay particles by force of the attraction is known as double-layer water. The innermost layer of the double-layer water, which is held very strongly by the clay, is known as adsorbed water. This orientation of water around the clay particles gives fine-grained soils their plastic properties. The water in the pore space that is not absorbed by the clay particles and moves easily in the soils called free water. The free water determines the liquid behavior of the soil (Das 1994). Unlike the water molecule, the gas oil molecule is not dipole.\nTherefore, as the gas oil is mixed with soil, it covers the soil particles and does not allow water molecules to develop the diffuse double-layer, more water needed for the soil to obtain plastic properties. This might be the reason for the increase in plastic limit. However, if the oil orients the soil particles, most of the water added to the soil during the test will join the free water, so liquid limit show a slightly increase with the increasing gas oil content (Kermani and Ebadi 2012).\nOn the other hand, because of increase in the cohesion of the CL and ML samples after contamination with gas oil and development of flocculation fabric in the soils, more water needed to flow the soil due to its own weight (i.e., shear strength equals zero). It means an increase in liquid limit. With increasing of liquid and plastic limit, plasticity index (PI) of the soil specimens decrease.\nFigure 1. Influence of oil content on the Atterberg limits and plasticity index.\nThe results of compaction tests are plotted in Figure 2 in the form of dry density versus water content curves. Standard Proctor compaction tests were carried out on the three soil types. The compaction curves for contaminated soils generally moved to the left side of the uncontaminated soils curve as oil content increased. The variation of the density in the soils shows a significant drop when 4 % oil was added to the soils and for the heavily contaminated samples, the density remains almost constant.\nThe reduction of dry density in sandy soil is low, since the void spaces are larger and gas oil can move through the soil grains with the same rate as water and it has similar lubricating effect. The results are in agreement with those reported by Shah et al. (2003) and Khamechiyan et al. (2007), but inconsistent with the findings of Al-sanad et al. (1995) and Meegoda et al. (1998). The relationship between the moisture content and gas oil percent revealed a drawdown trend of optimum moisture content with increasing of gas oil content in all of soil types.\nIt implies that the water content needed to achieve maximum density has decreased when gas oil content increased. It is probably attributed to lubricating effect of gas oil that alters the soils to a state of looser material than uncontaminated soils (Rahman et al. 2010). It should be noted that optimum moisture content of uncontaminated sandy soil is relatively high which can be caused by the presence of montmorillonite in the soil and its water absorption\nThe following conclusions can be asserted from the study discussed above:\n- The presence of an oil-contaminated sand layer under the footing resulted in a significant decrease in the bearing capacity and an increase in the settlement of footings.\n- The bearing capacity factor (Nγ ) decreases significantly with the increase of the percentage of oil contaminating the sand.\n- Atterberg limits decrease with increasing oil contamination in CL soil. The observed behavior is due to the nature of water in the clay minerals’ structure and performance of existing non-polar and viscous fluids in soil.\n- Increasing of crude oil content in soils causes a reduction of optimum water content and increase in maximum dry density. The reduction in optimum water content is more in artificially oil-contaminated soil samples, indicating excess oil in the soil.\n- In general, oil contamination induces a reduction in permeability and strength of all the soil samples. However, effect of oil contamination on shear strength parameters is not uniform and it depends on the soil type but it leads to decreased peak shear strength in all studied samples.\n- The long-term effect (aging) of oil contamination on the selected soil properties and their behavior should be determined and compared with the results of the present tests. Also, the future study should consider the problem of interaction between the functional groups that exist in the soil solids and the contaminated oil.\n- The compressibility and permanent deformation of the oil-contaminated sand increase as the temperature increases above room temperature.\n- The shear strength parameters based on as-molded triaxial tests are not sensitive to the testing temperature when samples are compacted to their maximum dry densities.\n- Increasing oil content and kinematic viscosity have a tendency to decrease the hydraulic conductivity of a soil.\n- The total stress friction angle decreases with increasing oil content. The magnitude of decrease in the friction angle increases with the increase of kinematic viscosity of oil.\nAiban, A. 1998. The effect of temperature on the engineering properties of oil-contaminated sand. J. Environ. Int. Div. 24, 153–161.\nA1-Sanad, H.A., and Ismael, N.F. (1997). “Aging effects on oil-contaminated Kuwaiti sand,” Journal of Geotechnical and Geo-environmental Engineering, ASCE, 123(3), 290-293.\nA1- Sanad, H.A., Eid, W.K., and Ismael, N.F. (1995). “Geotechnical properties of oil-contaminated Kuwaiti sand,” Journal of Geotechnical and Geo-environmental Engineering, ASCE, 121(5), 407-412.\nCook, E.E., Puri, V.K., and Shin, E.C. (1992). “Geotechnical properties of crude oil-contaminated sand,” Proceedings, 2 “d International Conference on Offshore and Polar Engineering, Vol. 1,384-387.\nEvgin, E., Amor, F.B., Altaee, A., Lord, S., and Konuk, I. (1989). “Effect of oil spill on soil properties,” Proceedings, 8 t~ Intemational Conference on Offshore Mechanics and Arctic Engineering, Vol. 1,715-720.\nFan, C.Y., Krishnamurthy, S., and Chen, C.T. (1994). “A critical review of analytical approaches for petroleum contaminated soil,” in Analysis of Soils Contaminated with Petroleum Constituents, ASTM STP 1221, 61-64.\nPuri, V.K., Das, B.M., Cook, E.C., and Shin, E.C. (1994). “Geotechnical properties of crude oil-contaminated sand,” in Analysis of Soils Contaminated with Petroleum Constituents, ASTM STP 1221, 75-88.\nKhamehchiyan, M., Charkhabi, A. H. and Tajik, M. 2007. Effects of crude oil contamination on geotechnical properties of clayey and sandy soils. Eng. Geol. Div. 89, 220–229.\nDas, B. M. 1994. Principles of Geotechnical Engineering, 3rd ed. Boston, MA: PWS Publishing Company, p. 436.\nMeegoda, N. J. and Ratnaweera, P. 1994. Compressibility of contaminated fine grained soils. Geotech. Test. J. Div. 17(1), 101–112.\nPuri, V. K. 2000. Geotechnical aspects of oil-contaminated sands. Soil Sediment Contamin. J. 9(4), 359–374.\nShin, E. C. and Das, B. M. 2001. Bearing capacity of unsaturated oil contaminated sand. Int. J. Offshore Polar Eng. 11(3), 220–227.\nSridharan, A. and Rao, V. G. 1979. Shear strength behavior of saturated clays and the role of the effective stress concept. Geotechnique 29(2), 177–193.\nUr-Rehman, H., Abduljauwad, S. N. and Akram, T. 2007. Geotechnical behavior of oil-contaminated fine-grained soils. E. J. Geotech. Eng. 12 A, 15–23.\nGhaly, A.M., 2001. Strength remediation of oil contaminated Sands. The Seventeenth International Conference on Solid Waste Technology and Management, Philadelphia.\nRatnaweera, P., Meegoda, J.N., 2006. Shear strength and stress–strain behavior of contaminated soils. ASTM Geotechnical Testing Journal 29 (2), 133–140.\nTajik, M., 2004. Assessment of geoenvironmental effect of petroleum pollution on coastal sediments of Bushehr province-Iran. M. Sc. Thesis, Tarbiat Modares University, Tehran–Iran (IN Persian), 97p.\nKermani M, Ebadi T (2012) The effect of oil contamination on the geotechnical properties of fine-grained soils. Soil Sediment Contam 21:655–671.\nKhosravi E, Ghasemzadeh H, Sabour MR, Yazdani H (2013) Geotechnical properties of gas oil-contaminated kaolinite. Eng Geol 166:11–16.\nNazir AK (2011) Effect of motor oil contamination on geotechnical properties of over consolidated clay. Alex Eng J 50:331–335."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:ab768a97-70fd-4635-8f13-43532024ba24>","<urn:uuid:4db853d1-1daf-4cb3-a061-8c579569f157>"],"error":null}