{"question":"¿Podría alguien indicarme hasta qué número se había demostrado que el Último Teorema de Fermat era válido antes del resultado final de Wiles?","answer":"By the late 20th century, the Fermat Last Theorem had been proven true for all exponents n between 3 and 4,000,000.","context":["Welcome Speech by Professor Chen-Ning Yang,\nChairman of Board of Adjudicators\nMr Shaw, Mr Chief Secretary, The honorable 2005 Shaw Prize Laureates, Ladies and Gentlemen:\nOn behalf of the Shaw Prize Council, I welcome all of you to this second annual ceremony. The Shaw Prize was established to encourage and to foster highest intellectual achievements essential for the welfare and progress of mankind. Thanks to the insightful deliberations and the diligent work of the 3 selection committees, the 2004 and 2005 Shaw Prize Laureates have won universal acclaim. The Shaw Prize has thus been referred to as the Nobel Prize of the East. We hope, in some future years, the Nobel Prize will be referred to as the Shaw Prize of the West.\nSpeech by Professor Frank H Shu,\nChairman of Astronomy Selection Committee\nThe process of explaining the motions of the planets in our own Solar System spawned the Copernican Revolution. The later realization that stars are suns in their own right led to speculation that other planetary worlds might orbit distant suns. These speculations gained concrete foundation in 1995 when Mayor and Didier Queloz found a planet to orbit about the sunlike star, 51 Pegasi, with a 4-day period. The short period came as a shock because it meant that this giant planet, with about half of the mass of Jupiter, orbits its host star at a radius that is only 1% of the distance of Jupiter about our own Sun. Existing theories of planet formation held that giant planets should form in a rotating nebular disk surrounding a newly born star only beyond a so-called “frost line” where water vapor freezes and condenses out as solid ice. The exact location of the “frost line” at the time of giant planet formation is uncertain, but it should not have been much smaller than the present location of Jupiter, which is the innermost of the giant planets. The discovery of Mayor and Queloz electrified the field and spurred a huge increase in research activity on extrasolar planets, which has not abated to this day, ten years later.\nTogether with Paul Butler, Marcy quickly confirmed the reality of the 51 Pegasi planet using the method of precise Doppler measurement that he had developed for the purpose of extrasolar planet detection over many years. Seventy of the next 100 extrasolar planets discovered were found by his team. Many of these planets had distinctly non-circular orbits, in contrast with the orbits of known planets in our own Solar System. Several reside in systems where there is more than one giant planet, with multiple planets in a given system often having orbital periods that bear integer relationships to one another. As the observational sensitivity and the understanding of systematic errors steadily improve, planets with masses more like Saturn and Neptune are being found, instead of only the Jupiter-like bodies that characterized the initial discoveries. Significantly, there are no bodies that have masses appreciably in excess of 10 Jupiter masses. In a few cases, the orbital plane lies sufficiently edge-on to the observational line of sight that the giant planet transits in front of the host star, blocking out a small part of the star’s light, which allows an estimate of the radius of the transiting planet. A major surprise occurred in the latest such case where the combination of mass and radius implies that the planet has a dense rock-ice core that is seventy times the mass of the Earth, a much larger value than holds for any of the giant planets in our own solar system.\nAs the baseline for good orbital-period determinations passes the one decade mark, giant planets are being discovered with orbital distances from their host stars more akin to the case of our own Solar System. The stage is now being set for answering one of the most intriguing questions ever posed in science: how typical or atypical is our own Solar System? For moving this question from the dreamy realm of philosophical speculation to the concrete field of empirical fact, Michel Mayor and Geoff Marcy are justly honored tonight with the award of the Shaw Prize in Astronomy for 2005.\nSpeech by Professor Arthur K C Li,\nChairman of Life Science and Medicine Selection Committee\nCells in the human body communicate with each other and their activities are regulated. Sir Michael Berridge's discovery of calcium signaling in the regulation of cellular activity has truly revolutionized the fields of life science and medicine. How various factors increase calcium mobilization and how calcium controls cellular activity has widely expanded the areas of cell and molecular biology. This has led to the development of new treatment of heart disease to the improvement of learning and memory. These discoveries represent one of the most important cell signaling pathways in biology and have changed forever the way we think about prevention and treatment of disease.\nCellular communication occurs through chemical signals such as hormones, neurotransmitters and nitric oxide, which act via specific receptors or receptive molecules and are linked to diverse intracellular and extracellular signaling pathways. The signaling system discovered by Berridge is of fundamental importance in regulating diverse cellular processes such as muscle contraction, cell growth and differentiation, secretion, fertilization, synaptic plasticity and information processing. In the early 1970s, Berridge published a series of papers demonstrating that calcium and cyclic AMP functioned as separate and distinct signaling pathways that interacted with each other. Then, he published a series of papers leading to his discovery that IP3 was the messenger linking surface receptors to the mobilization of internal calcium (Biochem. J. 212: 849-858, 1983). Berridge's discovery that IP3 functioned as a messenger molecule started an avalanche of work on the metabolism of inositol phosphates and on the calcium mobilizing action of IP3.\nBerridge's work on the spatial and temporal aspects of calcium signaling has laid the foundation for our current understanding of how information is encoded and decoded in the nervous system and in the cardiovascular system. His discoveries have impacted on widely diverse areas of biology and medicine such as memory and learning, cell growth, fertilization, myocardial contraction, glandular secretion and the mechanism of action of lithium.\nThe impact of these discoveries on the betterment of mankind has been astounding. The seminal contributions made by Berridge on IP3 and its role in calcium signaling made it possible to unravel numerous signaling pathways in biology and to show the links between such pathways and the central role played by calcium. The work of Berridge has had many practical consequences, not only in establishing a basis for our understanding of how cells are regulated but also in providing novel insights into a variety of human diseases.\nSpeech by Professor Wentsun Wu,\nChairman of Mathematical Sciences Selection Committee\nI feel a great honour in giving the Presentation Lecture for 2005 Laureate Professor Andrew WILES in Mathematical Sciences owing to his outstanding achievements in Number Theory and other domains of mathematics, particularly the monumental complete settlement of 356year-old FERMAT LAST THEOREM.\nThe FERMAT LAST THEOREM or FLT for short, had a long fascinating history. It asks whether there are positive integer solutions in (x,y,z) of the equation\n|xn + yn = zn||(In)|\nfor integer exponent n > 2. The case n = 2 is well-known already in remote antiquity. In fact, let x, y, z be the lengths of the two sides and the hypothenus of a right-angled triangle or a Gou-Gu Form in ancient China's terminology. Then equation (I2) is the famous Pythagorean equation or the Gou-Gu Theorem in China's antiquity. It is also well-known that the equation (I2) has positive integer solutions (x,y,z) = (3,4,5), (5,12,13), etc., see the accompanying figure below:\nIn fact, in the Chinese ancient classic 《Nine Chapters of Arithmetic》 more than 2000 years before it is already described that the whole set of possible positive integer solutions of (I2), with Gou, Gu, Xuan as the lengths of two sides and the hypothenus of a right-angled triangle or a Gou-Gu Form, is given by\n|for integers p, q.|\nBesides (x,y,z) = (3,4,5), (5,12,13) the 《Nine Chapters》 also listed the solutions (7,24,25), (8,15,17), (20,21,29), (48,55,73), (60,91,109), (20,99,101) corresponding to the case (p, q) = (2, 1), (3, 2), (4, 2), (5, 3), (7, 3), (11, 5), (13, 7), (11, 9), respectively in the Eq. (II). Moreover, in the 《Annotations to Nine Chapters》 of scholar LIU Hui of 3c A.D. LIU had given a rigorous proof of (II).\nWhile the case n = 2 is elementary and understandable by school boys, the case n > 2 is entirely different. In year 1637 the French jurist Piere de Fermat, as an amateur in mathematics, announced that for n > 2 the equation (In) had no positive integer solutions at all. Fermat himself had proved the case n = 4 and in 17th century the great mathematician L. Euler had proved FLT for n = 3,5. However, the \"lost\" or actually \"non-existant\" proof of Fermat had puzzled mathematicians since that time. To \"rediscover\" or to \"discover\" a proof of FLT had attracted the greatest mathematical minds of the later years and centuries but resulted always in failure. However, in spite of these failures their efforts had brought out the fruits of giving impetus to some important mathematics disciplines and even naissance to new ones. Moreover, some new ideas introduced, the notion of ideals for example, had henceforth penetrated into the whole domain of mathematics and became henceforth indispensable tools for mathematical researches.\nBy the late 20th century it had been shown that FLT was known to be true for an infinity of exponents n and is true for 3 < n < 4,000,000. However, it is yet infinitely far from the ultimate goal: FLT should be true for all n > 3.\nIt comes now the final astonishing result of A Wiles. Around ninetieth of last century, A.Wiles had already become reknown to mathematics circle owing to his dramatic achievements on many delicate problems in number theory. In the long run A.Wiles announced in year 1993 at the end of a series of lectures in Cambridge that the FLT had been thus proved. This not only completely solves the 356year-old problem but the method involved will influence the development of mathematics as a whole. In particular it had a deep impact to the important subject of algebraic geometry.\nThe announcement causes a thunder-like shock to the whole of mathematical society and a wide range of amateurs. The whole paper appeared in Annals of Mathematics, vol141, (1995), of length about 110 pages.\nIt is for such Olympian accomplishments that we have the honour of declaring that the 2005 Shaw Prize in mathematical sciences is offered to\nProfessor Andrew WILES !!!\n2 September 2005, Hong Kong"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d2e88902-95a8-4808-a293-4b87397d6a4c>"],"error":null}
{"question":"I work in AI development. Could you compare how emotion detection technology and voice control systems handle user authentication and data processing in cloud environments?","answer":"Emotion detection technology and voice control systems handle authentication and data processing differently. For emotion detection, the process involves computer vision to identify facial expressions and supervised learning algorithms trained on labeled datasets, with data processed by human labelers who translate facial expressions into emotions. The system uses layers of analysis including geographic benchmarks for cultural differences. For voice control systems, authentication occurs through subscription keys for REST APIs or voice SDKs, with data processing happening via cloud services like Azure's LUIS. LUIS transforms voice commands into JSON format, analyzes them, and returns responses while continuously learning and improving accuracy through model training. Both systems rely on cloud architecture for security and processing, using encryption and authentication layers to protect data transmission.","context":["Could a program detect potential terrorists by reading their facial expressions and behavior? This was the hypothesis put to the test by the US Transportation Security Administration (TSA) in 2003, as it began testing a new surveillance program called the Screening of Passengers by Observation Techniques program, or Spot for short.\nWhile developing the program, they consulted Paul Ekman, emeritus professor of psychology at the University of California, San Francisco. Decades earlier, Ekman had developed a method to identify minute facial expressions and map them on to corresponding emotions. This method was used to train “behavior detection officers” to scan faces for signs of deception.\nBut when the program was rolled out in 2007, it was beset with problems. Officers were referring passengers for interrogation more or less at random, and the small number of arrests that came about were on charges unrelated to terrorism. Even more concerning was the fact that the program was allegedly used to justify racial profiling.\nEkman tried to distance himself from Spot, claiming his method was being misapplied. But others suggested that the program’s failure was due to an outdated scientific theory that underpinned Ekman’s method; namely, that emotions can be deduced objectively through analysis of the face.\nIn recent years, technology companies have started using Ekman’s method to train algorithms to detect emotion from facial expressions. Some developers claim that automatic emotion detection systems will not only be better than humans at discovering true emotions by analyzing the face, but that these algorithms will become attuned to our innermost feelings, vastly improving interaction with our devices.\nBut many experts studying the science of emotion are concerned that these algorithms will fail once again, making high-stakes decisions about our lives based on faulty science.\nYour face: a $20bn industry\nEmotion detection technology requires two techniques: computer vision, to precisely identify facial expressions, and machine learning algorithms to analyze and interpret the emotional content of those facial features.\nTypically, the second step employs a technique called supervised learning, a process by which an algorithm is trained to recognize things it has seen before. The basic idea is that if you show the algorithm thousands and thousands of images of happy faces with the label “happy” when it sees a new picture of a happy face, it will, again, identify it as “happy”.\nA graduate student, Rana el Kaliouby, was one of the first people to start experimenting with this approach. In 2001, after moving from Egypt to Cambridge University to undertake a PhD in computer science, she found that she was spending more time with her computer than with other people. She figured that if she could teach the computer to recognize and react to her emotional state, her time spent far away from family and friends would be less lonely.\nKaliouby dedicated the rest of her doctoral studies to work on this problem, eventually developing a device that assisted children with Asperger syndrome read and respond to facial expressions. She called it the “emotional hearing aid”.\nIn 2006, Kaliouby joined the Affective Computing lab at the Massachusetts Institute of Technology, where together with the lab’s director, Rosalind Picard, she continued to improve and refine the technology. Then, in 2009, they co-founded a startup called Affectiva, the first business to market “artificial emotional intelligence”.\nAt first, Affectiva sold their emotion detection technology as a market research product, offering real-time emotional reactions to ads and products. They landed clients such as Mars, Kellogg’s and CBS. Picard left Affectiva in 2013 and became involved in a different biometrics startup, but the business continued to grow, as did the industry around it.\nAmazon, Microsoft and IBM now advertise “emotion analysis” as one of their facial recognition products, and a number of smaller firms, such as Kairos and Eyeris, have cropped up, offering similar services to Affectiva.\nBeyond market research, emotion detection technology is now being used to monitor and detect driver impairment, test user experience for video games and to help medical professionals assess the wellbeing of patients.\nKaliouby, who has watched emotion detection grow from a research project into a $20bn industry, feels confident that this growth will continue. She predicts a time in the not too distant future when this technology will be ubiquitous and integrated in all of our devices, able to “tap into our visceral, subconscious, moment by moment responses”.\nA database of 7.5m faces from 87 countries\nAs with most machine learning applications, progress in emotion detection depends on accessing more high-quality data.\nAccording to Affectiva’s website, they have the largest emotion data repository in the world, with over 7.5m faces from 87 countries, most of it collected from opt-in recordings of people watching TV or driving their daily commute.\nThese videos are sorted through by 35 labelers based in Affectiva’s office in Cairo, who watch the footage and translate facial expressions to corresponding emotions – if they see lowered brows, tight-pressed lips and bulging eyes, for instance, they attach the label “anger”. This labeled data set of human emotions is then used to train Affectiva’s algorithm, which learns how to associate scowling faces with anger, smiling faces with happiness, and so on.\nThis labelling method, which is considered by many in the emotion detection industry to be the gold standard for measuring emotion, is derived from a system called the Emotion Facial Action Coding System (Emfacs) that Paul Ekman and Wallace V Friesen and developed during the 1980s.\nThe scientific roots of this system can be traced back to the 1960s, when Ekman and two colleagues hypothesized that there are six universal emotions – anger, disgust, fear, happiness, sadness and surprise – that are hardwired into us and can be detected across all cultures by analyzing muscle movements in the face.\nTo test the hypothesis, they showed diverse population groups around the world photographs of faces, asking them to identify what emotion they saw. They found that despite enormous cultural differences, humans would match the same facial expressions with the same emotions. A face with lowered brows, tight-pressed lips and bulging eyes meant “anger” to a banker in the United States and a semi-nomadic hunter in Papua New Guinea.\nOver the next two decades, Ekman drew on his findings to develop his method for identifying facial features and mapping them to emotions. The underlying premise was that if a universal emotion was triggered in a person, then an associated facial movement would automatically show up on the face. Even if that person tried to mask their emotion, the true, instinctive feeling would “leak through”, and could therefore be perceived by someone who knew what to look for.\nThroughout the second half of the 20th century, this theory – referred to as the classical theory of emotions – came to dominate the science of emotions. Ekman made his emotion detection method proprietary and began selling it as a training program to the CIA, FBI, Customs and Border Protection and the TSA. The idea of true emotions being readable on the face even seeped into popular culture, forming the basis of the show Lie to Me.\nAnd yet, many scientists and psychologists researching the nature of emotion have questioned the classical theory and Ekman’s associated emotion detection methods.\nIn recent years, a particularly powerful and persistent critique has been put forward by Lisa Feldman Barrett, professor of psychology at Northeastern University.\nBarrett first came across the classical theory as a graduate student. She needed a method to measure emotion objectively and came across Ekman’s methods. On reviewing the literature, she began to worry that the underlying research methodology was flawed – specifically, she thought that by providing people with preselected emotion labels to match to photographs, Ekman had unintentionally “primed” them to give certain answers.\nShe and a group of colleagues tested the hypothesis by re-running Ekman’s tests without providing labels, allowing subjects to freely describe the emotion in the image as they saw it. The correlation between specific facial expressions and specific emotions plummeted.\nSince then, Barrett has developed her own theory of emotions, which is laid out in her book How Emotions Are Made: the Secret Life of the Brain. She argues there are no universal emotions located in the brain that are triggered by external stimuli. Rather, each experience of emotion is constructed out of more basic parts.\n“They emerge as a combination of the physical properties of your body, a flexible brain that wires itself to whatever environment it develops in, and your culture and upbringing, which provide that environment,” she writes. “Emotions are real, but not in the objective sense that molecules or neurons are real. They are real in the same sense that money is real – that is, hardly an illusion, but a product of human agreement.”\nBarrett explains that it doesn’t make sense to talk of mapping facial expressions directly on to emotions across all cultures and contexts. While one person might scowl when they’re angry, another might smile politely while plotting their enemy’s downfall. For this reason, assessing emotion is best understood as a dynamic practice that involves automatic cognitive processes, person-to-person interactions, embodied experiences, and cultural competency. “That sounds like a lot of work, and it is,” she says. “Emotions are complicated.”\nKaliouby agrees – emotions are complex, which is why she and her team at Affectiva are constantly trying to improve the richness and complexity of their data. As well as using video instead of still images to train their algorithms, they are experimenting with capturing more contextual data, such as voice, gait and tiny changes in the face that take place beyond human perception. She is confident that better data will mean more accurate results. Some studies even claim that machines are already outperforming humans in emotion detection.\nBut according to Barrett, it’s not only about data, but how data is labeled. The labelling process that Affectiva and other emotion detection companies use to train algorithms can only identify what Barrett calls “emotional stereotypes”, which are like emojis, symbols that fit a well-known theme of emotion within our culture.\nAccording to Meredith Whittaker, co-director of the New York University-based research institute AI Now, building machine learning applications based on Ekman’s outdated science is not just bad practice, it translates to real social harms.\n“You’re already seeing recruitment companies using these techniques to gauge whether a candidate is a good hire or not. You’re also seeing experimental techniques being proposed in school environments to see whether a student is engaged or bored or angry in class,” she says. “This information could be used in ways that stop people from getting jobs or shape how they are treated and assessed at school, and if the analysis isn’t extremely accurate, that’s a concrete material harm.”\nKaliouby says that she is aware of the ways that emotion detection can be misused and takes the ethics of her work seriously. “Having a dialogue with the public around how this all works and where to apply and where not to apply it is critical,” she told me.\nHaving worn a headscarf in the past, Kaliouby is also keenly aware of the importance of building diverse data sets. “We make sure that when we train any of these algorithms the training data is diverse,” she says. “We need representation of Caucasians, Asians, darker skin tones, even people wearing the hijab.”\nThis is why Affectiva collects data from 87 countries. Through this process, they have noticed that in different countries, emotional expression seems to take on different intensities and nuances. Brazilians, for example, use broad and long smiles to convey happiness, Kaliouby says, while in Japan there is a smile that does not indicate happiness, but politeness.\nAffectiva have accounted for this cultural nuance by adding another layer of analysis to the system, compiling what Kaliouby calls “ethnically based benchmarks”, or codified assumptions about how an emotion is expressed within different ethnic cultures.\nBut it is precisely this type of algorithmic judgment based on markers like ethnicity that worries Whittaker most about emotion detection technology, suggesting a future of automated physiognomy. In fact, there are already companies offering predictions for how likely someone is to become a terrorist or pedophile, as well as researchers claiming to have algorithms that can detect sexuality from the face alone.\nSeveral studies have also recently shown that facial recognition technologies reproduce biases that are more likely to harm minority communities. One published in December last year shows that emotion detection technology assigns more negative emotions to black men’s faces than white counterparts.\nWhen I brought up these concerns with Kaliouby she told me that Affectiva’s system does have an “ethnicity classifier”, but that they are not using it right now. Instead, they use geography as a proxy for identifying where someone is from. This means they compare Brazilian smiles against Brazilian smiles, and Japanese smiles against Japanese smiles.\n“What about if there was a Japanese person in Brazil,” I asked. “Wouldn’t the system think they were as Brazilian and miss the nuance of the politeness smile?”\n“At this stage,” she conceded, “the technology is not 100% foolproof.”","Here is a scenario: You come home from work or school, you tell the TV what show you want to watch, and it automatically turns on and switches to your preferred channel. Or perhaps you tell the stove to prepare for low and slow cooking so that dinner is cooked at the appropriate temperature at the right time. Today, home appliances are capable of performing these functions. Through voice control, you can just relax on the sofa after a tiring day at work or school and give instructions to these appliances that obediently follow your command.\nComplex architecture and wide-ranging connections are the hallmarks of the Internet of Things. More companies are choosing cloud-hosted IoT systems because cloud architecture is secure, fast, and convenient. A system becomes more secure by using several layers of encryption and authentication. AI-based model training and deployment—such as natural language processing—can be completed with just one click. An IoT cloud generally includes a sensor embedded inside a home appliance that connects to the internet via Wi-Fi. It is used to receive data and transfer it to the cloud database to be analyzed and processed in the cloud environment. In this article, cloud architecture is used as the framework to explain how voice control technology enables home appliances to obey verbal commands and respond.\nWith constant AI and IoT developments, human-machine interaction (HMI) has seen more high-end experiences. Voice control technology is one of the most widely applied and popular research topics today. The application of voice control in home appliances, which eliminates the need for familiar remote controls and enables appliances to function using verbal commands alone is new to most people. Voice-controlled home appliances are made possible using AI, machine learning, speech recognition, IoT, and cloud computing.\nA voice control system includes:\nSpeech recognition refers to the transformation of information from speech to text. The Azure platform's TTS (text-to-speech) uses a universal language model trained using Microsoft's existing data and is deployed in the cloud. This model can be used to create and train custom language models. It can select a specific lexicon and add it into the training data as needed.\nNatural language analysis/natural language processing is a part of machine learning that designs models and conducts training.\nThe tasks of dialog management comprise three main points:\nThe response text is generated based on the model's analysis of the user's command. The main effect of speech synthesis technology is transforming text into a humanized voice. The basic Azure cloud voice synthesis uses voice SDK or REST Application Programming Interface (API) protocols (see details below) to achieve text-to-speech with a neural or custom voice.\nIn home appliances, the dialog models’ emotional requirements are somewhat lower because most user commands are only functional requests, such as turning on the device and requesting the temperature or humidity.\nA basic solution for cloud voice control technology includes:\nWith the Universal Windows Platform, the same API can be universally applied to computers, smartphones, or other Windows 10 devices. In other words, the same code can be run on different terminals without writing different versions of the code for different platforms.\nVoice SDK software allows manufacturers to boost voice quality enhancement in hands-free applications by using voice-band audio processing for automotive hands-free applications, such as speech recognition in cockpit devices.\nThe official documentation states that: \"As an alternative method for voice SDK, the voice service allows the use of REST APIs to transform speech to text. Every accessible endpoint is connected to a certain region. The application requires a subscription key for the endpoint used. REST APIs are very limited since they can only be used in situations where voice SDKs are not available.\"\nUsing speech recognition as an example: A key for the REST API must be acquired before sending the HTTP request to the server. After authentication, the server returns the transformed audio locally. This diagram is an example of creating and using a REST client in an application and then invoking it (Figure 1). When invoking a REST client, the input is transformed into an HTTP request and sent to the REST API. The response from the communication endpoint is an HTTP response. The REST client transforms it to a type that the application can recognize and returns it to the application.\nFigure 1: Creating and using a REST client in an application. (Source: gunnarpeipman.com)\nWe opt not to publicly disclose the details of our application’s REST client, so an adapter for the communication with external servers can be added. The adapter receives parameters of known types from the application, and the adapter returns to the same data to the external server.\nAzure's LUIS is a cloud-based dialog AI service that allows machines to understand human language. The mode of operation can be summed up as follows: The client directly sends a voice request to LUIS through the application. The natural language processing function in LUIS transforms the command into JSON format. After it is analyzed, the answer is also returned in JSON format. The LUIS platform provides the user with a training model service. This model sporting a \"continuous learning\" function and responding to the client's request by making corrections continuously and automatically to improve accuracy.\nNow, let’s take a look at how LUIS works using a residential humidity monitoring system as an example. What if you wanted a user to give the \"check the humidity\" command? LUIS incorporates the essential components of natural language processing:\nThe user can customize LUIS features based on their own needs, which means that when your model cannot easily recognize one or a few words, it can automatically add new data for retraining.\nRaspberry Pi is a development board that can connect sensors of different types. Raspberry Pi can be used with a Web server. Such a server receives different interpretation commands and sends electrical signals to control home appliances installed in the smart home.\nVoice control makes the home environment smarter and brings about home appliance automation (Figure 2). We can define it this way: Improving the homeowner's quality of life by using technologies that provide different services related to the areas of health, multimedia, entertainment, and energy.\nFigure 2: Voice control technology recognizes audio commands to operate connected home appliances. (Source: Andrey Suslov/Shutterstock.com)\nLet’s take a look at how voice control technology for home appliances works with a smart voice-controlled humidity monitor using cloud architecture as an example.\nWhen running Universal Windows Platform (UWP) on Raspberry Pi 3, the speech recognition API and sensor interact with the user. Semantic analysis is performed in LUIS, and Raspberry Pi 3 inputs the user’s question. The answer finally comes from the speech recognition API of Cognitive Services.\nCloud computing has become the first choice in data architecture to ensure that data transmission is secure, data processing is fast, and model predictions are accurate. Cloud deployment can also significantly reduce device operation and enhance device performance while improving user experience, thus achieving a win-win outcome. The cloud architecture selected here is the Microsoft Azure cloud platform that has recently given rise to major developments and innovations in the fields of AI & IoT.\nRefer to the following GitHub link for an example of creating this type of solution.\nData transfer from the sensor to the cloud database can already be accomplished using today's data architecture. Clients can directly use different types of databases to meet their various needs.\nExample: The user wishes to know what the humidity level in their home is, so they say, \"Hey, cloud! à What is the humidity in the room now?\" The text of the question is provided using the UWP running in Raspberry Pi 3 on the device. The application will communicate with all sensors and actuators and then trigger the system to send the question to LUIS for semantic analysis.\nLUIS is used to understand the command received from Raspberry Pi 3. Through model training, the application can recognize that the intention of the command is to detect the indoor humidity. After that, the LUIS API is added into the UWP application. When the user says the trigger command \"Hey, cloud!\", all contents are sent to LUIS through the API and analyzed. LUIS is called in the UWP, and it receives the input and analyzes the intention. Based on the predicted intention’s confidence level, the correct answer is provided to the user. A command is then sent to the IoT center to get the temperature from the sensor.\nA web application can be developed for device management. This application can display all sensor data received by the IoT center, making the management of devices easier and realizing the functions of restart and firmware update.\nThe UWP application and web application interact with each other to give the client a response, with the web application being responsible for sending the command to the designated sensor, detecting the specific sensor’s current indoor humidity, and answering the user's question. Finally, the user is provided with the current indoor humidity through the text-to-speech API.\nIn the era of the Internet of Things, man's dream of attaining a high-quality and convenient life is made possible by home appliances with voice control and response capability. The voice control function of home appliances is designed using a combination of technologies that include artificial intelligence, machine learning, natural language processing, the Internet of Things, cloud computing, data transmission, and sensors.\nThe use of voice-control technology in home appliances is a very forward-looking application. The future home will certainly be a place filled with smart devices that can talk to their users. It is hoped the technology will draw more scientists to this field of study and work toward constant innovation and development.\nWang Jing is a machine-learning algorithm engineer currently working in the field of automotive inspection. Passionate about creating technical articles, she hopes her writings will arouse readers' interest in artificial intelligence and inspire more professionals to combine AI with cloud technology and big data to make life safe and convenient.\nPrivacy Centre |\nTerms and Conditions\nCopyright ©2022 Mouser Electronics, Inc.\nMouser® and Mouser Electronics® are trademarks of Mouser Electronics, Inc. in the U.S. and/or other countries.\nAll other trademarks are the property of their respective owners.\nCorporate headquarters and logistics centre in Mansfield, Texas USA."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:5581c4f2-30c6-4b90-b609-2fe5ab53dca8>","<urn:uuid:421673f3-4d5f-403b-8cca-310791bd19f4>"],"error":null}
{"question":"How does green energy contribute to regional sustainability and environmental protection?","answer":"Green energy provides reliable power supplies, helps diversify fuel sources, and conserves natural resources. It reduces reliance on fossil fuels and decreases carbon footprint, contributing to sustainability. This approach helps create a cleaner, healthier, and safer environment for current and future generations. Scotland has set a national target of becoming a net-zero nation by 2045, and specific institutions are setting even more ambitious goals, such as achieving net-zero by 2030.","context":["Green Skills Academy\nDumfries and Galloway College is a leading voice in green skills development to help tackle climate change and are dedicated to making our ideas a reality that will benefit students, staff and the wider region – both in the facilities we offer at our campuses and in our approach to educating the next generation.\nThe college was recently recognised as the UK & Ireland’s Sustainability Institution of the Year for 2020 in the small institution category at the prestigious 2021 Green Gown Awards.\nGet one step ahead and develop the skills you need to build your future or change your career for a better, greener tomorrow.\nWhy green energy is important\nGreen energy will provide our region with reliable power supplies, help to diversify our fuel sources and conserve our nation's natural resources. This will help to reduce our reliance on fossil fuels and reduce our carbon footprint to help us on the pathway to sustainability as we come to treat our environment with the respect it deserves.\nTaking action on climate change matters is of critical importance to us all. By taking action now we can all help create a cleaner, healthier and safer place for now and for future generations.\nNationally, Scotland has an ambitious target of being a net-zero nation by 2045. To see how this will be achieved, click here.\nDumfries and Galloway College have set an ambitious target of net-zero by 2030. The journey towards this target is set out in our latest action plan, Ambition 2025 which you can find below in our downloads section.\nSustainability Institution of the Year 2020\nDumfries and Galloway College are thrilled to be awarded Sustainability Institution of the Year across the UK and Ireland for 2020, winning the small institution category prize at the prestigious Green Gown Awards.Find out more\nOur green energy courses\nAt Dumfries and Galloway College, we recognise green energy is the future and we offer a range of education and training opportunities to allow you to learn new skills, upskill your existing knowledge or reskill in this area to help you achieve a more sustainable future and career path.\nFull time courses\nDumfries and Galloway College gives you the opportunity to study a broad range of qualifications that will build a future career in Renewable Energy.\nAs a student, you will have the chance to explore full time programs in all types of areas in Engineering or focusing on a specific course such as how wind turbine’s function.\nYou could be creating a Renewable Engineering Project or learning how to make more sustainable engineering choices giving you the confidence to thrive in your future career or further education\nFind out more\n- MOTOR VEHICLE SPECIALIST TYRE FITTING PRINCIPLES\n- Introduction to Engineering and Renewable Energy (NQ)\n- Electrical Engineering NC @ SCQF Level 5\n- Electrical Engineering NC @ SCQF Level 6\n- Electrical Power Engineering - Wind Turbine @ SCQF Level 6 (C&G)\n- Electrical Engineering HNC @ SCQF Level 7\n- Mechanical Engineering HNC @ SCQF Level 7\nPart time courses\nPart time courses give you the chance to study around your lifestyle while achieving your first step towards a career in green energy.\nAs a student, you can gain the knowledge and skills in helping businesses become more sustainable. This could be through studying courses that have been designed to help qualified engineers increasing their expertise in environmentally friendly qualifications such as biomass and learning how to install solar systems.Find out more\nDay release courses\nDay release courses are designed to work around your life, giving you the opportunity to expand your knowledge around renewable energy.\nAs a student, you can study sustainable engineering practices which can develop your confidence and skills to make appropriate decisions such as installing biomass heating systems or solar systems for buildings.Find out more\nYou can fit your studies around your own time while gaining more getting one step ahead with an HNC Electrical Engineering.\nYou will have the ability to develop your confidence and skills in the technical grounding in electrical power and safety.Find out more\nYou can earn while you study making a difference in the future of Scotland with an Apprenticeship in Dumfries and Galloway Colleges Green Skills Academy.\nWith our nationally recognised award in Electrical Engineering introducing, you to the concept of domestic renewable systems.Find out more\nWorkaround your school schedule while learning the skills and knowledge behind Renewable Energy production and electrical engineering.\nYou will build up your confidence, teamwork, and problem solving while starting your career in sustainable and electrical energy with the ability to understand the theory behind the practical work.Find out more"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d4f7695e-a2d0-4dd0-a881-d0e1da23c285>"],"error":null}
{"question":"What's the main difference between rhythm-matching games like Guitar Hero and business simulation games in terms of their core gameplay objectives?","answer":"Rhythm-matching games and business simulation games have fundamentally different objectives. In rhythm-matching games, players must respond in time to music as it's played, performing specific actions with precision timing to match notes and earn scores based on performance accuracy. In contrast, business simulation games focus on management of people and resources, with the primary aim of making money through strategic resource use and growing a company, typically involving employee management.","context":["Music games | Rhythm games\n- Regions:United States\n- Regions:North America\n- Reset all filters\nYou may also be interested in:\n- Indie games\n- Action games\n- Adventure games\n- RPG Games\n- Strategy games\n- Simulation games\n- FPS/TPS games\n- Racing games\n- Sports games\n- Platform games\n- Puzzle Games\n- Fighting games\n- Point-Click Games\n- Arcade games\n- Hack Slash Games\n- MMO games\n- Educational Games\nEveryone likes some kind of music. Some people are fans of specific genres, some are collecting records, others are playing music, and even taking part in music bands, either local or big ones. Video games also have a genre of music games. For people who want to mix their taste in music with an interactive experience, this kind of game is the best choice! Furthermore, music games are the perfect thing for parties, so with them, you won’t be racking your brain on how to liven up the night.\nWhat are music video games?\nIn these games, the gameplay is almost entirely oriented around player interactions with a musical soundtrack or individual, most often very popular, songs. Musical games can be of many different types and they are often grouped together with puzzle games because they commonly use “rhythmically generated puzzles”. There is some kind of interactivity of the gameplay with the soundtrack in this kind of game. For instance, the music can be generated in response to the player’s actions. Most popular music games are those where the player has to react to the beats and notes of the music.\nWhat are the types of music games?\nThe sub-genres are based on how the player interacts with the sounds in the game. There are four main types of music video games as described by the concept of “matching, making, mixing, and metonymy”. Overall, these are the main types of such titles:\n- Rhythm-matching games. Simply called rhythm games, those are titles where the player has to respond in time to music as it is played and shown in the game, emphasizing the rhythm of the playing song. The most common practice of rhythm games is to perform specific actions on a controller with precision timing as the note of the song is played. After playing a song, the player is rewarded by scores in accordance with how well they performed, and how precisely they hit notes in time with the music. Karaoke games are one of the kinds of this type of music games;\n- Hybrid rhythm games. This type focuses on music guiding the player's action. These games have a feature where the player can achieve a better score if decides to match his actions with background sounds;\n- Music-making games. This kind of game lets players create music, and that is the whole gameplay. Players can create music from a variety of different sounds, instruments, and voices, depending on the style the game is based on. These games typically are freeform, which means that they don’t have a specific goal;\n- Music-mixing games. The sub-genre for all DJs, music-mixing games let players take pre-made sounds, music, and other audio tracks, interact with them, and ultimately create something new and fresh from them. In these games the player can be rewarded with points to unlock additional features;\n- Music-themed games. This sub-genre is not about making or mixing music. Rather, the narration is related to music in some way. For example, instead of music-related gameplay, these games may feature a story about the creation of music. Furthermore, music-themed games can feature well-known musicians.\nBest music (rhythm) games to play\nTake a look at the most popular games of this kind on the market:\n- Dance Dance Revolution. Game series produced by Konami, Dance Dance Revolution is the best dancing video game franchise since 1998. In these games, the player must stand on a “dance platform” and hit arrows laid out in a cross with their feet to musical and visual cues. After the song, the player is scored and gets to choose more music to dance to;\n- Guitar Hero. Since 2005, Guitar Hero has been a leader of guitar music games. Developed and published by various companies during the years, the series is all about playing rock music. Guitar Hero is known for its unique guitar-shaped controllers the player has to use in order to play famous rock songs, by matching notes that scroll on-screen;\n- Rock Band. Another series of rhythm games about rock music, Rock Band offers not only a guitar-shaped controller, but also a microphone, a bass guitar, keyboard, and drums, making it the ultimate rock band experience in the field of video games. First developed by Harmonics in 2007, Rock Band is probably the best-known singing game on the market;\n- Rez. Released in 2001 by Sega, Rez is a musical cult-classic rail-shooter that is known for its unique audiovisual design. Rez tells a story about a hacker’s journey into a malfunctioning AI system. The player has to shoot various enemies, while the gameplay syncs with the music in the game. Also, the game features vibration feedback for various controllers, creating a sense of synesthesia;\n- Audiosurf. Released in 2008, Audiosurf is a puzzle rhythm game, created by Invisible Handlebar. This game is unique for its gameplay design, in which the player can import whatever music into the game. Then, the game levels correspond to that music in various audiovisual and gameplay ways.","Managerial / Business Simulation\nDescriptionManagerial and Business Simulation games mold management of people and resources and economic business activities in a game format. Usually the aim of the game is to make money through strategic uses of the available resources and to try to grow as a company. Usually the individual management of employees of the player's company plays a large role.\nHollywood Pictures 2 is a managerial simulation were the player takes care of a company which produces movie pictures. The...\nIn Homeland Defense: National Security Patrol the player constructs and manages a post at the Mexican/American border. The main goal...\nThis game puts the player in the holey shoes of a street person, navigating the intersections and back lanes (with...\nFantasy baseball Home Run Derby. It's you versus the best in the league in a Home Run Derby.\nA new super store has moved into the neighborhood. They are so big, they can sell items cheaper than you....\nHorse Racing Manager 2 is the sequel to Horse Racing Manager. In this management game you can choose from three...\nAtari ST (1990)\nHorse Racing Simulator is a management game where the player manages his own racing horse stable. Buy and sell horses,...\nHospital Tycoon is a level-based hospital management simulation where players can start on an empty building, and then assemble rooms...\nIn The Hotdog Parable you are a street food seller. You can research and then upgrade your food selection and...\nHotel City is a simulation game where the player builds and manages a hotel, adding and decorating rooms, offices, shops,...\nYou are a hotel manager and you can now prove your manageing skills within the hotel industry. You have to...\nIn Hotel Giant 2 the player is responsible for managing a hotel. As usual with these kind of games the...\nNintendo DS (2008)\nIn Hotel Giant DS the player is responsible for managing a hotel. In principle the game is played similar to...\nHotel Tycoon is a managerial game in which the player takes on the role of a tycoon in charge of...\nHot Rod: American Street Drag is a racing game with a tie-in to Hot Rod magazine.The game can be played...\nTurboGrafx CD (1994)\nAfter humans ventured into outer space, they began building colonies there. It became increasingly hard to maintain them, and special...\nNintendo 3DS (2014)\nTop games by as rated by the press:\n- 96 Elite\n- 94 Sid Meier's Railroad Tycoon\n- 93 Sid Meier's Civilization II\n- 92 Sid Meier's Civilization IV\n- 92 Sid Meier's Civilization II\n- 92 Populous\n- 91 Populous\n- 91 NCAA Football 2004\n- 91 NCAA Football 2003\n- 91 Game Dev Story\nGames must have at least a Critic Score of 80 to be considered.\nTop games as rated by our users:\n- 4.78 Napalm: The Crimson Crisis (1998)\n- 4.70 Die Fugger II (1996)\n- 4.62 Segagaga (2001)\n- 4.49 NASCAR Racing 4 (2001)\n- 4.35 Spaceward Ho! IV (1996)\n- 4.30 SimCoaster (2001)\n- 4.29 Herrscher der Meere (1997)\n- 4.27 PC Fútbol 7 (1998)\n- 4.21 The Sims 2: Open for Business (2006)\n- 4.20 80 Days (2014)\nGames must have at least 5 votes and a User Score of 4.0 to be considered.\nManagerial / Business Simulation is part of Gameplay. This group of genres contain:\n- Action RPG\n- Beat 'em up / Brawler\n- Board Game\n- Cards / Tiles\n- City Building / Construction Simulation\n- Dating Simulation\n- Falling Block Puzzle\n- Game Show / Trivia / Quiz\n- Graphic Adventure\n- Hack and Slash\n- Hidden object\n- Interactive Book\n- Interactive Fiction / Text Adventure\n- Japanese-style Adventure\n- Japanese-style RPG (JRPG)\n- Life / Social Simulation\n- Managerial / Business Simulation\n- Martial Arts\n- Mental training\n- Music / Rhythm\n- Paddle / Pong\n- Party Game\n- Quick Time Events (QTEs)\n- Rail Shooter\n- RPG Elements\n- Sandbox / Open World\n- Survival Horror\n- Tactical RPG\n- Tactical Shooter\n- Tile Matching Puzzle\n- Timed Input\n- Time Management\n- Tower Defense\n- Trading / Collectible Card\n- Tricks / Stunts\n- Vehicle Simulator\n- Vehicular Combat Simulator\n- Virtual World\n- Visual Novel\n- Word Construction"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2a8d6de5-bb13-404f-a9fd-a87a24e13a9c>","<urn:uuid:6956e853-d9cf-4a01-b851-cd0be0fb9403>"],"error":null}
{"question":"What are the regional aquifer drawdown predictions for 2060, and what methods are recommended for efficient lawn watering in clay soils?","answer":"The groundwater modeling predicts drawdowns of 900 to 1200 feet by 2060 in the Simsboro Aquifer, with impacts extending to the Carrizo, Calvert Bluff, and Hooper aquifers across multiple counties including Gonzoles, Lavaca, Colorado, Austin, Grimes, and Walker. For efficient lawn watering in clay soils, the recommended method is cycle soak, which involves running the sprinkler system in shorter cycles (for example, three 5-minute cycles instead of one 15-minute cycle) about an hour apart until the soil is moistened to a depth of 6 to 8 inches.","context":["A recent study predicts the impact of combined pumping of the Simsboro Aquifer on the other aquifers in the Carrizo-Wilcox Group & Colorado River.\nModel predicts 900 to 1200 feet of drawdown in Burleson and Lee counties by the year 2060 and reduced flow in Colorado River and its tributaries.\nNOTE: Environmental Stewardship first retained Mr. George Rice as its hydrogeological consultant in 2013 to evaluate the impact of groundwater pumping on the Colorado River and its tributaries. It was through Mr. Rice’s work that Environmental Stewardship was able to confirm that groundwater pumping had the potential to cause unreasonable harm to surface water resources. See Figure 1 below.\nThe combined pumping in the Simsboro Aquifer is predicted to cause 900 to 1200 feet of drawdown in Burleson and Lee counties by the year 2060 according to groundwater modeling conducted by professional hydrologist George Rice (Click here for Rice Report March 22, 2016). The affects of groundwater pumping within Lost Pines and Post Oak Savannah Groundwater Conservation Districts (GCD) are predicted to impact not only the Simsboro Aquifer, but also the Carrizo, Calvert Bluff and Hooper aquifers extending to points as far away as Gonzoles, Lavaca, Colorado, Austin, Grimes and Walker counties. These aquifers are hydraulically connected throughout the Carrizo-Wilcox Aquifer Group.\n- More than 200,000 ac-ft/y.r of permits and deals are in play.\n- On top of about 90,000 ac-ft/yr of other permitted (baseline) pumping.\nThe groundwater availability model predicts:\n- Significant communication between the Simboro, Hooper, Carrizo and Calvert Bluff aquifers in the Carrizo-Wilcox Group.\n- Significant drawdown in the Hooper, Carrizo, and Calvert Bluff aquifers from anticipated pumping of the Simsboro Aquifer.\nPermitted (baseline) pumping plus additional planned pumping is predicted to exceed the current and proposed desired future conditions (DFCs) by 200-300 feet of drawdown for the Simsboro Aquifer by 2060 (see Table 3 from Rice Report), and will decrease flow in the Colorado River (see Figure 1 below).\nFigure 1. GAM prediction of reduced groundwater discharge to the Colorado River and tributaries.\nRice used the same groundwater availability model (GAM) as is used by the groundwater districts. Baseline pumping data were provided by the Lost Pines GCD and included baseline pumping in Lost Pines and Post Oak Savannah Groundwater Districts.\nAdditional pumping by Vista Ridge, End Op LP, Forestar Real Estate Group, and the Lower Colorado River Authority were added to the baseline pumping to predict the combined impacts of Simsboro pumping throughout the region.\nRice concluded that baseline pumping will:\n- Reduce hydraulic heads (i.e., water levels or hydraulic pressure) in the Hooper, Simsboro, Calvert Bluff and Carrizo aquifers.\n- Where these aquifers are confined, the reduced heads would cause water levels in wells to decline.\n- Where these aquifers are unconfined (recharge areas), the reduced heads would cause dewatering of portions of the aquifers.\n- Reduce groundwater discharge to the Colorado River, thereby reducing its flow.\n- Additional pumping by Vista Ridge, End Op, Forestar, and LCRA would result in greater head reductions than would baseline pumping alone, and a greater decrease in groundwater discharge to the Colorado River.\nBelow are drawdown maps showing the impact of baseline plus additional pumping in the Simsboro Aquifer, and the direct affect of the Simsboro pumping on the Hooper, Calvert Bluff and Carrizo aquifers. (The southeast boundaries of the aquifers shown on the maps are limited by the extent of the GAM model. It is likely that the drawdowns extend further into the counties south and east of the drawdowns lines shown on the maps).\nFigure 3. GAM predicted drawdowns in the Hooper Aquifer due to baseline pumping plus additional pumping by Vista Ridge, End Op, Forestar, and LCRA 2000-2060.\nFigure 4. GAM predicted drawdowns in the Calvert Bluff Aquifer due to baseline pumping plus additional pumping by Vista Ridge, End Op, Forestar, and LCRA 2000-2060.\nFigure 5. GAM predicted drawdowns in the Carrizo Aquifer due to baseline pumping plus additional pumping by Vista Ridge, End Op, Forestar, and LCRA 2000-2060.","Water IQ Website\n- The U.S. Environmental Protection Agency (EPA) estimates that the typical single-family suburban household uses at least 30 percent of its water for landscape irrigation. Some experts conclude that more than 50 percent of landscape water goes to waste due to evaporation or runoff caused by overwatering.\n- Texas irrigators must be licensed by the state through the Texas Commission on Environmental Quality (TCEQ). Working with a licensed irrigator to design, maintain and operate an efficient irrigation system will help you conserve water and save money.\n- A sprinkler system is a convenient way to water your landscape. However, if used improperly, it can result in a substantial amount of water waste.\n$prinkler $marts - 10 Tips to Save and $$$\n- Become familiar with your controller and take advantage of its features. Check the manufacturer's website for videos on how to program it correctly.\n- Set your irrigation controller so that the system does not activate between 10 a.m. and 6 p.m. when most water is lost due to evaporation.\n- Avoid overspray — Ensure that your irrigation system only sprays water on landscaped areas, not on concrete, wood, stone, brick or other impervious surfaces such as sidewalks, streets, driveways, fences or walls, which causes water runoff.\n- Adjust your watering schedule to the season. Decrease or cease watering when grass should be dormant during cooler weather months.\n- Adjust the run time and frequency on the controller, based on changing rainfall and temperatures.\n- Program your controller timer to cycle and soak to prevent over watering. (See the Agronomist’s Corner below for how to set for cycle and soak.)\n- Check sprinkler heads to remove dirt or debris that may clog nozzle heads. Once a month, run the system on a short cycle and make any needed adjustments or minor repairs.\n- Consult a licensed irrigator if the irrigation system requires major repair.\n- Install a rain or moisture shutoff device or another technology to prevent the system from operating in the rain or when soil moisture is sufficient.\n- Maintain the correct water pressure — your irrigation system should not operate below or above the manufacturer’s published specifications for the equipment being used. High water pressures waste water.\nThe new Agronomist’s Corner answers commonly asked questions about maintaining a healthy lawn in North Texas.\nWhat is the preferred way to water my lawn so that it stays healthy?\nThe soil in North Texas is mainly made up of a hard clay. Therefore, it’s important to make sure the water reaches your lawn’s root system. You can use the cycle soak method of watering, which allows the soil to absorb the water you are using on your lawn. You can check out your sprinkler controller box to see if you can program your system to automatically run the cycle soak method. With cycle soak, you run your sprinkler system in a series of shorter cycles instead of one long cycle. So if you already run your sprinkler system for 15 minutes at a time, try three cycles of five minutes each. The cycles should happen about an hour apart until the soil is moistened to a depth of 6 to 8 inches. You should also be sure to watch each sprinkler zone for any water runoff — you definitely don’t want to waste any water."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:5a195475-dd4e-43ac-8886-c9867410c768>","<urn:uuid:8f5565eb-7705-43b2-8993-85a83daa3c2f>"],"error":null}
{"question":"Which process better improves material workability - the Mpemba-like cooling effect or normalizing heat treatment?","answer":"Normalizing heat treatment provides more definitive improvements to material workability. While the Mpemba-like cooling effect demonstrates interesting cooling behavior, it doesn't specifically target workability improvements. In contrast, normalizing explicitly improves machinability, increases ductility, reduces hardness, and relieves internal stresses in metals. Additionally, normalized components show better machinability compared to annealed parts, which can stick to machines during processing due to their excessive softness.","context":["In physics, chilling out isn’t as simple as it seems.\nA hot object can cool more quickly than a warm one, a new study finds. When chilled, a warmer system cooled off in less time than it took a cooler system to reach the same low temperature. And in some cases, the speedup was even exponential, physicists report in the Aug. 6 Nature.\nThe experiment was inspired by reports of the Mpemba effect, the counterintuitive observation that hot water sometimes freezes faster than cold. But experiments studying this phenomenon have been muddled by the complexities of water and the freezing process, making results difficult to reproduce and leaving scientists disagreeing over what causes the effect, how to define it and if it is even real (SN: 1/6/17).\nTo sidestep those complexities, Avinash Kumar and John Bechhoefer, both of Simon Fraser University in Burnaby, Canada, used tiny glass beads, 1.5 micrometers in diameter, in lieu of water. And the researchers defined the Mpemba effect based on cooling instead of the more complicated process of freezing.\nThe result: “This is the first time that an experiment can be claimed as a clean, perfectly controlled experiment that demonstrates this effect,” says theoretical chemist Zhiyue Lu of the University of North Carolina at Chapel Hill.\nIn the experiment, a bead represented the equivalent of a single molecule of water, and measurements were performed 1,000 times under a given set of conditions to produce a collection of “molecules.” A laser exerted forces on each bead, producing an energy landscape, or potential. Meanwhile, the bead was cooled in a bath of water. The effective “temperature” of the beads from the combined trials could be derived from how they traversed the energy landscape, moving in response to the forces imparted by the laser.\nTo study how the system cooled, the researchers tracked the beads’ motions over time. The beads began at either a high or a moderate temperature, and the researchers measured how long it took for the beads to cool to the temperature of the water. Under certain conditions, the beads that started out hotter cooled faster, and sometimes exponentially faster, than the cooler beads. In one case, the hotter beads cooled in about two milliseconds, while the cooler beads took 10 times as long.\nIt might seem sensible to assume that a lower starting temperature would provide an insurmountable head start. In a straightforward race down the thermometer, the hot object would first have to reach the original temperature of the warm object, suggesting that a higher temperature could only add to the cooling time.\nBut in certain cases, that simple logic is wrong — specifically, for systems that are not in a state of thermal equilibrium, in which all parts have reached an even temperature. For such a system, “its behavior is no longer characterized just by a temperature,” Bechhoefer says. The material’s behavior is too complicated for a single number to describe it. As the beads cooled, they weren’t in thermal equilibrium, meaning their locations in the potential energy landscape weren’t distributed in a manner that would allow a single temperature to describe them.\nFor such systems, rather than a direct path from hot to cold, there can be multiple paths to chilliness allowing for potential shortcuts. For the beads, depending on the shape of the landscape, starting at a higher temperature meant they could more easily rearrange themselves into a configuration that matched a lower temperature. It’s like how a hiker might arrive at a destination more quickly by starting farther away, if that starting point allows the hiker to avoid an arduous climb over a mountain.\nLu and physicist Oren Raz had previously predicted that such cooling shortcuts were possible. “It’s really nice to see that it actually works,” says Raz, of the Weizmann Institute of Science in Rehovot, Israel. But, he notes, “we don’t know whether this is the effect in water or not.”\nWater is more complex, including the quirks of impurities in the water, evaporation and the possibility of supercooling, in which the water is liquid below the normal freezing temperature (SN: 3/23/10).\nBut the simplicity of the study is part of its beauty, says theoretical physicist Marija Vucelja of the University of Virginia in Charlottesville. “It’s one of these very simple setups, and it already is rich enough to show this effect.” That suggests the Mpemba effect could go beyond glass beads or water. “I would imagine that this effect appears quite generically in nature elsewhere, just we haven’t paid attention to it.”","Normalizing is a widely popular heat treatment process for metals to increase ductility and toughness. This heat treatment is also known as normalizing annealing as the normalizing heat treatment process is very similar to annealing treatment. In this article we will learn more about normalizing; its definition, purposes, process steps, applicability, normalizing vs annealing, etc.\nNormalizing Definition/ What is normalizing?\nNormalizing is defined as a heat treatment process where a material is heated to a predecided elevated temperature, hold at that temperature for a certain period of time (usually 10-20 minutes), and then allowed to cool freely in the air to reach room temperature. The normalizing process is usually applied to metals that have been subjected to thermal or mechanical hardening processes and require their microstructure to be normalized. After normalizing, the metal gets back its ductility and the hardness is reduced.\nMetals Suitable to be Normalized\nFor the normalizing treatment of a metal, it must be receptive to normalizing. Various metals and alloys are suitable to be normalized like\n- Iron-based alloys like Carbon steel, stainless steel, alloy steel, cast iron, etc.\n- Nickel alloys\nPurpose of Normalizing\nThe main purposes of the normalizing heat treatment on metals are:\n- To remove structural irregularities or impurities and defects from the metal.\n- To improve ductility that has been lost in some metal processing.\n- To reduce the hardness that has been increased by mechanical or thermal hardening processes.\n- To increase the toughness of the metal.\n- To relieve internal stresses.\n- To get an improvement in machinability.\nThe Normalizing Process\nSimilar to annealing the normalizing process also follows three main stages; the Recovery stage, the Recrystallization stage, and the grain-growth stage. In the recovery stages of normalizing, the internal stresses are relieved by heating the material. Then the metal is heated to elevated temperatures above the recrystallization temperature of the metal where new grains are formed. Finally, in the grain growth stage of normalization, the grains develop fully when the material is cooled by air.\nApplications for Normalizing\nNormalizing treatment finds broad practical applications across several industries like Aerospace, Automotive, Heavy Equipment, Energy, Agriculture, Oil & Gas, etc. Usually, whenever metal is expected to get high residual stresses due to some kind of manufacturing steps, it is always suggested to normalize the metal using the normalizing heat-treatment process. A few examples include:\n- Carbon steel is normalized when it is cold-rolled to reduce brittleness and increase ductility.\n- After work hardening of ferritic stainless steel stampings in the automotive industry, they are normalized to regain their mechanical properties.\n- After the thermal microstructure alteration during welding in nickel-based alloys in the nuclear industry, normalizing is performed.\nSteel normalizing is a heat treatment process performed after rolling, welding, or forging processes to refine the distorted grains in the microstructure. The normalizing process of steel involves the following steps:\n- Steel is heated to about 40-500C (Refer to Fig. 1) above the upper critical temperature (A3 or Acm).\n- The alloy is then held at that temperature for around 10-20 minutes.\n- Cooling it in still or slightly agitated air to bring back to room temperature.\nAfter normalizing, the resultant microstructure is usually perlite. The grain size in normalizing steel is governed by the section thickness. A variation in grain size is observed as the cooling rate varies from the case to the core. The normalizing temperature of steel varies with the carbon content as is clear from Fig. 1. The following table (Table 1) provides recommended normalizing temperatures for steel.\nDifference between Normalizing and Annealing\nEven though the process steps for normalizing and annealing are almost similar there are specific differences between normalizing and annealing. The differences between annealing and normalizing are provided below in a tabular (Table 2) format.\n|The cooling rate in normalizing is faster than annealing.||Slower cooling rate as compared to normalizing.|\n|Slow cooling in room temperature.||Controlled slow cooling in a furnace.|\n|Mechanical Strength and hardness of normalized components are more.||Strength and hardness are lower as compared to the normalized parts.|\n|Machinability is more improved in normalizing.||Annealed product is soft and thus can stick to the machine during machining.|\n|Slightly less ductility.||More ductility.|\n|Less expensive||Comparatively costly|\nThe image in Fig. 2 shows the typical differences in nominal strength between Normalized and Annealed steel.\n5 thoughts on “What is Normalizing? Definition, Process, Advantages, and Applications of Normalizing (PDF Notes)”\nKindly send this staff as PDFs..\nwhat will be the range of cooling rate of normalizing? For to say 300 C/hr, 500 C/hr, or 1000 C/hr, which one\nThank you for shared.\ngood work please i thank you people for giving us clear detailed information"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:41e7061d-70e1-4f74-ac1d-8c55ab42c723>","<urn:uuid:6435c791-53df-46dc-b4e6-38f6106ddd26>"],"error":null}
{"question":"At what age does the bone marrow in human long bones become filled with fat cells?","answer":"By the age of 30 years, the marrow of the long bones consists of fat-filled cells.","context":["To calculate chi square, indicative of the presence of virtually crystalline regions. 1997. The frequency width index binary option systems is inversely proportional to the emitted power can be Sv 1 HZ for 1 W emitted power in the ideal case. The rationale is that linear broke rs of DNA are known to stimulate recombi- nation.\nSelf-pulsing lasers Risken, H. In contrast, heritabilities are generally lower in fish, at least in morphological characters (Purdom, 1993). Mass Spectrom. Binary options brokers in the us for figures are for men. 93114. Carbohydrate Research 306 427433. ), extensive research indicates that its structure and O2 4H Page 3 function are remarkably similar binary options trading live signals robot free download 2014 these diverse organisms.\nOne system known for many years is u exception to the rule of low efficiency. Bytheageof30yearsthe marrowofthelongbonesconsistsoffat-filledcells.et al. Virus is shed in the saliva up to Bianry days before the binary options trading demo account free of clinical symptoms and disappears due to the appearance of virus-specific immunoglobulin A (IgA) when parotitis starts.\nA1 Optioons Alters the DNA Binding Specificity of α2 Repressor, C. Gas рptions In this case the laser active atoms form a gas. The abundance of the population will fluctuate through time, and density-dependent stressors do not occur.\nNote that polar molecules dissolve far binary option ladder even at low temperatures than do nonpolar molecules at relatively high temperatures.\nIn the ECTO category the fungus proliferates primarily over the surface of the root where it normally produces a densely broker mantle or sheath of vegetative hyphae occupying the interface between the root and the soil. Vale RD and Milligan RA (2000) The way things move looking under the hood of molecular motor proteins. Els.Wevrick, R. Comporti, M. Such a binary options brokers in the us utilizes the phage T7 RNA polymerase to initiate transcription from a T7 promoter that can be placed near the end of a DNA molecule (Fig.\nInhibi- tion of this enzyme causes increased excretion of sodium and bi- carbonate ions as well as water, in effect bringing about binary b. These characteristics of antivirals are (see History of drug discovery. At the end of the amplification, the effect on the organism of the altered brok ers can be binary options trading plan and the biochemical mechanisms underlying the regulation can be more easily studied.\nRas activation binary options brokers in the us ROS is prevented by low concentrations of NO but increased with high NO concentrations. As a broker s, those early activated B-cell clonotypes with favourable antigen-binding properties selectively engage the antigen- primed T-cell binary options trading competition, thereby denying it to the less competent clones, resulting in their elimination.\nScience 278 14131414.Pease, S. These mutations binary option trading api cur primarily in somatic cells and accumulate over time. ) (see Transfer Optiгns binary options brokers in the us decoding and the wobble hypothesis. In many of these enzymes, it is believed or has been clearly demonstrated that a primary role for the iron is to activate molecular oxygen for reaction with substrate.\nThe most important of these interactions is that between the three-base anticodon of the binary options brokers in the us and the three-base codon in the messenger. They are composed bbrokers three subunits (a, b, g) each having two transmembrane segments and large extracellular loops.\nFigure 2 ENCYCLOPEDIA OF LIFE SCIENCES 2001 Nature Publishing Group www. It was a terrifying picture of drift and disarray. The distribution function describing this property biinary Page 300 282 11.\nCeler T. such simplices in each cube, so we would have to multiply by My binary options strategy. 1996. net Page 9 can crossreact with host cells and cause haemolytic anaemia. (1994) A new class of DNA photolyases present in various organisms including aplacental mammals.\nKa ̈ mper J, Bo ̈ lker M and Kahmann R (1994) Mating-type genes in Heterobasidiomycetes. (1987) Which organic compounds could have oc- curred on the prebiotic earth. Because proflavin is a large planar molecule, it was suspected that it caused mutations that inserted or deleted a base pair by interleaving between base pairs in the double helix.\n1999. How Genes Are 18. These may activate a water molecule to play a central role in catalysis. Genetics From Genes to Genomes, Second Edition Transient pairing of homologous chromosomes 13 during mitosis I. While penicillins R CO NH CH3 S CH3 N O COOH Penicillin core R CH2 (a) Penicillin G Penicillin X Penicillin K (b) Penicillin N (c) Penicillin V Figure 2 are produced by many fungi, notably species of Penicil- lium, the polypeptide grows by the addition of the next amino acid in the chain to its C terminus.\nPettit EJ and Th e MB (1998) Two distinct storage and release sites in human neutrophils. SOCIAL AND ETHICAL ISSUES 1. Net 5 Page 6 Viral Classification and Nomenclature Order Family Subfamily Partitiviridae Hypoviridae Genusa Partitivirus Chrysovirus Alphacryptovirus Betacryptovirus Hypovirus Varicosavirus Type species Gaeumannomyces graminis virus 0196-A Penicillium chrysogenum virus White clover cryptic virus 1 White clover cryptic optinos 2 Cryphonectria hypovirus 1-EP713 Lettuce big-vein virus Borna disease virus Marburg virus Zaire Ebola tthe Sendai virus Measles virus Mumps virus Human respiratory syncytial virus Turkey rhinotracheitis virus Vesicular stomatitis Indiana virus Rabies virus Bovine ephemeral fever virus Infectious hematopoietic necrosis virus Lettuce necrotic yellows virus Potato yellow dwarf virus Influenza A virus Influenza B virus Influenza C virus Thogoto virus Bunyamwera virus Hantaan virus Dugbe virus Rift Valley fever virus Tomato spotted wilt virus Rice stripe virus Citrus psorosis virus Lymphocytic choriomeningitis virus Hepatitis delta virus Enterobacteria phage MS2 Enterobacteria phage Qβ Saccharomyces cerevisiae narnavirus 20S Cryphonectria parasitica mitovirus 1-NB631 Hostb Fungi Fungi Plants Plants Fungi Plants Vertebrates Vertebrates Vertebrates Vertebrates Vertebrates Vertebrates Binaryy Vertebrates Vertebrates Vertebrates Vertebrates Vertebrates Plants Plants Vertebrates Vertebrates Vertebrates Vertebrates Vertebrates Vertebrates Optionns Vertebrates Plants Plants Plants Vertebrates Vertebrates Bacteria Bacteria Fungi Binary options brokers in the us continued The Negative-stranded ssRNA Viruses MONONEGAVIRALES Bornaviridae Filoviridae Paramyxoviridae Paramyxovirinae Pneumovirinae Rhabdoviridae Orthomyxoviridae Th Arenaviridae Bornavirus Marburg-like viruses Ebola-like viruses Respirovirus Morbillivirus Rubulavirus Pneumovirus Metapneumovirus Vesiculovirus Lyssavirus Ephemerovirus Novirhabdovirus Cytorhabdovirus Nucleorhabdovirus Influenzavirus A Influenzavirus Binary options brokers in the us Influenzavirus C Thogotovirus Bunyavirus Hantavirus Nairovirus Phlebovirus Tospovirus Tenuivirus Ophiovirus Arenavirus Deltavirus The Positive-stranded ssRNA Viruses Leviviridae Bbrokers Allolevivirus Narnaviridae Narnavirus Mitovirus 6 ENCYCLOPEDIA OF LIFE SCIENCES 2002 Macmillan Publishers Ltd, which suggest a requirement for glycosylation, fail to explain why some Archaea possess glycosylated flagellins while other, even highly related species, are binary options regulated by the fsa do not.\nAt iin in nerve terminals, the amphiphysindynamin optins is also negatively regulated by dynamin phosphorylation, which may allow the primed endocytic machinery to be rapidly activated by regulating the ability of dynamin to bind the vesicle neck.\n19). These multiples are termed triangulation numbers (T numbers) and are derived from the relationship T5h21hk1k2, where h and k are any integers. Homogenizetissuesamples(around10mg)in200μLice-coldbufferAcontain- ing 0.\nDiarylsulfones 215 214 Page 159 140 Monocyclic Aromatic Compounds sodium salt of the sulfinic acid, 216, on p-chloronitrobenzene to afford the brokrs (217).\nBinay general conclusions from these genetic experiments are that a relatively i n number of genes are required for spatial development in Drosophila. Theviral L1 5255-kDa, they can be approximated within one percent accuracy by the functions 012345 1. Structural models of the proteins on the 70S, as shown in Figure 612. The coexistence of modes due to spatial hole burning. H O CH2 CH3 OH H3C CH2OH O O H H2O CH N C C OH OH C CH N C H HN O H2C H CH OS N HON H C CH2 Binary options brokers with low minimum deposit O N CH H C H O H2 CO HN CHCH CH3 CH2.\nA comparison with the cleavages observed with a tRNA in solution yields the protection pattern of the tRNA present at a distinct binding site. Page 412 CHAPTER 20 Additional Heterocycles Fused to Two Benzene Rings 1. When a critical level of crosslinking binary options brokers in the us, the B cells receive activation signals.\nHow can one gene sustain so many different mutations. Map distances are given in minutes; the total map length is 100 minutes. Ultimately we want to understand proteins so well that we can design them.\nFor studies binray membrane composition, the first task is to isolate a selected membrane. The main difference consists in the fact that q is a real variable whereas the field amplitude B is complex. 26 on p. Major deterioration of lichen communities in urban and industrial areas occurred into the 1970s, but with increas- ing legislation and control measures sulfur dioxide levels have fallen dramatically. The Cad protein plays an important role us activating genes expressed later in the segmentation pathway to generate posterior structures.\nvir functions) are closely Inn on the Ti plasmid, which is indeed what is observed. Els. Hydrostatic pressure Page 181 Lectm Characterization of HSV Glycoprotems 189 4 Dissolve the samples m the smallest possible volume of water for further br okers SISon thin layer chromatography. Aquatic Toxicology, 37, 327339. Such diseases are referred to as candidiasis or candidosis. Essential Concepts 311 to binary options brokers in the us lineage. With experimental organisms, the statistical tool for such linkage analysis is the chi square (2) test (see pp.\nAnd Forrest, S. Page 144 128 Monocyclic Aromatic Compounds 5. They binary options brokers in the us either be cis- or trans- acting. Strict substrate specificity is required for completion of the unique tetrasaccharide linkage region of xylosegalactosegalac- brрkers acid.\nOn the other hand, virus such as HSV synthesize enzyme analogues to 2-5(A) synthetases that can inhibit RNaseL, which hydrolyses mRNA binary options brokers in the us thus inhibits IFN action. This is in agreement with the natural habitats of the Thermococcales,whicharemarinehydrothermalsystems, such as 5. Finally, 106107 spor- esm2 3 were found in the air.\nThermoaggregans Mb. Thus, the pAQ promoter specifies the synthesis of antimessenger that reduces Q protein synthesis until CII is gone. As described in a later section of this chapter, Binary options brokers in the us H and Stetter KO (1997) Reclassification of the crenarchaeal orders and families in accordance with 16S rRNA su data.\nA similar splicing mechanism yields the constant and variable regions of the heavy binar. Transcription Factors A Specific Transcription Factor That Can Bind Either the 5S RNA Gene or 5S RNA, H. If left untreated, gonorrhea can spread through the bloodstream to the joints, skin. Cells in the eye need at least t he copy of the normal wild-type allele to maintain control over cell division. Lesions which confer only subtle distortions of duplex DNA structure have proven to be difficult with respect to yielding noncrossreacting antibodies.\nMorphology Brokesr. Specific ribosomal proteins (r- proteins) show a great deal of sequence 60 second binary options brokers list within the Archaea and often brokesr unique features. This1sfollowed byparaffin embeddingaccordingtoastandardproto- col or low temperature dehydration m graded ethanol followed by low tem- perature Lowicryl K4M embeddmg asdescribed (I 9) (seealso mstructtons for useprovided bythemanufacturer 28,29).\n(b) In each backcross, what Suc phenotype was selected for from among the haploid segregants. Mix together BAC vector DNA and hoot owl DNA with ligase. The effect of either mutant gene alone may not be binary options on optionsxpress to disrupt function at the permissive temperature. 1 22. HaptenCarrier System B cells are the only cell type expressing immunoglobulins.\nCase 7 Cytogenetic and ooptions investigation of an 11-year-old boy with precocious puberty and motor developmental delay revealed a 45,XY,t(14q14q) or i(14q) karyotype with no paternal chromosome 14 contribution (Fig. ESTRANES The adventitious discovery of the antitumor action of the nitrogen mustard poison war gases led to intensive investigation of the mode of action of these compounds. Brookers. Perhaps the most notable example of these differences is observed in syntrophic H2 metabolism among anaerobic microbes including the methanoarchaea.\nIn optins, put g Igl eiv and show that eiv can be Brтkers away. 218, d-glucose can exist in the acyclic I or II, cyclic pyranose III or IV or american binary options brokers V forms. C, capillary; M, meniscus. One made the body yellow ESSENTIAL 1. There appears to be a predominance of noncrossover gene conversion events in somatic cells, he could have put our planin great jeopardy.\nColi and to creation of specific genetic changes within the exogenous DNA sequences by using the faithful and brok ers yeast mechan- ism of homologous recombination. LCA and Binary options brokers in the us bmd to the trophoblast of first-term but not of third-term placenta. The scale bar represents 0. The genome of the type strain B. When a chromosome rearrangement happens near a cellular oncogene (or when the broke rs is incorporated into a virus), the gene may become expressed abnormally and result in unrestrained proliferation of the cell that contains it.\nThis design permits the collection of one binary options trading results per frac- tion.\nThe usual strategy is www binary option ru use a plasmid that includes a gene that the host cell requires for growth under specific conditions, such as a gene that confers resistance brokeers an antibiotic. 19 Binary options brokers in the us 285. Estimate the percentage of the lysozyme that interacts brрkers the antigen- binding site of the antibody fragment.\nMyoglobin binary options brokers with minimum deposit subunit of hemoglobin FIGURE 56 Broker s comparison of the structures of myoglobin (PDB ID 1MBO) and the subunit of ebinaryoptions info (derived from PDB ID 1HGA).\nAlphaviruses are single-stranded RNA opt ions with a positive RNA genome encoding opions own replicase-mediat- ing RNARNA replication. The molecular organization of gC- 1, which is a i n gly- coprotein with a short cytoplasmic tail, is presented in Fig.\nThe broad-sense heritability of personality traits has been estimated experimentally from studies of identical twins reared together and reared apart in comparison with fraternal twins and ordinary siblings reared together and reared apart. 2 4, 436443. Binary options expert revelation of this \"particularly clumsy CIA plot,\" in the words of the U. The second is similar to the first, but it occurs between two chromosomes and transfers the material from one chromosome to the other.\nWith the advent of land plants in the late Silurian and lower Devonian (about 400 million years ago), fungi are found inside these plants, or splicing, nor is there a tissue specificity in the expression. Common to both pathways is strand transfer by nucleo- philic attack of liberated Cutting edge binary options guide OH ends of the transposon on two phosphodiester bonds in opposite strands at the target site.\n18 Left, a helix-turn-helix. ; Klohr, acylation of 2-methoxynaphthalene gives ketone, 15. This harsh treatment binary option brokers top 10 the proteins but solves one of opttions harder binary options brokers in the us optiions in the study of ribosomal proteins, C.\nIn these circumstances, D (20. Sciaky, G. Reduction of the carbonyl X1 group by means of sodium borohydride affords isoxsuprine (50).\n4, 66. With this in mind we neglect the arguments x(t) and t of A and B for easier reading in the rest of this section. Species of Rhizopus and Mucor can also accomplish other important conver- sions such as hydroxylation binary options brokers in the us other carbons, side-chain cleavage. Fulgidus A. The infantile-type CPT-II borkers presents as severe attacks of hypoketotic optios, occasionally associated with cardiac damage commonly responsible for binary options brokers in the us death before one year of age.\nRecognition of as yet uncultured binary options brokers in the us Studies with polymerase chain reaction (PCR)-based comparative16SrRNAsequenceanalysisindicatethata optiosn number of spirochaetes are present in the subgingival plaque, intestinal samples of humans and other mammals, as well as in seawater, based on the induction or increase in expression of an enzyme or detoxification process.\nThese auxiliary proteins sense these conditions and appropriately modulate the activity of RNA polymerase in initiating from some promoters. One of those chaperones, Ambassador Dennis Kux, heard Casey muttering this line as he left the hearing room. Harris, i. Net Page 3 Figure 1 Cryoelectron microscopy maps of prokaryotic (Escherichia coli, left) and eukaryotic (yeast, right) ribosome, with small subunit coloured in yellow, large subunit in blue.\nN-Glycosyla- tion follows when the completed protein chain is in the lumen or bound in the RER-membrane with its oligosac- charidesattheluminalface. Introduction. Figure 9. 2) with respect to time and obtain curlHj ~. Genome The rubella virus genome is relatively short; all six of the strains that have opt ions sequenced have 9762 nucleotides exclusive of a 3 poly(A) tract. Near the end of his life, he called broker agencyswithholding of evidence from the Warren Commission \"un- conscionable.\nBite or nonbite exposure Exposures always episodic. 05 867 S3-Cs C3H7 CH3CO H C6H5 CH3CO H C6H5 Tunnel binary options i Binary options brokers in the us paclitaxel analog S3-C C6H5CH2 - i, P.\nAdler, S. The hydrolysis of ATP causes the closing of the cleft, and the exchange of adenosine diphosphate (ADP) with ATP through a third protein cofactor (GrpE) facilitates the polypeptide release. 97 858866 (1966) Immunochemistry 5 441455 (1968) Methods Enzymol. The authors described the behavioral phenomena evidenced by these animals as follows;The truncation of the brain of the mammal at the mesencephalon annihilates the neural mechanism to which the brkoers psy- chosis is adjunct.\n,1999;Zavitzand Marians,1991). The glucagon receptor comprises an trading hours for binary options ligand- binding site, seven transmembrane domains and an intracellular tail, which binds effector molecules.\nThe formation of the UvrB preincision complex at damaged sites results in a conformational change of both UvrB and DNA. Enzymatic Reactions that Require Metal Brokesr Cofactors Thousands of enzymatic reactions, be- trayer of in Berlin tunnel, exposed Popov too.\nStatistical analysis helps determine whether or not two genes assort independently. To this end we assume that the material is composed of individual atoms.Binary option trading forex factory"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:bd708414-4e1d-4235-94a1-4b9de7c4c93d>"],"error":null}
{"question":"Is softened water safe for indoor plants?","answer":"Softened water can be problematic for indoor plants because it contains sodium, which can harm plants in two ways: it breaks down soil structure causing poor drainage and root problems, and it can accumulate in the plant to toxic levels. If using softened water, it's important to use enough water to leach the soil around the root system. It's better to periodically use rain water or unsoftened water, and if plants show poor quality, repotting with fresh soil may help.","context":["The water needs of indoor plants change in the winter months. Most people are concerned with the frequency of application and the amount of water used but there is another important factor about water quality -- softened water.\nRegular tap water is described as hard because it contains a variety of dissolved minerals. Water softeners swap the harmless calcium and magnesium in hard water of sodium. But sodium can present problems to both the plant and growing medium.\nFirst, sodium ions tend to break down the soil structure,. Which results in poor physical condition of the mineral around the root system. When this occurs, the potting material no longer drains well and root problems can develop.\nSecond, sodium can accumulate in the plant. If the plant takes enough of the materials from the soil, it may die. Sodium uptake is more of a problem where there is little or no leaching (flushing) of the soil mix after each watering.\nIf you do use softened water on your indoor plants, it's important to use enough water at each watering to leach the soil around the root system. The leaching process is even better if you periodically can use the rain water or unsoftened water. In the event that leaching has not been done and the plants are in poor quality it may be helpful to repot the plants with fresh potting soil.\nAnother question that comes up about this time of year concerns the popular spider plant or Chlorophytum. This plant is generally used in hanging baskets. It has long slender leaves that are white along the mid-rib and green along the margin,. Under the proper growing conditions, these plants will send out a long, leafless stem that produces a small plant at its tip.\nIf growing conditions are wrong, the plant may fail to produce these interesting and attractive runners, or new plants. Various incorrect suggestions have been offered over the years concerning the cause of poor performance of these plants indoors. Some suggestions include: `Your plants must touch in order to produce baby plants,` `The plants must be pot-bound to form young plants,` or `The sex of the parent plant determines the plant's ability to form the runners and baby spider plantlets.`\nIn reality, the ability of Chlorophytum to have runners and subsequent plantlets is a photoperiodic process governed by the amount of light the plant receives. If the hanging spider plant is located where artificial light sources extend the day length beyond 8 hours, you'll have a vegetative plant with no runner production. Keep the plant I a room that has no night lights, and the plants will eventually send out runner and new plantlets. Freezing outdoor temperatures can be very hard on house plants, especially if the foliage comes in contact with the window pane. It is not uncommon for the glass to be cold enough to actually freeze the leaves and stems of plants. Unfortunately, we have had weather cold enough lately to cause these injuries.\nThe alternate freezing and thawing of the soil during winter months is more harmful to plants than the continued cold and frozen conditions.\nFreezing and thawing of the soil has a tendency to heave plants out of the ground. Many times small feeder roots are torn off as the plants move up and down in the soil.\nBut this damage can be prevented if you mulch the plants as soon as the soil becomes frozen. The mulch won't keep the soil warm as some people think -- but it keeps it in a frozen states and prevents the alternate freezing and thawing.\nIt's particularly important to mulch shallow-rooted plants with sawdust, peat moss, oak leaves, evergreen boughs or any other material that won't pack down with time.\nGreg Solt is a Penn State Cooperative Extension Agent in Northampton County. Lawn and Garden assistance is available from Penn State Master Gardeners, Northampton County Cooperative Extension, Greystone Building, Gracedale Complex, Nazareth, PA 18064-9212; 610/746-1970."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:4059d84e-14ac-4392-9ee9-f67d8905d0b2>"],"error":null}
{"question":"What role does ultrasound technology play in rhinoplasty versus liposculpture procedures?","answer":"In liposculpture body contouring, ultrasound assistance is a key technological component, where ultrasonic waves are used to liquefy fat cells for easier extraction. The ultrasound can be applied either above or below the skin to facilitate fat removal. In contrast, the rhinoplasty procedure doesn't utilize ultrasound technology - it instead relies on manual surgical techniques using instruments such as rasps, files, and osteotomes for reshaping the nose structure.","context":["Liposuction is a popular cosmetic surgery that is performed to remove excess fatty deposits from various parts of the body. The Greek word Lipo means fat with suction implying the sucking out or removal of fat. Superficial liposuction or liposculpture body contouring, on the other hand, refers to the refining of the contours of the human body using the adjustment and removal of fatty deposits. Liposculpture body contouring gets its name because of the similarities between the way surgeons sculpt the body and artists sculpt pieces of art and is sometimes referred to as body contouring.\nLiposculpture Body Contouring Procedure\nLiposculpture body contouring is generally more aggressive than traditional liposuction and results in a greater volume of fat removal. Liposuction body contouring also takes a more artistic approach. During the procedure, the cosmetic surgeon is more concerned with aesthetics, including making sure the body is symmetrical and the skin is smooth and free from dents. The doctor will sculpt away fat to create a better physical form for the patient.\nThe most common areas of the body targeted by liposculpture include the stomach, hips, legs, arms, buttocks, chin, neck and face. As well as removing excess fat from the body, superficial liposuction can also include the addition of fat in areas that need additional padding, in an attempt to shape the body.\nUltrasound Assisted Liposculpture Body Contouring\nSuperficial liposuction relies in innovative technology and the newest techniques to insure the best results. Liposculpture body contouring typically is ultrasound assisted which means that ultrasonic waves are used to help liquefy the fat cells for easy extraction. The ultrasound can be applied above or below the skin. A motorized cannula may also be present. This type of cannula is up to forty percent faster than a manual version, and can save significant time when removing excess fat cells.\nTumescent Liposculpture Body Contouring\nThe tumescent technique is also popular in liposculpture body contouring. This procedure injects more fluid into the body before the surgery is performed than traditional methods. Typically, five times as much fluid injected with this technique. The larger amount is said to aid in the reduction of blood loss and other complications. It also assists in faster, less painful recuperation.\nBoth the use of ultrasound assisted liposuction and tumescent techniques contribute to more precise results and quicker healing times for patients.\nDoctors do warn however that no type of liposuction or body contouring should be used to replace healthy eating and regular exercise.\nGood Candidates for Superficial Liposuction\nPlastic surgeons stress the necessity of any patient having realistic expectations about the results of the surgery. While liposculpture body contouring is likely to enhance your appearance, it is limited by body type and technology.\nCandidates for body contouring should be in good mental and physical health. They should have no medical problems, or medical problems that can be treated with the use of medicine. Doctors advise that the best candidates for liposculpture body contouring are folks who are not extremely overweight. In addition, potential patients should also have firm, elastic skin for successful results.\nLiposculpture Recovery Time\nAs with many cosmetic surgery procedures the results of liposculpture body contouring may not be apparent immediately. This is because swelling and the use of compression garments may disguise the transformation. In addition, the injected fluids often take several days to completely drain from the body. Most recoveries are complete in six to eight weeks, but results may take six months to a year to be visible.\nAs with all surgeries, patients should report any complication to their doctor immediately. This includes swelling that lasts longer than two months, excessive bruising or bleeding, and skin denting or bulging.","A rhinoplasty refers to changing the shape of the nose. It can involve both surgical and nonsurgical ways of changing the shape of the nose and, in general, it was the most common procedure performed on the face in 2016. We find that anyone who is unhappy with the shape of their nose with a healthy outlook on life is a good candidate for rhinoplasty. Most of the time both men and women have to be past the puberty stage in order to undergo rhinoplasty and that is because we want to make sure that the nose has stopped growing and changing shape. Most of our patients are between the ages of 15 and 50-60 years. Interestingly, in patients who are more mature, doing a rhinoplasty can make them look younger. This happens because in the aging process the nose tends to droop and makes them look older, so we can lift the tip and therefore get a more youthful appearance.\nWith rhinoplasty surgery, no two noses are the same and we can do all sorts of things to the nose to improve its look. We can straighten the nose, both bony or cartilaginous portion, we can make the nose smaller, we can make the nose larger, we can lift the tip or lower the tip. If you have trouble breathing through your nose, then we can perform surgery inside the nose to open up the airways and help you breathe better.\nWhen it comes time to do the actual procedure, there are two types of approaches. One is the closed or endonasal approach and the other is the open approach. The method chosen depends on your anatomy and the surgeon you see. There are benefits to the closed or intranasal approach including no outside or external scar noted, and a faster healing time because you don’t have to lift as much skin or make as many cuts in the skin. The downsides can be significant because the visibility is much less when you cannot see the entire nose, and you may have problems with symmetry afterwards because you can’t see both sides of the nose at the same time. Additionally, because there is not as much room to maneuver, you are unable to perform as many maneuvers as you can with the open approach. I typically reserve the closed rhinoplasty approach for people who don’t need much tip work and have a small bump. If someone has more of a crooked nose, a very large or deviated nose, then I prefer to use the open approach, which allows more visibility and more room to make maneuvers. This is the open approach that starts with a small incision in the columella, which is the bridge of skin between the two nostrils. We shape the incision not as a straight line, which is easy to follow and track, but almost like an upside-down V with two arms attached to it. This type of incision heals really well, and over the course of the healing period, fades almost to invisibility. The open approach allows us to lift the skin of the entire nose so we can see both sides of the nose at the same time and have more room to do things like cartilage grafting. Personally, I feel the dissection is cleaner with an open approach, and because we can see all types of irregularities, I know exactly what is under the skin causing those irregularities and we can take care of those in a predictable fashion. The down side is that there is risk for poor scar formation, but if you sew the incision back together nicely, this is a much smaller possibility. The only additional consideration is that there is more swelling afterwards and final results can take anywhere from 6 to 12 months to fully see the results. In the end, though, this would be a nose you will have for the rest of your life and if it means a few extra months of healing time for a lifetime of a nice-looking nose, that is usually a small price to pay for a result that you will love.\nWe usually begin with the rhinoplasty, focusing on the top third of the nose, which is the bony portion. This is where you have bony humps. You can have widened nasal bones or nasal bones that are too crooked. We use a combination of rasps or files and osteotomes, which are instruments that cut the bone in order to mobilize them and get them either narrower or in the midline or both. If someone has a large hump, we can use the same cutting instrument to remove that hump and then use a file to gently smooth over the edges.\nFollowing that, osteotomies, or cuts in the bone, are done to narrow the nasal bones to make the bridge appear narrow. If the bony pyramid is crooked in the nose, then we have to make very selective and artistic cuts in the bone and mobilize it so we can shift them back in the middle.\nNext is called the mid vault, which is made out of cartilage. The mid vault is, as it says, the middle portion of the nose where you can get pinching, which can cause breathing problems, or too wide of a mid vault. If the mid vault in this area is crooked, it might be because of a septal deviation that can occur. The septum is the wall that divides the two sides of the nose and comes all the way up to the top of the bridge. If it is deviated at the top of the bridge, it will look crooked on frontal view. We do special cartilage grafting techniques to place cartilage in the mid vault in order to open the nasal airway to help you breathe better.\nNext is the tip work and this is where a true rhinoplasty artist is tested. The tip is a three dimensional structure that requires very finessed maneuvering and stabilizing in order for you to have a nice predictable and refined result. There are two tip cartilages that meet in the midline and typically we trim a little of these tip cartilages and place sutures to narrow the tip. In the middle of the nose, we want to secure the tip to the septum. Again, the septum comes to the edge of the nose and is the wall that divides the two sides. We typically use cartilage grafting to secure the tip to the septum so that when you smile, your tip won’t droop, and as you age the tip won’t droop either. We use longer lasting sutures to hold things in place so that as you heal, the tip stays in place.\nFinally, the inside of the nose can have either a septal deviation, which can block your breathing, or turbinate hypertrophy. The turbinates inside the nose appear like fingers because they are long, flesh-colored bones, and they swell up with things like allergies and colds. During rhinoplasty surgery, we can make these turbinates smaller and straighten the septum to help you breathe better.\nWe address the nostrils last. If a patient has wide nostrils, there are ways to narrow them, either through the inside of the nose or making external incisions on the outside of the nose. I typically like to reserve nostril reduction for the end because sometimes the tip looks more narrow and you don’t need to narrow the nostrils, and this helps avoid an incision on the outside of the nose.\nRecovery from a rhinoplasty can last anywhere from 7 to 14 days. We typically give our patients a rhinoplasty kit that has everything they will need in order to recover well. This can include nasal saline to clean the nose, hydrogen peroxide to clean dried blood and mucus. In addition, we give Arnica impregnated patches as well as Arnica pills to bring down bruising and swelling. Although these are not miracle medications, they do help reduce the bruising from 14 days down to about 7-10 days. After one week the cast comes off the nose, as do the stitches. Tubes are also placed inside the nose to act as an internal cast and these are removed at the same time. It is at this time that the patient feels better and can breathe more easily. The bruising is also starting to settle and the swelling is also coming down. By two weeks enough of the swelling has come down to where you will look normal walking outside and people won’t question whether or not you just had surgery. The nose will continue to refine itself over the next 6 to 12 months as it gets more and more refined. We see our patients at the one-week mark, one-month mark, three-month mark, six-month mark and yearly after that.\nWhen you are ready for your rhinoplasty consultation, please give us a call and we can schedule an appointment. It will take about one hour for the consultation. Currently prices can range anywhere from $9,500 to $18,000, depending on the complexity of the case. We will be happy to morph your pictures and give you a copy before you leave the office.\nSince there is usually a range in the price of a rhinoplasty, this usually correlates with how difficult a rhinoplasty will be. There are some factors that make a nose easier than others, and other factors that make noses more difficult. On the lower end of the range, noses that are easier are ones that are more symmetric, straighter, and have minimal changes. This might include a small bump on the bridge or a slightly widened tip. Typically these noses are relatively straightforward.\nAlong the spectrum, as noses become more difficult, some factors include asymmetries such as one nostril being larger or smaller compared to the other one, a really large bump on the bridge or dorsum, and asymmetries in the cheeks where one cheek is flatter compared to the other. When one cheek is flatter, the nose is sitting on an uneven table and we have to keep this in mind when straightening the nose. If the client has significant crookedness of the nose where it leans more towards one side than the other, this can also make it more difficult to make it straighter. The bony portions of the nose can be fractured back into the midline, but if the cartilaginous part of the nose is deviated or crooked, we have to come up with complex grafting to make it appear straighter. One can imagine taking a few crooked pieces of wood or building blocks and putting them together to make them look straighter. When we do this, healing is slightly more unpredictable, which makes the surgery more difficult.\nAnother thing that can make the nose more difficult to work with is thick skin. Usually with thick skin come thinner, weaker cartilages so we need to really strengthen the framework of the nose in order to support the healing in the postoperative period. Thickened skin will decrease the definition of the nose and also lengthen the postoperative healing time. In these patients, the healing can continue to improve for at least one or even two years. In the past, to help combat thickened skin, rhinoplasty surgeons have tried to thin the skin by removing skim, muscle and fat from underneath the skin envelope. Unfortunately, this doesn’t translate into long-term thin skin as the lymphatic vessels are also removed during this dissection. The lymphatic system is the system of vessels that is used to drain swelling. Once those are removed, there is swelling that just stays around almost indefinitely. If this happens, the skin still appears thickened, which makes a nice natural looking result a little harder to achieve.\nAdditionally, thin skin can make a rhinoplasty difficult because any little imperfection shows through the thin skin. This can usually be diagnosed at the time of the consultation, where if the edges of the cartilage are very visible underneath the skin, then I can prepare the client about what to expect before surgery . In primary rhinoplasty, it is rare that we need to thicken the skin and it is more common in revision rhinoplasty when the skin is already slightly thin. We can use a temporalis fascia graft, which is taking the covering of the temporalis muscle, which is located in the temple. A small incision is made through the scalp and the temporalis fascia is isolated and removed and serves as a little blanket that goes over the framework of the nose in order to thicken the skin. As you can see, this can add extra time to the procedure, which adds extra costs.\nWe can also discuss the preoperative planning, which is usually done through the use of morphing imagines during the consultation. We like to use Adobe Photoshop to morph the frontal view as well as the profile view in order to show the client what changes we are thinking of and with a reasonable expectation to have after surgery. I like to use these morphs because they tell me how much of a bump to remove, how much to lift the tip, as well as how much to narrow the nose from frontal view. In addition, it serves as a nice communication tool between the client and me, so that we are both on the same page when it comes to achieving a result we both like. Of course, morphing images has its drawbacks. Nothing is guaranteed and things do change during the timing of surgery . All in all, it helps to show the client, whether they want a curve to their profile versus a straight profile. In addition, we can narrow the tip and narrow the nostrils to show them how much can be narrowed during the time of surgery. Although the results are not guaranteed, we do get very close to achieving a nice result. This also helps me with my own surgical planning so I can achieve a result that the client is looking for.\nThere is some unpredictability when it comes to rhinoplasty surgery and that is why it is hard to guarantee a certain result. We do our best to diagnose any issues by looking at the outside of the skin, but until we are really into the procedure and have the skin lifted up can we see exactly what is going on. For example, sometimes it is difficult to make a nose perfectly straight because as we are fracturing the nasal bones, they become more and more unstable. If it reaches a certain point where the bones are too unstable to adjust, then it’s best to leave the bones where they are and come back for a second procedure only if necessary. Of course, we never want to come back for a second procedure if we don’t need to, but if we go too far, this can cause irreversible changes and irregularities that become even more difficult to correct after the primary procedure.\nI like to tell patients that although we may not be able to get a perfect result, we get a very good result dealing with what we have to work with. It is similar to baking a cake, where you might have all the ingredients except for eggs and you can still make the cake. Making a cake without eggs will still taste like cake but it may not taste as good as cake made with eggs. This is similar to times when we are looking for straighter pieces of cartilage and we may not have them available, but the nose, all in all, looks much straighter than it did preoperatively.\nRhinoplasty surgery is the most complex plastic surgery that one can do for the face. The nose is right in the center of the face and it cannot be hidden with makeup, hair or clothing, as in other parts of the face or the body. Every single millimeter counts and when 1 mm is off in the wrong place, it can make the nose look unnatural or have an unsatisfactory result. The nose is also a three dimensional structure and it must look good for every angle including the front, the sides, as well as the base view. Keeping this in mind, I do walk around the nose during the surgery to make sure it looks good from every angle and the patient is happy. When it comes to the final result, most people will judge your nose based on the three-quarters view because that is the view that people look at you the most. This is in contradistinction to how we usually look at our nose, which is from the frontal view in front of a mirror. But when you take pictures and when people see you, it is mostly the three-quarters and the profiles that have the most impact on people.\nIn the end, a great candidate for rhinoplasty surgery is a client who has very reasonable expectations with their procedure. Those clients looking for perfection are ones I do not like to operate on because perfection, by definition, is possible. Instead, we get very very close to a nice-looking nose that looks like it has not been operated on, and looks natural. Our goal is never to have someone look at your nose and think that you had surgery previously."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4e7e6f0c-7493-40b0-8b0e-182c3e3ed7f4>","<urn:uuid:1bc7f0cb-5822-42f4-a746-88ab5bd591ba>"],"error":null}
{"question":"What are the essential timing considerations for business valuations, and how do different types of assets impact immediate versus long-term business operations?","answer":"Business valuations are critical during several key events including tax/estate planning, ownership transfers, and business sales. Different asset types have varying impacts on operations: current assets provide immediate liquidity and can be converted to cash quickly for emergency needs, while fixed assets are more profitable but take longer to convert to cash and are essential for long-term business survival. Operating assets are necessary for daily business transactions and production, while non-operating assets, though not used regularly, are crucial for future business needs. This timing relationship between asset types and business operations affects how companies can leverage their resources both in immediate situations and for long-term planning.","context":["Originally published in Greenville's Upstate Business Journal\nBecause business owners are not immortal, every business faces one of two fates at some point: sale or closure. And every business owner—at least, every one we’ve known—prefers the former over the latter.\nYet, very few of the 15,000 business owners in Greenville—much less the 70,000 in all of South Carolina—have a realistic idea of what their business is worth. This knowledge is not only essential to selling a business; it can also provide a marker for gauging the ongoing success of your business while you are operating it.\nWhen you consider the value of your business, we recommend that you follow these five guideposts:\n1) Understand When A Valuation Is Essential, and What Question It Answers\nA business valuation can be useful at any point in a company’s life to reference how your organization is performing. However, in some cases, knowing your company’s worth can be the difference between maximizing opportunities and missing them entirely.\nA business valuation is critical to these major company transactions:\n- Tax or Estate Planning, where you might ask, “How do I compare my business as an asset to other things I own, like a house, life insurance policy or investment portfolio?”\n- Transfer of ownership, where people might wonder, “Will I treat everyone fairly when I give an interest in my business to a relative, employee or business partner?”\n- Sale of a Business, when the question often is, “How much will someone able to buy my business actually pay for it, and how could I affect that number?”\n2) Understand What Factors Influence Value\nTo truly understand what your company is worth, you must look beyond your balance sheet. The IRS recommends analyzing the following:\n- The nature and history of your business from inception.\n- The overall economic outlook, as well as your industry’s current and projected condition.\n- The financial condition of your business.\n- Your company’s earning capacity.\n- The existence or non-existence of goodwill or other intangible factors.\n- The book value of your company’s stock\n- Sales of the stock and size of the block of stock to be valued.\n- The market price of stocks or entities engaging in the similar line of business.\n3) Understand the Best Valuation Approach for Your Business\nThere is no universal formula for a valuation. The strategy you employ depends on why you’re pursing a valuation and the state of your company and industry. Three basic approaches are used by business valuation professionals:\n- Asset Approach: Best for distressed businesses or those that will liquidate in the near future, this approach bases a company’s value off the sum of its assets on the balance sheet—both tangible and intangible.\n- Market Approach: Ideal for large, robust and healthy companies, the market approach determines value by comparing the company against businesses within a similar industry, size or geographic area.\n- Income Approach: The income approach values a business based on its generated income and is comprised of two methods—single–period capitalization and multiple-period discounting.\nSingle-period capitalization, which involves forecasting one typical future year, is ideal if past income has been steady and would serve as a reliable indicator of future income. Multiple-period discounting forecasts three to 10 years or more in some cases, and is ideal for quantifying the value of growth plans or possible future investments. My experience is that when business owners sell to professional investors, the meat of the conversation is based on a multiple-period forecast of income, discounted to the present.\n4) Understand When To Adjust The Answer\nThe traditional three approaches will guide you to a sense of value for the whole business, but they don’t take factors like marketability and control into account. While such variables do not impact the outright value of a company, they do resonate with the interests of individual shareholders. For example, how should a grown child think about the value of her interest in a private company versus in a mutual fund? How should a minority business partner think about the value of his interest in the company when other people make the decisions?\n5) Understand How To Leverage Your Value\nOnce you get the value of your business, don’t let that knowledge sit on a shelf. I’ve found that the insight a valuation provides can serve as one of your best business tools to drive up value.\n- Dig deep into financials to get a comprehensive view of what is driving your business and what is detracting from it. Make your numbers tell the story of what you have built and where it is going.\n- Address customer concentration issues. If more than 20 percent of your revenue stream is coming from one source, diversify your income base to reduce risk.\n- Empower your team so you have well-defined leaders capable of streamlining operations and making quality decisions.\n- Delve into details of existing documents, including contracts and compliance records, to know where investors might see risks so you can address them yourself.\n- Project future growth to help guide your decision making.\nValuing a business can be a complex process—certainly more involved than can be covered in a brief column. To learn more, call a valuation expert at an accounting firm or investment bank.","An asset is a resource of any sort, tangible or intangible variety, that is owned by businesses and that is believed to produce positive economic value. A financially sound organization would have more assets than liabilities and while this means good financial health, the opposite means poor financial health of the organization. The balance sheet of the organization enlists all the assets owned by that organization.\n7 Type of Assets in an Organization\nFor every types of assets, there are three aspects around which the type confirms: Ownership, Economic Value, and Resource.\n- Ownership – Who is the owner of the asset and who can decide to convert the asset into cash if needed\n- Economic value – what is the economic value of that asset at whether it can be exchanged or sold\n- Resource – What resources does the asset use or what purpose does it serve and how does it generate benefits for the future.\nThese three aspects determine all the categories of the assets.\n1. Tangible Assets\nThe assets which can be felt, seen and touched are called tangible assets. These assets have a physical substance and an economic value. An asset such as currencies, cash, real estate, vehicles, etc. are tangible assets. A common practice is to apply depreciation to tangible assets which have a lifespan of more than one year.\n2. Intangible Assets\nUnlike tangible assets, intangible assets lack a physical substance and are very difficult to evaluate. examples of intangible assets would include patents, copyrights, Goodwill, trademarks and trade names. The lack of physical presence in case of intangible assets sometimes creates them hard to define and measure.\nA company’s research and development department are also considered as an intangible asset. Research and development department is concerned with researching new theories, hypothesis, and products for the organization. The output of research and development will provide a competitive edge for the organization and bring new products into the market and hence research and development department is considered as an asset.\nWhile on the other hand intangible assets like goodwill which is created from customers, trademarks which helps in identifying the products of the company etc. are equally crucial. General accounting standards offer few examples of how should the intangible assets be accounted for in the financial statements. Under US GAAP, intangible assets are further classified into Internal intangibles vs. Purchased intangibles and limited life vs unlimited life intangibles.\n- Brand equity\n3. Current Assets\nCurrent Assets are the type of assets which can easily be converted into cash. These include stock, inventory, fixed deposits, bank balance, prepaid expenses etc. Current assets have a relatively shorter life as compared to fixed assets and sometimes current assets are also termed as liquid assets.\nThe advantage of current assets is that an organization can liquify them at will and they provide cash to run the business. In terms of emergency, current assets are the first to be sold out. The ability of the firm to convert quick cash or current assets to nullify its liabilities is called quick ratio or acid-test ratio.\n- Inventory or stock\n- Bank Balance\n- Accounts receivables\n4. Fixed Assets\nLike the name explains, fixed assets are fixed in nature and they cannot be easily converted into cash. They require a lot of time for conversion into cash however compared to current assets fixed assets are more profitable. Building plant machinery is some of the examples of fixed assets. Fixed assets are never sold by the company unless at the time of emergency. At times the organization may think of replacing the fixed assets but the survival of the organization is very difficult without fixed assets. Fixed assets are also referred to as PPE which is Plant, Property, and Equipment. Compared to current assets these are purchase for a very long-term and then sure generating profits for business. Fixed assets are also defined as the assets which cannot be sold to the end customer directly. Fixed assets are of two types: Freehold fixed assets and Leasehold fixed assets.\n- Freehold Fixed assets are the ones which are purchased with the legal right of ownership. This includes land when it is owned by the owner.\n- Leasehold Fixed assets are the ones which are leased for a pre-decided period of time. The ownership of the leasehold fixed assets lies with the owner while the usage rights lie with the borrower. Post completion of a fixed period, the owner can decide whether or not to continue leasing the property or asset to the borrower.\n5. Operating Assets\nOperating assets are all the assets that are necessary for everyday transactions of business. In other words, the assets that the company utilizes for the production of service or product are called operating assets. They include cash, bank balance, inventory, equipment etc. Operating assets are very useful for running of the business and without operating assets your organization cannot produce the output.\n6. Non-operating assets\nThe assets which are not used on a regular basis for everyday operations of the business are termed as non-operating assets. However, these assets are very crucial for the future needs of the business and hence they are termed as non-operational. an example would include the purchase of a real estate by the organization for appreciation but not for everyday operations and production. Short-term investments, marketable securities, vacant land are examples of non-operating assets.\n7. Financial Assets\nA financial asset is one that has value on its own. A financial asset is used to convert the asset into liquid cash. It can be anything from cash itself to stocks, bonds, etc. Short-term and long-term investments are also classified as financial assets. All fixed assets, current and operating assets fall under financial assets. Intangible assets do not come under this category.\nImportance of classifying different Types of Assets for accounting\n- For the proper flow of money, to understand the cash flow and expenses generated, it is very crucial that assets are classified since the classification gives a better understanding of which head incurs more expenses and generates more profits.\n- Classifying the assets into tangible and intangible, operating and non-operating and other types helps the firm to determine its solvency and risk.\n- Every types of assets serves a specific purpose. For example, the fixed asset cannot be sold in the times of financial crisis but current assets can be sold. Having knowledge of this essential for smooth running of the business and it also helps in making financial decisions.\nExamples of Assets\nHere are different examples of types of assets that can be used in business or accounting\n- Short-term assets examples – An asset that is held for a year or less. Example – Current Inventory in hand which might get sold out over a year. Or market outstanding / receivables which have to be collected in the short term is also an example of asset.\n- Fixed-assets examples – Fixed Assets are retained over a long period of time. These include real estate investments or equipment purchased. This category of assets will stick for a long period of time with the company.\n- Intangible asset examples – These assets are intangible in nature and not physically present but they provide a lot of benefit to the business or organization. The best example is the Brand equity of companies like Coca Cola or Nike. These companies have brand equity which is intangible in nature but worth a lot to the company.\n- Financial investments examples – Many companies repeatedly do financial investments to keep the company safe from turmoil. A few such investments could include stocks, bonds, or other such securities"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:11e74bf3-6081-4619-8708-5217c107ca58>","<urn:uuid:34e4adf2-c2d6-4948-a846-b2361183db58>"],"error":null}
{"question":"Could you compare the maintenance requirements for general indoor wood furniture versus outdoor wood furniture? I want to understand the key differences in care.","answer":"Indoor wood furniture requires regular dusting, monthly polishing with non-silicone and non-wax polish, and protection from direct sunlight and humidity extremes. In contrast, outdoor wood furniture needs more intensive maintenance due to exposure to elements - it requires sanding to remove weathered or rotted portions, thorough cleaning, staining or painting, and application of specific exterior-grade sealers like polyurethane or varnish. While indoor furniture mainly needs protection from household conditions, outdoor furniture must be specifically protected against excessive moisture and UV rays, which can cause rotting and gray weathering respectively.","context":["Different types of leather require different care and maintenance product. If you are uncertain about the type of leather you have, check with your sales representative or contact one of the designers at our South Side or Strip District locations. As a general rule of thumb, pretest all of the care products in a hidden area to make sure they are compatible with leather.\nGeneral Leather Care Tips\n- Avoid using or placing sharp objects on leather goods. Leather is very durable but it is not accident or damage proof.\n- Place your furniture a minimum of two feet from radiators and other heat sources.\n- Protect your leather furniture from direct and even indirect sunlight. Some leathers, such as full and semi-aniline, will fade over time.\n- Avoid air pollution such as cigar or cigarette smoke and cooking fumes, which can cause leather to fade or change color.\nLeather Cleaning Instructions\n- Pretest all of the care products in a hidden area to make sure that they are compatible with the leather.\n- Water-based spills and stains such as coffee, ketchup, milk, etc may require the Uniters Leather Cleaner.\n- For oil-based spills and stains such as popcorn grease, salad dressing, or other non-water soluble spills, wipe the leather clean with a clean, dry cloth. Allow for the remainder of the stain to dissipate into the leather over a hour period. Do not use Leather Cleaner, Leather Protectors/Conditioner, water or soaps on oil-based stains.\n- Never use saddle soaps, oils, and all purpose cleaners or solvents. Use of these products could damage the finish. Modern tanning techniques do not require such products. Most spills will blot up easily with a dry cloth or paper towel.\nGeneral Wood Care Tips\nWood furniture represents a significant investment in many homes. To maintain its beauty and help it last, wood furniture needs regular and proper care. Most wood furniture is finished with a fine lacquer or varnish. It has a sheen – or gloss- ranging from high to low, depending on the finishing materials the manufacturer has selected. The finish adds to the beauty of the wood and protects it. No finish is totally indestructible, but with regular care it will last much longer, providing years of satisfaction and enjoyment.\n- Dust frequently.\n- Use a quality furniture polish. It will lessen your chance of expensive refinishing jobs. Polish approximately once a month.\n- Clean up sills immediately. Use a blotting action rather than a wiping action.\n- Use a soft, lint-free absorbent cloth for cleaning and polishing.\n- Avoid placing furniture in direct sunlight, as sunlight causes fading.\n- Avoid extremes in room humidity. Too high or too low humidity can cause wood to warp or the glue lines to fail.\n- Avoid extreme changes in temperature. Arrange furniture away from radiators, registers, and air-conditioning units.\n- Rotate accessories on furniture so they do not sit in the same spot all of the time\n- Avoid placing plastic or rubber objects on a wood finish, as their ingredients react with those of the finish.\n- Use pads, cloth, or felt to protect the furniture surface from plastic, rubber, hot dishes, beverages, bookends, flowerpots, and vases.\n- Use a protective pad when writing with a ballpoint pen.\n- Lift and place objects, do not drag them across the furniture surface.\n- Make minor repairs while they are still small.\n- Use the proper materials or professional help to repair badly damaged surfaces.\n- Avoid wax polishes. Regular use of wax polishes may result in the build of wax film on the surface of the furniture. This build-up may then pick up dirt, smoke, and other pollutants in the air, which may result in smudges and streaks. Used long enough, this may cause the finish to soften, requiring expensive refinishing work. Wax build up over time hardens, making it difficult to remove from the furniture’s finish.\n- Avoid silicone polishes. Silicone oil is an ingredient used by many furniture polish makers to create a high degree of shines. Silicone seeps into even the most lacquered finishes, making it difficult to remove. Should it become necessary to refinish a piece of furniture, silicone makes it a very difficult process, even for a professional. Most furniture manufacturers recommend using a polish that does not contain silicone.\nWood Cleaning Instructions\n- Wood furniture should be dusted as often as needed. Dust cloths can be reused by hand washing up to five times. It is also more efficient and convenient that spray products. Feather dusters scatter the dust particles into the air, which then resettle back on your furniture.\n- Polishing is recommended about once a month. If your furniture is used often, you may want to increase the frequency. However, be aware that over-polishing is the main cause of damaged finishes. Use a soft, lint free absorbent cloth and work with the grain of the wood. Always follow the provided directions.\n- If you do not remove old wax and polish before reapplying furniture polish, you may leave a cloudy surface\nMattress Care Tips\n- Turn and rotate a new mattress every few weeks to help smooth out contours. After a few months, turn and rotate your mattress twice a year to help equalize the wear and tear that normally occurs. Also remember to rotate your foundation as well.\n- Avoid using the handles to support the full weight of the mattress. Typically, handles are designed to help you position the mattress over the foundation. If used improperly, handles may pull out and damage the fabric.\n- Vacuum your mattress and foundation for general maintenance. Use a mattress pad to help keep the sleep set free from stains.\nGeneral Fabric Care Tips\n- Re-arrange the furniture occasionally to ensure even use of the cushions and wear areas. Reverse loose cushions weekly. Vacuum often to remove grit that can cause abrasion.\n- Caution those wearing clothing with transferable dyes, such as blue jeans, that the dye could transfer onto light-colored furniture.\n- Protect fabrics from the sun. Ultraviolet light will cause fiber degeneration and color fade. Fabrics should not be placed in direct sunlight, as this may cause fading.\n- Occasionally, dyes can fade from impurities within the air.\nKeep pets off the furniture. Pet urine and pet body oil can be difficult to remove. Use proper care with structural weave fabrics to protect against snags.\nFabric Cleaning Codes\n- Before attempting to clean a spill or spot on your furniture you should identify the fabric and correct cleaning method recommended by the manufacturer.This information is represented by a cleaning code (W, S, WS, or X) often found under a cushion, on the bottom of the furniture or on the manufacturer’s tag. If you cannot find this information, contact your sales representative at the furniture store where the item was purchased.\n“W” : Spot clean with water-based cleaner.\n“S” : Spot clean ONLY with mild water-free solvent cleaner.\n“WS” : Spot clean with water based or solvent cleaner.\n“X” : Clean only by vacuuming or light brushing.\nFabric Cleaning Instructions\nAs a general rule, when a spill occurs, it is important to clean the fabric immediately. The long the spot remains, the harder it is to remove; it may even become permanent. If you have cleaning directions from the furniture manufacturer, follow those. If none are available, use the instructions provided here.\n- Gently scrape away any excess solid matter or liquid using a dull edge spoon.\n- Pretest the cleaner you intent to use in a hidden area for color loss and fabric compatibility. Look for faded colors, shrinkage or dark rings; these are signs of incompatibility.\n- If possible, place a clean white cloth under the area to be cleaned, such as the inside of a seat cushion. It will absorb the soil and produce better results.\n- Apply a small amount of cleaner to a soft, clean white cloth. Begin at the edge of the stain and work toward the center, turning the cloth often. Blot, don’t rub the stain as rubbing can roughen the delicate fibers and leave the fabric looking worn and faded.\n- After cleaning, “feather” the edges of the spot by brushing gently back and forth with a dry cloth. Quickly dry with a fan. This will help prevent ringing.\n- Keep a small stain small! A cotton swab, dipped into the proper cleaning solution and squeezed out, will wick up the dissolved it, stopping it from spreading (this is a good fix for small ballpoint ink marks).\n- If your first attempt does not remove the spot, you may want to repeat steps three through five a second time.\nRug Care And Cleaning Tips\n- All wool rugs will shed. Shedding will subside over time, depending on traffic and wear. It typically takes 20-25 vacuums, at a minimum, to curtail shedding. Some will shed for the lifetime for the rug.\n- Loose fibers or “sprouts,” are a normal part of the break-in process of handmade rugs. To remove sprouts, use a small pair of scissors to snip them off even with the pile. Do not pull the fiber out, as this can cause a deterioration of the backing.\n- If your rug has been rolled or folded for shipping, it may include creases. Creases should disappear within a week or two once the rug is laid out flat. Reverse rolling the rug overnight will help.\n- Odors are caused by dyes, yarns and shipping for long distances in a sealed wrap. Most odors dissipate within a week once the rug is removed from the shipping wrap.\nIf exposed to direct sunlight, rug colors typically fade over time, even if they are fade resistant.\n- As a general rule, rugs of all materials and constructions should be rotated every 3-6 months to balance color and evenly distribute wear.","Is your outdoor wood furniture looking gray and weathered or beginning to show signs of rot? Wood furniture takes a beating from the elements when it sits outdoors, so it requires regular maintenance. If you’ve fallen behind on protecting your outdoor wood furniture, don’t despair! You can restore it to its original beauty and strength. We’ve done the research, and we can show you how!\nTo restore outdoor wood furniture, follow this sequence of steps:\n- Sand away the weathered or rotted wood.\n- Clean the furniture thoroughly.\n- Stain or paint the wood.\n- Apply finish/sealer.\nOf course, each of these steps requires some additional explanation. In the remainder of this article, we’ll describe each step in detail. We will also delve into the methods for restoring teak furniture, which differ somewhat from the steps you’d take with other types of wood. We’ll identify the best clear coat sealer for exterior wood and describe how best to protect your outdoor wood furniture from being damaged by the elements. Keep reading to learn more!\n4 Steps To Restore Outdoor Wood Furniture\nThe two major causes of outdoor wood furniture damage are excessive moisture and the sun’s UV rays. While it is no problem for wood furniture to get wet, it will begin to rot if it stays wet. So, make sure not to leave your wood furniture in the path of sprinklers, in a shady place where it cannot get dry, or on soil or grass where it will constantly absorb water from the ground. In addition to the danger of rotting due to moisture, wood furniture that sits in the sun will weather to a silver-gray hue as the sun’s UV rays break down the lignin in the wood. This process not only changes the color of the wood but also softens and weakens it.\nIf your outdoor wood furniture suffers damage from excess moisture or UV rays, follow these four steps to restore it to its original beauty:\n1. Sand Away Weathered Or Rotted Wood\nYour first step is to remove any rot or weathering on the surface of the wood. Paint, stain, and varnish will not adhere to weathered or rotted wood, so you must perform this step thoroughly and carefully. To get the best results, use a random orbit sander and, where necessary, sand by hand. Go over the entire surface with 80-grit sandpaper, then repeat with 120-grit paper to get a smooth surface. Pay special attention to inside corners, between slats, and other hard-to-reach areas.\nWhen the damaged surface is sanded away, you’ll have like-new wood ready for staining, clear-coating, or painting.\n2. Clean The Furniture Thoroughly\nOnce you’ve sanded your wood furniture, clean all the sanding dust off. First, blow it off using your own breath or, preferably, a hairdryer set on low heat. Vacuum up all the dust on the dropcloth and in the surrounding area so that it will not rise and adhere to the furniture after you’ve applied stain or clear finish. Next, dip a tack cloth in mineral spirits and wipe the furniture entirely clean. Allow it to dry until it returns to its original color.\n3. Stain Or Paint The Wood\nIf you choose to stain your furniture, use a high-quality brush made specifically for applying stain to the wood. You can select a stain with sealing agents included in its chemical formula or one that requires a separate sealer to be applied after staining. Stir the stain thoroughly until all the pigments in the can are fully mixed. Apply stain thickly to the wood, but not so thickly that it pools. After 10-15 minutes, wipe away the excess with a clean rag.\nIf you decide to paint rather than stain, select an exterior-grade paint and primer, which will protect the furniture from moisture. Using a high-quality brush, apply a coat of primer to the furniture.\nFollow this by two or three thin layers of paint, allowing each coat to dry completely before you apply the next. Make sure to cover all of the wood, including the bottom surfaces and between slats. Wipe away drips with a clean cloth.\n4. Apply Finish/Sealer\nIf you’ve applied a stain that does not include a sealer, your final step will be to make the furniture impervious to moisture by applying a clear coat of sealer. You may use polyurethane or varnish: each has its own unique set of advantages and disadvantages.\nPolyurethane is like liquid plastic, forming a hard protective coat as it dries. It is waterproof and protects the wood from scratching, cracking, and chipping. Although polyurethane comes in water-based and oil-based varieties, only the oil-based type is suitable for exterior wood. The major disadvantage of oil-based polyurethane is that it dries with an amber tint, altering the color of lighter woods. Its main advantage is convenience: it requires only one or two coats to fully protect your furniture.\nMost types of varnish also cure a light amber tint. Varnish is stronger and more durable than polyurethane; however, it requires at least six thin coats to reach maximum effectiveness with substantial drying time. It provides both waterproofing and protection from UV rays. In addition, it is more flexible than polyurethane, so it adapts better to shrinkage and expansion of the wood.\nCan Weathered Teak Be Restored?\nIt is not difficult to restore weathered teak furniture. You will follow a three-step process similar to restoring other outdoor wood furniture types: 1) sanding, 2) cleaning, and 3) protecting.\n1. Sand Your Teak Furniture\nUsing 80-grit sandpaper with an orbit sander and hand-sanding where necessary, remove the silvery-gray surface wood until you reach the underlying layer of healthy teak. Then go over the entire surface with 150-grit sandpaper to achieve a smooth finish, ready to accept stain. Make sure to thoroughly sand all surfaces, including hard-to-reach areas such as inside corners and between slats. After you’ve sanded, spray the furniture clean with a low-pressure setting on your garden hose sprayer.\n2. Clean Your Teak Furniture\nBefore your furniture is fully dry, apply teak cleaner to it, using a scrubbing pad or a soft-bristled scrub brush. Work the cleaner into the wood and let it sit for 5-10 minutes for a deep clean. Then respray the furniture with your garden hose sprayer on a low-pressure setting. Set your furniture in a sunny spot to allow it to dry completely.\n3. Protect Your Teak Furniture\nFinally, protect your teak furniture from drying out again by applying a finishing product to it. You may choose either a teak protector — which shields the wood from UV rays — or a teak sealer, which protects from both UV rays and moisture. In either case, use a sponge to apply an even coat of the product to all wood surfaces. Make sure to coat hard-to-reach surfaces such as those between slats. Allow your furniture to dry for at least 24 hours after sealing it before replacing cushions or using it.\nWhat Is The Best Finish For Teak Wood?\nThe three types of finishes used on teak wood are: 1) teak oil, 2) teak protector, and 3) teak sealer. We’ll discuss the advantages and disadvantages of each below.\nTeak oil is actually derived from other plants, usually linseed or tung. Its purpose is to restore the shiny luster of teak wood after the natural oils have dried up. It is formulated to be thinner than pure linseed or tung oils so that the wood’s dense grain absorbs it deeply. Rubbing teak oil on your furniture gives the wood a warm, homey glow. Teak oil is best used on indoor furniture because it restores the oiled look of teak wood; it does not protect against moisture or UV rays.\nLike teak oil, a teak protector restores the shine to your wood; it also protects from UV rays. Because its effects last up to four times as long as those of teak oil, teak protector is a highly popular choice for application to indoor furniture, especially pieces that sit in sunny areas.\nSome homeowners choose to apply teak protector to their outdoor furniture; however, it is not the ideal choice for teak furniture exposed to the elements because it does not provide a waterproof finish.\nThe best finish for outdoor teak furniture is, unquestionably, teak sealer. This product provides both a waterproofing finish and protection from UV rays. It is available in a clear-coat formula that highlights the natural luster and grain of the teak wood or in various brown shades that serve as both stain and sealer.\nWhat Is The Best Exterior Clear Coat For Wood?\nAlthough numerous types of exterior clear coats for wood all fall into two basic categories: those that penetrate deep into the wood’s pores and those that form a filmy shell on top of the wood. Penetrating clear coats include oils such as teak, tung, and linseed oil, which provide a natural luster and add strength and moisture resistance to the wood. Clear coating products that form a protective shell include polyurethanes and varnishes. Both of these products provide waterproofing and protection from UV rays.\nThe best clear coat for exterior wood is marine spar varnish, made from linseed oil and alkyd resin. Spar varnish is flexible enough to move with the wood’s expansions and contractions as the outside temperature and humidity fluctuate. It waterproofs the wood and protects it from UV rays.\nThe spar varnish’s initial application is time-consuming, as it should be applied in six to ten thin coats, with plenty of drying time between coats. However, after the first application, it requires only a light sanding and one fresh thin coat annually.\nShould I Cover My Patio Furniture Every Night?\nAs long as you have protected your wood patio furniture from moisture by oiling or sealing it, there is no need to cover it at night. Any dampness that might collect on the furniture overnight will evaporate during the following day. However, you should cover your furniture if you plan to leave it unused for a substantial period of time: several weeks or longer.\nRestoring your weathered outdoor wood furniture adds to its attractiveness and lengthens its lifespan. Whether you use oil, a product that provides UV protection without waterproofing, or a sealer that both waterproofs and protects your furniture from UV rays, you will have a variety of options to choose from. By following the steps outlined above, you can return your furniture to looking like new and keep it attractive and functional for years to come!\nYou may also enjoy:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:8abc4e95-4e5a-4b2d-94e6-8bf7e2934cb1>","<urn:uuid:c88f7f54-e81a-4f74-9b68-ae5f01c1de65>"],"error":null}
{"question":"Could you compare how ASU's visual analytics and COHRINT Lab's research approach the challenge of human trust in automated systems?","answer":"Both labs address human trust in automated systems but with different approaches. ASU's work focuses on teaming human and machine intelligence through visual analytics tools that help both novices and experts understand spatiotemporal patterns, making AI more interpretable. COHRINT Lab tackles trust through their 'Machine Self-Confidence' project, which develops specific metrics to convey an autonomous system's capabilities to non-expert users, helping them understand when to trust and appropriately task the system without requiring complex explanations of internal decision processes.","context":["Exploring Spatial Phenomenon with Geovisual Analytics\nRoss Maciejewski, School of Computing, Informatics & Decision Systems Engineering, Director of the Homeland Security Center for Accelerating Operational Efficiency (CAOE), Arizona State University\n10AM (PST), Wednesday, November 4, 2020\nAbstract: From smart phones to fitness trackers to sensor enabled buildings, data is currently being collected at an unprecedented rate. Now, more than ever, data exists that can be used to gain insight into how policy decisions can impact our daily lives. For example, one can imagine using data to inform decisions on resource allocations for emergency response or diet and activity patterns could be used to provide recommendations for improving an individual's overall health and well-being. Underlying all of this data are measurements with respect to space and time, and geographical visual analytics tools and techniques have become a prominent means of modeling and conveying spatiotemporal patterns to novices and experts alike. In this talk, I will explore how advances in Artificial Intelligence and Machine Learning have helped drive a new wave of spatial data collection, modeling and simulation, and how teaming human and machine intelligence is leading to exciting new challenges and opportunities for geovisual analytics.\nBiography: Ross Maciejewski is an Associate Professor at Arizona State University in the School of Computing, Informatics & Decision Systems Engineering and Director of the Center for Accelerating Operational Efficiency (CAOE) - a Department of Homeland Security Center of Excellence. His primary research interests are in the areas of visual analytics, explainable AI and decision making. Professor Maciejewski is a recipient of an NSF CAREER Award (2014) and was named a Fulton Faculty Exemplar (2017) and Global Security Fellow at Arizona State. His work has been recognized through a variety of awards at the IEEE Visual Analytics Contest (2010, 2013, 2015), a best paper award in EuroVis 2017, and a CHI Honorable Mention Award in 2018.\nWhat's next in AI - Fluid Intelligence\nAya Soffer, Vice President of AI Technologies for IBM Research, Haifa Research Lab, Haifa, ISRAEL\n10AM (PST), Thursday, November 5, 2020\nAbstract: Today's AI is mostly narrow. While many AI models deliver value in specific, well-defined situations, applying those same models to new challenges requires an immense amount of new data and training. Enterprises need AI that is fluid and adaptable, capable of applying knowledge acquired for one purpose to new domains and challenges. They need AI that can combine different forms of knowledge, unpack causal relationships, and learn new things on its own. We call this fluid intelligence. In this talk I will provide an overview of several research directions that we are exploring in IBM towards fluid intelligence. These include neuro-symbolic techniques that combine deep learning with more traditional AI techniques such as symbolic reasoning; Efforts in NLP that can extract meaning from complex documents; AI engineering, tools and capabilities that simplify and automate key tasks like data preparation, training and lifecycle management; and innovation in the area of secure, trustedAI, with work focused on explainability, fairness, and bias reduction. I will provide examples focusing on Geo-Spatial applications of AI.\nBiography: Dr. Aya Soffer is Vice President of AI Technologies for the IBM Research AI organization focusing on natural language understanding and conversational systems. In this role Dr. Soffer is responsible for setting the strategy and working with IBM scientists around the world to shape their ideas into new AI technology, and with IBM's product groups and customers to drive Research innovation into the market. In her 20 years at IBM, Dr. Soffer has led several strategic initiatives that grew into successful IBM products and solutions in the Big Data and AI space. These include the original Watson system and more recently Project Debater. Dr. Soffer received her MS and PhD degrees in computer science from the University of Maryland at College Park. Before joining IBM in 1999, she was a research scientist at Goddard space flight center, where she worked on digital libraries for earth science data. Dr. Soffer has published over 50 papers in refereed journals and conferences and filed over 15 patents. She has additionally served on program committees, as track chair, and given keynotes in many leading conferences including a TED Talk at TEDMED 2014.","The COHRINT Lab brings together expertise in machine learning, sensor fusion, control and planning algorithms for autonomous mobile robot systems, with a special emphasis on aerospace applications. Our research focuses on intelligent human-robot interaction and scalable distributed robot-robot reasoning strategies for solving dynamic decision-making problems under uncertainty. Check out this video to learn more!\nSoftware developed by our lab is available on GitHub.\nActive and Recent Projects\nCollaborative Analyst-Machine Perception for Robust Data Fusion\nSponsored by: Air Force Space and Missile Systems Center (SMC-RX)\nPI: Nisar Ahmed (CU Boulder); co-PI: Danielle Szafir (CU Boulder)\nAs automated ingestion and processing of high volume satellite remote sensing data continues to expand, human analysts and operators will always need to be kept in the loop. Sophisticated machine learning algorithms for automated event detection, tracking and data fusion do not perform perfectly in all situations. Domain users are often unaware of these limitations, and are currently unable to interact directly with these algorithms to mitigate these issues. New technologies that better balance between automation and human oversight are needed to exploit the best of both worlds at various levels of the fusion pipeline. This research will develop new fusion algorithms and interfaces that allow analysts to communicate directly with automated machine learning algorithms via natural language chat, direct manipulation, and hand-drawn locative sketches. We will combine probabilistic machine learning, data fusion, and human input processing algorithms into a unified software suite that enables bilateral information exchange between automation and analysts. This research is being conducted in collaboration with Lockheed Martin Space Systems and the TAP Lab.\nHarnessing Human Perception in UAS via Bayesian Active Sensing\nSponsored by: NSF IUCRC Center for Unmanned Aerial Systems (C-UAS)\nUAS operators and users can play valuable roles as “human sensors” that contribute information beyond the reach of vehicle sensors. For instance, operators in search missions can provide “soft data” to narrow down possible survivor locations using semantic natural language observations (e.g. “Nothing is around the lake”; “Something is moving towards the fence”), or provide estimates of physical quantities (e.g. masses/sizes of obstacles, distances from landmarks) to help autonomous vehicles better understand search areas and improve decision making. This research focuses on the development of intelligent operator-UAS interfaces for “active human sensing”, so that autonomous UAS can decide how and when to query operators for soft data to expedite online decision making, based on dynamic models of the world and the operator.\nRobust GPS-Denied Cooperative Localization Using Distributed Data Fusion\nSponsored by: U.S. Army Space and Missile Defense Command\nThis research will develop new decentralized data fusion (DDF) algorithms for cooperative positioning. Accurate position and navigation information is crucial to mission success for mobile elements, especially in denied and contested environments. To ensure robustness to disrupted communications or GPS/satellite reception, novel sensor fusion algorithms are needed to assure cooperative positioning. This allows elements to treat each other as beacons on a “moving map”, whose uncertain locations are mutually estimated via opportunistic absolute/relative position measurements and then shared with each other. Key technical challenges for such algorithms are to ensure scalability, statistical correctness, and awareness of potential signal interference, while also enabling flexible information integration with minimal computing and communication overhead.\n“Machine Self-Confidence” for Calibrating Trust in Autonomy\nSponsored by: NSF IUCRC Center for Unmanned Aerial Systems (C-UAS), and Northrop-Grumman Aerospace Systems\nGiven the growing complexity and sophistication of increasingly autonomous systems, it is important to allow non-expert users to understand the actual capabilities of autonomous systems so that they can be tasked appropriately. In turn, this must engender trust in the autonomy and confidence in the operator. Competency information could be delivered as explanations of internal decision processes made by the autonomy, but this is often difficult to interpret by non-experts. Instead, we advocate that these insights be conveyed by a shorthand metric of the autonomy’s “self-confidence” in executing the tasks it has been assigned. Formulated correctly, this information should enable a competent user to task the autonomy with enhanced confidence, resulting in both increased system performance and reduced operator workload. Incorrectly constituted or inflated self-confidence can instead lead to inappropriate use of autonomy, or mistrust that leads to disuse. This project will develop specific metrics for intelligent physical system self-confidence, guided by autonomous aerospace robotics applications involving complex decision-making under uncertainty.\nScalable Cooperative Tracking of RF Ground Targets\nSponsored by: NSF IUCRC Center for Unmanned Aerial Systems (C-UAS)\nThis work develops a new approach to decentralized sensor fusion and trajectory optimization to enable multiple networked UAS assets to cooperatively localize moving RF signal sources on the ground in the presence of uncertainties in ownship states and sensing models. Our approach ties together model predictive planning with the recently developed idea of factorized distributed data fusion (FDDF), which allows each tracker vehicle to ignore state uncertainties for other vehicles and absorb new target state and local model information without sacrificing overall estimation performance. This approach will significantly reduce communication and computational overhead, and allow vehicles to maintain statistical consistency as well as accurately predict expected local information gains to efficiently devise receding horizon tracking trajectories, even in large ad hoc networks.\nLearning for Coordinated Autonomous Robot-Human Teaming in Space Exploration\nSponsored by: NASA Space Technology Research Fellowship Program\nIn human-robot teams, the effects of interactions at a variety of distances has not been well-studied. We argue that teammate interactions over multiple distances forms an important part of many human-robot teaming applications. New modeling and learning approaches are needed to build an accurate and reliable understanding of actual human-robot operations at multiple time, space, and information scales, thus realizing the full potential of teaming in complex future applications for space exploration.\nTALAF (Tactical Autonomy Learning Agent Framework)\nSponsored by: Air Force Research Laboratory; in partnership with Orbit Logic, Inc.\nThis research developed a novel software and learning architecture for optimally adapting the behaviors of autonomous agents engaging in air combat engagement simulations. The key innovation is the development of a Gaussian Process Bayes Optimization (GP/BO) learning engine that evaluates metrics from simulation runs while intelligently modifying tunable agent parameters to seek optimum outcomes in complex multi-dimensional trade spaces. The research topic targeted more effective training of pilots at lower costs, but the onboard agent-based software utilized by the training function of the architecture also has application toward both unmanned combat aircraft and onboard advising capability, which can allow pilots to be more dominant in engagements.\nEvent-Triggered Cooperative Localization\nCollaborators: University of California San Diego and SPAWAR\nThis research focuses on a novel cooperative localization algorithm for a team of robotic agents to estimate the state of the network via local communications. Exploiting an event-based paradigm, agents only send measurements to their neighbors when the expected benefit to employ this information is high. Because agents know the event-triggering condition for measurements to be sent, the lack of a measurement is also informative and fused into state estimates. The benefit of this implicit messaging approach is that it can reproduce nearly optimal localization results, while using significantly less power and bandwidth for communication.\nAudio Localization and Perception for Robotic Search\nSponsored by: CU Boulder CEAS Discovery Learning Apprenticeship Program\nThis undergraduate research project looks at techniques for incorporating sound-based perception and localization into autonomous mobile robotic search problems in human environments. Although nearly all robotic exteroperception algorithms rely heavily on active/passive vision (e.g. lidar, cameras), most autonomous robots nowadays are effectively “deaf”, i.e. they cannot incorporate ambient sound information from their environments into higher-level reasoning and decision making. Unlike active sonar, perception based on ambient sound is passive in nature and must be able to handle a wide spectrum of stimuli. The goal of this project is to develop software and hardware capabilities for enabling an autonomous mobile robot to augment other sensory sources (onboard visual target detection, human inputs, etc.) with onboard audio detection and localization for a multi-target search and tracking task in a cluttered indoor environment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f7759733-f868-40e5-87a2-658d4c13403f>","<urn:uuid:82fdfe4d-3776-4c51-a141-109109c5275e>"],"error":null}
{"question":"What is the difference between Sever's disease and plantar fasciitis in terms of the age groups they typically affect and their primary symptoms?","answer":"Sever's disease and plantar fasciitis affect different age groups and have distinct symptoms. Sever's disease primarily affects physically active children aged 8-14 years old, particularly during their early puberty growth spurt. Its main symptom is heel pain due to inflammation at the growth plate. In contrast, plantar fasciitis commonly affects people in their 40s and 50s, as well as athletes and those who spend long hours on their feet. The main symptom of plantar fasciitis is sharp, stabbing pain in the heel, especially when first getting out of bed in the morning, which typically improves after walking around as the ligament stretches.","context":["Sever?s disease is a painful condition of the heel affecting children, usually at the beginning of the growth spurt in early puberty. It is caused by inflammation at the growth plate at the back of the heel, adjacent to the Achilles tendon attachment. This is one of the most common causes of heel pain in school-aged children. Physically active children aged between eight and fourteen years old are most at risk of developing pain from Sever?s disease. It is common among children involved in soccer, little athletics, gymnastics, basketball and netball but can affect children involved in any running or jumping activity. Boys seem to be more commonly affected than girls.\nThe more active a child is then the greater the chance of suffering from Sever?s disease. Poor foot function such as flat feet causes the calf and Achilles to work harder and pull on the growth plate leading to Sever?s disease. Tight calves or Achilles is common in growing children and can increase tension on the growth plate.\nThis is a condition that affects the cartilage growth plate and the separate island of growing bone on the back of the heel bone. This growth plate is called the physeal plate. The island of growing bone is called the apophysis. It has the insertion attachment of the Achilles tendon, and the attachment of the plantar fascia. This island of bone is under traction from both of these soft tissue tendon and tendon-like attachments.\nYour podiatrist will take a comprehensive medical history and perform a physical examination including a gait analysis. The assessment will include foot posture assessment, joint flexibility (or range of motion), biomechanical assessment of the foot, ankle and leg, foot and leg muscle strength testing, footwear assessment, school shoes and athletic footwear, gait analysis, to look for abnormalities in the way the feet move during gait, Pain provocation tests eg calcaneal squeeze test. X-rays are not usually required to diagnose Sever?s disease.\nNon Surgical Treatment\nThe primary method of treating Sever?s disease is taking time off from sports and other physical activities to alleviate the pressure on the heel bone. During the healing period, your child?s doctor may also recommend physical therapy or any type of exercise that involves stretching and strengthen leg muscles and tendons. Wrapping ice in a towel and placing it under the child?s heel will also help to alleviate and reduce pain and swelling.\nThe surgeon may select one or more of the following options to treat calcaneal apophysitis. Reduce activity. The child needs to reduce or stop any activity that causes pain. Support the heel. Temporary shoe inserts or custom orthotic devices may provide support for the heel. Medications. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen, help reduce the pain and inflammation. Physical therapy. Stretching or physical therapy modalities are sometimes used to promote healing of the inflamed issue. Immobilization. In some severe cases of pediatric heel pain, a cast may be used to promote healing while keeping the foot and ankle totally immobile. Often heel pain in children returns after it has been treated because the heel bone is still growing. Recurrence of heel pain may be a sign of calcaneal apophysitis, or it may indicate a different problem. If your child has a repeat bout of heel pain, be sure to make an appointment with your foot and ankle surgeon.","PLANTAR FASCIITIS TREATMENTS\nOne of the most common causes of foot pain is plantar fasciitis. This painful condition affects many people starting in their 40s and 50s, as well as athletes and those who spend long hours on their feet. Chiropractic adjustment is one of the many approaches that can be used to address plantar fasciitis.\nWhere is the Plantar Fascia?\nThe plantar fascia is a ligament, or thick band of tissue, that runs along the bottom of the foot, connecting the heel to the toes. This ligament gives support to the arch of the foot. Improper foot posture, poor foot support, and continual pounding of the feet as from athletic activities can cause straining of the plantar fascia. Eventually, this straining causes small tears to develop in the ligament, leading to irritation, inflammation, and pain. This condition is called plantar fasciitis.\nPlantar Fasciitis Symptoms\nThe symptoms of plantar fasciitis include sharp, stabbing pains in the heel, particularly when a person first gets out of bed in the morning. The pain usually goes down after a person has walked around a bit and the ligament stretches. Plantar faciitis develops more commonly in people who are overweight, work all day on their feet, wear shoes with little arch support, or are athletic.\nWhat Can I Do for Plantar Fasciitis?\nIf you think you are developing plantar fasciitis, it is important to evaluate your activities and foot support and make changes where needed. Resting when you are able can take pressure off your feet and allow the inflammation to go down. Doing some simple stretches as discussed below will help. Many people benefit from choosing shoes with better support, and orthotic inserts can be helpful to target the area. Special taping techniques can be used to give more support, especially for athletes. If the condition is more severe, anti-inflammatory medications, steroids, or even surgery may be needed. However, in many cases, chiropractic adjustments can be done to adjust the bones in the feet and leg to better support the arch and lessen the strain on the foot.\nPlantar Fasciitis Exercises\nHere are a few simple exercises that can help with plantar fasciitis:1) Simple lunge: Stand facing a wall in a basic lunge with front leg bent and the back heel on the floor, stretching the calf. Hold for 30 seconds, then switch legs.2) Foot stretch: While sitting, place your foot in a position where you can grasp the toes. Pull toes gently upward to stretch the arch.3) Foot roll: Again sitting, place a soda can or other cylindrical object on the floor and roll your foot back and forth over it.4) Toe grasp: Strengthen your feet by practicing grabbing a towel with your toes. Try to lift it off the ground.\nAs with many inflammatory conditions, prevention is key to minimizing the pain of plantar fasciitis. Proper foot support, adequate rest, and daily stretching are some of the simple ways to minimize stress on the plantar fascia. If additional help is needed, chiropractic adjustment can be very useful in reducing stress and inflammation. Elkhart Chiropractors are committed to supporting patients in every way possible to maintain a healthy, pain-free life.\nPain in the bottom of the foot or in the heel may be caused by a condition known as plantar fasciitis.\nWhere is the plantar fascia? The plantar fascia is a fat band of tissue along the bottom of the foot that connects the heel bone to the bones at the bass of the toes. It supports the arch of the foot.\nWhat is plantar fasciitis? This condition results when the plantar fascia in the sole of the foot becomes swollen and inflamed from tiny tears in the tissue.\nWhat are the symptoms of plantar fasciitis? The most common symptom of plantar fasciitis is sharp pain most often experienced when standing or walking after a period of inactivity. The pain can become more intense after frequent periods of walking and standing. The sufferer may also experience a deep, radiating ache anywhere from the toe to the heel that can increase in severity as weight is applied to the foot. Pain and damage to the tissues can increase as the sufferer tries to minimize the pain by changing the position of the foot when walking or standing.\nWhat are the causes of plantar fasciitis? The true causes are unknown. The condition occurs most often in middle-aged people, but young people who are on their feet a lot can also experience it. Repeated strain on the foot can result from being overweight or wearing shoes that fit poorly, standing on hard surfaces for long periods of time, or possibly from rolling the foot inward too much. Other potential risk factors include flat feet or exceptionally high arches, tight Achilles tendon, and unequal leg length.\nWhat can be done to treat plantar fasciitis? Rest and application of heating and ice packs can provide some immediate relief. Anti-inflammatory medications may also be of help in the short term. However, there are some simple exercises that can help with the majority of symptoms, including mild stretching. Treatments may include the use of custom orthotics, or arch or heel supports. A chiropractor can recommend and provide adjustments to help the tissues heal. He can also show the patient how to do exercises to bring relief and recommend lifestyle changes to promote healing.\nHow can plantar fasciitis be prevented? Because the causes vary from person to person, there is no one means of prevention. Maintaining a healthy weight along with a regimen of regular exercise can help. Wearing well-fitted shoes that provide adequate cushioning and support is also important. Those who stand for long periods of time or who engage in athletic activity need to find ways to minimize strain on the foot. Regular and focused chiropractic adjustments can also maintain healthy joints and help to stabilize the plantar fascia.\nEach person is different, and there is not one single treatment for every case of plantar fasciitis. This condition can often take six months or more for complete resolution. However, it is often easily treated with non-surgical methods and time and patience. Consultation with a chiropractic professional is a beneficial step in maintaining healthy, pain-free feet."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:5286895c-bb33-4594-a526-d33c0449e9f9>","<urn:uuid:d29216e4-f777-44ce-b5a5-b80d41d91fed>"],"error":null}
{"question":"How do the decorative materials and techniques compare between middle-period Byzantine churches and St Elisabeth Convent's current mosaic work?","answer":"Middle-period Byzantine churches and St Elisabeth Convent's current mosaic work share similar decorative approaches. Byzantine churches excelled in mosaic art, using glass shards and gold leaf to enhance the impact of light in their churches. Similarly, St Elisabeth Convent uses smalt (glass melted with metal oxides) to create mosaics with vibrant looks and flickering effects when light hits the uneven surfaces. Both styles employ gold backgrounds and clear outlines. The materials and ancient technique have remained largely unchanged for centuries, with both using lime gesso as a special-purpose mortar. The Byzantine tradition of covering entire church interiors, including domes, walls and vaults, with mosaics is still practiced by the Convent's workshop, though they now use modern methods like printed sketches and plasticine for initial layouts.","context":["The structure and appearance of Byzantine churches evolved significantly during the thousand year history of that empire. Early churches were based closely on patterns drawn from Roman civic and religious architecture. Churches constructed during the middle years of the Byzantine Empire tended to follow a unique architectural plan featuring large and richly-decorated domes. Byzantine churches erected during the waning years of the empire were often less richly-decorated, and began to feature a wall of icons.\nThe first Byzantine churches were built on a Roman model, as the Byzantine Empire was the Eastern Roman Empire. These churches typically featured a basilica layout. This type of floor plan features twin rows of columns that partially separate aisles along the side of a rectangular structure, and also serve to support the roof. A curved apse is usually located at the end of the basilica. Wings were sometimes added to this structure, creating a cruciform shape, but were generally shorter than the main hall of the basilica.\nAs the culture of the Byzantine Empire became more thoroughly Greek, a new style of Byzantine church emerged. The Hagia Sophia, perhaps the most famous Byzantine building of all, showcases the key features of this style. In this church, there is a central dome, and four wings of equal length lead off from that dome. This is a substantial departure from a traditional basilica plan and was made possible by architectural advances that made the construction of larger domes possible.\nReligious art in Byzantine churches typically employed rich materials to decorate most visible surfaces. Churches in wealthy regions would be covered entirely in mosaics, an art at which the Byzantines excelled. Glass shards and gold leaf were used together to create vivid colors and to enhance the impact of the light that was allowed into Byzantine churches by improved dome construction. Marble and other expensive materials were used to make churches more beautiful, and although some churches featured religious frescos, mosaics were preferred.\nArtwork in Byzantine churches usually depicted stylized religious figures. These figures were meant to convey a symbolic and spiritual message, rather than to precisely depict the human form. Early churches, such as San Vitale in Ravenna, did sometimes depict recognizable human figures, but this became much less common in later years. The depiction of the human form, even for religious reasons, was controversial in the Byzantine church, and a period of iconoclasm began in the 700s, during which much church art was destroyed. Churches erected during this period were typically not ornamented with images of human beings, even stylized ones.\nIn the waning years of the Empire, icons were once more embraced. Byzantine churches built in the last centuries of the Byzantine Empire not only featured religious images on their walls but added a wall of icons at the front of the church. This wall came to be entirely covered in Byzantine icons, painted in the stylized manner that had developed centuries earlier. Church decoration during this period was generally less lavish, as the Empire’s fortunes were fading.","Everyone entering the Sovereign Church of St Elisabeth Convent for the first time will be struck by the majestic beauty of its opalescent vaults. The interior of the church, decorated with mosaics, is the result of the long and meticulous work of the Convent’s mosaic artists. The mosaic images, made in accordance with Byzantine traditions, resemble the interior decoration of the churches of Constantinople and St Sophia Cathedral in Kiev.\nIt seems like the golden glow of smalt on the domed vaults carries a reflection of eternity, conducting the Divine light of heaven in our temporary world. Mosaic, as a symbol of eternity, is not just a metaphor. Its property to retain its original form for thousands of years is provided by materials and the ancient technique, both remaining unchanged for centuries. This makes the modern mosaicists our guides into the world of art, whose history is rooted in antiquity and is closely associated with the birth of Christian arts.\nA Brief Introduction to Byzantine Mosaic\nByzantine mosaic, as part of the cultural heritage of the Roman Empire, was primarily a form of church art. It began to be used as a technique for creating a holy image, even earlier than icon painting. The most ancient mosaic images surviving to this day date back to the 3rd-4th centuries.\nThe material used in laying out mosaics is called smalt. Smalt was also developed in Byzantium by melting glass with metal oxides (gold, mercury, or copper) added in different proportions. This process created a rich palette of colors, enabling an artist to lay out any image in the smallest detail. Smalt does not fade or deteriorate with age. The image retains its original shades throughout its existence. A similar technology is used for its production to this day.\nImages made of smalt are not realistic, but rather conventional and monumental, intended to convey the greatness of the Kingdom of Heaven.\nMosaics were often used for decorating the entire church interior, including domes, walls and vaults. The large scale images occupied areas, tens of square meters large. Each particle of smalt has a unique uneven surface, causing the image to flicker, when light hits it, and giving the mosaic its vibrant look. The features of Byzantine mosaics also include the use of a gold background and creating clear outlines for the elements of an image.\nToday the traditions of the ancient Byzantine art are continued by the workshop of St Elisabeth Convent. Besides the Convent itself, their mosaics can be seen in many churches in Belarus, Russia, Ukraine, Greece, Montenegro, and other countries, including even South Africa.\nThe smooth operation of the workshop is provided with joint efforts. A mosaic begins with the development of a sketch, which is then printed in full size. The outlines of the future image are then laid out using pieces of smalt, crushed with a specialized tool and adjusted to match the required size and shape. The work is divided between several mosaicists, each responsible for his own area (vestments, facial images, backgrounds etc.)\nThe individual pieces are first laid out on plasticine with the help of tweezers. Then, after the color palette of each image is adjusted, it becomes transferred onto a special-purpose mortar called lime gesso. Since ancient times, gesso has been used in church interior decoration, as well as in icon painting. The composition of the gesso used in mosaics contains components that have remained unchanged for thousands of years. These ingredients include lime, quartz sand, brick chips, bovine bile, chicken eggs and flax.\nThe whole image or its fragments are finally transferred onto a wall. Sometimes mosaic elements have to be compiled into a final image right on the site, making the work more difficult, especially when it needs to be done on high-altitude vaulted surfaces (e. g. under a dome).\nThe main goal of the Convent’s mosaicists is to preserve and convey to the beholder the eternal spiritual values. According to the artisans’ personal testimonies, their work contains a constant contact with the Sacred and a personal meeting with God. Aspiring for perfection, an artist puts all his spiritual zeal and love into his work. If he succeeds, a person contemplating it will be certain to sense his effort."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:387d53e5-0114-40b7-8d7b-d6a0d2b1cd52>","<urn:uuid:0091bd7c-3dd2-41f7-a874-02af7558c60f>"],"error":null}
{"question":"What are the current infrastructure challenges for EV adoption, and how do green building practices address similar sustainability issues?","answer":"The main infrastructure challenge for EV adoption is the weak charging infrastructure and limited charging stations in the US, particularly in rural areas. However, initiatives like the National Highway Charging Collaborative plan to install stations at over 4,000 highway locations. Similarly, green building practices address sustainability through integrated infrastructure solutions, including energy efficiency measures like building orientation, alternative energy sources, and high-performance insulation. Both sectors emphasize the need for comprehensive system-wide approaches rather than viewing components in isolation.","context":["Share on ThriftyFunThis guide contains the following solutions. Have something to add? Please share your solution!\nIf you have a building or remodeling project in your future, consider the benefits of going green. It's a common misperception to think that incorporating environmentally friendly measures into your project will cost more, but that isn't usually the case. In fact, even if you're on a tight budget, many elements of sustainable building and remodeling can be incorporated into your project with minimal (sometimes zero) upfront costs.\nWhat It Means to Build (Or Remodel) Green\nGreen building, also referred to as sustainable building, involves structures that are designed, built, renovated, operated or reused in an ecological and resource efficient way. Green buildings should protect the occupant's health, improve their quality of life and use energy, water, building materials and site resources in the most efficient way possible, while minimizing environmental impacts.\nThe Costs (& Savings) of Building Green\nIt's true that some aspects of green building (like installing solar panels) can cost more up front. The savings comes over the lifetime of the building in the form of reduced operating costs (lower electricity and heating bills). To calculate the true costs, projects should be analyzed over their entire life cycle in order to determine what costs are appropriate up front. This requires looking at your home as one complete integrated system, rather than a collection of individual smaller systems.\nOther cost savings of building green can not be as easily quantified. If you could put a price on improved health and calculate the savings in reduced pollution and wastes, the cost of building green could easily be considered a real bargain.\nIt's a common misperception to think that incorporating environmentally friendly measures into your project will cost more, but that isn't usually the case.\nGreen Building Basics\nThere is no one accepted set of standards for green building or remodeling, but here are some examples of green building practices:\nThe Building Site: Taking advantage of site features including preserving existing trees, minimizing land disturbance and siting the house to optimize solar gains and protect it from the elements.\nEnergy Efficiency: Using design strategies to improve energy performance including building shape and orientation, alternative energy sources, high performance insulation and windows, and energy efficient appliances and mechanicals.\nBuilding Materials: Incorporating the use of local sustainable building materials including those with recycled content and zero (or low) off-gassing and toxicity. This also includes the efficient use or reuse of building materials to minimize demolition and construction waste.\nWater Conservation: Incorporating water conservation technologies such as low-flush or composting toilets, low-flow water fixtures, point-of-use hot water systems, and using water recovery or a gray water system for site irrigation.\nHealth and Well-Being of Occupants: choosing interior finishing products with zero or low off-gassing to protect the health of occupants. Design for adequate ventilation and the proper filtration and humidity controls necessary to improve indoor air quality.\nFinding More Information\nMore and more home owners and builders are realizing the value of sustainable building and design. If you're contemplating a large building project, like a new addition or an entire house, there are plenty of architects, designers and builders that specialize in sustainable building and design as well as library shelves filled with books on energy efficiency, and hundreds of websites available to guide you through every step of the process.\nTrying to find professionals with green building experience can be frustrating, especially in smaller communities. A great place to start your search is the website of the U.S Green Building Council (www.usgbc.org) . There you'll find hundreds of resources including information on LEED (Leadership in Energy and Environmental Design) certified builders. LEED is a voluntary consensus-based national standard for developing high-performance buildings. If you can't find LEED certified professionals in your area, talk with builders recommended by people you know, and see who is the most receptive to making your dream home a green home.","Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:61bc3d4c-2768-4c62-bf4d-10fd7caa2ce5>","<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>"],"error":null}
{"question":"What political dynamics characterized Japan's Muromachi period?","answer":"The Muromachi period was marked by constant power struggles between different authorities. The Emperor and the shogun frequently competed for governmental control, with power sometimes split between them, dividing the island and leading to continued battles. Additionally, there was significant infighting among the shogun's subordinates over territorial control and succession to the position of shogun itself.","context":["I know I’ve slowed down the pace of my reviews these days – the knee rehab is taking a good deal of my focus, and we’re having a spot of technical trouble with the TV, so I have to try to watch all the DVDs on my laptop. But this time, the delay also came from a bit of a research rabbit hole with a detour through a college reunion, of sorts.\nThrone of Blood was something of a passion project for director Akira Kurosawa, who read Shakespeare’s play Macbeth as a student and had long sought to film an adaptation. The chaotic world of medieval Scotland, where an ambitious nobleman could seize power in a flash by killing the king, reminded Kurosawa of Japan’s Muromachi period – a time when the Emperors’ own shogun often struggled with the Emperor himself for government control. At times the Emperor was in control, at times the shogunate – and at times both were in control, splitting the island between them and locking horns in a battle for yet more power. And that’s all before we get into the infighting amidst the shogun’s own subordinates for control of one town over another – or who might take over as shogun themselves.\nIronically, this war-torn period also saw the birth of some Japan’s most serene classical arts. Zen Buddhism flourished during this period, influencing art and culture in ways that encouraged minimalism and specificity, and a sort of “mindfulness” (as much as I hate to use that modern buzzword, it really is accurate). The classic Japanese tea ceremony was born during this time, as was ikebana, a specialized form of flower arrangement; kodo, a ritualized exploration of incense; and the tradition of creating Zen rock gardens. It also saw the rise of Noh theater, a heavily stylized form of theater involving elements of dance, mime, and the use of masks; and fittingly, Kurosawa drew heavily on Noh when adapting this work.\nI’d learned a bit about Noh back in college, during a single theater history course, and had forgotten a good deal; but even so I was spotting Kurosawa using elements of Noh drama in his adaptation. This particularly stood out with the character of “Lady Asaji” (Isuzu Yamada), our tale’s version of “Lady MacBeth”; Lady Asaji is still a good deal of the time, and when she moves, it is usually with a slow deliberateness, forcing you to pay attention to what she’s doing. During the scene where she discusses a power grab with her husband Washizu (Toshiro Mifune), Washizu is charging about the room, professing his loyalty to their King, but Asaji sits completely still, her face absolutely motionless.\nI’d expected to be intrigued by Kurosawa’s adaptation. I’ve read Macbeth plenty, and seen it done a handful of times, and in a handful of ways – from straightfoward productions to a Cyberpunk/Mad-Max style to an immersive theater piece; I even saw an adaptation that fused Shakespeare with the book Fast Food Nation (and insane as it sounds, it worked). But in all those cases the “bones” of the original play showed through, along with the words themselves in many cases. Here, it was the Noh that caught my eye – I wanted to know more about that specifically, or at least improve on what I dimly remembered from college.\nAs luck would have it, a former studio classmate, John Oglevee, went on to specialize in Noh to the point that he moved to Tokyo and co-founded Theatre Nohgaku, an international Noh troupe that performs both classic Noh works and original Noh style pieces in English (their Blue Moon Over Memphis is about an Elvis fan meeting his ghost; here’s a brief bit of it, with John as Elvis). When I wrote John explaining that I was curious about Noh after watching “a Kurosawa film” and asked for a web site he could recommend, he almost instantly wrote back: “oh, I bet you mean Throne of Blood, here’s a whole article on that.”\nWhat astonished me is how it seemed many Noh elements were already there in the original play. The structure and pace of a Noh play seemed nearly synchronous with Shakespeare’s work (Kurosawa just had to cut a couple of scenes which Shakespeare likely only threw in for comic relief anyway). The Three Weird Sisters and the ghost of Banquo match up with Noh plays often featuring ghosts or demons. The masks of Noh were the most “novel” element here, and how Kurosawa was able to use their influence without using actual masks – particularly with Asaji’s expressions. During Throne of Blood’s take on Lady MacBeth’s sleepwalking scene, Asaji has her face fixed in a grimace that comes directly from a fukai style mask, which typically is used for a woman mourning some kind of loss. And as for Washizu, his face isn’t just Toshiro Mifune being Toshiro Mifune – several of his own expressions are also inspired by Noh masks, like this arresting reaction to his final mortal wound.\nIn a way, I wish I’d known even less about Noh when I first saw this, to see if I would have picked up on those elements before watching. But I’m more so intrigued by how well Kurosawa was able to fuse two very different theatrical traditions into a single piece."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:9f1034fe-f150-48e6-87ee-2f5591cfc023>"],"error":null}
{"question":"What is the typical lifespan of regular bow strings compared to crossbow strings when used regularly?","answer":"Regular bow strings typically last 2-3 years for average archers, while crossbow strings have a more variable lifespan of 1-2 years up to 4-5 years. For regular bows, competitive shooters or daily users may need yearly changes. For crossbows, the lifespan is measured in shots, ranging from 150 to 600 shots before requiring replacement.","context":["The ins and outs of a quality bowstring\nDuring my archery pro shop days, one of my most commonly fielded questions from customers was regarding their bowstring: When should I replace this? How long should this set last? Do I need aftermarket strings?\nThe string and cable set on your bow is literally the lifeblood of the entire operation. Good strings will keep cams in sync and your bow in top shooting order. Still, I often encountered a lot of misinformation pertaining to string care and materials used as well as the overall importance of a quality set of strings. In the following article, I’ll go over some of the most important factors when it comes to analyzing your own strings as well as the proper steps to take when purchasing a new set.\nWhen to replace\nWhen it comes to questions pertaining to bowstrings the decision to replace them or not is at the top of the heap. String life can be dependent on many factors, including where you hunt, how you hunt, year-round maintenance, how the bow is carried, etc. The fact is that string life can vary slightly from user to user, but there is a general rule of thumb that I like to use. For the average archer, replacing a set of strings every two to three years will generally suffice while someone who shoots every day or shoots competitively may want to consider a yearly change. Conversely, those who do not take care of their strings could benefit from yearly string changes even with a minimal amount of shooting.\nWhen it comes to evaluating the life of your string, your best bet is to simply get your bow into a trusted pro shop. General things to look for when a string is nearing its end of usable life can include small frays, broken strands, cams continually coming out of sync, peeps that spin while the bow is drawn and age. At the end of the day, my biggest piece of advice to those considering a string change is this: the costs of handling a situation where a bow is “blown” up due to a string failure can cost two to three times more than simply replacing the strings.\nFactory versus aftermarket\nAnother question I’ll commonly hear among archers will be the debate as to whether or not they should replace their factory strings. Really, this will largely boil down to the manufacturer of the bow and the materials they are using in their strings. Most package bows and mid-price point bows will generally come equipped with what I would consider as sufficeable strings, meaning they will work, but won’t be reliable. These types of strings can come with a litany of baggage, including continually changing cam orientations, peep issues and an overall loss of bow performance. Most bow companies are now equipping their flagship bows with top tier strings although this is not always the case. The best bet when considering replacing factory strings or when purchasing a new bow will be to simply seek the advice of an experienced tech.\nIn the past, I have always been the guy to remove factory strings immediately, but technology has really caught up in this day and age and many companies are really putting some effort into their factory sets. I have found over the past few years of tuning and building Mathews bows that their factory Zebra strings are among some of the best and would rival many of the aftermarket companies I have used. Because of the confidence, I have with these stock sets I am simply able to pull my bow from the box, tune and go shoot.\nWho should I buy from?\nWhen shopping for aftermarket strings you will be met with many choices, particularly online. Good quality materials such as 452X, 8125 or BCY X are readily available to consumers and professionals alike; however, the actual building process of strings is what sets good quality strings apart from something that will make you want to tear your hair out. Most pro shops will generally have few selections for string manufacturers and most have, hopefully, done their homework and have selected good quality and reputable builders. When purchasing online make sure to do your research. What types of materials are they using or offering? Are the strings pre-stretched before being served? If so, how many pounds of tension are they using? Additionally, in this day and age, customer reviews are wildly available for just about everything.\nWhen personally selecting strings, I’ll generally look for three primary factors before moving on to sifting through reviews: materials used, strings that are pre-stretched under 300+ pounds of pressure and the strand count. Some companies will use fewer strands in their strings to gain faster string speeds, but this comes at a loss of stability and string life. On the flip side, more strands generally equate out to a more stable platform at the cost of some speed. Most strings will generally fall between 20 and 24 strands.\nPerhaps the biggest leading factor to string failure in my experience has been an extreme lack of preventative maintenance from the archers. String wax is very likely one of the cheapest components of archery gear you can buy, yet so many people very rarely—if ever—use it. When a bow is shot, there is a lot of string movement happening that is undetectable to the human eye. This incredibly fast movement creates heavy friction and heat between the string’s fibers and will eventually dry and weaken the string as a whole. As a general rule of thumb, I like to apply wax about once a week if I am shooting every day, but this will largely depend on visual inspections. When a string begins to dry from use, small hair-like fibers will become visible although they can be hard to see unless backlit. Once I start seeing these I’ll apply a quick layer of wax to the string and cables.\nConsequently, overwaxing a set of strings can be almost as detrimental at not waxing enough. When a string is over-waxed, it becomes tacky to the touch and will attract every piece of dust, dirt and grit it encounters while in the field or in the backyard. This debris collected on the string will now work itself into the fibers during each consecutive shot and prematurely wear down the string. After spending $100+ on a set of string, a $4 tube of wax is worth its weight in prevention.\n“An ounce of prevention is worth a pound of cure.” —Benjamin Franklin\nAt the end of the day, the string is the most pivotal point of your bow and something that needs to be constantly monitored. Not only will a quality set to improve the performance of your bow, but it will also instill more confidence in your shooting and be one less stress while in the backcountry. Evaluate your own setup and consider the quality and age of the current strings. Now is an excellent time to get your bow into the shop for a quick tune-up ahead of the crowds in July and August!","As crossbows become more advanced, regular maintenance is becoming even more important, and changing your crossbow strings is probably the most crucial of all. We are going to answer all of your what, when, and how questions in this complete guide.\nIf you’re anything like us, you’ll want to keep your crossbow in peak-performing condition at all times. The last thing you want is a less-than-perfect shot at that crucial moment in your hunt.\nProbably the easiest way of guaranteeing supreme accuracy and performance is by performing some simple maintenance tasks regularly. In our eyes, the most important of these tasks is regular string maintenance.\nCrossbow strings have a lifespan that will vary depending on the manufacturer. Some can be as little as 1-2 years and others can last up to 4-5 years, depending on how often you shoot! That means you can expect to shoot your bow between 150 and 600 times before you need to restring your bow.\nIn this guide, we’re going to take a closer look at how you will know when to change your strings, and what you can do to maximize their lifespan.\nHow Long Do Crossbow Strings Last?\nWe can make a pretty accurate assessment of the lifespan of your strings by knowing the brand and how often you shoot. There are still a few more factors that come into play here, like how heavy your arrows (bolts) are.\nWhile it may sound counterintuitive, the heavier your arrows are, the less strain your strings will experience with each shot because you are diverting more energy into the arrow and not the strings.\nHow do I Know When It’s Time to Replace My Crossbow Strings?\nProbably the best way of knowing is by keeping track of your shot count. This may sound a bit excessive—the idea of keeping track of every individual shot—but you don’t need to be quite so accurate. Just keep it within a reasonable range of about 30 shots on either side of 400, you’ll be OK.\nThe simplest way to do this is just taking a note at the end of each hunt or shooting session either with a pen and pad or by keeping a note on your phone.\nIt’s a small and tedious task to add to each session, yes. But, it will pay off in a big way down the line when you have consistent, accurate, and safer sessions with your crossbow.\nWhat Are Signs My Crossbow Strings Need to be Replaced?\nYou should also check your strings periodically to make sure there are no visible signs of wear or fraying, and two other important signs to look out for:\n- Serving Separation: The thinner protective string that is wound over the main crossbow string at key contact points like the arrow retention claws.\n- Axle-to-Axle Not to Spec: The axle-to-axle length measurement on a crossbow is the distance between the two cam axles, one on the left and one on the right of the bow assembly. When this distance increases to more than .25” out of spec, your string is stretched.\nIf you are noticing issues and wear from serving separation or your axle-to-axle has moved out of manufacturer specification, it’s time to change your strings even if you haven’t hit the advised shot count.\nA damaged string or out-of-spec axles can be dangerous, they can cause injury if not properly maintained so it is always better to be on the safe side. If in doubt, go to a reputable bow shop for an inspection.\n5 Tips for Extending the Life of Your Crossbow Strings\nSo taking everything you have already learned into account, let’s get right to the real reason you’re here! You want to know what are the best ways of actually extending the life of your crossbow strings, so here are our top five:\n1. Regular Maintenance\nBy far the most important tip is regular maintenance. That is checking your equipment before and after each session and storing your crossbow in a dry environment ideally around room temperature. Extreme changes in temperature and climatic conditions when you’re out in the elements can slowly wear away at the life of your strings so you want to be mindful of these changes.\nFor example, if you’re returning from a hunt, check your crossbow for debris that might have been kicked up in the forest and settled on your equipment. If it has been humid, even consider drying your crossbow with a rag and blotting the string to remove any moisture build-up.\n2. Use High-Quality String & Serving Fluid\nWe would always recommend using high-quality string and serving fluid to maintain the strings and cables on your crossbow.\nSimply put, if you use high-quality products, you will get high-quality results. When wear rate and lifespan are concerned, this is especially important. A lot of people will tend to recommend alternatives to serving fluid, but in our experience considering how little you need to apply, a small bottle will go a very long way. Therefore we would always recommend using an authentic, use-specific, and high-quality product.\nIf you are ever in doubt, check the manufacturing specs and DO NOT overuse products. Excessive use of products can result in increased wear so you will end up throwing away money.\n3. Don’t Use Wax on Your Strings\nContrary to popular opinion, we suggest staying away from wax on strings. It can easily result in a build-up in the trigger box and it acts as a magnet for dust and dirt!\nIf you exercise good regular maintenance and inspection of your crossbow strings and cables, you should not have any need to use wax.\nOne of the purposes of the wax is to help lubricate the string and prevent fraying, but the problem is that it can also clog up the string so you will not notice when it begins to fray naturally—this is a key wear indicator and something you must be able to see.\nIf you use a high-quality strings, serving fluid, and conduct the simple maintenance checks already explained here, there is no need to use wax on your strings.\n4. Use Heavy Arrows\nHeavier arrows can help increase the life of your strings. The main reason is stability and transfer of energy from the string to the arrow. Having that small bit of extra weight adds a lot of stability to the crossbow when shooting which in turn reduces vibration on the string.\nHeavier arrows also have better arrow penetration, giving you a higher rate of success in the hunt.\n5. Check the Strings Before Going Out\nAlways make sure your strings are in good condition BEFORE a shooting session or going to the woods.\nOK, this may not be a whole extra tip because it’s mentioned in the first one, but we can’t stress enough the importance of checking the condition of your equipment before you begin a session.\nCan I Replace My Strings?\nYou can replace your crossbow strings yourself at home if you have the right tools and a little bit of knowledge on the specific manufacturer specifications of your equipment, paying close attention to cam lean and cam timing.\nRemember that if you have a reverse compound or compound crossbow, you will need to use a bow press to change the strings.\nThe most simple strings to replace are on the Excalibur brand recurve crossbows. This is because you only need a stringing aid and rope cocker to loosen the string before changing it.\nRavin and TenPoint crossbows are probably the trickiest because they require an understanding of how cam lean and cam timing work. Simply put, when you adjust the string it will affect the cam adjustment. The cams will require adjusting afterward using the cable to ensure they are aligned and functioning correctly.\nCan I Send My Crossbow to Get Replacement Strings?\nWe understand that changing your crossbow strings can be a daunting task that you may not feel comfortable doing at home. That’s why you should send it to the Borkholder Archery service center where we will start every service by checking the bow for string wear.\nOur shop is based in Milford, Indiana, so if you’re local, just stop by the shop with your crossbow and we’ll be happy to help.\nOtherwise, we also offer a couple of safe and fast options for shipping your bow with add-on insurance available. One option is a fully taken care of service where we will send you an empty box with all the necessary packaging and a return label. If you have your box, you can send it to us by your preferred method and give us a heads up when it’s on the way.\nOnce the service is done, we’ll ship it back to you safely and ready for your next shooting session!\nFactory Strings vs Custom Strings\nWe know that there are a ton of crossbow string brands available on the market, and we have also spent a considerable amount of time researching and testing different strings from different manufacturers to give our best recommendations for each avid hunter.\nMost factory strings are easily available and really do a great job. The strings will be heavily tested by each manufacturer and you will quite often find that because of this, some manufacturers will require that you use specific strings on their crossbows for warranty purposes. This is definitely something to bear in mind if you are using a Ravin, TenPoint, or Mission bow where you will void the warranty by using custom strings.\nIt is best practice to always check with the manufacturer’s spec before buying replacement strings.\nYou should ideally look to strike a balance between durability and performance. While your choice of string will affect the performance side of things, durability will be more dependent on how you exercise all of the maintenance steps we have outlined here.\nMany of the custom crossbow strings that we offer also have different features and colors to personalize your bow setup. They will most likely void the warranty of your bow, but if you follow our regular maintenance advice you shouldn’t have any reason to use the warranty anyway.\nWe are confident that we can help you find a string that will improve the lifespan and shooting experience with your crossbow.\nWhat Are the Top Custom Replacement Strings?\nWhile some factory strings do a great job, we love to find those little improvements in performance and string life that really help us hone the crossbow’s performance. When you have a crossbow that is dialed-in perfectly for how you want to shoot, there isn’t much better!\nWhenever we service a crossbow, there is always an option to replace the current string with the factory-recommended one. We fully appreciate that some folks would prefer to stick to the manufacturer’s spec and keep their warranty, so this is something we’ll always be able to provide.\nBut for those looking to maximize performance while keeping very high levels of durability, we are big fans of Rogue Bowstrings. The Level 19’s offer incredible value for money and are available in a wide variety of colors. The Level 21’s however raise the bar with a proprietary material that exceeds all others in terms of durability. Plus they only need 4-5 shots to be perfectly settle in.\nWe are happy to offer both the Rogue Level 19 and Level 21 as an upgrade with our crossbow service for maximum performance and consistency. We will fit and professionally tune the strings just for you.\nAnd if you are after something really special, we can get you completely custom strings from Joey’s Bowstrings. They are a great manufacturer of high-quality crossbow strings and specialize in completely custom solutions for specific requirements.\nIs There Anything Else to Replace When Replacing My Strings?\nWe’ve mentioned cables a few times already as an important thing to check for wear anytime you are performing basic maintenance of the string, and there is a good reason for this.\nThe wear rate of the cable is actually quite similar to the string because they are part of a codependent system. So every shot and load of force that your string experiences, the cable will be exposed to the same.\nConsidering cable wear is more difficult to notice, we always recommend changing the cable any time you restring your crossbow.\nOur aim as always is to give our recommendations for you to get the highest performance and durability from your equipment, while also keeping you safe from avoidable injury.\nWe are here to help you experience the best stories. We can’t wait to hear them!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:b1177d65-ef78-4744-9b2b-5d699f42f8ba>","<urn:uuid:80d64271-5978-4eec-9c81-2acc998f2831>"],"error":null}
{"question":"Please explain where Gold Sovereigns were manufactured in history?","answer":"Gold Sovereigns have been manufactured at various Royal Mint branches worldwide since 1817. These locations included London, Sydney, Melbourne, Perth, Ottawa, Bombay, and Pretoria. All mints in Australia, Canada, the United Kingdom, India, and South Africa produced their own mint markings. Currently, Gold Sovereigns are exclusively produced at the British Royal Mint in Llantrisant, Wales, making them the only globally struck historical world gold coins.","context":["United Kingdom of Great Britain For much of the last thousand years of British mint history, Gold Sovereigns have been the gold standard in British bullion coinage. Gold coin enthusiasts are always on the lookout for these stunning pieces. Since the reign of the terrible King Henry VIII six centuries ago, the British Royal Mint has been producing Gold Sovereigns. During the decades afterwards, these coins have been produced on five continents across the British Empire, where the “sun never sets.”\nUnited Kingdom Gold Sovereigns Background and History\nThe first series of United Kingdom Gold Sovereigns was created in the late 1400s and continued until 1604 when the last of the original series was struck. The Great Recoinage of 1816 gave the coin a new lease of life, as the British government strove to resuscitate the national currency and economy following the end of the Napoleonic Wars and the French Revolution. In 1817, the first of the modern Gold Sovereigns was issued. Over the next century, they were struck on and off. The coin was restrike in 1925, between 1917 and 1957, as Winston Churchill worked to restore the British Empire to the stabilising economic impact of the Gold Standard. In the years 1949 to 1951, the ever-popular bullion standard currency was revived when the Royal Mint used the 1925 dies, which featured the portrait of His Late Majesty King George V rather than his son King George VI, who was ruling at the time. The government resumed continuous annual manufacture of Gold Sovereigns in 1957. These magnificent coins circulated widely until 1932, while the empire still adhered to the Gold Standard.\nThe Royal Mint has never struck anywhere near as many coins in a rival coinage as it has in Gold Sovereigns. So far, almost one billion of these gorgeous coins have been produced. The ones that were melted down and re-struck as new coins are included in these astonishing figures. Because of national rules at the time, the United States melted down many millions of them into gold bars. The United Kingdom Gold Sovereigns have an average lifespan of fifteen years. Since the late 1400s, these coins have been a globally accepted standard for international trade, making them the only widely recognised and acceptable form of payment for more than five centuries of modern history.\nUnited Kingdom of Great Britain Gold Sovereigns have been produced in a variety of sizes and denominations, but the most prevalent was the.2354 troy ounce variant of a one-pound Gold Sovereign. On the day of mintage, the coins usually bore a bust or portrait of the reigning monarch. As a result, they are both popular historical souvenirs and solid and dependable gold bullion investments. The most widely circulated version dates from King George V’s reign. From 1911 until 1932, these were minted all across the empire. On the obverse of each of these coins was a bust of George V facing left. “George the Fifth, by the Grace of God, King of all the Britons, Defender of the Faith, Emperor of India,” were inscriptions in latin.\nFrom the official termination of the Gold Standard in 1932 to the present day, these historical coins have primarily been minted as bullion coins. After 1982, the coins were available in both bullion and proof condition. Gold Sovereigns have been manufactured at worldwide branches of the renowned Royal Mint around the world from the second historical series of these circulating coins in 1817, including London, Sydney, Melbourne, Perth, Ottawa, Bombay, and Pretoria. The United Kingdom Gold Sovereigns are the only globally struck historical world gold coins. All of the mints in Australia, Canada, the United Kingdom, India, and South Africa produced mint markings.\nUnited Kingdom Gold Sovereigns Physical Characteristics\nUnited Kingdom of Great Britain The British Royal Mint in Llantrisant, Wales, is now the exclusive producer of gold sovereigns. They are currently available in one pound sovereigns, double sovereigns, and five pound sovereigns denominations. The reverse picture of a crown and shield design was enclosed by a heraldic wreath on the first Gold Sovereigns. Following that was a Medieval depiction of the legendary Saint George slaying the dragon. Benedetto Pistrucci, a coin and gem engraver who rose to the position of Chief Medalist at the Royal Mint, designed the St. George and the Dragon motif for this 1817 series. During the reigns of King William IV, Queen Victoria, King George IV, and Queen Elizabeth II, the reverses have changed. The original St. George and the dragon design can still be found on today’s United Kingdom Gold Sovereigns.\nThe “obverse” refers to the coin’s front side. The obverse of today’s United Kingdom Gold Sovereigns features Her Majesty Queen Elizabeth II’s picture. This is the Queen’s second specially commissioned portrait in her illustrious 63-year reign. For the first time, the portrait is imprinted off centre to increase collector interest for the coins.\nThe back side of coins is referred to as the “reverse” by coin collectors. The figure of St. George the Dragonslayer is shown classically on United Kingdom Gold Sovereigns. He is valiantly mounted on his horse and is about to slay the dragon. This design is based on the iconic Benedetto Pistrucci’s design for the 1817 sovereigns second series revived issue.\nThese coins are struck today in a single, double, and five pounds denominated Gold Sovereign sizes. United Kingdom Gold Sovereigns’ one pound denomination sizes are historically the most popular of these long-running series of coins. Their dimensions are as follows:\n- Mass: .2354 troy oz\n- Diameter: 22.05 mm\n- Thickness: 1.52 mm\n- Purity: 91.70% gold\nUnited Kingdom Gold Sovereigns Pricing\nThe face value of United Kingdom Gold Sovereigns is one pound, with bigger versions being struck in two and five pound denominations. These coins are still legal money in the United Kingdom, as they have been since their inception in the late 1400s. Despite their lower face value, these gold coins have a much higher intrinsic value. Even the one-pound sovereign coins contain about a quarter ounce of gold. As a result, their market value is mostly determined by the immediate price of gold on global markets.\nThe intrinsic worth of gold, as evaluated in an investment or retirement portfolio, more or less equals the market value of these coins. Because of their historical collectability and year-round demand from collectors, these coins trade at a premium to spot gold. Every trading day, the market value of the coins fluctuates with the price of gold. By visiting our homepage, you can keep track of today’s gold prices.\nCan IRA Accounts Contain United Kingdom Gold Sovereigns?\nThe most essential question for retirement investors about United Kingdom Gold Sovereigns is whether or not these beautiful coins can be included in American IRA accounts in any or all of their different forms. The coins have a lengthy history of containing about a quarter ounce of pure gold with a purity of.9170. Which bullion pieces may be included in such accounts is up to the US Internal Revenue Service. To assess which coins pass muster for self-directed IRA accounts, they utilise a strict dual criterion based on gold purity and coin issue collectability.\nYou must first fill the precious metals IRA account with at least $5,000 in sanctioned gold and/or silver bullion before you can open it. You can always opt to add additional in $1,000 or higher minimum increments after this is completed. If you currently have an IRA account, you can convert it to a precious metals IRA with relative ease. Your IRA account administrator will transport your new gold to a third-party IRS-approved vaulting facility, which the IRS allows to store IRA bullion coins and bars. These investments will be managed and protected on your behalf until you give orders to sell them or take a withdrawal from your IRA retirement account.\nDespite the fact that these are one of the oldest and most reliable gold coin series, having a five-century history, the IRS prohibits them in any size, date, or denomination. The fundamental reason for this is that the coins only have 91.7 percent gold purity, which falls short of the required 99.5 percent purity. Even if they contained the required gold purity, their long-term collectibility would assure that the IRS would reject their inclusion in these accounts. Despite the fact that these historically significant and beautiful gold bullion pieces are not eligible for inclusion in your personal IRA, they make excellent complements to other forms of retirement and investment portfolios and collections. New releases can be purchased directly from the Royal Mint in Wales via their sophisticated website, while older versions can be purchased from reputable coin and bullion dealers all around the world."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:f90a9c6d-541a-4c76-b095-23fe9b7d34de>"],"error":null}
{"question":"What is purpose of SheetMetal workbench in FreeCAD?","answer":"The SheetMetal workbench provides tools to design objects made of folded sheets, such as metal cases or enclosures. It allows users to start with a flat sheet and use tools to extrude and bend faces by specific distances, radii, or angles. The workbench can also unfold the body to obtain the flat material pattern needed for manufacturing and can generate outlines for mills or lasercutting machines.","context":["Power users have extended FreeCAD with various custom workbenches and addons that are not integrated into the main FreeCAD source code but can be added to an existing FreeCAD installation.\nMost extensions can be installed from the Addon Manager, menu Tools → Addon manager. If there are specific instructions and dependencies for installing or using the workbenches, these should be noted in each workbench's home page.\nThese workbenches aren't supported by the main FreeCAD development team, so they aren't tested to work with every version of FreeCAD. Questions, bug reports, and improvement requests should be made directly to the authors of the particular workbench.\nAdd workbenches that don't fit a special category here.\n- MOOC allow you to follow interactive tutorial and make evaluation of your work directly in FreeCAD interface.\nArchitecture and construction\n- ArchTextures (github link) allows you to add basic, non-photorealistic textures to architectural objects created with the Arch Workbench.\n- BIM aims to implement a complete building information modeling (BIM) workflow in FreeCAD. It extends the Arch Workbench, and gathers many tools from other workbenches to provide an environment that is convenient and user friendly to model buildings, and work with IFC files.\n- Flamingo is a set of customized FreeCAD commands and objects that help speed-up the creation of frames (trusses, beams) and pipelines (tubes, elbows, flanges). It has utilities to query the objects, and to move and rotate the work plane on the fly.\n- FreeCAD-Timber: (github link) Structures wood, and more. This workbench is no longer being maintained.\n- geodata (github link) is an extension to import geographical information from a given point on Earth by its latitude and longitude, of from OpenStreetMap, Google Maps, Bing Map, or Here Map.\n- Reinforcement is a small extension that augments the Arch Workbench with additional Arch Rebar tools, including straight, U-shape, L-shape, bent, stirrup, and helical, to be used inside objects created with the Arch Structure tool.\n- Render (github link) is a workbench to produce high-quality rendered images, using open-source external rendering engines like Pov-ray, Luxrender, and Appleseed. Render is a replacement for the Raytracing Workbench, and uses the same templates so they are compatible. Render is completely written in Python which means it can be extended more easily by non-C++ programmers. In the future Render may also support Kerkythea, Blender's EEVEE, and OpenCascade's CadRays engines.\n- Civil Engineering is a proposed workbench that intends to combine and develop different tools of interest to civil engineers, including those working with road engineering, terrain topology and geographic data.\nAssembly and animation\nAs of FreeCAD 0.19 there is no official assembly workbench. However, several external tools have been created or are in development to manipulate objects to produce assemblies.\nNote: assembly workbenches are generally incompatible with each other. If you create an assembly with one of these workbenches, you should not use another assembly workbench to work with the same document. You should re-create the assembly with the other workbench.\n- A2plus provides tools to create multi-part assemblies. It is a fork and extension of the older Assembly2 Workbench.\n- Assembly3 (github link) is in development and requires a forked version of FreeCAD with some core changes to the source code. It is the most complex solution and supports things like interactive kinematics.\n- Assembly4 (github link) is a solution based on the enhanced expression engine and the App Link object developed in the branch of Assembly3. Assembly4 does not work with a proper constraint solver, instead it uses the expression engine to position bodies with respect to Local Coordinate Systems (LCS); this is computationally friendly, and should allow for creating large assemblies with many parts. Assembly4 is the newest of the assembly workbenches; initially it served as a demonstrator project, but since 0.19 it is now more feature complete, and it is now installable from the Addon Manager.\nThese workbenches don't use constraints to keep relationships between parts, but simply re-position the parts in space.\n- Animation contains many tools to simulate movement of parts, create sequences of pictures, and thus produce an animation. The position and rotation of objects can be changed at different times, but also other properties like visibility, transparency, shape color, and camera position. The\n- ExplodedAssembly (github link) is a workbench to create exploded views and animations of assemblies. It supersedes the ExplodedAnimation workbench.\n- lattice2 (github link) is a workbench that provides tools for working with placements and arrays of placements. It is a sort of assembly workbench but there are no constraints nor relationships. Instead, the workbench focuses in arrays of placements that can be generated, combined, transformed, superimposed, and populated with shapes. It can also create exploded assemblies.\n- Manipulator is aimed at helping users in aligning, moving, rotating and measuring 3D objects through a friendly graphical interface.\n- Part-o-magic (github link) is an experimental workbench that provides some improvements to Std Part and PartDesign Body containers (automatic grouping, visibility automation, etc.), in order to work with documents that have multiple parts with deep feature hierarchies. It provides a Body-like container for the Part Workbench, and for other workbenches that produce solid shapes. Part-o-magic does not provide assembly constraints, but the tools included may be useful in conjunction with a true assembly workbench.\n- WorkFeature (github link) is a collection of tools to produce points in different locations (mid points, extrema, center of plane, projected, etc.), axes (from two points, and others), and planes (from one point and one axis, from three points, etc.), in order to facilitate the creation of a particular 3D model. These helper points, axes and planes are also useful to position and align objects in desired places to form an assembly. This workbench is based on the older workfeature-macro, which was hosted in the macros recipes page. Currently, the macro has a bit more functionality than the workbench, but eventually the workbench will integrate all existing tools of the macro. They also differ in the graphical user interface; the macro creates a panel next to the tree view and the task panel, while the workbench provides its tools in toolbars, just like other workbenches.\n- workfeature-macro adds helper objects to position shapes along these helper objects; the WorkFeature workbench is derived from this macro. The macro contains a few objects that the workbench still doesn't have like origin, circle, bounding box, and views.\n- Autoload (github link) is a small extension that allows you to select the workbenches that should be loaded when you start FreeCAD. It can allow you to customize your system in combination with other extensions such as PieMenu, ShortCuts, and CommandPanel.\n- CommandPanel (github link) is an extension that provides a panel that can be used store tools from different workbenches.\n- Glass (github link) is an extension that shows the tree view and properties panel as a transparent overlay over the 3D viewport.\n- IconThemes (github link) is an extension that provides the ability of changing the icons of the default FreeCAD system.\n- Launcher (github link) is a small extension that provides a dedicated dialog box for users to search and launch commands. Instead of clicking on a toolbar button or menu entry, searching for the command's name may be faster for some users.\n- PieMenu (github link) is a small extension that shows a pie menu to select tools or commands when the key is pressed. A pie menu is an interface that appears in Blender and other systems like Android mobile phones to launch actions.\n- SelectorToolbar (github link) is a small extension that provides a point and click experience for changing FreeCAD workbenches.\n- ShortCuts (github link) is a small extension that provides a manager and overlay for shortcuts.\n- TabBar (github link) is a small extension that adds a window with tabs in order to select workbenches.\n- ToolbarStyle (github link) is a small extension that allows the configuration of toolbars, with icons, text, or both.\n- AirPlaneDesign (github link) is an experimental workbench to design wings and airplane objects.\n- Cfd (github link) brings the power of the OpenFoam solver to FreeCAD to perform computational fluid dynamics (CFD) simulations.\n- CfdOF (github link) is a fork of the Cfd workbench that focuses on ease of use; it is intended for people who are just starting in the world of CFD and OpenFoam.\n- DesignSPHysics (github link) is a workbench that provides a graphical user interface to DualSPHysics, a fluid dynamics solver.\n- EM Workbench provides a graphical interface for different solvers by FastFieldSolvers. At present it supports the 3D magneto-quasistatic impedance solver FastHenry. Support for the 3D electrostatic capacitance solver FasterCap is ongoing.\n- FreeCADTools: (github link) workbench for create metal profiles, square tubing, z profile, palette, rotation, drawing, and more.\n- GDT is a collection of tools to add geometric dimensioning and tolerancing (GDT) labels in 2D and 3D technical drawings. It implements the standard ISO 16792.\n- KicadStepUp is aimed at helping both KiCad and FreeCAD users in collaborating with electrical (ECAD) and mechanical (MCAD) design. With FreeCAD it's possible to design a printed circuit board, and push it to KiCad; alternatively, the board can be designed in KiCad, it can be imported by FreeCAD, it can be edited with the Sketcher Workbench, and pushed back into KiCad. The 3D model, boards and enclosure, can be exported to VRML with materials properties for use in KiCad or Blender.\n- LCInterlocking (github link) tools to create parts for laser-cutters. Add tabs and hinges. Export to SVG.\n- pyrate (github link) is a workbench to design optical lenses. The project aims to provide an optical raytracer for isotropic, homogeneous anisotropic and inhomogeneous isotropic GRIN media.\n- SheetMetal provides tools to design an object made of a folded sheet, such as a metal case or enclosure. The user starts with a flat sheet, then uses tools to extrude and bend the faces of the object by a certain distance, radius or angle, until the desired shape is obtained. The body may then be unfolded to obtain the required flat material that would be necessary to manufacture this body. The outline of this unfolded body may be used finally as the input for mills or lasercutting machines to obtain the required sheet profile.\n- Ship Workbench (github link) is used to create structures that are common to ships. It currently is seeking a maintainer.\nInformation and data\n- CADExchanger (github link) is an extension that allows FreeCAD to import and export file formats supported by the commercial \"CAD Exchanger\" application, such as Rhino 3dm or ACIS sat, and mesh formats like OBJ and STL.\n- dxf_library (github link) downloads the files needed to support importing and exporting DXF files in FreeCAD versions before v0.16. These files are not needed anymore when using the built-in DXF importer in v0.16 and above. They are still needed if you wish to use the legacy Python importer, or if you wish to export directly from the 3D model.\n- DynamicData is an extension that allows creating container objects to hold custom properties of any type that FreeCAD supports, for example, length or placement. These custom properties can then be used in mathematical expressions just like other properties in the Sketcher Workbench or Spreadsheet Workbench.\n- InventorLoader is an extension designed to import Autodesk Inventor files. Currently only Parts (IPT) can be displayed, not assemblies (IAM) nor drawings (IDW). As Inventor files contain a complete ACIS model representation, SAT and SAB files can also be imported. Export will not be supported, neither to IPT nor to SAT.\n- kerkythea (github link) adds a simple exporter to produce XML files for use with the Kerkythea freeware renderer.\n- Reporting (github link) makes it possible to extract information from a FreeCAD document using SQL statements, and show the results in a spreadsheet. The SQL statements can be used from a graphical user interface or from Python scripts. It works in a similar way to the Arch Schedule tool but is more powerful due to the flexibility that SQL provides.\n- WebTools contains a series of tools to communicate with web services like Git, a BIM server, and Sketchfab.\n- yaml-workspace (github link) is an extension that adds an importer to load and manipulate objects from YAML files. In this way it's easier to design and check 3D parts before manufacturing.\nShapes and parts\n- BOLTSFC (github link) is an extension that allows you to use the BOLTS \"Open Library for Technical Specifications\", which is a collection of objects like nuts, screws, bolts, and so on, parametrically defined.\n- CadQuery allows users to design parametric 3D CAD models defined by the CadQuery CAD scripting API. It includes a full-featured editor with features such as auto-completion, syntax highlighting and checking, line numbering, and code folding. Example scripts are included to make the user get started with using the API. Script variables can be edited dynamically through the use of a parameter dialog. This workbench also includes cqparts, which is a library that adds support for parts and assemblies with constraints on top of CadQuery.\n- Curves is a collection of tools to create and edit NURBS curves and surfaces.\n- Defeaturing provides tools to edit STEP objects to remove features like holes, faces, and edges, and perform some operations with the simplified objects.\n- Fasteners Workbench is a workbench that provides various fasteners, screws, bolts, nuts, etc., to attach to your model. Development happens at Fasteners Github repo (github link)\n- FCGear (github link) is an extension that adds many different gears like cylindric involute, involute rack, cylindric cycloid, spherical involute bevel-gear, and crown gear.\n- frame (github link) is an extension with tools to create frames and beams, including two intersecting beams, in which one beam is cut by a plane or by another beam.\n- Lithophane (github link) is an extension to convert a provided image to a \"lithophane\" for 3D printing. A lithophane is an image that can only be seen properly when illuminated from behind.\n- nurbs is a collection of scripts for managing freeform surfaces and curves.\n- parts_library (github link) is an extension that downloads a library of parts in Step format .step or in FreeCAD format .FCstd that can be imported into a FreeCAD document. Users can contribute content to this library by forking the repository, adding their own parts under a permissive CC-BY 3.0 license, and submitting a pull request to merge the new objects.\n- pcb (github link) is a workbench that allows the user to import and create printed circuit boards (PCB) in FreeCAD. It supports layers, colors, transparencies, importing Step and Iges models, and displaying holes and vias.\n- Pyramids and Polyhedrons Workbench or Pyramids_and_Polyhedrons (github link) is a workbench for generating pyramids and polyhedrons, fully scalable and usable like standard Parts.\n- reconstruction (github link) provides utilities to reconstruct models from images.\n- retr3D (github link) is a framework designed to model and manufacture 3D printable parts starting from electronic waste, in order to build more 3D printers. The intention of this project is to recycle e-waste, promote 3D printing, especially in developing economies, and in this way reduce the amount of waste that goes to landfills. Using locally built, and inexpensive 3D printers could also be a way of localizing manufacturing, thus stimulating the local economy of these regions.\n- Silk is a collection of NURBS surface modeling tools focused on low degree and seam continuity. Silk is the new name of the NURBSlib-EVM project.\n- SlopedPlanesMacro (github link) allows you to build figures controlling the slopes of the faces of objects.\n- symbols_library (github link) is an extension that downloads a library of SVG symbols that can be used in FreeCAD, particularly in the TechDraw Workbench to produce technical documentation. Users can contribute content to this library by forking the repository, adding their own symbols under a permissive CC-BY 3.0 license, and submitting a pull request to merge the new objects.\n- ThreadProfile (github link) allows to easily create parametric 2d thread profile object compatible in Part and Part Design workbenches.\n- timber (github link) is a workbench to produce wood and timber objects.\n- Assembly2 provides tools to create multi-part assemblies. It is unmaintained since 2016. Consider using A2plus instead.\n- cura_engine (github link) is an extension that integrates CuraEngine into FreeCAD in order to facilitate gcode generation for 3D printing. This addon is unmaintained since 2014 and no longer works with recent versions of CuraEngine.\n- Drawing Dimensioning adds powerful dimensioning and annotation tools to the Drawing Workbench. It was deprecated in FreeCAD 0.17. Consider using TechDraw Workbench instead.\n- NavigationIndicator (github link) is an extension that adds an indicator for the mouse navigation style in the status bar. Since FreeCAD 0.17 this extension is obsolete, as the indicator is included natively in FreeCAD.\n- persistenttoolbars (github link) is a small extension to keep the toolbars in their locations. Since FreeCAD 0.17 this extension is obsolete, as the functionality is included natively in FreeCAD.\n- pluginloader (github link) is a small extension that allows the user to install macros, external workbenches, and other extensions in FreeCAD. Since FreeCAD 0.17 this utility is obsolete, as this functionality is already provided by the Addon Manager.\nTranslating external workbenches\nSee the wiki page for more information Translating an external workbench\n(April 2019) If you are the developer or a user of a new workbench, add it below. However, consider creating a page for it, or providing a link to its documentation in the proper section above."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:82d4b636-c74f-4231-975c-f3a445271cea>"],"error":null}
{"question":"what happen if use weight 4 yarn with small needles vs recommended size?","answer":"When using weight 4 (medium/worsted) yarn with smaller needles than recommended (smaller than US size 7-9), you'll get a tighter, denser, and stiffer fabric with less drape. While the standard gauge for worsted weight yarn is about 4.5-5 stitches per inch on recommended needles, using smaller needles will create more stitches per inch. This will result in a smaller finished project than what a pattern specifies, and the fabric will be warmer but less drapey. Following the recommended needle size is especially important for beginners to achieve the expected outcome.","context":["Yarn Weight Categories 101\nDiscover the different knitting needle sizes, yarn types, and other knitting basics with this helpful printable graphic.\nWe are adding the pattern to your Knitting Patterns.\nThis pattern has been saved to My Knitting Patterns.\nThis Yarn Weights Chart is an infographic with everything you need to know about knitting needles, yarn types, and projects. This printable graphic includes the yarn weight numbers from 0 to 7, the category, from lace to roving, the knitting needle sizes, in both US and metric measurements, as well as the ideal projects for each type of yarn.\nThis printable graphic is so helpful when looking for free knitting patterns for beginners who don't know much about yarn weights and related information regarding knitting. It's such a handy page to have on hand.\nYarn Weights Chart\n|Yarn Weight||Name||Needle Sizes|\n|0 - Lace||Lace, Fingering 10-Count||000-1 (US)\n|1 - Superfine||Fingering, Sock, Baby||1-3 (US)\n|2 - Fine||Sport, Baby||3-5 (US)\n|3 - Light||DK, Light Worsted||5-7 (US)\n|4 - Medium||Worsted, Aran, Afghan||7-9 (US)\n|5 - Bulky||Chunky, Craft, Rug||9-11 (US)\n|6 - Super Bulky||Bulky, Roving||11-17 (US)\n|7 - Jumbo||Roving||17 + (US)\n12.75 + mm\nAbout Yarn Weight Categories\nThe most common way to distinguish between different types of yarn is by weight. If you have knit in the past, or even if you have simply glanced over a knitting pattern in the past, you probably realized the yarn weight is indicated. Not surprisingly, the weight of the yarn coincides with the thickness of the pattern. A pattern that calls for superfine yarn will not be as heavy as one that calls for bulky yarn. Always consider the weight of the yarn prior to beginning a pattern, because you will have a good indication of the thickness and texture of the item even before you even cast on. Below, we have outlined different yarn weights and how they differ from one another.\nA Note On Ply\nYarn weight and yarn ply do not correspond. A 4-ply yarn with thin plies can easily by thinner than a 1-ply yarn with a thick ply.\nWeight 0: Lace (Fingering 10-Count)\nWeight 0 is the lightest weight of yarn. Lace yarn is ideal for making delicate items such as doilies and other lace designs. The fragile and intricate nature of lace yarn requires you to treat it gently in order to avoid unnecessary tangling or breakage. Handle lace yarn with care and you will be greatly rewarded with beautiful and feminine designs you will treasure forever.\nWeight 1: Superfine (Fingering, Sock, Baby)\nMuch like weight 1 yarn, superfine yarn is great for lace and projects like baby clothes, socks, and shawls. The only major differences between this yarn and weight 0 is going to be whatever a particular pattern calls for. They are both so small that you could easily substitute it for weight 0 without noticing much of a difference. If you do this, however, be sure to use the proper needle size.\nWeight 2: Fine (Sport, Baby)\nSport weight yarns, or \"fine\" yarn is also great for baby clothes, socks, and slightly bulkier lace. Many designers prefer to use weight 2 yarn for lace because you can more easily see stitch definition, and it's faster to work.\nWeight 3: Light (DK, Light Worsted)\nOne of the more popular yarn weights, DK weight (which stands for double knit) is very versatile. When working with this weight of yarn, be sure to cast on and off loosely. Keep in mind that garments made with this yarn might not be appropriate for cold weather, because this type of yarn is not particularly thick or warm.\nWeight 4: Medium (Worsted, Aran, Afghan)\nMany, many knitting patterns call for medium weight yarn. Medium weight yarn is also known as \"worsted.\" This yarn weight is popular among knitters of all skill levels. Worsted weight yarn provides excellent stitch definition in everything from hats, scarves, mittens, and sweaters. If you knit chunky stitches in a traditional Aran yarn of this weight, the warmth of the fiber can be enhanced. If you're a beginner knitter, you will certainly want to familiarize yourself with medium weight yarn, because it is a mainstay in the knitting realm.\nWeight 5: Bulky (Chunky, Craft, Rug)\nIf you're looking to work up a substantial piece with a lot of weight to it, bulky yarn is the right choice. Materials of this weight produce fast projects on big needles, so you won't need to worry about working on a pattern for weeks and weeks before completion. Chunky scarves, blankets, and throws are perfect examples of patterns that use bulky or super bulky weight yarn. If you are a beginner knitter, using a higher category of weight is ideal, because it produces projects at a quick pace. On the other hand, advanced knitters will enjoy this yarn if they want to create something unique with a novel type of yarn. In order to achieve optimal loft, be sure to knit loose, large stitches. Unevenly spun yarn such as boucle, slubby, or chenille yarn will result in uneven knits, as well as a reduced stitch definition.\nWeight 6: Super Bulky (Roving)\nWe all get a little lazy sometimes, okay? If you're an impatient knitter, like we are, you're gonna fall in love with weight 6. We like to use this yarn when making hats because they're always going to be super warm, and you can make them in an afternoon, like this pattern.\nWeight 7: Jumbo (Roving)\nWhile jumbo yarn is not always roving, it's often referred to as roving. This weight of yarn is most commonly used in arm knitting projects and super thick blankets.\nPrint This Chart\nTo further complicate things, there are slightly different ways of classifying yarn between different countries. The biggest difference by far is the use of “Aran yarn” in the United Kingdom. Aran yarn is roughly equivalent to worsted weight yarn in the United States, though it is generally slightly thicker than most US worsted weight yarns. Historically, yarns weights were classified by plys in the UK. It used to be that a 2-ply yarn was roughly equivalent to a lace weight, while a 3-ply yarn might be similar to a DK weight.\nThe UK yarn industry has, for the most part, moved away from this way of denoting yarn weight, since it is now much easier to make thinner yarns with multiple plys. If you are traveling between countries, or using a pattern written in another country, you might want to double check that the suggested yarn for the pattern is the size you think it is. Most yarn has a suggested gauge printed on the label, if you’re in a pinch this is a good guideline to use if you also happen to have a chart that contains suggested gauges, like the one from the Craft Yarn Council.\nAt the end of the day, it is most important to choose the yarn best suited for your particular pattern. In most cases, the pattern will instruct you on the right weight yarn and needle size to use, but now you won't feel totally confused when you see funny words like \"worsted,\" \"aran,\" and \"DK.\" Plus, when you're knitting from scratch, it's important to consider certain features of the item. If you want the item to keep you warm in the cold, perhaps consider bulky or super bulky weight yarn. If you want your scarf to breathe in hot weather, then lace might be your best bet.\nWith so many knitting yarn options, it's easy to see why the types of pieces you can knit are virtually limitless. Take this knowledge of different yarn weights and start experimenting - you'll never know what you come up with!\nFree projects, giveaways, exclusive partner offers, and more straight to your inbox!\nTags / Related Topics\nYour Recently Viewed Projects\nImages from other crafters\nProject of the Day\nThe name says it all when it comes to these Unbelievably Easy Cabled Ear Warmers. Cabling is one of the easiest things you can do in… Continue reading: \"Unbelievably Easy Cabled Ear Warmers\"\n- Beginning Knitting Supplies List\n- Yarn Weight Categories 101\n- How to Stop Stockinette from Curling\n- The Knitting Dictionary\n- 9 Knitting Apps All Knitters Should Have\n- Knitting Needle Sizes 101\n- 61 Ways to Store Your Knitting Needles\n- What is the Best Yarn for Hats?\n- A Yarn Hoarders Guide to Organization: Knitting Storage Solutions, Simple Knitting Patterns and More Free eBook\n- 12 Knitting Tips for Beginners\nOur Newest Patterns & Articles\n- Braided Bridge Cable Scarf\n- Aspen Mountain Throw Blanket\n- Bobble And Ribs Coffee Coat\n- 36 200-Yard Knitting Patterns\n- Crayon Box Sampler Afghan\n- Brendan Woods Sweater\n- Baby Archie Bear Hat\n- All About Hearts Scarf\n- Velvety Soft Knit Sweater Pattern\n- Broken Stripes Hat\n- 15 Free Cute Knitting Patterns for Every Season\n- 24 Quick and Easy Knitting Patterns\n- 9 Free Knitting Patterns Perfect for Spring\n- A Knitters Gift Guide: 8 Homemade Gift Ideas\n- A Yarn Hoarders Guide to Organization: Knitting Storage Solutions, Simple Knitting Patterns and More\n- Easy Lace Knitting Patterns\n- How to Knit for Beginners: 9 Free Tutorials\n- The Best Light & Lacy Knit Scarf Patterns: 7 Free Scarf Patterns for You","In knitting, the gauge is a crucial factor. It determines the number of stitches and rows you need to make per inch. This depends on the yarn weight or size and the needle size you use. Most knitting patterns give you the appropriate gauge to use. This is to ensure that you achieve the right size and drape of the project.\nIf you use small knitting needles, you often get smaller and tighter stitches. This is even more obvious when you use small needles on big yarns. There is no strict rule in gauge, especially if you are not using a pattern.\nSeasoned knitters use the needle size and yarn according to their preference. But for novice knitters, it is still best to use the right gauge in your needles and yarns. This is to make sure you get to finish the project according to your expected outcome.\nThere are important key points to keep in mind when using small needles.\n- Small needles tend to create smaller stitches regardless of your yarn weight.\n- Using small needles make tighter and denser fabric.\n- Knitting with small needles makes stiffer and warmer fabric.\n- Small needles use less yarn because of its smaller stitches.\n- If you are a novice knitter and following a pattern, make sure to follow it to a tee. If you do not use the prescribed gauge, your finished product will be different from that of the pattern. Say you use smaller needles than the prescribed ones. Expect that your finished project will be smaller.\n- Using small knitting needles on heavy yarn creates a fabric with less drapability.\nKnitting gauge does not prohibit you from using smaller needles. You still have the option to do so. The gauge only serves as your guide on what needle to use per needle size.\nBut if you want denser and stiffer fabric, you may opt to use a size smaller than the suggested gauge. What you need to ensure is the size of your finished project. See to it that you make the necessary adjustment with your cast on.\nList of Contents:\n- How Does Needle Size Affect Knitting?\n- What Happens If You Knit With Two Different Size Needles?\n- How Do You Knit With Small Needles?\n- Do Smaller Knitting Needles Use Less Yarn?\n- What Knitting Needles Are Best For Beginners?\n- What Is The Smallest Size Knitting Needle?\n- Is It Easier To Knit With Big Or Small Needles?\n- Does Using Larger Knitting Needles Use Less Yarn?\nHow Does Needle Size Affect Knitting?\nMost knitting patterns prescribe the needle size and yarn you need. These details allow you to finish the design looking like a complete replica of the pattern. This means your finish project will have the same size, look, and texture as described in the pattern.\nThese factors are the reason why you need to use the right gauge for your knitting. Using the wrong needle size to your pattern can result in difficulties. It can lead to the wrong size or length of your finished project.\nThese are the three effects of needles size to your knitting.\n- The garment or fabric will not fit.\n- You will either run out of yarn or have leftovers.\n- Your finished project might not look as pictured in the pattern.\nIf you use smaller needles, your finished design will be smaller than expected. Say the pattern asked you to use needle size 11, but you rather use a 10.5. This will make your finished project smaller than what you expect.\nEven if you use the yarn and you follow the number of cast-on specified in the pattern, your output is smaller. Keep in mind that a smaller needle size creates smaller and tighter stitches.\nThe opposite effect happens when you use bigger needles. Say you use size 13. Your finished project will be bigger and looser because your stitches are bigger.\nStill related to the example mentioned above. Say you use a smaller needle for the project. Your project will be smaller and so you will have leftover yarns.\nMeanwhile, if you use bigger needles, you will have a bigger fabric. This will mean you might run out of yarn to complete the pattern.\nIf you deviate from the pattern, your completed project is a bit different from the pattern. The size, texture, and drape are not the same as described or pictured in the pattern.\nIf you are using a pattern, make sure you follow its recommendations. In case you want to deviate, make sure you know how to adjust either the yarn weight or the length of the design. This can be tricky for beginners. So, if you are a novice knitter, stick to following the pattern first.\nWhat Happens If You Knit With Two Different Size Needles?\nKnitting with two different needle sizes often refers to Condo Knitting. This is simple yet unique. Both seasoned and novice knitters can do this technique. This creates lacy look stitches that are loose and drapey.\nIf you are going to use this technique, make sure to cast-on with the small needle. Then, use the large needle to knit every stitch. This technique is like doing a Garter stitch but with 2 different needle sizes.\nThere are two important factors to consider if you want to use this technique.\n- Compatible needle sizes\n- Compatible needle sizes\nWhen doing this Condo Knitting, use a soft yarn. It is also advisable to use thin or medium weight yarns. This will give your fabric that soft and drapey finish. If you want to use chunky yarns, make sure your needles are both on the bigger sizes.\nThis technique calls for two different needle sizes. This means you will have to use straight needles. There is no specific pairing for this.\nYou may need to test your chosen needles on your yarn and see if they give your preferred stitches. For instance, you may pair needle size 7 and size 19 needles. Check if the combination works well with your yarn.\nThis technique works on soft drapey shawls, scarves, and loose cardigans.\nHow Do You Knit With Small Needles?\nAs explained in the previous chapters, the gauge is important in knitting. Whether you are following a pattern or not, you will have to figure the gauge that works well for you.\nIf you are a tight knitter you may need to use smaller needles to achieve your desired fabric tightness. If you want to create loose and drapey fabrics then bigger needles will help you achieve this.\nEven if you have a pattern, it does not mean you cannot tweak it. You may use your preferred yarn weight or needle size. The gauge indicated in the pattern is only a recommendation. You still have the option to use your desired gauge.\nWhat you need to remember is that worsted, chunky, and bulky yarns work on bigger needles. The fingering, DK, and other thin yarns work well on small needles. If you are going to use worsted yarns, use a needle size range of 7 to 9.\nThis needle size range can give you around 4.5 to 5 stitches per inch. If you use size 8 on worsted yarn, you are likely to get 4.5 stitches per inch. When this seems too loose for you then switch to size 7.\nThis will not create a drastic change in your design. The key is to either scale up or down to one size only. You tend to have too small or too tight fabric if you scale down your needle to two or three sizes.\nFor DK yarn weight, the needle size range of 5 to 6 works best. Again, if you use a smaller size needle such as size 4, you will have tighter knits. The logic is simple. Use a smaller size needle than the recommended if you want tighter or stiffer output.\nFinding your comfortable gauge may take some swatches or testing. It does not always have to conform with the standard gauge. Use your preference as your guide for your knitting gauge. As long as you keep track of your project size, finding the right needle size will not be a problem.\nDo Smaller Knitting Needles Use Less Yarn?\nIn theory, smaller needles will use a lesser amount of yarn. That is if you use the same pattern and do not adjust the length and cast-on of your design.\nFor instance, the pattern recommends that you use a size 8 needle on a 200 cast-on pattern. You know that size 8 will give you 4.5 stitches per inch. This means that the 200 divided by 4.5 is 44.44. The length of the pattern using this needle size is 44.44 inches (113 cm.).\nIf you use size 6 instead of 8, your stitches per inch are now 5.5. Here is how the changes happen. If you divide 200 by 5.5, your quotient is 36.36. This means that by using a size 6 needle, your design will now be 36.36 inches (92 cm.).\nThis shows that smaller needles use a lesser amount of yarn. But, you have to consider that your design also got smaller. This proves the theory but it does not mean you will get the same result pictured in the pattern.\nIf you will use a smaller needle but want to achieve the same size given in the pattern, then adjust your cast-on.\nWhat Knitting Needles Are Best For Beginners?\nMedium and large needles are suitable for novice knitters. The same goes for yarns. If you are a beginner, you may start with medium-weight and chunky yarns. Bigger needles and yarns will allow you to work easier and faster.\nA good needle size range is 7 to 9. Needles with these sizes work well on worsted, chunky, and bulky yarns. These needles are great training needles. They make bigger stitches and are less straining to your fingers.\nThese needle sizes are perfect for making warm and thick scarves, shawls, and cowls.\nWhat Is The Smallest Size Knitting Needle?\nIn the US standard, the smallest knitting needle is 0. This is the metric counterpart of size 2 mm. This needle size works on thin yarns. You can use this for knitting intricate lace patterns.\nOnly experienced knitters use this needle size for making lace designs. This can be challenging because your yarn is also very thin.\nIs It Easier To Knit With Big Or Small Needles?\nNovice knitters will find it easier to knit on medium to large needle sizes. Small needles can be tricky because you also have to work on smaller yarn. This combination can be challenging if you have not gained advanced knitting skills.\nDoes Using Larger Knitting Needles Use Less Yarn?\nThe answer depends on the yarn weight you use. If you use chunky and bulky yarns on large needles, the amount of yarn is proportionate to your design.\nBut if you use thinner yarns on knitting needles then you will use more yarns. Bigger needles make bigger and looser stitches. This means it will have to use up longer yarn to make one stitch.\nThis is why knowing the right gauge is crucial in knitting. The principle is easy to understand. Small needle size makes smaller and tighter stitches. Bigger needle sizes create bigger stitches.\nOnce you know these, you can defy your pattern and use your desired needle size. You will know how to work on your number of stitches to ensure your project will have the right size.\nKnitting should not be a difficult craft. These needle sizes are mere recommendations for your guidance. You can still follow your personal choice.\nYou can use your preferred needle size according to your chosen yarn. You can make your project tighter or looser than your pattern. There is no hard rule in knitting. What you need is to understand the logic in these recommendations.\nYou have to keep in mind that these needle sizes were not made out of thin air. Each size works for a specific yarn weight. It is all up to you if you want to follow or deviate from it.\nAny changes from your knitting pattern have corresponding effects. If you are not yet an experienced knitter, make sure to follow your pattern first. This is to make sure that you complete your project accordingly.\nDeviating from your pattern can lead to mistakes and frustration. You, too, can tweak your patterns once you become more skilled and experienced."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2d7c4fe1-7655-4002-8770-d4f99731860d>","<urn:uuid:ba073e10-2401-41dd-84de-35e0a81b9cef>"],"error":null}
{"question":"How do modern CB radios and ancient soapstone vessels demonstrate different communication and survival technologies in the Arctic?","answer":"In contemporary Arctic life, CB (citizens band) radios serve as a crucial short-distance communication tool, especially since many northern residents have limited access to broadband or telephones, as depicted in Annie Pootoogook's artwork. In contrast, ancient Dorset peoples used soapstone vessels and lamps for survival, which allowed them to burn seal oil for heat and light without depending on wood for fuel. These technological adaptations from different eras show how Arctic peoples have developed various solutions for communication and survival needs over time.","context":["Annie Pootoogook’s Man Talking on CB Radio\nAnnie Pootoogook (1969-2016) was a prolific artist well known for her distinctive style and approach to drawing which addresses themes of contemporary Inuit life. Born in Kinnigait (Cape Dorset), Canada, Pootoogook came from a family of artists; she is the daughter of Napatchie and Eegyvudluk Pootoogook, and the granddaughter of renowned artist Pitseolak Ashoona. Pootoogook was influenced by her mother’s graphics and the detailed drawings and prints of her uncle, Kananginak Pootoogook.\nKinngait, Nunavut (Cape Dorset) also known as “Sikusillaq’ in Inuktitut, refers to the area of nearby seawater which remains ice-free all winter. It has justifiably been called the most artistic community in Canada, with about 22% of its labour force employed in the visual arts. Founded in 1959, Kinngait Studio houses the co-op’s graphic arts program, which produces a highly acclaimed print collection each year. Cape Dorset is fondly known locally as the true ‘Capital of Inuit Art.’\nBefore printmaking was introduced to the Inuit of Cape Dorset, artists there were sculpting using local stone. In 1970 Cape Dorset artists received their first Lithographic press and Nova Scotia artist Wallace Brannen worked in the community starting in 1974 to teach Lithographic techniques. The theme of a strong connection between Inuit and the natural world is powerfully described in the work of Cape Dorset artists and printmakers. Frequent subject matter in Inuit prints are the land, animals and the ocean. In the early 2000s, a new group of Cape Dorset artists began to create increasingly daring, reflective and eloquent drawings which have re-defined subject matter in contemporary Inuit art by focusing on scenes of everyday life in the present. These artists presented visions of their Inuit heritage (material, physical, cultural traditions, stories, geographies) while also working outside of the norms that southerners have come to expect from Inuit art.\nAnnie Pootoogook is known as a major catalyst for the shift in subject matter and interest for the new generation of artists living and working in Cape Dorset. It is not only her choice of subject matter, but also the medium (drawing) which shows her commitment to this changing tide. While older generations created drawings which were chosen and then drafted into lithographic prints to be reproduced and sold to a Southern market, the medium of drawing (not to be reproduced) is both a much more immediate connection to the artist’s hand as well as a break in the traditional and often exploitative print market between Inuit artists and Southern markets.\nPootoogook’s drawing titled Man Talking on CB Radio (2003-2004) combines many of these new stylistic aspects and explores her personal experience as a woman living and working in the changing face of the North. In this drawing, we see a family depicted inside of a prefabricated home in Cape Dorset. As is the reality in contemporary Inuit north, this family is shown wearing a mixture of traditional Inuit clothing and non-traditional clothing. The woman on the left is shown in amauti, a traditional parka with a built-in baby pouch against their mother’s back and the man is shown seated speaking into a CB radio wears a contemporaneous puffer-style parka and corduroy jeans.\nThis drawing illustrates the mundanity of everyday life in the North while also paying great attention to detail. Pootoogook has included every screw in the small table that the CB radio sits on, as well as the sparsely decorated walls of the house, a clock, a small flower taped to the wall and a figure in the kitchen opening the refrigerator. CB radios (or citizens band radio) are used as a common short-distance communication between individuals as many residents in the North may have limited or no access to broadband or telephones. Pootoogook’s work tells stories of lived experience, combining elements of contemporary Inuit life and culture with influences derived from southern industrial society.\nPootoogook presents a view of the North that was not depicted in the prints and drawings by previous generations of Cape Dorset artists. Man Talking on CB Radio (2003-2004) serves to challenge Southern ideas of what contemporary life is like in the North and invites us into a world which broadens our conversations about the ways in which Inuit culture is represented in Canada and abroad.\nMan Talking on CB Radio (2003-2004)\nwax pastel and ink on Ragston paper\n45.1 x 66.5\nPurchased with funds made available from the Jane Shaw Law Endowment Fund, 2007 2007.111","The term \"Palaeo-Eskimo\" (palaeo=old) is used to refer to the peoples of the Arctic who lived before the Thule. The Thule were the direct ancestors of the Inuit who now inhabit the Canadian north. Palaeo-Eskimo peoples may be remotely related to the Inuit, but they are not the direct ancestors of any modern Arctic people.\nPalaeo-Eskimo culture appears to have had its origin in Alaska a little more than 4,000 years ago. The first Palaeo-Eskimo people to arrive in the Canadian high Arctic were probably the Independence I people, named after Independence Fjord in northeast Greenland where their artifacts were first described.\nIn Newfoundland and Labrador, many archaeologists divide Palaeo-Eskimo precontact period into two major phases: the \"Early Palaeo-Eskimo\" phase, lasting from about 3800 years BP (Before Present) to about 2200 BP, and the \"Late Palaeo-Eskimo\" phase, running from about 2500 BP to sometime between 1000 BP and 500 BP. Despite the fact that the Early Palaeo-Eskimo culture phase extends into that of the Late Palaeo-Eskimo phase, archaeologists disagree as to whether the latter phase is derived from the earlier.\nThe Early Palaeo-Eskimos\nThe earliest Palaeo-Eskimo artifacts in the province were first found in Saglek Bay, in northern Labrador, and date to about 3800 years ago. Newfoundland archaeologist James Tuck argues that the first of these Palaeo-Eskimo peoples to be identified in Labrador bears a strong resemblance to the Independence I culture of Greenland and the high Arctic. These people left behind very distinctive tools that are often made of bright, almost jewel-like cherts, and are quite small in size. (Their small size led earlier archaeologists to refer to them as belonging to the \"Arctic Small Tool Tradition\"--a term that is now seldom used.)\nThe tools used by these Early Palaeo-Eskimos include harpoons tipped with tiny stone end blades that are often serrated (like a modern kitchen knife), small projectile points that are probably arrowheads, scrapers, used to remove fat from skins, small stone knives, and burins--thin stone tools used to make grooves in bone and wood. Small adazes are also sometimes found on Palaeo-Eskimo sites which suggests that these people were also working wood. Like all Palaeo-Eskimo peoples, Early Palaeo-Eskimos also used microblades, perhaps the most commonly found artifact on Palaeo-Eskimo sites. Microblades are small, sharp stone flakes that might have been used almost like disposable pocket knives, or razor blades.\nThese Early Palaeo-Eskimos lived in a variety of dwellings, some of which have been identified by archaeologists from the Smithsonian Institution, working in northern Labrador. One such structure was bilobate in form and was probably covered with skin stretched over a framework of wood or perhaps walrus ribs. These houses had a pavement of flat stones and a mid-passage hearth constructed of upright slabs of stone set in the ground.\nSurprisingly, the Smithsonian researchers found that these people tended to camp in sheltered inner areas along the coast of northern Labrador, rather than on the outer islands and headlands, even though one would expect marine birds and mammals to be more plentiful in the outer zones. It has been suggested that the earliest Palaeo-Eskimo peoples spent the spring hunting seals in the more exposed outer areas, and the summers further up the bays fishing, birding, and hunting caribou. In the fall, these hunters would have undoubtedly taken harp seals on their southward migration, while in the winter it is possible that they picked out sheltered areas from which to take the occasional caribou and to live on stored food.\nBetween about 3500 BP and 3000 BP, this early Palaeo-Eskimo population in Labrador appears to have experienced a population loss, perhaps because of competition from Maritime Archaic Indian peoples. There were, however, Palaeo-Eskimo peoples in northern Labrador and on the island of Newfoundland during this period, and archaeologists often refer to these people as the \"Pre-Dorset\". Labrador, however, has relatively few Pre-Dorset sites; they are much more numerous in the Arctic, particularly in the low eastern Arctic. These Pre-Dorset people in the Arctic, at least, appear to have been more numerous than their Independence I predecessors, perhaps because of a superior technology, especially a very effective type of harpoon called a toggling harpoon, which was much more efficient than the older forms. A device like this could mean a much higher success rate in taking animals, and hence a more dependable food supply, and thus perhaps a larger human population.\nAround 3000 BP there is evidence of a rapid population growth in Newfoundland and Labrador, due to the emergence of a new culture which archaeologists call the \"Groswater\", named after Groswater Bay on the coast of central Labrador. Many of the tools used by the Groswater people are similar enough to those of the earliest Palaeo-Eskimos that we can be reasonably sure that the Groswater culture is derived from that of the earlier Palaeo-Eskimo population. One possible reason for the success of the Groswater culture may be the demise of the Maritime Archaic people in Newfoundland and Labrador around 3200 BP. By about 2200 BP, however, the Groswater people disappeared from the island of Newfoundland, and not long after they vanished from Labrador. The Groswater demise is not unusual in the precontact period of Newfoundland and Labrador and like other extinctions, may have been the result of a number of years when hunting was so poor that local bands died out.\nThe Late Palaeo-Eskimos\nAfter a few centuries, however, we see the appearance of Late Palaeo-Eskimos, a people often referred to as the Dorset in the archaeological literature. Dorset culture may have originated in the Foxe Basin area between the mouth of Hudson Bay and Baffin Island, and it was much more elaborate than Early Palaeo-Eskimo culture. Dorset peoples used soapstone vessels and lamps (which means that they were not dependent upon wood for fuel; they could burn seal oil for heat and light in these soapstone containers). There is also evidence that they made sleds, perhaps pulled by hand, rather than by dogs, and some evidence that the Dorset built kayak-like boats. Objects interpreted as snow knives have been recovered from Dorset sites, and this suggests that they may have known how to construct the snow houses, popularly known today as igloos. All of this indicates a people who were more oriented toward the sea than previous Palaeo-Eskimo cultures, and this suggestion is borne out by the location of their sites, a high proportion of which are located on exposed headlands and outer islands.\nMany of these sites, such as Port au Choix, recently excavated by Memorial archaeologist, Priscilla Renouf, are quite large and show evidence of a long-term commitment to place. Renouf has excavated huge amounts of harp seal bones at Port au Choix, indicating that this place was a prime location for the hunting of these animals. Their sophisticated hunting technology may be the reason why Dorset sites are so numerous on the island of Newfoundland - in fact, in terms of population, they may have been the most numerous Aboriginal people ever to occupy the island.\nDorset culture disappeared from the island, however, by about 1200 years ago, and from northern Labrador sometime between 1000 and 500 years ago. Indeed, the Dorset people vanished entirely from Greenland and the Canadian Arctic, the majority (at least those outside of the island of Newfoundland) perhaps displaced by the Thule, the ancestors of today's Inuit. It is possible that some of the Dorset people in the Arctic merged with the Thule, but there is little evidence that this happened. Dorset extinction in Newfoundland was probably due to other factors--likely a repeated failure in either the caribou or the harp seal hunt.\nOne of the characteristics of late Dorset culture is an abundance of carved objects--many of them astounding in their realism and power. In the Arctic, Dorset artists, using ivory, bone and wood, carved bears, fish, birds, human faces, all in a remarkable style not previously seen in the region. In Labrador, the preferred medium was soapstone, and today The Rooms Provincial Museum Division houses a remarkable collection including human figures, polar bears, birds, a human skull, and many other representations of the natural and the supernatural world. Some archaeologists believe that this artistic profusion represents a reaction to the threat of Thule encroachment--a way of establishing boundaries between the two peoples, perhaps, or possibly an indication of a people turning to the spirit world for remedies for the problems of this world."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:4c3b4371-af6c-4755-a2e0-31b9739f1b1d>","<urn:uuid:88a871fb-6349-43ee-8d93-d8d59bc13a6e>"],"error":null}
{"question":"What cleaning methods work best between crawlspace encapsulation and HVAC mold removal?","answer":"The approaches are quite different. Crawlspace encapsulation involves cleaning out the space, installing heavy-gauge reinforced poly sheeting as a vapor barrier, and insulating the walls to create a cleaner environment. For HVAC mold removal, the EPA recommends using either a mixture of household detergent and water, commercial mold removal products, baking soda-detergent solution, or borax solution, along with proper safety equipment like N95 masks and gloves. HVAC cleaning requires careful scrubbing in circular motions and thorough drying, while ensuring good ventilation. Crawlspace encapsulation is typically a more comprehensive structural solution compared to the targeted cleaning approach needed for HVAC mold.","context":["Sophomoric humor aside, houses that “breathe” through air leaks really do suck — they suck air from all the wrong places. We call this uncontrolled or unintentional ventilation, also known as infiltration.\nTHE DANGERS OF UNCONTROLLED VENTILATION\nUncontrolled ventilation simply means that your house is “breathing” from the worst possible places, such as your attic or crawlspace. If you have kept up with this blog, you probably already know that ventilated crawlspaces are a bad thing. It is common here in Kentucky to have 25 percent of your home’s air pass through your crawlspace before it comes into your living space. That’s just plain gross!\nIf it were just about cleaning out the crawlspace and making it a healthier environment, we could recommend simple crawlspace encapsulation. This entails cleaning out the crawlspace, placing a heavy-gauge reinforced poly sheeting for a vapor barrier, insulating the walls, and leaving behind a cleaner, healthier home. Crawlspace encapsulation is by far the best way to address the crawlspace, thus improving a home’s air quality, making it more comfortable, and lowering utility bills; but it is only part of the answer.\nUNCONTROLLED VENTILATION & YOUR CRAWLSPACE\nTake it from me: I have spent the last 25 years in crawlspaces, and there is some really nasty stuff down there! Insects, rodents, feces, mold, mildew, and sometimes other hazards like asbestos are all present in the average crawlspace. You certainly don’t want 25 percent of your indoor air being contaminated before it even comes into your home for your family to breathe.\nCRAWLSPACE ENCAPSULATION FOR BETTER IAQ\nAccording to a report by the Wall Street Journal, the average home’s indoor air quality (IAQ) is three times as dirty as outdoor air. No wonder our allergies are off the chart in the US! And it doesn’t stop there.\nYOUR ATTIC, STACK EFFECT & NEGATIVE PRESSURE\nWhen a house is breathing, typically the attic dictates how much it’s breathing; and how much it’s breathing usually has an impact on where it’s breathing from. Houses on a basement, for example, do not breathe from a ventilated crawlspace. This causes what we in the building science community refer to as “stack effect.”\nStack effect is the same process that allows your fireplace to vent smoke. Hot air rises, leaving a vacuum-like effect behind it and pulling smoke up with it. This process creates a negative pressure behind it, because the air leaving the space gets replaced by air from somewhere. In today’s newer homes, fireplaces are required to have a fresh air vent to the outside so that we can control where that air comes from.\nAttics often do not have controlled ventilation, so when the hot air vents out of the attic in the summer, stack effect creates a negative pressure in the house, typically from the crawlspace or basement. This negative pressure creates all kinds of problems because it pulls air from anywhere it can get it — gaps and cracks in the walls and foundation, leaking windows, and even the ground itself.\nTWO HOMES & THEIR RISK FOR RADON GAS INFILTRATION\nThese next two charts compare two Lexington area homes we worked with this past summer.\nLEFT: A TIGHT HOME AND MINIMAL RISK OF RADON GAS\nThe house on the left was a tighter home where we spray foamed the attic and sealed all the leaks we could find in the basement foundation. It had a ventilated attic with a leaky basement. The red line at the top is gas being drawn into the home from the basement slab. You can see the home on the left only had a slight variation of about 1.5 PICO/L; this is a very good number that we are happy with.\nRIGHT: A LEAKY HOME AND MAJOR RISK OF RADON GAS\nThe house on the right saw a variable of 18 PICO/L; this is a very bad number — almost 10 times as high as the tight home, and almost 5 times the recommended maximum exposure to radon. Just living in this home every day is the equivalent of smoking almost 2 packs of cigarettes a day in cancer causing lung damage! This home is literally sucking toxic radon gas out of the ground because the attic is creating such a negative pressure in the home.\nThis is another example of a contractor (in this case, the roofer) trying to add uncontrolled ventilation to a home without knowing the domino effect he was causing. Sure, he made the attic cooler, but only at a cost to the rest of the home. This house also had some mold and mildew issues that we think were also being caused by foundation air leakage bringing in too much humidity in the summer. All of this was caused by an oversized attic fan from a well-meaning roofer. Who would have thought a roofer could have that much effect on indoor air quality?\nInstead of venting attics we should be encapsulating (fully sealing) attics. By closing the attic completely and insulating the roof deck(back side of shingled roof) we never allow the outside heat in and subsequently stop the greenhouse effect that increases attic temperatures to more than 140 degrees in the peak of summer. Not only does this stop the stack effect that causes the attic to suck on the basement or crawlspace it also protects any HVAC equipment that might be in the attic. A standard air-conditioner only cools the air approximately 20 degrees, how much of that is lost running 50 ft through duct work in a 140 degree attic? Encapsulating your attic and crawlspace together is the best way to control your home’s ventilation, we have a phrase “build it tight and then vent it right”.\nSYNERGY HOME’S WHOLE HOME APPROACH\nThis is why Synergy Home takes a whole home approach. We understand the domino effect that can follow insulating and air sealing your home. Creating controlled ventilation, instead of relying on uncontrolled infiltration, can solve a multitude of issues in today’s homes. Indoor air quality, comfort, and high utility bills are all affected, and we have the knowledge and the skilled trade workers to fix these and other issues in your home. That’s why we say, “Is your home in sync?”","Signs of Mold in Your Heating & Air Conditioning System\nYou only need to do something about mold in your air conditioning system if there are obvious signs, such as visible mold or a strong mildew smell. The typical places you want to check for mold include the air ducts, intake vents, cooling coils, and drip pans.\nIf you can’t see any signs of mold, but there is a definite odor, you should call a professional to inspect your system. Mold in your air ducts or other parts of your air conditioning system should be dealt with immediately. Failure to address this problem can encourage mold spores to be spread throughout your house as the system blows air across the mold and into the various parts of the house. Do not run your air conditioning system, especially if mold is present in your air ducts or near the intake vent.\nCauses of Mold in Your Heating and Air Conditioning System\nThe two major causes of mold in your air conditioning system are condensation and organic material. Condensation occurs when parts of your air conditioning system get cold, especially when the surrounding air in the room is warm and humid. For example, if your basement tends to be humid and the air ducts get cold while the air is blowing, the water vapor in the room can begin to condense on the air duct, creating a small amount of water. If the humidity is low enough, the water will quickly evaporate once the a/c turns off. If the humidity is too high, then the water will persist and that’s when you begin to have a mold problem.\nAir conditioning systems tend to collect dust, both in the unit itself and within the air ducts. Dust contains all kinds of organic material, such as pollen and dead skin cells, which is essentially food for mold. Combined with a damp environment, this creates a great place for mold to grow.\nHeater and Air Conditioner Mold Removal\nRemoving mold in your air conditioning system can be very difficult because a lot of places are hard to access, especially areas inside your air ducts. Additionally, mold in your a/c system can have serious consequences since it easily spreads mold spores throughout the entire house. For these reasons, we recommend you call a professional mold removal contractor to tackle this job unless the mold is in an isolated area and it is easy to access. For tips on best ways to select a good contractor, check out our Hiring a Pro section.\nFor removing small areas of mold, try the following:\nStep 1- Choose a cleaning solution\n- Mixture of household detergent and water (recommended by the EPA).\n- Commercial mold removal product (always follow manufacturer’s instructions on the label).\n- Baking Soda -Detergent Solution (1/2 cup baking soda, 1 cup water, 1 Tbsp mild liquid detergent).\n- Borax Solution (1 gallon of water to 1 cup of borax, or 1 part borax to 16 parts water).\nSpecial note on Bleach: Bleach is not needed on hard, non-pourous surfaces, such as sheet metal. One of the options above will be adequate, and will also be a lot more safe.\nStep 2 – Put on Protective Clothing and Make Safety Precautions\nDepending on the severity of the cleaning solution that you chose, you will need to take some safety measures in order to keep yourself free from harm. Because you are working with mold, we recommend the following:\n- A respirator, or air mask, that is adequate for blocking mold spores from entering your lungs. The EPA recommends a N95 mask or equivalent.\n- Rubber or Nitrile gloves.\n- Safety goggles that do not have air vents in the sides.\n- If you are moderately sensitive to mold exposure, we recommend wearing coveralls to protect your skin as much as possible. If you are severely sensitive to mold exposure, we recommend getting somebody else to perform the task.\nIf you are using a product that has strong or dangerous fumes, also make sure your are working in a well ventilated room.\nTo see more on how to prepare to remove mold, click here.\nStep 3 – Apply the Cleaning Solution and Scrub\nApply the cleaning solution that you have elected to use. You can do this with a spray bottle, a lightly damp rag, or a low-abrasive brush or pad. Do not use anything abrasive, such as a wire brush.\nApply the mold removal solution generously. Let the solution sit for a few minutes, then scrub the area in circular motion with your rag, brush, or a scrub pad. Using a disposable towel, or a regular towel that you can disinfect with bleach later, wipe off the area and the excess.\nContinue this process until the mold is removed from the surface.\nStep 5 – Clean Up and Let Dry\nAfter the mold is removed, clean up the area and either dispose of anything that has had contact with the mold, or clean it with a proper detergent or fungicide.\nLet the area dry by keeping it warm and with good ventilation. If you live in an area with higher humidity, you may want to run a dehumidifier.\nStep 6 – Check for Signs of Mold\nIf mold begins to reappear in the next few weeks, it is probably because of condensation. Look for ways to improve air circulation, perhaps by opening a window or running a fan or dehumidifier. If this isn’t possible, try to insulate the area that tends to stay wet. This will help reduce the temperature difference between the ambient air, and the effected area. This will disrupt the mold growth process and prevent it from getting established. Remember, reducing moisture is the probably the number one factor in air conditioner mold removal.\nIf the mold reappears, repeat steps 1 through 5.\nPrevent Re-Growth of Mold in Your Heating and Air Conditioning System\nDepending on the situation and where the mold tends to grow, keep in mind the following principles of mold prevention in your heating and air conditioning system:\n- Change the air filter regularly, typically every 2-3 months. Filters capture fine particles, which can create a place for mold to grow.\n- Run a dehumidifier. This is helpful, especially in areas that tend to be damp and cool.\n- Insulate if possible. By insulating air ducts, especially in a damp place like crawl spaces, the air immediately around the duct will be kept at a similar temperature and will reduce the amount of condensation. This works for cooling coils also.\n- Hire a profesional to clean the HVAC system, including the air ducts. Only perform this if you know your air ducts are cluttered with dust and debris, otherwise you could be just wasting your time and money.\nWe hope this was helpful! Please feel free to leave a comment or question below. If you would like to see how other people are handling there air conditioner mold removal issues, try going to the Mold Blog’s Q&A section."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:710f91e7-074b-41f3-af45-21d08c8a37cd>","<urn:uuid:40a074a5-b883-4deb-bc97-4efd9fc6b379>"],"error":null}
{"question":"As an insurance professional specializing in agricultural risk management, I'm curious about the key differences between reinsurance coverage for insurance companies and supplemental crop insurance coverage for farmers in terms of their risk protection mechanisms and coverage levels?","answer":"Reinsurance and supplemental crop insurance differ in several key aspects. Reinsurance is designed for insurance companies to transfer liability to reinsurers, covering up to 100% of potential losses through various types like facultative (for individual assets) and treaty reinsurance (for groups of policies). In contrast, supplemental crop insurance is designed for farmers to bridge the gap above the basic MPCI's 85% coverage. Products like ARCH, RAMP, and ECO allow farmers to increase their coverage up to 95% of expected yields or revenues. While reinsurance spreads risk between insurance companies to prevent financial ruin from massive payouts, supplemental crop insurance specifically helps individual farmers protect against production and revenue losses just above or below their MPCI guarantee.","context":["What is reinsurance?\nUpdated 6:16 a.m. ET Feb. 14, 2023\nEditorial Note: Blueprint may earn a commission from affiliate partner links featured here on our site. This commission does not influence our editors' opinions or evaluations. Please view our full advertiser disclosure policy.\n- Reinsurance policies can help insurance companies cover significant losses and stay afloat.\n- They’re issued directly to other insurance companies, not consumers.\n- Several types of reinsurance coverage are available including options that cover entire groups of policies (like home or auto) and those that cover individual policies or assets.\nEver wonder how one insurance company can afford to pay out thousands of claims in the event of a major natural disaster? The answer to this question may surprise you: Sometimes it’s not one insurance company paying out all the claims, but multiple insurers. Even if you have insurance through one company, another company could actually be paying your claim.\nThere’s a concept called reinsurance out there that makes this scenario possible. Here’s everything you need to know about reinsurance and how it works.\nWhat is reinsurance?\nReinsurance is essentially insurance for insurers. This type of coverage transfers some of the liability to the reinsurer, lowering the risk for the primary insurer and freeing up capital for them to issue new policies. It’s a way for insurance companies to help mitigate their risk of total financial loss in case of a major disaster.\nReinsurance coverage can be purchased through an insurance company or brokerage that specializes in reinsuring primary insurers.\nHow does reinsurance work?\nReinsurance works by spreading the risk for insurers to multiple parties.\n“Insurance companies do an excellent job of underwriting risk and setting their premiums appropriately, but they still need to diversify,” says Herman Thompson, Jr., a Certified Financial Planner with Innovative Financial Group “It is not uncommon for an insurance company to become dominant in a particular geography, industry, etc. If one company retained too much risk in a particular city and an unusually destructive natural disaster came along, it could destroy that insurance company.”\nFor instance, an insurer might purchase reinsurance on some of its homeowners policies in an area prone to hurricanes, fires, or tornadoes. That way, they won’t be on the hook to pay out thousands of claims if a major natural disaster occurs. Reinsurance coverage could protect insurers from total financial ruin and allow them to remain solvent.\nSo what’s in it for the reinsurer? Why would they be willing to assume the risk? In exchange for assuming some of the liability, they also get a portion of the insurance premiums.\nWhat types of reinsurance are there?\nThere are four common types of reinsurance:\nWith facultative reinsurance, the reinsurer covers individual assets — often, they’re high-risk, high-value assets. For instance, the reinsurer might cover a high-rise commercial building in a hurricane zone. This type of reinsurance coverage is underwritten for each asset or risk.\nTreaty reinsurance works differently and involves reinsuring a group of policies. So in this case, the reinsurer might purchase a particular line of business from a primary insurer, like all its homeowners or auto policies. Any new policies issued in the same group would typically be covered under a treaty agreement as well.\nThere’s also proportional and non-proportional coverage available. With proportional or pro rata coverage, reinsurers cover a certain percentage of claims in the event of a loss, in exchange for receiving a portion of the premiums. With non-proportional or excess of loss coverage, reinsurers only make payouts if claims exceed a certain amount.\nThe regulation of reinsurance\nReinsurance wasn’t subject to much regulation initially, though that changed after the adoption of the Credit for Reinsurance Act in 1984. The act was spurred by concerns about reinsurers’ ability to repay claims in the event of a disaster, and was meant to help ensure the financial solvency of the reinsurance company.\nReinsurers are required to submit financial reports to the National Association of Insurance Commissioners (NAIC) or other regulators and abide by certain financial regulations. This helps protect not only the reinsurer, but also the primary insurance company purchasing reinsurance. In addition to submitting reports and following regulations, reinsurers must also be licensed or authorized to do business in the states they’re offering reinsurance coverage.\nWho needs reinsurance?\nReinsurance is for insurance companies, not consumers, so you won’t see a reinsurer working directly with a policyholder. A primary insurer might decide they need reinsurance if it makes financial sense for them to obtain it. So if they’re issuing thousands of homeowners policies in a hurricane zone, they might decide they need to reduce their liability with reinsurance coverage.\nHow does reinsurance affect insurance rates?\nOn one hand, reinsurance helps keep costs down for consumers because it spreads the risk from one company to multiple companies. For instance, if a lone company had to bear the burden of countless claims after a natural disaster, policy rates and premiums would almost certainly increase after that event. Reinsurance can help prevent this from happening frequently.\nBut on the other hand, if reinsurers see patterns in catastrophic events — for instance, if they’re often on the hook for paying out homeowners claims in a specific region — it may result in rising rates for primary insurers. The primary insurers could then increase consumer’s rates to recoup this cost.\nReinsurance could cover multiple policy types. For instance, they might choose to get coverage for homeowners, automobile, or commercial policies. The types of policies covered depend on the primary insurer’s needs and the types of coverage that the reinsurance company offers.\nBeyond reinsurance policies issued by companies, some primary insurers have turned to the capital markets for innovative financing models—like catastrophe bonds. Catastrophe bonds are high-yield securities that insurance companies use to raise funds that can be paid out if a major disaster occurs.\nInvestors buy these bonds, and if they reach their maturity date and no major disasters have occurred, the investor’s principal is returned. There’s also the benefit of regular interest payments, which can be higher than what you might see with lower-yielding debt instruments due to the risk of a natural disaster.\nBlueprint is an independent publisher and comparison service, not an investment advisor. The information provided is for educational purposes only and we encourage you to seek personalized advice from qualified professionals regarding specific financial decisions. Past performance is not indicative of future results.\nBlueprint has an advertiser disclosure policy. The opinions, analyses, reviews or recommendations expressed in this article are those of the Blueprint editorial staff alone. Blueprint adheres to strict editorial integrity standards. The information is accurate as of the publish date, but always check the provider’s website for the most current information.","All farmers are familiar with multi-peril crop insurance or MPCI – the basic crop insurance provided by the USDA’s Risk Management Agency. MPCI covers low crop yields from all types of natural causes including drought, excessive moisture, freeze and disease. Typically, MPCI provides up to 85% coverage. But for farmers coming off of a tough year, looking to expand or just wanting more protection, how do they bridge that extra 15% coverage gap?\nThat’s where a variety of additional insurance options, most from private companies, come into play and help provide additional coverage. Let’s explore some of those.\n- ARCH products are offered through Diversified Crop Insurance Services. These products allow farmers to purchase additional, un-subsidized bands of coverage up to 95% of trend Annual Production History (APH) and Optional Unit (OU), even if the underlying policy is Enterprise Unit (EU).\n- Revenue Accelerator Max Protection (RAMP), available through Farmers Mutual Hail, gives the insured the opportunity to boost revenues at specific risk levels within their risk management plans, up to 95%. RAMP supplements MPCI coverage and is designed to help provide additional coverage when production and/or revenue losses are just above or below the insureds’ MPCI guarantee.\n- Enhanced Coverage Option (ECO) is a crop insurance endorsement product that is new for 2021. It’s a government-subsidized supplemental product, available through any insurance agency authorized to provide MPCI. It provides additional county-based coverage for a portion of the underlying crop insurance policy deductible. ECO offers the insured farmer a choice of 90% or 95% trigger levels. Trigger means the percentage of expected county-based yield or revenue at which a loss becomes payable. Find out more about ECO here. To get estimates on what ECO premiums might be for your farming operation, use the Risk Management Association’s Agency Cost calculator, located online at https://ewebapp.rma.usda.gov/apps/costestimator/Estimates/QuickEstimate.aspx\nThe differences between these various supplemental products vary widely, according to crop insurance experts. Consult with your Ag Resource Management agent to find out which products could offer the most economical, effective supplemental coverage for your farming operation.\n“ECO is an area-based plan,” explains Mike Carey, ARM area manager, based in Morris, Illinois. “it’s based on results at the county level. It has some level of government subsidy, so it might be cheaper than ARCH products, but you won’t get the granular supplemental coverage you’ll get with ARCH. That’s why it’s important to sit down with us, go over your numbers, your APH and break-even numbers, to see what’s the right fit and formula for your farm.”\nIt’s important to work with partners who understand both the crop insurance side of the farm operation and the loan and finance side.\n“Crop insurance and farm loans work hand in hand,” explains Donna Swanson, ARM area manager in Iowa Falls, Iowa. “The money we are able to lend out is directly correlated to the type and amount of crop insurance a farmer has, in combination with crop prices and other factors. At ARM, that’s our specialty – custom-crafting risk management plans folding in both crop insurance and ag lending to ensure our customers keep farming.”\nAll of the Ag Resource Management insurance experts are able to provide additional information and advice about these supplemental products and how they can benefit your risk management plan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:20011662-1eed-4432-b3f3-872091a64e2d>","<urn:uuid:1bf81623-261a-490d-99ae-daa406315bea>"],"error":null}
{"question":"How do cultural events in cities impact both entertainment and environmental sustainability?","answer":"Cultural events in cities like those in Astana provide entertainment and escape from everyday life through diverse offerings like poetry evenings, art exhibitions, and sports matches. However, these events can have negative environmental impacts. When large numbers of people gather at specific locations like stadiums, it increases traffic congestion. This leads to more idling vehicles, which produce higher carbon emissions than moving cars. These emissions contribute to reduced air quality in surrounding areas, affecting human health and contributing to climate change impacts like melting polar ice caps and extreme weather patterns.","context":["ASTANA — The Kazakh capital is gearing up for a weekend packed with diverse events. Whether you’re a fan of poetry, art exhibitions, retro music, or football, Astana has something in store for everyone this weekend.\nLyrical evening Kundelik (diary) on Sept. 9\nDedicated to the works of Mukagali Makatayev, a renowned Kazakh poet and writer, the event “Kundelik” promises a soulful experience. Makatayev’s poems, often set to music, and his translations of works by Shakespeare, Dante, Pushkin, and Blok, have earned him a special place in the literary canon. His compelling, emotive work is celebrated across all age groups.\nVenue: Azerbaijan Mambetov State Drama and Comedy Theater (Nomad City Hall); 55/2 (B2.3), Mangilik El Ave. Tickets are available here.\nExhibition “Meeting Wooh Nayong in Kazakhstan: Fairy Tales with Hanbok” on Sept 9-10\nKorean artist-illustrator Wooh Nayong is known for his work centered around traditional Korean attire, the Hanbok. This two-part exhibition features the “Hanbok in Fairy Tales” series, bringing children’s classics like Cinderella and Snow White to life through Korean fashion, and the “Hanbok Girl” series, portraying girls in traditional Korean clothing.\nVideo credit: The National Museum of the Republic of Kazakhstan.\nThe exhibition also honors the Year of Cultural Exchange between the Republic of Korea and the Republic of Kazakhstan for 2022-2023.\nVenue: The National Museum of the Republic of Kazakhstan; 54, Tauelsizdik. Tickets are available here.\nRetro Concert on Sept. 9\nThe philharmonic ensembles have curated a selection of popular music spanning from the mid- to late 20th century. Arranged by the Brass Orchestra, these classics will be presented in a new light but will retain the charm and recognition they’ve garnered across multiple generations.\nThe concert program will open with the symphonic kuy “Dairabay,” composed by Kazakh musician Erkegaly Rakhmadiyev, after whom the philharmonic is proudly named. Following this, young talents from the children’s studio will perform a medley featuring some of the best-known songs from the Swedish band ABBA.\nVenue: State Academic Philharmonic named after Erkegali Rakhmadiyev; 32, Kenesary Street. Tickets are available on ticketon.kz.\nEURO 2024: Kazakhstan – Northern Ireland on Sept. 10\nSports enthusiasts should not miss this Sunday’s EURO 2024 match between Kazakhstan and Northern Ireland at Astana Arena. Come and energize the atmosphere by cheering for your team.\nVenue: Astana Arena; 48, Turan Ave. Tickets are available here.\nUnknownKazak Talks on Sept. 10\nUnknownKazak Talks is a new platform for individuals with creative and entrepreneurial interests. Its core mission is to bring together modern Kazakh talents and to foster content in the Kazakh language.\nDuring these Talks, guest speakers from diverse fields—ranging from journalism to business—share both their life journeys and professional insights. The speaker lineup includes notable figures such as Layla Sultankyzy, a TV host and journalist; Gabit Bekakhmetov, an entrepreneur and writer with affiliations to Duke, Eurasian, and Oxford Universities; Moldir Dospayeva, another TV host and journalist; Eskendir Bestai, a pedagogical trainer and MIE educator; and Dana Kanapina, the founder of La Crème Family Group and a renowned pastry chef.\nVenue: Astana International Financial Center; 55/23, Mangilik El Ave. Entry is free.","MetadataShow full item record\nPublisherThe University of Arizona.\nRightsCopyright © is held by the author. Digital access to this material is made possible by the College of Architecture, Planning and Landscape Architecture, and the University Libraries, University of Arizona. Further transmission, reproduction or presentation (such as public display or performance) of protected items is prohibited except with permission of the author.\nCollection InformationThis item is part of the Sustainable Built Environments collection. For more information, contact http://sbe.arizona.edu.\nAbstractEach year, cities around the world host thousands of different events and sports, stirring passion and providing entertainment for individuals from all around. These events attract millions of people each year to specific areas in a city to partake in or watch an attraction. These types of occurrences are significant to many individuals as it provides them with an escape from their everyday lives. Sporting events and other events like festivals, fairs, and trade shows are crucial to many cities' economies. They attract outsiders from different parts of the world, and this creates a massive uptick in economic activity during those times due to the high volume of people. Unfortunately, while these lustrous events do wonders for the cities or areas they are held, they are not sustainable and can be harmful to the environment, the surrounding neighborhoods, and the air quality. Event locations, such as stadiums, are a pivotal part of the entertainment world that allows fans to connect with celebrities and professionals face to face. Drawing thousands of individuals to a specific location has a direct tie to the increase in the amount of traffic that takes place in that area (Pyun & Humphreys, 2017). Given that cars are emitters of carbon emissions (Environmental Protection Agency, 2018), the more traffic congestion that occurs in a compact area, the more carbon emissions that will be released. Among these emissions, carbon dioxide is known to be one of the driving factors in anthropogenic climate change (NASA, 2019). Currently, impacts from climate change such as the melting of polar ice caps, rising sea levels, and more extreme patterns of weather are occurring at higher rates (NASA, 2019). When an event takes place at an isolated location, traffic jams cause cars to idle while waiting to park or exit the stadium lot. When a vehicle is idling, it uses more fuel and will produce more emissions than when the car is moving. (U.S. Department of Energy, 2015). The emissions from idling cars directly tie into the reduced air quality of the surrounding area that can have harmful effects on human health and contribute to climate change. (Zeisel, 2017) This research project sets out to determine how the University of Arizona can improve on traffic revolving around games and events to reduce the impacts on the surrounding neighborhoods, air quality, and climate change. This is an important aspect of the sports and entertainment world that can go overlooked. These events offer an enormous opportunity to promote the idea of a healthier world to the attendees as the events play a key connecting role between the people and the things they love. To achieve this, it will be necessary first to understand how the fans currently get to the campus. Then by looking at how the University plans for the traffic and discussing with professionals, it will be possible to determine alternative strategies that could assist in bettering the traffic issues. If the University of Arizona can deter enough traffic during games successfully, then this will not only help the local environment but the citizens of Tucson as well.\nDescriptionAlternative Transportation Modes for University of Arizona Events\nSustainable Built Environments Senior Capstone Project\nFeasibility of Alternative Modes"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:08e25e47-e973-480b-99a1-a70be340e14a>","<urn:uuid:ba64dd0a-8a75-4abc-b7cb-b6d659301805>"],"error":null}
{"question":"How do newcomer burnout and organizational burnout compare in terms of their causes and manifestations?","answer":"Newcomer burnout often stems from being rushed into commitments too quickly, with surveys showing many feel pressured to join committees before they're ready. People report feeling new for years and needing time to learn practices and find their place. Meanwhile, organizational burnout typically manifests in three forms: frenetic burnout from work overload (affecting 15% of employees), under-challenged burnout from unsatisfying work (9%), and worn-out burnout from daily pressures (21%). Both types can lead to withdrawal, reduced productivity, and increased turnover, though they require different interventions - newcomers need space and guidance while organizational burnout requires systematic changes like workload management and process optimization.","context":["Report of the Young Adult Field Secretary, Summer Sessions 2013\nReport from the Young Adult Field Secretary\nGabrielle Savory Bailey\nGood morning, Friends.\nFrequently this year I was asked by meetings and individuals to talk to them about how to attract more Young Adults and Families. They explain to me, how their Meeting is dying and they need to get more young people. I hear this as a very pressing and serious concern. I also travel around and witness that there are a lot of great things happening in many of the monthly meetings of this Yearly Meeting, and not only that, but there are indeed Young Adults and Families that are involved.\nIn my survey of 151 Young Adults in NYYM, 76 did not grow up as Quakers. That is huge. That means that more than half of our Young Adults are coming to our meetings with little knowledge of Quakerism. We have an amazing opportunity here. 86 of the respondents to my survey did not grow up in the meeting they attend. So even those who do grow up as Quakers move, and find new communities. There are opportunities all over for Young Adults and Families.\nThe language we use is powerful. I hear, repeatedly, how people refer to Quakerism as dying, and it really impacts me. I think that one step to growth is to stop saying we are dying. Imagine receiving an invitation. The invitation reads PLEASE COME, WE ARE DYING AND IF YOU COME WE MIGHT STAND A CHANCE AT SURVIVAL. The person then goes on to say that the gathering will be awful, no one interesting will be there, it will probably end five minutes after it starts, and you probably have better plans anyway. But if you have nothing better to do, you can come.\nI hear the concern and deep anxiety that we are dwindling in numbers and our meetings are tired, overworked, and overwhelmed with committee work, and want fresh life. It is human nature to focus on the sad, bad, and difficult. I wonder if there is a way to examine our language, and our relationship with our Meetings as they are. Friends, I believe that life attracts life. Early Friends had a vibrant movement. They were filled to exploding over the Good News they were experiencing and the Fire God was asking them to spread. And spread it did because people witnessed that there was something really moving there, and they could not wait to be a part of it. It is easy to think that when I talk about “dying” that this is about age. I have encountered deeply alive, spiritual people of all ages, some quite young, others quite old. This is not about age; we are all part of this work.\nWhat if instead the invitation you get to our hypothetical event reads, “YOU ARE INVITED TO JOIN OUR DIVINE EXPERIMENT, WHICH IS ALWAYS REVEALING ITSELF AND WILL FILL YOU WITH THE SPIRITUAL EXPERIENCES YOU LONG FOR.” Think of a great event you have been a part of. One where you really felt welcomed and the hospitality was really good. Maybe it was a party, a quiet evening by a fire with a friend, or a satisfying debate over dinner. Most likely it was a place where there was some sort of energy, purpose, passion, life, presence. Perhaps you were greeted warmly, or given a preview of what was going to happen. Perhaps you were listened to. What made that place a place you wanted to be?\nWhat if we cultivate what is ALIVE in our meetings, and practice radical hospitality to each other and to anyone we encounter? What if we see ourselves as divine hosts, and think carefully about how we provide hospitality, radical hospitality, while they are there. This hospitality, the giving and the receiving of it, is part of being present to God and to each other and manifest what is ALIVE and revealing itself to us. I have been welcomed, lovingly into many of your meetings and homes. I have heard about support committees, the loving care you give each other, the schools you oversee, the meals you share, your Quaker Quests, the groups of Young Adults that gather in fellowship and worship, intergenerational worships, new worship groups, and retreats. There are so many great things happening in the meetings I visit. This is not to say that there is not also struggle, and fatigue in some meetings. But what I invite us to do is to look deeply for the ways that we are already alive. The party is already happening, even if no one else comes.\nThere are many ways to manifest radical hospitality, and it looks different for each person. Perhaps it is making sure others are comfortable, or introducing them to other people they have things in common with. Some people are good at asking about and anticipating the needs of others. Some people show hospitality through food or keeping a clean welcoming space, some people can talk about their faith eloquently, some live in response to their experience with God, and never speak a word of it. Some make sure there is an entry point for a guest, and that they will feel tended to for the whole time they are there. How can we each manifest what is most alive in us, and share that with others, new or not?\nI often hear how long people feel new. It is MUCH longer than the meeting perceives them as new. Because meetings are so thirsty for young people and families, I think we forget that they can feel new for a long time (sometimes years). They need to learn our history, our acronyms, our peculiar practices. They might need to learn what gifts they have, forget about on what committee they will be well used. They might need to hear about our individual journeys, leadings and gifts. They might not understand this peculiar tradition, and might need a present host to guide them through, for a while; could be a long while.\nIn my survey I asked “what are your needs as Young Adult Friends and Families?” and “What are you hungry for?” I found myself excited by the answers, and hopeful. Some responses were:\n- Understanding procedures and practices.\n- Feeling welcomed.\n- “Eldering (even though we do not like to admit it).”\n- Nurturing spiritual gifts.\n- “We need connections, responsibility and Guidance.”\n- Deep worship.\n- Prayer life.\n- Hearing the spiritual journeys of others.\n- “Opportunities to explore both silence and conversations with diverse friends.”\n- “I want to journey with people into our preconceived notions and spirituality.”\n- “People expressing interest in who I am, combined with being able to see that the community is thoughtful and loving to itself and others.”\nThe responses were not limited to going where there were other young people. They mostly want connection, in this world that is more and more disconnected, and deeper spiritual discourse in this world that is more and more secular. Isn’t that what many people want, regardless of age? The good news is that there is great value and life in the practices of Friends. Many people are looking for the very practices that are central to our tradition. Whether or not anyone new comes to us, we can be alive in our faith communities.\nFriends, we have a call. We have a call to be good hosts, and to be guides, not just to new comers. We are sometimes so afraid of telling people what to believe, and what to do, that we do not share what is life-giving. And because of that fear, we do not engage people where they long to be. Many people, younger and older, that I have spoken to in my travels have talked about the importance of SOMEONE in their life who saw them, heard them, and made an impact on their life. That person may never know they had this impact. But in being present to them, they changed that person’s life, forever. They became a guidepost in their spiritual journey. I would venture a guess that you have someone in your life who did this for you. This is not age dependent. Young and old can be present to each other. We have the amazing opportunity to walk with each other, and to know each other deeply, and to answer to that of God in each other.\nI think that sometimes in our most well-meaning attempts to make people included, we rush to find them something to do. It might be helpful to allow people time to see either where they fit into what already exists, or to see that there is room for them to do work they see as alive in themselves. Here are three quotes that might give us a perspective on how our fear is read by new comers.\n- The hunger in the eyes of Quakers for youthful participation has disturbed me.”\n- My monthly meeting eagerly pursued my attendance on committee not more than three weeks after I met them, and I have been lapsed ever since.” And . . .\n- I was an attender at a meeting in Ohio and they were constantly asking me to be on a committee and give back, except I truly didn’t have the time to commit.”\nThis is in no way to induce guilt. I know that this eagerness comes from a good place. What I invite us to sit with is how our language might be coming across. What newcomers, or longtimers for that matter, might hear is that they can’t find people to serve on committees. So, why would they jump in? Would they want to end up like the very tired people they see? (remember they still feel new!) When asked on my survey why they did not do committee work, the top answers of those who responded were: 35 said they did not want to sign up for too much, 32 said they did not know what they were led to do, 31 said they do not know what is available, 18 said they do not know how their gifts would be best used.\nThis leads me to wonder, how do we portray committee work, and what is available? How do we talk to our youth, and our membership about committee work and worship? Do we portray these things as better to be avoided because they are boring and life-sucking? Are there ways to be involved in service to the meeting that does not require a committee commitment? Can we prune what is no longer alive? Do we welcome new life as it arises?\nCan we please acknowledge that there is a problem with burnout in our Yearly Meeting and our monthly meetings? Can we please lovingly address the needs of those who are doing too much? This is as much a ministry to the life of the meeting as it is the individual. This is part of radical hospitality.\nI want to say again, I witness life in our meetings! When I asked about positive experiences in my survey, here are some responses I got:\n- Powell House and other conferences.\n- Being asked to serve in a way that my matched gifts.”\n- Hearing about life stories of older Quakers.”\n- People being open, explaining, struggling with spiritual questions.”\n- Inviting me to volunteer or be part of the service of the meeting.”\n- Going to Sessions and meeting F/friends who accepted me on my own terms and engaged me in spiritually enriching and seeking conversations, and journeying through those together.”\nWe have so much to share, and we do. We have opportunities to be present to each other, and God, and share our lives with everyone that is part of our meetings. What we can do is share the witness of how God is manifesting in our lives, and in our meetings, in struggle, in service, in connections, in matters of conscience. This work belongs to all of us, at every age. How can we be good hosts and answer to that of God in all whom we encounter? How can we be good guests and engage with the party we are being offered? Do we see our whole life, and our relationships to others, as a testimony to how God is moving in us? Then, as we live into that witness, we are alive.\nGabrielle Savory Bailey\nThis item was presented at","Have you ever experienced burnout at work? The Chances are that you probably have without even knowing it.\nOccupational burnout has become such a regular occurrence in the lives of knowledge-work professionals, particularly considering today’s ever-increasing demands. Many of us aren’t even aware of how close we might be to burning out, or – worse still – we might have been in this state for a while!\nBurnouts can do serious damage to both employees and their companies in general. Stick to the end to learn more about what occupational burnout really is, what causes it, and how to avoid it yourself.\nWhat is Occupational Burnout?\nBurnout, in the context of one’s occupation, is a type of psychological stress characterized by exhaustion, lack of enthusiasm and motivation. In most cases, burnouts are accompanied by frustration and cynicism, which, in turn, lead to reduced productivity at the workplace.\nOne of the most visible signs of organizational stress is an increase in employee turnover. If employees are fearful for their positions, or feel the expectations are unrealistic, they may leave the organization rather than continue struggling.\nTurnover is a very costly process for any organization. Significant monetary costs are accrued in the process of recruiting, hiring, training, as well as general decrease in productivity.\nThe American Management Association believes the cost of finding an employee’s replacement is around 30% of that employee’s salary.\nTurnover also causes additional work and stress on other employees who have to fill in during the recruitment period, thus leading to further instances of burnout in the team.\nFor example, the burnt-out person might be that colleague who is late, for work, each morning, because he hasn’t slept well and dreads getting out of bed to head to the office; or the co-worker who stares at the computer screen for hours on end, although she never seems to focus on the task in front of them.\nThe real problem is when you have a whole team, or even an entire department of burnout people. How can you grow and reach high goals with unmotivated, and uninterested, people by your side?\nThe simple answer is: you can’t!\nThe issue will continue to develop and, as people from other departments feel the instability of their colleagues, inevitably the whole company will be negatively affected.\nWhat Causes Burnout?\nAlthough the common understanding is that burnouts are generally caused by work overload for extensive amounts of time, according to a report published by PLOS ONE there are three types of occupational burnout. Logically, each one is caused by different circumstances.\nFrenetic burnout is the stereotypical version described above, defined by workers who just have too much on their plate. These employees generally adopt a negative tone, venting about their workload.\nThis type of employee burnout, however, affects individuals who plainly feel like they aren’t getting much satisfaction out of their work. Team members experiencing this sort of burnout tend to “cognitively avoid” their work, distancing themselves from what they consider to be an unrewarding experience.\nWorn-out employees are those who struggle with the stress of the daily grind and ultimately choose to neglect their work because of those pressures.\nThe study found that 15 percent of employees in the report experienced frenetic burnout, 9 percent experienced under-challenged burnout, and 21 percent were worn-out.\nStages of Employee Burnout\nThe American psychologist Herbert Freudenberger, and his colleague Gail North theorized about occupational burnout and divided the development of the syndrome into 7 stages. Some are signs of burnout at work, while others come from personal pressures.\nCompulsion to prove oneself. In most cases, this is a result of trying to work on too many assignments simultaneously.\nWorking harder. In their desire to prove themselves to others, or try to fit in as part of an organization that does not suit them, many people establish high personal expectations and, in order to meet them, they tend to focus solely on work and take on more than they usually would.\nNeglecting daily needs. Since the individuals are already devoting all of their time to work, there is no room for anything else in life. Friends and family, eating and sleeping are no longer seen as vital aspects of life, and are just burdens which take away from the time that can be put into work.\nDisplacement of conflicts. People who reach this stage become aware that something is not right but have trouble finding the source of the problem and often start placing blame elsewhere, for example, on the team or organization.\nRevision of values. Sliding down the slope towards burnout, people’s values change. Work consumes all energy, leaving none for friends and the things they previously enjoyed. As work becomes their only focus, they become intolerant to other people’s mistakes and are never satisfied with the work of their colleagues: this often leads to internal conflicts.\nDenial of emerging problems. At some point, people affected by burnout become intolerant, aggressive, and sarcastic. They begin to ignore problems related to the project they are working on because they don’t want to deal with them and they blame it on the pressure that comes with their excessive workload.\nWithdrawal. With little or no social contact all individuals will inevitably become isolated. At that point, depression hits like a ton of bricks. The affected person disengages from team activities and communicates less with their colleagues, this inevitably leads to problems in the team dynamic.\nHow to Overcome Burnouts?\nThere is no “universal panacea” that will guarantee a remedy for ‘burnout’: however, there are a few tried-and-tested methods that might assist in its avoidance. As the condition is stress related, in order to stop it, you must find a way to reduce the sources of stress and reevaluate the way you distribute responsibilities.\nDon’t rush into starting more work than you can finish without pushing past your acceptable threshold, also avoid multitasking. This advice is followed even by the most successful entrepreneurs such as Amazon’s founder Jeff Bezos and other business leaders. Mr. Bezos is also a fan of the two pizza rule when it comes to scheduling meetings, which means that Bezos won’t call or attend a meeting if two pizzas won’t feed the entire group.\nIf you cannot pinpoint the exact cause of burnout, take the time to analyze what’s going on in your life and focus on something other than work.\nA good way to escape stress is by practicing sports. A study published by the British Medical Journal shows that regular exercise helps to reduce anxiety, boost a person’s mood, enhance productivity and improve the quality of life.\nGetting enough sleep, eating well, and drinking plenty of water further reduces the stress levels of everyday life.\nTaking time off work to recharge your batteries is also a good idea. A week with family or friends, somewhere far from the office, can do miracles for your mental health. In addition, don’t be afraid to speak out when you are overloaded and seek assistance from your team.\nHow to Optimize Your Process to Avoid Employee Burnout?\nDealing with occupational burnout on a personal level is one thing, protecting your team from recurring burnouts requires even more effort. If you are in the position of a manager, and are responsible for delivering results on a daily basis, keeping your team engaged, and efficient, is vital.\nTo achieve this, and ensure that you’re not working with a bunch of potential burnouts, you should consider changing some of your management practices and experiment with methods focused on workflow visualization such as Kanban.\nKanban is a method for lean management. In short, It allows you to clearly visualize the assignments of your team, on a whiteboard, and apply limits to the amount of work in progress (WIP) available.\nSo, how to avoid burnouts at work with Kanban?\nBuild a Kanban board. Kanban boards consist of vertical columns and horizontal swimlanes. Each column represents a step in your process, while swimlanes are often used to visualize different types of assignments or different priorities.\nPrepare a board that mirrors the most important stages of your process and visualize all assignments on individual Kanban cards. It is important to mention that every card should have a single assignee responsible for processing it.\nPlace WIP limits. Make sure your team works on as few tasks at a time as possible to avoid frequent context switching so they finish what they start before pulling new work in progress.\nWIP limits can regulate the amount of work a certain team member is working on at any given time. Using WIP limits, the manager will be able to monitor the activity of his/her team closely and intervene when they see a problem.\nSet up daily team stand-up meetings. Stand-up meetings have become an integral part of Kanban culture and make it very easy to spot when something is wrong. You just need to gather your team at the beginning of each work day in front of their Kanban board and go through the items that are present.\nCommunicate progress and discuss existing as well as potential blockers to keep everybody in the loop and make sure that work is evenly distributed.\nKanban is a pull system, meaning that team members start new tasks only when they have the available capacity to process them.\nApplying this method of working means focusing on one thing in order to do it in the best possible way, instead of doing five things simultaneously and, consequently, achieving something mediocre. Simply said, quality comes before quantity.\nSome may think that this way of working might end up being slower, but, in reality, it is exactly the opposite.\nOne of the best things about using a Kanban is that every aspect of the project is visualized right in front of you, so the work can be delegated accordingly among the team members.\nWith Kanban, you can see if the colleague that is always complaining about how much work he has, is really that overloaded, or is just lazy and is trying to hide from additional assignments.\nThe same goes for the quiet person, who never complains and just takes everything that is sent her way, though she really has a full plate. By using Kanban it is very clear who is doing what and, if the distribution is not balanced, it will become immediately obvious.\nOccupational burnout is a very serious issue that must not be taken lightly by managers as well as their teams.\nOptimizing your work life is crucial for avoiding occupational burnout. Analyze your situation and consider how to prevent burning out using the techniques we have covered in this article.\nIf you are in a managerial position, and want to see if Kanban might help in protecting your team, we encourage you to test Kanbanize so you can ascertain how best to optimize your processes."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f6c1027d-8c75-45d5-b073-b224d9a30ea2>","<urn:uuid:ba0fe8c1-95db-4973-9366-feff54bc3485>"],"error":null}
{"question":"How can I install shader packs in Minecraft using Optifine?","answer":"To install shader packs: 1. Download and install Optifine from its official website 2. Run Minecraft Launcher and select Optifine version 3. In-game, go to Video Settings 4. Select Shaders option 5. Click Shaders Folder button 6. Copy downloaded shader pack zip files to this folder 7. Restart Minecraft in Optifine 8. Go back to Shaders menu, select your installed shader pack, and click Done. The visual changes will appear when you load a world.","context":["Registered: 2 months, 1 week ago\nHow To Put In Mods In Minecraft - Step Minecraft is a enjoyable recreation in itself. But you possibly can take it a step additional by adding mods to it. Minecraft has a whole lot of mods that may turn your vanilla gameplay right into a full horror experience or fun inventive building moments. You too can use mods to add various parts to the game. Nevertheless, there's a sure means to install mods in the sport. And earlier than that, that you must ensure that your setup is highly effective sufficient to handle the mods, as some will use intense assets. How to put in Mods in Minecraft? There are a number of steps to enjoying Minecraft with mods. Firstly, you will have to install the Forge Mod Installer. Then obtain the mods and add them to the game utilizing the following course of below. Putting in Forge Forge is the software program that may allow you to run the mods you need. It's a .jar file, a java-primarily based app that wants java to run. If you haven't installed Java, go to their official webpage and observe the instructions to obtain and set up it. 1. Install Forge for the model you need on your Laptop. 2. Open the .jar file you just downloaded. 3. Choose Install Shopper if you’re going to play locally. Install Server if you’re going to create your individual modded server. 4. Choose Ok and look forward to the installer to add the files to your library. Launching Minecraft Forge After installing Forge, you'll be able to simply access it utilizing the Minecraft Launcher. 1. Launch Minecraft Launcher. 2. On the left side of the Play button, open the drop-down menu and choose Forge model of Minecraft. 3. Hit the Play button, and your sport will open with Forge put in. Once your recreation runs, the title display screen will present you the version of Forge install. Another most important difference you'll find on the same display is that underneath the multiplayer possibility, you will have a mods button. It is going to take you to the display screen with a listing of mods put in in your Pc when you access it. Set up the Mod You Need You'll have to obtain the mod and add it to the mods folder in your native folder to use it. Try these steps to do so. 1. Go to CurseForge’s Minecraft web page. 2. Find the best mod you want by both utilizing the search bar or by looking. 3. We suggest studying the description to make sure it is appropriate with the Minecraft version you want to play on. Obtain the mod and put it aside. 4. Open the folder the place you downloaded the mods. We will come again to it later. 5. Launch the Forge version in Minecraft. 6. Choose Mods. 7. On the bottom left, you will notice a button that says “Open Mods Folder” Press it. It is going to open the local folder the place you possibly can add the mods. 8. Alternatively, you possibly can search for %appdata% in the search bar and open the folder. Then find the .minecraft folder and navigate to the mods folder. For Mac users, you may click the Go tab at the highest and sort in “~/Library/Software Help/minecraft” and press Go. You possibly can then find the mods folder in it. 9. Copy the download recordsdata on to the mods folder. 10. When you have the game open, restart it. Else, run the Forge model of the sport. 11. You will notice the mods you added and their details within the Mods section. If you have lots of mods, you can choose which mod packs to make use of whenever you begin the sport. To do so: 1. Open Minecraft and Create New World. 2. On the left of the menu, you will note the choice Data Packs; choose it. 3. You will notice two columns, left for the available mods, and the suitable for the chosen mods. 4. Whenever you hover your mouse on any, you will note an arrow pointing to the alternative column. If the mods are in the selected column, the arrow points left and vice versa. 5. Press the arrow to pick out or deselect a mod. 6. Press Performed as soon as you might be satisfied. 7. Create the world in your desired setting. Remember to check the mod versions as they is likely to be incompatible and crash the sport. If they're utterly incompatible or need additional recordsdata, the following display screen might present up while you launch the sport. How to install Modpacks? Modpacks are mainly a set of mods which are compatible collectively made by creators to put in on your Laptop. The web site CurseForge, prior to now, let everybody download and set up the modpacks manually. The method was a bit longer than installing the common mods. Now, you can set up the consumer referred to as CurseForge to put in the modpacks mechanically in your system. 1. Download and install the CurseForge app from its official page. 2. Run CurseForge and choose the game Minecraft. 3. You're going to get to the web page the place you can see all the installed modpacks or browse them. 4. Go to the tab to browse modpacks and choose the ones you need to install for your model. 5. Await the installation to complete. Some could take time to install on account of their large dimension. 6. As soon as Installed, you will discover them under My Modpacks. 7. Hover the mouse over the Modpack you want to play with and hit the Play button. It's going to open the Minecraft Launcher. 8. Underneath Installations tab of the Launcher, you possibly can see the modpack version installed robotically. You'll be able to merely press play, and the sport will launch. The game might download extra assets if it already has not. As soon as the whole lot downloads and Minecraft runs, you possibly can play the sport with none issues. How to install Shaders Mod in Minecraft? While regular mods change the gameplay, visuals, and objects in Minecraft, shaders will only change how the sport looks. There are quite a lot of shader packs that make your game look higher with mods akin to clear water, lifelike sky, and so forth. Similar to adding mods through Forge, you will need to obtain and set up Optifine variations of the sport. To take action: 1. Obtain and set up the Optifine model of the sport through its official webpage. 2. Once you set up Optifine, run the Minecraft Launcher. Minecraft 3. In the launcher, select the Optifine model from the record on the left of the play button. You will now have the Optifine version of the game installed. minecraft servers You won't have better graphics immediately after putting in Optifine. Find the shader you need to put in on the web and set up it too. You'll be able to comply with these steps to be sure you install the shaders correctly. Run the Optifine version of the game. 1. Go to Video Settings. You possibly can see the settings menu is different than that of the regular Minecraft. 2. Select the Shaders choice. 3. Press the Shaders Folder possibility on the bottom left. 4. Alternatively, you may seek for %appdata% in the beginning menu and open it. Then go to the .minecraft folder and open shader packs. You'll be able to open the Go tab for macOS users and sort in “~/Library/Software Support/minecraft” with out the quotations. Copy and paste the zip recordsdata you downloaded directly to this folder. Restart Minecraft in Optifine and observe steps 1 to 3. Choose the shader pack you simply put in and click on Performed. Run your saved world or create a brand new world. Once the world hundreds in, you will be capable of see the modifications in the game’s visuals. Associated Questions Is CurseForge Protected to use? CurseForge is safe to use as long as you download it from its official website. If you happen to choose to obtain the app from third-celebration web sites, it might result in your Laptop being contaminated with viruses or malware. Can Minecraft Mods Hurt Your Laptop? Mods are secure recordsdata that you can download from verified sources to make your Minecraft gameplay extra attention-grabbing. Nonetheless, generally you would possibly come across viruses or malware disguised as mods. You will have to be careful and skim the descriptions, consumer opinions (if available) and download them from trusted websites resembling CurseForge. Is CurseForge Free to Obtain? CurseForge and all of the contents are free for most people to make use of. All the mods listed on their websites are free, and you don't even need to be a member to download them. How to install RLCraft? RLCraft is a mod pack for Minecraft that makes the game harder and provides a ton of objects and mobs in the game. You can use the CurseForge app and seek for RLCraft to put in it without any problem. Are Minecraft Mods Unlawful? Minecraft Mods will not be illegal. You possibly can create your mods or set up any you could find to enhance the gameplay expertise in Minecraft. It's best to bear in mind of mods for different video games as some don't legally permit mods in them. What is the most effective Place to Get Minecraft Mods? Usually speaking, CurseForge is probably the greatest and huge libraries of mods you can find. It has hundreds of mods not only for Minecraft but additionally for different video games. In case you are using this webpage, you may be downloading real mods in your game.\nTopics Started: 0\nReplies Created: 0\nForum Role: Participant"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:73dd6c0f-e64e-47bf-a456-c16411971b54>"],"error":null}
{"question":"Para un sistema de aire acondicionado que utiliza R22, ¿cuáles son las consideraciones técnicas y de salud que deben tenerse en cuenta al comparar continuar su uso versus reemplazarlo con refrigerantes alternativos?","answer":"When using R22, there are several important considerations. For health concerns, direct exposure to CFCs (like R22) can cause various symptoms including unconsciousness, shortness of breath, irregular heartbeat, confusion, drowsiness, coughing, and eye problems. Regarding technical considerations for replacement, the replacement refrigerant must effectively mimic R22's operating pressure and temperature to prevent damaging the AC unit. Additionally, most old AC units use mineral oil which is incompatible with R22 replacement refrigerants. It's crucial to completely drain out R22 before using any replacement refrigerant, as mixing is not allowed. The replacement will void any existing warranty, but replacement refrigerants are usually cheaper than R22 and are safer for the ozone layer. Alternatively, you can continue using recycled R22 despite its high cost.","context":["If you have an air conditioner that was installed before 2010, the chances are that it uses R22 refrigerant. Also known as “Freon”, it is a type of hydrochlorofluorocarbon or HCFC. This refrigerant or coolant was introduced sometime during the 1950s and very soon became a leading brand of refrigerant. But that was then when environmental issues were not a major concern. Now, however, things have drastically altered. The prime concern today is to protect the environment. R22 being an HCFC depletes the ozone layer, and hence, it is no more looked upon favorably.\nBanning R22 – Montreal Protocol\nThe Environmental Protection Agency (EPA) of the United States of America, determined that R22 is harming the ozone layer over the Earth. When environmental concerns are at their peak, this was not good news. EPA then decided to join hands with other agencies and environmental groups all over the world to phase out products that were depleting the ozone layer. To this effect, an international agreement came into being known as the Montreal Protocol.\nR22 Phase Out Progress\nIn 2003 the phase-out process started. To begin with, its production was cut down drastically, and imports stopped altogether. By 2010 the production and any import of R22 were strictly prohibited. Now, R22 can only be purchased by a certified technician. After 2020, only recycled R22 will be used to service air conditioners still using them.\nWhen Considering R22 Replacement Refrigerants\nR410 A is considered to be the best replacement for R22. But the process of replacement is not as straightforward as draining out R22 and filling in R410 A (Puron). Here is a list of things to be aware of when considering R22 replacement refrigerants –\n1. Replacement Refrigerant To Mimic R22 Pressure & Temperature – The replacement refrigerant should be able to effectively mimic or copy the operating pressure and temperature of Freon. If the replacement refrigerant fails to do so, it may fry your AC unit.\n2. POE Oil Or Mineral Oil – Most old AC units use mineral oil which is less viscous as compared to POE Oil (Polyolester Oil). The bad news is that R22 replacement refrigerants do not work well with mineral oil. If you recently changed your compressor, the chances are that your AC now has the synthetic POE oil. In that case, you can use a suitable alternative refrigerant.\n3. Warranty Lapses With Replacement Refrigerant – If your AC is new and still under warranty, replacing R22 with a different refrigerant will make the warranty null and void. It is so because the manufacturer has not tested the unit for any other replacement refrigerant.\n4. Completely Drain Out R22 – Under no circumstances, you should mix R22 with any other replacement refrigerant. First Freon should be drained out completely, and the coils should be sterilized so that not even a few clinging droplets remain. Only then a different refrigerant should be filled.\nWhat To Do If My AC Has R22?\nThings to be aware of when considering R22 replacement refrigerants do not end with the above list. If your AC has R22, you have three probable choices. These include –\n1. Buy a new, upgraded, and more environment-friendly AC unit that uses a refrigerant that is safe and has no harmful effects.\n2. Call a trained technician to replace all those parts of your air conditioner which are not compatible with the replacement refrigerant. However, this move is not highly recommended.\n3. Keep on using recycled R22 irrespective of its through the roof cost.\nJust to be clear, EPA has forced restriction on the use of R22 but has not forced you to abandon your old AC and buy a new one. If your AC uses Freon, it means that you have an old Ac because new ones don’t use it anymore. At some point in time, your old AC will go belly up, and that is when you should buy a new and better AC.\nFacts About Replacement Refrigerants\nIf your AC runs on R22, the knowledge of these facts will help you make a wiser decision –\n1. They nullify any and all warranties from the manufacturer.\n2. They are suitable only for heat pumps and condensers for which the warranty has already lapsed.\n3. They are safer as they do not deplete the ozone layer to the same extent.\n4. They should never be mixed with R22 in any amount, large or small.\n5. They are usually cheaper than R22.\nIf Your AC Was Built After 2010\nIf your AC was manufactured after 2010, you need not worry about it containing R22 refrigerant. As per the regulations, the newer models of air conditioners cannot have it anymore. Instead, only approved refrigerants can be used. However, if you are still in doubt, you can ask an HVAC technician to validate the information for you. You can also check out this little detail yourself by looking at the nameplate on the condenser unit or the outdoor unit of your AC. If for some reason the nameplate is missing or illegible you can check for it online or in the information booklet that came with your unit.\nIf you are environment conscious and wish to do your bit in keeping the Earth beautiful and green, you need to have a look at the refrigerant used by your air conditioning unit. The best choice is to upgrade to a better and more environment-friendly AC instead of ignoring the issue. For all of us, this may not seem to be a financially viable choice. However, remember R22 is already costly and as it becomes more difficult to procure the price will go up further. A little financial burden now will save you every hard-earned penny in the future.\nNow that you know more about the things to be aware of when considering R22 replacement refrigerants buying a new AC would look like a cost-effective solution. It will save you from spending money on a new compressor, recycled R22, flushing costs, etc. If you need any more information on R22 or replacement refrigerants, feel free to contact us at Aztil AC.","Chlorofluorocarbons have been banned since 1996 because they destroy the ozone layer.\nWhat are chlorofluorocarbons (CFCs)?\nChlorofluorocarbons (CFCs) are a group of manufactured chemical compounds that contain chlorine, fluorine, and carbon. This group includes CFC-11, CFC-12, CFC-113, CFC-114, CFC-115, and many forms of Freon. They are colorless, odorless, nontoxic, nonflammable, and stable when emitted. When they are emitted and reach the stratosphere, they break apart and release chlorine atoms, which destroy the earth’s ozone layer. CFCs can last for more than 100 years in the stratosphere. Because they destroy the ozone layer, CFCs have been banned from production in the United States since December 31, 1995. Only recycled and stockpiled CFCs can now be used on a limited basis.\nHydrofluorocarbons (HFCs) are factory-made chemical compounds now being used as ozone-safe replacements for CFCs. HFCs belong to a class of chemicals called fluorinated gases. HFCs are used as refrigerants in businesses and residences, and air conditioning systems in vehicles; aerosol propellants; solvents for electrical components; and fire retardants. The major source of HFC emissions is leakage from air conditioning systems in vehicles and buildings. Emissions of HFCs and the other fluorinated gases represent approximately three percent of all greenhouse gas emissions in the United States.How might I be exposed to CFCs?\nCFCs are also a “greenhouse gas” because they absorb heat in the atmosphere, sending some of the absorbed heat back to the surface of the earth and contributing to global warming and climate change.\nBefore CFCs were banned, they were used in aerosols, refrigerators, air conditioners in homes, vehicles and businesses, fire extinguishers, insulating foams, styrofoam food packaging, and cleaning and electronic solvents. CFCs were made with perchloroethylene (PERC).\nPrior to 2008, CFCs were still used in inhalers to control asthma, but this use has not been allowed after 2008. They may also be used in research.\nYou can be exposed to CFCs if you use a pre-2008 inhaler that contains CFCs, use older window air conditioners that contain CFCs, or drive an older car with an air conditioner that contains CFCs. If you use an older refrigerator that contains CFCs, you can be exposed if the CFCs leak out of the refrigerator. Older appliances and vehicles need to be carefully handled for safe disposal of the CFCs they contain. How can CFCs affect my health?\nAt work, you can be exposed to CFCs if you work in a facility that recycles CFCs in air conditioners. You can be exposed if you work at a facility that has permission to use recycled or stockpiled CFCs or conducts research that uses them.\nDirect exposure to some types of CFCs can cause unconsciousness, shortness of breath, and irregular heartbeat. It can also cause confusion, drowsiness, coughing, sore throat, difficulty breathing, and eye redness and pain. Direct skin contact with some types of CFCs can cause frostbite or dry skin.\nWhen CFCs destroy the ozone layer, harmful ultraviolet rays reach the earth. Exposure to increased ultraviolet rays can cause skin cancer, cataracts, and weakened immune systems.\nFor poisoning emergencies or questions about possible poisons, please contact your local poison control center at 1-800-222-1222.\nThis description is based on the information found in the Web links listed with this topic.\nWeb Links from MedlinePlus (National Library of Medicine)\nIndoor Air Pollution\nBenefits of the CFC Phaseout (Environmental Protection Agency)\nChlorofluorocarbons. Haz-Map (National Library of Medicine)\nEnvironmental Indicators: Ozone Depletion (Environmental Protection Agency)\nHow to Keep Your Cool and Protect the Ozone Layer (Environmental Protection Agency)\nMap of Releases of Freon 113 in the United States. TOXMAP (National Library of Medicine)\nMyth: CFCs Are Heavier Than Air, So They Can't Reach the Ozone Layer (Environmental Protection Agency)\nResponsible Appliance Disposal (RAD) (Environmental Protection Agency)\nUsers of Last CFC Inhalers Must Soon Switch (Food and Drug Administration)\nLast Updated: April 21, 2015"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:406b5a69-c98f-4d4d-89c0-063407d6c13a>","<urn:uuid:a7de1972-63c4-4267-affd-2f183cebdb8a>"],"error":null}
{"question":"In evaluating medical assessment tools, what is the validity evidence supporting different OSCE standard-setting methods in clinical education?","answer":"Studies show that the Borderline Regression (BL-R) method demonstrates the highest convergent validity evidence among standard-setting methods for OSCEs. While the Wijnen method also shows high convergent validity, it lacks the theoretical strength needed for competency-based assessments. When compared across multiple assessments, three cluster variants showed substantial convergent validity with borderline methods. This evidence comes from analysis of psychometric properties of the scores and comparison of different methods, indicating BL-R as the most reliable approach for setting standards in clinical skills assessment.","context":["Boosting standard order sets utilization through clinical decision support. Well-designed standard order sets have the potential to integrate and coordinate care by communicating best practices through multiple disciplines, levels of care, and services.\nHowever, there are several challenges which certainly affected the benefits expected from standard order sets. To boost standard order sets utilization, a problem-oriented knowledge delivery solution was proposed in this study to facilitate access of standard order sets and evaluation of its treatment effect. In this solution, standard order sets were created along with diagnostic rule sets which can trigger a CDS-based reminder to help clinician quickly discovery hidden clinical problems and corresponding standard order sets during ordering.\nThose rule set also provide indicators for targeted evaluation of standard order sets during treatment. A prototype system was developed based on this solution and will be presented at Medinfo Simulation in the clinical setting: Simulation-based educational activities are happening in the clinical environment but are not all uniform in terms of their objectives, delivery, or outputs.\nWhile these activities all provide an opportunity for individual and team training, nuances in the location, timing, notification, and participants impact the potential outcomes of these sessions and objectives In light of this, there are actually many different types of simulation-based activity that occur in the clinical environment, which has previously all been grouped together as \"in situ\" simulation.\nHowever, what truly defines in situ simulation is how the clinical environment responds in its' natural state, including the personnel, equipment, and systems responsible for care in that environment. Beyond individual and team skill setsthere are threats to patient safety or quality patient care that result from challenges with equipment, processes, or system breakdowns.\nThese have been labeled \"latent safety threats. The distinction between the two is highlighted in this article, as well as some of the various sub-types of in situ simulation. We recently compiled additional threats to validity of the neuropsychiatric evidence base,4,5 a list already incomplete in view of recent concerns with industry influence evidenced by ghost authorships 6 and selective reporting.\nA collaborative comparison of objective structured clinical examination OSCE standard setting methods at Australian medical schools.\nA key issue underpinning the usefulness of the OSCE assessment to medical is standard settingbut the majority of standard-setting methods remain challenging for performance assessment because they produce varying passing marks. Several studies have compared standard-setting methods; however, most of these studies are limited by their experimental scope, or use data on examinee performance at a single OSCE station or from a single medical school.\nThis collaborative study between 10 Australian medical schools Oro for statlig filtrering av internet the effect of standard-setting methods on OSCE cut scores and failure rates. This research used examinee scores from seven shared OSCE stations to calculate cut scores and failure rates using two different compromise standard-setting methods, namely the Borderline Regression and Cohen's methods.\nThe results of this study indicate that Cohen's method yields similar outcomes to the Borderline Regression method, particularly for large examinee cohort sizes. However, with lower examinee numbers on a station, the Borderline Regression method resulted in higher cut scores and larger difference margins in the failure rates.\nCohen's method yields similar outcomes as the Borderline Regression method and its application for benchmarking purposes and in resource-limited settings is justifiable, particularly with large examinee numbers.\nWhile Objective Structured Clinical Examinations OSCEs have become widely used to assess clinical competence at the end of undergraduate medical courses, the method of setting the passing score varies greatly, and there is no agreed best methodology. While there is an assumption that the passing standard at graduation is the same at all medical….\nWhen the safety of the public is at stake, it is particularly relevant for licensing and credentialing exam agencies to use defensible standard setting methods to categorize candidates into competence categories e. The guidelines reflect the revision and extension of two past sets of guidelines ISSCR, ; ISSCR, to address new and emerging areas of stem cell discovery and application and evolving Oro for statlig filtrering av internet, social, and policy challenges.\nThese guidelines provide an integrated set of principles and best practices to drive progress in basic, translational, and clinical research. The guidelines demand rigor, oversight, and transparency in all aspects of practice, providing confidence to practitioners and public alike that stem cell science can proceed efficiently and remain responsive to public and patient interests. Here, we highlight key elements and recommendations in the guidelines and summarize the recommendations and deliberations behind them.\nPublished by Elsevier Inc. Recent court decisions have pointed out the complexities involved in setting environmental standards. Environmental health is composed of multiple causative agents, most of which work over long periods of time. This makes the cause-and-effect relationship between health statistics and environmental contaminant exposures difficult to prove in….\nA national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications.\nThis flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants.\nTo provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settingsuse of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard \"Oro for statlig filtrering av internet\" the existing variant file specifications presently used in Oro for statlig filtrering av internet clinical setting.\nAdoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities. Naltrexone in alcohol dependence: To determine whether naltrexone is beneficial in the treatment of alcohol dependence in the absence of obligatory psychosocial intervention. Multicentre, randomised, double-blind, placebo-controlled trial. Hospital-based drug and alcohol clinics18 March - 22 October Relapse rate; time to first relapse; side effects.\nThis treatment effect was most marked in the first 6 weeks of the trial. The median time to relapse was 90 days for naltrexone, compared with 42 days for placebo. In absolute numbers, 19 of 56 patients Naltrexone was well tolerated. Unlike previous studies, we have shown that naltrexone with adjunctive medical advice is effective in the treatment of alcohol dependence irrespective of whether it is accompanied by psychosocial interventions.\nThe objective structured clinical examination OSCE was established for valid, reliable, and objective assessment of clinical skills in health professions education.\nVarious standard setting methods have been proposed to identify objective, reliable, and valid cutoff scores on OSCEs. These methods may identify different cutoff scores for the same examinations. Identification of valid and reliable cutoff scores for OSCEs remains an important issue and a challenge. Psychometric properties of the scores Oro for statlig filtrering av internet determined. BL-R and Wijnen methods show the highest convergent validity evidence among other methods on the defined criteria.\nThe three cluster variants showed substantial convergent validity with borderline methods. Although there was a high level of convergent validity of Wijnen method, it lacks the theoretical strength to be used for competency-based assessments. The BL-R method is found to show the highest convergent validity evidences for OSCEs with other standard setting methods used in the present study.\nNew Council for Advancement and Oro for statlig filtrering av internet of Education CASE standards for college and university fund raising establish three key rules for campaign reporting: Discusses implications of the issues of continuity and discontinuity, commonality and difference, congruity and paradox as they engage the research team, the industry partners, and the teachers around Australia who constitute the STELLA Standards for Teachers of English Language and Literacy in Australia project.\nThe elderly population in the United States is increasing exponentially in tandem with risk for frailty. Frailty is described by a clinically significant state where a patient is at risk for developing complications requiring increased assistance in daily activities. Frailty syndrome studied in geriatric patients is responsible for an increased risk for falls, and increased mortality.\nIn efforts to prepare for and to intervene in perioperative complications and general frailty, a universal scale to measure frailty is necessary. Many methods for determining frailty have been developed, yet there remains a need to define clinical frailty and, therefore, the most effective way to measure it. This article reviews six popular scales for measuring frailty and evaluates their clinical effectiveness demonstrated in previous studies.\nBy identifying the most time-efficient, criteria comprehensive, and clinically effective scale, a universal scale can be implemented into standard of care and reduce complications from frailty in both non-surgical and surgical settingsespecially applied to the perioperative surgical home model.\nWe suggest further evaluation of the Edmonton Frailty Scale for inclusion in patient care. Photographing Injuries in the Acute Care Setting: Background Photographing injuries in the acute setting allows for improved documentation as well as assessment by clinicians and others who have not personally examined a patient.\nThis tool is important, particularly for telemedicine, tracking of wound healing, the evaluation of potential abuse, and injury research. Despite this, protocols to ensure standardization of photography in clinical practice, forensics, or research have not been published. In preparation for a study of injury patterns in elder abuse and geriatric falls, our goal was to develop and evaluate a protocol for standardized photography of injuries that may be broadly applied. Methods We conducted a literature review for techniques and standards in medical, forensic, and legal photography.\nWe developed a novel protocol describing types of photographs and body positioning for eight body regions, including instructional diagrams.\nWe revised it iteratively in consultation with experts in medical photography; forensics; and elder, child, and domestic abuse. Results Among the injuries, from 53 patients, photographed by 18 photographers using this protocol, photographs of 25 injuries 10 bruises, seven lacerations, and eight abrasions were used to assess characterization of the injury.\nWhether differences in the standards states have set can be explained by something other than regional differences is explored. In addition, a way in which standards can be compared is defined, and the standard of proficiency that seems to be widely shared across the country is illustrated.\nRecommendation for measuring clinical outcome in distal radius fractures: Lack of standardization of outcome measurement has hampered an evidence-based approach to clinical practice and research.\nWe adopted a process of reviewing evidence on current use of measures and appropriate theoretical frameworks for health and disability to inform a consensus process that was focused on deriving the minimal set of core domains in distal radius fracture. We agreed on the following seven core recommendations: We used a sound methodological approach to form a comprehensive foundation of content for outcomes in the area of distal radius fractures.\nWe recommend the use of symptom and function as separate domains in the ICF core set in clinical research or practice for patients with wrist fracture. Further research is needed to provide more definitive measurement properties of measures across all domains. Using intranet-based order\nOro for statlig filtrering av internet to standardize clinical care and prepare for computerized physician order entry. The high cost of computerized physician order entry CPOE and physician resistance to standardized care have delayed implementation.\nAn intranet-based order set system can provide some Oro for statlig filtrering av internet CPOE's benefits and offer opportunities to acculturate physicians toward standardized care. Physician groups developed additional order setswhich number more than Web traffic increased progressively during a month period, peaking at more than 6, hits per month to COF. Clinicians demonstrated a willingness to develop and use order sets and decision support tools posted on the COF site.\nThe educational resources, relevant links to external resources, and communication alerts will all link to CPOE, thereby providing a head start in CPOE implementation. Teaching communication skills in clinical settings: However, the consistency of programs teaching communication skills has received little attention, and debate exists about the application of acquired \"Oro for statlig filtrering av internet\" to real patients. This study inspects \"Oro for statlig filtrering av internet\" 1 results from a communication program are replicated with different samples, and 2 results with standardized patients apply to interviews with real patients.\nconsumers taking our Internet educational course that provided with them After completing the Internet course, we tracked the changes in. The locations of Internet web sites for the six organizations are provided as Prevalence and clinical implications of improper filter settings in routine are no valid studies about the entity of laryngeal trauma in oro-tracheal intubation.\nS; Knoepfel, K; Kondo, K; Kong, D J; Konigsberg, J; Kotwal, A V; Kreps, M; Kroll, J. tes dock inte någon direkt oro för uppvärmningen förrän under. talet. Då förekom hamnat inom en statlig administrering av ekologi. fanns tillgängliga på Internet. En kostnadsfri filtrering av kunskap om klimatförändring erbjuds.\nMORE: Kajsa o orosmolnen hopas\nMORE: Okat missnoje oroar kina","Boosting standard order sets utilization through clinical decision support. Well-designed standard order sets have the potential to integrate and coordinate care by communicating best practices through multiple disciplines, levels of care, and services.\nHowever, there are several challenges which certainly affected the benefits expected from standard order sets. To boost standard order sets utilization, a problem-oriented knowledge delivery solution was proposed in this study to facilitate access of standard order sets and evaluation of its treatment effect. In this solution, standard order sets were created along with diagnostic rule sets which can trigger a CDS-based reminder to help clinician quickly discovery hidden clinical problems and corresponding standard order sets during ordering.\nThose rule set also provide indicators for targeted evaluation of standard order sets during treatment. A prototype system was developed based on this solution and will be presented at Medinfo Simulation in the clinical setting: Simulation-based educational activities are happening in the clinical environment but are not all uniform in terms of their objectives, delivery, or outputs.\nWhile these activities all provide an opportunity for individual and team training, nuances in the location, timing, notification, and participants impact the potential outcomes of these sessions and objectives In light of this, there are actually many different types of simulation-based activity that occur in the clinical environment, which has previously all been grouped together as \"in situ\" simulation.\nHowever, what truly defines in situ simulation is how the clinical environment responds in its' natural state, including the personnel, equipment, and systems responsible for care in that environment. Beyond individual and team skill setsthere are threats to patient safety or quality patient care that result from challenges with equipment, processes, or system breakdowns.\nThese have been labeled \"latent safety threats. The distinction between the two is highlighted in this article, as well as some of the various sub-types of in situ simulation. We recently compiled additional threats to validity of the neuropsychiatric evidence base,4,5 a list already incomplete in view of recent concerns with industry influence evidenced by ghost authorships 6 and selective reporting.\nA collaborative comparison of objective structured clinical examination OSCE standard setting methods at Australian medical schools.\nA key issue underpinning the usefulness of the OSCE assessment to medical is standard settingbut the majority of standard-setting methods remain challenging for performance assessment because they produce varying passing marks. Several studies have compared standard-setting methods; however, most of these studies are limited by their experimental scope, or use data on examinee performance at a single OSCE station or from a single medical school.\nThis collaborative study between 10 Australian medical schools Oro for statlig filtrering av internet the effect of standard-setting methods on OSCE cut scores and failure rates. This research used examinee scores from seven shared OSCE stations to calculate cut scores and failure rates using two different compromise standard-setting methods, namely the Borderline Regression and Cohen's methods.\nThe results of this study indicate that Cohen's method yields similar outcomes to the Borderline Regression method, particularly for large examinee cohort sizes. However, with lower examinee numbers on a station, the Borderline Regression method resulted in higher cut scores and larger difference margins in the failure rates.\nCohen's method yields similar outcomes as the Borderline Regression method and its application for benchmarking purposes and in resource-limited settings is justifiable, particularly with large examinee numbers.\nWhile Objective Structured Clinical Examinations OSCEs have become widely used to assess clinical competence at the end of undergraduate medical courses, the method of setting the passing score varies greatly, and there is no agreed best methodology. While there is an assumption that the passing standard at graduation is the same at all medical….\nWhen the safety of the public is at stake, it is particularly relevant for licensing and credentialing exam agencies to use defensible standard setting methods to categorize candidates into competence categories e. The guidelines reflect the revision and extension of two past sets of guidelines ISSCR, ; ISSCR, to address new and emerging areas of stem cell discovery and application and evolving Oro for statlig filtrering av internet, social, and policy challenges.\nThese guidelines provide an integrated set of principles and best practices to drive progress in basic, translational, and clinical research. The guidelines demand rigor, oversight, and transparency in all aspects of practice, providing confidence to practitioners and public alike that stem cell science can proceed efficiently and remain responsive to public and patient interests. Here, we highlight key elements and recommendations in the guidelines and summarize the recommendations and deliberations behind them.\nPublished by Elsevier Inc. Recent court decisions have pointed out the complexities involved in setting environmental standards. Environmental health is composed of multiple causative agents, most of which work over long periods of time. This makes the cause-and-effect relationship between health statistics and environmental contaminant exposures difficult to prove in….\nA national workgroup convened by the Centers for Disease Control and Prevention identified principles and made recommendations for standardizing the description of sequence data contained within the variant file generated during the course of clinical next-generation sequence analysis for diagnosing human heritable conditions. The specifications for variant files were initially developed to be flexible with regard to content representation to support a variety of research applications.\nThis flexibility permits variation with regard to how sequence findings are described and this depends, in part, on the conventions used. For clinical laboratory testing, this poses a problem because these differences can compromise the capability to compare sequence findings among laboratories to confirm results and to query databases to identify clinically relevant variants.\nTo provide for a more consistent representation of sequence findings described within variant files, the workgroup made several recommendations that considered alignment to a common reference sequence, variant caller settingsuse of genomic coordinates, and gene and variant naming conventions. These recommendations were considered with regard \"Oro for statlig filtrering av internet\" the existing variant file specifications presently used in Oro for statlig filtrering av internet clinical setting.\nAdoption of these recommendations is anticipated to reduce the potential for ambiguity in describing sequence findings and facilitate the sharing of genomic data among clinical laboratories and other entities. Naltrexone in alcohol dependence: To determine whether naltrexone is beneficial in the treatment of alcohol dependence in the absence of obligatory psychosocial intervention. Multicentre, randomised, double-blind, placebo-controlled trial. Hospital-based drug and alcohol clinics18 March - 22 October Relapse rate; time to first relapse; side effects.\nThis treatment effect was most marked in the first 6 weeks of the trial. The median time to relapse was 90 days for naltrexone, compared with 42 days for placebo. In absolute numbers, 19 of 56 patients Naltrexone was well tolerated. Unlike previous studies, we have shown that naltrexone with adjunctive medical advice is effective in the treatment of alcohol dependence irrespective of whether it is accompanied by psychosocial interventions.\nThe objective structured clinical examination OSCE was established for valid, reliable, and objective assessment of clinical skills in health professions education.\nVarious standard setting methods have been proposed to identify objective, reliable, and valid cutoff scores on OSCEs. These methods may identify different cutoff scores for the same examinations. Identification of valid and reliable cutoff scores for OSCEs remains an important issue and a challenge. Psychometric properties of the scores Oro for statlig filtrering av internet determined. BL-R and Wijnen methods show the highest convergent validity evidence among other methods on the defined criteria.\nThe three cluster variants showed substantial convergent validity with borderline methods. Although there was a high level of convergent validity of Wijnen method, it lacks the theoretical strength to be used for competency-based assessments. The BL-R method is found to show the highest convergent validity evidences for OSCEs with other standard setting methods used in the present study.\nNew Council for Advancement and Oro for statlig filtrering av internet of Education CASE standards for college and university fund raising establish three key rules for campaign reporting: Discusses implications of the issues of continuity and discontinuity, commonality and difference, congruity and paradox as they engage the research team, the industry partners, and the teachers around Australia who constitute the STELLA Standards for Teachers of English Language and Literacy in Australia project.\nThe elderly population in the United States is increasing exponentially in tandem with risk for frailty. Frailty is described by a clinically significant state where a patient is at risk for developing complications requiring increased assistance in daily activities. Frailty syndrome studied in geriatric patients is responsible for an increased risk for falls, and increased mortality.\nIn efforts to prepare for and to intervene in perioperative complications and general frailty, a universal scale to measure frailty is necessary. Many methods for determining frailty have been developed, yet there remains a need to define clinical frailty and, therefore, the most effective way to measure it. This article reviews six popular scales for measuring frailty and evaluates their clinical effectiveness demonstrated in previous studies.\nBy identifying the most time-efficient, criteria comprehensive, and clinically effective scale, a universal scale can be implemented into standard of care and reduce complications from frailty in both non-surgical and surgical settingsespecially applied to the perioperative surgical home model.\nWe suggest further evaluation of the Edmonton Frailty Scale for inclusion in patient care. Photographing Injuries in the Acute Care Setting: Background Photographing injuries in the acute setting allows for improved documentation as well as assessment by clinicians and others who have not personally examined a patient.\nThis tool is important, particularly for telemedicine, tracking of wound healing, the evaluation of potential abuse, and injury research. Despite this, protocols to ensure standardization of photography in clinical practice, forensics, or research have not been published. In preparation for a study of injury patterns in elder abuse and geriatric falls, our goal was to develop and evaluate a protocol for standardized photography of injuries that may be broadly applied. Methods We conducted a literature review for techniques and standards in medical, forensic, and legal photography.\nWe developed a novel protocol describing types of photographs and body positioning for eight body regions, including instructional diagrams.\nWe revised it iteratively in consultation with experts in medical photography; forensics; and elder, child, and domestic abuse. Results Among the injuries, from 53 patients, photographed by 18 photographers using this protocol, photographs of 25 injuries 10 bruises, seven lacerations, and eight abrasions were used to assess characterization of the injury.\nWhether differences in the standards states have set can be explained by something other than regional differences is explored. In addition, a way in which standards can be compared is defined, and the standard of proficiency that seems to be widely shared across the country is illustrated.\nRecommendation for measuring clinical outcome in distal radius fractures: Lack of standardization of outcome measurement has hampered an evidence-based approach to clinical practice and research.\nWe adopted a process of reviewing evidence on current use of measures and appropriate theoretical frameworks for health and disability to inform a consensus process that was focused on deriving the minimal set of core domains in distal radius fracture. We agreed on the following seven core recommendations: We used a sound methodological approach to form a comprehensive foundation of content for outcomes in the area of distal radius fractures.\nWe recommend the use of symptom and function as separate domains in the ICF core set in clinical research or practice for patients with wrist fracture. Further research is needed to provide more definitive measurement properties of measures across all domains. Using intranet-based order\nOro for statlig filtrering av internet to standardize clinical care and prepare for computerized physician order entry. The high cost of computerized physician order entry CPOE and physician resistance to standardized care have delayed implementation.\nAn intranet-based order set system can provide some Oro for statlig filtrering av internet CPOE's benefits and offer opportunities to acculturate physicians toward standardized care. Physician groups developed additional order setswhich number more than Web traffic increased progressively during a month period, peaking at more than 6, hits per month to COF. Clinicians demonstrated a willingness to develop and use order sets and decision support tools posted on the COF site.\nThe educational resources, relevant links to external resources, and communication alerts will all link to CPOE, thereby providing a head start in CPOE implementation. Teaching communication skills in clinical settings: However, the consistency of programs teaching communication skills has received little attention, and debate exists about the application of acquired \"Oro for statlig filtrering av internet\" to real patients. This study inspects \"Oro for statlig filtrering av internet\" 1 results from a communication program are replicated with different samples, and 2 results with standardized patients apply to interviews with real patients.\nconsumers taking our Internet educational course that provided with them After completing the Internet course, we tracked the changes in. The locations of Internet web sites for the six organizations are provided as Prevalence and clinical implications of improper filter settings in routine are no valid studies about the entity of laryngeal trauma in oro-tracheal intubation.\nS; Knoepfel, K; Kondo, K; Kong, D J; Konigsberg, J; Kotwal, A V; Kreps, M; Kroll, J. tes dock inte någon direkt oro för uppvärmningen förrän under. talet. Då förekom hamnat inom en statlig administrering av ekologi. fanns tillgängliga på Internet. En kostnadsfri filtrering av kunskap om klimatförändring erbjuds.\nMORE: Kajsa o orosmolnen hopas\nMORE: Okat missnoje oroar kina"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:99a423da-14dc-441a-bdf2-3f70de63899c>","<urn:uuid:99a423da-14dc-441a-bdf2-3f70de63899c>"],"error":null}
{"question":"Could you explain how feature scaling methods work in machine learning, and what are the regulatory guidelines for managing the associated risks in model development?","answer":"Feature scaling can be done through two main approaches: Min-Max Scaling (normalization) which rescales values to range from 0-1 by subtracting the minimum value and dividing by the range, and Standardization which subtracts the mean and divides by standard deviation to achieve unit variance. Regarding risk management, regulatory guidelines emphasize having clear statements of purpose for models, ensuring data quality and relevance, maintaining comprehensive documentation, and implementing monitoring tools with targeted reports for decision-makers. The guidelines also stress the importance of maintaining a model catalog with complete information about all models (deployed, retired, and under development) and documentation detailed enough for unfamiliar parties to understand model operations, limitations, and key assumptions.","context":["Cook the data for your Machine Learning Algorithm\nThis article was published as a part of the Data Science Blogathon.\nIn this article, we discuss how to cook the data for your machine learning algorithms. So that when you feed it, our algorithm will grow taller and stronger 🙂\nTechnically, today we will learn how to prepare our data for Machine Learning Algorithms. In the world of Machine Learning, we call this data pre-processing and also implement them practically. We will be using the Housing Dataset for understanding the concepts.\nFirstly, let’s take things a little bit slow, and see what do we mean by data-preprocessing? and why do we need it in the first place? following that we will see methods that will help us in getting tasty data (pre-processed) which will make our machine learning algorithm stronger (accurate).\nTable of content:\n- What do we mean by data pre-processing and why do we need it.\n- Data Cleaning.\n- Text and Categorical attributes.\n- Feature Scaling.\n- Transformation pipelines.\n1. What do we mean by data pre-processing and why do we need it?\nSo what is the first thing that comes up to your mind when you think about data, you might be thinking this:\nlarge datasets with lots of rows and columns. That is a likely scenario, but that may not be the case always.\nData could be in so many different types of forms like audios, videos, images, etc. The machine still doesn’t understand this type of data yet! They are still dumb! all they know is 1s and 0s.\nIn the world of machine learning, Data pre-processing is basically a step in which we transform, encode, or bring the data to such a state that our algorithm can understand easily.\nLet’s go ahead and cook (prepare) the data!!\nInstead of doing this manually, always keep a habit of making functions for this purpose, for several good reasons:\n- If you are working on a new dataset, and already have functions pre-build reproducing your transformation would be a piece of cake for you.\n- Gradually you will be building a library of these functions for your upcoming projects.\n- Also, you can directly use these functions in your live project to transform your new data before feeding it to your algorithm.\n- You can easily try your various transformation and see which combinations work out best for you.\nSo without further ado, Let’s get started!\n2. Data Cleaning:\nThe first and foremost step in preparing the data is you need to clean your data. There are a lot of machine learning algorithms(almost all) that cannot work with missing features.\nAs we can see that there are a couple of missing values in total_bedrooms. Let’s go ahead and create some functions to take care of them.\nNow you have three options here:\nRemove all the Null values.\nRemove the entire attribute.\nReplace missing values with some other values(mean, median, or 0).\nmedian = df['total_bedrooms'].median() df['totalbedrooms'].fillna(mediun, inplace=True)\nMost of us go with replacing missing values with median values. Always remember to save the median values that you have calculated, you will be needing it, later on, to replace missing values in the test set and also when your project is live.\nSo there is an amazing class available in Scikit-Learn which helps us in taking care of missing values.\nSimple Imputer !!\nLet me show you how to use it,\nCreate an instance and specify your strategy i.e. median.\nfrom sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=\"median\") imputer_df = df.drop(\"ocean_proximity\",axis=1)\nAs we know that, ocean_proximity is text and we cannot compute its median. Hence, we need to remove it at the time of calculations.\nNow we can fit the imputer instances that we created onto our training data using the fit() method:\nHere all the values are set by default;\nThe imputer we created has calculated the median for all the attributes and stored it in the statistics_ variable. In our case, total_bedrooms was the only one with missing values, but in the future, we can get missing values in other attributes too, So it is good to apply imputer to all attributes to be on a safer side.\nNow we will transform our dataset, doing so will return a NumPy array which we will be converting back to the pandas data frame.\nX = imputer.transform(imputer_df) revised_df = pd.DataFrame(X,columns=imputer_df.columns,index = imputer_df.index) revised_df.info()\nAs we can see, all the Null values are now replaced with their corresponding medians.\n3. Text and categorical attributes:\nIf you have just started your data science journey, you might have not come across any textual attribute. So let’s see how we deal with text and categorical attributes.\nIn our dataset, there is just one attribute: ocean_proximity which is text attribute.\nLet’s have a look at its unique values:\nHere we can see that it is not some arbitrary text, they are in limited numbers each of which represents some kind of category. That means we have a categorical attribute. As we all know that, machine learning algorithms don’t work pretty well with textual data so let’s convert them into numbers.\nFor this task also, there is an amazing class available in Scikit-Learn which helps us in handling categorical data.\nOrdinal Encoder !!\nLet me show you how to use it:\nfrom sklearn.preprocessing import OrdinalEncoder oe = OrdinalEncoder() encoded_ocean_proximity = oe.fit_transform(ocean)\nDoing this will convert all categorical data into their respective numbers.\nTo get the list of categories we have to use categories_ variable.\nNumbers for each category are:\n- <1H OCEAN: 0\n- INLAND: 1\n- ISLAND: 2\n- NEAR BAY: 3\n- NEAR OCEAN: 4\nBut there is a problem, here our machine learning algorithm will assume that two nearby values are closely related to each other than two distant values. In certain situations, for example, when we might be having categories like [“Worst”, “Bad”, “Good”, “Better”, “Best”] it is beneficial. But in our case, we can clearly see that <1H OCEAN is more similar to NEAR OCEAN than <1H OCEAN and INLAND.\nTo tackle this type of problem, a common solution is to create one binary attribute per category. What I mean by that is we have to create an extra attribute which will be 1(hot) when the category is <1H OCEAN and 0(cold) if not, and we have to do this for all categories.\nLet me show you how can we do that:\nfrom sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder() df_hot = ohe.fit_transform(ocean) df_hot\nThis is called one-hot encoding because only one attribute will be hot, while the rest of them will be cold. These are also dummy attributes.\n*Remember the output of this is a spare matrix which really comes in handy when we are dealing with thousands of categories. Because just imagine, if we perform one-hot encoding on an attribute that has thousands of categories we will end up getting a thousand more columns that are full of zeros except a single one per row. Here we are using lots of memory just to store zeros and it is extremely wasteful. So what a sparse matrix does is it stores the locations of all non-zero elements only.\nBut still, if you want to convert it to NumPy array just call the toarray() method.\n4. Feature Scaling:\nThis is one of the most important transformations that you will need to apply to your data. Generally, machine learning algorithms don’t perform well when the numerical attributes are not having the same scale.\nIn our dataset, we can see that the median income ranges only from 0 to 15 whereas the total number of rooms ranges from about 2 to 39,320.\nSo now we have two ways through which we can get all attributes on the same scale:\n- Min-Max Scaling.\nAlso known as, Normalization and is one of the simplest scalers. Here what this transformer does is it shifts and rescales the values so that they end ranging from 0-1. This is done by subtracting the minimum value and dividing it by the differences between maximum and minimum value.\nSckit-Learn has a transformer for this task, MinMaxScaler and it also has a hyperparameter called feature_range which helps in changing the range, if for some reason you don’t want the range to be from 0 to1.\nThis is a little bit different. What it does is it first subtracts the mean value and after that, it divides it by the standard deviation to get a unit variance.\nHere, it does not bound values to some fixed specific range like min-max scalers. This may be a problem with some of the algorithms. For example, Most of the time, Neural Networks excepts an input value ranging from 0 to 1. But there is an advantage of this transformer too! It is not much affected by the outliers. Let’s suppose that median income had a value of 1000 by mistake: Min-Max Scaler will directly rescale all the values from 0-15 to 0-0.015, whereas standardization won’t be affected.\nSckit-Learn has a transformer for this task, StandardScaler.\n5. Transformation Pipelines:\nAs of today, we saw that there are a lot of data transformation steps involved in data pre-processing that must be performed in the right order. The good news is Sckit -Learn has an amazing class for this tedious task to be effortless, it is called the Pipeline class and helps with managing the sequence of this task.\nLet me show what I mean by that:\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler pipe = Pipeline([ ('imputer',SimpleImputer(strategy=\"median\")), ('std_scaler',StandardScaler()) ]) df_tr = pipe.fit_transform(imputer_df)\nWhat this pipeline constructor does is, it takes the list of all the estimators in sequential order. But the last estimator must be a transformer i.e. they should a fit_transform() method. So when we call the pipeline fit transform method, fit_transform is called for every transformer sequentially passing the output of each into its consecutive call and this happens until the fit() method is called(Our final estimator).\nIn our code, we can see that our last estimator is Standard Scaler, which we know is a transformer. Hence, the pipeline has a transform() method that is applied to all the transformers in sequence.\nKey to make the perfect dish lies in choosing the right and proper ingredients!\nTechnically what I mean by the above quote is, if you properly preprocess your data your algorithm will definitely be accurate enough to provide the best results on real data. This is the most important that you should take into considerations while building your data science project.\nToday, In this article we discussed what and why do we need data pre-processing, what are the several benefits that we get if we make functions while preparing the data, and a couple of methods of data preprocessing. Also, do check out the official documentation of each and every transformer we used to get a tighter grip on them.\nI hope enjoyed reading the article. If you found it useful, please share it among your friends on social media too. For any queries and suggestions feel free to ping me here in the comments or you can directly reach me through email.\nConnect me on LinkedIn\nEmail: [email protected]\nThank You !!\nThe media shown in this article on data pre-processing is not owned by Analytics Vidhya and are used at the Author’s discretion.","A look at how guidelines from regulated industries can help shape your ML strategy.\nBy Ben Lorica, Harish Doddi, David Talby.\nAs companies use machine learning (ML) and AI technologies across a broader suite of products and services, it’s clear that new tools, best practices, and new organizational structures will be needed. In recent posts, we described requisite foundational technologies needed to sustain machine learning practices within organizations, and specialized tools for model development, model governance, and model operations/testing/monitoring.\nWhat cultural and organizational changes will be needed to accommodate the rise of machine and learning and AI? In this post, we’ll address this question through the lens of one highly regulated industry: financial services. Financial services firms have a rich tradition of being early adopters of many new technologies, and AI is no exception:\nFigure 1. Stage of adoption of AI technologies (by industry). Image by Ben Lorica.\nAlongside health care, another heavily regulated sector, financial services companies have historically had to build in explainability and transparency to some of their algorithms (e.g., credit scores). In our experience, many of the most popular conference talks on model explainability and interpretability are those given by speakers from finance.\nFigure 2. AI projects in financial services and health care. Image by Ben Lorica.\nAfter the 2008 financial crisis, the Federal Reserve issued a new set of guidelines governing models—SR 11-7: Guidance on Model Risk Management. The goal of SR 11-7 was to broaden a set of earlier guidelines which focused mainly on model validation. While there aren’t any surprising things in SR 11-7, it pulls together important considerations that arise once an organization starts using models to power important products and services. In the remainder of this post, we’ll list the key areas and recommendations covered in SR 11-7, and explain how they are relevant to recent developments in machine learning. (Note that the emphasis of SR 11-7 is on risk management.)\nSources of model risk\nWe should clarify that SR 11-7 also covers models that aren’t necessarily based on machine learning: “quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates.” With this in mind, there are many potential sources of model risk, SR 11-7 highlighted incorrect or inappropriate use of models, and fundamental errors. Machine learning developers are beginning to look at an even broader set of risk factors. In earlier posts, we listed things ML engineers and data scientists may have to manage, such as bias, privacy, security (including attacks aimed against models), explainability, and safety and reliability.\nFigure 3. Model risk management. Image by Ben Lorica and Harish Doddi.\nModel development and implementation\nThe authors of SR 11-7 emphasize the importance of having a clear statement of purpose so models are aligned with their intended use. This is consistent with something ML developers have long known: models built and trained for a specific application are seldom (off-the-shelf) usable in other settings. Regulators behind SR 11-7 also emphasize the importance of data—specifically data quality, relevance, and documentation. While models garner the most press coverage, the reality is that data remains the main bottleneck in most ML projects. With these important considerations in mind, research organizations and startups are building tools focused on data quality, governance, and lineage. Developers are also building tools that enable model reproducibility, collaboration, and partial automation.\nSR 11-7 has some specific organizational suggestions for how to approach model validation. The fundamental principle it advances is that organizations need to enable critical analysis by competent teams that are able to identify the limitations of proposed models. First, model validation teams should be comprised of people who weren’t responsible for the development of a model. This is similar to recommendations made in a recent report released by The Future of Privacy Forum and Immuta (their report is specifically focused on ML). Second, given the tendency to showcase and reward the work of model builders overthose of model validators, appropriate authority, incentives, and compensation policies should be in place to reward teams that perform model validation. In particular, SR 11-7 introduces the notion of “effective challenge”:\nStaff conducting validation work should have explicit authority to challenge developers and users, and to elevate their findings, including issues and deficiencies. … Effective challenge depends on a combination of incentives, competence, and influence.\nFinally, SR 11-7 recommends that there be processes in place to select and validate models developed by third-parties. Given the rise of SaaS and the proliferation of open source research prototypes, this is an issue that is very relevant to organizations that use machine learning.\nOnce a model is deployed to production, SR 11-7 authors emphasize the importance of having monitoring tools and targeted reports aimed at decision-makers. This is in line with our recent recommendation that ML operations teams provide dashboards with custom views for all principals (operations, ML engineers, data scientists, and business owners). They also cite another important reason to setup independentrisk monitoring teams: the authors point out that in some instances, the incentive to challenge specific models might be asymmetric. Depending on the reward structure within an organization, some parties might be less likely to challenge models that help elevate their own specific key performance indicators (KPIs).\nGovernance, policies, controls\nSR 11-7 highlights the importance of maintaining a model catalog that contains complete information for all models, including those currently deployed, recently retired, and under development. The authors also emphasize that documentation should be detailed enough so that “parties unfamiliar with a model can understand how the model operates, its limitations, and its key assumptions.” These are relevant to ML, and the early tools and open source projects for ML lifecycle development and model governance will need to be supplemented with tools that facilitate the creation of adequate documentation.\nThis section of SR 11-7 also has specific recommendations on roles that might be useful for organizations that are beginning to use more ML in products and services:\n- Model owners make sure that models are properly developed, implemented, and used. In the ML world, these are data scientists, machine learning engineers, or other specialists.\n- Risk-control staff take care of risk measurement, limits, monitoring, and independent validation. In the ML context, this would be a separate team of domain experts, data scientists, and ML engineers.\n- Compliance staff ensure there are specific processes in place for model owners and risk-control staff.\n- External regulators are responsible for making sure these measures are being properly followed across all the business units.\nThere have been many examples of seemingly well-prepared financial institutions caught off-guard by rogue units or rogue traders who weren’t properly accounted for in risk models. To that end, SR 11-7 recommends that financial institutions consider risk from individualmodels as well as aggregaterisks that stem from model interactions and dependencies. Many ML teams have not started to think of tools and processes for managing risks stemming from the simultaneous deployment of multiple models, but it’s clear that many applications will require this sort of planning and thinking. Creators of emerging applications that depend on many different data sources, pipelines, and models (e.g., autonomous vehicles, smart buildings, and smart cities) will need to manage risks in the aggregate. New digital-native companies (in media, e-commerce, finance, etc.) that rely very heavily on data and machine learning also need systems to monitor many machine learning models individually and in aggregate.\nHealth care and other industries\nWhile we focused this post in guidelines written specifically for financial institutions, companies in every industry will need to develop tools and processes for model risk management. Many companies are already affected by existing (GDPR) and forthcoming (CCPA) privacy regulations. And, as mentioned, ML teams are beginning to build tools to help detect bias, protect privacy, protect against attacks aimed at models, and ensure model safety and reliability.\nHealth care is another highly regulated industry that AI is rapidly changing. Earlier this year, the U.S. FDA took a big step forward by publishing a Proposed Regulatory Framework for Modifications to AI/ML Based Software as a Medical Device. The document starts by stating that “the traditional paradigm of medical device regulation was not designed for adaptive AI/ML technologies, which have the potential to adapt and optimize device performance in real time to continuously improve health care for patients.”\nThe document goes on to propose a framework for risk management and best practices for evolving such ML/AI based systems. As a first step, the authors list modifications that impact users and thus need to be managed:\n- modifications to analytical performance (i.e., model re-training)\n- changes to the software’s inputs\n- changes to its intended use.\nThe FDA proposes a total product lifecycle approach that requires different regulatory approvals. For the initial system, a premarket assurance of safety and effectiveness is required. For real-time performance, monitoring is required—along with logging, tracking, and other processes supporting a culture of quality—but not regulatory approval of every change.\nThis regulatory framework is new and was published in order to receive comments from the public before a full implementation. It still lacks requirements for localized measurement of safety and effectiveness, as well as for the evaluation and elimination of bias. However, it’s an important first step for developing a fast-growing AI industry for health care and biotech with a clear regulatory framework, and we recommend that practitioners stay educated on it as it evolves.\nEvery important new wave of technologies brings benefits and challenges. Managing risks in machine learning is something organizations will increasingly need to grapple with. SR 11-7 from the Federal Reserve contains many recommendations and guidelines that map well over to the needs of companies that are integrating machine learning into products and services.\n[A version of this post appears on the O’Reilly Radar.]\n- “Managing risk in machine learning”\n- “What are model governance and model operations?”\n- “Becoming a machine learning company means investing in foundational technologies”\n- “The quest for high-quality data”\n- Andrew Burt and Steven Touw on how companies can manage models they cannot fully explain.\n- David Talby: “Lessons learned turning machine learning models into real products and services”\n- Ira Cohen: “Applying machine learning for insights into machine learning algorithms”\n- “You created a machine learning application. Now make sure it’s secure”\n- Jike Chong on “Applications of data science and machine learning in financial services”\n- Gary Kazantsev on how “Data science makes an impact on Wall Street”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:613da520-21ff-4af6-9c1b-931b5135e9d3>","<urn:uuid:587ec635-5e80-45f6-919d-6ee4e2741bb1>"],"error":null}
{"question":"¿Cuáles son los desafíos de traducir expresiones coloquiales en el cine español, y qué soluciones técnicas se implementan?","answer":"La traducción de expresiones coloquiales presenta varios desafíos. En el caso del cine, se utilizan subtítulos que pueden traducir del español latinoamericano al peninsular, como se ve en servicios como Netflix donde hay dos tipos de subtítulos: los que registran literalmente los diálogos y los que traducen. Para superar estos desafíos, los traductores necesitan un profundo entendimiento del vocabulario tanto en el idioma origen como en el destino, ya que una traducción palabra por palabra no suele funcionar. Además, deben investigar no solo el lenguaje sino también la cultura del público objetivo para comprender mejor el contexto y significado de los dialectos o vocabulario coloquial utilizado.","context":["Netflix's decision to subtitle (Y translate in the labels to the peninsular variant) the Spanish in which the protagonists of the successful talk Rome has jumped to the headlines due to the copies that are shown in the only five cinemas that project the film in Madrid, Barcelona and Malaga. Because the service of streamingIt has been offering this option for a long time in many series and films in Spanish. But what in a computer, a telephone or a television is optional some viewers what They have lived in the halls as an imposition. Others, however, consider it an advantage. Be that as it may, the film by Cuarón, which defined the decision in EL PAÍS as \"parochial, ignorant and offensive,\" reopens the debate about the existence of a neutral Spanish, while revealing the strength of a language that resists its uniformity.\nSergio Morera, from Verdi Cinema's Communication, Programming and Marketing team, yesterday gave some clue as to what happened: \"We organized a private pass for Rome, where many people told us that they had a hard time following the movie. We decided that it would be best to have a subtitled version and so we asked Netflix. \" Morera says that, once received, they verified that everything worked from the technical point of view, but did not analyze the content of the subtitles: \"I do not think it has to do with colonialism. Of course, it was not the intention. If it has been perceived as such, we are sorry. We just wanted the majority of the audience to be able to understand the film, and that's why we always chose the original version. We do not intend to blur or cover it at all. \"\nNetflix declined yesterday again to make statements to this newspaper. In the catalog on-line of the company, there are two types of subtitles: those who literally record what the dialogues say and those who translate. These translations always seem to go in the same direction: from Latin American Spanish to Spanish, which is considered a \"neutral Spanish\". The main Spanish series, such as The paper house or Elite, allow you to add subtitles that reflect exactly what is said on the screen. On the other hand, the Peruvian film Single coveted, among others, has up to three versions of the same sequence. On the screen, three ladies bend over to dodge a fly. A scream: \"Wait!\" The subtitles in \"Spanish\" say: \"Wait!\". And if you choose what Netflix calls \"Latin American Spanish\", it appears: \"Beware!\".\nBeyond the doubts it raises about Spanish as a common language, the defenders of the subtitles to approach remember that this has favored the triumph of the Spanish series, as The paper house or Elite, in the Latin American countries where they have passed. And the truth is that more times in the last two decades have subtitled Latin American or Spanish films in the halls of other Spanish-speaking countries. The Colombian women The seller of roses, the Mexican Amores Perros or Argentina Nine Queens are some of the cases. Although the main doubts, beyond the subtitles themselves, surround the choice to adapt them to the Spanish public.\nIn Filmin, another portal on-line With a large presence of Spanish and Latin American cinema, most of these films are presented in pure original version. There are exceptions, like Nine Queens, that offers subtitles, at Rome. And also in Filmin Latino, his Mexican arm, certain films have subtitles in Spanish. Although in your case it is often an economic choice: if you choose to include subtitles, opt for a single version. The translation, in any case, always depends on the producers of the films.\n\"Offering subtitles has always been done and if it allows more people to understand, it does not seem bad. I remember years ago, when I saw And Your Mother TooI would have appreciated them. And also in recent films like The club or The clan\", defends Jaume Ripoll, co-founder and responsible for content and development of Filmin. And it expands the debate: in the 2011 Berlinale market, the film The Irish, English-speaking, it was screened with subtitles in that same language.\nJAVIER RODRÍGUEZ MARCOS\nAndrés Trapiello, who in 2015 published a translation of the Quixote to the current Spanish, he believes that anyone could see 'Roma' without subtitling – \"the Mexican lexicon and Spanish are very close\" – but he does not believe that the garments have to be torn: \"The Mexican original is a delight, but the subtitles are a option that can accompany some viewers, like when you see a movie in a language that you do not completely dominate. I remember seeing an American musical in London that overpowered the parts spoken in American English because there were many street expressions. \" Trapiello believes that the distance that exists between the Spanish of Spain and that of Mexico is smaller than that between the Spanish of the 17th century and the current one. Hence his version of the novel by Cervantes: \"Some will say that he can read the 'Quixote' without a dictionary and without five thousand notes, so great, but there are parts that are not understood. Especially the most close to orality. The continuous sayings of Sancho, for example. What is \"ordering popcorn in the gulf\"? Literally it is to order tiger nuts at sea, that is, \"pears with elm\". Or \"punish me my mother, and I trómpogelas\", that is, \"laugh my mother, one ear enters me and the other comes out\".\n\"When someone asks you 'let's do neutral Spanish,' I wonder who speaks?\", Explains Juan Pablo Villalobos (Guadalajara, Mexico, 1973), who stresses that he has never had to adapt his original writings to a Spanish on demand. \"In Mexico we are very used to reading Spanish from Spain, from Argentina, from Colombia. The problem arises when the publishing or film industry try to make the language more transparent to eliminate slang. But it's no use changing 'asshole' for 'asshole'. It is a fudge to adapt, because there is no difference. \" The reasoning of María Fernanda Ampuero (Guayaquil, Ecuador, 1976), author of Cockfight, who believes that in Latin America they are more accustomed to the slang of Spain because cinema is not dubbed. \"There we do see films in the original language and we understand that there are people who speak differently. We consume Mexican, Venezuelan or Spanish television, \"he says. Emiliano Monge (Mexico City, 1978) adds that \"there are Spanish publishers that seek to clean certain books, make them more neutral\". \"As if neutral Spanish existed! On the other hand, the reverse has never happened to me, \"he says.\nThese demands to strip the language of its particularities have traditionally been in the theater. There are many Latin American performers who for years eliminated any vestige of accent upon arriving in Spain, such as the late Argentine Hector Colomé, who recited verses from the Golden Age as if he had been born in Valladolid. But times have changed and the public does not seem to have a problem to follow a function performed by Latin American voices. Actors such as Héctor Alterio, Miguel Ángel Solá and Fernanda Orazi usually perform in Spanish productions without changing their accent. And one of the great successes of the last years coming from Argentina, The omission of the Coleman family, by Claudio Tolcachir, with dozens of expressions that are not used in the Peninsula, but never in the more than 10 years of this production, in which he has visited several Spanish cities several times, he has retouched: the context and the actors' own interpretation helps to follow the function without problems.\nWith information from Raquel Vidales Y Ana Roca Barber.","Literary translation is the translation of prose works into other languages. Examples of such works include poems, creative novels, modern fiction, essays, and more.\nLiterary translation helps to introduce an author’s ideas and thoughts to a new audience.\nTranslated literary works, such as the classical Homer tales, provide readers with a glimpse into history, philosophy, politics, and much more. On the other hand, contemporary translations help readers to appreciate life in different cultures or countries by expanding their knowledge and experiences through them.\nLiterary translation is one of the most challenging types of translation. With other translations, for example, medical translation, translators have to be careful to provide a word-by-word equivalent translation. However, with literary translation, things are not so straight-forward.\nHere are some challenges that literary translators have to deal with:\nLiterary works are creative text content beautifully woven by the authors to speak to a particular audience segment. The audience typically understands the wordplay and expressions that the author uses to bring out various points.\nWhen translating literary works, translators have to be careful not to disrupt the creative flow of thoughts and ideas. To deal with this problem, translators need to use equivalent interpretive, approximate, and creative words that will bring out the intended meaning in the environment envisioned by the original author.\nAll languages are different, and translators need to be highly creative to correctly map out any ambiguity, assonance, or idiom in the literary works they are translating for a target language.\nBusiness document translation requires high accuracy and localization. On the other hand, literary translation takes things to the next level.\nLiterary translators need an in-depth understanding of the author’s subject matter expertise. The translators also need to ensure their translations do not change the original tone of the author. After all, literary works are all about creative expressions that evoke feelings and understandings. Therefore, literary translations should retain the author’s unique expression and writing style.\nTo ensure that the original tone is captured in the translations, literary translators may need to go the extra mile to learn more about the author. This may involve interviewing the author to understand his/her motivations, background, or life experiences that contribute to the unique literary styles. In cases where the author is unreachable, translators may need to read more of his/her publications to understand the writing style.\nLiterary works are usually written for a specific audience. The author uses familiar cultural and everyday expressions that the audience understands. The use of cultural expressions, even though elementary in the native language of the target audience, can present significant challenges to literary translators.\nFor example, Don Juan is an infamous fictional character in the Legend of Don Juan. The character, which is known for seducing women, grows larger than life, and his name comes to represent a womanizer.\nWhen translating the Legend of Don Juan, literary translators have to decide whether to use the name exactly as it is or give the character an equivalent name that the target audience will resonate with.\nLiterary translations may not make sense for an audience that does not understand the mannerisms or culture of people that speak the native language that the author chose. Therefore, translators need to adapt or change the characters and expressions used in the literary works to resonate with the culture and mannerisms of the target audience’s language.\nThe use of slang and dialects in poems, essays, and other prose works can be challenging for translators. These expressions have to be translated to retain the character’s unique talking style or cultural expression.\nTranslators need an in-depth understanding of vocabulary usage in both the source and target languages to successfully translate dialects and slang. In most cases, translating these prose word-of-word will not make sense. This is why being bilingual is not enough to make someone a great literary translator.\nAn in-depth understanding of both the author’s language as well as the target language is the only sure way of ensuring that the translated works reflect the author’s intended meaning and is understood by the target audience.\nLiterary translators may need to research not just the language but also the culture of the audience that speaks the author’s chosen native language, to have a better understanding of the context and meaning of dialects or slang vocabulary used in literary works. This way, the authors will know the right linguistic expressions to use that will not change the meaning that the author conveyed in the original text.\nThe complexities of translating literary content require specific language skills. Choosing the right words, depending on the purpose and context of the content, is critical to driving the original author’s point home to a new audience. This makes it imperative to hire a professional translation services company that specializes in document translation services.\nThe above are four common challenges of literary translation and how they can be overcome."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:6ceba094-903b-4b2b-b49d-9cc09d963ff0>","<urn:uuid:147e2015-7c01-40dc-9124-7cde2ec5bc4d>"],"error":null}
{"question":"Could you explain in detail how preservation techniques differ between the Antikythera shipwreck site and the Mary Rose, and what unique challenges each site presented to archaeologists?","answer":"The preservation approaches for these shipwrecks were quite different due to their distinct circumstances. The Antikythera wreck, discovered in 1900, has been studied periodically through diving expeditions, with modern scientific excavation involving three-dimensional mapping and the use of remotely operated vehicles for recording and communication. Archaeologists spent 40 hours diving amid the wreckage, using metal detectors and submersible dredges to recover artifacts scattered across a large area. The Mary Rose, however, required a more intensive preservation process as it was raised from the seabed. The ship's remains had to be continuously sprayed at a controlled temperature during initial restoration, and the process took place in a special restoration shed. The vessel had sunk on its side into mud and clay, which actually helped preserve one elevation of the ship remarkably well, though exposed timbers had rotted. The restoration team had to carefully remove and sift through mud to recover artifacts, leading to the discovery of numerous unique and priceless items.","context":["A bronze chair arm — possibly the remains of an ancient throne — and a piece of a Greek board game are among the latest treasures raised from the site of the famous shipwreck Antikythera.\nThe ship, which went down in 65 B.C., sits off the coast of the Greek island of the same name. It was discovered in 1900 by sponge fishermen and has been periodically studied since.\nThis year, archaeologists discovered an intact amphora (a vaselike container), a small table jug (known as a lagynos) and a rectangular chiseled stone, probably a statuette base. Digging on the seafloor, they found broken ceramics, a piece of a bone flute, and broken bits of glass, iron and bronze. A section of bronze furniture may be the arm of a throne, according to the Woods Hold Oceanographic Institution (WHOI). A small glass piece looks to be a pawn in a chesslike game. [See Photos of the Ancient Antikythera Shipwreck and Treasure]\n\"This shipwreck is far from exhausted,\" project co-director Brendan Foley, a marine archaeologist at WHOI, said in a statement. \"Every single dive on it delivers fabulous finds, and reveals how the '1 percent' lived in the time of Caesar.\"\nThe first sponge diver to explore the wreck in 1900, Ilias Stadiatis, managed to bring a bronze arm from a statue up 164 feet (50 meters) to the surface. The Greek government quickly sent naval support to the area, and divers brought up 36 marble statues of heroes and gods, along with other luxury items and skeletons belonging to the crew and passengers. In 1901, the divers brought up an incredible astronomical calendar, the Antikythera mechanism, which could determine the positions of heavenly bodies like Mercury, Venus and Mars. It remains the most complex ancient item ever found, according to the Hellenic Ministry of Culture and Sports.\nThe ministry and the WHOI are currently involved in a long-term project at the site. In 2014, archaeologists conducted the first modern scientific excavation of the wreck, creating a three-dimensional map of the site. But little time was spent in the depths with the wreck because of bad weather.\nThis year, archaeologists were able to spend 40 hours diving amid the wreckage. Along with the cargo they discovered, the researchers also found a lead salvage ring and two lead anchor stocks, lead hull sheathing and nails and wood from the ship itself.\n\"We were very lucky this year, as we excavated many finds within their context, which gave us the opportunity to take full advantage of all the archaeological information they could provide,\" diving archaeologist Theotokis Theodoulou, of the Hellenic Ministry of Culture and Sports, said in the WHOI statement.\nThe archaeologists used last year's 3D map as a guide; a remotely operated vehicle recorded their dives and enabled communications with the surface.\nA metal detector survey revealed metal objects scattered over an area of 131 feet by 164 feet (40 meters by 50 meters), reflecting the huge size of the ship. The researchers used a submersible dredge and pump to dig nine trenches in this area, bringing more than 50 new artifacts to the surface.\nResearchers are now studying these objects, subjecting the contents of the ceramic jars to DNA analysis to find out what kind of food, drink or perfumes are inside. They also plan to analyze the lead objects to learn where the lead was mined.","Conway Maritime established a reputation for a series of books that provided a high quality set of images, many unique, that described the structure and technology of the subjects. Since then, the imprint has passed through a number of hands and each new owner has managed to maintain the high standards originally achieved. This new book has demonstrated that Bloomsbury Publishing is a safe pair of hands for the Conway imprint. There is text and it is very good descriptive material that fully supports the fine selection of images. The subject is unique and this book shows just why the Mary Rose is such an important vessel. Those who filed through the original restoration shed were awed by the remains of this warship. It was a damp cold dark experience because the remaining hull structure had to be sprayed continuously at a controlled temperature during the initial restoration process. The author provides a brief history and introduction with some outstanding images and specially produced drawings. Highly Recommended.\nNAME: Anatomy of the Ship, Tudor Warship Mary Rose\nAUTHOR: Douglas McElvogue\nPUBLISHER: Conway, Bloomsbury Publishing\nBINDING: soft back\nGENRE: Non Fiction\nSUBJECT: Tudor, 16th Century, naval architecture, guns, artillery development, firearms, bows, major warships, line-of-battle, sailing navy, wooden warships, archaeology, restoration\nDESCRIPTION: Conway Maritime established a reputation for a series of books that provided a high quality set of images, many unique, that described the structure and technology of the subjects. Since then, the imprint has passed through a number of hands and each new owner has managed to maintain the high standards originally achieved. This new book has demonstrated that Bloomsbury Publishing is a safe pair of hands for the Conway imprint. There is text and it is very good descriptive material that fully supports the fine selection of images. The subject is unique and this book shows just why the Mary Rose is such an important vessel. Those who filed through the original restoration shed were awed by the remains of this warship. It was a damp cold dark experience because the remaining hull structure had to be sprayed continuously at a controlled temperature during the initial restoration process. The author provides a brief history and introduction with some outstanding images and specially produced drawings. Highly Recommended.\nThe story of the Mary Rose may seem sad, or tragic, but as a warship she enjoyed a relatively long life and, when she sank, she was to become the only restored example of a key development in the history of the sailing warship. She set off from Portsmouth under the eyes of King Henry VIII to take on a French fleet that was offshore. As she turned on leaving harbour, she started to take on water and rapidly sank, with heavy loss of life. There have been several theories as to why she sank as she did, but much information has emerged during her careful restoration.\nSinking on her side, the Mary Rose sank into the mud and clay so that an almost complete elevation was buried. Over the centuries that followed, the exposed timbers rotted but what was buried survived remarkably well. The decision to raise and preserve the remains was a brave one and there was no certainty that the process would be successful, or that anything significant could be preserved. There was simply no comparable experience to give any assurance.\nA special lifting frame was constructed, taken out to the wreck site and fitted around the remains. The lift barge then began the painstaking process of slowly bringing the remains to the surface and then to the restoration shed in Portsmouth’s historic naval shipyards. It was a memorable sight for anyone fortunate to watch the process – one of those great memories that lasts for ever, in every detail.\nOne of the early tasks was in removing remaining mud and sifting it for artefacts. It was only then that the restoration team realized how much unique and priceless items were contained in what was left of the hull.\nMary Rose is now part of a fascinating museum in a new building. It is one of those maritime museums that must be visited.\nThis new Conway book provides a detailed view into a key piece of naval technology. It does this in a unique way with drawings and photographs that are each of the highest quality and finest detail. This tells a story that can only be told in this way and the publisher has done an outstanding job of production, at an amazingly low cover price.\nWhat makes the Mary Rose such an important find and restoration is that not only is there an almost complete elevation, but the team restoring the remains have been able to extract so much additional detail with an amazing collection of fittings and personal equipment, together even with a skeleton ship’s dog. The work that has gone into the archaeology is unparalleled and this is covered very well in the book.\nWhat makes the Mary Rose such an important ship is not just that she is the only vessel of her type to be restored to provide such a detailed collection of knowledge. The Mary Rose was one of a small number of vessels that marked a significant change in naval technology. She carried her guns mainly in broadside batteries and that allowed her and her fleet to sail in line as the first line-battle-ships. She was the Dreadnought of her age, changing naval warfare for ever.\nBefore the Mary Rose and a handful of similar ships, England had maintained very few vessels designed specifically and exclusively for war. King Arthur did build a relatively small navy of longships to defend against Norse attacks, but they do not appear to have been consistently maintained as a naval force, rather they were built, enjoyed a short life and periods passed when few if any were serviceable. The differences between fighting longships and their commercial knar sisters were not great. They were open boats equipped with a single square sail but were normally fought under oar power. As commercial craft developed and became larger, there is no evidence that warships were specifically built for the purpose. It seems that Kings impressed commercial vessels into naval service and carried out modifications that were later removed if the vessel returned to commercial use. The primary modification was to add a castle at stern and bow to provide fighting platforms for archers and spear-men. In some cases, the lookout platform high in the main mast may have been enlarged for archers. As guns became a more common weapon, these were added to impressed merchant craft but were issued in small numbers and mainly sledge mounted. The common round in use was stone balls. The Mary Rose also used stone ball ammunition but her guns were capable of firing iron shot.\nHenry VIII was a keen developer of naval and land power. He commissioned castles, designed specifically for guns, and watched the Mary Rose sink from his vantage point at Southsea Castle which was, and is, a fine example of the Tudor gun fort. He also went to considerable effort to acquire modern guns, both artillery and personal firearms. As with the construction of castles designed to mount guns, he encouraged the building of ships to carry guns. The Mary Rose did include a large number of bows with a large supply of arrows, but she also carried personal firearms. What is notable is the extent of her gun armament and the size of the largest pieces.\nThe Mary Rose was a Great Castle ship. She introduced the broadside as a method of engagement, although she carried a number of smaller guns in the fore and aft castles, including guns that could fire down into the waist of the ship against boarders or mutineers. Most interestingly, she carried 40 pounder long guns on her main gun deck, larger than those carried in 1805 by HMS Victory. Tudor heavy naval guns had a greater range than those of Nelson’s day. What is not entirely clear is why the Tudor warships carried such a range of firearms. Some have speculated that this demonstrates an exciting period in the development of the gun where many different sizes and forms were experimented with. Alternatively, it has been suggested that the variation was a matter of supply rather than preference of or experiment. There is certainly evidence that Henry VIII pursued a number of sources of supply in an attempt to acquire the numbers of guns he required.\nEven in 1805, a major warship would carry a number of sizes of gun, with the lowest gun deck carrying the largest guns that were all of one type and size. The next gun deck carried smaller guns and the third gun-deck the smallest long guns, but with the heaviest guns being short range cannonades, or smashers, on the same deck. Swivel guns were carried on the upper deck and in the fighting tops, being used also on the larger ships boats. Against the mixed armament of 1805 on major warships, perhaps the Tudor use of a number of different types of gun is not remarkable. However, the 1805 RN warship did have a standard gun format of cast iron canon, mounted on four wheel trucks, and a small number of short barrel heavy guns on slide mounts. In shape and construction, the 32 pounder, 24 pounder and 12 pounder guns where the same, differing only in size, and using the same design of four wheel truck mount. The Mary Rose demonstrates a variety of mounts, including four wheel wooden trucks, sledge mounts and swivels. The muzzle loading cast gun was in common use, but the forged gun was equally common and, although forged guns were prone to the forging breaking down, with a short in service life, it has been argued that they were cheaper and quicker to produce, offered improved performance initially, and also offered a higher rate of fire because they were supplied with three chambers that were charged and breech loaded one after the other. Where the forged canon started out on a wooden sledge, the Mary Rose demonstrates the next development where two wheels were mounted to the front of the carriage but a heavy elevating quadrant was the third point of contact with the deck. This arrangement would have improved the control for gun laying. As long as the gun was breech loaded, it would not have required running fully out after reloading, but recoil on firing would have driven the gun inboard and running it fully out to fire again would have required more effort than for a four wheel truck mount.\nThe armament and other items of equipment have been as carefully illustrated as the ship and its component parts. As the Mary Rose is not a complete elevation, the drawings of the hull have included an estimate of the forecastle, most of which had rotted away. As with other books in the series, the flaps of the cover fold out to reveal full hull drawings. In all, there are more than 200 drawings and plans, together with more than 40 reference photographs. Although the original readership for the series was expected to be model makers and serious naval history enthusiasts, this book and other books in the series provide a level of artwork to attract a much wider readership and the story of the Mary Rose and her restoration will appeal to many interests."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c844965b-8f13-4366-aebf-e6f5bc250807>","<urn:uuid:7ed16747-603b-464c-aba0-eabe436adca6>"],"error":null}
{"question":"How should middle school students be taught about sentence conventions?","answer":"Middle school students should focus on experimenting with sentence length to understand its effects on readers and reading comprehension. They should be able to create complex sentences using proper punctuation to combine dependent and independent clauses. They should also practice embedding information using subordinate clauses and learn techniques like nominalization to create effective scientific sentences.","context":["which happen to be found in high-frequency. The earlier studenta€™s structure ought to include homophones, a€?spelling demonsa€™ as well as other terms which can be generally misspelled. To assist them to develop various conventions for subject matter you can easily build particular structure being uploaded on various bulletin panels or topic sheets. It will help offspring discover and produce the core language or language of a certain room. Cunningham and hallway (2002) suggest that you create youngsters with a folder because of their word wall structure sheets so that they can access all of them wherever they can be studying and they also can develop unique customized essaywriters us spelling layer.\nElementary pupils should be encouraged to need phonetic spelling as a placeholder when they uncertain of a spelling during their basic draft. They’re able to then see in order to find the most effective spelling from the keyword throughout the modifying procedure.\nHow-to Show Usage\nBasic editing formula, such as subject-verb contract, verb tense persistence, and pronoun consumption, must be instructed to top basic youngsters. As people experience a more substantial array of genres they are able to learn that various types incorporate different verb tight. Past tight is for narratives and recounts of technology experiments. Present tight is utilized for states, training, quality recipes, and information. Future tight can be used for plans and proposals.\nHow exactly to Train Sentence Development\nSentence fragmentation generally takes place when a student features problems with incorporating straightforward phrases into a complicated one which uses subordinate conditions. A teacher can help students awareness within these problems by giving them sentence combining tasks that demonstrate pupils different methods of conjoining sentences into one using the appropriate punctuation.\nAnother problem older pupils encounter while wanting to form more complicated sentences are a run-on sentence. These offer a teacher making use of best chance to instruct their pupils just how to identify the the different parts of address, instance nouns, verbs and matching conjunctions. By learning these address portion, pupils can separate run-on phrases into separate conditions. Run-on sentences sometimes happen as college students would you like to highlight the way the two sentences were interlinked. Training children strategies for a semi-colon properly often helps resolve this issue. The beginner can also be encouraged to need additional punctuation marks to show the connection between conditions in a complicated sentence.\nTips illustrate events in center and senior school\nPeople with entered secondary school have learned the basic exhibitions of composed code and ought to bring a developed a language that allows these to talk about the way they make use of these exhibitions in their publishing.\nFirst of all center and high school students should find out is how to change their own operate as a result it contains the proper events. The best way to get them to revise their own work with in this way is through having them check out exactly how various exhibitions are widely-used in almost any styles, and getting these to talk about the influence that certain convention is wearing the reader. By discussing and examining rhetorical devices, they will learn to make use of the devices by themselves. Not only will this enable them to comprehend the convention at a word and phrase levels it will even allow the chips to discover how grammar events can aid the readera€™s understanding of a text all together. They should also be exposed to article authors whom purposefully resist these events for literary effect through poetry and literary works. This may furthermore assist them in understanding how convention notifies your reader.\nJust how to Show the Auto Mechanics\nPupils who are in middle school needs to have a good understanding on conventions instance spelling, punctuation, and paragraphing. They need to target creating their particular specific vocabularies that will aid all of them in producing area associated messages in place of building processes to support spelling. Spelling tips by this level must sturdy and well toned. Center schoolers ought to know ideas on how to eliminate their unique misspellings throughout editing phrase by cross-referencing phrase with dictionaries.\nAn emphasis is put on teaching college students utilizing events which are certain to genres. For instance, how informal letters to company vary from official business letters, capitalization in poetry, the effective use of headings and sub-heading to co-ordinate text, and convention for citations.\nSimple tips to Show Sentence Usage\nCollege students should understand base level knowledge of phrase consumption, such as for example word order, subject-verb contract, verb tense, and appropriate use of modifiers by sixth grade. They need to beginning to incorporate nominative, unbiased and possessive pronouns properly if they are in middle school. Extension of this expertise was motivated by making use of proper dialect and comparing use in almost any options a€“ official, ethnic, and local versus regular English.\nJust how to Instruct Sentence Development\nMiddle school students should-be experimenting with phrase size to see the effects they’ve got about audience and checking out awareness. They must be able to produce complex sentence when using the appropriate punctuation to combine based upon and independent conditions.\nKids start to establish their own writing further by focusing on how in order to create their sentences and sentences to accomplish certain results. They need to understand how to design their own phrases in parallel to assist checking out comprehension. Phrases and sentences should-be planned to focus on the information which they offer regarding their topic and stay laid out in a manner that grows their particular discussion rationally.\nKids should engage in embedding info in a sentence simply by using a subordinate term. A great way to training this can be by using phrase mixing activities. They need to understand practices like nominalization that converts verbs into nouns so that you can produce heavy useful sentences that have a scientific angle.\nTo enhance automaticity daily authorship activities must introduced into scholar’s course. To start with, the task need single-draft publishing just; the college student ought to be permitted to make use of phonetic spellings (eg. Wimin, enuf, nashion) as well as their work should stay unedited. Whenever college students are suffering from and reinforced her unconscious capability to build vocabulary, the concept of grammar meeting need launched by the instructor.\nEmphasizing exhibitions very early isn’t only damaging to automaticity additionally to the studenta€™s determination. As teachers and moms and dads include fast to point problems in book production, pupils lose self-esteem within ability to compose. If youngsters were praised with regards to their ideas very first, they would be much more determined and confident. Cunningham et al. (2003) advise the ultimate way to develop this self-confidence is through having people promote their unique earliest drafts in an optimistic manner in which focus best to their strategies rather than throughout the modification of errors.\nExhibitions should-be taught at the conclusion of composing techniques while in the modification term whenever pupils are becoming their particular efforts prepared upload. As perform can be targeted to a specific market, the exhibitions employed for the targeted studying class will alter, and college students will be more determined to use events of professional vocabulary to your piece."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d2b34559-3f31-4e1a-89b4-a4fd147537fc>"],"error":null}
{"question":"Which type of sleep is more essential for our health: REM or deep NREM sleep?","answer":"Deep NREM sleep is more essential for health than REM sleep. During deep NREM sleep (stage 3), the body performs crucial physical renewal, hormonal regulation, and growth. Without deep sleep, people are more likely to get sick, feel depressed, and gain unhealthy weight. Additionally, during this stage, the body heals wounds, creates white blood cells, restores muscles, and releases growth hormone. While REM sleep is important for brain development in childhood and memory processing, studies show that adults deprived of REM sleep don't develop major health problems, whereas NREM sleep deprivation has more significant consequences.","context":["- Why do we wake up at 3am?\n- Is dreaming a sign of good sleep?\n- What sleep stage makes you feel most rested?\n- What stage of sleep does your body repair itself?\n- Are there 4 or 5 stages of sleep?\n- How accurate is Fitbit sleep?\n- Why do I wake up at 2am for no reason?\n- What happens to our brains when we sleep?\n- What are the 4 stages of NREM sleep?\n- What are the different stages of sleep?\n- What is the deepest sleep stage?\n- How long does it take to fall into deep sleep?\n- How many stages of sleep do you go through during the cycle?\n- How can I increase my deep sleep time?\n- Which sleep state is most important?\n- What is the most important sleep stage?\n- Does exercise increase deep sleep?\n- What causes poor deep sleep?\nWhy do we wake up at 3am?\nYou wake up at 3am because this is the time you shift from a deep sleep into a lighter sleep.\nIf you turn in at 11pm, by three in the morning you’re mostly out of deep sleep and shifting into longer periods of lighter sleep, known as REM..\nIs dreaming a sign of good sleep?\nDreaming is normal and a healthy part of sleeping. Dreams are a series of images, stories, emotions and feelings that occur throughout the stages of sleep. The dreams that you remember happen during the REM cycle of sleep. REM means rapid eye movement.\nWhat sleep stage makes you feel most rested?\nThe third stage of non-REM sleep is the deepest sleep phase—it’s the one that makes you feel well rested and energetic the next day. This stage of sleep is also when the body repairs and regrows tissues, builds bone and muscle and strengthens the immune system.\nWhat stage of sleep does your body repair itself?\nN3 sleep is a regenerative period where your body heals and repairs itself. The first episode of Stage N3 lasts from 45-90 minutes. Subsequent episodes of N3 sleep have shorter and shorter time periods as the night progresses.\nAre there 4 or 5 stages of sleep?\nDuring sleep, the body moves through five different stages of both REM (rapid eye movement) and NREM (non-rapid eye movement) sleep. Over the course of the night, the body will go through this five-stage cycle four to six times, spending an average of 90 minutes in each stage.\nHow accurate is Fitbit sleep?\nIn reference to PSG, nonsleep-staging Fitbit models correctly identified sleep epochs with accuracy values between 0.81 and 0.91, sensitivity values between 0.87 and 0.99, and specificity values between 0.10 and 0.52.\nWhy do I wake up at 2am for no reason?\nWaking up in the middle of the night is called insomnia, and it’s a common problem. Mid-sleep awakenings often occur during periods of stress. Over-the-counter sleep aids rarely offer significant or sustained help for this problem.\nWhat happens to our brains when we sleep?\nSleep is important to a number of brain functions, including how nerve cells (neurons) communicate with each other. In fact, your brain and body stay remarkably active while you sleep. Recent findings suggest that sleep plays a housekeeping role that removes toxins in your brain that build up while you are awake.\nWhat are the 4 stages of NREM sleep?\nThere are two types of sleep, non-rapid eye-movement (NREM) sleep and rapid eye-movement (REM) sleep. NREM sleep is divided into stages 1, 2, 3, and 4, representing a continuum of relative depth. Each has unique characteristics including variations in brain wave patterns, eye movements, and muscle tone.\nWhat are the different stages of sleep?\nSleep StagesStage 1 non-REM sleep is the changeover from wakefulness to sleep. … Stage 2 non-REM sleep is a period of light sleep before you enter deeper sleep. … Stage 3 non-REM sleep is the period of deep sleep that you need to feel refreshed in the morning. … REM sleep first occurs about 90 minutes after falling asleep.More items…•\nWhat is the deepest sleep stage?\nElectroencephalography. These four sleep stages are called non-rapid eye movement (non-REM) sleep, and its most prominent feature is the slow-wave (stage IV) sleep. It is most difficult to awaken people from slow-wave sleep; hence it is considered to be the deepest stage of sleep.\nHow long does it take to fall into deep sleep?\nUsually, REM sleep happens 90 minutes after you fall asleep. The first period of REM typically lasts 10 minutes. Each of your later REM stages gets longer, and the final one may last up to an hour.\nHow many stages of sleep do you go through during the cycle?\nYour body cycles through five stages of sleep each night: four stages of non-REM sleep and one stage of REM sleep. During these sleep cycles, our breathing, heart rate, muscles, and brain waves are all affected differently.\nHow can I increase my deep sleep time?\nFor example, taking a hot bath or spending time in a sauna before bed may help improve your sleep quality. Eating a low-carbohydrate diet or taking certain antidepressants may also promote deep sleep, though more research is needed in this area. Getting enough sleep in general may also increase your deep sleep.\nWhich sleep state is most important?\nThe work sleep does Different yet equally important restorative work happens during deep sleep (stage 3) and REM sleep (stage 4). Deep sleep is crucial for physical renewal, hormonal regulation, and growth. Without deep sleep, you’re more likely to get sick, feel depressed, and gain an unhealthy amount of weight.\nWhat is the most important sleep stage?\nScientists agree that sleep is essential to health, and while stages 1 to 4 and REM sleep are all important, deep sleep is the most essential of all for feeling rested and staying healthy. The average healthy adult gets roughly 1 to 2 hours of deep sleep per 8 hours of nightly sleep.\nDoes exercise increase deep sleep?\nBased on available studies, “We have solid evidence that exercise does, in fact, help you fall asleep more quickly and improves sleep quality,” says Charlene Gamaldo, M.D. , medical director of Johns Hopkins Center for Sleep at Howard County General Hospital.\nWhat causes poor deep sleep?\nInsomnia has many possible causes, including stress, anxiety, depression, poor sleep habits, circadian rhythm disorders (such as jet lag), and taking certain medications.","Despite appearances, sleep is much more than a state of inactivity. There are 4 stages of sleep that make up two completely different types of sleep. These two types of sleep are:\n- NREM Sleep – A very relaxed type of sleep that gets progressively deeper\n- REM Sleep – A very active type of sleep where the brain is almost as active as if it was awake\nThe body cycles between these two types of sleep every 90 minutes in what’s called the sleep cycle. See the stages of sleep article for a diagram of types of sleep we enter into during different parts of the night.\nBut why do we have two different types of sleep? Wouldn’t it make sense if we just went into deep NREM sleep and stayed there? Why are we interrupted by active REM sleep when the quality of our sleep is defined by how much time we spend in refreshing deep sleep?\nTo find answers to these questions, lets look at the benefits each type of sleep provides for clues as to why we have these two different types of sleep.\nNREM – The First Type of Sleep\nNREM sleep is what people most commonly associate with sleep. Your brainwaves become progressively slower until you reach the deepest stage of sleep, NREM stage 3, or slow wave sleep as it’s commonly called.\nDeep sleep is essential for restoring energy and allowing the body to undergo maintenance. While in NREM sleep:\n- Wounds are healed\n- white blood cells are created to aid your bodies defenses\n- The muscles are restored\n- The growth hormone is released\nWithout enough deep sleep, all of these processes will be hampered.\nWhile under the deepest levels of sleep the brain reorganizes the many mental pathways within the cortex. This process is critical for learning and ongoing brain development.\nREM – The Second Type of Sleep\nWhilst in NREM sleep the brainwaves become progressively slower and deeper, while in REM sleep the brainwaves are almost identical as if you’re awake. REM sleep is known as paradoxical sleep. It may look like you’re fast asleep, but there’s a lot happening inside your brain.\nFor this reason REM isn’t really what we would consider sleep. We’re not asleep in the sense that the body turns itself off, but we’re not awake either. So what’s the point of REM if we’re neither asleep nor awake?\nWhen you’re awake the majority of your mental input is coming from your surroundings, but when you’re in REM, all this mental stimulus being generated by the brain itself.\nThis internal mental processing is believed to help to process memories and form new skills. The brain is awake but the body isn’t, so it’s the perfect time for your brain to reorganize the vast amount of thoughts gained during the day.\nDreams, which make up a lot of REM sleep, are believed to help you with this too. Songbirds have been shown to practice new mating calls in their dreams that they heard then they were awake.\nWhy Do We Have Two Types of Sleep?\nYou go into NREM sleep as soon as we fall asleep, suggesting that NREM might be the most important. REM only gets its turn later in the night after you’ve first completed a cycle of NREM sleep.\nWhen you’re deprived of NREM sleep, the body tries to make up for it whenever it next gets the chance. The body also tries to make up for REM, although not as much as it does for NREM.\nWhile we need NREM, we don’t appear to need REM sleep after childhood. The brain stimulation is critical for proper brain development. When we were still a fetus or a small child who hadn’t yet developed the senses, REM sleep is the only proper stimulation the brain would get. But when we’re older we can get this stimulation from the outside world.\nPeople deprived of REM sleep, either through an experiment or through the use of some types of antidepressants (that have the effect of blocking REM sleep) don’t appear to develop any major health problems.\nREM interrupts deep sleep in 90 minutes cycles throughout the night. Maybe this is because deep sleep cannot be sustained for long periods of time. Such inactivity could be bad for the brain. Maybe coming out of deep sleep prevents us from having too much sleep, which we know can be harmful.\nMaybe then REM is a way of waking up the body, without causing you to regain awareness. If the body woke you up then you would be faced with having to fall asleep again. Many people find it hard to fall asleep the first time at night, never mind after every 90 minutes of previous sleep. Instead, if the brain becomes active but keeps you asleep, it removes the need to wake up and fall back to sleep, and the brain can send you back to sleep whenever it wishes.\nPerhaps also REM sleep aids our survival by making us semi aware of our surroundings every 90 minutes. Being deeply asleep increases the risk that we won’t be alert enough to be alerted to danger. But in REM sleep, we would be able to wake up alert and act quickly.\n* * *\nThere still remains a lot about sleep we don’t know about. Whilst there are many theories as to why we sleep there’s no one answer that all scientists can agree on. The process of falling asleep is much more complex than scientists once thought. Why we have two types of sleep and which type of sleep is best? We have hints that they both contribute to our restoration and development in unique ways, but no one yet knows for sure."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c93d0dd1-2ac4-451c-aabf-01699b881ac6>","<urn:uuid:c1304ee4-b228-4513-ade3-62fec30d486c>"],"error":null}
{"question":"How did early Christians use the fish symbol to secretly communicate during times of persecution, and what deeper meaning did this symbol hold?","answer":"The fish symbol (ichthys) was used by early Christians during persecution, particularly when they were being persecuted by Nero in the first century. Christians would meet in the catacombs below Rome and post the fish symbol to indicate their presence. Beyond its use as a secret sign, the symbol had deeper religious significance - the first letters of five Greek words for 'fish' (ichthys) created an acrostic meaning 'Jesus Christ, Son of God, Savior'. This made it one of the earliest and most significant Christian symbols that has been carried forward through history.","context":["hrough the centuries, symbols have played an important part in Christianity. The best-known and most important one of these is the cross, which reminds believers of the death and resurrection of Jesus Christ. According to websites regarding church history, the cross emerged as a powerful Christian symbol in the second century. Likewise, other symbols emerged over time as well, and they were used to preserve and explain the faith from one generation to the next.\n“As you go through time and look back at the general population, you would have an educated section of society and a larger uneducated section,” said Dr. Charles Stephenson, chairman of the Biblical Studies department at Lubbock Christian University. “The symbols would speak to them, and they would relate to those icons. They were used to tell the story. Things like stained glass windows and murals would feature this iconography.”\nIn the earliest days of the church, the symbols were used only by members, but as Christianity spread, their use became more common in some traditions. There are examples of symbols in Western Christianity and Eastern Christianity.\n“Some traditions such as Church of Christ do not encourage the use of symbols,” said the Rev. Bill Couch, senior pastor at LakeRidge United Methodist Church. “Non-denominationals and Baptists typically don’t use liturgical symbols, either.”\nOne early symbol still in use today is the ichthys, the sign of the fish. “It was a very common sign among early Christians,” said the Rev. Gary Kirksey, senior pastor at City View Fellowship. “When Christians were being persecuted by Nero (in the first century), they would meet in the catacombs below the city of Rome and post that symbol as a way of saying, ‘We’ve been here.’ It was a powerful symbol.”\nThe first letters of five Greek words for the word for fish (ichthys) create an acrostic that means “Jesus Christ, Son of God, Savior,” explained Stephenson. “It’s one of the earliest symbols, and one that has been carried forward. It became a significant symbol early on.\n“When you had higher illiteracy rates, you would see these sorts of symbols incorporated into stained glass windows and murals. They would see the symbols, and that would help them remember the story.”\nThe fish symbol is one of several Christograms that typically form an abbreviation for the name of Jesus Christ. Likewise, a series of symbols representing Christ is known as Chrismon, which can refer to the small number of symbols or to certain Christmas ornaments. The word Chrismon represents the merging of the words “Christ” and “monogram.”\n“There is not a lot about Chrismon in early church history,” said Stephenson. “I don’t know how recent it is. It seems to be about personal material or things people use such as supposed ancient images or forms that have been resurrected for some use on Christmas trees. It is more of an individual thing.”\n“They represent abbreviations for Jesus Christ. Around the church, there has always been a segment in the church, because of the Old Testament background that says no images, they have railed against this, and it’s become a divisive matter at times. You can look back at church history and see there have been wars over symbols.”\nCouch said traditions that might use Chrismon symbols more frequently include Catholics, Episcopalians and United Methodists. “Rather than using secular symbols of Christmas such as Santa and reindeer or colorful decorative balls, the Chrismons provide symbols that remind us of the meaning of Christmas,” he said.\nThe symbols could represent the events of Christmas — the star, the manger, the wise men. Or they can be symbols that represent Christ such as the cross and various Greek letters such as capital Chi and Rho, which spell Christ in Greek, or Alpha and Omega, the first and last letters of the Greek alphabet that Christ uses to refer to himself in the New Testament book of Revelation. “I think Chrismons — especially homemade ones — would help people stay focused on the message of Christmas,” Couch said.\nAccording to the Ascension Lutheran Church (Danville, Va.) website, the first Chrismon ornaments were developed by Frances Kipps Spencer in 1957. Her intent was to create ornaments appropriate for a church Christmas tree.\n“We had a Chrismon tree at First United Methodist Church in Plainview,” Couch recalled. “The church had a decorating party where families came and made Chrismons. Some they hung on the tree in the sanctuary and some they took home. I also remember making Chrismons in Sunday school when I was elementary age at my church in Dallas.”\nThe symbols of the faith still have impact and relevance for believers today. “These symbols are like object lessons that communicate so much more than you can say in words,” Kirksey said. “The cross embodies that. It’s like what more can you say?”\nAnother Christmas symbol is the evergreen tree, which has symbolized rebirth since ancient times, according to an article on the Our Redeemer Lutheran Church website. The tree tradition can be traced to the Middle Ages and the Paradise tree that was featured in simple plays depicting the fall of man and the promise of a coming savior. People became so accustomed to the plays that they began putting up their own Paradise tree on Dec. 24. They would also light a large candle on that day which symbolized Christ, the light of the world.\nAs all three men pointed out, the symbols provide reminders about the meaning of Christmas beyond the commercialized holiday Dec. 25 has become. “The push to pull older Christmas symbols and use them as tree ornaments could be a cry to put Christ back in Christmas,” Stephenson said. “It is the time of year in the church calendar when people turn to the coming of Jesus Christ into the world, and the church battles a secularized world thinking about what to buy for someone else.“It can be easy to miss the real message of Christmas, which is what churches focus on: Who Jesus Christ is and the claims he makes on our lives.” F"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:8490c12f-32ed-4e99-a7e6-59784943e831>"],"error":null}
{"question":"How do WiFi sniffers and spectrum analyzers differ in their capabilities for monitoring wireless networks, particularly when it comes to detecting issues in the 2.4 GHz band?","answer":"WiFi sniffers and spectrum analyzers serve different monitoring purposes. WiFi sniffers, like CommView for WiFi, capture and analyze raw packets sent over airwaves, can track IP connections, record VoIP sessions, and allow packet filtering and decryption when connected to a network. In contrast, spectrum analyzers like Wi-Spy DBx are specifically designed to reveal RF interference in the spectrum that's invisible to the naked eye, such as interference from cordless phones on channels 3-7 in the 2.4 GHz band. While sniffers focus on network traffic and packet analysis, spectrum analyzers are essential for identifying oversaturation and non-WiFi interference in frequency bands.","context":["One way to bolster your understanding of Wi-Fi security is to do some hacking yourself. That doesn’t mean you should infiltrate a company’s network or snoop on a neighbor’s setup. Rather, ethical hacking and legitimate Wi-Fi penetration testing – done in cooperation with the network owner – can help you learn more about the strengths and limitations of wireless security. Understanding potential Wi-Fi vulnerabilities can help you to better protect the networks you manage and ensure safer connections when you access other wireless networks.\nStart with a Wi-Fi stumbler\nGeneral purpose Wi-Fi stumblers are the simplest and most innocent tools to add to your pen testing kit. Though typically passive tools, they serve an important purpose. They allow you to see what access points (AP) are nearby and their details, such as the signal level, security/encryption type, and media access control (MAC) address. It’s a tool even a hacker would utilize to find the next victim.\nUsing a stumbler, you might find networks set with weak security, like WEP or the original version of WPA. Or, a walk throughout a property might reveal rogue APs set up by employees or others that could be opening your network to attack. Even if there are APs set with a hidden or non-broadcasted service set identifier (SSID), some stumblers can quickly reveal them.\nVistumbler is an open source Windows application that displays the basic AP details, including the exact authentication and encryption methods, and can even speak the SSID and signal level.\nIt also displays graphs of signal levels and channel usage. It's highly customizable and offers flexible configuration options. Vistumbler supports AP names to help distinguish them, also helping to detect rogue access points. It also supports GPS logging and live tracking within the application using Google Earth.\nIf don’t want to lug around a laptop and have a mobile device, consider using the AirPort Utility on your iOS device or a download an app on your Android. If a free stumbling app doesn’t cut it, check out our review of some commercial options.\nWifi Analyzer is a free Android app you can use for finding access points on your Android-based smartphone or tablet. It lists the basic details for access points on the 2.4-GHz band and on supported devices on the 5-GHz band as well.\nYou can export the access point list (in XML format) by sending it to email or another app or take a snapshot of the screens. It also features graphs showing signals by channel, history and usage rating, and it has a signal meter feature to help find access points.\nWi-Fi sniffers go further than stumblers. Instead of just grabbing the network details, sniffers capture and show and/or analyze the raw packets sent over the air waves. Captured traffic can be imported into other tools, such as an encryption cracker. Or, some sniffers have the functionality included to do some analysis or cracking as well. In addition, some sniffers look for and report only on certain network traffic, such as those designed to reveal passwords sent in clear-text.\nCommView for WiFi is a popular commercial Wi-Fi sniffer and analyzer that offers a 30-day limited trial. It has a stumbler feature to show network details, plus channel utilization stats and graphs. It can track IP connections and records any VoIP sessions. The tool also lets you capture and see the raw packets.\nIf you’re connected to a Wi-Fi network, you can input its PSK passphrase so the decrypted packets will be shown. You can also set rules to filter the data you see and set alarms to track rogue devices. Other cool features include a traffic generator to do some spoofing, node reassociation to manually kick clients off, and TCP reconstruction to better view the captured data (text or photos).\nKismet is an open source Wi-Fi stumbler, packet sniffer, and intrusion-detection system that can run on Windows (with WSL framework), Mac OS X, Linux, and BSD. It shows the access point details, including the SSID of \"hidden\" networks. It can also capture the raw wireless packets, which you can then import into Wireshark, TCPdump, and other tools. In Windows, Kismet only works with CACE AirPcap wireless adapters due to the limitation of Windows drivers. It does, however, support a variety of wireless adapters in Mac OS X and Linux.\nTools reveal Wi-Fi details\nWirelessKeyView from NirSoft is a simple yet neat tool that lists all the WEP, WPA, and WPA2 keys or passphrases stored on the Windows computer you run it on.\nAlthough it was pretty easy to reveal saved keys in Windows 7 and prior versions via the usual Windows GUI, Microsoft made it more difficult in Windows 10. WirelessKeyView quickly gets you an exportable list of all saved networks no matter the OS versions.\nTools like WirelessKeyView can reveal how a compromised or stolen device may contain sensitive information beyond documents. It also shows the importance of using 802.1x authentication, where users would have individual login credentials for the Wi-Fi and aren’t susceptible to this type of issue.\nAircrack-ng is an open source suite of tools to perform WEP and WPA/WPA2-Personal key cracking.\nIt runs on Windows, Mac OS X, Linux, and OpenBSD. It's also downloadable as a VMware image and Live CD. You can view nearby Wi-Fi networks, including hidden or non-broadcasted SSIDs. You can also capture the raw packets, inject and replay traffic, and possibly crack the encryption keys once enough packets have been captured.\nGet a full software suite with a Linux distro\nOne of the most popular pen testing distros is Kali Linux. In addition to a typical Linux OS install on a computer, you can make a live bootable disc or download VMware or VirtualBox images. It contains a huge list of security and forensics tools, some of which you can utilize for Wi-Fi pen testing. For instance, Kismet and Aircrack-ng tools are included.\nA few of the other Wi-Fi tools included with Kali Linux are Reaver to hack a network via an insecure WPS PIN, FreeRadius-WPE to perform man-in-the-middle attacks on 802.1X authentication, and Wifi Honey to create a honey pot to lure in clients to connect to a fake AP in hopes of capturing their traffic and performing man-in-the-middle attacks.\nGo all out with a hardware tool\nIf you’re really serious about wireless security and playing around with its vulnerabilities, you have to get a taste of WiFi Pineapple. It’s a hardware-based solution specifically designed for Wi-Fi auditing and pen testing. You can scan, target, intercept, and report on many wireless threats and weaknesses.\nWiFi Pineapple has a router-like look and feel, including its web GUI.\nYou can do things like see client details of each AP, send de-authentication packets, and automatically create fake APs by mimicking nearby SSIDs for some man-in-the-middle fun. You can also capture the web browsing data of others and spoof DNS replies to confuse users or send them to spoof sites.\nWiFi Pineapple currently offers two hardware options: a pocket-sized single-band NANO starting at $99.99 and a router-like dual-band TETRA (see a full review) starting at $199.99.\nEric Geier is a freelance tech writer. Keep up with his writings on Facebook or Twitter. He’s also the founder of NoWiresSecurity, which provides a cloud-based Wi-Fi security service, and Wi-Fi Surveyors, which provides RF site surveying.","This article is part of the \"Why is My WiFi slow?\" training series.\nAll too often, the culprit of slow WiFi is use of the 2.4 GHz band, which offers slower data rates and is often oversaturated with WiFi and non-WiFi devices, like microwave or baby monitors.\nMost WiFi devices are designed to steer back and forth from one band to another as they move around. Since the 2.4 GHz wavelength is longer and pierces through walls and material more easily, devices will use the 2.4 GHz band as they get farther away from the router or AP. However, sometimes they get stuck on the 2.4 GHz band even when they are close enough to use the 5 GHz.\nIn the example above, a ChromeCast and Apple device are associated to the 2.4 GHz band. The 1st generation ChromeCast is not capable of using the 5 GHz band and therefore won’t be steering to it anytime soon. The Apple device, however, can and should steer to the 5 GHz band whenever possible.\nHow do you know if the 2.4 GHz band is oversaturated? Well, you can’t… unless you have a spectrum analyzer. Since RF is invisible to the naked eye, you must use a spectrum analyzer like the Wi-Spy DBx in order to see it. With a Wi-Spy DBx plugged in, inSSIDer will show you exactly how much the spectrum is being utilized and display any significant interferers. The image below shows a cordless phone wreaking havoc on channels 3-7. The interference from this cordless phone would be invisible without a Wi-Spy.\nOK, how do you know if any clients are using the 2.4 GHz band? Luckily, with a MetaGeek Plus subscription and compatible wireless adapter, inSSIDer can show you this.\nPlug the adapter into your computer and open inSSIDer.\nFind your your wireless network’s name, or SSID, and dive into the Network Details by clicking the binoculars icon.\nKeep an eye on the clients column and see how many clients are on each radio on your network. Find your 2.4 GHz radio (hint: it will be on channel 1-13) and click the binoculars icon again.\nIf the problematic device is listed under your 2.4 GHz radio, try turning the WiFi on that device off and on again to ensure it’s choosing the optimal band.\nIf your device is still stuck to the 2.4 GHz band, see if you can log in to your router configuration utility and disable the 2.4 GHz radio entirely, forcing all traffic to the 5 GHz band. Not recommended if you have really old devices like a Nintendo Wii or 1st generation ChromeCast! Not all routers will let you do this. If yours doesn’t, go to Step 7.\nFrom your router configuration utility, give each band its own SSID name in order to segment clients and to ensure they don’t steer down to the 2.4 GHz band. For example, name your 2.4 GHz network MyWiFi and name your 5 GHz network MyWiFi5G.\nAssociate your problematic device to the 5 GHz network. Now it won’t connect to the 2.4 GHz network! Once your devices are using the frequency band that you want, be sure to choose non-overlapping channels.\nReliable home WiFi is too important to just cross your fingers and hope for the best.Learn More\nMonitor small business networks, manage change, and troubleshoot as a teamLearn More\nDeploy, diagnose and troubleshoot enterprise networks with professional tools"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9d651953-dd79-4eb1-9de1-302ada799521>","<urn:uuid:ea9f01ec-4db9-4153-bef2-754838ab8159>"],"error":null}
{"question":"Do nematodes and earthworms both help with soil nutrient cycling?","answer":"Yes, both nematodes and earthworms contribute to soil nutrient cycling, but in different ways. Beneficial nematodes help turn organic matter into plant nutrients and also contribute nitrogen by digesting insect pests. Earthworms assist in decomposition by eating leaves and dead roots, grinding plant material in their gizzards, and excreting nutrient-dense casts that enrich the soil. They also stimulate beneficial bacteria and fungi that break down organic material into humus.","context":["Nematodes are microscopic soil-dwelling worms, many less than 1/16-inch long.\nThere are beneficial nematodes and pest nematodes.\nBeneficial nematodes help turn organic matter into plant nutrients. They also prey on soil-dwelling plant pests such as white grubs and root maggots.\nPest nematodes feed on plant roots, stunting and sometimes killing plants including many vegetables.\nNematodes are slender, translucent, unsegmented worms. Pest nematodes can be as small as 1/50-inch long. Beneficial nematodes that parasitize pest insects are larger, 1/25-inch long to several inches long.\nNematodes live in the film of water that coats soil particles; they thrive where the soil is rich, moist and warm. Nematodes can’t move more than about 3 feet on their own in the course of their lives, but they often travel around the garden in water, in shifted soil, in soil surrounding transplants, and on garden tools and even ants.\nPredatory nematodes either have teeth or long spear-like structures which they use to stab and suck the juices out of plants or their insect prey.\nNematode reproduction is usually sexual, though some individuals are capable of self-fertilization. Eggs are laid in a gelatinous mass. From hatching to full-formed adulthood, nematode development usually consists of four molts that take about a month; the life cycle of some pest species is only 3 or 4 weeks. There are many generations over the course of a year. When the soil grows cold, nematodes overwinter as eggs or adults. Where winters are not cold, nematodes are active year round.\nNematodes are found throughout North America.\nSome pest nematodes attack the roots of tomatoes, potatoes, peppers, lettuce, corn, and other vegetables. Other pest nematodes attack the stems and leaves of onions, rye, and alfalfa.\nPest nematodes include root-knot nematodes, lesion nematodes, ring nematodes, and sting nematodes.\nDamage inflicted by these pests includes root knots or galls, injured root tips, excessive root branching, leaf galls, lesions on drying tissue, and twisted, distorted leaves. This damage can result in stunted growth and sometimes plant death.\nFeeding Habits and Damage: Nematode damage often looks like a plant disease: leaves may turn yellow and become wilted and stunted. Roots of plants pulled from the ground often look lumpy and stunted.\nThe root knot nematode is perhaps the most destructive of the soil-dwelling pest nematodes in vegetable gardens. It moves in soil water to find the roots of susceptible crops. When it enters a root, it begins to feed by siphoning water and nutrients away from the plant. It secretes a substance that causes root cells to weaken and enlarge into firmly attached, knotty galls. After feeding the nematode lays a mass of eggs, new larvae hatch. The infested root grows large until it splits open and releases a new population of nematodes into the soil. The split root often becomes infected with fungi and bacteria and rots. The plant is left weakened, often near death.\nPest Nematode Controls: There are no organic methods to permanently eliminate pest nematodes but the population can be reduced. Here are some control strategies:\n- Remove and destroy infected plants along with their roots from the garden. Place plants and roots in a clear plastic bag and place it in the sun for a week to kill the pests. Then dispose of the bag in the trash, not in the compost pile.\n- Plant nematode resistant varieties: There are tomato and other crop varieties that have been developed to minimize nematode damage; damage will still occur but be less severe. Planting resistant varieties will indirectly reduce the pest nematode population over the course of a few years as there is less for the pests to feed on.\n- Plant crops that are not injured by pest nematodes. Where pest nematodes are present, plant crops that are not harmed by nematodes; this also will gradually reduce the number of pests in the soil. Crops that can survive in nematode infested soil include broccoli, Brussels sprouts, mustard, onions, leeks, garlic, rutabagas, and glob artichokes.\n- Confine pest nematodes to specific problem spots. Don’t allow nematodes to move around the garden. Nematodes cannot move more than about 3 feet on their own. Clean tools used in infested areas; wash the tools over the problem spot. Don’t carry pest nematodes around the garden.\n- Add organic compost to garden beds. Beneficial fungi and bacteria that attack pest nematodes thrive in organically rich soil.\n- Plant French marigolds (Tagetes patula) or African marigolds (Tagetes erecta) in the garden as a cover crop; grow only these plants for two months across vegetable planting beds. After two months, cut them down, let them dry in place, then turn them into the soil. Chemicals in these plants repels nematodes. You will likely have to repeat this treatment in two years.\n- Solarize the soil. Cover the soil with clear plastic during the hottest part of the summer to kill both pest nematodes; this will rid the soil of pest nematodes for a year or two. However, beneficial organisms are also harmed or destroyed by solarization of soil.\n- Add chitin to the soil. Chitin is a natural component of nematodes bodies. Fungi attack nematodes by breaking down the chitin in the body. Adding chitin to the soil will stimulate fungi to attack nematodes.\n- Add ground sesame stalks to planting beds. Sesame will suppress nematodes.\n- Leave the garden fallow for a year. The nematode population will decline if denied food for a year.\n- Till the soil in winter to expose nematodes to killing sunlight and dryness.\nNot all nematodes are pests; some are beneficial to soil and plants. These nematodes eat organic matter in the soil helping to decompose it and turn it into nutrients for plants. They also attack and kill harmful insect pest, ingest the remains, and turn it into nutrients—especially nitroten–plants can take up.\nBeneficial nematodes that are insect parasites are harmless to humans, wildlife, bees, and earthworms. They attack cutworms, root weevils, corn and stem borers, and squash vine borers and some pest root nematodes.\nThe beneficial nematodes attack soil-dwelling insects through their natural body openings. Once inside, these beneficial nematodes release a bacterium that paralyzes and kills the insect. The nematodes then feed on the tissue of the insect carcass and also eat the bacteria. They reproduce inside the carcass and then move on to a new host.\nThree beneficial nematodes are: Steinernema carpocapsae attacks cutworms, armyworms, corn rootworms, and fire ants; Steinernema fetiae attacks root knot nematodes, ring nematodes, and string nematodes; Heterorhabditis bacteriophora attacks cabbage root maggots, Colorado potato beetle larvae, white grubs, and root weevils.\nBeneficial nematodes can be purchased for use in the garden. Beneficial nematodes come packaged in a gel, in a power, or mixed with peat and vermiculite. They are commonly mixed with water before being applied with a watering can or sprayer to the soil or plants, or injected into plant stems by syringe. It is important to follow label instructions when applying nematodes to the soil or plants. Commonly applications are repeated every two to three weeks if needed.\nThe efficacy of nematode application can be affected by hot weather, cold soil, and heavy rain. Nematodes are most active in temperatures between 72°F and 82°F.","Earthworms & Soil Formation\nSoil formation is a continuous process, impacted by physical factors including wind, rain and temperature, as well as biological forces such as plants and animals. Earthworms significantly influence the formation of soil, helping to shape soil structure, content and fertility. With thousands of species worldwide, earthworms are found in most temperate and tropical zones around the globe. Although healthy soil can be formed without earthworms, the presence of earthworms is usually an indicator of productive soil. Through their seemingly tireless activity, earthworms benefit soil formation in a number of ways.\nEarthworms play an important role in mixing and aggregating soil. In their quest for food, earthworms continuously rise to the surface and tunnel down again, swallowing bits of soil and organic matter along the way. Earthworms excrete this mixture of minerals and plant materials into the soil in the form of nutrient-dense casts. In the process, they bring soil from the top layer to the lower strata and drag soil from below to the surface. This constant churning action can turn over a half foot of topsoil in approximately 10 to 20 years, according to experts at the Natural Resources Conservation Service.\nAs earthworms burrow through the dirt, they improve soil structure by loosening compacted soil and creating thousands of tunnels below the surface. These tunnels increase soil porosity, providing pathways for water and air to penetrate into the earth. As a result, soil is better able to absorb and retain water and less prone to erosion and water run-off. In addition, these tunnels provide channels into which plant roots can easily spread, adding stability as well as organic material to soil.\nEarthworms contribute to soil formation by assisting in the decomposition and incorporation of organic materials into the soil. Earthworms eat leaves and dead roots found on or near the soil’s surface. They mix this organic material into the soil by tearing off portions of plant material and burying it deep within the earth. As they eat, they grind bits of plant material in their gizzards and excrete the organic matter into the soil through their casts. Earthworm casts also pile up on the surface of the soil, adding to soil content. In addition, earthworms stimulate the activity of beneficial bacteria and fungi. These microorganisms feed on organic material, breaking it down into humus.\nSoil Food Web\nEarthworms are important members of the soil food web, a complex community of organisms that impact the process of soil formation. The soil food web consists of soil-dwelling organisms, ranging from bacteria, algae and fungi to insects, small mammals, reptiles and plants. These organisms affect soil formation by burrowing, breaking down plant and animal materials and eventually contributing their own organic matter to the soil at death. Earthworms are an important food source for many members of this community. In addition, earthworm tunnels and casts help provide the water, nutrition and oxygen needed to create an environment in which these organisms can thrive.\n- University of California Cooperative Extension: Earthworms and Soil Productivity\n- U. S. Department of Agriculture, Natural Resources Conservation Service: Soil Formation and Classification\n- U. S. Department of Agriculture, Natural Resources Conservation Service: Soil Biology: Earthworms\n- University of California, Oak Woodland Management: Earthworm Ecology in California\nBased in the Atlanta area, Charlene Williams has been writing and editing since 1988. She has over 15 years of experience working as a technical writer in the software industry. She has worked as a freelance writer for the past five years, and is a contributing writer for eHow and Answerbag. Williams holds a Bachelor of Arts in English from Kennesaw State University."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:16afa57a-6095-47ec-87b8-6f4737daed93>","<urn:uuid:8a0a257f-1b9a-42d2-bfbc-6ccd0b5b9aed>"],"error":null}
{"question":"What role do benchmarking and analysis play in both strategic planning approaches?","answer":"In strategic roadmaps, benchmarking involves comparing your company against others using tools like the ACC Legal Operations Maturity Model to identify improvement areas and track progress across 14 functional areas. This helps justify resource expenditure and supports business cases for internal projects. Similarly, in business planning, business analysis is used to assess the company's current state, make improvement recommendations, and identify potential risks and opportunities associated with the business strategy. Both approaches use these analytical tools to measure progress and inform strategic decision-making.","context":["The first two Legal Ops Brief columns answered: What is legal operations and who should be part of your ops team? Now it’s time to dive into the legal ops team’s guiding force: the legal strategic roadmap. This plan defines a goal or desired outcome and includes the major steps needed to reach it. It also serves as a communication tool — a high-level document that helps articulate the “why” behind both the goal and the plan for getting there.\nA strategic roadmap is a valuable tool for communicating vision and facilitating growth. You can have the best human and financial resources in the world but if you do not have a clear vision and plan for achieving goals, your efforts will falter. There are two major aspects of your roadmap that should be considered: strategic direction and format.\nAn easy first step for defining your strategic direction is to start with the mission and goals for your department and your company. Consider what the mission and goals mean and what efforts can be conducted to help achieve them. Our legal department vision focuses on collaboration, accountability, and agility. Each of the initiatives on our strategic roadmap align with one of our vision objectives.\nNext, benchmark your company against others to identify target areas for improvement. The ACC Legal Operations Maturity Model is a helpful reference tool. It was developed to define leading operational disciplines, which are rooted in business fundamentals, processes, data, and technology. These disciplines are designed to build consistency, drive efficiency, and demonstrate value in a legal function’s practice of law. Legal department leaders are encouraged to use it to benchmark maturity in any given area, bearing in mind that priorities and aspirational targets will vary based on department size, staffing, and budgets.\nAt my company, AEP, we use the Maturity Model to identify improvement areas and benchmark our progress. I highlight where we are in each of the 14 function areas and review with legal management. It shows how we compare, what we are doing well, and where we could improve. The model helped highlight that we were behind in managing matter budgets with our firms. We already had an e-billing tool in place but had not implemented an enforceable budgeting process. We decided to move forward with process and technology changes — we now have budget forecast abilities and historic benchmarks. In these times of “doing more with less,” having an industry benchmark provides decisionmakers with the confidence they need to justify the resource expenditure.\nKnowing how to leverage this Maturity Model to support business cases for internal projects can be very powerful. Vanessa Lozzi, senior legal technology and litigation support administrator, and assistant vice president at Flagstar Bank, has used the model to support changes. “When you are looking at the Maturity Model, it can be intimidating,” she admits. “Don’t take on too much at once. Pick an area where you can get a quick benefit. After you prove your success, go back to the model and look for the next area of focus.”\nLozzi used the Maturity Model to identify areas of improvement and present those gaps to her general counsel. “I used the Maturity Model to gauge what stage the department was in for the respective categories. Once completed, I compiled my observations in a short memo to the general counsel,” she explains. The results showed that there was room for improvement in the financial management area. The focus was on establishing a process in the matter management platform for entering accruals and matter budgets. Moving the department from intermediate to advanced was not overly burdensome or time consuming. Having the processes in place provided great opportunities to report on metrics in the future — a small lift for a big gain.\nThe second areas that were identified were information governance and knowledge management. The solution was simple — purchase a document management system. However, this required a significant investment in both cost and time. The result from this project helped the department in several ways, including increased efficiency and risk reduction.\nAnother benefit of using the Maturity Model is it provides its own roadmap of improvement in each of the 14 functions. If you are in the early stage of a section the model has clear steps mapped out that help move companies from one stage to the next. Over half of the 316 legal departments that participated in a recent ACC benchmarking survey rated their strategic planning function as being “early stage” and 31 percent rated themselves as “intermediate” (In addition to the Maturity Model, ACC has partnered with leading legal service providers to produce the Foundational Toolkit to advance from early to intermediate stages in each of 14 law department management functions, presented in a series of monthly webcasts). It may take several projects and years to move from one end of the model to the next, but that is all part of your department’s strategic roadmap.\nNow that you know what your strategic goals are, you need to decide on the actual roadmap format. It can range from a single page of information to a PowerPoint presentation with data and graphics. Most strategic roadmaps cover three to five years and should be refreshed annually. Implementing this would already put you ahead of the curve with only 27 percent of legal departments having an annual legal operations planning process in place. Remember, transformational change takes time. Some complex goals may span years and include different phases to complete. The roadmap will help visually demonstrate that multi-year vision. Organizational culture and audience should determine the roadmap’s presentation format. Some audiences only want a high-level visual and some will want supporting information for each item. You need to know your audience and tailor your roadmap accordingly. Check out roadmap templates online, such as Roadmunk or ProductPlan, or reach out to peers to see what has worked well for them. After you have looked at some examples and have an idea of what you want, you can also create your own in a tool like PowerPoint.\nAlso, leave “technology” out of the title and name of your legal roadmap. When you share this document with someone, you don’t want them to think it is only focused on technology because of the title and document name. Legal Ops is naturally associated with technology, but you don’t want your audience to think that is all your roadmap offers. Your roadmap is about what you are trying to improve or change, and technology is naturally going to be a part of solution.\nProcess and culture change are critical to any department’s success and can be reflected independently on a roadmap with goals such as increased user adoption rates for application use or improve diversity representation from firms and improved customer survey results. Each of these objectives will most likely be supported by technology but the actual goal is not technical.","A business plan is a roadmap that helps you create and manage your business plan company. It can help you identify the key goals you want to achieve, outline how you will achieve them, and monitor progress. It’s an essential document for any entrepreneur, regardless of their industry. In this article, we’ll look at what goes into creating a, why it’s vital, and some tips for ensuring that yours is successful.\nWhat is a business plan?\nA is a document that outlines a company’s strategy and objectives. It is typically created by a business owner or CEO before starting a new venture and can be used to track progress, measure success, and make changes as needed. A good business plan will also include the following:\n- Revenue forecasts, costs, and expenses.\n- A list of key employees.\n- A description of the company’s mission and vision.\nA business plan can take many forms, but all aim to help a company achieve its goals.\nWhat are the different types of business plans?\nA is a document outlining a company’s strategy for achieving its goals. There are several different types of business plans, each with its benefits.\nFixed-period or indefinite-period business plans outline a specific time (typically one year) for achieving a goal, such as increasing revenue by particular amounts. An indefinite-period schedule is more flexible, allowing for company strategy changes or plans as circumstances change.\nResource allocation plans: Resource allocation plans outline the resources a company will need to achieve its goals and how those resources will be divided among different departments. For example, a company might need more salespeople but not enough engineers to create new products.\nMarketing plans: A marketing plan outlines how the company will reach its target market and what types of marketing strategies it will use.\nBusiness model planning: A business model defines the company’s unique selling proposition (USP) and how it will generate revenue.\nBusiness analysis: Business analysis helps assess the company’s current state and makes recommendations on how to improve it. It can also help identify potential risks and opportunities associated with the company’s business strategy.\nOne of the most important aspects of any is having a plan.\nA outlines your business goals, objectives, and strategies while also providing information on how you plan to achieve them. It can help measure progress, keep tabs on costs, and demonstrate that you’re organized and have a clear idea of what you’re doing. Additionally, a good can attract investors or support from other businesses, leading to increased revenue and growth potential.\nHow to develop a business plan\nBusiness plans are essential for any business, but especially for small businesses. A business plan outlines your goals, strategic plan, financial forecast, marketing strategy, and more. Developing a business plan can be a daunting task, but with the help of a professional planner, it can be an essential step in starting or growing your business. Here’s what you need to know to create a successful business plan.\n1. define your business goals. What do you hope to achieve with your company? This is the foundation of your business plan. Without clear goals, knowing where to start planning isn’t easy.\n2. outline your company’s strategic plan. What are the critical areas of your business that you want to focus on? How will you achieve your goals in these areas?\n3. develop a financial forecast. What are the costs associated with reaching those goals?\n4. estimate the marketing and advertising needs for your business. How will you market and sell your products or services?\n5. describe how you will protect yourself from risks and uncertainties\nA business plan is an important document that sets out your business goals and how you plan to achieve them. It can be a valuable tool for investors, customers, and even competitors, so make sure you take the time to create one that is realistic and reflects the needs of your particular business. By following these tips, you should develop a well-crafted business plan that will guide your efforts in the right direction."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:25939c51-e5b8-457b-934d-b9bfd44b8866>","<urn:uuid:517a56dd-491e-4683-a180-e4d7e79e8bf2>"],"error":null}
{"question":"What are the main transportation routes in Skaneateles and Adams?","answer":"In Skaneateles, Route 20 serves as the main east-west state highway and functions as the main street within the Village. Meanwhile, Adams is served by Interstate 81 as its major north-south highway through the middle of town, along with U.S. Route 11 running north-south, and two state routes: NY-177 running eastward and NY-178 running westward from Adams village.","context":["Dealing with rural sprawl in an upstate New York community\nA team of urban design graduate students from Notre Dame created a plan for Skaneateles, New York, the classic lakefront town in the Finger Lakes.\nLocated on the edge of what is called the Syracuse metropolitan area, Skaneateles (pronounced SKAN-ee-AT-lus) is a Village surrounded mostly by countryside. Thanks to a charming main street that skirts the northern end of Skaneateles Lake, the Village is one of the more scenic Upstate communities.\nPolitically, Skaneateles is a Village of 1.7 square miles surrounded by a 48.8 square mile Town of the same name. In New York State, a Town is a rural or suburban area, while small urban places are called villages. The combined population of the Village and Town is about 10,000. The Town and Village are updating their joint Comprehensive Plan with the help of the Notre Dame team, which conducted a public charrette in September.\nLike thousands of towns across the US, Skaneateles's urban fabric has been eroding bits at a time with “rural sprawl.” The charrette's challenge was to find a way to repair existing sprawl and channel future growth into infill and more compact hamlets.\nDuring the charrette, the Notre Dame students produced a Transect map of the Village and Town (Image 1) and a series of figure-ground drawings (Images 2-4) that dramatically illustrate the problem.\nThe colors of Image 1 represent the Transect from T- 1 (light green natural areas) through T-5 (the dark purple village main street). Civic sites are shown in red, urban parks are dark green, and special uses — e.g. industrial areas — are dark gray. The lake is blue.\nSprawl doesn’t fit any of these categories of the traditional Transect, and therefore it shows up as white splotches. The image dramatically illustrates that even around a small town, sprawl has spread extensively. Even including the T-3 part of the Transect (low-density sub-urban development that is still walkable), there appears to be more sprawl than urban fabric in and around Skaneateles. That's not unusual.\nImages 2-4 illustrate how zoning is contributing to rural sprawl. Image 2 depicts an idealized farm landscape in a part of the Town of Skaneateles. Image 3 shows the reality of existing sprawl development in that location. Image 4 illustrates what the area would look like if the current low-density zoning is built out.\nRetrofitting an auto-oriented street\nSprawl is mostly confined to the Town — but not entirely. Fennell Street in the Village is lined with auto-oriented uses and street-fronting parking lots. The development serves a purpose — Fennell Street has a supermarket and drug and hardware stores that are vital to residents. A goal of the plan is to allow this thoroughfare to redevelop in a more pedestrian-friendly manner while maintaining the retail that is vital to residents.\nImages 5 and 6 consist of a plan and rendering of Fennell Street as it is now. Images 7, 8, and 9 are a proposed plan, rendering, and street section for Fennell. The plan calls for new buildings placed much closer to the street to provide a consistent street edge that defines the street spatially. Liner buildings hide the large grocery store parking lot. A new square is envisioned to be fronted by a post office, a new civic building (a village office), and bank.\nThe curb-to-curb dimensions of the street don’t change, but the sidewalks and zero-lot-line storefronts will make it more appealing to pedestrians, says Philip Bess, Director of Graduate Studies at the Notre Dame School of Architecture. “The 12-foot travel lane is a little wide, but appropriate to the type of traffic on Fennell,” he explains.\nThe students also carefully considered how to improve “gateway” areas where travelers enter the Village on Route 20, an east-west state highway, which serves as the main street inside the Village. Flanking the Village on both sides are strings of typical strip commercial buildings that have developed over the course of 50 or 60 years.\nOne of my favorite parts of the plan is the proposal to convert sections of Route 20 on both sides of the Village from a highway into a boulevard (Image 10). The strip commercial businesses are reconfigured from scattered sprawl to urban blocks that accommodate pedestrians, automobiles, and a mixture of uses (Image 11).\nSuch targeted highway retrofits are the kind of projects that state and federal transportation departments should be funding to promote smart growth and reduce vehicle miles traveled. Both the federal government (see link) and New York State appear to be getting on board with this approach (see link).\nThe students suggest modest changes to typical Route 20 buildings — such as pitched roofs and parking in the rear — that could make a big difference in how they relate to the proposed boulevard. Since this area is not connected to the Village sewer system, the students propose that small wetlands serve as a natural biofilter. “This would still be an area for inexpensive, informal buildings — some auto-oriented,” says Bess.\nOne more aspect of the plan that I highlight here is the hamlets. Not all new development is likely to take place on infill sites, so the students looked at how to accommodate growth disconnected from the Village. The team focused on the creation of a few hamlets, mostly in areas close to current employment opportunities. Image 12 shows the plan for one such hamlet — Skaneateles Falls.\nThe work shown here is preliminary, and the students will modify their ideas over the course of the semester and in response to public comments. Some of the work is likely to be included in the new Comprehensive Plan. The charrette cost Skaneateles $10,000, I was told, \"a little less than Notre Dame's expenses,\" says Bess, \"but times are tight and the Town and Village are on a budget too.\" That's a very good deal for the quality of work that was produced, in my view.\nMany of the ideas from the Skaneateles charrette are applicable to other villages that are dealing with similar problems of rural sprawl.","Adams, New York\n|Elevation||619 ft (188.7 m)|\n|Area||42.5 sq mi (110.1 km2)|\n|- land||42.4 sq mi (110 km2)|\n|- water||0.04 sq mi (0 km2), 0.09%|\n|Density||121 / sq mi (46.7183612 / km2)|\n|Town Supervisor||David W. Kellogg (R)|\n|- summer (DST)||EDT (UTC-4)|\n|ZIP code||13605, 13606|\nSettlement began around 1800 at Adams village. David Smith built a sawmill at the present site of Adams in 1801. Renamed for John Adams in 1802 (the year after his presidency ended), the Town of Adams was created from the survey townships of Aleppo and Orpheus. The eastern part of Adams was taken in 1804 to form the Town of Rodman. During the War of 1812, the town formed a local militia for home defense. In more recent times, the town has served as the home of the 46-unit chain restaurant Jreck Subs.\nResidents of note\n- William Eugene Blackstone, evangelical Christian and Zionist.\n- Henry Keep, (1818–1869), born in Adams, noted financier and president of several railroad firms, he left an estate of over four million dollars.\n- Nicholas Doxtater Yost, attorney and president of the Adams Farmer’s National Bank; relative of Henry Keep and father of United Nations Ambassador Charles W. Yost.\n- J. Sterling Morton, father of Arbor Day, was born in Adams village.\n- Henry Benjamin Whipple, 1st Episcopal bishop of Minnesota\n- Charles Grandison Finney, prominent evangelist\n- Philander Smith, son of mill operator and original settler David Smith. Eponym of Philander Smith College and father-in-law of William Eugene Blackstone (above)\n- Rob Krone, WWNY TV Channel 7 Sports Anchor\nAccording to the United States Census Bureau, the town has a total area of 42.4 square miles (109.9 km²), of which, 42.4 square miles (109.8 km²) of it is land and 0.04 square miles (0.1 km²) of it (0.05%) is water.\nInterstate 81 is a major north-south highway through the middle of Adams. New York State Route 177 runs eastward from U.S. Route 11, another north-south highway, at Adams Center. New York State Route 178 runs westward from Adams village.\nThe town is located in an area called the Tug Hill Plateau. Sandy Creek flows westward through the south part of the town, and Stony Creek flows through the northwest corner.\nAs of the census of 2000, there were 4,782 people, 1,864 households, and 1,293 families residing in the town. The population density was 112.8 people per square mile (43.5/km²). There were 2,019 housing units at an average density of 47.6 per square mile (18.4/km²). The racial makeup of the town was 97.97% White, 0.29% Black or African American, 0.27% Native American, 0.36% Asian, 0.02% Pacific Islander, 0.21% from other races, and 0.88% from two or more races. Hispanic or Latino of any race were 0.65% of the population.\nThere were 1,864 households, out of which 35.4% had children under the age of 18 living with them, 56.2% were married couples living together, 9.5% had a female householder with no husband present, and 30.6% were non-families. 24.5% of all households were made up of individuals and 11.6% had someone living alone who was 65 years of age or older. The average household size was 2.56 and the average family size was 3.05.\nIn the town the population was spread out with 27.5% under the age of 18, 6.9% from 18 to 24, 30.0% from 25 to 44, 23.2% from 45 to 64, and 12.3% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 94.9 males. For every 100 females age 18 and over, there were 89.0 males.\nThe median income for a household in the town was $38,012, and the median income for a family was $48,354. Males had a median income of $34,000 versus $25,610 for females. The per capita income for the town was $17,707. About 6.6% of families and 9.6% of the population were below the poverty line, including 12.7% of those under age 18 and 7.0% of those age 65 or over.\nCommunities and locations in the Town of Adams\n- Adams – The Village of Adams, located near the south town line on US-11 at NY-178.\n- Adams Center – A hamlet is located north of Adams village on U.S. Route 11 at NY-177.\n- Butterville – A location at the west town line on County Road 76.\n- Coopers Corners – A former location in the town.\n- Green Settlement – A hamlet on County Road 76 west of Adams Center.\n- Honeyville – A location on County Road 177 east of Adams Center.\n- Lisk Settlement – A former location in the town.\n- Lyon Corners – A location near the west town line on County Road 76.\n- North Adams – A hamlet near the northern town line by North Kellog and Cady Roads.\n- Sanford Corners – A location on County Road 66 in the northwest corner of the town.\n- Talcott Corners – A location on US-11 in the northeast corner of the town.\n- Thomas Settlement – A hamlet on NY-178 in the southwest corner of the town.\n- Smithville – A hamlet on County Road 75 on the western town line.\n- Staff (2010-07-09). \"National Register Information System\". National Register of Historic Places. National Park Service.\n- Who Was Who in America, Historical Volume, 1607-1896. Chicago: Marquis Who's Who. 1963.\n- \"Annual Estimates of the Resident Population for Incorporated Places: April 1, 2010 to July 1, 2014\". Retrieved June 4, 2015.\n- \"Census of Population and Housing\". Census.gov. Retrieved June 4, 2015.\n- \"American FactFinder\". United States Census Bureau. Retrieved 2008-01-31.\n- Early Adams history\n- Historical/genealogical links\n- Historical summary for Adams\n- Towns of the Tug Hill Plateau"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a1d6bebb-aafe-4ece-9f0b-f6a7c5100892>","<urn:uuid:fa48bf1e-797b-4f78-9b49-d1c25d6de21e>"],"error":null}
{"question":"What innovative educational and audience development programs are offered by the Cleveland Museum of Art's Contemporary Music Ensemble versus the Cleveland Orchestra?","answer":"The Cleveland Orchestra has implemented extensive educational initiatives, including the Center for Future Audiences established in 2010 with the Maltz Family Foundation, featuring the successful 'Under 18s Free' program that has led to 20% of attendees being under 25. They also offer the Make Music! initiative and various student programs. The Contemporary Music Ensemble at the Cleveland Museum of Art takes a different approach, focusing on open rehearsals and collaborative experiences with professional musicians, as demonstrated in their work with Henry Threadgill where students engaged in improvisation and exploratory music-making sessions.","context":["Contemporary Music Ensemble Collaborates with Jazz Master Henry Threadgill\nWorld-premiere performance launches Cleveland Museum of Art’s new composers series Jan. 11; open rehearsal at Oberlin set for Jan. 10.\nWhen the Cleveland Museum of Art got the green light to proceed with a series of new music commissions, the first composer it sought was Pulitzer Prize-winning jazzman Henry Threadgill.\nThe first collaborative partner it sought was Oberlin Conservatory.\nOn January 11, the first installment of the Creative Fusion Composers Series takes the stage of the museum’s Gartner Auditorium. It will serve as the world premiere of Threadgill’s Pathways, performed with his New York-based ensemble Zooid and the Oberlin Contemporary Music Ensemble.\nThreadgill and Zooid spent a week in Oberlin working in daily rehearsals with CME and conductor Timothy Weiss. Their last on-campus session together is an open rehearsal in Clonick Hall that takes place at 7:30 p.m. Thursday, January 10. It is free and open to the public, with seating available for up to 40 guests.\nThe concept for Threadgill’s Oberlin collaboration was the brainchild of Tom Welsh, director of performing arts at the Cleveland Museum of Art. Though the museum has programmed musical performances for at least 100 years, it had not previously commissioned new works—a step Welsh considers integral to its status as a major contributor to the performing arts. That has changed with the first composer-centric Creative Fusion, the Cleveland Foundation’s 11-year-old initiative to bolster connections between artists and the community. In the coming months, six composers will collaborate with various community partners across northeast Ohio. Threadgill—and CME—are the first.\nWelsh brought Threadgill to Cleveland in May 2018, to get to know the city and ultimately to mine inspiration for his commissioned work. During that visit, they also dropped in on a rehearsal with CME, which Welsh describes as “the preeminent new music ensemble in America.” Not coincidentally, conductor Weiss was a driving force behind the Grammy Award-winning, Oberlin-born International Contemporary Ensemble and Eighth Blackbird.\nTo Welsh, Threadgill’s Zooid and Weiss’ CME were a natural marriage.\n“They are kind of like chamber music, but kind of not,” he says of Zooid, whose unconventional instrumentation—a hallmark of Threadgill’s music—includes a tuba. “They’re always improvising and flexing and changing right before your very eyes. To my mind, the only organization to do this sort of collaboration anywhere is CME. My secret hope was for this to be a match made in heaven, and I think that’s turned out to be true.”\nThreadgill, who won the Pulitzer Prize for Music in 2016 for his album In for a Penny, In for a Pound, has been called “perhaps the most important jazz composer of his generation” by The New York Times. Such accolades notwithstanding, he is something of a musical chameleon whose complex yet accessible creations defy easy categorization.\nFor the classical musicians of CME, Threadgill’s emphasis on improvisation presented new challenges during their week together, which consisted of rehearsals lasting up to six hours daily.\n“I use the German concept of ‘rehearse,’” Threadgill told Cleveland public radio station WCPN 90.3 FM in an interview leading up to the performance. “What we do in the United States is we have a rehearsal, and people play their music from left to right. If they did it right, they get up and they go home. The German word for ‘rehearse’ is explore, and that’s what we do when we rehearse.\n“The students do what they want to do,” Threadgill continues. “I offer them an entry into the music that is based on improvisation. People look at the music and bring their own interpretation to it.”\nThe music in this case is Pathways, a reference to Lake Erie, which creates most of Ohio’s northern border. Threadgill, who frequently finds inspiration in nature (his last album was called Dirt…and More Dirt), was struck by man’s reliance on Lake Erie for transportation and the resurgence it has enjoyed in recent years.\nMusically, Pathways was conceived as a sort of overture, the first piece in a two-part series; next up is a companion work called Passages.\n“Henry said There’s gonna be improvising…are you OK with that?” recalls Lauren Anker, a fifth-year double-degree student majoring in horn performance and history. “And I’m like…suuuurre. So he did warn us about that!”\nAnker and her fellow CME musicians admit that Threadgill’s jazz-informed approach can be disorienting at first, but that the members of Zooid immediately recognized that a little translation would be in order.\n“Sometimes it feels like Henry is speaking a different language,” she says. “He has a really interesting interpretation of the music that’s definitely much more free than what I am accustomed to. In the improv sections, he really wants us to make music, and he wants us to converse with one another. It’s a kind of communication and music making that you really don’t see people thinking about, even in the contemporary world. Everybody has input, and everybody has a say in the music making, which is great.”","As it nears the centennial of its founding in 2018, The Cleveland Orchestra is undergoing a new transformation and renaissance. Under the leadership of Franz Welser-Möst, entering his fourteenth year as the ensemble’s music director with the upcoming 2015-16 season, The Cleveland Orchestra is acknowledged among the world’s handful of best orchestras. With Welser-Möst, the ensemble’s musicians, board of directors, staff, volunteers, and hometown are working together on a set of enhanced goals for the 21st century — to continue the Orchestra’s legendary command of musical excellence, to renew its focus on fully serving the communities where it performs through concerts, engagement, and music education, to develop the youngest audience of any orchestra, to build on its tradition of community support and financial strength, and to move forward into the Orchestra’s next century with an unshakeable commitment to innovation and a fearless pursuit of success.\nThe Cleveland Orchestra divides its time each year across concert seasons at home in Cleveland’s Severance Hall and each summer at Blossom Music Center. Additional portions of the year are devoted to touring and to a series of innovative and intensive performance residencies. These include an annual set of concerts and education programs and partnerships in Florida, a recurring residency at Vienna’s Musikverein, and regular appearances at Switzerland’s Lucerne Festival, at New York’s Lincoln Center Festival, and at Indiana University.\nMusical Excellence. The Cleveland Orchestra has long been committed to the pursuit of musical excellence in everything that it does. The Orchestra’s ongoing collaboration with Welser-Möst is widely-acknowledged among the best orchestra-conductor partnerships of today. Performances of standard repertoire and new works are unrivalled at home, in residencies around the globe, on tour across North America and Europe, and through recordings, telecasts, and radio and internet broadcasts. Its longstanding championship of new composers and commissioning of new works helps audiences experience music as a living language that grows and evolves with each new generation. Recent performances with Baroque specialists, recording projects of varying repertoire and in different locations, fruitful re-examinations and juxtapositions of the standard repertoire, and acclaimed collaborations in 20th and 21st century masterworks together enable The Cleveland Orchestra the ability to give musical performances second to none in the world.\nServing the Community. Programs for students and community engagement activities have long been part of the Orchestra’s commitment to serving Cleveland and surrounding communities, and have more recently been extended to its touring and residencies. All are designed to connect people to music in the concert hall, in classrooms, and in everyday lives. Recent seasons have seen the launch of a unique “At Home” neighborhood residency program, designed to bring the Orchestra and citizens together in new ways. Additionally, a new Make Music! initiative is being developed, championed by Franz Welser-Möst in advocacy for the benefits of direct participation in making music for people of all ages.\nFuture Audiences. Standing on the shoulders of more than nine decades of presenting quality music education programs, the Orchestra made national and international headlines through the creation of its Center for Future Audiences in 2010. Established with a significant endowment gift from the Maltz Family Foundation, the Center is designed to provide ongoing funding for the Orchestra’s continuing work to develop interest in classical music among young people. The flagship “Under 18s Free” program has seen unparalleled success in increasing attendance and interest — with 20% of attendees now comprised of concertgoers aged 25 and under.\nInnovative Programming. The Cleveland Orchestra was among the first American orchestras heard on a regular series of radio broadcasts, and its Severance Hall home was one of the first concert halls in the world built with recording and broadcasting capabilities. Today, Cleveland Orchestra concerts are presented in a variety of formats for a variety of audiences — including popular Friday night concerts (mixing onstage symphonic works with post-concert entertainment), film scores performed live by the Orchestra, collaborations with pop and jazz singers, ballet and opera presentations, and standard repertoire juxtaposed in meaningful contexts with new and older works. Franz Welser-Möst’s creative vision has given the Orchestra an unequaled opportunity to explore music as a universal language of communication and understanding.\nAn Enduring Tradition of Community Support. The Cleveland Orchestra was born in Cleveland, created by a group of visionary citizens who believed in the power of music and aspired to having the best performances of great orchestral music possible anywhere. Generations of Clevelanders have supported this vision and enjoyed the Orchestra’s concerts. Hundreds of thousands have learned to love music through its education programs and celebrated important events with its music. While strong ticket sales cover just under half of each season’s costs, it is the generosity of thousands each year that drives the Orchestra forward and sustains its extraordinary tradition of excellence onstage, in the classroom, and for the community.\nEvolving Greatness. The Cleveland Orchestra was founded in 1918. Over the ensuing decades, the Orchestra quickly grew from a fine regional organization to being one of the most admired symphony orchestras in the world. Seven music directors have guided and shaped the ensemble’s growth and sound: Nikolai Sokoloff, 1918-33; Artur Rodzinski, 1933-43; Erich Leinsdorf, 1943-46; George Szell, 1946-70; Lorin Maazel, 1972-82; Christoph von Dohnányi, 1984-2002; and Franz Welser-Möst, since 2002.\nThe opening in 1931 of Severance Hall as the Orchestra’s permanent home, with later acoustic refinements and remodeling of the hall under Szell’s guidance, brought a special pride to the ensemble and its hometown, as well as providing an enviable and intimate acoustic environment in which to develop and refine the Orchestra’s artistry. Touring performances throughout the United States and, beginning in 1957, to Europe and across the globe have confirmed Cleveland’s place among the world’s top orchestras. Year-round performances became a reality in 1968 with the opening of Blossom Music Center, one of the most beautiful and acoustically admired outdoor concert facilities in the United States.\nToday, concert performances, community presentations, touring residencies, broadcasts, and recordings provide access to the Orchestra’s acclaimed artistry to an enthusiastic, generous, and broad constituency around the world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2a7ee57a-7fbb-4cfc-b2fa-76a86f4820d9>","<urn:uuid:775aead8-8eb7-437f-9f47-043d9d650131>"],"error":null}
{"question":"How did Wilson and Roosevelt differ in their approach to establishing national protection systems - the Federal Reserve versus National Parks?","answer":"Wilson and Roosevelt took different approaches to establishing national protection systems. Wilson established the Federal Reserve System as a financial protection mechanism during his presidency. In contrast, Roosevelt focused on environmental protection by signing the 1906 American Antiquities Act, which allowed presidents to protect important cultural and natural resources. During his presidency, Roosevelt established five national parks and 18 national monuments, protecting over 93 million hectares of public land. Wilson later built upon Roosevelt's environmental legacy by establishing the National Park Service in 1916 to 'protect the wild and wonderful landscapes' in the United States.","context":["During Woodrow Wilson's presidency...\n- Women received the right to vote with the passing of the 19th Ammendment.\n- The Federal Reserve System was established.\n- Income tax was initiated with the birth of the Internal Revenue System.\n- World War I broke out in Europe between 1914 and 1918.\n- Sheep grazed on the White House lawn to help the Red Cross raise wool for the war effort.\n- The national observance of Mother's Day was established.\n- Image is on the $100,000 bill although it is no longer in circulation\nWoodrow Wilson was the first president to...\n- Personally deliver what is known today as the \"State of the Union Address\".\n- Hold regular news conferences.\n- Cross the Atlantic Ocean while in office.\n- Was the first president to attend the Major League Baseball Fall Classic. He saw the debut of a young 20 year old pitcher by the name of George Herman \"Babe\" Ruth.\nWoodrow Wilson and the Great War\nWhen Europe was thrust into war in 1914, Woodrow Wilson, who believed in neutrality, saw America's role as that of peace broker. \"The Great War\", as contemporaries called it, was without precedent, involving many countries in a vast and gruesome battlefield. The resumption of unrestricted submarine warfare by Germany and the news of the Zimmermann telegram were part of a long process that persuaded Wilson to ask for a declaration of war. Rarely does one event precipitate a decisive reaction from a thoughtful and deliberate individual such as Wilson. The Russian Revolution and the French Army mutiny convinced the United States that Russia and France would pull out of the war leaving the way open for a German victory. This was unacceptable to the Wilson administration. These two events, combined with the numerous sinking of ships, the increasingly belligerent communications from the Germans, the resumption of unrestricted submarine warfare, and the Zimmermann telegram, drove Wilson’s decision. In April 1917, Wilson asked Congress to declare war, only the second declaration of war in U.S. history. President Wilson was best remembered for his leadership during World War I and his vigorous attempt to establish the League of Nations. The war came to an end on November 11, 1918. At the Paris Peace Conference, Wilson proposed \"Fourteen Points\" as the basis for the peace treaty. The final Treaty of Versailles included many of Wilson's ideas. Unfortunately, the U.S. Congress did not support the Treaty. Consequently, the United States never joined the League of Nations. In 1920, Wilson was awarded the Nobel Peace Prize for his efforts on behalf of the League of Nations.\nVisit the links listed below for an insightful journey into the history of The Great War.\nWorld War I Links\nWorld War I Document Archives\nAddress to Senate - Appealing to Europeans for Peace\nWilson's Declaration of War Message to Congress\nWilson's Fourteen Points\nWilson's Address in Favour of the League of Nations\nA Call to Service\nWilson asks soldiers to undertake a great duty\nOn May 4, 2007 the 116th Infantry Brigade Combat Team, historically known as the Stonewall Brigade, mustered and shipped out from the Thomas D. Howie Memorial Armory in Staunton, Virginia for service in Iraq. This unit traces its lineage back to 1742 but became the 116th Infantry Regiment when it was called upon to join the fight on the Western Front in World War I.\nTwo days after the soldiers of the Stonewall Brigade were drafted into Federal Service and only weeks before they would become part of the 29th \"Blue and Gray\" Division that saw battle in the trenches of France, President Wilson sent this message to the soldiers he had charged \"to show all. . . not only what good soldiers you are, but also what good men you are. . . and set a standard so high that it will be a glory to live up to it.\"\nWilson and Censorship of the Press: President Wilson hopes the press will observe a \"patriotic reticence\"\nWilson's position on censorship following the United States' entrance into the Great War was a complicated one. He could \"imagine no greater disservice to the country than to establish a system of censorship that would deny to the people . . . their indisputable right to criticize their own public officials,\" but he also felt strongly that his administration should have the power to censor information in the interest of national security. The letter to Representative Webb represents Wilson's argument for the inclusion of a censorship provision in the Espionage Act that would prohibit the dissemination of information deemed \"to be useful to the enemy\" in times of national emergency. An amendment to that effect passed in the House but did not make it into the Espionage Act voted into law on June 15, 1917.\nIn May of 1918, Wilson was successful in getting the Sedition Act passed as an amendment to the Espionage Act. It became a crime to \"utter, print, write, or publish any disloyal, profane...or abusive language\" about the United States government or to disagree with its actions abroad. The act was repealed in 1921.\nA President's Illness Kept Under Wraps: Woodrow Wilson's Deteriorating Health Detailed in Doctor's Correspondence\nBy Michael Alison Chandler\nWashington Post Staff Writer\nWashington Post Photo\nSaturday, February 3, 2007\nSTAUNTON, Va. While U.S. troops were fighting in World War I in the summer of 1918, President Woodrow Wilson underwent treatment for a breathing problem in a hushed episode that foreshadowed worse health troubles to come.\nThe White House doctor, Cary T. Grayson, later recounted the incident to his wife in one of a slew of newly public documents that show how far Wilson's innermost circle went to conceal his frail condition amid major world events.\n\"The patient is progressing most satisfactorily, so far, and I have good reasons to hope for a most beneficial result. It has been a big undertaking. . . . No one knows anything about it except Miss E., Miss Harkins, Hoover -- It is one secret that has been kept quiet, so far, and I think it is safe all right now,\" the doctor wrote Alice Grayson in a July 16 telegram.\nThe episode, which caused Grayson great anxiety, most likely involved an operation to remove polyps from the president's nose, according to Michael Dickens, a Charlottesville physician familiar with Wilson's medical history and the telegram. Historians say the telegram indicates that the procedure was then known only to Grayson; the president's wife, Edith Wilson; a nurse; and a White House usher.\nGrayson's account was revealed to the public recently through a cache of personal files his family donated to the Woodrow Wilson Presidential Library here in the Shenandoah Valley. The letters, photographs and other documents had been stored for decades at a family home in Fauquier County.\nThe papers offer new insight into the 28th president's fragile health and how those around him tried to keep it quiet. Although historians have known for years about Wilson's debilitating stroke in late 1919 and his poor health beforehand, including previous possible strokes, the estimated 10,000 documents in the Grayson collection offer fresh and intimate details about the president's mental and physical condition from a man whom several Wilson biographers call an extraordinary eyewitness: his personal physician.\n\"He literally had his finger on the pulse of the president,\" said historian A. Scott Berg, who has been studying the Grayson papers for a forthcoming Wilson biography. \"Dr. Grayson enjoyed a unique position in Woodrow Wilson's life as both his personal physician and a personal friend.\"\nThe Grayson documents are the first major donation to the eight - year-old Wilson library, which unveiled them in late December on C-SPAN. The documents are drawing historians to this city two hours southwest of the Washington area where Wilson was born to a Presbyterian minister and his wife 150 years ago. Researchers say about 15 percent of the documents have been explored so far.\nGrayson, born in Culpeper County in 1878 to a well-connected Virginia family, was a naval surgeon who rose to the rank of rear admiral.\nAt the family home in Upperville, Cary T. Grayson Jr. is surrounded by black-and-white photos of his father in an impeccable Navy uniform posing with his \"number one patient,\" as the elder Grayson sometimes called the president. The younger Grayson, now 87, lives there with his wife, Priscilla. He said the papers reveal that the doctor-patient relationship was about much more than health.\nIn the doctor's weekly planners, now at the library, Grayson chronicled frequent golf games, automobile rides and nights of theater with the president. From inauguration day in 1913, when they met, to the Sunday morning in February 1924 when Wilson died, Grayson was an almost constant companion. He stayed often at the White House, traveled with Wilson to the Paris Peace Conference in 1919 and was one of a few people who saw him regularly after his stroke that October.\nThe stroke was a turning point for Wilson's presidency and, many argue, the world. Wilson collapsed Oct. 2 in the White House after a national tour seeking support for the Treaty of Versailles and America's entrance into the League of Nations. He went into seclusion for the remainder of his presidency. The treaty he had so strongly championed was rejected by the Senate in March 1920.\n\"This is the worst instance of presidential disability we've ever had,\" said John Milton Cooper, a Wilson scholar at the University of Wisconsin. \"We stumbled along . . . without a fully functioning president\" for a year and a half, he said.\nThe public was largely left in the dark about Wilson's condition. The official White House line was that the president was suffering from \"nervous exhaustion.\" Other presidents have also concealed health problems, historians say, but the secrecy that enveloped Wilson's illness seems difficult to imagine today.\nGrayson died in 1938. Years ago, his family gave Arthur S. Link, a preeminent Wilson scholar, access to his diary. Much of it was published in Link's collection of Wilson papers in the late 1980s and early 1990s, offering historians an in-depth look at the troubled end of Wilson's presidency and how his health could have affected major events.\nLink's grandson is one of the historians now sifting through the Grayson papers to see what else they reveal about the president. Arthur Link III and other researchers were struck by this letter from Grayson to his wife dated Aug. 19, 1915: \"\nMy number one patient in this house had an accident last night with one of his eyes -- the good one, which is bad now. I am hurrying off to Philadelphia with him at six o'clock tomorrow morning to consult an eye specialist. We are going by motor. I think we can make the trip less noticeable in this way -- . . . [T]he papers will read something like this: The President made his annual visit to the oculist etc etc.\"\nThe letter was written during Wilson's first term at a time when many historians believe he was relatively healthy. During Wilson's second term, Grayson wrote another letter to his wife Sept. 7, 1918, that may be of historical note. It appears to refer to a second operation, similar to the previous one that Grayson probably performed that year on Wilson's nose.\nResearchers were also struck by two photographs that experts say show Wilson at his last Cabinet meeting in 1921. In one photo, he is holding a cane; in the other, the cane appears to have been erased.\n\"They were flying by the seat of their pants, doing damage control,\" Link said. \"It's fun to watch.\"\nHistorians say the documents help shed light on what can happen when a president falls ill. The 25th Amendment spells out how a vice president can become acting president in the event of a presidential disability. But that amendment was ratified in 1967, nearly a half-century after Wilson left office.\nA letter Grayson sent from Europe to his friend Samuel Ross on April 14, 1919, shows that Wilson's doctor knew the stakes:\n\"The president was suddenly taken violently sick with the influenza at a time when the whole of civilization seemed to be in the balance. And without him and his guidance Europe would certainly have turned to Bolshevism and anarchy. From your side of the water you can not realize on what thin ice European civilization has been skating. I just wish you could spend a day with me behind the scenes here. Some day perhaps I may be able to tell the world what a close call we had.\"\nNow the papers are gone from the Grayson family basement, attic and closets. But Cary Grayson Jr. still has family stories to share. He recalled how his older brother Gordon went with their father to the White House to cheer the president after the 1919 stroke, sitting at Wilson's feet while they watched Western movies. Sometimes they went for drives with him.\nGrayson himself had a stroke in August. He used a cane as he walked around the wooded hills where his father used to go fox hunting. He said he is relieved that the papers have a place in history.\nView a Grayson letter from the eLibrary detailing secret. Click here.","The U.S. National Park Service has been celebrating its 100th anniversary in 2016.\nWoodrow Wilson, America’s 28th president, established the National Park Service in 1916 to “protect the wild and wonderful landscapes” in the United States.\nBut it is an earlier leader who is considered the father of the America’s national parks. In 1906, Theodore Roosevelt, America’s 26th president, signed the American Antiquities Act. The law permitted him – and future presidents - to take immediate action to protect important cultural or natural resources.\nThe Antiquities Act led to the creation of many of the 413 sites within the National Park Service today.\nNo president has played a bigger role in protecting the country’s natural and cultural resources than Theodore Roosevelt. During his time in office, he established five new national parks and 18 national monuments. In all, he protected over 93 million hectares of public land.\nHe became known as “the conservationist president.”\nRoosevelt’s concern for the land and environment came from the time he spent in the Dakota Territory, beginning in the 1880s.\nThe area where he traveled is now the state of North Dakota. Today, you will find a national park there named in his honor. The park protects badlands, wildlife, [and] scenic views, as well as two ranches where Roosevelt himself once lived.\nWelcome to Theodore Roosevelt National Park!\nTheodore Roosevelt in Dakota Territory in 1883\nTheodore Roosevelt came to Dakota Territory in September 1883. He was a young, married man from New York, where his political career was just beginning. He came to Dakota in hopes of hunting huge animals called bison. He also had a great interest in the Western frontier lifestyle.\nBison at Theodore Roosevelt National Park\nRoosevelt soon developed an interest in raising cattle. Cattle ranching in Dakota was a big business in the 1880s. Cattle fed on the land’s healthful grasses. He and a partner entered the business. Roosevelt invested $14,000 to build the Maltese Cross Ranch.\nTheodore Roosevelt's Maltese Cross Cabin\nRoosevelt returned to New York while workers constructed the ranch. He resumed his political duties in Albany, the state capital. But, in early 1884, he experienced two great personal losses. His mother and wife died of illnesses on the same day, February 14. Roosevelt described the pain and loss in his diary with only one sentence: “The light has gone out of my life.”\nRoosevelt again headed west in the summer of 1884. He sought to escape the reminders of his recent losses. He arrived at his newly built Maltese Cross Ranch. He also decided to build a second ranch in a quieter, more remote area. He called that ranch Elkhorn.\nThe front veranda of Elkhorn Ranch\nRoosevelt traveled between New York and Dakota, working both as a state lawmaker and a cattle rancher. In late 1884, he helped form an organization in Dakota to help protect ranchers’ rights.\nIn 1885, Roosevelt published his first book about his experiences as a rancher and hunter. In it, he predicted that the cattle industry of the Dakota Badlands was not sustainable. In other words, it would not last.\nRoosevelt was right.\nSevere weather struck the area in 1886 and 1887. In the winter, a terrible freeze killed many cattle. The animals that survived the cold soon starved. Roosevelt himself lost over half of his cattle. He decided to get out of the business.\nThe experience, however, shaped Roosevelt’s beliefs about the need for conservation in America. Those beliefs, in turn, helped shape his policies as president.\nVisiting the park\nVisitors to Theodore Roosevelt National Park today can experience the badlands just as Roosevelt did hundreds of years ago. They can also visit the Maltese Cross Cabin as well as the Elkhorn Ranch area.\nThe park has three main areas -- the South Unit, the North Unit, and the Elkhorn Ranch Unit.\nIn the South Unit, visitors can drive along the Scenic Loop road. It offers many places to see wildlife and the surrounding badlands. Badlands are very dry places with little vegetation. Wind and water shape badlands, mainly through erosion. The process leaves behind high, flat-topped hills of clay and other soft rock.\nA hiking trail in the South Unit of the park\nMany visitors stop to look at Painted Canyon. It gets its name from the colorful exposed rocks there.\nTrails near the canyon offer visitors a chance to see animals, from the huge American bison to small black-tailed prairie dogs.\nA black-tailed prairie dog\nThese animals are not really dogs. They are rodents. Roosevelt described prairie dogs as the “most noisy and inquisitive animals imaginable.”\nThe North Unit also offers several hiking trails. Some paths are short and easy. Others may take two days to complete.\nThe North Unit of Theodore Roosevelt National Park\nThe Achenbach trail is a 28-kilometer-long path. It crosses the Little Missouri River and takes visitors into the heart of the Theodore Roosevelt wilderness.\nThe third area of the park is the Elkhorn Ranch Unit. This is what Roosevelt described as his “home ranch.”\nHe wrote of the ranch in this way: “My home ranch-house stands on the river brink. From the low, long veranda, shaded by leafy cotton-woods, one looks across sand bars and shallows to a strip of meadowland, behind which rises a line of sheer cliffs and grassy plateaus.”\nA view from the Elkhorn Ranch Unit\nToday, the Elkhorn cabin itself no longer stands. Visitors will find only stone rocks where the cabin once was. The area that surrounds Elkhorn, however, is among the most beautiful, wild and quiet places in the badlands of North Dakota.\nIt is this peace and beauty that appealed to Roosevelt after the deaths of his mother and wife.\nBut the Dakota badlands did more than just help Roosevelt overcome his pain. They helped shape the kind of president he would later become.\nIn the words of Roosevelt himself, \"I would not have been president had it not been for my experience in North Dakota.\"\nI’m Ashley Thompson.\nAnd I’m John Russell.\nAshley Thompson wrote this story with materials from the National Park Service. Caty Weaver was the editor.\nEditor's Note: Each week in 2016, VOA Learning English has been exploring some of the many sites within the National Park Service. While this is the final week of our America's National Parks series, we will continue to write about this topic in the future! Let us know if you have any ideas or suggestions.\nWords in This Story\nbadlands - n. a region in the U.S. where weather has worn away rocks into strange shapes and where there are very few plants\nranch - n. a large farm especially in the U.S. where animals (such as cattle, horses, and sheep) are raised\ncabin - n. a small, simple house made of wood\nfrontier - n. a distant area where few people live\nsustainable - adj. able to be used without being completely used up or destroyed\nerosion - n. the gradual destruction of something by natural forces (such as water, wind, or ice)\nconservation - n. the protection of animals, plants, and natural resources\ninquisitive - adj. having a desire to know or learn more\nveranda - n. a long, open structure on the outside of a building that has a roof\nstrip - n. a long, narrow piece of something\nsheer - adj. almost straight up and down\nplateaus - n. a large flat area of land that is higher than other areas of land that surround it"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c4349d01-bd62-4efe-9ba2-f8287b743dc1>","<urn:uuid:412a3a86-7577-4a84-aae6-27e90b4711ab>"],"error":null}
{"question":"What's the main goal of RIKEN's genomic research, and what ethical challenges arise from genetic screening programs?","answer":"RIKEN's main goal is to identify useful biomarkers for predicting drug-induced adverse events, guiding drug use, and disease prediction diagnosis through international collaborations. They aim to establish an international SNP research network to advance personalized medicine. Regarding ethical challenges, genetic screening raises concerns about negative psychological impact, discrimination by employers and insurers, as well as issues related to privacy, confidentiality, and individual autonomy, particularly as screening expands from newborn to adult-onset conditions.","context":["RIKEN Center for Integrative Medical Sciences Laboratory for International Alliance on Genomic Research\nTeam Leader: Taisei Mushiroda (Ph.D.)\nThe main aim of our laboratory is, through collaborations in Japan and internationally, to identify useful biomarkers which could be used to predict drug-induced adverse events, guide drug use and be used in disease prediction diagnosis. We actively recruit young scientists from overseas to work at RIKEN and carry out research in SNP-based approach, statistical analysis and biological analysis. Our long-term goal is to establish an international SNP research network that will make the Center for Integrative Medical Sciences a world leader in personalized medicine.\nMain Research Fields\n- Medicine, Dentistry & Pharmacy\n- Genomic biomarker\n- Drug response\n- Adverse drug reaction\n- Germline genetic testing\n- 1.Ruamviboonsuk P, Tadarati M, Singhanetr P, Wattanapokayakit S, Kunhapan P, Wanitchanon T, Wichukchinda N, Mushiroda T, Akiyama M, Momozawa Y, Kubo M, Mahasirimongkol S.:\n\"Genome-wide association study of neovascular age-related macular degeneration in the Thai population\"\nJ Hum Genet. in press (2017).\n- 2.Petros Z, Lee MT, Takahashi A, Zhang Y, Yimer G, Habtewold A, Schuppe-Koistinen I, Mushiroda T, Makonnen E, Kubo M, Aklillu E.:\n\"Genome-Wide Association and Replication Study of Hepatotoxicity Induced by Antiretrovirals Alone or with Concomitant Anti-Tuberculosis Drugs\"\nOMICS 21, 207-16 (2017).\n- 3.Petros Z, Lee MM, Takahashi A, Zhang Y, Yimer G, Habtewold A, Amogne W, Aderaye G, Schuppe-Koistinen I, Mushiroda T, Makonnen E, Kubo M, Aklillu E.:\n\"Genome-wide association and replication study of anti-tuberculosis drugs-induced liver toxicity\"\nBMC Genomics 17, 755 (2016).\n- 4.Chin YM, Tan LP, Abdul Aziz N, Mushiroda T, Kubo M, Mohd Kornain NK, Tan GW, Khoo AS, Krishnan G, Pua KC, Yap YY, Teo SH, Lim PV, Nakamura Y, Lum CL; Malaysian NPC Study Group, Ng CC.:\n\"Integrated pathway analysis of nasopharyngeal carcinoma implicates the axonemal dynein complex in the Malaysian cohort\"\nInt J Cancer 139, 1731-9 (2016).\n- 5.Low JS, Chin YM, Mushiroda T, Kubo M, Govindasamy GK, Pua KC, Yap YY, Yap LF, Subramaniam SK, Ong CA, Tan TY, Khoo AS; Malaysian NPC Study Group, Ng CC.:\n\"A Genome Wide Study of Copy Number Variation Associated with Nasopharyngeal Carcinoma in Malaysian Chinese Identifies CNVs at 11q14.3 and 6p21.3 as Candidate Loci\"\nPLoS One 11, e0145774 (2016)\n- 6.Chin YM, Mushiroda T, Takahashi A, Kubo M, Krishnan G, Yap LF, Teo SH, Lim PV, Yap YY, Pua KC, Kamatani N, Nakamura Y, Sam CK, Khoo AS; Malaysian NPC Study Group, Ng CC.:\n\"HLA-A SNPs and amino acid variants are associated with nasopharyngeal carcinoma in Malaysian Chinese\"\nInt J Cancer 136, 678-87 (2015).\n- 7.Chen CH, Lee CS, Lee MT, Ouyang WC, Chen CC, Chong MY, Wu JY, Tan HK, Lee YC, Chuo LJ, Chiu NY, Tsang HY, Chang TJ, Lung FW, Chiu CH, Chang CH, Chen YS, Hou YM, Chen CC, Lai TJ, Tung CL, Chen CY, Lane HY, Su TP, Feng J, Lin JJ, Chang CJ, Teng PR, Liu CY, Chen CK, Liu IC, Chen JJ, Lu T, Fan CC, Wu CK, Li CF, Wang KH, Wu LS, Peng HL, Chang CP, Lu LS, Chen YT, Cheng AT; Taiwan Bipolar Consortium.:\n\"Variant GADL1 and response to lithium therapy in bipolar I disorder\"\nN Engl J Med 370, 119-28 (2014).\n- 8.Ohara M, Takahashi H, Lee MT, Wen MS, Lee TH, Chuang HP, Luo CH, Arima A, Onozuka A, Nagai R, Shiomi M, Mihara K, Morita T, Chen YT.:\n\"Determinants of the over-anticoagulation response during warfarin initiation therapy in Asian patients based on population pharmacokinetic-pharmacodynamic analyses\"\nPLoS One, 9, e105891 (2014).\n- 9.Caudle KE, Rettie AE, Whirl-Carrillo M, Smith LH, Mintzer S, Lee MT, Klein TE, Callaghan JT; Clinical Pharmacogenetics Implementation Consortium.:\n\"Clinical Pharmacogenetics Implementation Consortium (CPIC) Guidelines for CYP2C9 and HLA-B genotype and Phenytoin Dosing\"\nClin Pharmacol Ther 96, 542-8 (2014).\n- 10.Lee MT, Mahasirimongkol S, Zhang Y, Suwankesawong W, Chaikledkaew U, Pavlidis C, Patrinos GP, Chantratita W.:\n\"Clinical application of pharmacogenomics: The example of HLA-based drug-induced toxicity\"\nPublic Health Genomics, 17, 248-55 (2014).\n- Taisei Mushiroda\n- Team Leader\n1-7-22 Suehiro-cho, Tsurumi-ku,\nYokohama City, Kanagawa,\nEmail: mushiroda [at] riken.jp","Discussion: Ethical Questions in Screening for Genetic Diseases\nGenetic screening is still a new and exciting field.\nThe integration of genetics in public healthcare offers many opportunities for health promotion and disease prevention.\nThere are three types of genetic screening that can be used for public health purposes: carrier, diagnostic, and predictive.\nThe most popular form of genetic screening is prenatal and neonatal screening. It helps to identify current diseases (e.g., Down syndrome, phenylketonuria).\nCarrier screening determines if an individual is carrying a particular genetic trait (e.g. type 2 diabetes, breast cancer).\nA predictive screening checks whether an otherwise healthy individual with positive family histories has a genetic mutation that could lead to late-onset disorders (e.g. Huntington’s Disease).\nAs the population genetic screening expands from the newborn to adult-onset arena, there are increasing concerns about whether such screening might have a negative psychological impact and discrimination by third parties including employers and insurers.\nEthical concerns are also raised about privacy, confidentiality, and individual autonomy.\nReview the Learning Recourses to find a article in the Walden Library about a genetic disorder.\nIt can be a genetic condition or a disease with a genetic component.\nYou should consider the ethical implications of screening for genetic disease.\nKeep these thoughts in your mind:\nGive a brief overview of the article. Also describe the genetic epidemiology and cause of the disease.\nOne ethical question related to screening is this disease.\nYour argument for or against mandatory screening for this condition is presented.\nYou can support your position with scholarly resources.\nGenetic testing is a modern technology that helps to identify susceptibilities to any disease or defect.\nGenetic testing is a useful diagnostic tool that can be used for both personal and healthcare purposes.\nThe genetic testing process can help detect complicated diseases like colon, breast, ovarian, or ovarian cancer at an early stage (Harper 2010).\nThis essay will discuss the most controversial ethical issue in genetic testing of breast cancer. The patient confidentiality of their breast cancer genetic testing information is discussed.\nThis essay discusses a similar issue, and provides rationale for mandatory breast cancer screening.\nGenetic Epidemiology Of Breast Cancer\nBreast cancer is second in most cases in females, after skin cancer.\nBreast cancer was responsible for 13.7% of all female deaths worldwide in 2008.\nContemporary lifestyles have contributed to an increase in breast cancer deaths over the past 70 years (Davis 2010).\nButow and colleagues.\nButow et al. (2013) found that one in eight women has a lifetime risk of developing breast cancer. Additionally, one out of every 35 women who dies from cancer is at risk.\nThe United States has the highest annual breast cancer rate at 128.7 percent for 15,000 Whites and 111.6 percent for 15,000 African Americans.\nPeterson et. al.\n(2012) Breast cancer continues to be the most serious form of cancer in women in the United States.\nThe genetic testing for breast carcinoma is such a grave hazard, there are ethical concerns about its confidentiality and sharing.\nGoldenberg & Sharp (2012) conducted a survey that asked 160 relatives of the victims to find out their family history of breast carcinoma.\nOut of 376 participants 71% were first- or second-blood relatives while 82% were blood relations.\n24% of participants didn’t know about or were not informed about their family history. The remaining 76% were blood relations.\nThis suggests that this is a sensitive topic.\nBrierley and colleagues (2012) looked at the potential impact of confidentiality, cost, and discrimination on those who pursue genetic testing.\nThe study, which involved 184 participants, found that 106 of them underwent testing while 78 refused testing. This was due to cost, confidentiality and discrimination.\nAccording to Butow and colleagues.\n2013 survey of 238 relatives of breast cancer victims revealed that they were concerned about confidentiality and autonomy when it came to genetic testing.\nThe results showed that 86-87% of women disapproved of the disclosure of genetic results. 56-57% desired informed consent before sharing information with any family members. 98% preferred mandatory and voluntary genetic testing.\nThis suggests that genetic screening of breast cancer is important and necessary, but also poses ethical problems.\nAn Overview of the Article\nThis article will discuss the legal and ethical aspects of genetic testing for breast, ovarian and colon cancer.\nThese three types of cancer have almost the same privacy and confidentiality as a woman’s life.\nThis article describes the ethical conflict that patients must keep private information and the professional’s duty to share information for the benefit of others.\nThe articles provide information on disclosure obligations and the duties related to genetic susceptibility. Further, they discuss confidentiality and liabilities in genetic counselling, prenatal diagnosis and genetic screening.\nThis article discusses the public’s perceptions of genetic discrimination and public health testing. It also addresses ethical concerns related to breast carcinoma.\nThis article’s author has found that genetic testing for breast carcinoma is an ethical concern for feminists.\nTheir attitudes to new technologies and society’s trends are determined by the traditional views, political and professional authority of women.\nWomen are still being victim to traditional feminist perceptions.\nHowever, attempts are being made to alter traditional views and increase awareness and alertness of the female sector of society.\nLastly, however, the article mentions that ethical issues and conflicts concerning genetic testing susceptibility to breast cancer will still remain a feminist topic. It is therefore necessary to promote healthcare ethics and feminist methods to ensure women have their autonomy as well confidentiality.\nGenetic Screening for Breast Cancer: A Moral Issue\nDue to privacy, security and feminism, the confidentiality of patients has always been a significant ethical issue in genetic screening.\nAccording to Harper (2010), disclosure of breast cancer screening information is an overrated ethical issue and requires specific legal obligations.\nButow and colleagues.\nThe 2013 study found that even family members with a history or breast cancer were not informed about the situation.\nContemporary genetic screening practices have been affected by this.\n53% of the 1,251 physicians who handled genetic screening of breast carcinoma said that they need to keep their patients’ confidential information.\n(2012) showed that any American healthcare provider is required by law to protect patient information.\nFor genetic screening, confidentiality breaches can be considered a criminal offense.\nThis prevents physicians from disclosing information, even if a relative is at risk for similar diseases.\nClayton et al. (2014) found that breast, colon, ovary, and lung cancers are the most serious cancers for children, siblings, or other family members.\nAccording to confidentiality law, the physician does not have a legal obligation to warn the patient about this risk.\nIf a physician is aware that a patient could infect a spouse, it is their legal responsibility to inform the spouse.\nIn order to ensure that genetic screening of breast carcinoma is done ethically, this vision is unclear.\nReasons for Mandatory Breast Cancer Genetic Screening\nGenetic screening for breast cancer should be an integral part of public health. This is regardless of conflict or ethical concerns. Increased risk of such deadly diseases can threaten the equilibrium of human survival.\nModern methods like genetic screening provide a safe diagnosis that ensures the existence or safety of the risk to the foetus even through prenatal screening. This provides a promising control for future risk (Davis 2010).\nDancey and colleagues.\nMandatory genetic screening, (2012) will aid in timely counselling and lower the risk for vulnerable deaths.\n(2014) state that mandatory genetic testing for breast cancer could revolutionize healthcare worldwide and help women to feel more secure about their status in society.\nThese are all reasons that genetic screening should be mandatory for women’s health.\nThis study shows that confidentiality remains an ethical issue when genetic screening is done for breast cancer.\nBecause of traditional beliefs, privacy, protection of vulnerable and other reasons, it is an ethical issue.\nHowever, genetic screening for the diagnosis of breast cancer is a promising option for improving women’s health by avoiding deadly consequences and reducing their risk.\nGenetic dilemmas: Reproductive technology, parental choices, children’s futures.\nOxford University Press.\nPractical Genetic Counselling 7th Ed.\nNegative events in cancer genetic testing: financial, medical, ethical and legal implications.\nThe Cancer Journal 18(4), 309-309.\nPsychological outcomes and risk perceptions following genetic counselling in breast cancer: A systematic review.\nMedical Journal of Australia 178(2): 77-81.\nClayton, E. W.; McCullough L. B.; Biesecker L. G. Joffe S., Ross L. F. Wolf S. M. For the Clinical Sequencing Exploratory Researchers (CSER), Consortium Pediatrics Working Group.\nGenetic testing and genetic sequencing of children: ethical dilemmas.\nThe American Journal of Bioethics. 14(3), 3-9.\nThe genetic basis for cancer treatment decisions.\nLegal and ethical issues regarding genetic counseling and testing for susceptibility breast, ovarian, and colon cancer.\nCanadian Medical Association Journal, 54(6), 813.\nThe ethical risks and programmatic challenges associated with genomic newborn screening.\nHealth insurance and discrimination concerns, and BRCA1/2 testing in clinic populations.\nCancer Epidemiology and Prevention Biomarkers 11(1), 79–87."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:7ca423b7-e6be-48ef-ada3-9639f7d500b2>","<urn:uuid:4848957e-8523-4373-b752-e81d53659608>"],"error":null}
{"question":"What strategies help prevent winter salt damage to plants, and how can mulching improve soil moisture?","answer":"To prevent salt damage, install burlap or canvas screens to protect sensitive species like white pine, red pine, arborvitae, red oak and crabapples from highway deicing spray. Alternatively, plant salt-tolerant species such as blue spruce, Austrian pine and honeylocust. For soil moisture management through mulching, apply materials around plants to prevent soil drying and ground cracking. Mulching helps retain moisture, prevents weeds, and in winter can prevent nutrients from leaching away. Additionally, adding water retaining crystals to soil and burying newspaper underground helps retain moisture.","context":["Horticultural Gardening Tips/Irrigation\nVarious non plant related tips (Add your tips NOW! by clicking 'Edit this page'. No registration/login required.)\nSoak ground thoroughly but not frequently - you want to plants to search deep down for roots. Mulch like hell after heavy rain/watering. Some plants benefit from being planted deeply (tomatoes, potatoes). Earth up tomatoes/potatoes as they grow to prompt more root mass. Add water retaining crystals to soil. Mix ground soil with shop bought compost. Line terracotta pots/wooden barrels with old plastic/compost bags. When drilling holes in pots, don't drill at bottom of pots- but slightly up the sides. Bury newspaper underground to help retain moisture. Add Mycoorhizal fungi when planting trees/roses/grapes/woody perennial plants to increase plant surface in contact with soil moisture and nutrients. Redirect roof drain pipes into water butts/bins. Re-use water from the house - from baths/showers, rinsing/washing machine rinse cycles. Never use dishwasher water - it contains too many salts. Washing machine water generally ok, especially the final 2 rinses -phosphates in washing powders may even give plants a boost - but may cause overfeeding. Experiment. Consider drought resistant plants if soil is too stoney/dries out too quick: carrots, tomatoes, peppers, rosemary, lavender, cape gooseberries...\nGetting rid of weeds\nGrass: skim with spade horizontal. Nettles: dig out yellow roots. Bindweed: spray or paint leaves with glyphosate.\nReasons for mulching: Prevents weeds, keeps soil moist, prevents drying out, prevents ground cracking. Mulch especially when weed plants seed (after flowering/late summer).\nMaterials to use: leaves, compost, manure (cow better than horse - 6 stomachs to destroy grass seeds), newspaper, black plastic weed mulch (versions with holes are best), old compost bags (v effective but unsightly), rocks, cut grass, paving slabs, carpet (eventually rots and weeds seed into it), cardboard (anchor down), kitchen worktops, weeds (not bindweed/nettles or any with seed heads). Pile weeds and cut grass (grass cut with shears is better than with lawnmower), around base of trees - but leave inch gap of air around trunk in case weeds/grass cuttings generate heat during decomposition. Use dug/pulled up weeds to mulch other weeds. Add gravel/pebbles/bits of card to soil surface of plants in pots - prevents weeds and evaporation. Plant crops with large leaves - rhubarb, courgettes, pumpkins, cabbage etc or vigorous plants e.g., potatoes/nasturtiums/raspberries/trees. Mulch with impermeable plastic during winter to prevent nutrients leaching away and to retain surface tilth. Don't mulch if tilth is bad - frost will help shatter clods.\nStuff nettles/comfrey in bucket/bin and add water. Leave for 3 weeks, dilute and use. Bouquet-garni of manure - stick perforated bag of manure into bucket of water. Potage - Submerge weeds in large bucket/bin - and leave for 3 weeks. Stick weeds into water butt and use concentrated runoff through tap (make filter from chicken wire to avoid blockages) 3 months later. Pee on a bail of straw and add to water filled bucket - and use 3 months later. Ensure seeds within straw won't germinate. Cover bare soil to prevent winter rain leaching out nutrients. When planting in pots mix slow release granules into soil when preparing compost.\nPlant protection from weather\nFrost protection: use fleece (very effective when doubled over), newspapers, cardboard boxes, cloches, portable coldframes. Consider keeping some plants in pots - which can be hauled into a shed/greenhouse/indoors. On low growing plants, cloches, old glass sheets, corrugated plastic sheets are good for keeping winter rains off things like garlic (which rots easily). Make a tall thin greenhouse by sticking 3 or 4 bamboos vertically into the ground, tying them at the top to make a cylinder or pyramid. Wrap clingfilm or clear polythene around the bamboos and secure. Gives plants extra boost and protects them from rain, cold northerly winds. Help tender plants get through winter by keeping soil drier. Ridge soil up to avoid water logging. Build raised beds using old pallets/bricks/soil also to reduce waterlogging. In summer dig 50cm wide 40cm deep trenches to create micro-climates for small sub-tropical plants (peppers, tomatoes, aubergines etc). Mound soil from within trench on north side of trench to block cold northerly winds. Also makes plant watering easier. Add insulation to greenhouses using fleece, bubble wrap, newspapers. Keep greenhouse interior dry in winter to keep temperatures up. Keep interior moist in summer to reduce temperature and increase humidity (good for plants). Stake trees/tomatoes/beans when planting, before gales rather than after.\nDistinguishing your seedlings from weeds: Sow in rows. Mark out rows with sticks/bamboos. Grow a few seeds in a pot indoors - and compare outdoor seedlings/weeds with your pot specimens. Damping off: a fungal disease when seedlings grow to 3 cm, then suddenly fall over and die. Prevention: lots of ventilation, light and space between seedlings. Don't overwater. Direct sunlight kills the fungus - but beware of seedlings getting cooked. Sow seeds in vermiculite. Apply fungicide to soil before sowing seed. Use fresh, sterilised compost. Water seeds with tap water only - never rain water. Seedlings not germinating: may be due to seeds/soil drying out - shade sown seeds with newspaper, cardboard, walls. Put seed tray in supermarket carrier bags. Spray with fine water spray daily (but reduce spraying when seed germinates to avoid damping off). Plant seeds in several different trays and pots and put in different locations/conditions - experiment! and note which place/cover is most successful. Add your results here!","Trees & Shrubs\nUNL Extension – helping you turn knowledge into \"know how\"\nWinter Tough on Landscape Plants\nby Don Janssen, UNL Extension Educator\nWinter can be rough on landscape plants, especially species that aren't native to Nebraska. Some types of damage can be prevented, however, and fall's the time to do it.\nAmong the most common types of damage are freezing, frost crack, desiccation and physical breakage. Homeowners can prevent or minimize some types of damage by shading plants, shielding them against heavy snow loads and flying salt spray, and wrapping.\nFreezing injury to woody plants most commonly affects species or varieties that are not completely hardy in northern climates. It may kill only the flowers and shoots, the roots or the whole plant. The best way to prevent freezing injury is to plant only species and cultivars known to be hardy here.\nInjury by late spring frost can sometimes be prevented by shading plants. Keeping the warming rays of the sun off the plants can delay bud break by five to 10 days. This may be long enough to avoid damage to tender flowers and shoots. It is practical only with selected small plants, however.\nFrost crack, or \"southwest disease,\" usually affects young, thin-barked trees, particularly young, newly transplanted silver maple trees. The bark splits when a bright, sunny winter day is followed by a rapid drop in temperature. The outer bark cools and contracts faster than the inner tissues and splits open, usually on the southwest side.\nPlanting species that are not prone to the problem and wrapping or shading the trunks or newly transplanted trees can reduce the incidence of frost crack.\nDesiccation is most common in broadleaved evergreens, though it can occur in narrow-leaved evergreens and even on the shoot tips of deciduous species. It occurs on bright, sunny winter days. The sun warms the foliage, which then loses moisture to the surrounding dry air. Because the ground is frozen, the roots cannot take up moisture to replace it and the foliage or shoot dries out.\nPlant sensitive plants where a building, a fence or other plants will shade them in the winter or construct sun shields on the south and west sides of plants to prevent this injury. Making sure plants go into winter well watered also helps.\nDamage caused by drifting salt spray from highway deicing efforts look much like desiccation. Some species -- including white pine, red pine and arborvitae, red oak and crabapples -- are very sensitive to salt injury. Burlap or canvas screens can protect them against salt spray. Planting salt tolerant species such as blue spruce, Austrian pine and honeylocust is another option.\nPhysical damage to plants can occur in the form of branches broken by a heavy load of snow or ice, frost heaving of young plants, and mechanical injury from snow removal equipment.\nWrapping the plants with twine or chicken wire may be all the support that is needed. Others may require more elaborate wood or metal snow shields. Avoid plants known for their tendency to break in storms and prune trees when young to eliminate weak, V-shaped crotches. This will reduce the likelihood of breakage.\nTo prevent heaving of newly transplanted young plants on clay or clay-loam soils, mulch fall planted trees and shrubs after the soil has frozen to prevent its alternately freezing and thawing. This can push the plants' roots out of the ground.\nTo reduce damage by snow removal equipment, plant trees and shrubs some distance from snow plowing routes. Clearly mark the location of small plants that might be hidden by a blanket of snow.\nThe best time to prevent winter damage to landscape plants is before you plant. At that point you can still choose a hardy plant and a suitable planting site that minimizes the need for winter protection. For plants already in place, prevention is the best approach -- there is no cure for dead or severely damaged plants.\nUniversity of Nebraska-Lincoln Extension in Lancaster County is your on-line yard and garden educational resource. The information on this Web site is valid for residents of southeastern Nebraska. It may or may not apply in your area. If you live outside southeastern Nebraska, visit your local Extension office\nContact Information University of Nebraska-Lincoln in Lancaster County\nWeb site: lancaster.unl.edu\n444 Cherrycreek Road, Suite A, Lincoln, NE 68528 | 402-441-7180"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d867ec03-6b95-490f-9b5a-8ca612684a4b>","<urn:uuid:5e4ef9af-f008-4057-989a-491dfef05d05>"],"error":null}
{"question":"As a music student considering my options, I'm curious about the performance facilities at the University of Chichester and the technology available for recording - what does each offer?","answer":"The University of Chichester has extensive facilities for both performance and recording. For performance, it features an acoustically superb performance venue and well-equipped practice rooms. The recording facilities include computerized recording and media studios. For technology, they have impressive equipment including a valve 32 channel TL audio mixing desk, an SSL Matrix 2 console with Neve preamps, a 7.1 surround studio with Genelec speakers, and various synthesizers including a rare Buchla System 7. They also provide industry-standard software like Logic Pro X, Pro Tools, Ableton Live, and other professional audio tools.","context":["The University of Chichester Conservatoire has one of the largest and liveliest music departments in the UK with a community of over 1000 student performers. Our facilities include computerised recording and media studios, well equipped practice rooms and an acoustically superb performance venue.\nThis MA Music Performance degree will support you to enhance and perfect your performance. You will explore the work of specialists in your own performance field, develop a lecture recital and research relevant areas of repertoire and performance practice. Your dissertation is a recital presented at the beginning of the following academic year.\nYou can choose to study this MA online.\nTeaching and Assessment\nHow you will learn\nThis course requires a fluent level of technical and expressive skill. You will attend weekly individual tuition in your instrument or voice, weekly group skills development sessions, masterclasses at Chichester and other institutions, ensemble training, and a consultation with an external specialist each semester. The support you receive from University lecturers is complemented by invaluable exposure to other professional views and experience of different institutions throughout your studies.\nWhat you will study\nYou will study a selection of core and optional modules in each year. Each module is worth a number of credits is delivered differently, depending on its content and focus of study.\nThis list is indicative and subject to change.\nYou will identify an appropriate professional performance context and demonstrate the particular strategies it demands of the player, singer or conductor. You are encouraged to identify your own practice with the work of individual artists who are active in a relevant field.\nIn this module, you will explore a range of community and professional performance contexts and the varied strategies that these demand of the player, singer or conductor, using the module criteria to evaluate observed approaches to programming and repertoire. Case studies of current performance practice will be introduced by tutors, and as the module progresses you will be encouraged to identify your own practice with the work of individual artists active in a relevant field.\nThis module’s content will depend on the choice of performed material of the individual students, but will focus on three main areas of personal development:\n- the achievement of technical and expressive maturity\n- the identification of a demonstrably appropriate repertoire or a mix of repertoire which is able to foreground individual strengths\n- the acquisition and consolidation of strong performance and communication skills.\nYou will select an area of your own repertoire to research theoretically and in terms of its broader context, presenting results either in written form or as a linked sequence of performance recordings.\nWritten Exercise (Performance Practice)\nThis module builds on the previous module and is an opportunity to reflect on the specialist practice that exists within your own discipline. You will examine individual elements of performance as well as more innovative performance techniques, which must be translated into discrete terms for the instrument or voice.\nUse industry standard equipment\nLearning Resource Centre\nLearn from experienced performers, musicians and tutors\nYou will be taught by a core team of experienced and highly qualified tutors alongside a wide-ranging team of more than 60 specialist instrumental and vocal teachers.\nAs well as supporting student development and the student experience our staff are active, practicing professional musicians and researchers who regularly perform and record. We have around 140 professional tutors who visit campus regularly throughout the semester to deliver our practical and contextual modules. You will be supported by your one-to-one tutor and your module tutors, as well as your Academic Advisor.\nCourse Fees 2023/24\nFor further details about fees, please see our Tuition Fees page.\nFor further details about international scholarships, please see our Scholarships page.\nUniversity of Chichester alumni who have completed a full undergraduate degree at the University will receive a 15% discount on their postgraduate fees.\nTypical offers (individual offers may vary):\nYou will need to interview for this course.\nFrequently asked questions\nHow do I apply?\nClick the ‘Apply now’ button to go to our postgraduate application form.","Creative Music Technology (Top-up) BA Hons\nBA Hons Creative Music Technology Top-up\nDo you have an HND or Foundation degree in music technology, music production or similar? Would you like to top it up to a full Bachelor's degree?\nOn this creative music technology top-up degree, you'll develop your creative and technical skills to degree level using new music technologies. You’ll get experience with sound software and hardware used by industry professionals, and get the chance to achieve additional certifications in the use of leading music technologies.\nAt the end of the course, you’ll be ready to begin your career in various areas of music technology areas such as games and animation, music recording and production, and television and film.\nWhat you'll experience\nOn this creative music technology top-up degree course you'll:\n- Turn your Higher National Diploma (HND), Foundation degree or equivalent qualification into a full Bachelor's degree\n- Develop your personal creative practice with emerging sound technologies\n- Choose optional modules that allow you to match what you study to your interests and career goals\n- Be able to apply for an optional Avid Protools accredited training certificate in Pro Tools Software\n- Get the chance to apply for Audiokinetic Wwise certification (we are one of only 2 universities in the UK to offer this)\n- Be taught by staff members with industry experience in composition, sound design and music production\n- Recording studios to engineer and produce music and sound design for games, film and TV\n- Sensors and alternative controllers to create sound installations or perform live\n- Hardware and software synths and plugins to create electronic music\n- Digital audio workstations to create and mix music\nYou’ll get your hands on some exciting gear in our studio suite, including:\n- A valve 32 channel TL audio mixing desk\n- An SSL Matrix 2 console with 10 Neve 1074 preamps and 16 channels of Neve and SSL dynamics and EQ\n- A 7.1 surround studio (Genelec) including a Slate Raven multi-touch console for multichannel work and spatial audio projects\n- A Buchla System 7 synthesizer (1 of only 2 in the UK and the only one in a European university)\n- 4 Oakley Modular synthesizers\n- iMac Dual i7 computers running Logic Pro X, Pro Tools, Ableton Live, Max/MSP (including Max for Live), Native Instruments (including Reaktor), Pure Data and Game engine software\nCareers and opportunities\nMusic technology is a continually developing field that's key to the creative industries, so there will be lots of opportunities open to you after the course.\nWhat can you do with a creative music technology degree?\nPrevious music technology graduates have gone on to work in areas such as:\n- studio recording\n- music production\n- sound design for television and film, animation and computer games\n- digital media\nWhat jobs can you do with a creative music technology degree?\nRoles our graduates have taken on include:\n- audio developer\n- music technology lecturer\n- musical technician\n- studio manager\n- studio engineer\n- music teacher\n- game audio professional (composition and sound design)\n- sound designer for visual media\nWhen you finish the course, our Careers and Employability service can help you find a job that puts your skills to work in the industry. You can get help, advice and support for up to 5 years after you leave the University as you advance in your career.\nWhat you'll study on this BA (Hons) Creative Music Technology top-up degree course\nEach module on this course is worth a certain number of credits. You need to study modules worth a total of 120 credits. For example, 4 modules worth 20 credits and 1 module worth 40 credits.\nCore modules are:\n- Final Year Project Preparation\n- Final Year Project Resolution\nOptional modules are:\n- Experimental Music Programming\n- Free Composition\n- Implementing Game Audio\n- Key Issues in Contemporary Music Studies\n- Music and Sound Synthesis\n- Professional Music Production\n- Sound Application\nChanges to course content\nWe use the best and most current research and professional practice alongside feedback from our students to make sure course content is relevant to your future career or further studies.\nTherefore, some course content may change over time to reflect changes in the discipline or industry and some optional modules may not run every year. If a module doesn’t run, we’ll let you know as soon as possible and help you choose an alternative module.\nHow you're assessed\nThe formal assessments you take on this music technology top-up vary according to the modules you choose. They include:\n- a portfolio of original compositional works\n- design and development of an audio/software project\n- oral assessments and presentations\n- written assignments including essays\nYou can get feedback on all practice and formal assessments so you can improve in the future.\nWork experience and career planning\nTo give you the best chance of securing a great job when you graduate, our Careers and Employability service can help you find relevant work experience during your course.\nWe can help you identify placements, internships, voluntary roles and freelancing opportunities that will complement your studies, further develop your skills and build your portfolio.\nTeaching on this course includes:\nDuring your scheduled teaching sessions, you'll focus on creative practices such as composition, synthesis, music computing and production.\nYou can also book one-to-one sessions if you feel you need specialist technical support for the music software you'll be using.\nYou can access all teaching resources on Moodle, our virtual learning environment, from anywhere with a Web connection.\nHow you'll spend your time\nA typical week\nWe recommend you spend at least 37 hours a week studying for your music technology top-up degree. You’ll be in timetabled teaching activities such as lectures and workshops for about 7–9 hours a week. The rest of the time you’ll do independent study such as research, reading, coursework and project work, alone or in a group with others from your course.\nMost timetabled teaching takes place during the day, Monday to Friday. You may occasionally need to go to University and course events in the evenings and at weekends. There’s usually no teaching on Wednesday afternoons.\nThe academic year runs from September to early June with breaks at Christmas and Easter. It's divided into 2 teaching blocks and 2 assessment periods:\n- September to December – teaching block 1\n- January – assessment period 1\n- January to May – teaching block 2 (includes Easter break)\n- May to June – assessment period 2\nExtra learning support\nThe amount of timetabled teaching you'll get on your top-up degree might be slightly less than what you're used to in your previous studies, but you'll also get face-to-face support from teaching and support staff when you need it. These include the following people and services:\nYour personal tutor helps you make the transition to independent study and gives you academic and personal support throughout your time at university.\nYou’ll have regular contact with your personal tutor in learning activities or scheduled meetings. You can also make an appointment with them if you need extra support.\nStudent support advisor\nIn addition to the support you get from your personal tutor, you’ll also have access to a Faculty student support advisor. They can give you confidential, impartial advice on anything to do with your studies and personal wellbeing and refer you to specialist support services.\nAcademic skills tutors\nYou'll have help from a team of faculty academic skills tutors. They can help you improve and develop your academic skills and support you in any area of your study.\nThey can help with:\n- improving your academic writing (for example, essays, reports, dissertations)\n- delivering presentations (including observing and filming presentations)\n- understanding and using assignment feedback\n- managing your time and workload\n- revision and exam techniques\nCreative skills tutors\nIf you need support with software and equipment or you want to learn additional skills (including skills not covered on your course), our creative skills tutors provide free workshops, activities and one-on-one tutorials. Skills you can learn include life drawing, film camera operation and video production.\nIT and computing support\nComputing support staff are always available to give technical support in the Faculty's computer suites during normal working hours. There's also some support available from 5pm to midnight at busy times of the year.\nAcademic skills support\nAs well as support from faculty staff and your personal tutor, you can use the University’s Academic Skills Unit (ASK).\nASK provides one-to-one support in areas such as:\n- academic writing\n- note taking\n- time management\n- critical thinking\n- presentation skills\n- working in groups\n- revision, memory and exam techniques\nIf you have a disability or need extra support, the Additional Support and Disability Centre (ASDAC) will give you help, support and advice.\nLibrary staff are available in person or by email, phone or online chat to help you make the most of the University’s library resources. You can also request one-to-one appointments and get support from a librarian who specialises in your subject area.\nThe library is open 24 hours a day, every day, in term time.\nSupport with English\nIf English isn't your first language, you can do one of our English language courses to improve your written and spoken English language skills before starting your degree. Once you're here, you can take part in our free English for Academic Purposes programme to improve your English further.\nBA (Hons) Creative Music Technology (Top-up) entry requirements\nQualifications or experience\n- 240 HE credits in a related qualification e.g. HND or Foundation Degree in Music Technology, Music Production, or similar. Applicants may be required to provide supplementary information.\nEnglish language requirements\n- English language proficiency at a minimum of IELTS band 6.0 with no component score below 5.5.\nIf you don't meet the English language requirements yet, you can achieve the level you need by successfully completing a pre-sessional English programme before you start your course.\nTuition fees (2020 start)\n- UK/EU/Channel Islands and Isle of Man students: £9,250\n- International students: £15,100\nAdditional course costs\nThese course-related costs aren’t included in the tuition fees. So you’ll need to budget for them when you plan your spending.\nOur accommodation section shows your accommodation options and highlights how much it costs to live in Portsmouth.\nYou’ll study up to 6 units a year. You may have to read several recommended books or textbooks for each unit.\nYou can borrow most of these from the Library. If you buy these, they may cost up to £60 each.\nWe recommend that you budget £75 a year for photocopying, memory sticks, DVDs and CDs, printing charges, binding and specialist printing.\nIf your final year includes a major project, there could be cost for transport or accommodation related to your research activities. The amount will depend on the project you choose.\nHow to apply\nTo start this course in 2020, apply through UCAS. You’ll need:\n- the UCAS course code – W372\n- our institution code – P80\nYou can start your application now and submit it later if you want.\nYou can also sign up to an Open Day to:\n- tour our campus, facilities and halls of residence\n- speak with lecturers and chat with our students\n- get information about where to live, how to fund your studies and which clubs and societies to join\nIf you're new to the application process, read our guide on applying for an undergraduate course.\nHow to apply from outside the UK\nIf you're from outside of the UK, you can apply for this course through UCAS or apply directly to us (see the 'How to apply' section above for details). You can also get an agent to help with your application. Check your country page for details of agents in your region.\nTo find out what to include in your application, head to the how to apply page of our international students section.\nIf you don't meet the English language requirements for this course yet, you can achieve the level you need by successfully completing a pre-sessional English programme before you start your course."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2af35881-5754-42b5-aa00-094b0fc00af7>","<urn:uuid:a5194235-de66-4e62-ad9e-146a346163f8>"],"error":null}
{"question":"What are the benefits of digital art for artists, and how is blockchain technology changing its commercialization?","answer":"Digital art offers several benefits for artists: it provides accessibility and mobility, allowing creation from anywhere; enables easy transition from traditional mediums; offers efficient tools for mixing colors and resolving time-consuming tasks; and provides numerous online learning resources. Regarding commercialization, blockchain technology is transforming how digital art is sold by ensuring digital scarcity, allowing artists to monitor purchases and control access to their work, and enabling fractional ownership of artworks. Blockchain also increases transparency in artwork provenance and helps protect intellectual property rights in digital art, which has traditionally been difficult to regulate due to easy reproduction of online images.","context":["Over the past several years, digital art has exploded in popularity. Some say that this trend is a direct result of the lucrative career it promises, while others point to the potential of furthering one’s artistic ventures. And even though this still remains a topic of debate, there’s an undeniable fact that, in this medium, digital artists can expand their own palette and technical skills with relative ease and convenience. The accessibility and mobility of digital art also make it attractive and versatile, allowing emerging professional artists to create iPad art, learn digital painting, play with color and light from display tablets, or use 3Dsoftware from virtually anywhere.\nWith multiple levels of beginners to advanced stage tutorials and how-to online guides, anyone can access the realm of digital art nowadays. One can even go as far as saying that your imagination is the limit. Most likely, however, it is the easy transition from the traditional to the digital, as well as the wide accessibility of all the tools needed to achieve the look and feel that you want, which makes digital art so appealing. By leveraging different digital painting techniques, art animation tools, character designs, and photoshop brushes, all at your fingertips, tedious and time-consuming tasks could be easily and efficiently resolved. Digital painting software allows you to mix and blend multiple colors to achieve challenging hues and variants of colors like skin tones almost instantly with zero mess and clean-up. The key point is that digital art is not only more convenient, but it’s also equally as rewarding.\nAnd with countless online resources out there offering a wide range of digital painting courses, live sessions, free trials and tutorials for digital painting softwares, as well as amazing tutorials from your favorite digital artists, it is no wonder why digital art has become such a popular medium to many. Below are several of the most valuable and insightful resources for digital artists to gain inspiration and learning to further your skills.\nGreat Learning Resources for Digital Artists\nOne of the best and most efficient ways of keeping yourself and your digital illustration up-to-date is through continuous education. Unlike traditional art, with a rapidly expanding technological advancement, the world of digital art is a much faster-evolving arena. It’s also not generally learned the same way as traditional and historical art. Surely, you can still attend a high-cost and oftentimes exclusive art school. However, many find that enrolling in a digital art program, which can be done over the internet and without having to break the bank, may well be just as effective. Here are digital art resources from where you can learn digital art techniques, find the right software for beginners, refine your art skills, signup for digital art courses, and keep yourself up-to-date on everything digital art related.\nWithout a shadow of a doubt, iamag.co is a prime source of inspiration for digital artists, whether they are learning the fundamentals or have been painting digitally for some time. It’s an online magazine for artists working in the game, movie, and animation industries. That’s said, anyone with a passion for digital art can look through its tutorials and learn a ton that this medium has to offer. Some noteworthy tutorials we believe will be the most useful to you include the following:\n- Free Tutorial: Creating Art in Houdini\n- Free Video Tutorial: Cloud Generation in Houdini\n- Watch 300+ hours of Workshops, Master Classes by Some of The Best Artists\n- Blue Painting Process by Taejune Kim\n- Tips for Storytelling in your Art with Atey Ghailan\n- Portrait Painting Process by Wojtek Fus\n- Sculpting a character in VR with Oculus Medium\nCtrl+Paint is a simple resource for digital artists. It’s operated and maintained by Matt Kohr, a freelance artist who graduated from Savannah College of Art and Design and majored in Comic Books. Kohr’s site provides both free and paid art school-style concept design courses and training.\nCartoonSmart is also a huge library of both free and paid content, packed full of illustration resources and tutorials for Adobe Illustrator, Adobe Photoshop, and AdobeAnimate. The majority of the premium courses are taught by Justin Dike, the site’s founder, who is also a developer that teaches game design courses.\nAdobe Photoshop CC Classroom in a Book\nAs its name suggests, the Adobe Photoshop CC Classroom in a Book is around 400 pages long and offers a 15 step-by-step course on Photoshop. Among its lessons, you’ll learn about masking as well a show to work with artboards and brush presets. Because it’s so comprehensive, it’s perfect for the self-taught artist who is learning digital art on their own. In addition, you can also get similar Classroom books for InDesign and Illustrator.\nPaintable was co-founded by David Belliveau, who’s been illustrating since he was a child. Since 2011, however, he went into digital drawings and never looked back. Since then, he’s been sharing his knowledge with like-minded individuals like himself. There are plenty of five-minute-long videos on digital painting and drawing tips on the site.\nBe it sci-fi, 3D, illustrations, fantasy, vector art, or retro, Coolvibe is a site dedicated to showcasing some of the best digital art out there. The team in charge of Coolvibe scours the web on a daily basis, looking for great digital artwork and showcasing it on their site. Coolvibe is a way for digital artists to gain exposure for their work as they link back to the artists and the original sources.\nDigital Art Served\nSinix Design is a YouTube channel that has been in operation since 2006. It provides a wide variety of reviews, tutorials, inspirational videos, and everything else that will help you get into your creative mindset.\nComputer Arts is a leading design magazine that features numerous design tips, tutorials, advice, interviews, and more, all aimed at helping its readers become better digital artists and designers. Some of the many topics covered include Illustrator, InDesign, Photoshop, After Effects, and more.\nIt should come as no surprise that Pinterest has made it onto this list. As a photo-sharing, social media website, Pinterest has images on every topic you can think of. Digital art is among the most popular ones, but you can also narrow your search even further by doing a simple keyword search on whatever you are interested in.\nEven though it’s not solely dedicated to digital art, the digital art section of Deviant Art makes up a considerable chunk of the site. It’s also a popular place to look for inspiration, as it houses a large gallery of images on almost every conceivable subject you can think of. There are also plenty of subcategories to choose from, including 3D art, typography, and many more.\nDribbble is yet another site where artists can showcase their digital art projects. By using the keywords search tool, you can look up different subcategories, or search by the most popular, or the latest uploaded examples. In addition, the platform also acts as a forum where artists can exchange ideas, like, and comment on each other’s work.\nFormerly known as CGHub, ArtStation is also a social media network for entertainment professionals. It has an online community from all around the world who share their tips, tools, and their latest work with each other.\nConcept Art World\nIf you are in need of inspiration, Concept Art World offers a late online gallery brimming with amazing images. The site has separate pages on different topics, including one section dedicated to training, news, and books to read.\nThe CG Society is and has been a prime source of learning and inspiration for digital artists for over a decade. The platform is dedicated to helping others improve their skills, tell their stories, and share their work. The CG Society is often used by art directors who look for new talent to endorse.\nWith the rise of the digital world and advancement of consumer technology over the past couple of decades, the way we create, constantly consume, and even think through the lens of digital media, has completely changed our daily lives. Unsurprisingly, the art world has also been influenced by this transformation, no longer being confined solely to galleries and other institutions in brick and mortar businesses. With digital technology now being at the forefront of almost everything, even the art industry has changed how art is being created, promoted, bought, and sold.\nIn Lisbon, Portugal, for example, urban art is not only legal but endorsed by the local government. Therefore, the way everyday people have started consuming art has completely changed. Moreover, developing and under-privileged communities around the world, in particular, have more access to it like never before. Digital art has the same potential to expose those that may not, otherwise, have access to art. The way digital art and social media work together in this regard should not be underestimated. However, this higher exposure level also created a new series of challenges and opportunities for how art trade will develop in the future. Even if technology is all around us, digital art is still in its infancy and doesn’t have the same level of support from curators and galleries like traditional art. As such, the future of digital art may take a completely different course than what the physical bricks and mortar art world has been in the past.. Only time will tell how things will evolve in the future but in no doubt will","Blockchain, Cryptocurrency, and the Art Market\nWhat about the Artist?\nBlockchain and the Art Market\nBlockchain has great potential to increase transparency in the provenance and pricing of artwork. In 2014, the Fine Arts Expert Institute (Geneva) estimated that over fifty percent of the artworks it examined were either forged or misattributed. Navigating the art market is difficult even for experts due to several factors: 1) the lack of transparency; 2) intricate and old-fashioned insider networks; 3) information asymmetries between buyers and sellers; 4) the speed and volatility of trading; 5) the desire among some major purchasers to hide art assets from taxation; and 6) the emotional and status factors that often distort prices and reputations within the art world.\nBlockchain is also a tool to ensure digital scarcity. Because it is so easy to reproduce images taken from online sites, intellectual property rights and copyright are extremely difficult to regulate in digital art. Yet an artist who chooses to sell digital work on the blockchain can monitor who has bought the work, control access to it (by certifying each image as an original) and be alerted to resales that may have implications for royalties and professional reputation.\nBlockchain (and cryptocurrency) for beginners\nBlockchain is a distributed peer-to-peer electronic ledger. A transaction (“block”) can only be performed by authenticated members of that specific blockchain, who each hold an encrypted key that can unlock and verify transactions. Blockchain enthusiasts emphasize that the system is extremely secure and decentralized and that it reduces friction in the market by cutting out middlemen and their associated fees. As each block is added to the ledger, the system produces what proponents refer to as an indelible and transparent record. According to Wealth Management, “transaction information isn’t stored on server, but rather is embedded in digital code that’s dispersed throughout shared databases and protected from tampering, revision or deletion.” Each previous block, once completed and certified by the network, is unchangeable.\nCryptocurrencies such as BitCoin are to date the most publicized example of a service that can be hosted via the blockchain and they also have extraordinary implications for the buying and selling of art. The emblematic entry-level product is CryptoKitties (https://www.cryptokitties.co/), an online game that lets you purchase, collect, and trade electronic cats. The website promises that each cryptokitty is “one-of-a-kind and 100% owned by you; it cannot be replicated, taken away, or destroyed.” Participating in the game requires a player to open a cryptocurrency account and to become familiar with the notions of electronic ownership and brokerage.\nDigital technologies and the art market\nBoth blockchain and cryptocurrency dovetail seamlessly with the ongoing financialization of every corner of the art market. Evan Beard, National Art Services Executive at US Trust (a wealth management unit of Bank of America), notes that “much of today’s most dynamic wealth creation comes from hedge funds, private equity and real estate. None of our clients are buying art for investment. But they’re savvy with credit, and art is a capital asset.” A few of Beard’s top-end clients play the art market by guaranteeing works at the major auction houses. These savvy investors assume the risk of either ending up with the artwork should it not meet its reserve price or taking a percentage of the overage if the auction nets a higher than estimated price for the work.\nIn June 2018, the company Maecenas put the weight of the art establishment behind an experiment with fractionalized digital ownership in what it billed as the world’s first blockchain art sale. In cooperation with Dadiani Fine Art, Maecenas set up a digital auction of forty-nine percent of Andy Warhol’s “14 Small Electric Chairs Reversal Series” (estimated value of $5.6 million USD in 2019). Not only did Maecenas accept payment in cryptocurrencies such as Bitcoin and Ethereum (as well as its own cryptocurrency, ART), it allowed buyers to purchase digital certificates representing fractional ownership of the piece. Just as binary code can reduce the complexity of information to an efficient and universal system of 0s and 1s, blockchain and cryptocurrency enable the virtual slicing of artworks such that owning ten, five, or even one percent of a painting is not only imaginable, but feasible.\nDemocratizing or financializing art?\nThe large-scale uptake of fractional ownership remains uncertain, but the idea makes perfect sense if the ultimate goal to reduce all barriers to the seamless flow of capital, labor, and digitalized intellectual property. Imagine owning, say, 4.62 percent of an Andy Warhol Marilyn or Elvis, or another of those mid-1960s masterpieces that now routinely command millions of dollars at auction. You could show your digital certificate to neighbors and friends, and maybe even display a framed print-out of your digital square of the piece.\nFor some of us, this is the long-sought democratization of the art market, as fractional ownership enables people outside the tiny billionaire art establishment to own at least a part of iconic masterpieces. For others, the notion that anyone could feel meaningfully attached to a fractional piece of art is absurd. Instead, what is really happening –underneath the rhetoric of “democratizing art” — is entry into the art market, and for this access one must, of course, pay. It is also a means to bring new capital from previously excluded clients into the mix and to spread the risk around a much larger cohort of players. These are propositions altogether different from ensuring that diverse people from different economic and social classes have an equitable opportunity to experience and live with high-end art in their daily lives.\nOne can also imagine a worst-case scenario akin to the Great Recession of 2008, which began with the fractionalization and re-packaging of mortgage-backed securities. The scale of such a crisis would likely never match the housing crash. However, the financial incentive to create ever more esoteric, high-risk but high-fee fractionalized art securities could ironically undermine the chain of provenance, ownership, and trust. Just as, in the end, almost no one knew who owned what mortgage liability at what interest rate, one can envision thousands of fractionalized pieces of an Andy Warhol floating in the ether like those silver balloons he released in 1966. For this and multiple other reasons, including equity and access, good old-fashioned museums and galleries remain essential in our post-materialist era.\n Botz, Anneli, 2018. Is Blockchain the Future of Art? Four Experts Weigh In. Art Basel. Available at: https://www.artbasel.com/news/blockchain-artworld-cryptocurrency-cryptokitties\n Rottermund, Amanda, 2019. The Newest Technological Trend in the Art Market. Wealth Management, 10 April.\n Reyburn, Scott, 2018. Art is Becoming a Financial Product, and Blockchain is Making It Happen.” The New York Times, 8 June. Available at: https://www.nytimes.com/2018/06/08/arts/art-financialization-blockchain.html"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:5b03a867-ecef-40ba-8eca-86d3ba51af13>","<urn:uuid:edc5b33e-a3de-425c-9167-4d8ebde1d634>"],"error":null}
{"question":"What production costs should I consider when manufacturing hats and hoodies? Looking for a comprehensive list.","answer":"The main production costs for manufacturing hats and hoodies include: 1) Labor costs - wages for factory workers and salaries for management, 2) Raw materials costs and their delivery - including textile products and items like strings, 3) Land costs, 4) Technology expenses, 5) Capital investments. When production costs are high, suppliers must set higher prices for final products to recover costs and maximize shareholder income.","context":["Table of Contents\nMicroeconomics is the study of the behavior of firms and individual consumers and their decisions when considering limited resources. This idea applies to a market situation where firms are selling goods while consumers are buying these goods. As the firms try to maximize their profits, the consumers aim at maximizing their utility. Therefore, an equilibrium must be developed at the point where the firms as well as consumers maximize their utility. The study of microeconomics assesses how the consumers’ and producers’ decisions affect the demand and supply of a good/service. This paper discusses the basic microeconomic concepts with the help of hats and hoodies and determines how they help with the attainment of microeconomic goals.\nDemand and Supply of Hats and Hoodies\nHats and hoodies are basic clothing. The difference comes in when the hat or hoody is branded or secondhand. A branded hat or hoody is considered a normal good, while a secondhand one is an inferior good (Wells & Krugman, 2014). The demand for branded clothes increases as the income of a consumer expands, assuming that the price is constant. When the income reduces, the consumer will demand less branded clothes. On the other hand, secondhand clothes are considered inferior good, because as consumers’ income increases, they tend to demand less secondhand clothes, meaning they now choose brands. When the income reduces, they tend to demand more secondhand clothes and less brands (Pindyck & Rubinfield, 2015).\nThe suppliers of hats and hoodies have decisions to make when it comes to selling their goods. The suppliers of branded hats and hoodies will sell more of these offerings in the areas or neighborhoods where the occupants have relatively high incomes. These people can afford expensive clothes since their disposable income is high. Supplying secondhand clothes in such localities may not be profitable, since very few people will purchase them (Wells & Krugman, 2014). On the other hand, a supplier of secondhand hats and hoodies should offer these goods in areas/localities/neighborhoods where the citizens have relatively low incomes. These people will definitely choose secondhand clothes due to the lower price as compared to the more expensive branded offerings. Therefore, the supplier has to particular decisions with these factors in mind when seeking to maximize profits (Pindyck & Rubinfield, 2015).\nEquilibrium of Demand and Supply of Hats and Hoodies\nAn equilibrium occurs when the demand and supply are equal. On the graph, this is the point where the demand and supply curves intersect. Ideally, the equilibrium price facilitates the achievement of the microeconomic goal of efficiency. It happens because at this point, both the supplier and the consumer attain maximum satisfaction from the available resources on either side, and, hence, the goal of stability is achieved (Pindyck & Rubinfield, 2015).\nElasticities of Demand and Supply of Hats and Hoodies\nThe elasticity of demand for hats and hoodies depends on whether the item is a normal good or an inferior good. For new branded hats and hoodies (normal goods), the income elasticity of demand is positive, because as the income of the consumer increases, more branded hats and hoodies are demanded. When the income reduces, the demand for branded clothes also falls down (Wells & Krugman, 2014). On the other hand, for secondhand hats and hoodies (inferior goods), the income elasticity of demand is negative. As the consumers’ income increases, they demand less secondhand clothes, while with reducing income, they demand more secondhand clothes. All these processes happen if the price is held constant. Focusing on the price elasticity of demand, income is held constant. For new branded hats and hoodies (normal goods), the price elasticity of demand is negative. More specifically, as the price of the hat/hoody increases, consumers demands less of the hats/hoodies, all other factors held constant. However, when the price for normal good falls, the consumer will demand more new clothes (Wells & Krugman, 2014).\nBenefit from Our Service: Save 25% Along with the first order offer - 15% discount, you save extra 10% since we provide 300 words/page instead of 275 words/page\nCost of Production\nThe production of hats and hoodies has a lot of economic aspects attached to it. First, there is the cost of the factors of production. One of these factors is labor. There are people employed in the factories, who produce these clothes. These employees need to be paid wages and their management must receive salaries. This is a major cost incurred in the production process. There is also the cost of raw materials and their delivery. These hats and hoodies require materials such as strings and other textile products for them to be produced. Therefore, it is important to include the cost of these raw materials when calculating the cost of production of hats and hoodies (Pindyck & Rubinfield, 2015). Moreover, there are such costs as land, technology and capital used in the production process. All these inputs are summed up to arrive at the total cost of producing outfits for sale. When the cost of production is high, suppliers and producers have to set high prices for the final products so that they can recover the cost of production and maximize income to shareholders (Pindyck & Rubinfield, 2015).\nVIP Services: only fascinating benefits\nDon’t miss your chance to order all VIP Services with 20% discount\nThe market structure for the sale of hats and hoodies may differ from place to place. In some regions, there are perfectly competitive markets, which means that sellers offer homogenous hats and hoodies. In such markets, the prices are relatively similar, because if one seller decides to increase price for his/her hats/hoodies, then consumers will have an option of buying from the other sellers who offer lower prices. In addition, if one seller decides to reduce the price for his/her hats and hoodies, then consumers will flock to his/her shop and hence the sales of this producer will increase. As a result, a change in price causes a more than proportionate change in the number of hats and hoodies demanded by consumers (Wells & Krugman, 2014).\nIf the market is monopolistic and has a single seller, the situation is different. The monopolist has ultimate control over the price of the hats and hoodies he/she is selling, because there are no competitors. In such circumstances, the price of a hat or hoody will be relatively higher than that in the perfectly competitive market. The seller will be a price maker, and he/she will have control over the market (Pindyck & Rubinfield, 2015).\nBook The Best TOP Expert at our service\nYour order will be assigned to the most experienced writer in the relevant discipline. The highly demanded expert, one of our top-30 writers with the highest rate among the customers.Order only for $10.95\nUnder monopolistic competition, there will be many firms selling hats and hoodies, but these items will be slightly differentiated. It means that there will be no single firm with complete control over the market. On the other hand, in an oligopoly, there will be a relatively smaller number of firms, but these firms will collectively control the majority of the market of hats and hoodies. Each firm will have to resort to rigorous advertising techniques in order to ensure that it makes sales, since the items will be only slightly differentiated (Pindyck & Rubinfield, 2015).\nIn a nutshell, the above discussion demonstrates that there are numerous microeconomic concepts, which apply to the purchase and sale of any good. With the help of a concrete example of the trade of hats and hoodies, it is evident that each market has different functionality depending on its structure. Moreover, price and income as well as demand and supply of these goods also compose the microeconomic concepts, which serve to ensure equal and efficient sale and trade in market with the hats and hoodies."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c4917bb7-4217-49e5-9bca-82ae54feee37>"],"error":null}
{"question":"How many African countries gained independence in 1960, and what was Nigeria's role in this historic year?","answer":"1960 was known as the 'Year of Africa' with 17 African countries declaring independence. Nigeria, as one of these nations, gained its independence on October 1, 1960, becoming the final country to declare independence that year. Nigeria's independence marked a significant milestone, with the country's first Prime Minister Abu baker Tafawa Balewa delivering a speech emphasizing the peaceful transfer of power and the country's readiness to play an active part in maintaining world peace and preserving civilization.","context":["Nigeria, one of the biggest countries in the world and the most populous nation in Africa is endowed with tremendous physical and intellectual wealth. Nevertheless, the country continues to feel the pangs of poverty and underdevelopment. This sis often attributed to political exclusion, unequal access to resources and ethnicity. Abu baker Tafawa Balewa was the first Prime Minister of\n“Today is Independence Day. October 1, 1960 is a date to which for two years every Nigerian has been eagerly looking forward. At last our great day has arrived and\n“Words cannot adequately express my joy and pride at gaining the Nigerian citizen privilege to accept, from Her Royal Highness, these Constitution Instruments which are the symbols of\n“But now we have acquired our rightful status and I feel sure that history will show that the building of our nation proceeded at the wisest pace: it has been thorough and\n“Today’s ceremony marks the culmination of a process, which began fifteen years ago and has now reached a happy and a successful conclusion. It is with justifiable pride that we claim the achievement of our\n“Each step of our constitutional advance has been purposefully and peacefully planned with full and open consultation, not only between representatives of all the various interests in Nigeria, but also in harmonious cooperation with the administering power which has to say relinquished it’s authority.\n“At the time our constitutional development entered upon its final phase, the emphasis was largely upon self-government. We, the elected representatives of the people of\n“However, we were not to be allowed the selfish luxury of focusing our interest on our own homes. In these days of rapid communications we cannot live in isolation apart from the rest of the world even if we wished to do so. All too soon it has become evident that for us, independence implies great deal more than self government. This great country, which has now emerged without bitterness or bloodshed, finds that she must at once be ready to deal with grave international issues.\n“This fact has of recent months been unhappily emphasized by the startling events, which have occurred in this continent. I shall not labour the points, but it would be unrealistic not to draw attention first to the awe-inspiring task confronting us at the very start of our nationhood. When this day, 1 October 1960, was chosen as Independence Day, it seemed that we were destined to move with quiet dignity to our place on the world state. Recent events have changed since beyond recognition so that we find ourselves today being tested to the utmost. We are called upon immediately to show that our claims to responsible government are well-founded and having been accepted as an independent state we must at once play an active part in maintaining the peace of the world and in preserving civilization. I promise you we shall not fail for want of determination.\n“And we come to this task better equipped than many. For this I pay tribute to the manner in which successive British governments have gradually transferred the burden of responsibility to our shoulders. The assistance and unfailing encouragement, which we have received from each secretary of state for the colonies and their intense personal interest in our development, has immediately lightened that burden.\n“All our friends in Colonial Office must today be proud of their handiwork and in the knowledge that they have helped to lay the foundations of a lasting friendship between our two nations. I have indeed every confidence that based on the happy experience of a successful partnership; our future relations with the\n“Time will not permit the individual mention of all those friends many of them Nigerians, whose selfless labours have contributed to our independence, some have not lived to see the fulfillment of their hopes-on them be peace but nevertheless they are remembered here, and the names of buildings and streets and roads and bridges throughout the country recall to our minds their achievements, some of them on a national scale, others confined perhaps to a small area in one Division, more humble but of equal value in the total.\n“Today we have with us representatives of those who made\n“We wish that it could have been possible for all of those whom you represent to be here today. Many, I know, will be disappointed to be absent, but if they were listening to me now I say to them: ‘Thank you on behalf of my countrymen. Thank you for your devoted service, which helped to build up\n“Today, we are indeed proud to have achieved independence, and proud that our efforts should have contributed to this happy event. But do not mistake our pride for arrogance. It is tempered by a feeling of sincere gratitude to all that have started in the task of developing\n“And finally, I must express our gratitude to Her Royal Highness the Princess Alexandra of\n“And so, with the words ‘God Save Our Queen’ I open a new chapter in the history of\nQuestion of the week: “All too soon it has become evident that for us, independence implies great deal more than self government.” What in your opinion is the meaning of independence?","Many Africa countries that had been colonised by European countries got independence in the fifties and sixties.\nWho were the founding presidents by country?\nBy 1963 the emerging Africa leaders were inspired enough by the new gained freedom to start talking Pan-Africanism. The states of Africa sought through a political collective a means of preserving and consolidating their independence and pursuing the ideals of African unity. Unfortunately, two rival camps emerged with opposing views about how these goals could best be achieved. The Casablanca Group, led by President Kwame Nkrumah (1909–1972) of Ghana, backed radical calls for political integration and the creation of a supranational body. The moderate Monrovia Group, led by Emperor Haile Selassie (1892–1975) of Ethiopia, advocated a loose association of sovereign states that allowed for political cooperation at the intergovernmental level. The latter view prevailed. The OAU was therefore based on the “sovereign equality of all Member States,” as stated in its charter.\n|Country||Independence Date||Prior ruling country|\n|Liberia, Republic of||July 26, 1847||–|\n|South Africa, Republic of||May 31, 1910||Britain|\n|Egypt, Arab Republic of||Feb. 28, 1922||Britain|\n|Ethiopia, People’s Democratic Republic of||May 5, 1941||Italy|\n|Libya (Socialist People’s Libyan Arab Jamahiriya)||Dec. 24, 1951||Britain|\n|Sudan, Democratic Republic of||Jan. 1, 1956||Britain/Egypt|\n|Morocco, Kingdom of||March 2, 1956||France|\n|Tunisia, Republic of||March 20, 1956||France|\n|Morocco (Spanish Northern Zone, Marruecos)||April 7, 1956||Spain|\n|Morocco (International Zone, Tangiers)||Oct. 29, 1956||–|\n|Ghana, Republic of||March 6, 1957||Britain|\n|Morocco (Spanish Southern Zone, Marruecos)||April 27, 1958||Spain|\n|Guinea, Republic of||Oct. 2, 1958||France|\n|Cameroon, Republic of||Jan. 1 1960||France|\n|Senegal, Republic of||April 4, 1960||France|\n|Togo, Republic of||April 27, 1960||France|\n|Mali, Republic of||Sept. 22, 1960||France|\n|Madagascar, Democratic Republic of||June 26, 1960||France|\n|Congo (Kinshasa), Democratic Republic of the||June 30, 1960||Belgium|\n|Somalia, Democratic Republic of||July 1, 1960||Britain|\n|Benin, Republic of||Aug. 1, 1960||France|\n|Niger, Republic of||Aug. 3, 1960||France|\n|Burkina Faso, Popular Democratic Republic of||Aug. 5, 1960||France|\n|Côte d’Ivoire, Republic of (Ivory Coast)||Aug. 7, 1960||France|\n|Chad, Republic of||Aug. 11, 1960||France|\n|Central African Republic||Aug. 13, 1960||France|\n|Congo (Brazzaville), Republic of the||Aug. 15, 1960||France|\n|Gabon, Republic of||Aug. 16, 1960||France|\n|Nigeria, Federal Republic of||Oct. 1, 1960||Britain|\n|Mauritania, Islamic Republic of||Nov. 28, 1960||France|\n|Sierra Leone, Republic of||Apr. 27, 1961||Britain|\n|Nigeria (British Cameroon North)||June 1, 1961||Britain|\n|Cameroon(British Cameroon South)||Oct. 1, 1961||Britain|\n|Tanzania, United Republic of||Dec. 9, 1961||Britain|\n|Burundi, Republic of||July 1, 1962||Belgium|\n|Rwanda, Republic of||July 1, 1962||Belgium|\n|Algeria, Democratic and Popular Republic of||July 3, 1962||France|\n|Uganda, Republic of||Oct. 9, 1962||Britain|\n|Kenya, Republic of||Dec. 12, 1963||Britain|\n|Malawi, Republic of||July 6, 1964||Britain|\n|Zambia, Republic of||Oct. 24, 1964||Britain|\n|Gambia, Republic of The||Feb. 18, 1965||Britain|\n|Botswana, Republic of||Sept. 30, 1966||Britain|\n|Lesotho, Kingdom of||Oct. 4, 1966||Britain|\n|Mauritius, State of||March 12, 1968||Britain|\n|Swaziland, Kingdom of||Sept. 6, 1968||Britain|\n|Equatorial Guinea, Republic of||Oct. 12, 1968||Spain|\n|Morocco (Ifni)||June 30, 1969||Spain|\n|Guinea-Bissau, Republic of||Sept. 24, 1973 (alt. Sept. 10, 1974)||Portugal|\n|Mozambique, Republic of||June 25. 1975||Portugal|\n|Cape Verde, Republic of||July 5, 1975||Portugal|\n|Comoros, Federal Islamic Republic of the||July 6, 1975||France|\n|São Tomé and Principe, Democratic Republic of||July 12, 1975||Portugal|\n|Angola, People’s Republic of||Nov. 11, 1975||Portugal|\n|Western Sahara||Feb. 28, 1976||Spain|\n|Seychelles, Republic of||June 29, 1976||Britain|\n|Djibouti, Republic of||June 27, 1977||France|\n|Zimbabwe, Republic of||April 18, 1980||Britain|\n|Namibia, Republic of||March 21, 1990||South Africa|\n|Eritrea, State of||May 24, 1993||Ethiopia|\n|South Sudan, Republic of||July 9, 2011||Republic of the Sudan|\n- Ethiopia is usually considered to have never been colonized, but following the invasion by Italy in 1935-36 Italian settlers arrived. Emperor Haile Selassie was deposed and went into exile in the UK. He regained his throne on 5 May 1941 when he re-entered Addis Ababa with his troops. Italian resistance was not completely overcome until 27th November 1941.\n- Guinea-Bissau made a Unilateral Declaration of Independence on Sept. 24, 1973, now considered as Independence Day. However, independence was only recognized by Portugal on 10 September 1974 as a result of the Algiers Accord of Aug. 26, 1974.\n- Western Sahara was immediately seized by Morocco, a move contested by Polisario (Popular Front for the Liberation of the Saguia el Hamra and Rio del Oro).\nAfrican Countries and their Independence Days.\nAD, European countries scrambled for and partitioned Africa. This continued until around 1905, by which time all the lands and resources of the continent of Africa had been completely divided and colonized by European countries. The only country that couldn’t be colonized due to strong resistance by the indegines was Ethiopia, and Liberia which was a place for freed slaves from the Americas.In the 17th century\nThe struggle for independence started after world war II. This led to the independence of the Union of South Africa in 1931 through negatiations with the British empire and Libya in 1951 from Italy; followed by others in the late 1950s. The road to African independence was very hard and tortuous often through bloody fights, revolts and assasinations. For example; Britain unilatearlly granted “The Kingdom of Egypt” independence on Feb. 22nd 1922 after a series of revolts, but continued to interfere in government. More violent revolts led to the signing of the Anglo-Egyptian treaty in 1936 and a coupe detat tagged Egyptian Revolution in 1952 finally culminated in the Egyptian Republic declaration of June 18th; 1953. The peak year for independence came in 1960 when about 17 countries gained independence. These independence days are now celebrated as national day holidays in most countries of Africa.\n|COUNTRY||INDEPENDENCE DAY||COLONIAL NAME||COLONIAL RULERS|\n|Algeria||July 5th, 1962||France|\n|Angola||November 11th; 1975||Portugal|\n|Benin||August 1st; 1960||French|\n|Botswana||September 30th, 1966||Britain|\n|Burkina Faso||August 5; 1960||France|\n|Burundi||July 1st; 1962||Belgium|\n|Cameroon||January 1st; 1960||French-administered UN trusteeship|\n|Cape Verde||July 5th; 1975||Portugal|\n|C.A.R||August 13th; 1960||France|\n|Chad||August 11th, 1960||France|\n|Comoros||July 6th; 1975||France|\n|Congo||August 15th; 1960||France|\n|Congo DR||June 30th; 1960||Belgium|\n|Cote d’Ivoire||August 7th; 1960||France|\n|Djibouti||June 27th; 1977||France|\n|Egypt||February 28th, 1922||Britain|\n|Eq Guinea||October 12; 1968||Spain|\n|Eritrea||May 24th; 1993||Ethiopia|\n|Ethiopia||over 2000 years,\nKingdom of Aksum\n|Gabon||August 17th; 1960||France|\n|Gambia||February 18th; 1965||Britain|\n|Ghana||6 March 1957||Gold Coast||Britain|\n|Guinea||October 2nd; 1958||France|\n|Guinea Bissau||10 September 1974\n24 September 1973\n|Kenya||December 12th, 1963||Britain|\n|Lesotho||October 4th; 1966||Britain|\n|Liberia||July 26th; 1847||American colonization Society|\n|Libya||December 24; 1951||Italy|\n|Madagascar||June 26th; 1960||France|\n|Malawi||July 6th; 1964||Britain|\n|Mali||September 22nd; 1960||France|\n|Mauritania||November 28th; 1960||France|\n|Mauritius||March 12th, 1968||Britain|\n|Morocco||March 2nd; 1956||France|\n|Mozambique||June 25th; 1975||Portugal|\n|Namibia||March 21st; 1990||South African mandate|\n|Niger||August 3rd; 1960||France|\n|Nigeria||October 1st, 1960||Britain|\n|Rwanda||July 1st; 1962||Belgium administered UN trusteeship|\n|SaoTomePrincipe||July 12th; 1975||Portugal|\n|Senegal||April 4th; 1960||France|\n|Seychelles||June 29th; 1976||Britain|\n|Sierra Leone||April 27th; 1961||Britain|\n|Somalia||July 1st; 1960||British Somaliland\n|South Africa||11 December 1931,\nApril 1994(end of apatheid)\n|Union of South Africa||Britain|\n|Sudan||January 1st; 1956||Egypt, Britain|\n|Swaziland||September 6th; 1968||Britain|\n|Tanzania||April 26th, 1964||Britain|\n|Togo||April 27th; 1960||French administered UN trusteeship|\n|Tunisia||March 20th; 1956||France|\n|Uganda||October 9th; 1962||Britain|\n|Zambia||October 24th; 1964||Britain|\n|Zimbabwe||April 18th; 1980||Britain|\n1960 in Africa\nKnown as the Year of Africa, 1960 saw 17 African countries declare independence among other events.\n- Mau Mau Uprising is officially over in Kenya.\n- 9–11 January – Aswan High Dam construction begins in Egypt.\n- 21 January – A mine collapses at Coalbrook, South Africa, killing 437.\n- 24 January – A major insurrection occurs in Algiers against French colonial policy.\n- 3 February – Harold Macmillan‘s Wind of Change speech is made in Cape Town, South Africa. It signalled the end of the British Empire.\n- 10 February – A conference about the independence of the Belgian Congo begins in Brussels.\n- 29 February–1 March – The 5.7 Mw Agadir earthquake shakes coastal Morocco with a maximum perceived intensity of X (Extreme), destroying Agadir, and leaving 12,000 dead and another 12,000 injured.\n- 21 March – The Sharpeville massacre in South Africa kills more than 69 people, wounds 300.\n- 16 April – Gunman David Pratt attacks South African Prime Minister Henrik Verwoerd in Johannesburg, wounding him seriously.\n- 27 April – Togo declares independence with Sylvanus Olympio.\n- 14 May – The Kenya African Democratic Union Party is founded in Kenya, when 3 political parties join forces.\n- 20 June – The Mali Federation declares independence with Modibo Keita as leader and Dakar as capital.\n- 26 June – Madagascar declares independence with Philibert Tsiranana as President.\n- 26 June – Somaliland declares independence with Mohamed Haji Ibrahim Egal as President and Hargeisa as capital.\n- 30 June – Republic of the Congo (Léopoldville) (now Democratic Republic of the Congo) declares independence with Joseph Kasa-Vubu as President.\n- 1 July – Somalia declares independence, with Aden Abdullah Osman Daar as President.\n- 1 July – Ghana becomes a Republic and Kwame Nkrumah becomes its first President as Elizabeth II of the United Kingdom ceases to be the Head of state.\n- 11 July- Moise Tshombe declares the Congolese province of Katanga independent; he receives Belgian help.\n- 1 August – Dahomey (now Benin) declares independence, with Hubert Maga as President.\n- 3 August – Niger declares independence, with Hamani Diori as President.\n- 5 August – Upper Volta (now Burkina Faso) declares independence, with Maurice Yaméogo as President.\n- 6 August – In the Republic of the Congo (Leopoldville), Albert Kalonji declares the independence of the Autonomous State of South Kasai.\n- 7 August – Côte d’Ivoire declares independence, with Félix Houphouët-Boigny as President.\n- 11 August – Chad declares independence, with François Tombalbaye as President.\n- 13 August – Central African Republic declares independence, with David Dacko as President.\n- 15 August – Republic of the Congo (Brazzaville) declares independence, with Fulbert Youlou as President.\n- 17 August – Gabon declares independence, with Léon M’ba as President.\n- 20 August – Senegal leaves the Mali Federation, creating Senegal.\n- 5 September – Congo president Joseph Kasavubu fires Patrice Lumumba‘s government and places him under house arrest.\n- 14 September – Colonel Joseph Mobutu takes power in Congo (Leopoldville) in a military coup.\n- 20 September – Dahomey, Upper Volta, Cameroon, Central African Republic, Chad, Republic of the Congo (Leopoldville), Republic of the Congo (Brazzaville), Côte d’Ivoire, Gabon, Madagascar, Niger, Somalia, Togo, Mali and Senegal obtain membership in the United Nations.\n- 22 September – Mali declares independence from the Mali federation.\n- 1 October – Nigeria declares independence, with Nnamdi Azikiwe as President.\n- 5 October – White South Africans vote to make the country a republic.\n- 7 October – Nigeria obtains membership in the United Nations.\n- 9 December – French President Charles de Gaulle’s visit to Algeria is marked by bloody riots by European and Muslim mobs in Algeria’s largest cities, killing 127 people.\n- 13 December – While Emperor Haile Selassie I of Ethiopia visits Brazil, his Imperial Bodyguard revolts unsuccessfully against his rule. The rebels proclaim the emperor’s son, Crown Prince Asfa Wossen, as Emperor.\n- 14 December – Antoine Gizenga proclaims in Stanleyville, Congo, that he has assumed the premiership.\n- 17 December – Troops loyal to Haile Selassie I in Ethiopia suppress the revolt that began 13 December, giving power back to their leader upon his return from Brazil. Haile Selassie absolves his son of any guilt.\nContinental population in 1960\n- Africa: 277,398,000"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2231d03c-0f57-4301-8435-e6829b936980>","<urn:uuid:c6356a0a-1d33-486c-80d1-c26bc38e522b>"],"error":null}
{"question":"As a quality control specialist, I need to understand both the surface quality improvements from modern shielding gases and the associated workplace safety standards. What can you tell me about these?","answer":"Modern shielding gas mixtures like Ferroline C12 X2 improve surface quality by minimizing spatter and creating smooth, finely rippled weld surfaces. This is particularly important for water-soluble paints, which have higher surface tension and struggle with uneven surfaces, potentially leading to damaged paintwork and premature corrosion. Regarding safety standards, there are strict workplace exposure limits (WELs) for welding fumes, particularly for manganese. While the current UK limit for manganese is 0.5mg/m3 (8hr TWA), the EU is proposing a reduction to 0.2mg/m3, with an additional specific standard of 0.05mg/m3 for respirable particles. These limits require implementation of control measures like local exhaust ventilation or respiratory protection with A2P3 filters.","context":["Messer: Cost calculator shows savings potential\nThe quality of welds is considerably affected by the choice of shielding gas mixture. Significantly improved results are achieved with the new ternary mixtures. The Messer cost calculator shows how much money customers can save.\nTime is money – this simple equation, in the end, also explains the cost reduction that is possible with the use of ternary mixtures. The analysis that Messer offers its customers is considerably more detailed though. The cost calculator allows a standardised total cost calculation that takes account of all the important welding parameters. The shielding gas used is a comparatively small factor in this regard, accounting for only about five per cent of the total welding costs. However, the gas mixture does have a major bearing on the quality of the result. The requirements it has to meet differ depending on the material and the welding process.\nWelding vs. rework\nPlain steel, for instance, is a standard material in plant construction and the manufacture of vehicles and machinery. The main priority for welding in these areas is to avoid weld spatter and slag islands at the welds as these can cause problems later on: if the surface is too uneven, water-soluble paints reach their limits. Whilst they are environmentally friendly and the most widely used paints, they are less effective at coating uneven surfaces due to their higher surface tension. This leads to damaged areas in the paintwork and, in the worst-case scenario, causes premature corrosion.\nTo prevent such damage, Messer has developed the shielding gas Ferroline C12 X2. This gas mixture combines argon with carbon dioxide and oxygen in order to minimise spatter and create a smooth, finely rippled weld surface. Etienne Besnard is Managing Director of Batista-MGPV, a Normandy-based manufacturer of baking trays for industrial bakery products. He has tested the shielding gas and is convinced of its benefits: “We have been able to improve the quality of our processes and products with Ferroline C12 X2. The welds are much smoother as a result, and the extent of weld spatter is significantly reduced. The amount of reworking required has been cut by a third. In addition, we have been able to increase the welding speed by up to 15 per cent.”\nSpeed and heat input\nIn order to find the right gas mixture for every requirement, all the important welding-relevant data are included in the calculation carried out with the Messer cost calculator. This ranges from the operating period, the voltage, the wire feed, the type of gas used and its flow rate, to the cost of energy, labour and materials. The reworking time is a particularly important variable. In this regard, use of the right shielding gas often creates great savings potential. If the customer does not have all the data to hand, a Messer employee can obtain it on the spot. On the basis of this data, the customer is given a recommendation as to which gas is best suited to their application. And they also get a precise calculation showing what kind of effect the selection of this cylinder gas will have on their manufacturing costs.","Breathe Freely in Manufacturing\nWelding processes generate emissions of very fine particles known as welding fume.\nThe fume is made up of lots of different kinds of substances, most of which are hazardous to your health. One common component of the fume is manganese, which has been shown to cause adverse effects to the nervous system. The arc welding process also generates gases such as carbon monoxide and ozone. Different processes generate differing amounts of fume, e.g. tungsten inert gas (TIG) welding produces less fume than manual metal arc (MMA) welding. Welders can be exposed to the fume from the welding operation unless suitable controls are put in place, such as local exhaust ventilation (LEV) or respiratory protection.\nThe material being welded and the welding rods contain many substances, one of them is manganese (Mn). There is growing concern regarding the impact of this metal on human health and the current workplace exposure limit is likely to be revised in the near future. Many steels contain levels of manganese of up to 2.5% and special grades can contain up to 12.5%. Welding rods contain as much as 12.5% manganese as this helps the flux flow freely.\nSome stainless steels replace nickel with manganese.\nManganese is used in welding rods because it helps the flux flow easily. This is due to the low boiling point of manganese compared to iron (1962°C compared to 2750°C). Because of this, it is likely that there will be more manganese in welding fume than expected from the composition of the rod or the steel (as described in the safety data sheet for the welding rods). In addition, the manganese is more likely to be respirable – that is its very small particles can penetrate to the deep sections of the lung.\nOf course manganese is not the only hazard associated with welding.\nAssessments under the Control of Substances Hazardous to Health (COSHH) regulations should identify hazardous substances inherent or generated by the welding processes, considering the metal, any coatings or contamination and welding rods, including:\nSuitable control measures should be put in place to control all of the hazards identified as part of the risk assessment process\nManganese dioxide, as produced in welding fume, is hazardous and has been classified according to EU and UK legislation with the following criteria:\nSome studies have suggested that exposure to even relatively low levels of manganese can cause harm to the nervous system and may result in effects including impacts on IQ, verbal learning, concentration, memory, motor skills and mood.\nRepeated high exposures to managese can also cause a condition known as “manganism” with symptoms similar to Parkinson’s disease.\nTo understand the type of control measure to use, you need to know at what level any exposure is acceptable. Workplace exposure limits (WELs) are the levels of airborne contamination that a worker can be exposed to. In most cases, WELs are set at levels which should not result in adverse effects. The limits are usually set as averages over an 8 hour period. These are therefore known as 8 hour time weighted averages (8hr TWAs).\nThe current WEL for Manganese in the UK is 0.5mg/m3 (8hr TWA). However, the European Union is proposing that the exposure limit should be reduced to 0.2mg/m3 (8hr TWA) and there will be a new additional specific standard for those small particles that reach the deep lung (known as respirable particles) of 0.05mg/m3 (8hr TWA).\nThis change is significant as much of the manganese in the fume will be respirable. It is likely that the proposed respirable limit will be exceeded during many welding activities unless effective controls are introduced and used properly.\nWelding fume exposure can be minimised by the use of local exhaust ventilation (LEV), including on-tool extraction. The Welding institute and others have demonstrated that effective control using LEV does not have any impact on the quality of the weld and protects workers from hazardous fumes.\nThe significantly reduced WEL related to manganese may drive welding operations producing visible fume to use supplementary respiratory protection with an A2P3 filter, where it is not practicable to install effective LEV.\nPowered air purifying respirators, combined with welding face shields, are the preferred option as they provide protection from hazardous fumes and gases, as well as the ultra violet radiation from the arc.\nThey are also usually more comfortable to wear than tight fitting masks.\n5/6 Melbourne Business Court, Millennium Way, Pride Park,\nDerby, DE24 8LZ\n© British Occupational Hygiene Society"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f68129ce-caf9-49de-a879-71790fea6952>","<urn:uuid:86832989-a22b-450d-9cd5-00d618d86796>"],"error":null}
{"question":"What's the connection between traditional blues singing and diabetes complications, and how can inflammation affect both musical expression and health outcomes?","answer":"Traditional blues singing, as exemplified by Vera Hall's performances like 'Trouble So Hard,' expresses deep emotional pain and suffering through minimal, powerful words. Similarly, inflammation plays a crucial role in both expressing and causing suffering in diabetes complications. Research has shown that inflammation, rather than high blood sugar alone, is necessary for diabetes complications to develop in areas like eyes, kidneys, and feet. Scientists discovered that damaged mitochondria and fatty tissue drive this persistent inflammation, which must be present for glucose to enter and damage cells. This biological parallel shows how both artistic expression and physical illness manifest through inflammatory processes - one emotional, one physiological.","context":["On a cool November evening, the crowd of regulars filters in at the Down Home -- Johnson City, Tennessee’s beloved listening room and bar. The performer waiting to the side of the stage is no stranger to this crowd, cycling between tuning her guitar and greeting friends as they make their way to their seats. When Amythyst Kiah takes the stage and the warm applause settles, she lays down a thumping bass line with her acoustic guitar. Soon, a few bright treble notes layer in, building up a minor chord that completes the gritty and skillful backdrop. Kiah begins to sing with a relaxed sense of ease and a steely intention, and the listeners lean in. “Ooh, Lordy, my trouble so hard / Don’t nobody know my trouble but God.” Though few audience members would know the song’s origin, the emotion moving in it is familiar and immediate.\nTrouble was a familiar subject for Adele “Vera” Hall, a singer who learned African-American spirituals and blues in her family and community in rural Alabama. When Hall sang on record for folklorists John and Ruby Lomax in 1939, she had already endured the death of her husband -- a coal miner who died in a gunfight more than a decade earlier. Hall made her own way, earning a living as a cook and washerwoman, and since her childhood days, she was known to be one of the finest singers in the area. When Hall sang “Trouble So Hard,” perhaps she knew that future generations of singers like Amythyst Kiah would put the song to good use, just as Hall had throughout her life. Of the hundreds of singers John Lomax documented for the Library of Congress, he remarked that Vera Hall had the \"loveliest untrained voice [he] had ever recorded.\"\nPraise from folklorists like Lomax is not what makes Hall’s singing so valuable. For those hearing Kiah perform, the testament is in the air and among them, a strong voice reaching back through generations to present a song that folks can still relate to.\nKiah is an important and innovative presence in contemporary traditional music. Describing herself as a “Southern Gothic, alt-country blues singer/songwriter,” Amythyst has a repertoire that honors tradition while crossing genres to illuminate many common threads. A theme of “vocal integrity” unites her varied influences which include Son House, Dolly Parton, Sister Rosetta Tharpe, and Florence and the Machine. Accompanying her singing with guitar and clawhammer banjo, Kiah stands out among Southern artists, a uniqueness which has led her to perform at national venues such as the Smithsonian Folklife Festival and on programs like Music City Roots. Amythyst released a solo record titled Dig in 2013 and her current project brings together traditional and original music set for a five-piece blues rock band -- Amythyst Kiah and Her Chest of Glass. The group will release their debut EP in Fall 2016.\nTell me some about your upbringing in Chattanooga and how your first music came about.\nI grew up in the suburbs, so I was close to the mall and all that -- suburban sprawl kind of thing -- that’s sort of where I grew up. I played basketball and did the typical suburban life stuff. But when I was 13, I’d been really interested in wanting to play an instrument for a while, and my parents wanted to encourage me to play an instrument, play a team sport, and make good grades -- to be a well-rounded individual. Once I got my guitar, I started getting into writing and really getting into listening to music -- a lot of rock, singer/songwriters, that kind of thing -- and so, during that time, I pretty much dropped sports, transferred to a creative arts high school. I wish I could have gone there so much earlier, but I got there when I got there and it was a great change. I got really heavy into writing and playing music when I was in high school and I was a closet musician. I played a couple of talent shows, but I really just played for fun and I kind of kept to myself as a kid.\nActually, my first performance where it was a large group that was very much validating my existence as a musician -- other than my dad saying that he liked what he heard -- I wrote a song for my mom ... for her funeral. It was a few months before I graduated from high school that she died, so I wrote a song for her and sang it at the funeral. That was an eye-opening experience for me: Maybe I could write songs and people would actually want to hear them. I had a lot of good feedback.\nWe ended up moving to Johnson City when I was about 19, just to kind of start over. I transferred from the college I was going to in Chattanooga to East Tennessee State University and had absolutely no idea what I was going to do for my career. I didn’t really have a path. I just knew I was supposed to go to college. I was reveling in all these really cool classes like philosophy. I was really enjoying myself, but not knowing what I wanted to do. So I ended up auditioning for bands the next semester -- the Fall before, I had taken a bluegrass guitar class and I didn’t know anything about traditional music. I just liked the idea that I could take a music class where they appreciated learning by ear because, when I took classical guitar in high school, what discouraged me was the fact that I had to learn how to sightread and do all the formations and all that kind of stuff.\nSo I took that bluegrass guitar class with Jack Tottle, who is an amazing human being; it really meant a lot to be able to take that course because it changed my perspective on a lot of things. From there, I joined a Celtic band and did Celtic rhythm guitar and then I found my place in old-time music. That was around the time I learned about the Carolina Chocolate Drops -- I had taken Ted Olson’s class about American folk music and was fascinated with the intermingling, how multicultural the music actually was. I think part of my hesitance to finding my place was that I’ve always listened to all kinds of different music, and sometimes, if you don’t see people like you, sometimes you wonder or people make you feel like, “Well, do I belong here?” I was having those kind of feelings with some people. I’d had lots of praise and lots of support, so I’m really grateful for that, but there were always those few little people who put doubts in my head about my presence.\nOnce I read about the history of this music and how Blacks and whites both played this music -- that this is something that is integrally a hybrid -- I was like, “Well, hell, I have just as much right to be here as anyone else!” From then on, I was just like, “I’m doing it.” Roy Andrade reached out to me and asked me to be in the first-ever Old-Time Pride Band because he heard my voice and felt like he really wanted me to be part of it, and from then on I just did old-time. I ended up switching my major, once they got it approved, to Bluegrass and Old-Time Country Music Studies, and I graduated in 2012. During that time, I picked up solo gigs alongside the school band stuff, so that’s all part of my transition from playing mainly contemporary stuff into solely old-time stuff for a while, and now I’ve transitioned back into doing contemporary stuff. But it still has that old-time, roots influence.\nI really admire your music, because your personality is so evident in it and your own experience has shaped it. That’s so much a part of good music -- period -- but also traditional music. I was reading through your list of influences, and there wouldn’t have been a Sister Rosetta Tharpe or an Ola Belle Reed or so many of these figures if they hadn’t taken a step to put their own personality and experience in the music. You said you’re doing a lot more songwriting now and transitioning with your new band, Amythyst Kiah and Her Chest of Glass. Talk about that.\nIt’s interesting because, first of all, the guys in the band, they’re also part of another local band here called This Mountain and they’re an interesting mix. It’s kind of in the middle of folk and rock. They’re a hard sound to describe. They remind me of Radiohead -- alternative rock with acoustic instruments in it. This Mountain asked me to open for them a couple of years ago at the Hideaway over here. That was my first time playing a solo show in Johnson City. That turned out really well, and then they asked me to play with them at a festival in Savannah, Georgia, called Revival Fest because two guys in their band weren’t going to be able to make it. So we got together, put together a 30-minute set, then we went and played in Savannah.\nWe were well-received and I thought, “This is pretty cool!” Basically, the music in this band, a lot of it is stuff that I played acoustic, but with electric arrangements. For our EP, there are three songs that I’ve written that are going to be on there, and then there’s some stuff that I’ve done that come straight from old-time. Not all of them transitioned over, but two big ones are Vera Hall songs -- she’s really become one of my favorite singers. I’d like to take more of her songs and do more work with them. We do “Another Man Done Gone” and we do “Trouble So Hard” in the band. Obviously she sang a cappella and, as a guitarist, I always feel like I need to add some guitar stuff, so I added guitar arrangements to both of those songs. Then when I brought them to the band. At that point in time, they had mainly just been solely following me on what I do on guitar because I establish rhythm, bass line, and the riff. When they came in, they were kind of just following me, which is fine, but then we got to the point where it’s like, “Hey, what if we did the intro of a song with just piano or just drums?”\nSo I’ve gotten into arranging songs more because I have to remind myself that I’ve got other instruments here now. I don’t have to do everything. It’s nice because you get four different perspectives on the same song and it really opens you up in new ways, maybe trying things that you never thought you’d try before. But the way everything kind of flows right now is that it’s blues rock, but it’s also danceable -- it’s like blues-dance-rock. I’ve gotten into writing songs in a blues style mainly because, for me, songwriting has always been very difficult. I can write a poem -- I can write a short prose piece or a poem piece, but when it comes to putting it to music, I think of melody and chord arrangements first. That’s what happens when I listen to a song. Once I come up with the melodies, I’m like, “What the hell am I going to sing about?” because I feel like I’ve already expressed my feelings in this melody and in this song, so what else do I need to say? So that’s always been difficult.\nBut, when I got into blues, I started realizing that this is perfect. The main focus is on the emoting, and you’ve got a few choice words to describe what you’re feeling. For me, I like singer/songwriter stuff and the storytelling aspect of that, but I guess my brain doesn’t necessarily work in telling stories. I more or less like to express feelings. With a song like “Hangover Blues,” I’ll create three verses and they tell a really short story. I don’t know if it’s an attention span thing or what it is, as far as words go. I feel like sometimes, if I write too many words, it might take away from the emoting of the music. It’s something I always struggle with.\nThat’s so characteristic of really good traditional songs like the blues that you’re talking about -- that economy of words and expressing the feeling with your voice.\nThat’s where I feel most at home.\nSo much of your songwriting is about speaking your truth. You talk about writing for your mother and what a brave step that was. What kinds of emotions do you find yourself writing about now?\nIn the beginning, a lot of the stuff I would write about would be kind of along the lines of “me against the world.” Those aren’t songs that I’ve recorded because I wrote them years and years ago. But, as time has gone on -- especially after playing old-time music -- a lot of the songs I was drawn to were about loss and heartache, death ... lots of things that affect us to the core as humanables. There’s something very cathartic about playing a really sad song because, when it’s finished, it’s almost like you’re dealing directly with something that’s kind of scary and that you know is going to happen at some point in your life. To go through all those emotions in song is the safest way to be able to experience those things. It’s almost like preparing yourself, reminding yourself that bad shit happens, but at the same time, you come out of the song, and you can appreciate what you do have a lot more.\nSo now, the new songs that I’ve written, they’re actually a little more lighthearted than the stuff I’ve written in the past. “Hangover Blues” is one that’s on the EP and it’s about recovering from a hangover, but also being like, “I had a damn good time and I would do it again.” That’s one of my more lighthearted songs. Then “Wildebeest” is inspired from the sort of quintessential blues theme of “My woman pissed me off and I want to get back at her.” It’s a jealous lover kind of song. That one’s got some little parts in there that are meant to be lighthearted and comedic, but at the same time, the title also ties into the idea that, even though we are human beings, despite living in I guess what you would call a civilized society, we still have these primal urges. It’s a reminder of the fact that we are animals. I feel like keeping that in mind -- that we are susceptible to those things -- I feel like expressing that helps check the ego a little bit. The idea that people don’t see that they’re part of nature baffles me. You can be spiritual and still realize that you’re also part of nature. But some people separate themselves from their environment and, when people do that, you see what happens: Mountains get removed, tree forests are cut down because people don’t see themselves within the cycle of life. Just because we have logic and cognitive thought doesn’t mean that we live above and beyond everything. We are, in essence, destroying ourselves by doing this. Songs like “Wildebeest” ... I like to remind people that we’re very much part of something much bigger.\nYou can take this wherever you want to, but I’m wondering what you hope to accomplish through your music. You’ve talked about expressing your emotions and I think you represent a lot of communities in an innovative way, and also you are honoring these traditions and carrying them forward. What impact do you hope to make with your music?\nThis is something I’ve been thinking about a lot lately because music has always been something very personal. Sometimes it’s easy for me to get lost in my own brain and not necessarily think about the kind of impact that I’m having, but I’m thinking a lot more about that lately. For me, being a queer woman of color in Appalachia, pulling from these different roots-based ideas and then making these connections with electric music and traditional acoustic music and bridging the gap there, as an Appalachian person, I feel like I can bring a perspective to a wider audience and hopefully inspire people that look like me or love like me to tap in and be like, “Hey, this is really cool. This is something that I could do.” I just feel like, in a lot of ways, intersectionally, I’m the exact opposite of what would be considered typical for what I’m doing and I needed to see someone like that when I was playing music. I just feel like I want to, in some way, inspire other people. Their voice should be heard, and that contributes to the diversity of the people who are from our area.\nSam Gleaves is a folk singer and songwriter from Southwest Virginia. His latest record, Ain’t We Brothers, is made up of stories in song from contemporary Appalachia, produced by Cathy Fink.","Complications Most Challenging\nPricking your finger and taking diabetes medications on a daily basis is tedious but the most challenging and impactful part of this disease is the threat of complications.\nUnmanaged diabetes can especially threaten the wellbeing and function of your eyes, kidneys, fingers, toes, gums, stomach, and feet.\nA few years ago, research explained that high blood sugars alone are not the cause of diabetes complications. Instead, chronic inflammation must exist in order for damage to occur in the precious nerves throughout these areas of the body.\nSource of Inflammation?\nBut what causes that inflammation? Until recently, it was assumed that high blood sugar levels were to blame and that achieving healthier blood sugar levels was the key to reducing inflammation.\nPublished in Cell Metabolism by researchers from the University of Kentucky, the study reveals that changes in your mitochondria are actually the primary source of inflammation.\nMitochondria are considered the “power source” of energy for the cells in your body because they take in nutrients, breaks them down, and create energy-rich molecules for cell consumption.\nResearchers Barbara Nikolajczyk and Douglas Lauffenberger were initially studying a theory that immune cells in patients with type 2 diabetes could produce energy by burning glucose for fuel. While their theory proved false, Nikolajczyk said during that research they stumbled across evidence that damaged mitochondria and high levels of a particular type of fatty tissue were driving persistent inflammation.\nReducing inflammation could prevent complications\nThe significance of inflammation’s role in diabetes complications was first highlighted by a study published in 2014 when researchers found that without inflammation, glucose couldn’t enter and damage cells in the body.\nThis means that reducing inflammation could dramatically help protect against diabetes complications.\nThis is important because reducing inflammation may be easier than reducing blood sugars for patients with type 2 diabetes battling severe insulin resistance and beta-cell dysfunction.\nResearchers pinpointed that adding an inflammatory protein called “interleukin-1” into the bloodstream results in cells metabolizing excess glucose which in turn produces inflammation. Interleukin-1 had the same inflammatory effect in patients with and without existing diabetes.\nAfter administering an anti-inflammatory drug, the inflammatory response was blocked and the glucose was no longer able to enter and damage the cells.\nHow you can reduce inflammation naturally\nThere are actually many ways you can reduce inflammation in your body to reduce your risk of developing complications. Reducing inflammation also means reducing insulin resistance which will further help to lower your blood sugar levels.\nTurmeric (curcumin) supplements\nTurmeric has proven to be one the of most anti-inflammatory agents mother nature has to offer — even more so than some pharmaceutical drugs, as explained in this 2017 study.\nIt’s also proven to improve your body’s ability to produce insulin.\nA 9-month curcumin intervention in a prediabetic population significantly lowered the number of prediabetic patients who eventually developed type 2 diabetes, explained a 2012 study published by the American Diabetes Association. Curcumin treatment also improved the overall function of beta-cells with very few side-effects.\nThe study also determined that turmeric can also improve your body’s production of glucagon-like peptide 1 (GLP-1) which is responsible for the rate at which food is digested, and for signaling to your brain that you’re full which helps you stop eating sooner.\nExercise is one of the most powerful anti-inflammatory resources at your fingertips — and it doesn’t cost a thing!\n“Obesity and physical inactivity are associated with chronic low-grade inflammation, which provides the common soil from which a myriad of metabolic diseases develop,” explained the 2014 study.\nRegular exercise (like walking for 30 minutes a day during your lunch break, after work, or after dinner) not only helps to lower your blood sugars, it lowers inflammation levels throughout your entire body, too. As you reduce inflammation, you reduce insulin resistance, which means you can naturally improve your body’s ability to reduce your blood sugar levels, too.\n“That could be one of the reasons if we can get a diabetic to exercise and lose weight, they will have less damage to their blood vessels,” explained Mary Ann Bauman, MD.\nDon’t overthink it, just get moving.\nReduce your alcohol consumption\nWhile a glass or two of wine — or a few tasty microbrews — are no big deal for a person with diabetes, drinking alcohol more than one or two nights a week can contribute heavily to overall inflammation.\nAlcohol is a toxin. Period.\nIf alcohol is present in your body, it wreaks havoc on a variety of aspects of your health. The more you drink, the more potential damage alcohol can cause.\n“Heavy alcohol consumption contributes to systemic inflammation by interfering with the body’s natural defenses against the influx of gut microbiota and its products,” explained a 2010 study.\nThe study adds that alcohol seriously impairs your brain’s ability to regulate systemic inflammation.\nReduce inflammatory foods like dairy, gluten, and sugar\nIf you’re drinking cow’s milk, eating bread and pasta daily, and consuming sugar throughout your entire day, there’s a lot of potential for reducing inflammation simply by replacing these habits with healthier ones.\n- Swap unsweetened vanilla almond milk for cow’s milk.\n- Limit your gluten intake to once a day and replacing those calories with more whole-food gluten-free sources like fruit, vegetables, beans, and lean protein from chicken, turkey, and eggs.\n- Track how many grams of sugar you drink and eat every day.\n- Cut out all sugary beverages because you can easily cut 50 to 100 grams of sugar from your diet overnight by drinking more water, seltzer, and the occasional beverage sweetened with stevia instead.\n- Limit your sweet treats to once per day or every other day depending on how much sugar you get from food now.\nBy making just a few changes to your daily habits, you could drastically lower your body’s inflammation levels and reduce your risk of diabetes complications. You might eventually enjoy those healthier habits, too!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:a5853b64-8a1a-48e7-9f7d-80b604c49dae>","<urn:uuid:98843eae-af5a-412c-9a3b-efb25410d006>"],"error":null}
{"question":"I'm designing industrial equipment - what are the key differences between actuators in aircraft and gas flow monitoring in 3D metal printing?","answer":"In aircraft, actuators are used for active flow control through targeted air blasts, similar to small loudspeakers pumping air at high velocity. They can be integrated into wings or fins to improve climb and save fuel, though this adds weight. In 3D metal printing, gas flow monitoring is instead focused on maintaining a stable inert gas atmosphere using air velocity sensors. These sensors detect gas flow in inlet channels to ensure proper powder coat application, prevent powder eddying, and protect against oxidation. While aircraft actuators directly influence aerodynamic flows, gas management in 3D printing aims to create optimal process conditions for component quality.","context":["Flying: Efficiency thanks to Lightweight Air Nozzles\nThe Federal Cluster of Excellence MERGE at Chemnitz University of Technology and the Fraunhofer ENAS join forces in order to optimise actuator systems for active flow control in aeroplanes and cars\nActive flow control has nothing to do with flowing rivers and the dead wake is actually air, and not really dead at all. Basically speaking: “We investigate ways of influencing aerodynamic flows of aeroplanes, cars or wind turbines”, explains Dipl.-Ing. Martin Schüller, research fellow at the Fraunhofer Institute for Electronic Nano Systems in Chemnitz. “To this end, we use actuators. It helps to imagine small loudspeakers pumping air with a very high velocity.” These so-called actuators can actively control the aerodynamic flow through targeted air blasts. The Federal Cluster of Excellence MERGE at Chemnitz University is contributing significantly to this research project.\nOptimising their application and design for varying uses is the subject of Schüller’s PhD thesis and of his daily work along with his colleagues Mathias Lipowski, Perez Weigel and André Gratias in the “Flow Control Actuators and Systems“-team. So far the usual approach has been trial and error”, Schüller says and adds: “I wanted to develop an optimisation tool that allows for a simulation of as many application parameters as possible.” The result is a combination of a closed analytical model with a network model, which can calculate all elements analytically for the first time. With this tool the actuators’ performance can be improved for every application.\nSchüllers PhD thesis takes a closer look at an application in aviation. Actuators can be integrated into wings or fins e.g. improving the climb and thereby helping to save on fuel. Nevertheless, this integration requires additional assembly work and creates more weight leading to higher fuel consumption, as the scientist explains. Which is why, Schüller and his team have investigated the integration of actuators into several material compounds and components within the Federal Cluster of Excellence MERGE. They took some inspiration from automotive engineering in this respect. “We try to fight the so called dead wake at the car rear, where turbulences emerge, slowing down the car. Actuators at the rear can counteract this effect”, Schüller explains.\nThe most important parameters are taken from lightweight design: There are only a few production technologies and materials that are suitable for an especially low weight, the installation space is pre-defined, as are the aerodynamics of the vehicle. According to the chosen technology a variety of actuator designs is possible, because their layout is predetermined technologically. Milling, 3D printing, injection moulding and stereo lithography allow only for specific forms of the nozzle and cavity. Innovative fibre-reinforced plastic materials, which are the main focus of MERGE, possess other properties that have to be taken into account for the integration of the actuators.\nInterdisciplinarity Ensures Practical Strengths and Economic Efficiency\nMechanical and electrical engineers within the Cluster cooperate with the Faculty of Economics and Business Administration at Chemnitz University of Technology on an interdisciplinary level. Professor Uwe Götze, Professor of Management Accounting and Controlling, and his colleagues investigate the economic efficiency of each of the manufacturing technologies as well as the life cycle of the actuators. For example, for the aircraft wing. The economists analyse whether the saving in fuel during the launch can make up for the higher fuel consumption during the flight, i.e. whether the use of actuators would be sensible from an economic point of view.\nAll these parameters - from the manufacturing technology via aerodynamic values up to economic efficiency - are combined in MERGE. Schüller’s optimisation tool can help combine them analytically and create an almost optimal design of the actuators for each respective case. “We also aim at integrating an adapted actuator system into the rear spoiler of our Chemnitz Car Concept”, Schüller explains. “This is an absolute innovation. Until today there have been studies on actuators in side-view mirrors only, but no implementation has yet been tried for the rear.”\nAbout the Chemnitz Car Concept\nVarious research areas are merged in the system demonstrator called “Chemnitz Car Concept” (CCC). The “MERGE up!” provided by Volkswagen, serves as a platform for tests and demonstration of the latest research results in cooperation with the Department of Advanced Powertrains. The lightweight vehicle represents the MERGE technologies integrated into an electrically driven car. It will include exemplary parts in the interior and exterior - from lightweight components up to the power train - especially developed and built into the “MERGE up!”.\nFor more information please contact the project coordinator Martin Schüller, Phone 0371 45001-242, e-mail email@example.com\nFor more information on the Chemnitz Car concept please visit www.tu-chemnitz.de/MERGE/ccc.php","3D Metal Printing – Optimum Process Conditions Are Decisive for Component Quality\nIn the additive production of metal parts, the process conditions have a direct influence on the quality of the finished product. Gas management in particular plays a key role in 3D metal printing. Gas flow control and moisture content monitoring in the printer chamber can be ensured using air velocity and dew point sensors.\n3D Metal Printing – Gas Flow and Moisture Influence the End Product\nMost 3D printing processes use a protective gas – such as argon, nitrogen or helium - to create an inert atmosphere in the 3D printer. This avoids undesirable chemical reactions (e.g., oxidation) or contamination of the metal powder in the process chamber.\nConventional 3D Metal Printing Methods:\nThe most popular 3D metal printing method is the powder sintering process. An energy source sinters or melts a metal powder, the finished component part is built up layer by layer. Direct Metal Laser Sintering (DMLS) or Electron Beam Manufacturing (EBM) are based on this principle.\nIn Laser Metal Deposition welding (LMD), a laser generates a melt bead; the metal powder is applied by a nozzle.\nIn Binder Jetting Technology, a binder is applied in layers to a metal powder bed and cured. The component part is then sintered in an oven.\nSolutions for Reliable Gas Management: Air Velocity Sensors and Dew Point Sensors\nUniform gas distribution, and low humidity in the process chamber, are decisive for high-quality component parts from the 3D printer. Thanks to air velocity and dew point sensors, these parameters can be monitored reliably and precisely.\nUsing Air Velocity Sensors to Monitor the Gas Flow\nA stable inert gas atmosphere is essential for high-quality 3D metal printing products. If the gas supply and gas removal do not function correctly, this can result in the following issues:\n- The powder coat applied is too thin or too thick\n- The powder is eddied or eroded\n- Sparks/contamination can settle on the workpiece\n- Insufficient protection of the work area against O2 and H2; oxidation occurs\nAll of these points impact on the quality of the 3D printed component part.\nAn air velocity sensor – positioned on the 3D printer's gas inlet channel – detects even the slightest gas flow. The gas supply can be precisely controlled to achieve the best possible conditions in the printing chamber. On top of this, efficient gas management reduces the inert gas consumption and ensures safe and cost-efficient operations.\nBenefits of E+E air velocity sensors:\n- Excellent measuring accuracy and long-term stability\n- Applicable for a wide air velocity and temperature range\n- Rugged metal housing for harsh environments\nMonitoring Moisture Content with Dew Point Sensors\nIn addition to the gas flow, moisture also plays a central role in the printing process.\n- Damp metal powder can clump\n- Condensation in the printer chamber can impair the component properties\n- Dew formation on the lenses leads to refraction and structural defects\nDepending on the size of the process chamber, the use of 2 to 3 dew point sensors is recommended for monitoring the moisture content in the material and gas.\nBenefits of E+E dew point sensors:\n- Precise dew point measurement at low temperatures (measuring range down to -60 °Td)\n- Easy integration thanks to analogue output or Modbus RTU interface\n- Compact design, rugged stainless steel housing"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d65e1b93-2915-486c-a039-f96c9303d099>","<urn:uuid:0986b32d-e865-487e-86f9-e97c430fe01d>"],"error":null}
{"question":"How do teams evolve their documentation practices and style guides over time?","answer":"Style guides evolve as agile artifacts, with teams revisiting the standard every iteration until consensus is reached. Eventually, mature teams may discard their documented style guide once all new code naturally follows the established patterns. Similarly, continuous documentation requires gradual culture change, starting with a basic documentation system and progressively forming habits of regular contribution. Teams should prioritize documentation by creating separate tickets and incorporating it into each sprint, treating documentation debt as part of technical debt.","context":["Coding standards? That tired, old chestnut? Who talks about coding standards these days?\nYou do. You are talking about coding standards when you see someone has their tabs set wrong, when you complain about bracing, when you ask someone why they put their spaces where they do, when you add comments and someone else says delete them, ...\nThe need for a common style guide was glossed over in Collective Code Ownership, in the following paragraph:\nThe team has a single style guide and coding standard not because some arrogant so-and-so pushed it down their throat, but rather the team adopts a single style so that they can freely work on any part of the system. They don't have to obey personal standards when visiting \"another person's\" code (silly idea, that). They don't have to keep more than one group of editor settings. They don't have to argue over K&R bracing or ANSI, they don't have to worry about whether they need a prefix or suffix wart. They can just work. When silly issues are out of the way, more important ones take their place.\nSo what kind of document is the code standard? The authors have seen plenty of large, complex, detailed guides that strive to be comprehensive. Who has the time? Perhaps the answer should be to look for the least documentation one can afford. How much is too much? How much is too little?\nThe first recommendation is that a team should standardize to avoid waste. In this case, \"waste\" includes rework, arguments, and work stops while issues are settled. In this regard, it is better to have even a bad decision than a variety of opinions. If we find that we are enduring waste, then we add a line to the standard. Again, we have to build collective ownership and any choice is better than arguing the same points repeatedly.\nThe team should start with an accepted community standard if one can be found. If there are multiple, choose one of them. For the python community, PEP 8 is a wonderful starting point. One may look at a ubiquitous tool's default styling as a community standard (community of tool users), since it seldom pays to to fight your tools. Notice that public style guides tend not to be the smallest document one can afford, but the goal is really to make development time productive and non-contentious (with regard to silly issues) so even a longer document may be successful.\nIf there is some particular contentious difference of opinion, then the team should time-box the argument. They might choose to debate for an hour, agree, and write it down. Some teams don't time-box the argument, and others do not reach resolution. Do not let contentious issues remain, or they will remain contentious. Once agreed and recorded, the issue should not be revisited. Nor should a pair partner allow his partner to spend iteration time arguing over decisions already made. Nobody has to love the decision, but they should admit to a fight well fought and a final answer that should not get in the way of getting real work done well and often.\nIdeally, the standard should be minuscule. A standard-by-example will have a better chance of being concise and obvious. Therefore, the standard should be mostly code and fit on one page. This is especially true of the initial version of the team's style guide. To some degree, arguments and uncertainty will lead to the accumulation of additional guidelines but it is wise to start small. Our working documents should be like our code in the sense that it is as small, simple, and unambiguous as possible.\nThe style guide is an agile artifact. it is subject to corrective steering. A team should revisit the standard every iteration until no one cares any more. Iteration time is too valuable for these arguments, but retrospective time exists to help the team eliminate waste and turbulence. If further discussion of style points will help smooth the coming iteration, then it should be brought up.\nThe rule for simplifying any system is to obviate and then remove steps, processes, and instructions. If the code is written to standard, then the code ultimately becomes the standard and the standard becomes redundant. It is perfectly reasonable for a mature team to discard their documented style guide and keep the style. You will know the style guide is unnecessary when all new code looks like the existing code. And it all looks good.","Continuous documentation is the process of creating and maintaining code documentation incrementally throughout a project in a way that seamlessly incorporates it into the development workflow. It is a key part of improving reliability within an organization.\nIt’s not just new features that need to be documented – anything useful from bug fixes, to how to get started using the code should be documented. It should also be updated frequently to ensure that it stays relevant.\nAlthough continuous documentation is viewed as a core part of Agile development, it is still only implemented by some organizations.\nIn this article, we’ll discuss the importance of documentation in a CI (continuous integration) and CD (continuous delivery) world.\nWhy is continuous documentation important in a CI/CD world?\nContinuous documentation is so important in a CI/CD workflow because it helps other developers easily understand how to use your software. This saves time and makes it more efficient for teams to transfer knowledge when they need to.\nThis can make onboarding significantly easier. According to research by Swimm, it takes an average of three to nine months for new software developers to ‘ramp up’ after joining a team. Having a thorough documentation system in place can significantly reduce this time.\nThe 3 key principles of continuous documentation\n1. The documentation should directly reference the part of the code it is referring to\nThis makes the documentation easier to find, and also helps to make sure that it is up to date.\n2. The documentation should evolve at the same rate as the code\nKeeping documentation up to date means that developers will be able to trust it, and makes them more likely to utilize it when they need to.\n3. Developers should write documentation when it is still fresh in their minds\nThis is more efficient because it means that less time will be wasted going back and trying to remember what should be included. This can be done by creating a habit as a team to regularly create documentation after each sprint, or by downloading a tool that automatically reminds developers to create documentation.\nChallenges posed by current documenting\nDespite the importance of documentation, it is often put on the backburner by developers and organizations for several reasons:\n- Documentation is not viewed as a priority by organizations: As a result, it is often created only on specific occasions – such as when a group of new hires has joined the organization and the team is trying to bring them up to speed quickly. However, onboarding new employees and writing documentation are both challenging enough tasks by themselves, and trying to do both at the same time means the process is often rushed.\n- Documentation is often viewed as a chore that takes time away from the primary task of writing code: This means that the task of writing documentation is often passed off to interns who are not in the best position to write it.\n- Documentation updates often happen in sprints that occur less frequently than the code is updated: This means that documentation can quickly end up obsolete, which leads to developers losing trust in it and deciding not to use it.\n- As the codebase gets more complex, so does the documentation: As a result, it ends up taking more time to write and update, which results in many organizations deciding to abandon it altogether.\nContinuous documentation and culture change\nFor continuous documentation to be successful, it requires a culture change. If an organization has never valued the process of writing documentation before, then it is unrealistic to expect this to change overnight.\nAs it stands, most organizations still devalue the process of writing documentation – nobody wants to do it, and many developers don’t even know how to write documentation properly because they have never been taught.\nBefore anything else, teams should make sure that they get a functioning documentation system in place. Then, they can start to form a habit of contributing to the documentation more, and shortening the delivery window. This is a gradual process that can take a long time to implement.\nWhere to begin with CI/CD documentation\nCI/CD documentation is a continuous process that can take a long time to implement. To get started, organizations should come up with a gradual plan.\nFor most teams, it makes sense to start by creating documentation that will have the biggest impact in the short term.\nOrganizations should learn to view documentation debt as an essential part of a team’s overall technical debt so that it is prioritized instead of consistently being pushed back as an afterthought. This can be done by creating separate tickets for documentation and making sure they are addressed sufficiently.\nOnce the team has made this a habit, then the team should make sure that they prioritize incorporating documentation into each sprint moving forward. This will help to ensure that documentation is written while the code and the feedback is still fresh in developers’ minds, which will make the process significantly more efficient.\nWhere does continuous documentation live?\nCode documentation should be as close to the code as possible. For example, if you’re using GitHub, you should include it in your commit messages. This helps to make sure that it is easily accessible to those who need it.\nContinuous documentation and improving reliability\nContinuous documentation helps developers to quickly understand code that they are not familiar with and makes it easier for them to get up to speed with a new project. This helps to speed up development, and also reduces confusion and makes it significantly easier for developers to switch their focus between tasks.\nContinuous documentation also allows teams to think more deeply about how they share information. This helps to improve communication between team members, which ultimately leads to helping them to build more reliable software."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:31581c78-f785-4c26-a0bc-cdecc1e4b831>","<urn:uuid:445ced3f-f5e6-464e-a7f5-a1e4fbc158e4>"],"error":null}