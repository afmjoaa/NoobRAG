{"question":"What is more effective for protecting welders: auto-darkening helmets with traditional ventilation or standard helmets with powered air purifying respirators?","answer":"Powered air purifying respirators combined with welding face shields are more effective for overall protection. While auto-darkening helmets offer advantages for different welding processes with reaction times between 1/2,000 to 1/20,000 of a second and variable shade options, they only protect against arc radiation and visible/invisible rays. In contrast, powered air purifying respirators provide comprehensive protection from both hazardous fumes and gases, as well as ultra violet radiation from the arc. They are also more comfortable to wear than tight-fitting masks and are particularly important given the hazards of welding fumes containing substances like manganese.","context":["According to the National Safety Council, farms and ranches are one of the most dangerous workplaces in the country. Every year, thousands of farmers and ranchers suffer serious injuries while working on the farm. The vast majority of these tragedies involve equipment, machinery and tools, and welding equipment is no exception.\nUsing 5,000 watts of electricity or more to melt two pieces of metal together presents the risk of serious burns and electrical shock. However, following a few simple safety precautions can keep you welding injury-free for years to come.\nProper welding safety starts with familiarizing yourself and other operators with the welding equipment and the manufacturer’s recommendations. Take the time to read the operator’s manual thoroughly and follow all of the safety, operation and maintenance instructions it contains. Keep the manual handy so other users can acquaint themselves with the machine. Should the operator’s manual become lost or damaged, request a new one from the manufacturer. Miller Electric and many other manufacturers provide product manuals on-line.\nPlace the welding power source on a flat surface away from any water or flammable materials, including paper, cloth rags, oil and gasoline. Avoid working in wet conditions, since water conducts electricity.\n|An auto-darkening helmet, such as the new Fire Storm from Miller Electric Mfg. Co., provides variable shade lens adjustments for working with a variety of materials, parameters and processes.\nConnect the workpiece to a proper earth ground if possible and make sure the connection is metal on metal and unimpeded by paint or other foreign material. Always double-check the installation and verify proper grounding. Never use chains, wire ropes, cranes, hoists and elevators as grounding connectors.\nWhen using gas cylinders, chain them securely to a stationary, upright support or cart at all times. When moving or storing a cylinder, fasten the threaded protector cap to the top of the cylinder. Doing so shields the valve system from impact damage. Only use gas hoses designed for welding, available at your local welding supply or general hardware store.\nIf there will be other people in the shop while you are welding, use a weld screen to ensure passersby will not be subjected to the arc flash.\nKeep your work area free from clutter. This promotes safety and helps increase efficiency by making necessary equipment easier to find. Remove rags, paper or anything else that could be a fire hazard.\nCables and hoses can create a trip hazard. Organize the workspace to minimize the number of cables underfoot and position them so they are not in danger of being run over or stepped on. If possible, suspend hoses off the ground and coil up excess hose to prevent kinks and tangles.\nExamine hoses regularly for leaks, wear and loose connections. To check for leaks, spray them with a soap and water mixture and look for bubbles, which indicate a leak. Replace faulty gas hoses with new hoses.\nIt’s sometimes easy to forget that the metal you welded a couple minutes ago retains enough heat to cause serious and painful burns, but that’s exactly what can happen if you try to pick it up with bare hands or thin gloves. Even with heavy duty gloves, picking up a piece of hot metal poses a burn risk because the leather retains the heat from the metal and can continue burning your skin well after you’ve set the workpiece down.\nAppealing to the hunter and outdoorsman, the Camouflage helmet from Miller Electric is part of the company’s new Pro-Hobby™ line of economically priced, full featured auto-darkening helmets.\nPicking up hot metal with a good pair of pliers (two good pair are ideal) will help you avoid serious burns. Also, use appropriate tools for chipping and brushing off slag, grinding out the cracks and defects in broken parts, sanding or brushing off paint, rust and other surface contaminants and clamping the workpiece to the welding table.\nMany farms still rely on Stick welders for their welding needs. These durable machines are relatively inexpensive and get the job done, but they also produce the most fumes of any arc welding process. These fumes can be hazardous, and proper workplace ventilation is essential.\nMost farm workshops are large enough to provide sufficient ventilation without the need for additional fans. Smaller work areas should have some type of fume extraction fan in place, however.\nArc welding produces sparks and spatter and emits intense visible and invisible rays that pose several hazards to unprotected skin and eyes. Shorts, short sleeves and open collars all leave you vulnerable to burns from both flying sparks and the arc rays. Wear only flame-resistant clothing, and button your cuffs and pockets to prevent them from catching sparks. Pants cuffs, too, can catch sparks and should be avoided.\n|Proper attire—long sleeve shirts, pants, leather shoes, thick gloves—can prevent painful burns from spatter and sparks, which are especially prominent in Stick welding.\nWith respect to footwear, high top leather shoes offer the best protection. Tennis shoes and other cloth shoes are inadequate; they can catch a spark and smolder unnoticed, and their components can melt and stick to your skin.\nAlways wear proper gloves when welding or handling recently welded material to protect yourself from sparks, arc burns and the heat from the workpiece. Remember, even a quick tack weld requires the use of a welding helmet and appropriate apparel.\nAlthough the above sounds obvious, a common fault among welders is not wearing the right safety equipment.\nEven a brief exposure to the arc’s radiation may be causing symptoms such as a burning sensation or eye irritation. Repeated exposure can lead to permanent injury. Always wear proper face and eye protection, including safety glasses underneath the welding helmet, when welding or when exposed to a welding arc.\nAuto-darkening helmets offer the best solution if your welding needs require different processes—MIG, Stick, TIG—materials or parameters. All auto-darkening helmets must comply with the safety and protection requirements of the American National Standards Institute (ANSI).\nAuto-darkening helmets vary greatly in their response times to the light of the arc, generally between 1/2,000 to 1/20,000 of a second. Helmets also differ in the level of lens shade, with some remaining a fixed shade #10 and others adjustable between shade #8 and shade #13.\nAll auto-darkening helmets will meet most farm welding needs. However, farmers involved in more intensive welding, requiring frequent tack welds, TIG welding or other industrial operations, should consider helmets with reaction times of 1/10,000 of a second or faster and adjustable shade lenses.\nAuto-darkening helmets are more expensive than traditional welding helmets, though, so if you’re on a budget or your weld parameters and materials don’t vary, a fixed-shade lens may be right for you.\nMaking Safe Welds\nUsing your welder in a safe manner is only the first part to being a safe welder, however. Just as important to your safety is performing high-quality, structurally sound welds. Failing to fully grind out a crack or using insufficient current could leave you with a weld that appears sound, but in reality could fail under the demanding conditions common on farms.\n|Welding outdoors relieves the danger posed by welding fumes, but raises the possibility of insufficient shielding gas coverage if the wind is over 5 mph when MIG welding. Flux Cored and Stick are alternative processes that are not affected by normal wind conditions.\nLike operating the welder safely, strong welds begin with the owner’s manual. The owner’s manual will help you select the proper wire or electrode for your metal type and thickness and it will provide guides to setting the correct current parameters to ensure adequate penetration.\nOnce you have the wire/electrode and settings right, be sure to grind away the paint, rust and other surface material from the area to be welded. Stick and flux cored welding are more forgiving of these types of contaminants than MIG welding, but the metal should be cleaned as much as possible regardless.\nOne of the most overlooked steps in making strong welds is fully grinding out the cracks and holes that form in the equipment. Oftentimes, when a crack forms, the operator will simply get out their welder and start welding over the crack, ignoring the fact that the crack goes all the way through the metal. A “Band-aid” approach such as this creates an unsafe situation in which a small amount of weld material is being required to bear the same amount of weight and force that caused the crack in a much thicker piece of metal. To avoid this possibly hazardous situation, grind out the crack from both the front and back of the metal to make sure that the crack doesn’t reform and continue spreading.\nMaking multiple passes is another step toward creating sounds welds. Operators often go too slowly, mistakenly thinking that the extra metal they are depositing will result in stronger welds. Rather, weld at a recommended pace and go back and make multiple passes as necessary.","Breathe Freely in Manufacturing\nWelding processes generate emissions of very fine particles known as welding fume.\nThe fume is made up of lots of different kinds of substances, most of which are hazardous to your health. One common component of the fume is manganese, which has been shown to cause adverse effects to the nervous system. The arc welding process also generates gases such as carbon monoxide and ozone. Different processes generate differing amounts of fume, e.g. tungsten inert gas (TIG) welding produces less fume than manual metal arc (MMA) welding. Welders can be exposed to the fume from the welding operation unless suitable controls are put in place, such as local exhaust ventilation (LEV) or respiratory protection.\nThe material being welded and the welding rods contain many substances, one of them is manganese (Mn). There is growing concern regarding the impact of this metal on human health and the current workplace exposure limit is likely to be revised in the near future. Many steels contain levels of manganese of up to 2.5% and special grades can contain up to 12.5%. Welding rods contain as much as 12.5% manganese as this helps the flux flow freely.\nSome stainless steels replace nickel with manganese.\nManganese is used in welding rods because it helps the flux flow easily. This is due to the low boiling point of manganese compared to iron (1962°C compared to 2750°C). Because of this, it is likely that there will be more manganese in welding fume than expected from the composition of the rod or the steel (as described in the safety data sheet for the welding rods). In addition, the manganese is more likely to be respirable – that is its very small particles can penetrate to the deep sections of the lung.\nOf course manganese is not the only hazard associated with welding.\nAssessments under the Control of Substances Hazardous to Health (COSHH) regulations should identify hazardous substances inherent or generated by the welding processes, considering the metal, any coatings or contamination and welding rods, including:\nSuitable control measures should be put in place to control all of the hazards identified as part of the risk assessment process\nManganese dioxide, as produced in welding fume, is hazardous and has been classified according to EU and UK legislation with the following criteria:\nSome studies have suggested that exposure to even relatively low levels of manganese can cause harm to the nervous system and may result in effects including impacts on IQ, verbal learning, concentration, memory, motor skills and mood.\nRepeated high exposures to managese can also cause a condition known as “manganism” with symptoms similar to Parkinson’s disease.\nTo understand the type of control measure to use, you need to know at what level any exposure is acceptable. Workplace exposure limits (WELs) are the levels of airborne contamination that a worker can be exposed to. In most cases, WELs are set at levels which should not result in adverse effects. The limits are usually set as averages over an 8 hour period. These are therefore known as 8 hour time weighted averages (8hr TWAs).\nThe current WEL for Manganese in the UK is 0.5mg/m3 (8hr TWA). However, the European Union is proposing that the exposure limit should be reduced to 0.2mg/m3 (8hr TWA) and there will be a new additional specific standard for those small particles that reach the deep lung (known as respirable particles) of 0.05mg/m3 (8hr TWA).\nThis change is significant as much of the manganese in the fume will be respirable. It is likely that the proposed respirable limit will be exceeded during many welding activities unless effective controls are introduced and used properly.\nWelding fume exposure can be minimised by the use of local exhaust ventilation (LEV), including on-tool extraction. The Welding institute and others have demonstrated that effective control using LEV does not have any impact on the quality of the weld and protects workers from hazardous fumes.\nThe significantly reduced WEL related to manganese may drive welding operations producing visible fume to use supplementary respiratory protection with an A2P3 filter, where it is not practicable to install effective LEV.\nPowered air purifying respirators, combined with welding face shields, are the preferred option as they provide protection from hazardous fumes and gases, as well as the ultra violet radiation from the arc.\nThey are also usually more comfortable to wear than tight fitting masks.\n5/6 Melbourne Business Court, Millennium Way, Pride Park,\nDerby, DE24 8LZ\n© British Occupational Hygiene Society"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:16acc7bd-4cd1-4ac1-9e4c-a5f2c952f519>","<urn:uuid:86832989-a22b-450d-9cd5-00d618d86796>"],"error":null}
{"question":"How does gravitational lensing manifest in different astronomical phenomena, and what scientific insights has it provided?","answer":"Gravitational lensing manifests in several fascinating astronomical phenomena. The Einstein cross, first observed in 1985, shows four images of the same object arranged perpendicular to each other, while Einstein rings, discovered in 1988, appear as circular halos when a distant galaxy is warped by a closer one. The first confirmation of gravitational lensing occurred during the 1919 solar eclipse, when stars behind the Sun appeared shifted from their fixed positions. This phenomenon has proven invaluable for scientific research, as it helps astronomers study dark matter, which is estimated to be five times more massive than all ordinary matter in the universe. Through projects like Space Warps and the Dark Energy Survey, scientists can use gravitational lensing to investigate both visible and invisible structures in the universe.","context":["- Press Release\n- May 26, 2023\nSpace Warps Project Needs Your Help\nAstronomers are asking volunteers to help them search for “space warps.” More commonly known as “gravitational lenses,” these are rare systems with very massive galaxies or clusters of galaxies that bend light around them so that they act rather like giant lenses in space, creating beautiful mirages.\nAnyone can participate in Space Warps project, which was launched on 8 May 2013. Visit http://www.spacewarps.org and spot these spectacular and rare astronomical objects using data from large astronomical surveys. Astronomy enthusiasts can partake in the discovery of these magnificent lenses and help astronomers uncover the role that dark matter plays in the formation of galaxies.\n“Not only do space warps act like lenses, magnifying the distant galaxies behind them, but also the light they distort can be used to weigh them, helping us to figure out how much dark matter they contain and how it’s distributed,” said Dr. Phil Marshall, co-leader of the project at the University of Oxford.\nThe Space Warps project is a lens discovery engine. The first set of sky images to be inspected in this project is from the CFHT (Canada-France-Hawaii Telescope) legacy survey. “We have scanned the images with computer algorithms, but there are likely to be many more space warps that the algorithms have missed. Realistically simulated space warps are dropped into some images to train volunteers to spot them and reassure people that they are on the right track,” said Dr. Anupreeta More, co-leader of the project at the Kavli Institute for the Physics and Mathematics of the universe (Kavli IPMU), the University of Tokyo.\nPrevious studies have shown that the human brain is much better at identifying lenses than current computer algorithms, and members of the public are as good at spotting astronomical objects as experts. The team describes the project as a collaboration between humans and computers, as data from the human volunteers will help to train computers to become better space-warp spotters.\n“Even if individual visitors only spend a few minutes glancing over 40 or so images each, that’s really helpful to our research — we only need a handful of people to spot something in an image for us to say that it’s worth investigating,” said Dr. Aprajita Verma, another co-leader from the University of Oxford.\nJoining the search is easy: visitors to http://www.spacewarps.org are given examples of what space warps look like and are shown how to mark potential candidates on each image. After identifying possible candidates in images, participants will be able to discuss them via an online forum with other volunteers and experts, and create computer models of their discoveries. The final collection of space-warp candidates will be published for both amateur and expert astronomers to investigate further.\nFollowing the CFHT legacy survey, other surveys will also be using Space Warps as a means to find lens systems in their data. For example, the Dark Energy Survey led by the United States and the Hyper Suprime-Cam survey led by Japan are interested in collaborating with Space Warps, too. Several upcoming large area surveys such as with Large Synoptic Survey Telescope and Euclid will certainly benefit from Space Warps.\nSpace Warps is a citizen science project from http://www.zooniverse.org\nYoshihisa Obayashi / Tomomi Hijikata\nKavli IPMU Public Relation Office\n+81 4 7136 5974 / +81 4 7136 5977\nDr. Phil Marshall\nUniversity of Oxford\n+44 1865 273345\nDr. Anupreeta More\nKavli IPMU, The University of Tokyo\n+81 4 7136 6566\nDr. Aprajita Verma\nUniversity of Oxford\n+44 1865 273374","Gravitational lensing is a phenomenon by which a body containing a large mass (stars, for example) bends the space around it, thereby bending the path of light as it travels.\nSome physical objects are simple, but have profound applications. Take lenses, for example. The glasses your grandma wears to recognize a stranger is a lens. The microscope used by science geeks like us is a lens. The magnifying glass you use to read tiny text is a lens.\nIf you casually observe a lens, you might say that a lens distorts the view of things behind it (sometimes for the better). However, there are more complex applications of a lens, as well as a profound cosmological significance. There is a type of lens that is helping us explore new galaxies and see what is otherwise unseeable. That lens is a gravitational lens, powered by gravity!\nWhat is gravitational lensing?\nAlthough our understanding of gravity is limited, we can certainly say that it’s a pretty amazing force. It is good not just for pulling the poor Coyote from Looney Tunes abruptly off a cliff, but also for keeping our own feet on the ground. Gravity is also the force that binds together the planets of our solar system. Interestingly, the incredible force of gravity also has an eccentric bag of tricks up in its sleeves, and one of those tricks is its ability to act as a lens.\nGravitational lensing is a phenomenon by which a body containing a large mass (such as stars) bends the space around it and thereby bends the path of light as it travels. It might sound like a surreal idea, but it has fascinated scholars for years, and is extremely helpful when we look out into the vast cosmos.\nGeneral relativity and gravitational lensing\nIn 1915, Einstein put forth his new theory of gravity called general relativity. Einstein suggested that matter distorts the fabric of space and time around it. That implies that gravity bends light waves, i.e., making their paths curve around massive objects like stars or black holes.\nWith this theory, Einstein gave us a whole new perspective of looking into space. General relativity treats space as a malleable entity, wherein the “shape of space” can itself be distorted. Going by this theory, celestial objects don’t really orbit around other celestial objects—because they would eventually merge as a result of attraction. Instead, these celestial entities travel in a straight path through curved space. What this implies is that the orbit of Earth around the Sun is a straight line, when observed from the Earth. This is a mind-bending phenomenon to imagine, but let’s try to picture it more clearly with an example.\nImagine someone walking in a straight line on the surface of the Earth. For simplicity’s sake, let’s assume he can walk over anything on the surface (including liquid, like water). In principle, if this person manages to walk long enough, he would eventually come back to the same point from where he started. By walking in a “straight line” on the surface of a sphere (Earth), a person is able to trace a CIRCULAR PATH!\nBasically, that’s the whole idea of relativity! Now, let’s come back to light and how gravity is able to bend it.\nBending of light and its aftermath\nLight normally travels in a straight line, but if you see light traveling in a curved path in outer space, using principles of general relativity, you are witnessing the bending of space and thereby light.\nTo grasp how gravity acts as a lens by bending the path of light, you need to understand a few things.\nThe sources of light in our universe are primarily stars. These stars emit light in all directions, but we can only see light as a point, because the light must enter your eyes through a narrow opening (which can be considered a ‘point’) for you to see it. Also, only a small amount of emitted light goes through the eyes. Thus, the light you SEE follows a very narrow path.\nAccording to Einstein’s theory of relativity, a massive body will distort the space around it, thereby bending the light that passes through this distorted space. Let’s try to understand how this works with an example. Imagine that you’re facing two stars that are in your line of sight, with one behind the other:\nThe corner star (the star farthest from you, shown in yellow in the image below) emits light that is much brighter than the light emitted by the middle star (shown in blue in the image below). This middle star would bend the light coming from the more distant star, such that you would get to see the distant star—although the view would be distorted—as something like seeing blurry two stars.\nSolar eclipse and the manifestation of gravitational lensing\nDuring the solar eclipse of 1919, physicists Arthur Eddington and Frank Watson Dyson confirmed to have observed gravitational lensing (and this was likely the first confirmation of gravitational lensing). Stars behind the Sun were observed to be shifted from their fixed positions. In reality, they weren’t really shifted, but the light coming from the stars was bent by the gravitational influence of the Sun!\nSince then, scientists have tried to leverage the power of gravitational lensing to peer deeper into the unexplored corners of our Universe. In fact, by harnessing the power of gravitational lensing, astronomers have even been able to ascertain the distant yet pristine galaxies formed just a few million years after the Big Bang. This method of discovering distant stars and galaxies was later termed gravitational microlensing. In gravitational microlensing, stars or even galaxies in the foreground act as a lens for stars or galaxies in the background. These foreground stars/galaxies will facilitate one’s view of background stars/galaxies, if they emit enough light. While their appearance and location would be distorted, the presence of a celestial entity behind a given star/galaxy can be confirmed.\nEinstein cross and Einstein rings\nIn 1985, using gravitational lensing, astronomers were able to see this highly intriguing effect for the first time, which was later called the Einstein cross. In an Einstein cross, four images of the same object (galaxy or galaxy cluster) are arranged perpendicular to each other when viewed from the lens. The lensing effect, in this case, was generated by the galaxy called ZW 2237+030.\nSimilarly, in 1988, scientists first observed another beautiful phenomenon called Einstein rings. This was the effect arising from gravitational lensing wherein a distant galaxy appeared to be warped by a much closer galaxy inside a circle like a halo.\nFinally, for those interested in studying the bizarre things of the Universe, gravitational lensing can be a powerful tool. Besides observing beautiful phenomena like an Einstein cross and Einstein rings, gravitational lensing can also be used to detect and study the mysterious material in the Universe called dark matter. It would take a complete article to explain what dark matter actually is, but to put it simply, it’s an invisible form of matter that has a mass. Unlike ordinary mass that we can see, dark matter doesn’t emit heat, light, or any other form of electromagnetic radiation. This quality is what makes it so stealthy. It was only when techniques like gravitational lensing were discovered that we have been able to learn more about the elusive and invisible presence.\nThe key thing about dark matter is that it has a mass, so this dark matter, like any other massive celestial body, bends light in its vicinity. This has helped astronomers in confirming the presence of dark matter. Interestingly, it is speculated that in total, dark matter is around five times more massive than all the ordinary matter of the universe put together! Studies like the Dark Energy Survey (DES) is ongoing and seeks to confirm these figures by studying dark matter using gravitational lensing. This study may be able to provide a precise map of matter in the universe, including the mysterious dark matter.\nIn closing, Einstein’s work on general relativity has paved the way for countless astronomers, allowing them to study the very structure of the Universe—both the visible and the invisible. Gravitational lensing, a simple but powerful tool, is helping us achieve that goal!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:60ccea3a-8495-4165-9c15-ee696cc86ffa>","<urn:uuid:25eaa060-3236-44a2-8cac-2970a019b7d7>"],"error":null}
{"question":"Why are native solitary bees better pollinators than honey bees?","answer":"Native solitary bees are two to three times more effective pollinators than honey bees. Unlike honey bees, they don't live in hives - 70% nest underground in tunnels and burrows, while 30% nest aboveground in holes in logs and stems. They spend most of their time pollinating flowers within a small area since they don't fly far from home.","context":["The super-pollinators of the garden are … native bees! While honey bees have their place, it’s our native solitary bees—such as mason bees and leafcutter bees—which are most vital to our flowers and food. Learn more about these amazing heroes of pollination—and see how to bring these docile bees to your garden.\nSolitary Bees: The Heroes of Pollination\nMost of us grew up learning about the sophisticated social structures of honey bees and bumblebees, and we’ve come to think that their lifestyle represents all bee behavior. The truth is, the world is home to more than 20,000 species of bees, and a whopping 90% of them do not live together in hives.\nInstead, most of the world’s bees are solitary, meaning they live alone. Unlike social bees, each female solitary bee has to gather pollen and nectar, build nests, and lay eggs all on her own, without the help of hundreds or thousands of doting workers. And although honey bees tend to get all the credit for keeping our crops going, native solitary bees are almost two to three times more effective pollinators!\nSo, if 90% of bees don’t live in hives, where do they live? Well, about 70% of solitary bee species nest underground in tunnels and burrows, while the remaining 30% nest aboveground, in holes in logs and stems.\nMason bee nesting holes capped with a layer of mud and clay. Photo by Dominicus Johannes Bergsma/Wikimedia Commons.\nTwo of the most common hole-nesting bee species used for crop pollination are alfalfa leafcutter bees and blue mason bees. In the wild, both species nest in pre-made holes, such as old grub tunnels, crevices in peeling bark, or broken branches. As suggested by their names, leafcutter bees use pieces of leaves to build their nests, while mason bees use mud or clay. Other types of hole-nesting bees, such as sweat bees and carpenter bees, prefer to excavate their own holes in the ground, logs, reeds, or the dead canes of raspberry bushes.\nA curious carpenter bee (Xylocopa spp.)\nMost solitary bees have a short lifespan as flying adults. Male mason bees only fly for about two weeks—just long enough to mate—and females only live a few weeks longer. With such a short adult lifespan, solitary bees have to use their time wisely! They do not have time to make honey, nor do they like to fly too far from home, meaning they spend the bulk of their time preparing their nests and pollinating flowers within a relatively small area. A big chunk of a solitary bee’s life is spent in their mother’s nesting site, hibernating over winter in their cocoons.\nMason bee (Osmia spp.).\nBackyard Bee Houses\nSome reports indicate that nearly 40% of bees are facing extinction today, leaving many people wondering what they can do to help. Fortunately, the best thing you can do is to start local, in your own backyard. Making your garden as bee-friendly as possible is as easy as adding things like native wildflowers and native bee nesting sites, including bee houses.\nSimilar to birdhouses, bee houses (or hotels) provide vital and otherwise missing nesting habitat. They are relatively simple in form, consisting of a birdhouse-like structure containing a series of exposed, reed-like tubes that the bees can lay their eggs in. Hole-nesting bees desperately search for appropriate nesting sites, sometimes even nesting in the ends of old garden hose nozzles, openings in metal garden furniture, or the hollow ends of wind chimes. Bee houses provide a more natural structure for the bees, and also allow for a bit of human assistance when necessary.\nBee houses can be an eye-catching addition to your garden.\nOn an annual basis, bee houses do need to be maintained and managed, or else they’ll become uninhabitable. Don’t worry—maintaining a bee house is pretty simple: Just remove the bee-cocoon–filled nesting materials and store them in a cool place over winter. Then, in the spring, remove the cocoons from the old materials and place them alongside new materials in your bee house. The new bee generation will emerge and get right to work. In exchange for pollinating all of our fruits and vegetables, a little housecleaning and maintenance is the least we can do!\nFor more advice on maintaining a bee house in your garden, see How to Maintain a Bee House to Increase Pollination.\nSolitary Bee Pests and Diseases\nLike any other animal, hole-nesting bees are susceptible to a number of pests, diseases, and predators. The three greatest threats that the bees face are pollen mites (they eat the bee larva’s food supply before the bee can), chalkbrood (a fungal infection that converts a larva into a mass of fungal spores), and parasitic wasps (gnat-sized wasps that lay eggs inside of healthy larvae).\nTo reduce the spread of pests and diseases in bee houses, simply harvest cocoons and separate healthy ones from infected nesting chambers. As you harvest cocoons, you’ll learn how to identify infected chambers and keep healthy cocoons safe.\nFor more information on native bees and bee houses, see our top tips for maintaining a backyard bee house.\nTo learn about keeping honey bees, see our Beekeeping 101 series."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c6eb2b25-a3ce-4b11-be5b-1bb8038ffc29>"],"error":null}
{"question":"What are the key differences between metric and imperial length units, and how is volume measured in the metric system using liters?","answer":"Metric length units use standard SI prefixes with the base unit meter (or metre), making them highly standardized. For example, 1,000 meters equals 1 kilometer (1km). In contrast, imperial units often have multiple abbreviations due to historical changes and the need to avoid confusion with metric units - for instance, 'mile' changed from 'm' to 'mi' to avoid confusion with meter. For volume measurement in the metric system, the liter is used as a non-SI but accepted unit. One liter equals one cubic decimeter (1 L = 1 dm³), and like length units, it can be used with SI prefixes (e.g., milliliter, kiloliter). The liter is commonly used for container capacity measurements, while cubic meters are typically used for items measured by dimensions or displacement.","context":["Metric unit abbreviations of length are defined by standard SI prefixes and the abreviation for the base unit of length in the SI system, the metre, m. For example 1,000 metres is 1 kilometre, which is written as 1km in abbreviated form. It is unusual for metric units to have non-standard abbreviations. Where there is a non-standard abbreviation it is often from an obsolete name for the unit.\nThe word metre is spelt in two different ways. Users of American English use the spelling 'meter', whereas others use 'metre' from the original French spelling.\nSI Abbreviation: ym\nSI Abbreviation: zm\nSI Abbreviation: am\nSI Abbreviation: fm\nSI Abbreviation: pm\nThe picometre was also called the micromicron (Abbreviation: μμ) prior to standardisation on the term picometre.\nSI Abbreviation: nm\nThe nanometre was also called the millimicron (Abbreviation: mμ) prior to standardisation on the term nanometre.\nSI Abbreviation: μm\nThe micrometre is often referred to as a micron with abbreviation μ.\nSI Abbreviation: mm\nSI Abbreviation: cm\nSI Abbreviation: dm\nSI Abbreviation: m\nSI Abbreviation: dam\nSI Abbreviation: hm\nSI Abbreviation: km\nThe kilometre is sometimes abbreviated to klick in military use (especially when spoken)\nSI Abbreviation: Mm\nSI Abbreviation: Gm\nSI Abbreviation: Tm\nSI Abbreviation: Pm\nSI Abbreviation: Em\nSI Abbreviation: Zm\nSI Abbreviation: Ym\nThe myriametre is an obsolete metric length unit equavalent to 10,000 metres.\nThere are many Imperial length units, most of which are obsolete in the modern world. Those that are in use often have more abbreviations than their metric counterparts. This is sometimes due to historical changes but in most part due to attempts to disambiguate the abbreviations from metric abbreviations. For example the abbreviation for the mile was formerly m, but this abbreviation is in use for the metre so the mdern standard abbreviation for the mile is mi.\nImperial length units can be split into two sections land units and nautical units.\nThis section covers common units, such as miles, yards, feet and inches, and many less common units.\nAlso known as the thousandth (as it is one thousandth of an inch) or sometimes in the USA as the mil. Plurals are thousandths and mils respectively.\nThe double quote symbol \" is used to indicate inches. e.g. 3\" for three inches.\nThe single quote symbol ' is used to indicate feet. e.g 5' for five feet.\nThe rod is a unit of length also known as the pole or perch (neither of which appear to have abbreviations either).\nCommonly used for lengths of horse races, this unit is often abbreviated as just f.\nCommonly abbreviated as m. This abbreviation is being discouraged as it is the abbreviation for the metre.\nThis section covers Nautical miles, fathoms and other units that are used at sea.\nThe standard abbreviation for the nautical mile M, is very rarely used with NM being preferred by the International Civil Aviation Organisation. The IEEE prefers nmi and nm is often used in naval applications (this abbreviation is also used as the abbreviation for the nanometre).","The litre or liter (see spelling differences) is a unit of volume. There are two official symbols: the Latin letter L in lower (l) and upper case (L). The liter appears in several versions of the metric system; although not an SI unit, it is accepted for use with the SI. The international unit of volume is the cubic meter (m³). One liter is denoted as 1 cubic decimeter (dm³).\nThe word \"liter\" is derived from an older French unit, the litron, whose name came from Greek via Latin. The original metric system used the liter as a base unit.\nA liter is defined as a special name for a cubic decimeter (1 L = 1 dm³). Hence 1 L ≡ 0.001 m³ (exactly). So 1000 L = 1 m3\nSI prefixes applied to the liter\nThe liter may be used with any SI prefix. The more often used terms are in bold in the table below.\n|Multiple||Name||Symbols||Equivalent volume||Multiple||Name||Symbols||Equivalent volume|\n|100 L||liter||l||L||dm³||cubic decimeter|\n|101 L||decaliter||dal||daL||10–1 L||deciliter||dl||dL|\n|102 L||hectoliter||hl||hL||10–2 L||centiliter||cl||cL|\n|103 L||kiloliter||kl||kL||m³||cubic meter||10–3 L||milliliter||ml||mL||cm³||cubic centimeter (cc)|\n|106 L||megaliter||Ml||ML||dam³||cubic decameter||10–6 L||microliter||µl||µL||mm³||cubic millimeter|\n|109 L||gigaliter||Gl||GL||hm³||cubic hectometer||10–9 L||nanoliter||nl||nL||106 µm³||1 million cubic micrometers|\n|1012 L||teraliter||Tl||TL||km³||cubic kilometer||10–12 L||picoliter||pl||pL||103 µm³||1 thousand cubic micrometers|\n|1015 L||petaliter||Pl||PL||103 km³||1 thousand cubic kilometers||10–15 L||femtoliter||fl||fL||µm³||cubic micrometer|\n|1018 L||exaliter||El||EL||106 km³||1 million cubic kilometers||10–18 L||attoliter||al||aL||106 nm³||1 million cubic nanometers|\n|1021 L||zettaliter||Zl||ZL||Mm³||cubic megameter||10–21 L||zeptoliter||zl||zL||103 nm³||1 thousand cubic nanometers|\n|1024 L||yottaliter||Yl||YL||103 Mm³||1 thousand cubic megameters||10–24 L||yoctoliter||yl||yL||nm³||cubic nanometer|\nThe milliliter is defined as one cubic centimeter and one-thousandth of a liter. It is a commonly used measurement, especially in medicine and cooking. Acceptable symbols for the milliliter are mL and ml.\n|Liter expressed in non-metric unit||Non-metric unit expressed in liter|\n|1 L ≈ 0.87987699||Imperial quart||1 Imperial quart||≡ 1.1365225 liter|\n|1 L ≈ 1.056688||US fluid quart||1 US fluid quart||≡ 0.946352946 liter|\n|1 L ≈ 1.75975326||Imperial pint||1 Imperial pint||≡ 0.56826125 liter|\n|1 L ≈ 2.11337641||US fluid pints||1 US fluid pint||≡ 0.473176473 liter|\n|1 L ≈ 0.2641720523||US liquid gallon||1 US liquid gallon||≡ 3.785411784 liters|\n|1 L ≈ 0.21997||Imperial gallon||1 Imperial gallon||≡ 4.54609 liters|\n|1 L ≈ 0.0353146667||cubic foot||1 cubic foot||≡ 28.316846592 liters|\n|1 L ≈ 61.0237441||cubic inches||1 cubic inch||≡ 0.01638706 liters|\n|See also Imperial units and US customary units|\nA liter is the volume of a cube with sides of 10 cm, which is slightly less than a cube of sides 4 inches (or one-third of a foot). Twenty-seven cubes \"one-third of a foot on each side\" would fit in one cubic foot, which is within 5% of the actual value of exactly 28.316846592 liters.\nOne liter is also slightly more than one U.S. liquid quart and slightly less than one Imperial quart or the less common U.S. dry quart.\nA measured cup is roughly 237 mL. However, this number is usually rounded to 250 mL to ease metrication of recipies using English units of measurement.\nLiters are most commonly used for items measured by the capacity or size of their container (such as fluids and berries), whereas cubic meters (and derived units) are most commonly used for items measured either by their dimensions or their displacements. The liter is often also used in some calculated measurements, such as density (kg/L), allowing an easy comparison with the density of water.\nOne liter of water has a mass of almost exactly one kilogram. Similarly: 1 milliliter of water has about 1 g of mass; 1,000 liters of water has about 1,000 kg (1 ton) of mass. This relationship is because the gram was originally defined as the mass of 1 mL of water. However, this definition was abandoned in 1964 because the density of water changes with pressure and the units of pressure are dependent on the definition of mass.\nOriginally, the only symbol for the liter was l (lowercase letter l), following the SI convention that only those unit symbols that abbreviate the name of a person start with a capital letter.\nIn many English-speaking countries, the most common shape of a handwritten Arabic digit 1 is just a vertical stroke, that is it lacks the upstroke added in many other cultures. Therefore, the digit 1 may easily be confused with the letter l. On some typewriters, particularly older ones, the unshifted L key had to be used to type the numeral 1. Further, even in some computer typefaces, the two characters are barely distinguishable at all. This caused some concern, especially in the medical community. As a result, L (uppercase letter L) was adopted as an alternative symbol for liter in 1979. The United States National Institute of Standards and Technology now recommends the use of the uppercase letter L, a practice that is also widely followed in Canada and Australia. In these countries, the symbol L is also used with prefixes, as in mL and µL, instead of the traditional ml and µl used in Europe. In the UK and Ireland, lowercase l is used with prefixes, though whole liters are often written in full (so, \"750 ml\" on a wine bottle, but often \"1 liter\" on a juice carton).\nPrior to 1979, the symbol ℓ (script small l, U+2113), came into common use in some countries; for example, it was recommended by South African Bureau of Standards publication M33 and Canada in the 1970s. This symbol can still be encountered occasionally in some English-speaking countries, and its use is ubiquitous in Japan and South Korea. Nevertheless, it is no longer used in most countries and no longer officially recognised by the Bureau International des Poids et Mesures, the International Organization for Standardization due to confusion and because it is often not (anyway) available in many documentation systems.\nIn 1795, the liter was introduced in France as one of the new \"Republican Measures\", and defined as one cubic decimeter.\nIn 1879, the Comité International des Poids et Mesures adopted the definition of the liter, and the symbol l (lowercase letter L).\nIn 1901, at the 3rd General Conference on Weights and Measures conference, the liter was redefined as the space occupied by 1 kg of pure water at the temperature of its maximum density (3.98 °C) under a pressure of 1 atm. This made the liter equal to about 1.000 028 dm³ (earlier reference works usually put it at 1.000 027 dm³).\nIn 1964, at the 12th General Conference on Weights and Measures conference, the original definition was reverted to, and thus the liter was once again defined in exact relation to the meter, as another name for the cubic decimeter, that is, exactly 1 dm³. \nIn 1979, at the 16th General Conference on Weights and Measures conference, the alternative symbol L (uppercase letter L) was adopted. It also expressed a preference that in the future only one of these two symbols should be retained, but in 1990 said it was still too early to do so.\nColloquial and practical usage\nIn spoken English, the abbreviation \"mL\" (for milliliter) is often pronounced as \"mil\", which is homophonous with the term \"mil\", meaning \"one thousandth of an inch\". This generally does not create confusion, because the context is usually sufficient — one being a volume, the other a linear measurement. The colloquial use of \"mil\" for millimeter for an ambiguous topic as in \"5 mils of rain fell since 9am\" may, however, be confusing.\nThe abbreviation cc (for cubic centimeter, equal to a milliliter or mL)) is a unit of the cgs system, that preceded the MKS system, that later evolved into the SI system. The abbreviation cc is still commonly used in many fields including (for example) sizing for motorcycle and related sports for combustion engine displacement.\nIn European countries where the metric system was established well before the adoption of the SI standard, there is still carry-over of usage from the precursor cgs and MKS systems. In the SI system, use of prefixes for multiples of 1,000 is preferred and all other multiples discouraged. However, in countries where these other multiples were already established, their use remains common. In particular, use of the centi (10-2), deci (10-1), deca (10+1), and hecto (10+2) prefixes are still common. For example, in many European countries, the hectoliter is the typical unit for production and export volumes of beverages (milk, beer, soft drinks, etc); deciliters are found in cookbooks; centiliters indicate the capacity of drinking glasses and of small bottles. In colloquial Dutch in Belgium, a 'vijfentwintiger' and a 'drieëndertiger' (literally 'twenty-fiver' and 'thirty-threer') are the common beer glasses, the corresponding bottles mention 25 cL or 33 cL. Bottles may also be 75 cL or half size at 37.5 cL for 'artisanal' brews or 70 cL for wines or spirits. Cans come in 25 cL, 33 cL and 50 cL aka 0.5 L. Family size bottles as for soft drinks or drinking water use the liter (0.5 L, 1 L, 1.5 L, 2 L), and so do beer barrels (50 L, or the half sized 25 L). This unit is most common for all other household size containers of liquids, from thermocans, by buckets, to bath tubs; as well as for fuel tanks and consumption for heating or by vehicles.\nIn countries where where the metric system was adopted as the official measuring system after the SI standard was established, common usage more closely follow contemporary SI conventions. For example, in Canada where the metric system is now in wide-spread use, consumer beverages are labelled almost exclusively using liters and milliliters. Hectoliters sometimes appear in industry, but centiliters and deciliters are rarely, if ever, used. Larger volumes are usually given in cubic meters (equivalent to 1 kL), or thousands or millions of cubic meters. The situation is similar in Australia, although kiloliters, megaliters and gigaliters are commonly used for measuring water consumption, reservoir capacities and river flows.\nFor larger volumes of fluids, such as annual consumption of tap water, lorry (truck) tanks, or swimming pools, the cubic meter is the general unit, as it is for all volumes of a non-liquid nature. There are a few exceptions in which the liter is used for rather large volumes, such as the irregularly shaped boot of a car or the internal size of a microwave oven.\n- \"Appendix C: General tables of units of measurement\". NIST Handbook 44: Specifications, Tolerances, and Other Technical Requirements for Weighing and Measuring Devices. National Institute of Standards and Technology. 11 November 2000. Retrieved 9 October 2006.\n- Bureau International des Poids et Mesures (2006). The International System of Units (SI) (PDF). p. 159.\n- BIPM's \"SI Brochure\"\n- BIPM's \"(Table 6 -) Non-SI units accepted for use with the International System\"\n- NIST note on SI units\n- NIST recommends uppercase letter L\n- UK National Physical Laboratory's \"Internationally recognised non SI units\" page\nals:Liter ar:لتر ast:Llitru zh-min-nan:Li̍p br:Litr bg:Литър ca:Litre cs:Litr da:Liter de:Liter el:Λίτρο eo:Litro eu:Litro fa:لیتر gl:Litro ko:리터 id:Liter is:Lítri it:Litro he:ליטר lv:Litrs lb:Liter lt:Litras hu:Liter mk:Литар mn:Литр ml:ലിറ്റര് nl:Liter no:Liter nn:Liter sco:Litre simple:Litre sk:Liter sl:Liter sr:Литар sh:Litar su:Léter fi:Litra sv:Liter th:ลิตร uk:Літр (одиниця виміру)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:6905d18b-c3cd-4d88-b491-60bea8643657>","<urn:uuid:3a7587b6-22ed-4889-a5b5-613942507a6c>"],"error":null}
{"question":"What technical considerations go into selecting mining valves, and what environmental factors must be monitored in mining areas?","answer":"Valve selection requires evaluation of multiple factors including slurry solids composition, chemical composition, percent solids (typically 30-60%), pressure and temperature requirements, and seal type needed. The selection must account for wear from abrasion, friction between components, and pressure forces. For the environmental aspect, mining operations require monitoring of several critical factors: soil erosion, sinkhole formation, loss of biodiversity, chemical contamination of soil and water, and deforestation impact. These environmental issues can extend beyond the immediate mining area, affecting surrounding forest areas and threatening local plant and wildlife populations.","context":["Slurry valves are to the mining and other process industries what midfielders are to soccer teams: They are not often heralded, but they are hard working. For managers of such operations, victory means maintaining uptime in processes that often involve heavily abrasive and corrosive materials. Keys steps to winning any valve challenge include:\n- Understanding the specifics of the application.\n- Picking the right pumps and valves for the job.\n- Considering total cost of ownership over the life of the equipment.\nDefining Slurry ApplicationsSlurries are used to move solids through refining and other industrial processes by mixing them with water. Though slurries are most common in the mining industry, other process industries -- pulp and paper, chemical manufacturing and power generation -- also include applications for slurry valves.\nNickel mining, for example, involves digging and blasting ore and dirt out of a surface mine. Depending on the particular facility, the ore and dirt may be mixed with water to form slurry for transport. The fertile slurry travels from the surface mine to the processing facility. After the nickel is removed from the slurry at the processing facility, the leftovers, which are called tailings, are sent as a slurry to the tailings pond for water recovery. Heavy-duty slurry pumps and slurry valves are required to move the nickel-rich slurry and waste tailings where they need to go in the process.\nThe abrasiveness of the solids in the slurry is a key consideration in determining the right type of valve for each job. When a valve is in the open position, the solids will continuously scrape against a valve’s port like a metal file or a sandblaster. When the valve is closing, the gate will be exposed to the same abrasion as the port. Depending on the weight of the solid and particle size, the abrasiveness can be extreme.\nSome mining applications also involve corrosive chemical processes. In copper mining, for example, sulfuric acid is used to attract particles of copper as part of the refining process. The acid has the potential to corrode metal valve parts.\nLikewise, the production of acid itself creates the need for tough slurry valves because of these same issues. For instance, the manufacture of phosphoric acid creates a buildup of waste, called scaling, around the valve’s gate and port area that makes it harder to open and close the valve. In addition, the manufacturing process involves periodically flushing the system with sulfuric acid, which is highly corrosive and requires valves made of specially coated materials and special alloys. In fact, dense scaling in phosphoric acid production can cause the average valve to fail on its first closure.\nTypes of Slurry ValvesJust as each soccer coach customizes his own playbook, the managers of slurry processes can choose from an array of types, sizes and material options when selecting valves to find those that best suit his application. Most slurry valves can be classified into one of three categories based on the functioning of the valve’s final closure element.\n- In a knife gate valve, a relatively sharp metal gate pushes down and cuts through the slurry to create closure.\n- Somewhat similar to a knife gate, a ported slide gate has a hole or port in the gate for the media to flow through in the open position. When valve closes, the port slides out of the bottom of the valve, allowing the upper portion of the gate to stop the media flow.\n- In a butterfly valve, the gate remains in the slurry, parallel to the piping, and allows the slurry to pass through when the valve is open. When it is closed, the gate turns perpendicular to shut off the flow. Because the abrasion against the butterfly gate is constant, this style of valve is not suited to many slurry applications.\nThe valve seats also include a range of choices. They include metal-to-metal seats, which are more durable but tend to allow some leakage, and other sealing materials, which can provide drip-tight shutoff but must be selected carefully for durability and chemical compatibility. Many knife gate valves with elastomer seating have seat material options that can withstand temperatures around 300 to 400°F (149 to 200°C) and pressures up to 400 psi. This makes them suited to most typical slurry applications in mining and paper processing. For applications involving higher pressures and temperatures, some knife gate valves and ported slide gate valves are designed to meet ANSI Class 300 pressure limits and exceed 1,800°F (982°C).\nPorted slide gate valves most often are used with very heavy solid content media, both in wet slurries and dry applications. For instance, these valves are found in coal handling and high consistency stock in the pulp and paper industry. The sliding gate cuts through the thick slurry more easily than knife gates. These gates also cut through heavy scaling effectively, and the design lends itself to a long seat life because little stress is put on the seat from the way the gate closes.\nSlurry applications require valve designers to think about the three primary forces that can cause valves to malfunction: wear, friction and pressure. Wear is caused by the abrasion or corrosion of the slurry itself. The valve gates and seats must be made of materials (and coated if necessary) to withstand the materials in the process. Friction is another force. Whether it is metal-to-metal, metal-to-rubber, or some other combination, the sliding of the gate against the valve seat creates friction that stresses the valve components. The third force is pressure. Closing a valve under pressure is more difficult and requires harder materials. In designing slurry valves, the impact of all of these competing forces must be considered together. In choosing a harder material for a valve seat to withstand abrasion, for example, engineers must recognize that harder materials are more brittle and likely to break under pressure.\nChoosing a Valve for an ApplicationChoosing the appropriate valve requires matching the range of valve types, sizes and materials to the specific needs of each application. Fortunately, all valve-shoppers can consider a few common variables to help narrow the options. They include:\n- Slurry solids composition, which is the size of particles and hardness/abrasiveness of the material being processed.\n- Slurry chemical composition, which is the specific chemical and percent concentrations of the slurry. The specific concentrations greatly affect the material of construction choices.\n- Percent solids, which is the percent of the slurry material by weight that is solids. Typical slurry applications are 30 to 60 percent solids; extremes can be 60 to 70 percent solids, or even higher.\n- Pressure and temperature can be a factor to consider in some applications. For instance, bottom ash handling valves in the power generation industry must handle temperatures as high as 1,500°F (815°C), and high pressure tailings pipelines operate at pressures exceeding 650 psi.\n- The type of seal required also can affect the selection process. For example, is a drip-tight seal required at the specific point in the process that the valve will be used, or is some small leakage around the valve seat acceptable?\nThe initial purchase price of the valve is, of course, another important consideration along with the needs of the process. It is important for valve purchasers to make financial decisions based on total cost of ownership rather than simply the initial purchase price of the valve assembly. Total costs include:\n- Consumables (the gate, seat liner, seat/seals and packing).\n- The expected life of the product.\n- The risk of downtime in the event of catastrophic failure.\nJust as soccer coaches must carefully choose midfielders to have winning teams, mining and other process industries managers must take special care in selecting slurry valves to meet production and profit goals. To keep the total lifetime cost of a valve to a minimum, valve shoppers need to search for a valve with the right characteristics to match the unique application.","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:664a105a-8e28-46d1-9688-3c52aa1a4ceb>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"Which artistic works from 2007 and 2020 demonstrate Brazilian-German cultural connections?","answer":"In 2020, the video project 'Eu não falo português' explored Brazilian-German connections through the story of a German grandmother (Marlise Barth) who lived in Brazil from 1942-1947, created by German sisters Claudia and Isabelle Barth. From 2007, the German connection is represented in the Universal Germany compilation 'Seine grossen Erfolge' featuring the Brazilian composition 'Desafinado' by Jobim, demonstrating how Brazilian music reached German audiences.","context":["Sunday, 21 June17:00\nRehearsal of Eu não falo português, 2020\nMonday, 22 June10:00\nPerformance Space Compilation\nMonday, 22 June17:00\nExhibition Space Compilation\nTuesday, 23 June14:00\nTuesday, 23 June20:00\n2020, full HD transferred to MP4, 16:9, two-channel presented as one split screen video, 41'4'', stereo\nTwo sisters, Claudia and Isabelle Barth, research the life story of their grandmother Marlise Barth (1921-2016). They inherited several of her carefully curated photo albums which contain two blank pages of the album “Brazil 1942-1947” prompting the sisters to question and interview many family members as to why, and under what circumstances their grandmother had lived in Brazil. The sometimes conflicting explanations exposed a colorful life of intrigue and adventure from childhood years on to the time she lived in Brazil and was married to the son of a successful German Industrialist. From the new stories that were never talked about in the family before, Claudia and Isabelle created their first performance in 2017 by the name of fantôma oder die gespaltene Frau*\".\nA year later in 2018 the stories of their grandmother’s life still left open many questions and mysteries. The two sisters decided to travel to Brazil with a camera to retrace her life and hopefully fill in the blanks of those two pages in the photo album where the pictures are missing. Claudia and Isabelle visited many familiar and unfamiliar members of the Brazilian family, sought out places where Marlise stayed, researched archives and ended up with many more stories, pictures and documentations of her life.\nAfter their journey the sisters began to write a video script named Eu não falo português – I don’t speak Portuguese. The text grew into an imaginary road movie which followed along the traces of their journey in the style of a stream of consciousness or the flow of a poem.\nThe stories that were filling the blank pages of their grandmother’s photo album ended up blending with their own experiences and imagination which then became the stage of a new story.\nAfter presenting the script as a performance reading at the Maxim Theater in Zurich, the Zeitraumexit in Mannheim and the Gallery D21 in Leipzig, other works emerged out of the research: eu-você (Me-You), 2019 premiered at the festival ‘Soy Loco Por Ti Juquery’ in São Paulo and was also showed at the ‘EinTanzhaus’ in Mannheim. Two videos that were presented during the performance of \"eu-você\" originated out of the images from Brazil. Where she speaks I smell, a three-channel video installation and the single-channel video my grandmother spoke:. The two videos were recently shown in a solo exhibition of Claudia Barth at ’Raum*station’ in Zurich.\nFor the MA graduation at Contemporary Arts Practice (CAP) in Bern University of the Arts (BUA), Claudia and Isabelle rewrote the text and adapted the choreography into a new version of Eu não falo português. Together with Brazilian artist Dandara they produced a two-channel video which is currently shown as a one-channel screening on this page.\nCredits:Concept & Text: Claudia & Isabelle BarthPerformance: Dandara, Claudia BarthCamera: Arina HeusslerEditing: Claudia BarthAudio Recording: Nora RinggenbergSoundmix: Markus Fehlmann“Garota de Ipanema remix”Song composition and Sound: DandaraSoundmix: Maurício Caruso“Andança” Song by Dandara Quotes appearing in the script by: Quinn Latimer, Kathy Acker, Per Olov Enquist, Vinícius de Moraes, Oswald de Andrade and the influence of many many more.","Antonio Carlos Jobim - Universal and Universal International (continued)\nMax Greger, \"Seine grossen Erfolge\", Universal [Germany] #UNI302064 (2007).[CD Compilation]\nThis CD Compilation contains the Jobim composition \"Desafinado\".\nAntonio Carlos Jobim, \"Clasico/03\", Universal [Argentina] #9809127 (2007).[CD Compilation]\n1. Garota de Ipanema\n2. Agua de Beber\n3. Vivo Sonando\n4. O Morro Nao Tem Vez\n7. Samba de una Nota So\n9. So Danco Samba\n10. Chega de Saudace\n12. Aguas de Marco\n13. Felicidade, A\nSergio Mendes, \"Coffee & Bossa\", Universal [Philippines] #060249821503 (2007).[CD Compilation]\nCompilation features Jobim's \"Agua de Beber\" and \"Waters of March\".\nVarious Artists, \"Brazilidade - Selected by Gira Mundo\", Universal [Japan] #UCCU-1157 (2007).[Limited Edition CD]\nTracks composed by Jobim are listed on the tracklist below.\n1. ESPERANCA/ TRIO MOCOTO\n2. ATE LONDRES/ ROSINHA DE VALENCA\n3. ZAZUEIRA/ ELIS REGINA\n4. BATUCADA/ MARCOS VALLE\n5. MAIS QUE NADA/ SERGIO MENDES & BRASIL'66\n6. JANGADA/ EDU LOBO\n7. VIVO SONHANDO(DREAMER)/ LUIZ HENRIQUE (composed by Jobim)\n8. ROSA,MENINA ROSA/ JORGE BEN\n9. LARANJA DE CHINA/ NARA LEAO\n10. MOSQUITO(FLY)/ JOÃO DONATO\n11. O HOMEM,QUE MATOU O HOMEM,QUE MATOU O HOMEM MAU/ LANNY GORDIN\n12. AQUARELA DO BRASIL/ GAL COSTA\n13. O CRIOLAUTA/ TRIO MOCOTO\n14. O MULHER/ MARKU\n15. COISA NO7(QUEM E QUE NAO CHORA?)/ MOASIR SANTOS\n16. FESTANCA/ MARIANA AYDAR\n17. SAMBA E TUDO/ CELSO FONSECA\n18. DINDI/ ASTRUD GILBERTO (composed by Jobim)\n19. LA VOU EU/ CARLOS LYRA\n20. AMOR BRASILEIRO/ VINICIUS CANTUARIA\n21. CATAVENTO/ MILTON NASCIMENTO\nAstrud Gilberto, \"Astrud Gilberto - The Diva Series\", Universal [Japan] #4081 (2007).[CD Compilation]\nThis compilation contains several tracks composed by Jobim and may also contain performances by Jobim.\nCompilation Producer: Bryan Koniarz.\nSarah Vaughan, \"Sarah For Lovers\", Universal [Japan] #4101 (2007).[CD Compilation]\nThis compilation contains Jobim's \"Corcovado\" composition.\nLiner Notes by Al Young.\nCompilation producer: Bryan Koniarz.\nVarious Artists, \"The Bossa Nova 4 Ever\", Universal [Japan] #4204 (2007).[CD Compilation]\nJobim compositions included in this compilation include:\nThe Girl From Ipanema (with Jobim on piano)\nSamba De Uma Nota Só (performed by Jobim)\nEste Seu Olhar\nSo Danço Samba (with Jobim on piano)\nSo Tinha Ser Com Voce\nChovendo Na Roseira (with Jobim performing)\nAgua de Beber (performed by Jobim)\nVarious Artists, \"Bonita! Bossa Nova\", Universal [Japan] #4346 (2007).[CD Compilation]\nThis compilation includes Walter Wanderley performing Jobim's \"Wave\" composition.\nThis compilation also includes these other Jobim compositions:\nAgua de Beber\nGarota de Ipanema (with Jobim on piano)\nSamba de Uma Nota So\nSo Danço Samba\nTriste (performed by Elis Regina & Tom Jobim)\nChega de Saudade (performed by Jobim)\nSergio Mendes, \"Dance Moderno\", Universal [Japan] #6821 (2007).[CD Reissue]\nThis album contains Jobim's \"Outra Vez\".\nOba La La\nLove For Sale\nTristeza de Nos Dois\nWhat Is This Thing Called Love?\nOlhou Para Mim\nTema Sem Palavras\nOn Green Dolphin Street\nOutra Vez (Jobim)\nIn Ruy Castro's book \"Bossa Nova\", he states:\n[Armando] Pittigliani, [of Philips Records] had\ninvited Sergio Mendes to cut a disc at his recording company--a dance\nrecord. Mendes had resisted: he wanted to make a jazz record.\nPittigliani managed to persuade him to change his mind, and Dance\nModerno was made, but Mendes styled the arrangements with so many\ncomplicated, jazzy innovations that nobody could dance to it.\"\nPersonnel: includes Sergio Mendes, (piano);\nEdson Maciel (trombone), Durval Ferreira (guitar), & Bebeto\n(sax/flute/bass), Edson Machado (drums).\nBack to Recordings Index\nBack to Site Index\nBack to Home Page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:ebe3691d-94ec-429e-bb8d-06178e2eb4ad>","<urn:uuid:c22fbcb4-d44c-4953-93fd-1d963df6fdfa>"],"error":null}
{"question":"What are the elevation contrasts between Mount Logan and Prince William Forest Park's terrain?","answer":"Mount Logan stands at 19,551 feet and is the second-highest peak in North America, located in Canada's Kluane National Park Reserve. In stark contrast, Prince William Forest Park has much more modest elevations, ranging from approximately 10 feet to just under 400 feet above sea level. The park's terrain consists of undulating topography with narrow ridgetops and relatively steep-sided valleys, while Mount Logan is characterized by its massive height and serves as the source of the Logan and Hubbard Glaciers.","context":["Visitor Center Remodel 2014\nOver the next several months there will be new changes coming to the Visitor Center. Presently we are remodeling the bookstore area to give it more of a country theme. Next the exibit area will get all new exhibits. Thank you for your patience and support\nOak Ridge Campground and Chopawamsic Backcountry Closure\nOak Ridge Campground and Chopawamsic Backcountry area will be closed December 1st, 2013 to February 28th, 2014.\nOutside firewood is prohibited in Prince William Forest Park, unless it is certified USDA 'bug free' firewood. Dead and downed wood may be collected from designated areas for use while in the park. Help us protect the forest from invasive species!\nNatural Features & Ecosystems\nNational Park Service\nThe park also contains two physiographic provinces, the Piedmont and the Coastal Plain, and it straddles the southern and northern climates - a transition zone that supports many species to the outer limits of their ranges. This creates a wide diversity of habitat, vegetative communities, and species composition not generally found in any single forest type. Prince William Forest Park is approximately two-thirds in the Piedmont and one-third in the Coastal Plain. The topography is undulating, with narrow ridgetops and relatively steep-sided valleys. The park is underlain by late Precambrian to early Paleozoic rocks, which are overlain in the eastern part of the park by unconsolidated Cretaceous period deposits. The soils of the park are sandy, relatively infertile, and easily disturbed. The steep terrain and poor quality soils combine to create severe erosion problems. Learn more about the park's Geologic Formations.\nRelief is moderately high, and the elevation ranges from about ten feet to nearly 400 feet above sea level. Ridgetops are narrow to moderately wide and nearly level to gently sloping. Side slopes are moderately wide to narrow and sloping to very steep. In the Piedmont, the geology consists largely of granite gneiss, hornblende gneiss and mica schist rock types. The ridges of the Piedmont are capped with thin mantels of coastal plain or other alluvial sediments in many places. Fairly broad floodplains have developed along the larger streams. The coastal plain is underlain by stratified marine sediments of sand, silt, clay and gravel. The lowland soils are strongly acidic and of low natural fertility. The soils have low permeability, making them subject to at least seasonal wetness.\nThe slopes and gently sloping ridges are occupied by more porous soils that are more easily eroded. They also are strongly acidic and of low fertility. Unconsolidated soil types are generally located in the coastal plain, coastal plain caps, floodplains, and floodplain and stream terraces. The erosion potential in these areas ranges from moderate to high. Outcrops of folded and faulted rock are scattered throughout the park, and they dip nearly vertically in some areas, especially along streambeds. Many of the faulted rocks may represent the fall line, a unique geological feature where streams form falls or rapids as they leave the harder rocks of the piedmont and enter the softer rocks of the coastal plain.\nIn many places the ridges of Piedmont areas are capped with thin mantels of coastal plain or other alluvial sediments, and fairly broad floodplains have developed along larger streams. In addition to its geological diversity and observable geological processes, the park has large mineral deposits, in particular pyrite and associated minerals. The largest concentration of pyrite is at the confluence of the main branches of Quantico Creek, and the water's interaction with exposed mineral formations has formed unusual compounds and crystalline formations.\nDid You Know?\nPrince William Forest Park protects the federally threatened orchid Isotria medeoloides, small whorled pogonia, of which several colonies have been identified in the park.","What Is The Highest Mountain Peak In North America – Denali is the highest mountain in North America at 19,685 feet or 6,000 meters. The next three highest peaks exceed 5,500 m, while 21 peaks exceed 4,500 m. Of the 200 highest peaks in North America, 160 are in the United States and 30 are in Canada. Mexico has the most peaks on the continent with 11 of the next 200 peaks. Seven of the 200 highest peaks in North America are located on the border between the United States and Canada\nLocated in eastern Alaska, Mount Bona is the fifth highest mountain in the United States and the 10th highest on the continent. The mountain is an ice-covered stratovolcano and the country’s highest volcano, Mount Boa is the source of the Clotlan Glacier and other smaller glaciers It also supplies ice to the Russell Glacier system The first successful ascent of Mount Boa was in 1930\nWhat Is The Highest Mountain Peak In North America\nKing Peak, the ninth highest mountain in North America, was first climbed by several students from the University of Alaska in Yukon, Canada on June 6, 1952.\nHas Anyone Really Summited The World’s 14 Highest Mountains?\nIztachihutl, a 17,160-foot-tall dormant volcano, is located in Mexico. This mountain is located on the border of the Mexican state of Puebla, Mexico. The four peaks of Iztachihutl are snowy and on a clear day the mountain can be seen from Mexico City, which is 70 km away from the mountain. Iztachihut was first climbed to the top in 1889. But there is an assumption that local residents have climbed the mountain before.\nMount Lucania is the seventh highest mountain in North America This mountain is located in Yukon, Canada It is connected to Table Mountain by a long ridge Mount Lucania is 17,257 feet high and is named after the ship RMS Lucania. It is the third highest peak in Canada and the 15th highest peak on the continent.It was first climbed in 1937.\nMount Foraker is a 17,400-foot mountain located in Denali National Park in Alaska. The northern peak of the mountain was first climbed on August 6, 1934, and the southern peak was conquered four days later. The mountain was named after US Senator Joseph B. Foraker.\nPopocatepetl, an active volcano, is located in Mexico and has an elevation of 17,802 feet. It is the fifth highest peak in North America and the second highest peak in Mexico.The mountain is located about 70 kilometers from Mexico City and is often visible from the city.\nBlack Mountains (north Carolina)\nLocated on the border between the US state of Alaska and the Canadian Yukon Territory, Mount St. Elias is located about 40 kilometers southwest of Mount Logan, the second highest peak in both countries. On July 16, 1741, Mount St. Elijah was first discovered by the Russian Vitus Bering. The mountain rises to an elevation of 18,008 feet The mountain was first climbed on July 31, 1897 by Prince Luigi Amadeo di Savoia and his team.\nThe highest mountain in Mexico and the second highest volcanic peak in the world, Pico de Orizaba is the third highest peak in North America. The dormant stratovolcano rises 18,491 feet above sea level. Mount Pico de Orizaba, located in the eastern part of the Trans-Mexican Volcanic Belt, last erupted in the 19th century.\nLogan, the second highest peak in North America, at 19,551 feet The mountain is located in the Kluane National Park Reserve in the Yukon Territory of Canada and is the highest mountain in the country. Mount Logan is named after Canadian geologist William E. Logan. Logan and Hubbard Glaciers Begin at Mount Logan Due to tectonic movements, the mountains are in a state of growth and gain height over time.\nMount Denali is the highest peak on the North American continent The mountain rises to a height of 20,310 feet The mountain is famously the third highest peak in the world Mount Denali, the central feature of Denali National Park in Alaska, is also the third most isolated peak in the world. Denali was first climbed on June 7, 1913 by four climbers who completed the South Summit route. Currently, the most popular route to the summit of Denali is the West Buttress Route, first used by Bradford Washburn in 1951. The United States is home to a variety of spectacular landscapes, including some of the highest and tallest mountains in North America.\nAlaska Visit Denali National Park To See Stunning Array De Wildlife Y Gaze En Mt Mckinley La Highest Mountain Peak In North America With An Elevation De Pies Mount Mckinley Y Denali\nWhen it comes to mountains, no other state has peaks like Alaska Because of their remote location in the Last Frontier, some of these mountains are rarely visited, but they retain their awe-inspiring status as distinctive features of Alaskan geography.\nOf the 11 highest mountains in the United States, 10 are in Alaska, the last being Mount Whitney, which is part of the Sierra Nevada mountain range in California.\nDenali is the tallest mountain in the United States at 20,310 feet (6,190 meters). Denali is also the highest mountain in North America and the third highest of the Seven Summits (short for the highest mountain on each continent), after Mount Everest and Mount Aconcagua.\nLocated in the Alaska Range, Denali and the surrounding mountains were formed as a result of intense tectonic activity, and the height of the mountain is gradually increasing every year.\nThe Mount Mitchell Hike: The Highest Mountain In North Carolina\nThis feature was formed as a result of the convergence of the Pacific and North American tectonic plates, which caused the earth’s crust to rise in this area.\nAt about 18,000 feet (5,500 m) above Everest, Denali is one of the highest base climbs on Earth.\nDenali is the Athabaskan word koukon, meaning “high,” and the mountain has been of great importance to the Alaska Natives who have lived around it since ancient times.\nThe mountain is the centerpiece of Denali National Park and Preserve, a protected area of more than 5 million acres of wilderness that is home to a variety of wildlife, including grizzly bears, wolves, caribou, and bighorn sheep.\nThe Highest Mountains In Europe: The Top 10\nAdditionally, Denali National Park and Preserve is one of Alaska’s most visited destinations, attracting tourists from around the world during the summer months. Many of the park’s wildlife can be seen on the park’s unique trails\nMount St. Elias crosses the Alaska-Canada border in the St. Elias Range in northeastern Alaska. At 18,009 feet (5,489 m), it is the second highest mountain in the United States and Canada, after Mount Logan.\nMount St. Elias is part of the Mount St. Elias Range, part of the Pacific Coast Range that stretches from southeastern Alaska through the Yukon to northern British Columbia.\nMount St. Elias, Wrangel-St. Elias National Park and Preserve, the largest national park in the United States, is home to nine of the 16 highest peaks in the United States.\nUtah Mountain Ranges [maps & Recreation]\nThe mountain range is located near Glacier Bay, where there is a high density of glaciers and ice fields, and tectonic and volcanic activity, which has helped make the mountain range popular for current and past research.\nMount St. Elias, Lake Agassi, the confluence of the Libby Glacier and the Agassi Glacier. Photo: NPS/J. Frank is in the public domain.\nMount St. Elias saw its first recorded summit in 1897 by an Italian expedition. On July 31, 1897, an Italian team led by Prince Luigi Amedeo, Duke of Abruzzi, accompanied by American guides and climbers.\nMount Foraker is the third highest peak in the United States at 17,400 feet (5,304 m).\nDenali Claims To Be Tallest Mountain In World\nMount Foraker, located in the central Alaska range of Denali National Park and Preserve, is 14 miles from Denali, the highest peak in North America.\nMount Foraker also sits on a branch of the Kahiltna Glacier, the longest glacier in the Alaska Range, opposite Mount Denali and Mount Hunter.\nLook at the Cahiltana Glacier with the Crosson and Foraker Mountains on the right.In the center of the photo is the confluence of the main Cagiltana and the Northeast Branch, which is a larger opening in the lower left. Photo: NPS/Tucker Chenoweth, public domain\nThe first successful ascents of the North and South Peaks were made in 1934 by Charles Houston, T. Recorded by Graham Brown and Chichelle Waterston.\nThrough The Lens: Mount Denali, North America’s Highest Peak, In Photos\nMount Bona is a dormant stratovolcano located in the St. Elias Mountains. Mount Bona is located in the eastern part of the Alaska Range in Wrangel-St. Elias National Park and Preserve, the largest national park in the United States.\nAt 16,550 feet (5,\nWhat's the highest peak in north america, highest mountain peak in america, highest mountain peak in north america, the highest mountain peak in north america, what is the highest peak in north carolina, what is the highest peak in alaska, what is the highest peak in north america, what is the highest mountain peak, what is the world's highest mountain peak, highest peak in north america, what is the highest peak in europe, what is the highest mountain peak in south america\nHello my name is Sophia, Wellcome to my blog"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8bd3dcb9-2a68-4789-a3c3-47a2c040aebe>","<urn:uuid:1a9b6acc-c490-4323-a136-1b38847d08e6>"],"error":null}
{"question":"Why can't we use peak voltage or average voltage to measure AC power?","answer":"Peak voltage can't be used because the waveform is only at its peak for an instant, making power calculations incorrect. The average value also doesn't work because for any 'pure' AC that is symmetrical and spends equal time above and below zero, the average comes out to exactly zero. This clearly isn't accurate since we know AC power does work, like heating up a resistor.","context":["\"Bob Myers\" <nospamplease@xxxxxxxxxxxxxxx> wrote in message\n> \"RR\" <newspaper.20.broom@xxxxxxxxxxxxxxx> wrote in message\n>> As I understand it, RMS is calculated on the area under the sine or\n>> wave for the AC supply and the peak voltage.\n> Sort of, but you'll get a better idea of what it's really all about\n> by going through what \"RMS\" stands for in the first place.\n> The problem is one of determining the \"effective\" voltage, current, or\n> whatever of an alternating source; in other words - and to use the\n> popular example - if I pass an AC current through a resistor, how much\n> power is dissipated in that resistor? How can I compare AC to DC in\n> this sense?\n> You clearly can't use the peak voltage or current - the waveform isn't\n> at the peak but for an instant, so obviously doing a power or some\n> other such calculation based on that value would be wrong. The next\n> idea would probably be to try to find the \"average\" value of the\n> waveform, but that winds up even worse - if you average any \"pure\"\n> AC (meaning that it is symmetrical, regardless of the form of the\n> and spends as much time above zero as below), you get a result of\n> exactly zero. That's obviously not right, either, since the resistor\n> heat up.\n> So instead, we start by squaring the function that describes the AC\n> waveform; if you square such a wave, everything winds up above\n> zero, right? Then find the average, or mean, of the squared waveform\n> (so now you have a constant value), and to correct for the squaring\n> operation you did in the first place, find the square root of that\n> In short, you are finding the (square) Root of the Mean of the Square.\n> \"RMS,\" see?\n> It happens to work out to 0.707 peak for a pure sinusoid; other\n> waveforms wind up with different fractions of the peak value, and\n> this is really only a shortcut which can be applied to those examples\n> where one of these \"regular\" waveforms is in question. But the\n> above process - square the wave, average it, and take the square\n> root of the result - applies to all.\n> Obviously, frequency does not enter into this at all - RMS is 0.707\n> of peak for ANY sinusoid, regardless of frequency.\n>> I've attempted to follow the calculations here:\n>> and the frequency seems to always reduce to a factor of 0.5.\n> The final result is just a numeric value - it has no frequency (that\n> is the result of averaging the squared waveform - an average has\n> no frequency, since it's a constant value, right?). You seem to get\n> into some unexpected frequencies during all of this because the\n> squared waveform is itself a periodic wave with a frequency 2X\n> the original (at least for anything but a 50% duty-cycle square\n>> So, why does an appliance designed for 240volts 50Hz care whether you\n>> 240volts 60Hz or 240volts 100Hz, for that matter?\n> When an appliance \"cares\" about the line frequency, one of two\n> things are generally at issue:\n> 1. The appliance relies on the frequency of the line in order to\n> run at the proper speed - e.g., the motor in an electric clock.\n> 2. The appliance contains components (typically, transformers or\n> similar magnetics) which are designed to operate at one frequency,\n> and will be less efficient at others.\n> Bob M.\nWhere you a trained teacher?\nThats one of the best replies I have seen here to a very basic question.\nSometimes the posters get carried away with their own imagined expertise\nand launch off into long explanations that are technically way over the\nhead of the original poster and sometimes not even accurate.\nWot's Your Real Problem?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b38011f6-4381-4804-91c4-3089ca2f4bed>"],"error":null}
{"question":"What are the main advantages of large telescopes in terms of light gathering and resolution, and how do modern observatories manage the practical challenges of operating them?","answer":"Large telescopes offer significant advantages in their light-gathering and resolution capabilities. A larger aperture allows the telescope to capture more light, making fainter objects visible and providing more detail in a shorter time. The light-gathering ability is directly proportional to the mirror area, and larger telescopes also have higher angular resolution, allowing them to distinguish finer details between closely spaced objects. However, modern observatories face several practical challenges. They require professional observers and telescope operators rather than astronomers to collect data, and observations are controlled from separate rooms due to light sensitivity issues. The data is collected using cryogenically cooled electronic detectors, and color images are built by combining black and white images taken with different filters. Large telescopes also come with significant operational challenges, including higher costs, maintenance requirements, and the need for specialized locations with stable air and minimal light pollution.","context":["This article is part of the Science in Sci-fi, Fact in Fantasy blog series. Each week, we tackle one of the scientific or technological concepts pervasive in sci-fi (space travel, genetic engineering, artificial intelligence, etc.) with input from an expert.\nPlease join the mailing list to be notified every time new content is posted.\nAbout the Expert\nTom Benedict has a bachelor’s degree in astronomy, and has spent the past fifteen years working as an instrument specialist at the Canada-France-Hawaii Telescope at the summit of Mauna Kea. You can take a look at his blog, in which he writes about observatory life, engineering, and hobbies. You can also find him at A Ticcing Life, in which he writes about life with Tourette’s Syndrome.\nAstronomy in Science Fiction\nDespite the close link between astronomy and science fiction, few science fiction stories include the basic tool of astronomy: the telescope. Those stories that do tend to stand out, either because they got the details right or because they got them so very wrong. In the interest of having more stories that get the details right, here are some common misconceptions about the business of optical astronomy, a look at how optical astronomy is done today, and speculations about astronomy’s future:\nTelescopes in Deep Impact\nA good place to start is the movie Deep Impact. The first telescope scene, in which a high school astronomy club is watching the sky and one of the students, Leo Biederman, spots a strange object, is actually pretty accurate. Amateur telescopes outnumber professional observatories by a huge margin, so many transient events – supernovae, comets, or Earth-killing asteroids and comets – are reported first by amateur astronomers.\nWhere the movie’s portrayal of astronomy began to break down was in the second telescope scene, in which Dr. Wolf confirms Mr. Biederman’s object at the Adrian Peak Observatory. After aiming the telescope with a hand paddle, Dr. Wolf sits down at a bank of monitors next to the telescope to munch pizza and see what Mr. Biederman saw.\nUnfortunately, the light from a computer monitor or desk lamp is enough to blind a detector that’s sensitive enough to image the night sky. Like the reader of a good mystery novel, telescopes have to be kept in the dark. Modern observatories are controlled from rooms located somewhere else in the building or from a remote site far from the observatory.\nIncreasingly, astronomers aren’t even the ones making the observations. It’s more efficient and cost effective for observatories to hire professional observers and telescope operators to collect data, process it, and hand it over to the astronomers so they can use it for science. An accurate portrayal of that second scene would’ve had Dr. Wolf eating pizza in his office, requesting target-of-opportunity time for Mr. Biederman’s object to be observed by an observer (who might have had pizza of their own.)\nAll of this means that the classic science fiction setup – the lone astronomer witnessing that magic moment when they, and they alone, see the Doomsday Asteroid, and the fate of the world rests upon their shoulders – really doesn’t happen. That moment is more likely to be experienced by an amateur like Leo Biederman, or by an astronomer in an office thousands of miles from the telescope.\nObservatories are in the business of taking on-sky data and handing it over to astronomers for analysis. The modern observatory resembles, in many ways, a cross between a university research lab and a medium-sized factory. Depending on the size, the staff of a modern observatory ranges from a handful of employees up to several hundred, and includes every role you’d find in a typical office – accounting, HR, purchasing, IT, etc. – as well as the software developers, engineers, and technicians required to keep the place running, the observers and telescope operators who take data at night, and the staff astronomers.\nAstronomers themselves typically make up only a small percentage of the staff at a modern observatory, and some telescopes don’t employ staff scientists at all. Most astronomers work at universities rather than observatories.\nThe data is collected by electronic detectors, almost all of which are cryogenically cooled to reduce heat-induced signal noise. Astronomical detectors are almost all black and white devices. Color images are built up by taking several exposures with different filters in front of the detector. As backward as this may sound, it provides the most flexibility for gathering scientific data. Rather than only gathering red, green, and blue pixel data like a color detector, this allows an astronomer to request data through other filters that can pick out a single emission line, or image wavelengths the human eye can’t see.\nIt also allows astronomers to make new filters to fit a particular science case without having to replace the detectors in an instrument. What this also means, though, is that the pretty color pictures we see on posters and in calendars have to be made, after the fact, by combining black and white images.\nObservatory Dress Code\nContrary to popular belief, practically no one at an observatory wears a lab coat or a pocket protector. On the rare occasions when lab coats are worn it’s almost always by technical staff working around optics. When instruments are disassembled for servicing, that lab coat is traded out for clean room garb identical to that worn at factories that make computer chips. If there was an observatory dress code, it would be jeans and a shirt.\nAlong with the myth of the lab coat goes the myth of the technobabble. Although every technical and scientific field has its own form of jargon, for the most part people use everyday language to describe what they’re doing. For example, someone at an observatory is less likely to say, “Affix that support mechanism to the unstable apparatus,” than they are to say, “Brace that thing. It’s wobbly.”\nThe only time technical jargon comes into use is when it specifically applies. A lens really is a lens, and will be called a lens by layman and engineer alike. But when talking about optical aberrations the optical engineer will speak in terms of Zernike coefficients because that really is the right term to use. If a part of your story hinges on a technical point, be sure to look up the proper terms. Otherwise, plain language works just fine.\nPersonalities in Astronomy\nThere is no single overarching personality type that defines the people who work at observatories. As with any other professional field, they run the full gamut. I’ve worked with introverts, extroverts, focused people whose entire lives revolve around their professions, and people who have any number of outside interests. Those interests have included music, photography, art, dance, cooking, sports, woodworking, even pursuit of their own field in some other direction like home shop machining, DIY electronics, and robotics. When writing characters to inhabit an observatory, be sure to develop them the same way you would develop any other character.\nIn addition to the telescope itself, the kinds of machinery you find in a modern observatory include all of the machinery typical of a large building such as elevators and air conditioners, as well as the kind of machinery found in a factory, such as cranes, fork lifts, scissor lifts, air compressors, and hydraulic plants. Some observatories have their own machine shops and welding shops as well. Larger observatories have their own clean rooms for working on detectors, and lab space for working with optics and electronics.\nThe Future of Astronomy\nNone of these details are likely to change in the foreseeable future, even into future science fiction settings. But the larger scale picture changes constantly and has a couple of clear directions:\nNew telescopes only get built if they fill a niche that isn’t already filled. At the moment the three big drivers are mirror size, image quality, and access to new wavelengths. Desire for larger mirrors that will let us see further out into space has led us to build a slew of new telescopes including the Giant Magellan Telescope (GMT), the European Extremely Large Telescope (E-ELT) and the Thirty Meter Telescope (TMT). Desire for better image quality leads us to build these new telescopes in remote parts of the world at high altitude with stable air. Desire for new wavelengths drives development of new detectors, and to put telescopes in places where certain wavelengths aren’t blocked by atmospheric absorption, such as the South Pole or out into space.\nAnother key driver for future development is time. There are only 365 nights a year, or about 3000 hours available for observing. Weather and technical failures eat into that, so the actual figure is considerably less. This is true regardless of the size of the telescope or the instrument suite available to it. Some science programs require large blocks of uninterrupted time that may run into tens of thousands of hours of sky time. This has led to the development of dedicated survey telescopes that host only a single instrument, such as the Large Synoptic Survey Telescope (LSST), the Hobby-Eberly Telescope Dark Energy Explorer (HETDEX), and PanSTARRS – a telescope dedicated entirely to early detection of Earth-crossing asteroids and comets. (Sorry, Mr. Biederman.)\nWriting Realistic Astronomy in Science Fiction\nIf you are writing a story that includes a telescope that doesn’t already exist somewhere in the world, make sure it fills a niche that addresses at least one of these points or a new point that might become a consideration in the future, such has having telescopes on the far side of the moon or orbiting some distant star. And be sure to give it an appropriate name. If the previous section didn’t convince you that astronomers don’t always come up with great names, this might: The OWL Telescope was a conceptual design by the European Southern Observatory. OWL stands for “Overwhemingly Large”.\nBe sure to use your imagination.\nPlease Share The #ScienceInSF\nIf you liked this article, please share it with your writing friends using the buttons below. You can also click to send a ready-made tweet:\n|Click to Tweet Writing realistic astronomy in sci-fi, with astronomer Tom Benedict: http://bit.ly/1oY14Vi Part of the #ScienceInSF series by @DanKoboldt|\nFollow me and you'll never miss a post:\nFollow me and you'll never miss a post:","If you’re an avid stargazer, you’ve probably wondered if bigger telescopes really are better. The short answer is yes, they are. The size of a telescope’s aperture, or the diameter of its main lens or mirror, determines how much light it can gather. The more light a telescope can collect, the fainter the objects you can see, and the more detail you can observe.\nWhen it comes to telescopes, bigger is better. Larger telescopes have a higher resolving power, which means they can see finer details and more clearly distinguish between objects that are close together. They also have a higher light-gathering power, which allows them to see fainter objects and gather more data in a shorter amount of time. This is especially important when observing distant objects, such as galaxies and nebulae, which emit very little light.\nOf course, bigger telescopes also come with some drawbacks. They are more expensive, heavier, and harder to transport than smaller telescopes. They also require more maintenance and calibration to ensure they are functioning properly. However, for serious astronomers and researchers, the benefits of a larger telescope far outweigh the costs and challenges.\nLight Gathering Power\nWhen it comes to telescopes, bigger is usually better. One of the main reasons for this is light gathering power. In this section, we will explore the relationship between telescope size and light gathering power.\nThe Relationship Between Telescope Size and Light Gathering Power\nThe amount of light a telescope can gather depends on the size of its primary mirror or lens. A larger mirror or lens can capture more light, which means that fainter objects can be seen. This is because the light-gathering ability of a telescope is proportional to the area of its primary mirror or lens.\nTo illustrate this, let’s consider two telescopes: one with a 2-inch diameter mirror and another with a 4-inch diameter mirror. The 4-inch telescope has four times the light-gathering area of the 2-inch telescope. This means that the 4-inch telescope can capture four times as much light as the 2-inch telescope, making fainter objects visible.\nIt’s important to note that light gathering power is not the same as magnification. A telescope with a larger primary mirror or lens can gather more light, but it doesn’t necessarily provide higher magnification. Magnification is determined by the eyepiece used with the telescope.\nIn summary, a telescope’s light gathering power is directly proportional to the size of its primary mirror or lens. A larger telescope can capture more light, allowing us to see fainter objects in the night sky.\nWhen it comes to telescopes, resolution is a critical factor that determines how much detail you can see in an image. In simple terms, resolution refers to the ability of a telescope to distinguish between two closely spaced objects. The higher the resolution, the clearer and more detailed the image will be.\nHow Bigger Telescopes Improve Resolution\nOne of the key advantages of larger telescopes is their ability to gather more light, which improves their resolution. When a telescope has a larger aperture, it can collect more light from an object, allowing it to see fainter details that would be invisible to smaller telescopes.\nAnother way that larger telescopes improve resolution is by reducing the effects of atmospheric turbulence. When light passes through the Earth’s atmosphere, it can become distorted, causing images to appear blurry. This effect is known as atmospheric seeing. Larger telescopes are less affected by atmospheric turbulence because they have a larger collecting area, which allows them to capture more light and reduce the impact of atmospheric distortion.\nFinally, larger telescopes can produce sharper images because they have a higher angular resolution. Angular resolution refers to the smallest angle that can be resolved by a telescope. The larger the telescope’s aperture, the smaller the angle it can resolve, resulting in sharper images with more detail.\n|Telescope Size||Angular Resolution|\n|10 cm||2.5 arcseconds|\n|20 cm||1.25 arcseconds|\n|40 cm||0.625 arcseconds|\nAs you can see from the table above, larger telescopes have a significantly higher angular resolution than smaller ones. This means that they can resolve finer details in an image and produce sharper, clearer pictures.\nOverall, the larger the telescope’s aperture, the better its resolution and the more detail you can see in an image. If you want to see the faintest and most detailed objects in the night sky, a larger telescope is the way to go.\nWhen it comes to telescopes, sensitivity is a crucial factor. Sensitivity refers to the minimum signal that a telescope can distinguish above the random background noise. In other words, it is a measure of how well a telescope can detect faint objects.\nThe Importance of Sensitivity in Astronomy\nWithout sensitivity, astronomers would not be able to detect many of the celestial objects that exist in the universe. This is especially true for objects that are very far away or emit very little light, such as distant galaxies, brown dwarfs, and exoplanets.\nFurthermore, sensitivity is important for studying the properties of these objects, such as their composition, temperature, and motion. By detecting faint signals, astronomers can gather more data and make more accurate measurements.\nHow Bigger Telescopes Improve Sensitivity\nOne of the main advantages of larger telescopes is that they are more sensitive than smaller ones. This is because a larger primary mirror or lens can gather more light, which increases the signal-to-noise ratio.\nFor example, the Australia Telescope National Facility states that “all other things being equal, a telescope of larger primary mirror or lens is more sensitive than one with a smaller primary.” This means that a larger telescope can detect fainter objects than a smaller one.\nIn addition, larger telescopes can also have better spatial resolution, which allows astronomers to distinguish between objects that are closer together. This is important for studying complex systems such as star clusters, galaxies, and planetary systems.\nOverall, sensitivity is a critical factor in astronomy, and larger telescopes have a clear advantage when it comes to detecting and studying faint objects.\nCost and Practicality\nWhile bigger telescopes offer many advantages, there are also significant costs and practical limitations to building and operating them.\nThe High Cost of Building and Operating Bigger Telescopes\nBuilding and operating a large telescope is an expensive undertaking. The cost of the mirror alone can run into the hundreds of millions of dollars, and that’s before you factor in the cost of the telescope’s support systems, including the dome, the mount, the instrument package, and the control systems.\nFurthermore, the cost of operating a large telescope can be just as high as the cost of building it. The telescope must be staffed around the clock, and the cost of maintaining and upgrading the telescope’s hardware and software can also be significant.\nDespite these high costs, many astronomers believe that the benefits of large telescopes outweigh the costs. Large telescopes can help us answer some of the biggest questions in astronomy, and they can also inspire the next generation of scientists and engineers.\nThe Practical Limitations of Building and Operating Bigger Telescopes\nBuilding and operating a large telescope also comes with practical limitations. For example, the telescope must be located in a place with clear skies and minimal light pollution. This often means building the telescope on a remote mountaintop or in a desert.\nFurthermore, the telescope must be designed to withstand the harsh conditions of its location. This can include high winds, extreme temperatures, and even earthquakes. Building a telescope that can withstand these conditions can be a major engineering challenge.\nAnother practical limitation of large telescopes is their size. A large telescope can be difficult to transport and assemble, and it may require a large team of engineers and technicians to operate it.\nDespite these practical limitations, many astronomers believe that the benefits of large telescopes outweigh the challenges. Large telescopes can help us answer some of the biggest questions in astronomy, and they can also inspire the next generation of scientists and engineers."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:f852d4c9-abfd-4cb4-b595-b52fee01a75b>","<urn:uuid:6d3e2989-5452-42a1-bae8-d1b309d04aa8>"],"error":null}
{"question":"What's the difference between salicylic acid and witch hazel extract for treating skin concerns?","answer":"Salicylic acid is a beta-hydroxy acid that exfoliates skin by softening keratin protein, which helps loosen dry scaly skin and improve skin texture and color. It's particularly helpful for acne and acne scarring, with side effects generally limited to mild local irritation. Witch hazel extract, meanwhile, has multiple properties including astringent, anti-inflammatory, antioxidant, and anti-bacterial effects. It works as a vasoconstrictor (narrowing blood vessels) and promotes healing of broken skin by tightening skin proteins. The extract contains active components like hamamelitannin, catechins, and gallic acid, which provide these various benefits.","context":["|Ingredient name||what-it-does||irr., com.||ID-Rating|\n|Diethylene Glycol Monoethyl Ether||solvent, moisturizer/humectant, perfuming||0, 0|\n|Niacinamide||cell-communicating ingredient, skin brightening, anti-acne, moisturizer/humectant||superstar|\n|Aloe Vera Juice||soothing, moisturizer/humectant||goodie|\n|Witch Hazel Extract||soothing, antioxidant, antimicrobial/antibacterial||goodie|\n|Xanthan Gum||viscosity controlling|\n|Trisodium Ethylenediamine Disuccinate||chelating|\n|Sodium Hyaluronate||skin-identical ingredient, moisturizer/humectant||0, 0||goodie|\n|Ifra Certified Allergen Free Fragrance|\nMamaearth Niacin TonerIngredients explained\nGood old water, aka H2O. The most common skincare ingredient of all. You can usually find it right in the very first spot of the ingredient list, meaning it’s the biggest thing out of all the stuff that makes up the product.\nIt’s mainly a solvent for ingredients that do not like to dissolve in oils but rather in water.\nOnce inside the skin, it hydrates, but not from the outside - putting pure water on the skin (hello long baths!) is drying.\nOne more thing: the water used in cosmetics is purified and deionized (it means that almost all of the mineral ions inside it is removed). Like this, the products can stay more stable over time.\nA nice odorless liquid used mainly as a superior solubilizer and efficacy booster for cosmetic active ingredients such as skincare bigshot vitamin C, self-tanning active DHA or the anti-acne gold standard, benzoyl peroxide.\nOther than that it can also be used in hair care products where it gives a longer-lasting and more uniform coloring. According to a manufacturer, it might even prevent the formation of split ends.\nPropanediol is a natural alternative for the often used and often bad-mouthed propylene glycol. It's produced sustainably from corn sugar and it's Ecocert approved.\nIt's quite a multi-tasker: can be used to improve skin moisturization, as a solvent, to boost preservative efficacy or to influence the sensory properties of the end formula.\n- A multi-functional skincare superstar with several proven benefits for the skin\n- Great anti-aging, wrinkle smoothing ingredient used at 4-5% concentration\n- Fades brown spots alone or in combination with amino sugar, acetyl glucosamine\n- Increases ceramide synthesis that results in a stronger, healthier skin barrier and better skin hydration\n- Can help to improve several skin conditions including acne, rosacea, and atopic dermatitis\nAloe Vera is one of today’s magic plants. It does have some very nice properties indeed, though famous dermatologist Leslie Baumann warns us in her book that most of the evidence is anecdotal and the plant might be a bit overhyped.\nWhat research does confirm about Aloe is that it’s a great moisturizer and has several anti-inflammatory (among others contains salicylates, polysaccharides, magnesium lactate and C-glucosyl chromone) as well as some antibacterial components. It also helps wound healing and skin regeneration in general. All in all definitely a goodie.\nWitch hazel is a smallish tree (up to 5m) that's native to North-America, has nice yellow flowers and is similar to the hazelnut bush (hence the name).\nAs for skincare, it's loaded with active components that have a bunch of magic properties, like astringent, anti-inflammatory, antioxidant and anti-bacterial. It's also a well-known vasoconstrictor (it makes the blood vessels narrower) and promotes the healing of broken skin by tightening up the skin proteins and thus creating a protective covering.\nThe complication, however, is that different extracts and distillates can be made from different parts of the plant (bark, twigs, and leaves are typically used) and different extraction methods from different parts produce different results. So if you see only Witch Hazel Extract or Witch Hazel Water on the ingredient list, it's a bit hard to know what you're actually getting but we will try to summarize the possibilities to give an idea.\nThe main biologically active components in Witch Hazel are hamamelitannin (a potent astringent and antioxidant), catechins (anti-inflammatory and antioxidant) and gallic acid (antibacterial). The bark extract contains by far the most hamamelitannin and it has the most gallic acid and catechins. The twigs contain fewer catechins, less gallic acid, and much less hamamelitannin (4.77% vs 0.18%). The leaves contain hardly any tannins (0.04%) or catechins and contain a medium amount of gallic acid (compared to the bark and twigs).\nWitch Hazel also contains tiny amounts of the essential oil and fragrance component eugenol, but the amount is so small that it's probably not significant for the skin.\nApart from the differences in active components in different parts of the Witch Hazel bush, the extraction methods also vary. Witch Hazel Distillate contains 14% added alcohol according to the USP specifications and alcohol is, at best drying, and at worst skin-damaging. Luckily, there are also alcohol-free distillates, so if you prefer no alcohol check the ingredient list carefully. Witch Hazel Extracts can also be made in different ways: browsing Ulprospector, we could find hydroglycolic, hydroalcoholic and glicerine/water based extracts.\nWell-known skin care expert, Paula Begoun rates witch hazel as poor and says, \"depending on the form of witch hazel, you’re exposing your skin either to a sensitizing amount of alcohol or to tannins, or both.\" This might be the case if you are dealing with an alcoholic witch hazel bark water or extract, but looking at CosIng (the official INCI name listing of the EU), witch hazel bark water or witch hazel bark extract are not listed ingredients. Bark and leaf or bark and twig or all three are used together to create extracts, so the chance that there is too much hamamelitannin in the final cosmetic ingredient seems small. Also alcohol-free extracts and distillates exist; actually, the majority seem to be alcohol-free nowadays. So all in all, we think \"Hamamelis Virginiana Extract\" on the ingredient list is nothing to worry about.\nWe even found a German study that compared the efficacy of Hamamelis ointment to panthenol ointment for soothing the skin in children (from 27 days to 11 years old). They observed 309 children and concluded that both ointments were similarly effective but the one with Hamamelis was even better tolerated (98.2% vs. 92.3% tolerated well the ointments in the two groups).\nAll in all, Witch Hazel Extract is a sloppy INCI name (btw, not in the CosIng listing), and you do not really know what you're getting. Most probably though, you are getting a goody with nice astringent, soothing, antibacterial, and even antioxidant properties.\nSuper common soothing ingredient. It can be found naturally in the roots & leaves of the comfrey plant, but more often than not what's in the cosmetic products is produced synthetically.\nIt's not only soothing but it' also skin-softening and protecting and can promote wound healing.\nIt's one of the most commonly used thickeners and emulsion stabilizers. If the product is too runny, a little xanthan gum will make it more gel-like. Used alone, it can make the formula sticky and it is a good team player so it is usually combined with other thickeners and so-called rheology modifiers (helper ingredients that adjust the flow and thus the feel of the formula). The typical use level of Xantha Gum is below 1%, it is usually in the 0.1-0.5% range.\nBtw, Xanthan gum is all natural, a chain of sugar molecules (polysaccharide) produced from individual sugar molecules (glucose and sucrose) via fermentation. It’s approved by Ecocert and also used in the food industry (E415).\nA chelating agent that helps to preserve cosmetic products by neutralizing the metal ions (especially iron) in the formula (that usually get into there from water). Its special thing is that it also acts as a biostatic and fungistatic agent and remains active even at high pH.\nIt is often coupled with antimicrobial glycols (such as propanediol) to create a \"preservative free preservative system\" for cosmetic products.\nA helper ingredient that helps to neutralize the metal ions in the formula (they usually come from water) so it stays nice longer. The special property of this particular ingredient is that it's more effective against more problematic ions, like Cu (copper) and Fe (iron) compared to less problematic ones like Ca (calcium) and Mg (magnesium).\nIt’s the - sodium form - cousin of the famous NMF, hyaluronic acid (HA). If HA does not tell you anything we have a super detailed, geeky explanation about it here. The TL; DR version of HA is that it's a huge polymer (big molecule from repeated subunits) found in the skin that acts as a sponge helping the skin to hold onto water, being plump and elastic. HA is famous for its crazy water holding capacity as it can bind up to 1000 times its own weight in water.\nAs far as skincare goes, sodium hyaluronate and hyaluronic acid are pretty much the same and the two names are used interchangeably. As cosmetic chemist kindofstephen writes on reddit \"sodium hyaluronate disassociates into hyaluronic acid molecule and a sodium atom in solution\".\nIn spite of this, if you search for \"hyaluronic acid vs sodium hyaluronate\" you will find on multiple places that sodium hyaluronate is smaller and can penetrate the skin better. Chemically, this is definitely not true, as the two forms are almost the same, both are polymers and the subunits can be repeated in both forms as much as you like. (We also checked Prospector for sodium hyaluronate versions actually used in cosmetic products and found that the most common molecular weight was 1.5-1.8 million Da that absolutely counts as high molecular weight).\nWhat seems to be a true difference, though, is that the salt form is more stable, easier to formulate and cheaper so it pops up more often on the ingredient lists.\nIf you wanna become a real HA-and-the-skin expert you can read way more about the topic at hyaluronic acid (including penetration-questions, differences between high and low molecular weight versions and a bunch of references to scientific literature).\nCitric acid comes from citrus fruits and is an AHA. If these magic three letters don’t tell you anything, click here and read our detailed description on glycolic acid, the most famous AHA.\nSo citric acid is an exfoliant, that can - just like other AHAs - gently lift off the dead skin cells of your skin and make it more smooth and fresh.\nThere is also some research showing that citric acid with regular use (think three months and 20% concentration) can help sun-damaged skin, increase skin thickness and some nice hydrating things called glycosaminoglycans in the skin.\nBut according to a comparative study done in 1995, citric acid has less skin improving magic properties than glycolic or lactic acid. Probably that’s why citric acid is usually not used as an exfoliant but more as a helper ingredient in small amounts to adjust the pH of a formulation.\nA really multi-functional helper ingredient that can do several things in a skincare product: it can bring a soft and pleasant feel to the formula, it can act as a humectant and emollient, it can be a solvent for some other ingredients (for example it can help to stabilize perfumes in watery products) and it can also help to disperse pigments more evenly in makeup products. And that is still not all: it can also boost the antimicrobial activity of preservatives.\n|what‑it‑does||solvent | moisturizer/humectant | perfuming|\n|irritancy, com.||0, 0|\n|what‑it‑does||solvent | moisturizer/humectant|\n|what‑it‑does||cell-communicating ingredient | skin brightening | anti-acne | moisturizer/humectant|\n|what‑it‑does||soothing | moisturizer/humectant|\n|what‑it‑does||soothing | antioxidant | antimicrobial/antibacterial|\n|irritancy, com.||0, 0|\n|what‑it‑does||skin-identical ingredient | moisturizer/humectant|\n|irritancy, com.||0, 0|","Glycolic Acid / Salicylic Acid / Lactic Acid Face Pads\nGlycolic Acid / Salicylic Acid / Lactic Acid Face Pads are a combination of chemical exfoliants that have been formulated into a solution and are dispensed as saturated cloth pads. The active ingredients act to exfoliate the skin, resulting in oil and dead skin cell removal as well as increased cell turnover (peeling). The continued use of this formula may prevent the formation of new blemishes, result in visually smaller pores, and clearer healthy skin.\nGlycolic acid is an alpha hydroxy acid (AHA) naturally found in sugar that penetrates the skin to break down sebum and exfoliate skin cells. When applied to the skin, glycolic acid works to break the bonds between the outer layer of skin cells, including dead skin cells, and the next skin cell layer. This creates a peeling effect that can make the skin appear smoother and more even over time. Glycolic acid, especially the use of higher concentrations (10% or more), can cause skin irritation, itching, swelling, and sun sensitivity. Using lower concentrations can reduce the propensity of these common adverse effects.1\nSalicylic acid is a beta-hydroxy acid that exfoliates skin and can improve its texture and color. It also helps with acne and acne scarring. Salicylic acid works by softening keratin, a protein that forms part of the skin structure. This helps to loosen dry scaly skin making it easier to remove. Side effects to salicylic acid are generally limited to mild local irritation.2, 3\nLactic Acid is another alpha hydroxy acid that acts similarly to glycolic acid. Because of its larger molecular structure, it may not penetrate the skin as deeply as glycolic acid. One advantage of this feature, though, is that lactic acid is generally less irritating than glycolic acid.1\nPatients that would benefit from the administration of this preparation may opt for this compounded preparation as determined by a prescriber. Common formulations prepared at Galleria Medical Pharmacy include but are not limited to:\nGlycolic Acid 1% / Salicylic Acid 1% / Lactic Acid 3% Saturated Face Pads (for daily use)\nGlycolic Acid 2% / Salicylic Acid 2% / Lactic Acid 4% Saturated Face Pads (for weekly use)\nStore this medication at 68°F to 77°F (20°C to 25°C) and away from heat, moisture and light. Keep all medicine out of the reach of children. Throw away any unused medicine after the beyond use date. Do not flush unused medications or pour down a sink or drain.\n- Sheau-Chung Tang, Jen-Hung Yang. Dual Effects of Alpha-Hydroxy Acids on the Skin. Molecules. 2018 Apr; 23(4): 863.\n- Zander E, Weisman S. Treatment of acne vulgaris with salicylic acid pads. Clin Ther. Mar-Apr 1992;14(2):247-53.\n- Tasleem Arif. Salicylic acid as a peeling agent: a comprehensive review. Clin Cosmet Investig Dermatol. 2015; 8: 455–461.\nCompounding Pharmacy Statement\nThis preparation is compounded with drug components whose suppliers are registered with the FDA. While Galleria Medical Pharmacy adheres to USP <795> guidelines and applicable state and federal regulations to meet the required quality standards, the statements made regarding this preparation have not been evaluated by the FDA for safety or clinical effectiveness. As such, this preparation is not intended to diagnose, treat, cure, or prevent any disease. For inquiries concerning this preparation, please contact (504) 267-9876."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c6cd513b-4504-46b3-b2e8-ece766d044af>","<urn:uuid:a2585675-7c8c-4601-8785-6c006ef29c01>"],"error":null}
{"question":"Which substance has a higher boiling point: bromine or hydrogen fluoride, and what are their respective values?","answer":"Bromine has a higher boiling point at 332.0 K (58.8°C) compared to hydrogen fluoride which boils at 292.6 K (19.5°C). Both substances are liquid near room temperature, with bromine being a reddish-brown liquid and hydrogen fluoride being colorless.","context":["2007 Schools Wikipedia Selection. Related subjects: Chemical elements\n|Name, Symbol, Number||bromine, Br, 35|\n|Group, Period, Block||17, 4, p|\nsolid: metallic luster\n|Atomic mass||79.904 (1) g/mol|\n|Electron configuration||[Ar] 3d10 4s2 4p5|\n|Electrons per shell||2, 8, 18, 7|\n|Density (near r.t.)||(liquid) 3.1028 g·cm−3|\n|Melting point||265.8 K\n(-7.3 ° C, 19 ° F)\n|Boiling point||332.0 K\n(58.8 ° C, 137.8 ° F)\n|Critical point||588 K, 10.34 MPa|\n|Heat of fusion||(Br2) 10.57 kJ·mol−1|\n|Heat of vaporization||(Br2) 29.96 kJ·mol−1|\n|Heat capacity||(25 °C) (Br2)\n|Oxidation states||±1, 5\n(strongly acidic oxide)\n|Electronegativity||2.96 (Pauling scale)|\n| Ionization energies\n|1st: 1139.9 kJ·mol−1|\n|2nd: 2103 kJ·mol−1|\n|3rd: 3470 kJ·mol−1|\n|Atomic radius||115 pm|\n|Atomic radius (calc.)||94 pm|\n|Covalent radius||114 pm|\n|Van der Waals radius||185 pm|\n|Electrical resistivity||(20 °C) 7.8×1010 Ω·m|\n|Thermal conductivity||(300 K) 0.122 W·m−1·K−1|\n|Speed of sound||(20 °C) ? 206 m/s|\n|CAS registry number||7726-95-6|\nBromine ( IPA: /ˈbrəʊmiːn/, Greek: βρωμος, brómos, meaning \"stench\"), is a chemical element in the periodic table that has the symbol Br and atomic number 35. A halogen element, bromine is a red volatile liquid at standard room temperature which has a reactivity between chlorine and iodine. This element is corrosive to human tissue in a liquid state and its vapors irritate eyes and throat. Bromine vapors are very toxic upon inhalation.\nBromine is the only liquid nonmetallic element at room temperature and one of five elements on the period table that are liquid at or close to room temperature. The pure chemical element has the physical form of a diatomic molecule, Br2. It is a heavy, mobile, reddish-brown liquid, that evaporates easily at standard temperature and pressures in a red vapor (its colour resembles nitrogen dioxide) that has a strong disagreeable odour resembling that of chlorine. A halogen, bromine resembles chlorine chemically but is less active. It is more active than iodine, however. Bromine is slightly soluble in water, and highly soluble in carbon disulfide, aliphatic alcohols (such as methanol), and acetic acid. It bonds easily with many elements and has a strong bleaching action.\nBromine is highly reactive and is a powerful oxidizing agent in the presence of water. It reacts vigorously with amines, alkenes and phenols as well as aliphatic and aromatic hydrocarbons, ketones and acids (these are brominated by either addition or substitution reactions). With many of the metals and elements, anhydrous bromine is less reactive than wet bromine; however, dry bromine reacts vigorously with aluminium, titanium, mercury as well as alkaline earth metals and alkaline metals.\nElemental bromine is used to manufacture a wide variety of bromine compounds used in industry and agriculture. Traditionally the largest use of bromine was in the production of 1,2-dibromoethane which in turn was used as a gasoline anti- knock agent for leaded gasolines before they were largely phased out due to environmental considerations.\nBromine is also used in the manufacture of fumigants, brominated flame-retardants, water purification compounds, dyes, medicines, sanitizers, inorganic bromides for photography, etc. It is also used to form intermediates in organic synthesis, where it is preferred to iodine due to its much lower cost.\nBromine is used to make brominated vegetable oil, which is used as an emulsifier in many citrus-flavored soft drinks.\nAqueous bromine is orange and can be used in tests for alkenes and phenols.\n- When added to an alkene it will lose its colour as it reacts forming a colorless bromoalkane. For example, reaction with ethylene will produce 1,2-dibromoethane.\n- When added to phenol a white precipitate, 2,4,6-tribromophenol, will form. With aniline, 2,4,6 tribromoaniline will precipitate (even in water)\nBromine was discovered by Antoine Balard at the salt marshes of Montpellier in 1826 but was not produced in quantity until 1860. The French chemist and physicist Joseph-Louis Gay-Lussac suggested the name bromine due to the characteristic smell of the vapors.\nBromine occurs in nature as bromide salts in very diffuse amounts in crustal rock. Due to leaching, bromide salts have accumulated in sea water (85 ppm), and may be economically recovered from brine wells and the Dead Sea (up to 5000 ppm).\nApproximately 500 million kilograms ($350 million USD) of bromine are produced per year (2001) worldwide with the United States and Israel being the primary producers. The largest bromine reserve in the United States is located in Columbia and Union County, Arkansas.\nElemental bromine is a strong irritant and, in concentrated form, will produce painful blisters on exposed skin and especially mucous membranes. Even low concentrations of bromine vapor (from 10 ppm) can affect breathing, and inhalation of significant amounts of bromine can seriously damage the respiratory system.\nAccordingly, one should always wear safety goggles and ensure adequate ventilation when handling bromine.\nAluminium bromide (AlBr3), ammonium bromide (NH4Br), bromine monofluoride (BrF), bromine pentafluoride (BrF5), bromine trifluoride (BrF3), tetrabromomethane (CBr4), hydrobromic acid (HBr), iron(III) bromide (FeBr3), lithium bromide (LiBr), phosphorus pentabromide (PBr5), phosphorus tribromide (PBr3), potassium bromide (KBr), potassium bromate (KBrO3), silver bromide (AgBr), sodium bromide (NaBr), sodium bromate (NaBrO3).","3D model (JSmol)\nCompTox Dashboard (EPA)\n|Appearance||colourless gas or colourless liquid (below 19.5 °C)|\n|Density||1.15 g/L, gas (25 °C)|\n0.99 g/mL, liquid (19.5 °C)\n1.663 g/mL, solid (-125 °C)\n|Melting point||-83.6 °C (-118.5 °F; 189.6 K)|\n|Boiling point||19.5 °C (67.1 °F; 292.6 K)|\n|completely miscible (liquid)|\n|Vapor pressure||783 mmHg (20 °C)|\n|Acidity (pKa)||3.17 (in water),\n15 (in DMSO) \nRefractive index (nD)\n|8.687 J/g K (gas)|\nStd enthalpy of\n|-13.66 kJ/g (gas) |\n-14.99 kJ/g (liquid)\n|GHS Signal word||Danger|\n|H300, H310, H314, H330|\n|P260, P262, P264, P270, P271, P280, P284, P301+310, P301+330+331, P302+350, P303+361+353, P304+340, P305+351+338, P310, P320, P321, P322, P330, P361, P363, P403+233, P405, P501|\n|NFPA 704 (fire diamond)|\n|Lethal dose or concentration (LD, LC):|\nLC50 (median concentration)\n|1276 ppm (rat, 1 hr)|\n1774 ppm (monkey, 1 hr)\n4327 ppm (guinea pig, 15 min)\nLCLo (lowest published)\n|313 ppm (rabbit, 7 hr)|\n|NIOSH (US health exposure limits):|\n|TWA 3 ppm|\n|TWA 3 ppm (2.5 mg/m3) C 6 ppm (5 mg/m3) [15-minute]|\nIDLH (Immediate danger)\nExcept where otherwise noted, data are given for materials in their standard state (at 25 °C [77 °F], 100 kPa).\n|what is ?)(|\nHydrogen fluoride is a chemical compound with the chemical formula HF. This colorless gas or liquid is the principal industrial source of fluorine, often as an aqueous solution called hydrofluoric acid. It is an important feedstock in the preparation of many important compounds including pharmaceuticals and polymers, e.g. polytetrafluoroethylene (Teflon). HF is widely used in the petrochemical industry as a component of superacids. Hydrogen fluoride boils near room temperature, much higher than other hydrogen halides.\nHydrogen fluoride is a highly dangerous gas, forming corrosive and penetrating hydrofluoric acid upon contact with moisture. The gas can also cause blindness by rapid destruction of the corneas.\nIn 1771 Carl Wilhelm Scheele prepared the aqueous solution, hydrofluoric acid in large quantities, although hydrofluoric acid had been known in the glass industry before then. French chemist Edmond Frémy (1814-1894) is credited with discovering anhydrous hydrogen fluoride (HF) while trying to isolate fluorine.\nAlthough a diatomic molecule, HF forms relatively strong intermolecular hydrogen bonds. Solid HF consists of zig-zag chains of HF molecules. The HF molecules, with a short H-F bond of 95 pm, are linked to neighboring molecules by intermolecular H-F distances of 155 pm. Liquid HF also consists of chains of HF molecules, but the chains are shorter, consisting on average of only five or six molecules.\nHydrogen fluoride does not boil until 20 °C in contrast to the heavier hydrogen halides, which boil between -85 °C (-120 °F) and -35 °C (-30 °F). This hydrogen bonding between HF molecules gives rise to high viscosity in the liquid phase and lower than expected pressure in the gas phase.\nHF is miscible with water (dissolves in any proportion). In contrast, the other hydrogen halides exhibit limiting solubilities in water. Hydrogen fluoride forms a monohydrate HF.H2O with m.p.-40 °C (-40 °F), which is 44 °C (79 °F) above the melting point of pure HF.\n|HF and H2O similarities|\n|Boiling points of the hydrogen halides (blue) and hydrogen chalcogenides (red): HF and H2O break trends.||Freezing point of HF/ H2O mixtures: arrows indicate compounds in the solid state.|\nAqueous solutions of HF are called hydrofluoric acid. When dilute, hydrofluoric acid behaves like a weak acid, unlike the other hydrohalic acids, due to the formation of hydrogen-bonded ion pairs [·F-]. However concentrated solutions are strong acids, because bifluoride anions are predominant, instead of ion pairs. In liquid anhydrous HF, self-ionization occurs:\nwhich forms an extremely acidic liquid (H0 = -11).\nAbout 20% of manufactured HF is a byproduct of fertilizer production, which generates hexafluorosilicic acid. This acid can be degraded to release HF thermally and by hydrolysis:\nIn general, the anhydrous compound hydrogen fluoride is more common industrially than its aqueous solution, hydrofluoric acid. Its main uses, on a tonnage basis, are as a precursor to organofluorine compounds and a precursor to cryolite for the electrolysis of aluminium.\nHF reacts with chlorocarbons to give fluorocarbons. An important application of this reaction is the production of tetrafluoroethylene (TFE), precursor to Teflon. Chloroform is fluorinated by HF to produce chlorodifluoromethane (R-22):\nPyrolysis of chlorodifluoromethane (at 550-750 °C) yields TFE.\nHF is a reactive solvent in the electrochemical fluorination of organic compounds. In this approach, HF is oxidized in the presence of a hydrocarbon and the fluorine replaces C-H bonds with C-F bonds. Perfluorinated carboxylic acids and sulfonic acids are produced in this way.\nThe electrowinning of aluminium relies on the electrolysis of aluminium fluoride in molten cryolite. Several kilograms of HF are consumed per ton of Al produced. Other metal fluorides are produced using HF, including uranium hexafluoride.\nHF is the precursor to elemental fluorine, F2, by electrolysis of a solution of HF and potassium bifluoride. The potassium bifluoride is needed because anhydrous HF does not conduct electricity. Several thousand tons of F2 are produced annually.\nHF serves as a catalyst in alkylation processes in refineries. It is used in the majority of the installed linear alkyl benzene production facilities in the world. The process involves dehydrogenation of n-paraffins to olefins, and subsequent reaction with benzene using HF as catalyst. For example, in oil refineries \"alkylate\", a component of high-octane petrol (gasoline), is generated in alkylation units, which combine C3 and C4 olefins and iso-butane.\nHydrogen fluoride is an excellent solvent. Reflecting the ability of HF to participate in hydrogen bonding, even proteins and carbohydrates dissolve in HF and can be recovered from it. In contrast, most non-fluoride inorganic chemicals react with HF rather than dissolving.\nUpon contact with moisture, including tissue, hydrogen fluoride immediately converts to hydrofluoric acid, which is highly corrosive and toxic. Exposure requires immediate medical attention. It can cause blindness by rapid destruction of the corneas. Breathing in hydrogen fluoride at high levels or in combination with skin contact can cause death from an irregular heartbeat or from fluid buildup in the lungs."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a8ef79cc-f0d0-49bf-9216-ca43cab4e97b>","<urn:uuid:3effc318-d4fa-4e5a-a78e-d41622c2f66c>"],"error":null}
{"question":"Why city of Albuquerque decide to change Columbus Day to Indigenous Peoples Day in 2015?","answer":"The Albuquerque City Council voted on October 7 to recognize Indigenous Peoples Day following the leadership of Native American and Chicano leaders. According to activist Melanie Yazzie, this change was important because the history of conquest, slavery, and globalized violence that Columbus participated in continues to affect everyone. Another activist, Andrea Runyan, explained that the movement grew from recognizing the resilience of Native communities that have persisted despite genocidal efforts to eradicate them.","context":["October 12, 2015\nFNS Editor’s Note: As part of our special coverage marking the historic transformation of Columbus Day into Indigenous Peoples Day, we offer this contribution by Nicolás Cabrera. He is a graduate student who specializes in Spanish literature at NMSU. Cabrera’s piece discusses Latin American manifestations of the movement, as well as historical accounts of the early conquest period that provide essential background information to today’s debates and controversies.\nOctober 12, 2015\nNMSU Student Series\nFor several decades there have been movements to redefine the holiday that falls on October 12th. In the United States this holiday is traditionally known as Columbus Day while in many Latin American countries it goes by a different name. Recently Albuquerque joined dozens of other cities in a change that is being led by Native American and Chicano leaders and supporters.\n“It is true that this specific movement is about indigenous peoples,” said Albuquerque City Council member Rey Garduño. “As a Chicano and an ally I stand with my Indigenous sisters and brothers.”\nGarduño was instrumental in having the Albuquerque City Council join other municipalities across the country vote in voting October 7 to recognize Indigenous Peoples Day with a proclamation for October 12th.\nNative American activist Melanie Yazzie explained why this is an issue for everybody. She said, “Because the history of conquest, slavery, and globalized violence that Columbus actively participated in is one that continues and affects us all.”\nThe redefinition of the holiday began in Latin America in the 20th century where the trend to rename and redefine the October 12th has had the most success.\nFor example, in Costa Rica it’s called “El Día de las Culturas” (Day of the Cultures) while in Mexico and many other Latin American countries it’s called “El Día de la Raza” which is best translated as “Day of the People.”In Nicaragua it is called “Día de la Resistencia Indígena” (Day of Indigenous Resistance) and Peru calls it “Día de los Pueblos Originarios y del Diálogo Intercultural” (Day of the Native Peoples and Intercultural Dialog).\nLastly, in Spain October 12th was originally called “Fiesta de la Raza” but has since been changed to simply “Fiesta Nacional.”\nAndrea Runyan is another activist working to change people’s perceptions of the holiday. Runyan said the movement to replace the holiday is growing because people are “recognizing the resilience of Native communities that have persisted despite genocidal efforts to eradicate them.”\nGarduño’s solidarity, Yazzie’s resistance, and Runyan’s activism are rooted in history that is accessible to everybody who wants to learn more about what took place during the initial decades of contact.\nThe first Spaniards who made contact with Native Americans left several accounts of tremendous historical, anthropological, and literary value. The authors, who were all men, tell the story of conflict and struggle from their own personal experiences.\nChristopher Columbus wrote the first accounts and his two most important pieces are “Carta del descubrimiento” and “Diario de a bordo.” In these works he describes the first contact he and his crew had with Native Americans as they toured several Caribbean islands.\nHernán Cortés, the famed conqueror of the Valley of Mexico, wrote “Cartas de relación” of which“Segunda carta de relación” is the most important. In the second letter he gives a detailed account of his journey to topple Tenochtitlán, the Aztec capital.\nBernal Díaz de Castillo wrote “De historia verdadera de la conquista de la Nueva España” that presents his personal account of the conquest of Mexico as a solider of Cortés.\n“Naufragios” was written by Álvar Núñez Cabeza de Vaca. This work describes the Native Americans he and his fellow shipwrecked explorers met and encountered during their eight-year journey. They were lost near present-day Galveston, Texas and together they wandered from the Gulf Coast through the present-day U.S. Southwest back to Mexico City.\nIn “Brevísima relaciónde la destrucción de las Indias” author Bartolomé de las Casas gives an extraordinary account of several encounters he saw first-hand as he traveled throughout the Americas.\nHe was in a unique position as a friar and bishop to travel and he took an extended journey to an unprecedented number of places. His work is valuable because it documented many of the atrocities and the brutal treatment committed by the Spaniards against the local populations.\nUpon his return to Spain he became one of the most vocal critics of the abuse and cruelty he witnessed. Casas helped to push for new laws and protections thereby becoming one of the first Europeans to fight for human rights in the Americas.\nAn important local account about New Mexico can be found in “Historia de la Nueva México” which was written by Gaspar Pérez de Villagrá. In it he gives his account of the arrival of Juan de Oñate, soldiers, and colonists. It begins with the founding of the first Spanish-Mexican settlements in and around Santa Fe.\nBut most importantly it chronicles the brutalities committed against the members of the Pueblo de Ácoma by Oñate and his men.\nAll of these historical accounts of the first contacts between Native Americans and Europeans have been translated to English. As documents in the public domain, they can be consulted online as well as in public and university libraries.\nFrontera NorteSur: on-line, U.S.-Mexico border news\nCenter for Latin American and Border Studies\nNew Mexico State University\nLas Cruces, New Mexico\nFor a free electronic subscription"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:d7b80510-804f-4da2-90cc-bfd3515e9ed6>"],"error":null}
{"question":"What is the relationship between prolactin and oxytocin in promoting successful breastfeeding and maternal bonding?","answer":"Prolactin and oxytocin work together to support breastfeeding and maternal bonding. Prolactin is responsible for facilitating milk production and is thought to help bring on baby bonding feelings by increasing maternal sensitivity and instincts. Oxytocin controls milk expulsion (up to 80% of milk is not available without it), stimulates milk production, and creates a calm state in nursing women. Together, these hormones foster love for the infant, with oxytocin helping to maintain prolactin levels even in the presence of stress. When mothers interact with their babies through breastfeeding, both hormones are released, activating pleasure centers in the brain and strengthening the mother-infant bond.","context":["Extract from Kathy Fray’s book\nOh Baby…Birth, Babies & Motherhood Uncensored\nDid you pack the super-glue for bonding, honey?\nThe emotional Velcro ‘bonding theory’ first appeared in the mid-1970s, and by the 1980s ‘bonding’ had become an accepted maternity term; after which the process became analysed and scrutinised to the point of creating another term — ‘poor bonding’.\nThere are hundreds of factors, physical and emotional, which influence the mother–baby ‘bonding’ process. Lots of new mums do not always experience the ‘instantly-in-mother-love’ emotions.\nCertainly, there are correlations between how good or traumatic a birth is and how smoothly bonding occurs. For some mums with a new baby, there are feelings of bondage well before there are feelings of bonding.\nSo for many women, particularly in the first 2–3 days, their dominant feeling is a kind of weird, distant, blank indifference towards their baby and motherhood as they slowly begin to come to terms with the enormity of it all, and go over in their mind their experience of childbirth. That is a very normal reaction.\nBonding is a gradually unfolding experience that can take hours, days, weeks or months to evolve, like a flower opening its petals — some studies have shown that almost half of women don’t love their newborns straight away.\nBut most new mums (and dads) worry about whether they’re feeling adequately bonded with their new baby, as if bonding is some super-glue that gushes out of a mother at childbirth, instantly sticking together the new mother and her newborn in some wondrous connection. You may be wondering why yours is not the love-at-first-sight experience you were imagining it would be, but often it’s just not like that. Welcome to the new world of worrying about your child!\nFor lactation a mother’s body secretes a hormone called prolactin, which is thought by some to help bring on those baby bonding feelings, stimulating a mother’s connection with her baby, making her more in sync with her newborn through increased maternal sensitivity (instincts). In a way, this makes a load of sense — how do other mammals know how to mother their young if not by instinct? So, it would be logical that the human babies’ mothers too are provided with maternal instincts at a biological level.\nBut if, deep inside, you are already feeling anxiety or guilt over your indifference and lack of bonded feelings with your brand new charge (a total stranger, let’s face it), then realise that you are not abnormal in any way. Many new mums experience feelings towards their newborn that are like a pendulum swinging from tense exhilaration to nervous despair.\nThere is a substantial amount of literature available designed to enhance your skill at bonding effectively with your baby — and a lot of it, from an idealistic Mother Nature perspective, is quite correct. But some of it can feel like propaganda. It is possible to be beautifully bonded to your baby without following any particular theory — and it is possible to feel completely unbonded after following every theory ever invented.\nPerhaps the most useful bonding tools are calm, blissful moments of getting to know each other, such as sitting quietly, chatting face to face up close, or holding your baby skin to skin on your chest, or watching your baby sleeping, breathing, and her expressions.\nBonding is like childbirth: each journey takes its own unique path. It’s not like a switch that gets flicked on. Simply, one day you realise you know what your baby needs; or you miss your baby dreadfully when he’s with a babysitter; or you experience milk letdown at the same time your baby across town cries to her nana for a feed; or you have a beautiful and vivid dream of your baby when he’s older; or you find it hard to remember life without your wee bub; or you just know, intuitively, that there is some psychic connection between you both.\nThis is the day that you finally realise this little person has squirmed his way deep inside your soul, and that you would give your life for your wee babe … and by then, trust me friend, you are well and truly bonded!","27 Sep The Perfect Dance of Birth Hormones\nOur amazing birthing body working in total synchronicity!\n“The hormones that make birth happen, also prepare us for breastfeeding and mother-infant attachment” – Dr Sarah Buckley\nOur birthing hormones are critical to actually making labour and birth happen! In addition, our hormone systems prepare us in the final weeks, days and hours – for an efficient labour and birth, help with labour pain and stress, ensure a safe birth for our babies and, after it is all over, give us feelings of reward and pleasure as we meet our babies for the first time. But for all this to happen in perfect harmony – to work the most efficiently – the birthing hormones need to work together in perfect synchronicity.\nSo how do we achieve this perfect dance?\nOur birthing hormones are made up of 4 main hormones\n- Oxytocin – the Love hormone\n- Endorphins – the Pain-relieving hormone\n- Catecholamines – the Stress hormones – the fight or flight hormones\n- Prolactin – the Mothering hormone that facilitates breastfeeding\nWhen we are pregnant we need to choose a kind, caring healthcare professional who will create a supportive environment for pregnancy and birth. They need to answer our questions, allay our fears, be supportive and encouraging, increasing our confidence in our own ability to grow a baby and birth it.\nIt is important to attend childbirth education classes where our increase of correct knowledge can allay our fears and give us the confidence to trust our bodies to birth correctly. As well as to understand this perfect dance of birthing hormones in order to achieve the best outcome for mom and baby at birth.\nThis kind supportive environment will decrease pregnancy stress which decreases the release of our catecholamines – our stress hormones – which cause us to fight or take flight.\nExercise during pregnancy – this prepares our body for labour as well as generating higher levels of endorphins which will lead to a shorter, less painful labour. Research also shows that regular exercise decreases the risk of needing a caesarean.\nSlight increases of catecholamines in the last few weeks and days of pregnancy causes the foetal lungs to mature preparing the baby’s lungs for breathing after birth as well as maturing other foetal organs.\nThe hormones of mom and baby are interrelated and work in synchrony together to get the best outcomes for both mom and baby. What mom is exposed to during her pregnancy, labour and birth will influence what effects she experiences during this time, which goes on to effect the hormonal balance of the foetus and baby. They are totally interconnected – mom and baby can’t be separated. They promote and inhibit each other’s hormonal activity and dance.\nLet labour start on its own.\nBirth in a safe, private, relaxed atmosphere to keep your stress hormones and the balance of hormones dancing perfectly.\nOnset of labour – increases in oxytocin and prostaglandins will cause the uterus to supply effective contractions in labour.\nEndorphins prepare for pain relieving mechanisms\nIncrease in oxytocin prepares for breastfeeding and maternal-infant bonding\nMaintain a calm, low stress, supportive environment in labour so labour isn’t slowed down by the wrong hormones interacting – lowers catecholamines.\nLower levels of stress by labour support – mom able to cope with pain, not need an epidural\nLower levels of stress – promotes uterine blood supply, improving foetal circulation and newborn well-being\nEustress – healthy levels of stress – slightly increased catecholamines let mom remain alert and focused during labour. Short term foetal catecholamines also increase in late labour – protects foetus from hypoxia (low levels of oxygen) and promotes transition of breathing, temperature and glucose levels. Also promotes newborn alertness which benefits breastfeeding and maternal-infant bonding. Too much stress – high catecholamine levels – cause blood to flow away from uterus and baby – flight response – causes labour to stall. Moms labour often stalls as she is admitted to hospital in this unfamiliar stressful environment\nUse your relaxation techniques taught to you at childbirth education classes to keep you relaxed and with low stress levels. Relaxation, guided imagery and massage are wonderful tools to help keep this balance in place. Use non-pharmacological pain relieving methods like water, positions, massage – this promotes the release of endorphins – the natural pain relieving hormones.\nMovement and upright posture enhance endorphin release\nIn late labour – Oxytocin peaks and assists with the pushing stage of labour\nIf stressed, the catecholamines will disrupt labour.\nStressful labour – increase of catecholamines – prolongs labour – foetal hypoxia – increases mortality and morbidity. Slow labour and foetal hypoxia are common reasons for labour interventions\nCascade of interventions\nMom has an epidural – Mom’s oxytocin levels drops – leads to synthetic Pitocin being given to counteract this effect. Prolonged use of Pitocin blocks the real oxytocin receptors and increases the mom’s risk of postpartum haemorrhage (excessive bleeding after birth).\nWomen who give birth vaginally release oxytocin more effectively and sooner than a woman who had a caesarean\nSkin to skin\nPromote skin to skin – increases mom and baby oxytocin levels – supports breastfeeding success – enhances maternal-infant bonding – decreases risk of postpartum haemorrhage.\nFollowing birth, stress levels decrease – catecholamine levels decrease – increases in oxytocin and endorphins – the calming and rewarding hormones – mom views birth as more positive\nSupport the early initiation of breastfeeding, which also promotes the release of calming, rewarding hormones for mom and baby. The first breastfeed calms the baby after being born\nProlactin levels increase – facilitates breastfeeding\nFoetal oxytocin increases with skin to skin – promotes a calm and alert state – facilitates breastfeeding\nUninterrupted skin to skin – increases oxytocin and prolactin – promotes breastfeeding\nSkin to skin promotes moms vasodilation (blood vessels opening) – warms the infant\nSkin to skin reduces newborn stress and stress hormones – improves energy consumption, glucose levels, breathing, crying and breastfeeding behaviours\nOngoing skin to skin during those early days and weeks benefits moms mental health via oxytocin and prolactin peaks – both stress reducing\nWhen we interact with our babies – hold, carry, touch, talk to, breastfeeding – we are rewarded with the release of oxytocin, beta-endorphins and prolactin, which give us pleasure by activating the dopamine-related pleasure centres in our brains. The more pleasure we get from interacting, the more we want to be with our babies, which benefits their health and development.\nEncourage them to dance\nThe Dance of Normal Birth\n- Learn the steps – birth preparation – how the body works, understanding labour\n- Practice – imprinting coping strategies, visualization, affirmation, confidence building, trusting the system\n- Assign support roles – create a birth dance friendly environment , choose dance instructors – the role of support in normal birth, doulas, female support (bringing oxytocin to the birth), preparing or holding the space, carrying and lifting\n- Labouring (Dancing) – accepting the sensations of birth, feeling the rhythm, moving to the beat of labour, using the space, finding the path – relaxation, rhythm and ritual, preserving the natural hormonal benefits.\nOur Amazing Birthing Body – the Perfect Dance of Birthing Hormones\nThe Love Hormone\nWhat does oxytocin do?\n- Decreases blood pressure\n- Decreases stress hormones; causing the tend and befriend effect\n- Fewer aches and pains (increased pain threshold)\n- Sleepiness, feeling of laziness – not wanting to jump up and do something\n- Reduces muscle tension, relaxing (less circulation to muscles)\n- Calming, less upset about big life’s problems, sense of contentment, peaceful\n- Causes more curiosity and a sense of closeness to the people nearby\n- Makes women more courageous\n- Increases openness to touch\n- Increases digestion and fat absorption and storage. At first oxytocin reduces our appetite but when present over a long period of time, increases appetite\n- Increased skin and mucous membrane circulation\n- Causes blood vessels on the frontal side of body to dilate, thus rosy cheeks and warm chests during labour, nursing, or even just snuggling an infant\n- Facilitates learning\n- Promotes wound healing (twice as fast)\nWhat stimulates the release of oxytocin?\n- Stroking – especially rhythmic touch, therapeutic massage, sexual stimulation\n- Descent of the foetus (pressure against cervix, vagina and the perineal floor)\n- Good food\n- Companionship (positive emotional contact, doulas and being with women)\n- Quiet and low light\n- Positive thoughts, associations and memories, meditation and visualization\n- Suckling and baby kneading the breast\n- Pleasant smells and even sounds – as you smell your newborn baby – you have a rush of oxytocin released\n- The presence of oxytocin. In other words we produce more oxytocin when there is oxytocin present!\nOxytocin and Nursing\n- Controls the expulsion of milk (up to 80% of milk not available if this doesn’t occur)\n- Stimulates milk production\n- Redistributes heat in the mother’s body to her front to warm her nursing young (often have rosy cheeks when done nursing – both mom and baby)\n- Helps the body release stored nutrients in order to make milk\n- Reduces blood pressure and stress hormones in the mother – breastfeeding mothers have lower stress hormone levels than bottle feeding mothers\n- Creates calm in most nursing women\n- Makes mother more interested in close relationships (probably so she’ll have help in caring for her child)\n- Induces social memory and calmness in the infant\n- May have some role in the release of dopamine and endorphins which gets us hooked on our babies, and sometimes hooked on nursing\n- In combo with Prolactin, fosters love of infant\n- Helps maintain Prolactin levels in the presence of stress\nOxytocin and Touch\n- Lower BP\n- Higher pain threshold\n- Lower level of stress hormones\n- Increased growth in both young and adult animals\n- Increased social interaction\n- Improved learning\n- “Feel” better\n- “Psychological” touch can result in the same effects\n- Effect is sustained with repetition of touch/40 beats per minute\n- Ventral stroking the most effective – HUGS. Women whose partner hugged them more have higher oxytocin levels and were happier.\n- Dairy farmers get 25% more milk if cows are brushed and petted during milking.\n- Eating is considered an “internal massage” as the GI tract is made from the same germ layer as our skin and nerves.\n- Exercise/movement also results in the release of oxytocin (in the brain it results in calming and in the blood stream stimulates endorphin release—runners high).\nOxytocin and Relationships\n- Creates emotional bonds\n- Makes us seek out social affiliations, most often with other women, although plays a part in choosing and staying connected to male partners as well\n- Increases our willingness to trust\n- Good relationships reduce blood pressure and heart rate, cholesterol levels – lengthen life spans.\nMen and Oxytocin\n- Like mothers, dads get a rush of oxytocin when they see their babies for the first time. Men’s testosterone levels tend to plummet (for a couple of months anyway) after they become dads for the first time. Even more intriguingly, some men start to produce extra oestrogen, perhaps the clearest sign of the transformative power of fatherhood. Oestrogen helps make the brain more sensitive to oxytocin, helping dads become more loving and attentive.\n- Dads oxytocin levels increase throughout their partner’s pregnancy and increase his interest in physical, but not necessarily sexual, contact with the mother.\n- Prolactin also rises in dads throughout pregnancy but rises are sharper after parenting the new infant and makes the father feel protective and a little less interested in sex.\nOxytocin is released by stimulation of sensory nerves – as a new baby crawls on its mothers chest so this activates sensory cells in the skin to produce more oxytocin\nThe progress of labour is inhibited by\n- Anxiety and fear\n- Decreased release of oxytocin\nThe progress of labour is stimulated by\n- Increased release of oxytocin\nChanges in moms due to oxytocin release\n- Decreased levels of anxiety\n- Decreased detachment\n- Increased social interaction\n- Facilitated bonding\n- Decreased sensation of pain\n- Changed memory of labour – more positive memories\nIf mother and baby are separated after birth, the positive effects on other-baby interaction and the anti-stress effects normally caused by skin-to-skin contact after birth will not develop\nOxytocin released during labour is needed for sensitivity of the skin for further release of oxytocin\nConsequences of medical interventions\n- Decreased oxytocin levels\n- Decreased prolactin levels\n- Inhibition of milk ejection\n- Decreased milk production\n- Increased stress levels\n- Decreased interaction between mother and baby\n- Interference with bonding and attachment\nThe best way to provide oxytocin is by stimulation of natural oxytocin release\n- Skin to skin contact\n- Social support"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2d8844cd-ded1-49a7-aabb-9f6150423cec>","<urn:uuid:b1b4ef19-f7f7-49ce-9e4a-8038271d2a73>"],"error":null}
{"question":"How can I break free from the cycle of blame and negativity after being wronged?","answer":"While initial blame and negativity are normal reactions to being wronged, you can break free from this cycle by recognizing when you're stuck in maladaptive behaviors like constantly telling others about the unfairness, labeling the offender as evil, and ruminating about what should have happened. Forgiveness reminds us that after the initial helpful reaction of outrage, anger or pain, we have the choice about how long we continue to suffer. The key is accepting what happened and making peace with our full lives to take back power, lose the victim identity, and control negative feelings.","context":["Written by: Frederic Luskin, Ph.D. and Daniel E. Martin, Ph.D.\nIn a competitive and often vindictive society, the opportunity for being hurt, mistreated and injured remain high. Holding on to real or perceived slights creates bitterness, resentment, and sadness which take their toll on physical and mental health. Research on forgiveness interventions has shown them to reduce blood pressure, the experience of pain, anger, depression and stress and lead to greater feelings of self-efficacy, optimism, hope, compassion and ease.\nWhat is Forgiveness?\nForgiveness is a rich word, with varied descriptions and definitions over time and culture. While many exist, for the purposes of the Stanford Forgiveness Project – SFP (Dr. Luskin is the founder and co-director), we define forgiveness as the experience of peace and understanding that can be felt in the present moment no matter what has happened in the past. In this context, we are the only ones who can remedy a situation in which life threw us a curve; which caused us to get upset because what happened was not what we wanted to happen. We got lied to when we wanted the truth. We got abandoned when we wanted care. Forgiveness reminds us we hold the keys to being assertive and creating peace in the present, no matter how recent or long ago a painful experience occurred.\nLife often unfolds in a chaotic way, and it’s our unmet expectations about the way it should play out that create much of our suffering. All of us have been in situations where our well-intentioned plans have gone array, be it as a student and not getting the grade, we believed we deserved, a job candidate believing we were the right person for the job and not getting it, an employee seeking a promotion and not getting it, or a life partner finding out it is over, or the patient, getting a terrible diagnosis. Life can be very challenging, and we are here to say it is our continued and often escalating objection to not getting what we want that causes emotional, social and physical pain.\nForgiveness is the resolution of this objection, it’s making peace when you didn’t get what you want and letting yourself move forward. Forgiveness doesn’t justify cruelty, nor is it to forget that something painful has happened, or even reconciliation with an offender. It is the choice to accept, the making of peace with our full lives that helps to take back power, lose the victim identity and control negative feelings.\nMaking Choices and Moving On:\nReflecting on life’s capriciousness the question becomes how strongly and for how long do we want to suffer and how much do we want to make choices that may lead to more peace and ease. For example, as a parent if someone hurts your child, you are going do everything and anything possible to minimize that harm. Afterwards as we process the threat and fear, we often end up with resentment towards the person who did the harm.\nThis blame and negativity is normal, and can be helpful. We must care for ourselves and those we love through chaotic and difficult life situations, and this requires analyzing what went wrong, who did the harm and how that can be stopped, Unfortunately, we often get stuck in blame and negativity, and go off into the maladaptive. We tell everyone we can about how unfair what happened to us (and the ones we love), then label that person as evil as they caused us to suffer. We ruminate about the experience, thinking about what should have been the outcome, building more and more resentment and creating in our minds our victimhood. This creates a negative feedback loop and can create patterns that can be hard to break. Forgiveness reminds us that after the initial helpful reaction of outrage, anger or pain, we have over time the increasing choice as to how long we suffer and whose responsibility our healing is.\nDr. Luskin’s work has led to many successful interventions in his book Forgive for Good, but one powerful approach from the Stanford Forgiveness Project is the mnemonic HEAL: This is practiced as guided imagery.\n- H for Hope – What did we hope for, as a positive outcome that didn’t occur (Example: I wanted a new job to better support my family).\n- E for Educate – Educate oneself that Hopes may not be achieved in a specific fashion, but also (growth mindset) that each Hope may have several opportunities. (Example: I understand that not all candidates get every job they want, but there are more opportunities out there to apply to).\n- A for Affirm positive intention. Holding on to grievances connect us with those who have hurt us in a powerless fashion. Affirming our intention to grow and prosper reminds us that we can grow and learn from negative experiences (Example: I will review my applications, resume, join and interact with relevant associations while asking for help in my pursuit of my goal).\n- L for Long term commitment to one’s own well-being. Understanding that we might slip back into rumination of our grievance, this commitment reminds us to revisit the previous steps and continue to pursue our goals (Example: Yes! I got a new job, and it is going well, but I am still looking for something more meaningful and lucrative to support my family. I will continue to try my best and have patience. It may take a bit of time, but I am working in my own behalf.\nVulnerability allows for love and loss:\nIf we were afraid all the time, protecting ourselves from hurt, there would be no safe space for love. The free will that allows us to make mistakes allows us to pursue life’s happiness. The basic choice of trusting enough to love even though we may get hurt is one of life’s most important decisions.\nWe often review the grievances we have built up over life, and dwell on others poor treatment of us, or our own mistakes. Dwelling on losses and mistakes is an option. So is to dwell on acts of goodness and love. Think of the people who have loved you, supported you, cared for you, educated you. When you do, you connect with an unbelievable source of care and love. Remembering that each time those folks chose to be kind, chose to do the right thing and chose to love they made a decision. If we dwell too much on the bad choices people made its hard to remember how often and deeply people chose to love and care for us. Ultimately, we all have a regular decision to make: to forgive the past, so we can focus on what is good in our present.\nThe Stanford Forgiveness Project and Brightsity are happy to announce the application of the SFP to the Brightsity platform.\nRecognizing and Managing Exceptional, Average and Underperforming Individuals\nHow to Follow Your Financial Playbook To Victory\n3 Ways to Build Mental Toughness\nThe Four Ingredients of Great Customer Experience\n5 Sales Tips to Break Through the Clutter\nThe Plight of the Long-Term Unemployed and How to Overcome It\nWhat High-Converting Landing Pages Have in Common\n4 Ways to Boost Your Motivation at Work\nEntering Into the Sales Evolution Showroom\nMillennials and Responsible Investing: Bridging the Generation Gap\nLearn9 hours ago\nMillennials and Responsible Investing: Bridging the Generation Gap\nSocial Selling9 hours ago\nIs Spending Piles of Money on Marketing Just a Waste?\nBuilding Smarter Portfolios9 hours ago\nUnderstanding Hedge Fund Replication\nInsights2 days ago\nLeaders: How Your Audacious Goal Can Actually Hurt People\nDevelopment2 days ago\nDiscouraged? Remember Why You Got in This Business\nFinancial Podcasts2 days ago\nWhat to Expect this Holiday Season According to the National Retail Federation\nDevelopment3 days ago\nA Better Way to Seek Referrals\nBehavioral Intelligence3 days ago\nHuman Behavior Is Predictable, Markets Are Not"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3d42c2d0-ad70-4670-956c-df5866793e0a>"],"error":null}
{"question":"Do silkworms and sheep both produce natural protein fibers?","answer":"Yes, both silkworms and sheep produce natural protein fibers. Wool fiber from sheep is chiefly composed of the animal protein keratin. Similarly, silk is also an animal protein fiber, produced by the silkworm (Bombyx mori) during its larval stage. Both are classified as animal-based natural fibers, though they are processed differently - wool through shearing and spinning, silk through harvesting from cocoons.","context":["The raw, natural materials are spun into threads and yarns that are then woven or knit into natural fabrics. There are two general categories of natural fibers: animal-based or plant-based. Animal-based natural fibers include silk and wool, while plant-based natural fibers include cotton, linen, and jute.\nWhat type of yarn is wool?\nWool yarn is often the first type of yarn that people think of when it comes to knitting or crocheting. Wool is a warm fiber that is great for sweaters, jackets, hats and scarves. The range of wool yarns available is wide—from hearty Icelandic wool like Lopi to soft merino wool like Valley Yarns Valley Superwash DK.\nIs yarn a synthetic material?\nYarn is a strand composed of fibres, filaments (individual fibres of extreme length), or other materials,… Yarns are made from both natural and synthetic fibre, in filament or staple form. Filament is fibre of great length, including the natural fibre silk and the synthetic fibres.\nWhat are natural yarns?\nNatural Yarns. These types of yarns are derived from fibres which commonly occur in nature, either from plants (e.g. cotton/linen), animals (wool), or even minerals (basalt) where fibrous materials are spun into filaments which can then be further processed by weaving.\nIs cotton yarn natural or synthetic?\nMost people know that cotton is a natural fiber and polyester is a man-made, synthetic fiber. These differences are just the beginning, however, so let’s take a deeper dive on the unique properties of these fibers and how they’re processed.\nWhat is wool made of?\nWool fibre is chiefly composed of the animal protein keratin. Protein substances are more vulnerable to chemical damage and unfavourable environmental conditions than the cellulose material forming the plant fibres.\nIs yarn made of wool?\nWhat is Yarn Made From? Yarn can be made from a variety of different fibers. This includes both natural and synthetic fibers. … Animal fibers are also often used, such as wool, harvested from sheep, as well as cashmere (harvested from goats) Angora (from rabbits), and silk (from insect larvae).\nWhat is synthetic wool?\nAcrylic is known as artificial wool or synthetic wool because it resembles wool. Acrylic is cheaper than natural wool and can be dyed in various colour. This makes acrylic is very popular among other fabrics.\nHow is yarn made from wool?\nSpinning the Wool into Yarn\nSpinning turns the wool pieces into a material that’s usable. Spinning uses a wheel to spin 2-5 strands of wool together. This forms long, strong pieces of wool that you would recognize as yarn. Different processes create different kinds of yarn that work for distinct final products.\nAre yarn and wool the same thing?\nWool is a type of yarn. Yarn generally refers to long continuous interlocked fibers that may be used for textiles, knitting, weaving, etc. Yarn may be of different materials including cotton, animal fleece, synthetic, etc. Cotton or synthetic yarn would generally be used to produce textiles or fabric.\nIs yarn a natural resource?\nWe’re determined to produce high quality yarn along with providing a livelihood for our hardworking employees. Environmental benefits: Natural fibers like wool, cotton, and mohair are a renewable resources, whereas acrylic yarns are made with man-made plastic-like materials.\nIs yarn a synthetic fiber?\nThe difference between natural and synthetic yarns is that synthetic yarns are man-made fibers that are spun into yarn, while natural yarns are made up of fibers from animal hair, plants, or even certain types of insect cocoons.\nWhy is wool a natural fiber?\nWool is a natural fiber. It has evolved to produce a fabric that has become one of the most effective & renewable natural forms of all-weather protection known to man. Every year sheep produce a new fleece, making wool a renewable fiber source.\nIs wool a natural fiber?\nNatural fibres can be classified according to their origin. The vegetable, or cellulose-base, class includes such important fibres as cotton, flax, and jute. The animal, or protein-base, fibres include wool, mohair, and silk.\nIs polyester a synthetic?\nPolyester is a manufactured fibre made by combining a long-chain synthetic polymer with an ester from a substituted aromatic carboxylic acid. … It’s a strong fibre that’s resistant to stretching and shrinking, and is also wrinkle resistant. Polyester fabric can be combined with a variety of other fabric fibres as well.","Cultivating, Curating, Producing and Harvesting Silk For Weaving Rugs\nMost commercial silk is harvested from the Bombyx mori, which is also known as the mulberry silk moth, or more colloquially as the silkworm. The silk, an animal protein fiber, is produced by the moth when it is in its larval or caterpillar stage. It is not literally a worm, but bears some resemblance to a worm at this stage of its life cycle.\nSilk has been produced by domesticated species for thousands of years. The Bombyx mori was domesticated from the wild Bombyx mandarina, and eventually came to differ substantially from it. The Bombyx mori has lost its pigmentation and its ability to fly, and is largely dependent on humans for its reproduction.\nLife Cycle of the Silkworm\nCultivation of silk from domestic silkworms is called “sericulture.”\nThe first stage of the silkworm life cycle, and hence the first stage of the sericulture process, is the laying of eggs. This is typically done in an aluminum box or other container. Each egg is as tiny as a pinhead. An adult female silkworm (that is, one that is in the moth stage of its life cycle) typically lays 300-400 eggs at a time. It dies almost immediately upon laying its eggs, and the adult males die shortly thereafter. In fact, the adult stage of the life cycle is very brief; the adults possess only a rudimentary mouth and do not even eat.\nThe eggs are examined carefully for any sign of disease. If no such problems are detected, they are incubated for about ten days.\nWhen the eggs hatch, this commences the larva stage. The caterpillars are only about a quarter inch long when they emerge from the egg, but they grow rapidly to a size of about three inches as they do little more than eat constantly during this stage. The caterpillar will ultimately eat about 50,000 times its initial weight.\nA fine layer of gauze is placed over the caterpillars when the eggs hatch. They are fed primarily chopped white mulberry leaves, hence the alternate name of mulberry silk moth. Lettuce, Osage orange, and other plant material is also sometimes used as feed. The caterpillars eat and grow for about six weeks, shedding their skin four times along the way.\nAt this point, the caterpillars are ready to spin their cocoons, which takes three to eight days. This is when the silk is produced.\nThe silkworm climbs up onto some sort of frame that has been provided for that purpose in its container. A pair of salivary glands in the silkworm called “sericteries” produce a clear, viscous fluid called “fibroin,” while another pair of glands produce a gummy fluid called “sericin.” The silkworm forces these substances through openings—spinnerets—in its mouthpart. The fibroin secretions harden immediately when exposed to the air, forming twin filaments. These filaments are then immediately bonded together by the sericin.\nThis is the silk of the silkworm.\nFor the next several days, the silkworm meticulously builds its cocoon with this silk, following a figure-8 path hundreds of thousands of times. Over the course of this time it produces approximately one kilometer of silk.\nHarvesting the Silk\nAt this point, the cocoon is carefully heated to loosen the silk. The silk is softened, and then gradually unwound. Generally the silk of four to eight cocoons is unwound together, forming one thicker strand.\nThrough a process called “throwing,” the raw silk is then twisted into a strand strong enough not to split into its constituent fibers. This renders it suitable for knitting or weaving.\nThere are four different common types of silk thread produced this way, each by a slightly different method of throwing. Crepe is produced by twisting multiple threads together, and then twisting them a second time. Tram is produced similarly, but the silk threads are only twisted in one direction instead of two. Thrown singles are produced with just one thread rather than multiple. Like tram they are only twisted in one direction. Finally, organzine is produced by giving individual silk threads a twist before combining two of them and twisting them in the opposite direction.\nCrepe thread is used for weaving wrinkly fabrics, tram thread is used for the weft part of the weaving process, thrown singles thread is used for sheer fabrics, and organzine thread is used for the warp part of the weaving process.\nRaw silk still contains the binding agent sericin. Soap and boiling water are then used to remove this sericin. This leaves the silk as much as 30% lighter than the raw silk, as well as softer and more lustrous.\nThough a kilometer of silk from each cocoon sounds like a great deal, in fact very little survives this process as usable silk. It typically takes about 2,500 silkworms to produce one pound of raw silk.\nFilaments that cannot be used as silk can be salvaged and processed into a cheaper type of yarn called “spun silk.”\nAlternatives to Silkworms For Cultivating Silk\nMany other animal species produce silk. Silk is produced by many species of bees, wasps and ants, often for nest construction. Most famously, spiders and other arachnids use silk to spin their webs.\nNone of these other species have been used for commercial textile production, though research continues into whether any such alternative to the silkworm might be viable. Already the silk from some of these other animals has been used in weapons and optical instruments such as telescopes.\nWeaving Rugs Using Silk\nAs noted above, silk is an animal fiber that is derived from the cocoons of silkworms. It is an extremely costly and luxurious material for textile and rug production. Cultivation of these fibers began in ancient China where it was a jealously guarded secret. Eventually its use spread to Persia and then to Byzantium and Europe.\nThe expense notwithstanding, rugs with silk pile and or foundations, are not uncommon, although they tend to be high quality pieces in the tradition of court art.\nExtremely luxurious nomadic carpet weaving have been known to use the fibers as a sign of class and significance. The attraction to these rugs resides in the fineness of its fibers which are remarkably soft, as well as in it’s luminous, reflective quality.\nBecause of this the effect of color on silk is far more intense and brilliant than the effect of the same dye on even the finest wool. These fibers when treated (see below) are much more delicate and less durable than wool.\nConsequently, many less silk rugs are well preserved. This rarity, as well as the basic cost, places antique silk pieces among the most expensive rugs.\nThese fibers were bought and sold by their weight. As such, some traders would treat the fibers with metal. This metal treatment, increased the overall weight which resulted in more money for the dealer.\nUnfortunately, the metal corrodes with time. This is what causes some antique carpets to “break”. So when buying these types of antique rugs, it is important that the piece be in excellent starting off condition – that is a great sign that the fibers have not been treated and should last for many generations to come."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:836df2c3-09e1-428a-8b33-c8e70805131c>","<urn:uuid:60db1088-ab34-4c0d-94a3-3f69c7f05940>"],"error":null}
{"question":"How does the historical significance of the General Sherman sequoia compare to the role of elms in shaping U of T's campus development?","answer":"General Sherman, as the largest non-clonal tree by volume in the world at around 2,500 years old, stands as a majestic natural monument in California's Sequoia National Park. In contrast, elm trees played a pivotal role in U of T's architectural history, with one particular elm tree in 1856 actually determining the orientation of UC College's construction. The governor general, considering this elm 'the handsomest tree about Toronto,' insisted the building face south rather than west to preserve the tree. Ironically, that historic elm fell in a storm the following year. Elms were once the dominant species at U of T, with 90% of campus grove trees being American elms, though their numbers have drastically declined due to Dutch Elm disease.","context":["Environment Planet Earth The World's 10 Oldest Living Trees By Bryan Nelson Writer SUNY Oswego University of Houston Bryan Nelson is a science writer and award-winning documentary filmmaker with over a decade of experience covering technology, astronomy, medicine, and more. our editorial process Twitter Twitter Bryan Nelson Updated June 17, 2019 Vanea Cera / Flickr / CC BY 2.0 Share Twitter Pinterest Email Environment Weather Outdoors Conservation There are colonies of clonal trees that have lived for tens of thousands of years, but there's something majestic about a single tree able to stand on its own for millennia. These ancient trees have borne witness to the rise and fall of civilizations, survived changing climates, and even persevered through the fervent development of human industry. They are a testament to the long view that Mother Nature takes in tending the Earth. With that in mind, consider these 10 of the world's oldest living trees. 1 of 10 Methuselah Chao Yen / Flickr / CC BY-ND 2.0 Until 2013, Methuselah, an ancient bristlecone pine was the oldest known non-clonal organism on Earth. While Methuselah still stands as of 2016 at the ripe old age of 4,848 in the White Mountains of California, in Inyo National Forest, another bristlecone pine in the area was discovered to be over 5,000 years old. Methuselah and its unnamed senior pine's exact locations are kept a close secret in order to protect them. You can still visit the grove where Methuselah hides, but you'll have to guess at which tree it is. Could this one be it? 2 of 10 Sarv-e Abarqu Ninara / Flickr / CC BY 2.0 Sarv-e Abarqu, also called the \"Zoroastrian Sarv,\" is a cypress tree in Yazd province, Iran. The tree is estimated to be at least 4,000 years old and, having lived through the dawn of human civilization not far away, it is considered an Iranian national monument. Many have noted that Sarv-e Abarqu is most likely the oldest living thing in Asia. 3 of 10 Llangernyw Yew Emgaol / Wikimedia Commons / CC BY-SA 3.0 This incredible yew resides in a small churchyard of St. Dygain's Church in Llangernyw village, north Wales. About 4,000 years old, the Llangernyw Yew was planted sometime in the prehistoric Bronze Age — and it's still growing! In 2002, in celebration of the golden jubilee of Queen Elizabeth II, the tree was designated as one of 50 Great British trees by the Tree Council. 4 of 10 Alerce Gonzalo Zúñiga Solís / Wikimedia Commons / CC BY-SA 4.0 The Alerce is a common name for Fitzroya cupressoides, a towering tree species native to the Andes mountains. There's almost no telling how old these trees can get, since most of the larger specimens were heavily logged in the 19th and 20th centuries. Many botanists believe they are the second-longest living trees on Earth aside from the bristlecone pine of North America. To date, the oldest known living specimen is 3,646 years old and is appropiately called Grand Abuelo. 5 of 10 Patriarca da Floresta Marcus V. Bellizzi / Wikimedia Commons / CC BY-SA 3.0 This tree, an example of the species Cariniana legalis named Patriarca da Floresta in Brazil, is estimated to be over 2,000 years old, making it the oldest non-conifer in Brazil. The tree is believed to be sacred, but its species is widely threatened due to forest clearing in Brazil, Colombia and Venezuela. 6 of 10 The Senator Ebyabe / Wikimedia Commons / CC BY-SA 3.0 Though the Senator suffered tragedy in 2012 after a fire caused much of the tree to collapse, this iconic tree bears mentioning here. Formerly located in Florida, the Senator was the largest bald cypress tree in the United States, and was widely considered the oldest of its species known to exist. It was also likely the largest U.S. tree of any species east of the Mississippi River. Estimated to have been around 3,500 years old, the Senator was used as a landmark for the Seminole indians and other native tribes. The Senator's size was particularly impressive because it endured many hurricanes, including one in 1925 which reduced its height by 40 feet. The tree got its name from from Sen. M.O. Overstreet, who donated the tree and surrounding land in 1927. 7 of 10 Olive Tree of Vouves Eric Nagle / Wikimedia Commons / CC BY-SA 4.0 This ancient olive tree is located on the Greek island of Crete and is one of seven olive trees in the Mediterranean believed to be at least 2,000 to 3,000 years old. Although its exact age cannot be verified, the Olive Tree of Vouves might be the oldest among them, estimated at over 3,000 years old. It still produces olives, and they are highly prized. Olive trees are hardy and drought-, disease- and fire-resistant — part of the reason for their longevity and their widespread use in the region. 8 of 10 Jōmon Sugi Σ64 / Wikimedia Commons / CC BY 3.0 Jōmon Sugi, located in Yakushima, Japan, is the oldest and largest cryptomeria tree on the island, and is one of many reasons why the island was named a UNESCO World Heritage Site. The tree dates to at least 2,000 years old, but some experts believe it could be older than 5,000 years old. Under that theory, it's possible that Jōmon Sugi is the oldest tree in the world — even older than Methuselah and its brethren. Regardless of the numbers, it's a tree that deserves mention here. 9 of 10 Chestnut Tree of One Hundred Horses LuckyLisp / Wikimedia Commons / CC BY-SA 3.0 This tree, located on Mount Etna in Sicily, is the largest and oldest known chestnut tree in the world. Believed to be between 2,000 and 4,000 years old, this tree's age is particularly impressive because Mount Etna is one of the most active volcanoes in the world. The tree sits only 5 miles from Etna's crater. The tree's name originated from a legend in which a company of 100 knights were caught in a severe thunderstorm. According to the legend, all of them were able to take shelter under the massive tree. 10 of 10 General Sherman David Prasad/ Flickr / CC BY-SA 2.0 Believed to be around 2,500 years old, General Sherman is the mightiest giant sequoia still standing. The volume of its trunk alone makes it the largest non-clonal tree by volume in the world, even though its largest branch broke off in 2006, smashing part of its enclosing fence and cratering the pavement of the surrounding walkway. Perhaps this was a sign that General Sherman could not be caged in? Sherman can be found in Sequoia National Park in California, where five of the 10 largest trees in the world exist.","Looking for the oldest tree at U of T\nVelut arbor aevo. This Latin phrase is U of T’s motto, and means “as a tree through the ages,” expressing the hope that U of T will grow large and impressive like a mature tree. So to locate the oldest tree on campus is thus to find the tree that best embodies the motto of the school, and to find U of T’s very own living, breathing, and unacknowledged arboreal mascot.\nThis quest initially seemed like it would be over in a few minutes. Within two minutes of my 11:47pm email, Stan Szwagiel, head of grounds at U of T St. George, emailed me back with an enjoyably definitive answer: the oldest tree on campus is the elm tree behind the UTSU building. An impromptu hike to this location revealed an appropriately large and beautiful tree, which I was surprised I hadn’t noticed before. This tree has street cred: a few minutes after I heard from Mr. Szwagiel, Terry Carleton, a professor at the faculty of forestry, independently told me that this tree was his bet for the oldest on campus. Mr. Szwagiel thought that the size of the tree indicated that it could even be as old as 165 years, which would make it one year older than U of T (which came into existence in 1850), and that it could well be even older.\nThrough Paul Aird, an emeritus professor of forestry who has written a book about ecological fables and nature tales, I learned that the U of T Archives has a 1910 topographical map that lists all the trees that were then on campus. I consulted this map – which is over 6 feet tall and requires two people to open – but, intriguingly, the elm is not on it. The map does have some irregularities; could the cartographers just have missed the tree? For the UTSU elm is visible in an aerial photograph of the campus taken in 1933, and partially visible in a 1924 image. So it is at least 90 years old, and the size of the tree in those photographs makes it seem like it must have been more than a teenager then. And Mr. Szwagiel told me about another large elm tree on campus that recently had to be cut down after disease killed it. That tree had a trunk that was much smaller than the UTSU elm. When it was cut an arborist counted its rings and it was well over 150 years old. It’s enormous stump is still visible beside the path between Whitney Hall and the back campus field.\nIn order to learn more about trees in general I contacted Jennifer Gagné, a former forestry student who now works for the ministry of the environment. She used to lead tree tours on campus and took me on one. Our first stop was the two little copses planted at the centre of the forestry department building. According to Gagné, the one on the north-side is meant to replicate a Carolinian forest (sample species: a tulip tree, a Kentucky coffee tree), and the other a boreal forest (spruce, tamarack). According to Gagné, being enclosed within these buildings is not protecting or helping the trees; the boreal trees especially are not shade tolerant, and random shrubs have taken over the undergrowth. I later learned from Tony Ung, a forestry researcher, that the copses had gotten out of control and that they had to urgently cut out some of the brush. “We feared for the safety of students. People were sleeping in there. We found needles,” he told me. But today it seemed benign, soggy, lonely to me. Though for Gagné, the important personalities, the trees, are still here. To know the names and life stories of trees is to be surrounded by their presences. Human beings depend on trees quite as much as on rivers and the seas; the ability to know them is meaningful. And every tree has a different method to identify it, some more heuristic than scientific, like when Gagné began plunging her palm repeatedly into a conifer’s needles. If it hurts, it’s a Red Pine. And it did hurt. Leaving the copses, Gagné showed me a recent building at New College that had a hole in the third-story concrete balcony to allow the tree to grow through the building. A high-minded design, or maybe because the building was right beside the forestry department, where it’s probably harder to get rid of a perfectly healthy tree.\nThere is an oak tree on U of T’s coat of arms (rounded leaves, hence, a white oak), but an elm tree was probably the most significant tree in U of T’s history, being the deciding factor in the location of U of T’s most iconic building. In 1856, before UC College was built, the governor general and the architect were surveying the grounds. The governor general wanted the building to face west, though to the architect–and to us today–it seemed obvious that it should face south. The decision was made by the fact of a tall elm tree which the governor general, calling it, “the handsomest tree about Toronto,” did not want cut. He is reported to have told the architect: “I am sure that you can never put anything up half so pretty.\" So to save that tree, the building faced south. The tree was toppled in a storm the following year.\nU of T exists atop the ghosts of cut forests and lost trees. If you stood in front of the UTSU elm as time was rolled back a hundred and fifty years, you would see the UTSU building taken down, Soldier’s Tower unspooled, and Hart House deconstructed. You would arrive to see the elm growing beside a thicket of other trees, all nestled together around the banks of a small pond. The pond is McCaul’s Pond (the tree is still on a bit of hill: the rise that used to be the pond’s edge), which was drained in the summer of 1879. Back then, there were only about 200 students at U of T, and the college was a semi-rural retreat, somewhat removed from the city, and nature and trees were among the chief pleasures of the school. Students and professors spent the afternoons taking walks. “No professor,” a student at the time said, “if he could avoid it, lectured in the afternoon, which was reserved for recreation and walking.”\nPeople enjoy trees, but they also cut them down for heat and timber, and to make room for development. But perhaps the most destructive damage that people inflict on trees is accidental, though the spreading of disease. The UTSU elm is diseased with the scourge of elm trees, Dutch Elm disease, which spread to Ontario in 1951. Originating in the Himalayas, and named for the Dutch scientists who identified it, the disease is a fungal parasite. The effects of Dutch Elm are ruinous; Ms. Gagné explains that “elms almost kill themselves, because it triggers them to cut off the water system in their xylem, in order to get the fungus out.” In North America, elms have died by the millions. Many thought Dutch Elm would bring about the extinction of the American elm.\nElms used to be the dominant species at U of T, and now are among the more rare of native trees. There were more than 100,000 elms in metropolitan Toronto in the 1960s, 10 percent of all street trees, and 90 percent of the trees in the groves on the University of Toronto campus were American elm. There are currently 1368 maples to 158 elms. The main road into Kings College Circle used to be marked by a row of tall elms; now it has short young oaks. The introduction of Dutch elm disease in North America is the most significant event in the history of urban forestry. The disease altered urban forestry policy and law, and changed the public’s awareness of street tree management. Arborists trying to combat the devastation of this disease pioneered the profession of “tree health care,” opening up an entirely new industry for managing the care of urban trees. In fact, the term “urban forestry” was coined by a U of T professor, Erik Jorgensen, who was working on dealing with Dutch Elm disease. Even the current emerald ash borer pandemic is directly linked to Dutch Elm disease because most of the ash population are replacement trees for lost elms. In Toronto, 80% of the elm trees have been lost to Dutch Elm disease.\nMr. Szwagiel met me at the UTSU tree, and pointed out the tiny tell-tale holes at the base of the tree where it is has been injected with fungicide to stop the Dutch Elm. The tree gets hooked up to bottles of fungicide like a patient on an IV drip. If enough fungicide can be injected into the tree’s vascular system, then a spore introduced by a beetle won’t be able to germinate. Without this treatment, which is expensive, the tree could die within a year. Mr. Szwagiel also pointed out the extensive series of steel wires strung between the tree’s branches, which that add strength to it’s canopy and keep it from collapsing in storms. Without question this tree would be dead by now if it weren’t for the labour of the forestry professors, grounds crew, and arborists who contribute to it’s well-being. The university makes these efforts because these trees give campus much of its well-to-do, pastoral, collegiate identity. So perhaps trees should take as their motto “as a university through the ages.” The motto’s declaration to make the school like a tree—a line adapted from the roman poet Horace (Odes I.12, lines 45–46) “crescit occulto velut arbor aevo fama Marcelli”: Marcellus’ fame, its up-growth hid / springs like a tree—makes a fine picturesque image, but it does not reflect the fragility of our tree canopies in an age of globalized tree disease. The trees persist, but not without struggle and constant maintenance; which is also true enough for a university.\nU of T St. George currently has over 3,000 trees (more now than in 1910), and, though the UTSU elm is a strong suspect, the jury is out on which one of them is empirically the oldest. Tony Ung says that the only way to know for sure is to take a core sample of the tree. He showed me the apparatus, a sort of tree corkscrew—“from the 1940s, I think”—that he would use to get a sample; a process that, most likely, would not harm the tree. But it is still a risk and Mr. Szwagiel does not want to immediately pursue that option. The age of this tree will thus remain a mystery. But thanks to the care it receives, this elm could live another 300 years. We will send someone then to report on how the university is doing compared to the tree. In the meantime, we can enjoy it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:58632cc5-c726-48aa-b3fe-16e1add6fc97>","<urn:uuid:75273a8f-bf42-495d-bd61-3a5709b9e05c>"],"error":null}
{"question":"What role does neuroplasticity play in both psychedelic treatment and 60-day rehabilitation programs?","answer":"In psychedelic treatment, neuroplasticity is directly enhanced as these substances enable rapid formation of new neural connections and help grow new cells, particularly in the prefrontal cortex, breaking free from established unhealthy mental frameworks. In 60-day rehabilitation programs, neuroplasticity is engaged through various treatment methods including individual therapy, group therapy, and medication-assisted treatment, which work together to help patients develop new behavioral patterns and thinking processes over the two-month period. Both approaches leverage the brain's ability to reorganize itself, though through different mechanisms and timeframes.","context":["Mental health issues like depression and anxiety can take a major toll on your life. When it comes to treatment, these days there are old options such as therapy as well as medication like antidepressants or anxiolytics. But because mental health issues can take many different forms and our brains are always slightly different from each other, there’s no guarantee these traditional treatments will always work. These treatment options can help many patients, but their results are often inconsistent. In fact, one-third of patients do not respond to antidepressants. However, a growing number of recent, exciting developments suggest that psychedelics (substances like LSD, psilocybin, mescaline, or DMT) may be used as a non-traditional treatment method to fix the root cause of the problem and help your brain evolve.\nThe basics of neuroplasticity\nThe brain is a complex organ, which actually has the ability to reorganize itself, learn, and create new neural pathways. This process is called “neuroplasticity.” According to MedicineNet.com, neuroplasticity is defined as, “The brain’s ability to reorganize itself by forming new neural connections throughout life. Neuroplasticity allows the neurons (nerve cells) in the brain to compensate for injury and disease and to adjust their activities in response to new situations or to changes in their environment.”\nThis means that instead of your brain is a rigid organizational thinking machine, it’s actually an ever-evolving system. Therefore, it is possible to rewire your brain in a positive way.\nIf you suffer from depression, you can think of your neural pathways as being a cul de sac, wired, and communicating in one specific direction, keeping you stuck in this pathological thought pattern. The phenomena of neuroplasticity imply that your brain can find other pathways to utilize and break this pathological pattern. The implications of this are huge for those suffering from mental health disorders as it means that the brain can be rewired to overcome brain damage, negative thought patterns, and essentially evolve itself to once again become healthy.\nHow psychedelics can help aid neuroplasticity\nThere are a number of ways you can naturally boost neuroplasticity, such as exercise, new environments, novel stimulation, and meditation. Yet, psychedelics can enable a fast and significant increase in neuroplasticity, that in many ways could be considered “super-natural”.\nThe word “psychedelics” may evoke images of the ‘60s, including hippies dressed in neon colors at the Woodstock festival. However, when used in carefully controlled settings and with the proper preparation, psychedelics such as psilocybin, LSD, MDMA, and even ketamine have shown to be effective treatments for treating a range of serious mental health problems, such as depression, anxiety, and addiction, faster and more effective than the standard treatments available nowadays.\nOne of the main culprits of depression — and other mood disorders — is the degeneration of neurons in the prefrontal cortex. The prefrontal cortex affects decision-making, personality, and cognitive behavior. Psychedelics such as ketamine work to boost the plasticity in your prefrontal cortex. In other words, these powerful psychedelics have the ability to create new connections in the brain and help grow new cells (also known as neurogenesis). Like in the example above, psychedelics can help your brain evolve by allowing you to form new connections in your brain, effectively re-wiring your neural grooves and breaking you free from established unhealthy mental frameworks.\nWhat’s more is that psychedelics are known to be fast-acting, which can be crucial for someone dealing with a potentially life-threatening mental health condition. In contrast, many antidepressants can take six to eight weeks for their full effects to set in, meaning psychedelics can be a significantly better option in many situations. Having such potency and time-effectiveness makes psychedelics a unique and highly promising option for treating mental health disorders.\nThe main difference between psychedelics and traditional pharmacological treatments is that psychedelics are not exclusively trying to compensate for a chemical imbalance. Instead, it is helping the brain to regenerate, heal, and evolve and when used properly, effectively target the root cause of the disorder rather than the issue.\nThe potential for the future\nToday, we know that psychedelics allow the brain to create new connections faster than traditional therapies, and effectively heal the brain from unhealthy states, especially when used in conjunction with other forms of treatment such as therapy. However, powerful research continues to be conducted on these medicines by leading academic institutions such as Imperial College London, Johns Hopkins University, and Yale University. Therefore we can expect to continue seeing new discoveries about how psychedelics work to affect us, heal the mind, and enable humanity’s evolution.\nAs exciting as this information can be, it’s important to remember that this research has been conducted under carefully controlled conditions and by leading scientists and experts. Every situation is unique and any treatment should be taken only under the care and supervision of a medical professional who can help determine what is right for you.","60-Day Drug Rehab Programs\n60-day drug programs offer a variety of intensive medical and behavioral therapies designed to help people recover from substance abuse issues. There are many 28-to-30-day rehab programs, as well as many 90-to-180-day rehab programs available for substance abuse treatment. For those who are looking for a duration of treatment that is somewhere between these two ranges, 60-day drug rehab programs offer a solution by providing a compromise between a shorter and longer stay in substance abuse treatment.\nWhat is a 60-Day Drug Rehab Program?\nA 60-day drug rehab program will require you to stay overnight in a treatment center for around 60 days, or two months. Meals are typically provided by the treatment center, and friends and loved ones may visit during visiting hours. It is possible that a stay longer than 60 days is needed.\nSubstance abuse treatment can mirror the complexities of humans themselves – and that’s on purpose.1 Treatment options for substance abuse must be as intricate to promote long-term recovery and help someone sustain their sobriety. The treatment methods used during a 60-day drug rehab program stay are not the same in all treatment centers.1 Treatment methods are typically selected based on a person’s specific circumstances and needs, but typically include a combination of medication, individual therapy, family therapy, and group therapy.1 60-day drug rehab programs may also include 12-step programs as another treatment method option. There are differences across 60-day drug rehab programs, but all offer intensive treatment to assist people with substance abuse challenges.\nWays to Get in Contact With Us\nIf you believe you or someone you love may be struggling with addiction, let us hear your story and help you determine a path to treatment.\nThere are a variety of confidential, free, and no obligation ways to get in contact with us to learn more about treatment.\n- Call us at\n- Verify Your Insurance Coverage for Treatment\nWhat Happens During a 60-Day Drug Treatment Program?\nDuring a 60-day drug treatment stay, you will complete an intake assessment, which will collect demographic information, drug use history, and other information.1 You may take part in one or a combination of the following during a 60-day drug rehab stay:1\n- Detoxification – A team consisting of medical and behavioral health staff continually monitors you during the detoxification process. Medications are provided to lessen unpleasant withdrawals and to help with any medical needs that arise.\n- Medication-assisted treatment (MAT) – MAT includes the use of medications to help reduce unpleasant withdrawal symptoms and cravings beyond the detoxification period. Such medications include Methadone, Buprenorphine, Naltrexone, Topamax, Acamprosate, and Disulfiram. The medication prescribed depends on many factors, including the substance abused, how long it was used, and how much was used.\n- Individual therapy – Individual therapy may include cognitive-behavioral therapy (CBT) to uncover and change unhealthy thinking patterns, as well as contingency management therapy, which provides rewards for positive change.\n- Group therapy – Therapy provided in a group setting where the group is the healing platform instead of the person and therapist.\n- Family therapy – Therapy designed to uncover issues within the family related to substance abuse. The goal is to promote healing within the family, which will allow the family to be a support network for the person in treatment.\n- After–care planning – After-care planning typically begins on the first day of treatment. This process involves the patient, staff, and possibly family members discussing the best way to plan ahead for continued sobriety. After-care planning may involve referrals to community resources such as physicians, community behavioral health centers, case management, and other sources.\nHow Much Does a 60-Day Rehab Program Cost?\nThe cost of drug rehab can vary depending on several factors, including the state it’s located in and the amenities offered. In the U.S., private and state-funded insurance plans are required to provide behavioral health coverage.3 Most 60-day drug rehab programs in the U.S. accept both private and state-funded insurance. Medicaid and free rehab options are also available. This means that you may only have to pay a small amount for treatment. However, even with insurance, your treatment may come with a cost, which you should take into account when choosing a program.\nWhile the cost for 60-day drug rehab treatment may be an unexpected challenge, the cost of not receiving treatment may be greater. Substance abuse costs the U.S. over $600 billion annually and is a major public health crisis. Substance abuse treatment has shown to reduce these costs significantly.1 Ultimately, the cost of treatment is much less expensive than the costs of not receiving treatment.1 On an individual level, as substance use costs money, can lead to costly legal trouble, and can impact employment opportunities, treatment may be seen as an investment in your future.\nUse the tool below to see if your insurance may be in-network or accepted.\nHow Do I Find 60-Day Rehab Programs Near Me?\nAAC offers a variety of drug rehab programs. Since each person has different needs, AAC provides individualized drug rehab programs that vary in length of stay and treatment interventions used. AAC’s various nationwide treatment centers offer the following:\n- Outpatient treatment.\n- Residential treatment centers (varying stays from days, weeks, 30-days, 60-days 90-days, 180-days, etc. depending on a person’s needs).\n- Medical detox.\n- Partial hospitalization programming.\n- Individual and group therapy.\nRegardless of the treatment selected, most AAC programs include after-care planning and support to help people readjust after treatment. After a 60-day stay, AAC also offers the option for people to continue their treatment on an outpatient basis. Outpatient services are flexible to accommodate work, family, and life schedules. These services may help people transition back to home life after treatment.\nHow Do I Know I Need 60-Day Rehab?\nSubstance-related disorders are classified in the American Psychiatric Association’s Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition. This means that only a physician can determine if you have a substance-related disorder and if a 60-day drug rehab program is right for you. However, some signs may help you to determine if rehab treatment is needed for you or your loved one. Some of those signs are:4\n- Being preoccupied with thinking about using drugs/alcohol frequently.\n- Cravings to use drugs/alcohol more frequently.\n- Needing to use more drugs/alcohol to get high.\n- Feeling sick or being irritable and moody when you do not use drugs/alcohol.\n- Requiring the use of more drugs/alcohol to feel the effects.\n- Spending a lot of time on getting more drugs/alcohol.\n- Financial issues due to drug/alcohol use.\n- Family issues due to drug/alcohol use.\n- Loss of job due to drug/alcohol use.\n- Poor grades in school because of missing classes or skipping assignments due to alcohol/drug use.\nDeciding to enter into a 60-day drug rehab program is a personal decision. If you have a family physician, they can help you make a decision. You may also call American Addiction Centers to speak with a treatment specialist today.\nStill Unsure? Take Our Substance Abuse Self-Assessment\nIf you’re still unsure of whether you may have a substance abuse problem, take our free, 5-minute substance abuse self-assessment below. The evaluation consists of 11 yes or no questions that are intended to be used as an informational tool to assess the severity and probability of a substance use disorder. The test is free, confidential, and no personal information is needed to receive the result.\nHow Do I Choose the Best 60-Day Rehab Program?\nChoosing the best 60-day drug rehab program typically begins with a full assessment by a physician. A physician can help you determine if a 60-day drug rehab program is appropriate and which 60-day drug rehab program will be best. You may also want to consider where you want to go to receive services. For example, some people may choose to stay close to home while others may want to go somewhere far from home. Another consideration is if you have a co-occurring disorder such as depression, anxiety, or post-traumatic stress disorder. If you do, you will want to choose a program that can help you with co-occurring mental health and substance use disorders.\nRegardless of where you choose to receive treatment, the center should follow evidence-based treatment principles to ensure high-quality care. These principles include:1\n- An understanding that substance abuse is a complex process that has long-term effects on both behavior and brain functioning.\n- People are unique. Treatment for substance abuse should be just as individualized.\n- Substance abuse treatment should focus not only on the use of substances but all of the person’s needs.\n- Individual, family, and group therapy should be part of substance abuse treatment.\n- Substance abuse treatment should offer medication in addition to therapy.\n- A person’s treatment plan and service plan should be frequently reviewed and modified to reflect the person’s progress and current needs.\n- Medical detoxification is the first step in substance abuse treatment. Additional treatment should follow to promote long-term recovery.\n- Substance abuse treatment should include testing for HIV/AIDS, hepatitis B and C, tuberculosis, and other infectious diseases. Testing should include counseling to lower risks for contracting infectious disease and referrals to resources for the treatment of infectious disease as needed.\nSuccess Rates & Statistics for 60-Day Rehabs\nThe success of a 60-day drug rehab program varies from person-to-person but may be dependent on several factors. Some of these factors include:\n- A person’s willingness to take part in the treatment and to change.\n- Quality of the treatment methods used and the overall quality of care provided at the chosen treatment center.\n- The strength of the person’s support network.\n- The presence of mental health or physical health issues.\n- Adherence to their after-care plan recommendations.\n- Length of time the person has been abstinent.\nThe success rates of substance abuse treatment have been widely researched. Some of the results of these studies are: 5,6\n- Opioid abuse was reduced by roughly 90% after MAT in an inpatient treatment center.\n- About 1/3 of people completing an inpatient treatment program for alcohol use remained abstinent at the one-year mark.\n- There were large decreases in cocaine use 1 year after treatment.\n- National Institute of Health. (2018). Principles of drug addiction treatment: A research-based guide (Third Edition).\n- Petry, N. (2011). Contingency management: what it is and why psychiatrists should want to use it. The Psychiatrist, 35(5), 1 – 3.\n- S. Department of Health & Human Services. (2020). Health insurance and mental health services.\n- American Psychiatric Association. (2005). Diagnostic and Statistical Manual of Mental Disorders (Fifth Edition). Substance-related disorders (pp. 483 – 585). American Psychiatric Association Press, Washington; DC.\n- Miller, W., Walters, S., & Bennett, M. (2001). How effective is alcoholism treatment in the United States? Journal of the Study of Alcoholism, 62(2), 1 – 9.\n- Simpson, D., Joe, G., Broome, K. (2002). A national 5-year follow-up of treatment outcomes for cocaine dependence. Archives of General Psychiatry, 59(6), 1- 6."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:8babe3a8-ec18-4fc3-9653-2357e65fde56>","<urn:uuid:88edda50-4c36-4982-a231-386b94710f9b>"],"error":null}
{"question":"How do acoustic wood slat panels and acoustical ceiling tiles compare in terms of their sound absorption capabilities?","answer":"Acoustic wood slat panels have an NRC rating of 0.85, meaning they can absorb up to 85% of the sounds that hit them. Similarly, high-end acoustical ceiling panels can also absorb 85% of sound (over 90% in the common vocal range). However, more economical ceiling tile options are also available that absorb 50% of sound (over 70% in the common vocal range). Both solutions help reduce noise and improve acoustics, though ceiling tiles alone aren't sufficient to solve acoustic issues completely, as sound moves equally in all directions and can be reflected, absorbed, or pass through all surfaces in a room, not just the ceiling.","context":["Acoustic Wood Slat Panels\nAcoustic wood slat panels are a popular addition to modern office spaces. They add a touch of style and warmth while helping to reduce noise pollution and enhance speech intelligibility.\nDesigned for DIY installation, these decorative wooden wall panels feature stunning charred oak veneer and black felt backing. This gorgeous combination effortlessly merges style and sustainability, blending in well with a wide range of decor styles.\nReduces Noise Pollution\nNoise pollution can affect our mental health, causing stress, fatigue, and depression. Thankfully, there are ways to reduce the problem. One easy way is by installing acoustic wood slat panels. These panels eliminate the echo effect in your work area and reduce noise pollution. They also improve acoustics and make it easier for employees to communicate with each other.\nThese acoustic wood wall panels feature a fluted design and a sustainable felt backing that help them absorb unwanted noise and echoes. This makes them perfect for offices, conference rooms, and homes. The wood slats create a natural aesthetic that works with almost any interior design style. The panels are lightweight and easy to install. You can even use them to cover a curved surface.\nWhile many sound absorption methods require expensive and invasive amendments to walls, these acoustic wood slat wall panels are both cost-effective and simple to install. They’re the ideal option for those who want to achieve a modern, wooden look while improving room acoustics.\nThese acoustic wood wall panels are crafted from a combination of real wood veneer and MDF with an acoustical felt backing. They have an NRC rating of 0.85, which means that they can absorb up to 85% of the sounds that hit them. This makes them an excellent choice for office environments where clear communication is vital.\nWood slat acoustic panels add texture, depth, and warmth to your interior design. They come in different colors and finishes to suit various aesthetic preferences. These acoustic panels also insulate your home and help you save on energy costs. They’re easy to install and can be shaped to fit soft angles.\nThese acoustic panels are made of wood and natural fibers, so they are environmentally friendly. They also absorb and diffuse sound waves, reducing reverberation and improving speech clarity. They’re ideal for commercial applications, acoustic wood slat panels such as offices and conference rooms where clear communication is essential.\nAcoustic wood wall panels can be easily crafted to match your existing interior design. They’re a stylish alternative to traditional drywall and provide a modern aesthetic that’s easy to clean. They’re also incredibly durable and won’t require replacement for years.\nPosh Wood Charred Oak acoustic wood paneling is a stunning, eco-friendly solution for your interior design. Its charred oak finish paired with an elegant black felt background effortlessly merges style and sustainability. Its refined aesthetic complements a wide range of interior design styles, from rustic to contemporary.\nAcoustic wood slat panels help to create a more comfortable and productive work environment by reducing noise pollution. They also enhance aesthetics and improve acoustics by absorbing unwanted sound, reducing echoes, and enhancing speech intelligibility. This makes them ideal for modern offices, which rely on effective communication and collaboration to achieve business goals.\nIncorporating acoustic paneling into your office space is an easy way to increase productivity. However, choosing the right type of acoustic panels is important. Choosing ones that are durable and can withstand a lot of wear and tear is essential. Additionally, they must be easy to clean in order to avoid dust buildup, which can cause them to become damaged over time.\nThe Slatpanel range of acoustic panels is designed for hassle-free installation. This allows you to quickly and easily remodel your workspace with minimal disruptions to your day-to-day operations. They feature a high-quality wood veneer with a sleek and contemporary-looking lamella strip design, which is then mounted onto a specially-designed recycled acoustic felt material.\nThe natural acoustics of slatted wall panels make acoustic wood slat panels them suitable for many interior design styles. The clean lines of acoustic wood panels are perfect for modern walls, but they can also enhance traditional and transitional interior designs as well. Moreover, the textured surface of acoustic wood panels is compatible with mid-century modern and bohemian interior design styles.\nAcoustic wood slat panels are a great way to add extra sound absorption to a room. This helps to reduce the amount of noise pollution in a space, which can affect the health and productivity of those who are using it. Acoustic panels also help to make the space feel more comfortable and welcoming. This is because they help to create a more natural atmosphere. Additionally, acoustic wood slat panels are easy to install. They can be attached to the wall using black screws, which makes them a simple solution for anyone who wants to increase the sound absorption in their home or office.\nWhile traditional foam panels may be an option, they are not as effective as acoustic wood slat wall paneling when it comes to sound absorption. This is because foam is porous, which means it has many small holes in it that allow sound waves to pass through them. Acoustic wood slat panels, on the other hand, are designed with a dense felt backing, which is highly absorbent.\nUnlike the egg carton style panels that you might see in recording studios, acoustic timber slat panels look great in any room. They can be used to create walls, headboards, or even a room divider. They are also easy to install and can be customised, making them a great option for anyone who is looking for an affordable solution that looks good in any space.","Acoustical Ceiling Tiles and Panels\nIn a Few Words...\nGot a space with acoustic issues? Ceiling tiles and panels can make a difference, but we've got some bad news for you - by themselves they aren't going to solve the problem, they'll just help. And there is no Easter Bunny.. sorry about that.\nWhat to look for (now that you aren't searching for that bunny):\n- Does your room echo, echo, echo..? Noise Reduction Coefficient (NRC) is your friend, and it is the most common acoustic measurement for ceiling panels.\n- Annoying noises pounding down from the room above (or below!)? Sound Transmission Class (STC) can help reduce them.\n- Too much information coming through your ceiling from the office next door? Ceiling Attenuation Class (CAC) can help cut down on the TMI.\nIn a Few More Words...\nBuying a new ceiling tile strictly for its ability to control sound is a little like buying a new car strictly for its CD player. Some people might actually make the choice based on that single feature, but cars were no more invented as transport devices for surround sound systems than ceiling tiles were invented for their acoustics. Can ceiling tiles enhance the acoustical characteristics of a room? Absolutely, just like a Blaupunkt can make a sweet ride even sweeter. But when shopping for a car or a ceiling tile, it is probably best to evaluate the whole package and what you want from it not just how good it \"sounds\".\nBefore we get into the specific acoustic values of ceiling tiles and how they are measured and represented, here are a few things to consider:\n- The term \"acoustical ceiling tile\" has as much to do with marketing as it does with making a difference in the \"sound environment\" within a given room. Conventional drop ceiling tiles came into wide usage because they were easy to install, provided convenient access to the areas above the ceiling, and were cheap. People certainly didn't buy them because they were pretty and they didn't buy them for their acoustics. Yet, acoustically, mineral fiber tiles were significantly better than conventional drywall or the gypsum that was widely used for drop ceiling panels. Marketers seized on this as a way to differentiate these tiles as \"new and improved\" and began to sell them as \"acoustical ceiling tiles\". That name added perceived value to the product, and soon \"acoustical\" is what people thought of first when buying tiles for their ceiling.\nA conventional room has, at a minimum, six structural surfaces: four walls, a floor, and a ceiling. Unlike hot air, which rises, sound moves equally in all directions and has the potential to be reflected, to pass through, or to be absorbed by all surfaces - not just the ceiling.\nSo, why is it that we mostly think of the ceiling when we think of acoustics? Marketing. A carpeted floor can do much more to absorb sound than most ceiling tiles, yet we don't think of buying acoustical carpeting. Wall hangings and drapes can have a significant and easily noticeable effect on the sound signature of a room, yet they are rarely purchased for their acoustic characteristics. And what about furniture? One doesn't usually shop for an \"acoustical chair\", yet furnishings have a huge impact on the behavior of sound within a room.\nShould you consider the acoustical properties of ceiling tiles before making a selection? Certainly. But it is also important to make sure that your expectations are aligned with reality. Given all the factors that contribute to the total sound signature of a room, will you be able to actually hear the difference between a tile that absorbs 50% of the sound that hits it versus one that absorbs 25%? For most people in most situations, probably not.\nThink of it this way: Most drivers going 60 MPH down the freeway probably aren't even conscious of the difference between a $300 car stereo and one costing $3,000. Instead, they are appreciating the entire driving experience: the look, the feel, the handling, the convenience, the maintenance, the cost, the miles per gallon of the vehicle - the whole package. Approach your selection of ceiling tiles in a similar fashion and you can't go wrong.\nThere are three major ratings that relate to ceiling tile acoustics: Noise Reduction Coefficient, Sound Trasmittance Class, and Ceiling Attenuation Class. Only some tiles are rated in all three categories, and the value of each rating is only meaningful in the context of the actual installation where the tile will be used. Before describing each rating, it is important to note that these values are achieved in a laboratory, and the measurements are not considered field data. Said another way: \"Your results may vary,\" depending on where and how the tiles are used and what you expect of them.\nNoise Reduction Coefficient (NRC) is a general rating that tells you how much sound a surface (in this case a ceiling tile) will absorb. This rating is represented as either a percentage or a decimal, with 1 (or 100%) being perfect absorption and 0 (or 0%) being perfect reflection. In other words, the higher the decimal or percent, the more sound the tile will absorb. While this does give a very good idea of how a tile will perform acoustically, it does not tell you what happens to the sound the ceiling tile does not absorb.\nSound Transmission Class (STC) is used to rate several different types of building materials for their ability to block sound and prevent it from passing through them. This is commonly used on walls and partitions, as it is very helpful in deciding which types of partitions will properly isolate different areas of a building. STC ratings are represented as a number, usually between 1 and 100, although not limited to that range. The higher the number, the better the material is at blocking sound. A rating of 10 is considered terrible, allowing talking voices to be heard through the rated system, while a rating of 60 or more makes for an excellent sound blocker.\nThe STC rating is helpful, but does not tell you what will happen to sound once it is blocked. For example, a thick concrete wall might have an excellent STC rating, but it will send all sound bouncing right back into a room. This will keep the sound from passing through the wall, but will make for a very echo filled room.\nCeiling Attenuation Class (CAC) is really a completely separate rating system. While it does deal with sound absorption and attenuation, it is a bit more specific.\nIn an installation where a drop ceiling has been installed and walls have been installed that only reach up to the drop ceiling level, sound can pass up into the ceiling plenum, over the wall, and then back down into another room. The rate at which ceiling tiles inhibit this passage of sound is called the CAC. This rating is commonly misunderstood as a rating that describes how much sound reflects off of the face of a tile. This is a misconception, and the CAC rating should not be given as a rating to describe any situation other than the aforementioned.\nNRC, STC, CAC and YOU\nThere is no hard and fast rule about combining these acoustical values, but it is good to be aware of them while shopping. Unless you have a very specific need, they shouldn't be the sole deciding factor in your purchase.\nSo, how important are these ratings? If you are evaluating tiles in isolation, comparing one type to another based only on their acoustic performance, these numbers are all you've got to go on. Fortunately that isn't the case. You are also going to be considering appearance, ease of installation, price, selection, indoor air quality, availability, and a host of other things. Add these acoustical ratings to that mix, think about which overall qualities are most important in your specific installation, and in no time at all you'll know exactly which tile sounds right for you.\nCeilume's Acoustical Ceiling Panels - Good, Better, Best!\nWhen shopping specifically for acoustical panels, most people want to reduce echo or \"noise\" within a room, so they are most interested in NRC. And no wonder, a standard sheetrock ceiling reflects back 95% of the sound that hits it! The term \"balance\" is often used relative to sound, so Ceilume offers three choices that allow you to balance acoustic performance with aesthetics and price to create the ceiling that suits you best.\nBest price-to-performance ratio that is unmatched by any other decorative acoustical ceiling.\nA high end acoustic ceiling panel system that absorbs 85% of the sound that hits it (over 90% in the common vocal range).\n- 85% NRC\n- 11 beautiful colors/finishes\n- R-Value of 6\n- Starting at $6.95 per sq. ft.\nBetter sound absorption and designer style at a lower price per square foot for those on a budget.\nA cost effective acoustic tile system that absorbs 50% of the sound that hits it (over 70% in the common vocal range).\n- 50% NRC\n- 11 beautiful colors/finishes\n- R-Value of 6\n- Starting at $3.25 per sq. ft."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0937a197-13df-4545-b435-1e1dfd16774e>","<urn:uuid:5589b76e-d7aa-4384-82d5-ed3ad1551af8>"],"error":null}
{"question":"What's the key difference between regular bonds and zero coupon bonds in terms of how investors earn returns?","answer":"Regular bonds and zero coupon bonds differ in how they generate returns for investors. Regular bonds pay periodic interest through coupon payments, calculated as a percentage (coupon rate) of the bond's face value. For example, a $1,000 bond with a 6% coupon rate would pay $60 annually. In contrast, zero coupon bonds don't make any periodic interest payments. Instead, investors earn returns by buying the bond at a discount to its face value. The investor then receives the full face value at maturity - the difference between the purchase price and face value represents their return. For example, a 3-year zero coupon bond with $1,000 face value might be purchased for $816 (at a 7% discount rate) and pay $1,000 at maturity.","context":["How To Find A Bonds Coupon Rate\nListing Websites about How To Find A Bonds Coupon Rate\nCoupon Rate of a Bond (Formula, Definition) Calculate\n(8 days ago) The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond. Mathematically, it is represented as,\nWhat Is Coupon Rate and How Do You Calculate It\n(9 days ago) How Bond Coupon Rate Is Calculated Coupon rate is calculated by adding up the total amount of annual payments made by a bond, then dividing that by the face value (or “par value”) of the bond. For example: ABC Corporation releases a bond worth $1,000 at issue. Every six months it pays the holder $50.\nCoupon Rate Formula Step by Step Calculation (with Examples)\n(1 days ago)\nHow can I calculate a bond's coupon rate in Excel\n(5 days ago) In cell B2, enter the formula \"=A3/B1\" to yield the annual coupon rate of your bond in decimal form. Finally, select cell B2 and hit CTRL+SHIFT+% to apply …\nHow to Calculate a Coupon Payment: 7 Steps (with Pictures)\n(Just Now) If you know the face value of the bond and its coupon rate, you can calculate the annual coupon payment by multiplying the coupon rate times the bond's face value. For example, if the coupon rate is 8% and the bond's face value is $1,000, then the annual coupon payment is.08 * …\nCoupon Rate Calculator\n(5 days ago) Coupon Rate = (Coupon Payment x No of Payment) / Face Value Note: n = 1 (If Coupon amount paid Annual) n = 2 (If Coupon amount paid Semi-Annual) Coupon percentage rate is also called as the nominal yield. In other words, it is the yield the bond paid on its issue date.\nBond Price Calculator\n(7 days ago) Coupon rate is the annual rate of return the bond generates expressed as a percentage from the bond’s par value. Coupon rate compounding frequency that can be Annually, Semi-annually, Quarterly si Monthly. Market interest rate represents the return rate similar bonds sold on the market can generate.\nHow to Calculate Bond Value: 6 Steps (with Pictures)\n(4 days ago) Determine discount rate. Divide the discount rate required by the number of periods per year to arrive at the required rate of return per period, k. For example, if you require a 5% annual rate of return for a bond …\nHow to Find the Interest Rate on a Bond Budgeting Money\n(8 days ago) Multiply the coupon rate by the face value if the coupon rate is listed as a percentage. For example, a bond with a face value of $5,000 and a coupon rate of 6 percent pays a coupon rate of $300 per year. Sometimes the coupon rate is stated in dollars. If so, you can skip this step.\nCoupon Rate Calculator\n(Just Now) How to calculate coupon rate? First, determine the face value of the bond. Calculate or determine the market price of the face value of the bond. Next, determine the annual coupon payment received.\nHow to Calculate Coupon Rates Sapling\n(9 days ago) Most bonds pay the same coupon on a set schedule until the bond matures, which is when the issuer pays back the bond's face value and any remaining interest. You calculate a coupon rate by dividing the annual coupon payments by the bond's face value.\n(2 days ago) A bond's coupon rate can be calculated by dividing the sum of the security's annual coupon payments and dividing them by the bond's par value. For example, a bond issued with a face value of $1,000\nHow can I calculate a bond's coupon rate in Excel\n(1 days ago) A bond’s coupon rate is just the rate of curiosity it pays annually, expressed as a proportion of the bond’s par worth. (It’s known as the coupon rate as a result of, in days of yore, traders truly had possession of bodily paper bonds, which had literal coupons connected to them.\nBond Present Value Calculator\n(8 days ago) Annual Coupon Rate is the yield of the bond as of its issue date. Annual Market Rate is the current market rate. It is also referred to as discount rate or yield to maturity. If the market rate is greater than the coupon rate, the present value is less than the face value.\n(9 days ago) A coupon is stated as a nominal percentage of the par value (principal amount) of the bond. Each coupon is redeemable per period for that percentage. For example, a 10% coupon on a $1000 par bond is redeemable each period. A bond may also come with no coupon.\nZero Coupon Bond Calculator – What is the Market Price\n(6 days ago) Zero Coupon Bond Calculator Inputs Bond Face Value/Par Value ($) - The face or par value of the bond – essentially, the value of the bond on its maturity date. Annual Interest Rate (%) - The interest rate paid on the zero coupon bond. Years to Maturity - The numbers of years until the zero coupon bond's maturity date.\n25% OFF How To Find Bond Coupon Rate Verified\n(6 days ago) (2 days ago) To calculate bond coupon rates, use the formula C = i/P, where \"C\" represents the coupon rate, \"i\" represents the annualized interest rate and \"P\" represents the par value, which is the principal amount (or face value) of the bond. The coupon rate is based on a bond's face value, not current yield.\nBond Yield to Maturity (YTM) Calculator\n(2 days ago) On this page is a bond yield to maturity calculator, to automatically calculate the internal rate of return (IRR) earned on a certain bond.This calculator automatically assumes an investor holds to maturity, reinvests coupons, and all payments and coupons will be paid on time.\n(8 days ago) The Bond Calculator can be used to calculate Bond Price and to determine the Yield-to-Maturity and Yield-to-Call on Bonds Bond Price Field - The Price of the bond is calculated or entered in this field. Enter amount in negative value. Face Value Field - The Face Value or Principal of the bond is calculated or entered in this field.\nCalculate Coupon Rate Bond\n(1 days ago) Coupon Rate On A Bond Calculator. CODES (29 days ago) Coupon Rate Formula | Calculator (Excel Template) CODES (2 days ago) Below are the steps to calculate the Coupon Rate of a bond: Step 1: In the first step, the amount required to be raised through bonds is decided by the company, then based on the target investors (i.e. retail or institutional or both) and other parameters face value or par\nSolving for the coupon rate of a coupon bond\nHow to Calculate Reinvested Bond Interest Finance\n(Just Now) Subtract 1 and divide by the YTM rate. Multiply the result by the coupon payment amount and subtract the total amount of payments. As an example, if a bond offers a 10 percent YTM rate with 20\nVariable Coupon Renewable Note (VCR)\n(1 days ago) Coupons are expressed in the form of a percentage of the face value and are paid for the duration of the bond’s validity, which is from the issue date until the date of maturity. The sum of all coupons paid to the investor over a year, when divided by …\nBond Valuation: Formula, Steps & Examples\n(5 days ago) Bond Terms. Horse Rocket Software has issued a five-year bond with a face value of $1,000 and a 10% coupon rate. Interest is paid annually. Similar bonds in the market have a discount rate …\nHow Interest Rates Changes impact Bond Prices\n(6 days ago) A treasury bond is available at ₹1,000 and offers a 6% coupon rate for the next 10 years. You go ahead and purchase it. There are two things you need to understand here. 1. A bond’s coupon rate is the interest you will receive on an annual basis for holding the bond. This coupon rate is irrespective of whatever is the price of the bond.\n20% OFF Bond Coupon Rate Calculator Verified\n(5 days ago) BAII Plus Bond Valuation | TVMCalcs.com. COUPON (6 days ago) Draw a time line for a 3-year bond with a coupon rate of 8% per year paid semiannually. The bond has a face value of $1,000. The bond has three years until maturity and it pays interest semiannually, so the …\nCalculate Coupon Rate Of Bond\n(4 days ago) Coupon Rate of a Bond (Formula, Definition) Calculate . CODES (8 days ago) Formula The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond.\nCalculate Price of Bond using Spot Rates CFA Level 1\n(2 days ago) Spot rates are yields-to-maturity on zero-coupon bonds maturing at the date of each cash flow. Sometimes, these are also called “zero rates” and bond price or value is referred to as the “no-arbitrage value.” Calculating the Price of a Bond using Spot Rates. Suppose that: The 1-year spot rate is 3%; The 2-year spot rate is 4%; and\nHow to Price a Bond Using Spot Rates (Zero Curve\n(3 days ago) A better way to price the bonds is to discount each cash flow with the spot rate (zero coupon rate) for its respective maturity. Example 1. Let’s take an example. Suppose we want to calculate the value of a $1000 par, 5% coupon, 5 year maturity bond. We also have the following spot rates for the next 5 years:\nHow to Convert Bond Price to Yield Finance\n(Just Now) Step 1. Find out a bond’s price, coupon rate and par value from your broker, in a financial newspaper or in the Market Data section of the Financial Industry Regulatory Authority’s website.\nHow to Calculate the Break-Even Interest Rate on Bonds\n(3 days ago) To calculate the break-even interest rate, take (1 + 0.02) ^ 5 for the five-year bond, and (1 + 0.03) ^ 10 for the 10-year bond. The resulting numbers are 1.10408 and 1.34392, respectively.\n85% OFF what is bond coupon rate Verified CouponsDoom.com\n(8 days ago) How to Calculate Bond Discount Rate: 14 Steps (with Pictures) COUPON (3 days ago) A bond discount is the difference between the face value of a bond and the price for which it sells. The face value, or par value, of a bond is the principal due when the bond matures.\nHow to Calculate Spot Rate From Government Bonds Pocketsense\n(3 days ago) How to Calculate Spot Rate From Government Bonds. Calculating the implied spot rate on a coupon paying government-issued bond is not a complicated calculation if you have all of the necessary information. The spot rate refers to the theoretical yield on a zero-coupon Treasury security. Coupon paying government bonds\nHow to calculate bond yields\n(1 days ago) There are three main yields applicable to dated bonds: Coupon rate. This is the interest rate the bond initially pays on issue. It’s invariably given in the name of the bond. For instance Treasury 5% would have a coupon of 5%. i.e. If you bought it when it was first issued — before the price began to fluctuate in the market — you’d get\nHow to Value a Bond Using Forward Rates\n(5 days ago) We also saw that forward rates can be derived from spot rates. If so, we can also value a bond using forward rates instead of spot rates. Let’s take a specific cash flow in a bond to understand this. Say, a bond is going to pay $100 as coupon after 2 years. s 2 is the 2-year spot rate is 6%. The present value of this cash flow will be:\nHow to Calculate a Coupon Payment Sapling\n(6 days ago) In finance, a coupon payment represents the interest that's paid on a fixed-income security such as a bond. Par value is the face value of a bond. Calculate the annual coupon rate by figuring the annual coupon payment, dividing this amount by …\nZero Coupon Bond Value Calculator: Calculate Price, Yield\n(1 days ago)\nCoupon Rate Of Bond Formula\n(2 days ago) Coupon Rate of a Bond (Formula, Definition) Calculate . CODES (8 days ago) Formula The coupon rate of a bond can be calculated by dividing the sum of the annual coupon payments by the par value of the bond and multiplied by 100%. Therefore, the rate of a bond can also be seen as the amount of interest paid per year as a percentage of the face value or par value of the bond.\nHow to calculate yield to maturity in Excel (with Template\nHow to Calculate the Price of a Bond With Semiannual\n(7 days ago) Therefore, the example's required rate of return would be 2.5 percent per semiannual period. To convert this to a coupon payment, or the amount of money you'd actually receive each period, multiply the face amount of the bond by the required rate of return.\nWhat is yield and how does it differ from coupon rate\n(3 days ago) There are two ways of looking at bond yields - current yield and yield to maturity. Current Yield. This is is the annual return earned on the price paid for a bond. It is calculated by dividing the bond's coupon rate by its purchase price. For example, let’s say a bond has a coupon rate of 6% on a …\nHow to calculate bond price in Excel\n(1 days ago) For example there is 10-years bond, its face value is $1000, and the interest rate is 5.00%. Before the maturity date, the bondholder cannot get any coupon as below screenshot shown. You can calculate the price of this zero coupon bond as follows: Select the cell you will place the calculated result at, type the formula =PV(B4,B3,0,B2)\nHow To Calculate YTM With Coupon Rate Our Deer\n(8 days ago) Settlement date is the purchase date of the bond. Coupon rate is expressed as an annual percentage. The bond price and face value should be expressed as a percentage of par. Face value is usually 100, and price will look like 98.472 if you paid $98,472 for a $100,000 bond. Payment frequency for most bonds is twice a year, so the value will be 2.\nCorporate Bond Coupon Rate\n(1 days ago) Find the best Corporate Bond Coupon Rate. Save today with new coupon codes and shop the latest offers available online and in stores\nBAII Plus Bond Valuation TVMCalcs.com\n(4 days ago) Draw a time line for a 3-year bond with a coupon rate of 8% per year paid semiannually. The bond has a face value of $1,000. The bond has three years until maturity and it pays interest semiannually, so the time line needs to show six periods. The bond will pay 8% of the $1,000 face value in interest every year.\nWhat is Coupon Rate\n(9 days ago) The coupon rate is calculated on the bond’s face value (or par value), not on the issue price or market value. For example, if you have a 10-year- Rs 2,000 bond with a coupon rate of 10 per cent, you will get Rs 200 every year for 10 years, no matter what happens to the bond price in the market.\nHow is the coupon rate of a bond calculated?\nThe formula for coupon rate is computed by dividing the sum of the coupon payments paid annually by the par value of the bond and then expressed in terms of percentage. Coupon Rate = Total Annual Coupon Payment / Par Value of Bond * 100%\nWhat is coupon rate and how do you calculate it?\nA bond's coupon rate can be calculated by dividing the sum of the security's annual coupon payments and dividing them by the bond's par value. For example, a bond issued with a face value of $1,000 that pays a $25 coupon semiannually has a coupon rate of 5%.\nHow can I calculate a bond's coupon rate in Excel?\nMost bonds have a clearly stated coupon rate percentage. However, calculating the coupon rate using Microsoft Excel is simple if all you have is the coupon payment amount and the par value of the bond. The formula for the coupon rate is the total annual coupon payment divided by the par value.\nHow to calculate coupon bonds?\nThe formula for coupon bond calculation can be done by using the following steps:\n- Step 1: Firstly, determine the par value of the bond issuance, and it is denoted by P.\n- Step 2: Next, determine the periodic coupon payment based on the coupon rate of the bond based, the frequency of the...\n- Step 3: Next, determine the total number of periods till maturity by multiplying the...","The time value of money formulas can be used to calculate a zero coupon bond price.\nA business will issue zero coupon bonds when it wants to obtain funding from long term investors by way of debt finance. The bond will stipulate the term to be used, known as the maturity date, and the face value, which is the amount the bondholder will receive back at maturity.\nAs the name implies, a zero coupon bond does not have a coupon rate and does not make periodic interest payments. In order for the bondholder to get a return on their investment when buying zero coupon bonds, the bond is issued at a discount to its face value (hence the reason why a zero coupon bond is sometimes referred to as a discount bond or deep discount bond).\nThe zero coupon bond price or value is the present value of all future cash flows expected from the bond. As the bond has no interest payments, the only cash flow is the face value of the bond received at the maturity date.\nZero Coupon Bond Pricing Example\nSuppose for example, the business issued 3 year, zero coupon bonds with a face value of 1,000.\nThe future bond cash flow is presented in the diagram below:\nTo find the current price an investor would pay for the bond this cash flow needs to be discounted back to today, the start of period 1. The discount rate to use will depend on the risk associated with the cash flows from the zero coupon bond.\nSuppose the discount rate was 7%, the face value of the bond of 1,000 is received in 3 years time at the maturity date, and the present value is calculated using the zero coupon bond formula which is the same as the present value of a lump sum formula.\nThe zero coupon bond price is calculated as follows:\nn = 3 i = 7% FV = Face value of the bond = 1,000 Zero coupon bond price = FV / (1 + i)n Zero coupon bond price = 1,000 / (1 + 7%)3 Zero coupon bond price = 816.30 (rounded to 816)\nThe present value of the cash flow from the bond is 816, this is what the investor should be prepared to pay for this bond if the discount rate is 7%. The investor pays 816 today and receives the face value of the bond (1,000) at the maturity date, as shown in the cash flow diagram below.\nZero Coupon Bond Rates\nThe value of a zero coupon bond will change if the market discount rate changes. Suppose in the above example, the market discount rate increases to 10%, then the bond price would be given as follows:\nn = 3 i = 10% FV = Face value of the bond = 1,000 Zero coupon bond price = FV / (1 + i)n Zero coupon bond price = 1,000 / (1 + 10%)3 Zero coupon bond price = 751.31 (rounded to 751)\nAs the face value paid at the maturity date remains the same (1,000), the price investors are willing to pay to buy the zero coupon bonds must fall from 816 to 751, in order from the return to increase from 7% to 10%.\nZero Coupon Bond Price and Term to Maturity\nThe longer the term the zero coupon bond is issued for the lower the bond price will be. Using the example above, if the issue was a 10 year zero coupon bond, then the price at issue would be given as follows:\nn = 10 i = 7% FV = Face value of the bond = 1,000 Zero coupon bond price = FV / (1 + i)n Zero coupon bond price = 1,000 / (1 + 10%)10 Zero coupon bond price = 508.35 (rounded to 508)\nIn this example the bondholder has to wait 10 years before they receive the face value of the bond. Assuming the bond discount rate remains the same (7%), then the price investors are willing to pay must fall from 816 to 508 in order to compensate for the term increasing from 3 to 10 years.\nAbout the Author\nChartered accountant Michael Brown is the founder and CEO of Double Entry Bookkeeping. He has worked as an accountant and consultant for more than 25 years in all types of industries. He has been the CFO or controller of both small and medium sized companies and has run small businesses of his own. He has been a manager and an auditor with Deloitte, a big 4 accountancy firm, and holds a BSc from Loughborough University."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:e24e1a45-8c9d-4b05-bb80-2c73ad3d835c>","<urn:uuid:2bd57ac5-afc6-4402-91fe-b58deaee452d>"],"error":null}
{"question":"What's the main export certification program for organic products in New Zealand versus Canada?","answer":"New Zealand uses the Official Organic Assurance Programme (OOAP) for organic exports, requiring products to meet both standard regulatory requirements and OOAP requirements. Canada operates under the Canadian Organic Regime, overseen by the Canadian Food Inspection Agency (CFIA), which manages organic certification for exports and requires compliance with Canadian Organic Production Systems standards.","context":["Find out about the requirements for producing, selling, or exporting New Zealand-made organic products, or for importing organic products to this country.\nWhat 'organic' means\n'Organic' is a labelling term used on products produced in accordance with organic production standards, which may be certified by a certification body or authority.\nOrganic agriculture is based on minimising the use of external inputs. For example, avoiding or excluding the use of synthetic fertilisers and pesticides, antibiotics, growth promotants, genetic modification, and irradiation. Organic handlers, processors, and retailers follow voluntary standards to maintain the integrity of organically produced products.\nIt's important to note that organic certification standards are not food safety standards. Organic products must meet the same food safety standards that apply to all food for sale in New Zealand. Read about MPI's organic food policy and the background behind it:\nOrganic food is treated in the same way as all other foods by MPI. This means that the same risk management framework for assessing safety and suitability is applied to all food for sale in New Zealand, including organic food, and the products used in its production.\nMPI will, where appropriate, provide authoritative information in relation to organic food so consumers can make informed decisions.\nMPI will continue to develop its role as the government certifier for organic food exports.\nBackground to MPI’s organic food policy\nOrganic agriculture and food production systems avoid or exclude the use of most synthetic pest control compounds and fertilisers, antibiotics, growth promotants, and food additives derived from non-organic sources, as well as genetic modification and irradiation.\nOrganic food must meet the same safety and suitability standards as other foods for sale in New Zealand. These include, as appropriate, the labelling and composition standards of the Food Standards Australia New Zealand (FSANZ) Code – for example, requirements for allergen warnings, date markings, manufacturer/distributor/importer contact details, ingredient listing, and nutrition information.\nMPI monitors chemical residues and microbiological hazards in a range of foods, including organic foods. To date, no specific concerns have been identified relating to organic food products in general.\nFurther information about MPI’s organic food policy\nRequirements for organic products\nThe requirements your organic operation needs to comply with depend on the market for the organic product.\nOrganic products sold in New Zealand need to\n- meet the standard regulatory requirements for the type of product (for example, dairy, honey, and meat)\n- comply with the Fair Trading Act 1986 in respect to using the term 'organic' in labelling and marketing claims.\nExported organic products need to\n- meet the standard New Zealand regulatory requirements for the type of product – for example, dairy, honey, or meat\n- meet Official Organic Assurance Programme (OOAP) requirements if being exported under the OOAP\n- meet market access requirements of the country being exported to, where not covered under the OOAP\nAll imported organic products need to meet the standard New Zealand legal requirements for imported food. There are also additional requirements, depending on the type of organic product being imported and its final use.\nFurther information about organics\nFind more general information relevant to organic products.\nView national and international organics-related websites and organic certification bodies.\nFamiliarise yourself with any service-related costs you may be required to pay to MPI if operating under the Official Organic Assurance Programme (OOAP).\nAccess registers and lists including details of exporters and third-party agencies operating under the OOAP.\nBrowse a list of forms and templates relating to the OOAP.\nFind out more about general food safety requirements you may need to comply with:\nKeep up to date\nIt’s important to keep up with any new or revised organics information, including requirements, consultations, policies, and other content changes.","Introduction to Organic Farming\nOrganic farming is a method of crop and livestock production that involves much more than choosing not to use pesticides, fertilizers, genetically modified organisms, antibiotics and growth hormones.\nOrganic production is a holistic system designed to optimize the productivity and fitness of diverse communities within the agro-ecosystem, including soil organisms, plants, livestock and people. The principal goal of organic production is to develop enterprises that are sustainable and harmonious with the environment.\nThe general principles of organic production, from the Canadian Organic Standards (2006), include the following:\n- protect the environment, minimize soil degradation and erosion, decrease pollution, optimize biological productivity and promote a sound state of health\n- maintain long-term soil fertility by optimizing conditions for biological activity within the soil\n- maintain biological diversity within the system\n- recycle materials and resources to the greatest extent possible within the enterprise\n- provide attentive care that promotes the health and meets the behavioural needs of livestock\n- prepare organic products, emphasizing careful processing, and handling methods in order to maintain the organic integrity and vital qualities of the products at all stages of production\n- rely on renewable resources in locally organized agricultural systems\nOrganic farming promotes the use of crop rotations and cover crops, and encourages balanced host/predator relationships. Organic residues and nutrients produced on the farm are recycled back to the soil. Cover crops and composted manure are used to maintain soil organic matter and fertility. Preventative insect and disease control methods are practiced, including crop rotation, improved genetics and resistant varieties. Integrated pest and weed management, and soil conservation systems are valuable tools on an organic farm. Organically approved pesticides include “natural” or other pest management products included in the Permitted Substances List (PSL) of the organic standards. The Permitted Substances List identifies substances permitted for use as a pesticides in organic agriculture. All grains, forages and protein supplements fed to livestock must be organically grown.\nThe organic standards generally prohibit products of genetic engineering and animal cloning, synthetic pesticides, synthetic fertilizers, sewage sludge, synthetic drugs, synthetic food processing aids and ingredients, and ionizing radiation. Prohibited products and practices must not be used on certified organic farms for at least three years prior to harvest of the certified organic products. Livestock must be raised organically and fed 100 per cent organic feed ingredients.\nOrganic farming presents many challenges. Some crops are more challenging than others to grow organically; however, nearly every commodity can be produced organically.\nThe world market for organic food has grown for over 15 years. Growth of retail sales in North America is predicted to be 10 per cent to 20 per cent per year during the next few years. The retail organic food market in Canada is estimated at over $1.5 billion in 2008 and $22.9 billion in the U.S.A. in 2008. It is estimated that imported products make up over 70 per cent of the organic food consumed in Canada. Canada also exports many organic products, particularly soybeans and grains.\nThe Canadian Organic Farmers reported 669 certified organic farms in Ontario in 2007 with over 100,000 certified organic acres of crops and pasture land. This is an annual increase of approximately 10 per cent per year in recent years. About 48 per cent of the organic cropland is seeded to grains, 40 per cent produces hay and pasture and about five per cent for certified organic fruits and vegetables. Livestock production (meat, dairy and eggs) has also been steadily increasing in recent years.\nThe main reasons farmers state for wanting to farm organically are their concerns for the environment and about working with agricultural chemicals in conventional farming systems. There is also an issue with the amount of energy used in agriculture, since many farm chemicals require energy intensive manufacturing processes that rely heavily on fossil fuels. Organic farmers find their method of farming to be profitable and personally rewarding.\nConsumers purchase organic foods for many different reasons. Many want to buy food products that are free of chemical pesticides or grown without conventional fertilizers. Some simply like to try new and different products. Product taste, concerns for the environment and the desire to avoid foods from genetically engineered organisms are among the many other reasons some consumers prefer to buy organic food products. In 2007 it was estimated that over 60 per cent of consumers bought some organic products. Approximately five per cent of consumers are considered to be core organic consumers who buy up to 50 per cent of all organic food.\n“Certified organic” is a term given to products produced according to organic standards as certified by one of the certifying bodies. There are several certification bodies operating in Ontario. A grower wishing to be certified organic must apply to a certification body requesting an independent inspection of their farm to verify that the farm meets the organic standards. Farmers, processors and traders are each required to maintain the organic integrity of the product and to maintain a document trail for audit purposes. Products from certified organic farms are labelled and promoted as “certified organic.”\nIn June 2009, the Canadian government introduced regulations to regulate organic products. Under these regulations the Canadian Food Inspection Agency (CFIA) oversees organic certification, including accreditation of Conformity Verification Bodies (CVBs) and Certification Bodies (CBs). This regulation also references the Canadian Organic Production Systems General Principles and Management Standards (CAN/CGSB-32.310) and the Organic Production Systems – Permitted Substances List that were revised in 2009.\nThe Canadian organic regulations require certification to these standards for agricultural products represented as organic in import, export and inter-provincial trade, or that bear the federal organic agricultural product legend or logo. (Figure 1) Products that are both produced and sold within a province are regulated by provincial organic regulations where they exist (Quebec, British Columbia and Manitoba).\nFigure 1. Canadian Agriculture Product Legend (logo)\nThe federal regulations apply to most food and drink intended for human consumption and food intended to feed livestock, including agricultural crops used for those purposes. They also apply to the cultivation of plants. The regulations do not apply to organic claims for other products such as aquaculture products, cosmetics, fibres, health care products, fertilizers, pet food, lawn care, etc.\nFood products labelled as organic must contain at least 95 per cent organic ingredients (not including water and salt) and can bear the Canada Organic logo. Multi-ingredient products with 70 per cent to 95 per cent organic product content may be labelled with the declaration: “% organic ingredients”. Multi-ingredient products with less than 70 per cent organic content may identify the organic components in the ingredient list.\nExported products must meet the requirements of the importing country or standards negotiated through international equivalency agreements. Products exported to the U.S. must meet the terms of the Canada-U.S. equivalency agreement signed in June 2009. All products that meet the requirements of the Canada Organic Regime can be exported to the U.S. with the exception that agricultural products derived from animals treated with antibiotics cannot not be marketed as organic in the U.S. Canada is also exploring other international equivalency agreements with other trading partners to enhance trade opportunities for export and to assure the organic integrity of imported products.\nWhen considering organic certification, know the requirements and accreditation(s) needed in the marketplace where your products will be sold. When comparing certification bodies, make sure they have the certification requirements and accreditations needed to meet market requirements. As a minimum certification bodies should be accredited under the Canadian Organic Products Regulations. Some markets may require accreditation or equivalency agreements with countries in the European Union, or with the Japanese Agricultural Standard (JAS), Bio-Swisse or other international organic certification systems. As Canada develops international equivalency agreements the need for the certification body to have these international accreditations will diminish.\nFor more information on certification and links to Canadian regulations and standards see the Organic Agricultural section of the OMAFRA website at www.ontario.ca/organic or the CFIA website at www.inspection.gc.ca.\nThe first few years of organic production are the hardest. Organic standards require that organic lands must be managed using organic practices for 36 months prior to harvest of the first certified organic crop. This is called the “transition period” when both the soil and the manager adjust to the new system. Insect and weed populations also adjust during this time.\nCash flow can be a problem due to the unstable nature of the yields and the fact that price premiums are frequently not available during the transition since products do not qualify as “certified organic.” For this reason, some farmers choose to convert to organic production in stages. Crops with a low cost of production are commonly grown during the transition period to help manage this risk.\nCarefully prepare a plan for conversion. Try 10 per cent to 20 per cent the first year. Pick one of the best fields to start with and expand organic acreage as knowledge and confidence are gained. It may take five to 10 years to become totally organic, but a long term approach is often more successful than a rapid conversion, especially when financial constraints are considered. Parallel production (producing both organic and conventional versions of the same crop or livestock product) is not allowed. Use good sanitation, visually different varieties, individual animal identification and other systems to maintain separation and integrity of the organic and conventional products. Good records are essential.\nIn organic production, farmers choose not to use some of the convenient chemical tools available to other farmers. Design and management of the production system are critical to the success of the farm. Select enterprises that complement each other and choose crop rotation and tillage practices to avoid or reduce crop problems.\nYields of each organic crop vary, depending on the success of the manager. During the transition from conventional to organic, production yields are lower than conventional levels, but after a three to five year transition period the organic yields typically increase.\nCereal and forage crops can be grown organically relatively easily to due to relatively low pest pressures and nutrient requirements. Soybeans also perform well but weeds can be a challenge. Corn is being grown more frequently on organic farms but careful management of weed control and fertility is needed. Meeting nitrogen requirements is particularly challenging. Corn can be successfully grown after forage legumes or if manure has been applied. Markets for organic feed grains have been strong in recent years.\nThe adoption of genetically engineered (GMO) corn and canola varieties on conventional farms has created the issue of buffer zones or isolation distance for organic corn and canola crops. Farmers producing corn and canola organically are required to manage the risks of GMO contamination in order to produce a “GMO-free” product. The main strategy to manage this risk is through appropriate buffer distances between organic and genetically engineered crops. Cross-pollinated crops such as corn and canola require much greater isolation distance than self-pollinated crops such as soybeans or cereals.\nFruit and vegetable crops present greater challenges depending on the crop. Some managers have been very successful, while other farms with the same crop have had significant problems. Certain insect or disease pests are more serious in some regions than in others. Some pest problems are difficult to manage with organic methods. This is less of an issue as more organically approved biopesticides become available. Marketable yields of organic horticultural crops are usually below non-organic crop yields. The yield reduction varies by crop and farm. Some organic producers have added value to their products with on-farm processing. An example is to make jams, jellies, juice, etc. using products that do not meet fresh market standards.\nLivestock products can also be produced organically. In recent years, organic dairy products have become popular. There is an expanding market for organic meat products. Animals must be fed only organic feeds (except under exceptional circumstances). Feed must not contain mammalian, avian or fish by-products. All genetically engineered organisms and substances are prohibited. Antibiotics, growth hormones and insecticides are generally prohibited. If an animal becomes ill and antibiotics are necessary for recovery, they should be administered. The animal must then be segregated from the organic livestock herd and cannot be sold for organic meat products. Vaccinations are permitted when diseases cannot be controlled by other means. Artificial insemination is permitted. Always check with your certification body to determine if a product or technique is allowed in the Permitted Substances List and the organic standards. Organic production must also respect all other federal, provincial and municipal regulations.\nOrganic produce can usually qualify for higher prices than non-organic products. These premiums vary with the crop and may depend on whether you are dealing with a processor, wholesaler, retailer or directly with the consumer. Prices and premiums are negotiated between buyer and seller and will fluctuate with local and global supply and demand.\nHigher prices offset the higher production costs (per unit of production) of management, labour, and for lower farm yields. These differences vary with commodity. Some experienced field crop producers, particularly of cereals and forages, report very little change in yield while in some horticultural crops such as tree fruits, significant differences in marketable yield have been observed. There may also be higher marketing costs to develop markets where there is less infrastructure than for conventional commodities. Currently, demand is greater than supply for most organic products.\nOrganic farming can be a viable alternative production method for farmers, but there are many challenges. One key to success is being open to alternative organic approaches to solving production problems. Determine the cause of the problem, and assess strategies to avoid or reduce the long term problem rather than a short term fix for it.\nCOG – Canadian Organic Growers Inc.\n323 Chapel St., Ottawa ON K1N 7Z2\nPhone: (613) 216-0741, 1-888-375-7383\nEFAO – Ecological Farmers Association of Ontario\n5420 Highway 6 North,\nRR 5, Guelph, ON N1H 6S2\nPhone: (519) 822-8606\nOMAFRA – Ontario Ministry of Agriculture, Food and Rural Affairs\n1 Stone Road W., Guelph, ON N1G 4Y2\nAgr. Information Contact Centre\nOACC- Organic Agricultural Centre of Canada\nNova Scotia Agricultural College\nBox 550, Truro, Nova Scotia, B2N 5E3\nPhone: (902) 893-7256, Fax: (902) 893-3430\nGuelph Organic Conference\nFor information contact:\nTomás Nimmo, Box 116,\nCollingwood, ON L9Y 3Z4\nPhone: (705) 444-0923, Fax (705) 444-0380\nOCO - Organic Council of Ontario\nRR 5 Guelph, ON N1H 6J2\nPhone: (519) 827-1221, Fax: (519) 827-0721\n© Queen's Printer for Ontario, 2015\nThe information on this page was written and copyrighted by the Government of Ontario. The information is offered here for educational purposes only and materials on this page are owned by the Government of Ontario and protected by Crown copyright. Unless otherwise noted materials may be reproduced for non-commercial purposes. The materials must be reproduced accurately and the reproduction must not be represented as an official version. As a general rule, information materials may be used for non-profit and personal use."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:78a164a5-e884-4999-a30e-04571cb30409>","<urn:uuid:e6ddb9b3-68e1-422b-adc0-49ee5dd47471>"],"error":null}
{"question":"How do traditional horizontal farming and vertical farming compare in terms of water usage and space efficiency, and what role could this play in addressing Cladosporium mold issues?","answer":"Vertical farming systems significantly reduce water usage, requiring 90% less water than conventional horizontal farming while increasing yield per unit area through vertical space utilization. This efficient water management helps prevent the moisture problems that typically lead to Cladosporium mold growth, which thrives in environments with high humidity and water damage. While conventional agriculture requires large horizontal spaces and substantial water resources, vertical farming's controlled environment and water recycling systems offer a solution that both conserves resources and minimizes conditions favorable to mold growth.","context":["What Is Cladosporium Mold?\nCladosporium mold is a common household mold that may affect your health. In rare circumstances, this mold has even been known to cause infections.\nIt grows indoors and outdoors. Spores that are released by Cladosporium mold may be airborne if it is present indoors, which helps to spread the mold. It grows most often in areas where there are plentiful sources of moisture and humidity.\nMore than 500 different species of Cladosporium mold have so far been identified.\nIs Cladosporium Mold Toxic?\nAlthough Cladosporium mold may cause health issues for some people, it is not considered a dangerous mold to humans.\nAllergies are the most common health issue associated with exposure to this mold. Symptoms of an allergic reaction may include sneezing, coughing, a stuffy nose, a runny nose, and post-nasal drip.\nSome people may experience an itchy eyes, nose, or throat with exposure to Cladosporium mold.\nIn some circumstances, the allergic reaction may lead to a serious asthma attack, sinusitis, or a severe allergic reaction which constricts a person’s air pathways.\nRisk factors for an adverse reaction to Cladosporium mold include a family history of allergy, living in a home with lots of mold, or living in an environment which has consistently high humidity levels.\nPeople with chronic respiratory or skin problems are also at a higher risk of experiencing an adverse health issue when exposed to this mold.\nMost people can treat an allergic reaction or an allergy attack with their regular medications. Severe reactions should always be treated as a medical emergency.\nWhat Are the Cladosporium Mold Acceptable Levels?\nCladosporium is the most common mold species that is found in the home. Under current standards, optimum levels within the home are between 0-50 spores in an air sample report. At this level, it is considered to be a trace exposure.\nSpore levels of 50 to 200 spores is still considered to be very low with Cladosporium mold and is manageable for most individuals. Only people with severe sensitivities or compromise immune systems will typically struggle with this level of exposure.\nThe maximum acceptable level of Cladosporium mold sports is between 200 to 500 spores on the air sample report. This range is considered normal.\nIn some homes, spore levels of 500 to 1,500 spores may still be fine and not require some form of remediation. At this level, however, there is a greater risk of experiencing adverse physical effects over time.\nAnything about 1,500 spores may indicate water damage in the home, an issue with a building’s HVAC system, or an air conditioning system that requires service.\nCladosporium Mold in the Home\nThis mold is not easy to identify in many homes. Cladosporium mold grows with other mold types, appearing in black, green, and brown colors that are similar to other, sometimes more toxic molds.\nYou will find this mold growing on wallpaper, carpets, fabrics, and wood surfaces when there are high moisture levels present in the building. If there is condensation which forms on your windows, there is a good chance you’ll find Cladosporium mold growing in your window sills and tracks.\nIt will also grow in your HVAC vents, air conditioning systems, vent covers, and cabinets. If there is water damage somewhere in the building, there is a good chance this mold will grow.\nKitchens, bathrooms, and garages are the most common locations. Attics may see mold growth when moisture moves upward.\nHealth Effects from Cladosporium Mold\nMost people will not suffer ill health effects from an exposure to Cladosporium mold.\nA majority of people who do suffer allergy or asthma-like symptoms when exposed to this mold can treat their symptoms with common over-the-counter medications.\nOptions including decongestants, antihistamines, and nasal corticosteroids. Prescription anti-allergy drugs may be used when reactions are moderate to severe. For individuals who are sensitive to allergy medications, a nasal rinse may provide some symptom relief.\nFor people who suffer from asthma-like symptoms, which includes tightness in the chest, coughing, wheezing, or difficulty breathing, an inhaler or nebulizer will often provide relief.\nIf ongoing symptoms continue or grow in severity, immunotherapy may be recommended in rare instances.\nHow to Remove Cladosporium Mold?\nAlthough health issues with Cladosporium mold are uncommon, its presence indicates the structure of your walls, joists, floor, or other components are compromised. It is essential to remove the mold as soon as possible for best results.\nFor most homes, a professional inspection may be required to identify locations where the mold may be growing. The benefit of having such an inspection is that it can identify hidden areas of water damage where mold may be growing without knowledge.\nYou must identify the source of mold growth during your cleaning effort. If you only clean the moldy areas without addressing the source, then the Cladosporium will return in time.\nBefore cleaning any of the mold, be sure to wear personal protective equipment. That includes shoe covers, gloves, and a respirator. In severe cases, a protective body suit may also be required.\nSeal off the area with heavy plastic sheets. Use negative air equipment to stop the mold spores from spreading. Then remove any items with mold from the area. Follow this up by treating any moldy areas with a fungicide or bleach product. Then allow the entire area to dry.\nDepending on your renter’s or homeowner’s insurance policy, some (or all) of these costs may be covered. You will want to speak with your insurance agent to determine what benefits may be available to you.\nCladosporium mold growth is evidence that a building’s environment is compromised. It appears most often when there is water damage. There are times, however, when an improperly vented bathroom, kitchen, or spill that is allowed to remain can encourage mold growth over time.\nWith proactive cleaning efforts and proper appliance methods, it is possible to limit the damage of Cladosporium mold. You can reduce the unwanted health symptoms that may be experienced. Get into the habit of inspecting your home or building at least once per month to ensure that any mold growth is contained.","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9c60e224-e606-4c2a-8502-a4fc4ebe04d9>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"What are the differences between how terminally ill dogs are handled under India's Supreme Court guidelines versus Northern Ireland's animal welfare regulations?","answer":"Under India's system, according to Rule 9 as interpreted by the Supreme Court, dogs can only be euthanized when they are mortally wounded or terminally ill, and this must be medically proven by a Government certified Veterinary Doctor - specifically only when the dog has no chance of recovery and its survival causes it more pain. In Northern Ireland, under the Welfare of Animals Act (NI) 2011, the focus is on ensuring an animal's needs are met and preventing unnecessary suffering, with authorities having the power to take various actions including giving advice, issuing improvement notices, taking animals into their possession, or pursuing prosecution. The law requires those responsible for animals to take reasonable steps to ensure their welfare.","context":["The Interim order of the Supreme Court of India passed on the 18th Nov 2015 while hearing ‘all street dog related matters’ emphatically directs that the laws made viz. Prevention of Cruelty to Animals Act, 1960 and the Animal Birth Control (Dogs) Rules, 2001 have to be implemented and that there shall not be any indiscriminate killing of dogs. The Supreme Court Order observed that “There can be no trace of doubt that there has to be compassion for dogs and that they should not be killed in an indiscriminate manner…”\nThe key points in this Interim Order issued by the Honourable Supreme Court of India on 18th November 2015 are as follows:\nThe Order states- “There can be no trace of doubt that there has to be compassion for dogs and they should not be killed in an indiscriminate manner, but indubitably the lives of the human beings are to be saved and one should not suffer due to dog bite because of administrative lapse.”\nIt further reiterates that, “Rule 6 of the Animal Birth Control Rules 2001 enacted under the Prevention of Cruelty to Animals Act 1960, provides for obligations of the local authority. Rule 7 deals with capturing/sterilisation/ immunisation/release. Rule 8 deals with identification and recording and Rule 9 provides for euthanasia of street dogs. Rule 10 deals with furious or dumb rabid dogs.” and goes on to add that, ” As we find, the local authorities have a sacrosanct duty to provide sufficient number of dog pounds, including animal kennels/shelters, which may be managed by the animal welfare organizations, that apart, it is also incumbent upon the local authorities to provide requisite number of dog vans with ramps for the capture and transportation of street dogs; one driver and two trained dog catchers for each dog van; an ambulance-cum-clinical van as mobile centre for sterlisation and immunisation; incinerators for disposal of carcasses and periodic repair of shelter or pound. Rule 7 has its own significance. The procedure has to be followed before any steps are taken. Rules 9 and 10 take care of the dogs which are desirable to be euthanised.”\nTo explain to our readers Rule 9 – it means that only mortally wounded or terminally ill dogs i.e.when and only when a dog is unfit for survival/has no chance of recovery/healing which is medically proven by a Government certified Veterinary Doctor and it’s survival causes more pain for it is when u can think of euthanising.\n….and then the order hammers home the point, that for now that the Animal Birth Control Rules, 2001, (for short, ‘the 2001 Rules’) shall prevail over the provisions contained in any local Act/Municipality Act by stating that, “for the present it is suffice to say that all the State municipal corporations, municipal committees, district boards and local bodies shall be guided by the Act and the Rules and it is the duty and obligation of the Animal Welfare Board to see that they are followed with all seriousness. It is also the duty of all the municipal corporations to provide infrastructure as mandated in the statute and the rules. Once that is done, we are disposed to think for the present that a balance between compassion to dogs and the lives of human being, which is appositely called a glorious gift of nature, may harmoniously co-exist.”.\nAnd towards the end of this Interim Order passed by the Honourable Supreme Court of India, instructions are laid out for the Local bodies to follow, “The local authorities shall file affidavits including what kind of infrastructures they have provided, as required under the law. Needless to emphasize, no innovative method or subterfuge should be adopted not to carry out the responsibility under the 1960 Act or the 2001 Rules. Any kind of laxity while carrying out statutory obligations, is not countenanced in law.”\n“A copy of the order passed today be sent to the Chief Secretary of each of the States and the competent authority of Union Territories, so that they can follow the same in letter and spirit. We would also request all the High Courts not to pass any order relating to the 1960 Act and the 2001 Rules pertaining to dogs. Needless to say, all concerned as mentioned herein-above, shall carry out this order and file their respective affidavits as directed“…are the concluding lines of this order as the matter gets listed for Final Hearing and Disposal on 9th March 2016.\nThe Press Release issued by the Animal Welfare Board of India (AWBI), a statutory body under the Ministry of Environment, Forests and Climate Change, Government of India on the above stated order, is shared below.","Under the Welfare of Animals Act (NI) 2011 a person who is responsible for an animal must ensure that its needs are met, and it is an offence to fail to take reasonable steps to ensure its welfare.\nCausing an animal unnecessary suffering is also an offence and can be punished with a prison sentence of up to 5 years and an unlimited fine.\nThree separate bodies are responsible for enforcement of the Act depending on the type of animals involved and, in some cases, the nature of the welfare issue.\nOfficers from the enforcement bodies will investigate complaints and can use their statutory powers to take a range of actions including giving advice, issuing legally binding improvement notices, taking animals into their possession and prosecution.\n|During the warmer months when temperatures soar, make sure you don't leave your dog or pet alone in a car. This can prove fatal. Read our Dogs in Hot Cars leaflet for more information.\n|What to do if you become aware of an animal welfare case\nReport it to the appropriate enforcement body as set out below, providing as much information as possible.\nPlease do not:\n- Upload comments or photographs to social media sites or other media outlets as this may jeopardise potential court proceedings\n- Interfere with evidence as this may affect any subsequent criminal investigation\n- Attempt to rescue an animal from another person or property as you may be found to be acting illegally.\nCouncils are responsible for enforcement of the Welfare of Animals (NI) Act 2011, as it applies to Non-Farmed Animals. This means domestic pets of any vertebrate species such as cats, dogs, horses and donkeys.\nThe Council Animal Welfare Officers do not offer re-homing services for unwanted animals but will be able to refer you to other bodies that may be able to help.\nIf you need to report a Non-Farmed Animal welfare case during office hours (Monday – Friday, 9am-5pm) you should contact the appropriate telephone number or email address relating to the region you live in, these are set out below.\nPlease note that emails will only be responded to during office hours which are Monday to Friday 9am-5pm.\nReporting an animal welfare issue in Mid Ulster\nIf you need to report an animal welfare case in the Mid Ulster District Council area, call the Western Area animal welfare team (also covering Fermanagh & Omagh, Derry & Strabane)\nT: 028 8225 6226\nThe Department of Agriculture Environment and Rural Affairs (DAERA) is responsible for enforcement of the Welfare of Animals (NI) Act 2011, in relation to Farmed Animals. Welfare complaints in respect of Farmed Animals (that is animals bred or kept for the production of food, wool or skin or other farming purposes) should be referred to DAERA.\nOffice hours are:\nMonday to Friday, 9am-5pm\nT: 0300 200 7840.\nAfter 5pm and at weekends if you have a complaint relating to a Farmed Animal contact a local Private Veterinary Practice or the PANI who will, as necessary, refer the welfare complaint to the relevant “on call” officer.\nIf your issue relates to wildlife crime, badger baiting, poisoning of birds, destroying or disturbing bat roosts, release into the wild of non-native species, trapping wildlife illegally, trade in endangered species, deer poaching or if an animal is on the road or other criminal activity is suspected please contact the Police non-emergency number 101 or in an emergency 999.\nAlternatively if you would prefer to provide information without giving your details, you can contact the independent charity Crimestoppers and speak to them anonymously on 0800 555 111.\nInformation on animal welfare and how to care for animals is available at www.nidirect.gov.uk/animal-welfare."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:026fd7bd-f832-4659-802b-e0602772b302>","<urn:uuid:17d5ffda-719a-4635-99ad-25b24086d691>"],"error":null}
{"question":"How to identify if a child has failure to thrive symptoms?","answer":"To identify failure to thrive symptoms, look for the following signs: slow or stalled height and weight gain, delayed development milestones, fatigue or excessive sleepiness, emotional development delays, and delayed puberty in teens. If you notice one or more of these symptoms in your child, you should consult with your pediatrician for further evaluation.","context":["What is Failure to Thrive?\nIf your child isn't meeting appropriate growth expectations based on age, gender, or family history, it's possible that they could be diagnosed with failure to thrive (FTT). Failure to Thrive is a medical condition that occurs when a child gains weight significantly slower than expected due to inadequate nutrition. It is typically diagnosed when a child is below the third or fifth percentile or if weight crosses two percentile lines on the growth chart a pediatrician references for your child.\nChild Growth Charts & Nutrition\nAs a parent, it’s important to keep in mind that child growth charts are used to show average growth patterns for weight and height for specific ages and genders. Talk with your pediatrician about growth percentiles, and determining the appropriate height and weight range for your child.\nIt’s also important to know what growth standards are so you can be sure your child is getting the proper nutrition to grow strong and healthy. Child nutrition that includes essential nutrients, such as calcium, vitamin D, potassium, and fiber, is a key element to promoting growth and development in your child's life. Researching the My Plate Plan for kids through choosemyplate.gov can also help shed some light on what could be missing from your child’s diet.\nTypes of Failure to Thrive:\nOrganic Failure to Thrive vs. Non-organic Failure to Thrive\nOrganic failure to thrive is the result of a pre-existing medical condition that can lead to malnutrition, due to the fact that the child is not receiving the necessary nutrients to sustain healthy growth. Some medical conditions associated with organic failure to thrive are:\n- Celiac disease\n- Inflammatory bowel disease\n- Cystic fibrosis\n- Cerebral palsy\nNon-organic failure to thrive is often caused by a child not consuming enough calories to support growth and may be due to the following reasons:\n- Selective eating behaviors\n- Confusion around proper feeding techniques\n- Poor living conditions or poverty\n- Incorrectly prepared formula or lack of breast milk during infancy\n- Low birth weight or premature birth\nFailure to Thrive Symptoms\nFTT can sometimes be difficult to diagnose because the symptoms can be similar to other conditions. If you believe your child may have inadequate growth for their age, be on the lookout for the following possible failure to thrive symptoms:\n- Slow or stalled height and weight gain\n- Delayed development milestones\n- Fatigue or excessive sleepiness\n- Emotional development delays\n- Delayed puberty in teens\nIf you notice one or more possible symptoms in your child, talk to your pediatrician to further evaluate the possibility of FTT and possible next steps.\nAddressing Failure to Thrive\nOrganic Failure to Thrive\nIf your child’s failure to thrive is caused by a pre-existing medical condition, it is best to discuss proper treatment of the condition with your pediatrician.\nNon-Organic Failure to Thrive\nIf FTT is associated with an environmental issue, once growth charts and diet records are assessed by a pediatrician, treatment may take the form of proper child nutrition including a special diet or nutritional therapy. For older children living with FTT, re-thinking eating habits and increasing caloric intake may help promote height and weight gain.\nWhen to Talk to Your Pediatrician and What to Ask\nIf you feel that your child is struggling in either height or weight, schedule an appointment to get your pediatrician’s opinion and assessment. They will evaluate all factors that could be contributing to your child’s growth, including daily feeding habits, or any possible illnesses or underlying medical conditions that could be affecting growth.\nBefore your appointment, review these helpful questions to ask your pediatrician:\n- How is my child growing? Should I be concerned with his/her height/weight?\n- How much exercise should my child get per day? My child is very active/not active – does this change recommended daily caloric intake?\n- How many hours should my child sleep per night? Are naps acceptable? Is it normal for him/her to be tired often?\n- How many meals should my child eat per day? Are there nutritional supplements he/she should be receiving daily/per week?\n- Is there anything I could be doing as a parent to encourage proper eating habits or a better feeding schedule?\n- Is it possible my child has a medical condition that is hindering him/her from eating correctly?\nFor additional helpful advice on how to handle Failure to Thrive or incorporating better feeding routines into your child’s daily eating habits, contact a PediaSure feeding expert."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:3f41af07-1bfc-4e4a-95e5-4091aab4cb31>"],"error":null}
{"question":"Why should we treat others with kindness according to religious teachings?","answer":"According to religious teachings, specifically from the Bible, we should treat others with kindness for several reasons: 1) to be like God, 2) to show we are his children, 3) because God treated us well, and 4) because it is God's law. This is emphasized in Matthew 7:12 and John 13:34-35, which teach that loving one another is a fundamental commandment and a way to demonstrate being disciples of Jesus.","context":["Home / Lessons / Arts & Crafts / Activities & Games / Teacher Corner / Resources / Great Links\nIron Rule / Silver Rule / Golden Rule\n\"Do to others before they do it to me.\"\n\"Do to others as they do it to me.\"\n\"Do to others as I would like them do\nThe Golden Rule\nTo Remember: We should treat others as we want to be treated by them.\nVerse from the bible:\nMatthew 7:12 Therefore all things whatsoever ye would that men should do to you, do ye even so to them: for this is the law and the prophets.\nGod has always given His people guidance on how they should treat one another. We tend to be selfish, unless trained otherwise. But God wants us to be considerate of others. He said that we should treat others they way we would like to be treated. How do you like to be treated? With respect, listened to, not interrupted, loved, share things with, treated fairly, honest with, follow through on promises. If we like these things from others, it is only natural that others would like us to treat them the same.\nDisplay and discuss the iron rule, silver rule, golden\nDefine on a\nchart: Iron Rule: \"Do to\nothers before they do it to me.\" Silver Rule: \"Do to\nothers as they do it to me.\" Golden Rule: \"Do to\nothers as I would like them do to\nDefine on a chart:\nIron Rule: \"Do to others before they do it to me.\"\nSilver Rule: \"Do to others as they do it to me.\"\nGolden Rule: \"Do to others as I would like them do to me.\"\nJohn 13:34,35 A new commandment I give unto you, That you love one another; as I have loved you, that you also love one another. By this shall all men know that you are my disciples, if you have love one another.\nTrue or False:\n1. God didn't always want people to love each other. F\n2. We don't know how others want to be treated. F\n3. Jesus cared for others more than himself. T\n4. People around us can tell if we love one another. T\n5. It is easy to be unselfish. F\n6. The silver rule says: love those that love you. T\n7. Loving others makes us like God. T\nDiscussion: Have students choose the \"rule\" applied in each of the examples (iron, silver, gold)\nPrintable Worksheet: Unscramble\na verse about the golden rule\nReview Questions for gameboard\n1. Summarize the \"golden rule\". Treat others as you want them to treat you\n2. Unscramble the word: veol (love)\n3. Name the book where this lesson is taught. (Matthew, John)\n1. Act out someone who is not putting others first.\n2. Draw a heart with your eyes shut.\n3. Name something you will do this week for someone you don't like.\n1. How do you feel when others let you pick first? ( Special, happy )\n2. How do you feel when other people won't share with you? ( Unhappy, disliked )\n3. How does God feel when we treat each other nice? ( Happy, pleased )\n1. How should I treat those who don't like me? ( Tthe same as my friends )\n2. In what ways has God treated you better than you deserve? ( He always provides for us, he blesses us with more than we need, he give us always a way for Salvation)\n3. In what ways can you treat others \"like yourself\"? ( Share the best of what I have, let them go first, treat them politely and kindly, help when needed )\n1. Who treats us better than anyone else? God\n2. Jesus was the one who initiated the Golden Rule? Yes\n3. Who should we show kindness to? Everyone\n1. Why should we treat others well?\n1) to be like God, 2) to show we are his children, 3) because we were treated well by God, 4) it is God's law\n2. How do we determine how to treat others?\nWe do what we'd like done to us\n3. What example can we look to when we are uncertain how to act?\nPlay our S.S.L. board game to review that lesson"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:7da4ba0d-bee7-4fa9-89e8-6baf37c9ee41>"],"error":null}
{"question":"How did SM City Marikina design its mall structure to protect against severe flooding, and what specific architectural features were implemented to ensure disaster resilience?","answer":"SM City Marikina implemented several key design features for flood resilience. The mall was built 90 meters away from the Marikina River centerline - 20 meters farther than standard regulations required. It was elevated on cylindrical stilts that allow water to flow through the structure. The lower parking level was built without wall enclosures at natural ground level, while the upper ground floor was elevated 20.5 meters above the maximum recorded flood level. The mall itself stands on the third floor supported by stilts. These design elements allow water to flow freely through the parking structure during extreme floods while keeping the main mall area safe and operational.","context":["(29 November 2017. Pasay City, Philippines) SM City Marikina, which was opened in 2008, has been a model for disaster resilience, having been showcased in the Top Leaders Forum (TLF) hosted by SM Prime Holdings, Inc. (SM Prime) from November 28 to 29 at the Conrad Manila hotel in Pasay.\nSM’s investments in disaster resilience continues to pay off for SM City Marikina even after it withstood Typhoon Ketsana or Tropical Storm Ondoy, one of the strongest typhoons that hit the country in 2009 which submerged nearly half of the city.\n“When we were building the mall, we commissioned a study spanning a hundred years of flood data in the area, going beyond the annual flood cycles of areas for possible development, and then we added a meter more as allowance. We were on the right track as there was now damage on our properties and those of our affiliates, mall tenants, jobs and lives vis-à-vis the rest of Marikina City,” said Mr. Hans T. Sy, SM Prime Chairman of the Executive Committee and Philippine Private Sector Representative to the United Nations Office for Disaster Risk Reduction (UNISDR) said.\nBy design, SM City Marikina is an elevated mall resting on stilts on a six-hectare property within the Marikina River Watershed, a known flood-prone area. As part of its disaster resilience design, SM Prime built SM City Marikina 20 meters farther than the standard regulation of buildings – 90 meters away from the Marikina River centerline. The stilts are also cylindrical in shape, a science-based solution that allows water to flow through the mall structure. This distance helped reduce the risks of damaging the mall during floods.\nThe main roads surrounding the mall layout at the natural ground level was considered in the design. Thus, the lower parking level was built without any wall enclosure. The upper ground floor level was built at an elevation of 20.5 meters more than the maximum flood recorded. These features allowed water to flow freely through the parking structure while the mall stands on the third floor lifted by supportive stilts during extreme floods.\nWith these features, the mall is also able to open immediately for business while offering a safe haven and free parking to residents of the city during severe typhoons.\nWhen Typhoon Ondoy struck, the mall’s resiliency saved almost PHP1 billion in terms of losses from business sales alone. The employees of the mall including its agencies and tenants were able to report for work right after the typhoon. Additional costs of 15% to build a disaster-resilient mall were recouped due to the prevention of potential losses. SM City Marikina also managed to prevent severe damage that could have affected the lives and businesses of more than 1,400 mall employees, affiliates and agency personnel, over 200 merchandise suppliers as well as regular customers.\nToday, SM Prime allocates 10% of capital expenditure for disaster resilience in the construction of buildings.\n“By investing in resilience we minimize vulnerability, better safeguard physical assets, reduce recovery expense, and contribute to local government efforts to safeguard the communities we operate in,” Mr. Sy had said.\nSM Supermalls President Annie Garcia shares SM City Marikina’s story of resilience with UN Special Representative of the Secretary-General for Disaster Risk Reduction Dr. Robert Glasser (right). Withstanding Tropical Storm Ondoy being one of the strongest typhoons that hit country, SM City Marikina was showcased in the recently concluded 2017 Top Leaders Forum hosted by SM Prime, UNISDR, and ARISE Philippines at the Conrad Manila hotel in Pasay.\n# # #\nAbout the Top Leaders Forum\nThe TLF is an annual conference led by the United Nations in partnership with SM Prime which draws over 200 participants including CEOs, top managers and executives locally and from the region. Since its inception in 2012, the conference aims to find concrete measures that would make businesses more resilient against disasters. In 2015, the United Nations Office for Disaster Risk Reduction (UNISDR) established the Private Sector Alliance for Disaster Resilient Societies (ARISE) with SM Prime as the secretariat for the Philippine Chapter. SM Prime Holdings’ Mr. Hans Sy sits as the only Filipino board member of the ARISE international Board along with top global business leaders.\nAbout SM Prime\nSM Prime Holdings, Inc. (SMPHI) is one of the largest integrated property developers in Southeast Asia that remains committed to its role as a catalyst for economic growth, delivering innovative and sustainable lifestyle cities, with the development of malls, residences, offices, hotels and convention centers, thereby enriching the quality of life of millions of people."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e8e24892-d7e0-4882-9320-80b434492618>"],"error":null}
{"question":"What documents related to home ownership should be kept permanently?","answer":"You should keep housing records for as long as you own the home plus 6 years. This includes documents showing the purchase price, costs of permanent improvements (remodeling, additions, installations), and expenses from selling and buying the property (legal fees, real estate agent commission). These records are important because improvements and selling expenses are added to the original purchase price to calculate capital gains tax when you sell your house.","context":["When it comes to financial records retention and records retention guidelines, there aren’t any hard and fast records retention guidelines. The purpose of this document is to provide some basic rules of thumb for the retention of personal financial records. It’s always good to check with your accountant or other resource for your specific personal guidelines before implementing your policy. Also, to assist with the implementation of a policy while also reducing your risk of identity theft, it is a good idea to invest in a shredder and possibly a scanner.\nTax Returns and Supporting Documentation 7 Years\nWhether personal or business, the general rule is seven years. This may seem like a long time to hold onto these papers, but think of it as an annual cleaning out as new returns are filed. One in, one out and the old adage of “better safe than sorry” will apply. The IRS has 3 years from your filing date to audit your return if it suspects good faith errors. However, there are also exceptions to the three-year policy, including:\n- False or Fraudulent Return – Tax may be assessed at any time, without limitation.\n- Willful attempt to avoid tax – Tax may be assessed at any time, without limitation.\n- No return – Tax may be assessed at any time, without limitation.\n- Extension by Agreement – Assessment period defined by agreement between IRS and taxpayer.\n- Tax resulting from changes in certain income or estate tax credits – No timeframe defined.\n- Tax resulting form distributions or terminations from a life insurance company – 3 years\n- Termination of private foundation status – Tax may be assessed at any time, without limitation.\n- Substantial omission of items (generally defined as over/under reporting of income by 25%) 6 years.\nPersonal Health Records Indefinitely\nYour and your family’s personal health records should be kept indefinitely in your home file records and should include the following information: complete contact information of your personal physicians; your medical history; and your prescriptions and/or treatments prescribed. Having this complete information at your fingertips will save you time and energy in the future.\nMedical Records End of Treatment + 5 Years\nIf you are able to claim medical expenses on your tax return, it is recommended that you keep the records for seven (7) years from the end of the year in which they are claimed. For all other medical records it is recommended that you keep them for 5 years from the time you are no longer being treated for the symptoms that those records are pertaining to. For example, if you have chronic anemia, you should keep all records pertaining to those symptoms until you no longer have chronic anemia, but if you get treated for the flu, those records should be kept for five (5) years, if you did not claim them on your taxes.\nMedical Insurance Service Rendered + 5 Years\nThis includes your premium statements, doctor bills, copies of prescriptions, hospital bills, etc. The general rule here is five years from the date of the service rendered.\nLife Insurance Policies Life of Policy + 3 Years\nIt is recommended that you keep life insurance policy information for the life of the policy plus three (3) years.\nLegal Documents Indefinitely\nItems such as Marriage Certificates, Death Certificates, Divorce Papers, Trust documents, Wills, etc. should be retained indefinitely in a fire proof safe or safe deposit box.\nPay Check Stubs Current Year – Validate Against W-2 and SS Statement\n- Keep the year-long worth of stubs until you reach the year-end check of December 31st that recaps the entire 12 months worth of pay, social security, taxes, etc. When you receive your annual W-2 form from your employer, make sure the information on your stubs matches and then shred the stubs.\n- Also validate the W-2 statement with the annual Social Security statement.\nBank Statements 3 Months\nThe only reason you would really consider keeping bank statements on hand is if you were thinking about applying for a mortgage, and that would require a three month history. Otherwise, the bank has all of your records if a need arises.\nCredit Card Statements 3 Months\n- Credit Card Receipts – Keep your original receipts until you get your monthly statement; toss the receipts if the two match up. Keep the statements for seven years if tax-related expenses are documented.\n- Credit Card Statements – Here the records reflect only a proof of charges and the credit card companies can always reproduce the reports if need be. Generally it is recommended that you keep the current three months on hand.\nATM Receipts 3 Months\nThis is a very individual thing. Tons of people have glove compartments and wallets brimming with ATM slips from four years ago. A suggestion might be when you do an ATM transaction to immediately enter it into your check book listing and then toss the receipt. That way you might be able to find the CD’s in the glove compartment.\nRetirement and Savings Plans 1 Year to Permanently\n- Brokerage and Mutual fund statements – Retain documentation for all you money you’ve invested – this will allow you to determine your cost basis for any future sales. Once you have sold the investment, hold on to the documents for the three years referenced in the Tax Return section.\n- Keep the monthly/quarterly statements from your 401(k) or other plans until you receive the annual summary. Discard (shred) the monthlies/quarterlies statements once you receive the annual summary that reflects the yearly activity.\n- Keep the annual summaries until you retire or close the account.\nIRA Contributions Indefinitely\n- If you made a nondeductible contribution to an IRA, keep the records indefinitely to prove that you already paid tax on this money when the time comes to withdraw.\nFinancial Documents 3 Months\nChances are if you have any stocks, bonds, mutual funds, etc. you are inundated with prospectus, privacy notices, address confirmations, and on and on. Don’t keep any of these unless again, you plan to act on them within the next two weeks. In regards to your various benefit statements, you may want to keep this information indefinitely in order to determine your future retirement benefits. Public companies ask you to vote for the board of directors and special measures once a year. Unless you own a significant amount of stock or have a strong opinion, you may wish to save the company postage and toss the vote card. Again, either send in the paperwork or throw it away.\nMortgage Statements Ownership Period + 7 Years\nIt is recommended that you keep your mortgage statements for the ownership period of the mortgaged property plus seven (7) years.\nMortgage Documents Ownership Period + 7 Years\nThe good news is that the most important documents are probably recorded in your county records. For example, if you use a commercial bank for your financing then they will record a mortgage on the property when you take out the loan. After you’ve paid off the mortgage they are obligated to record a satisfaction of mortgage. If you look at that document, you should see some recording marks along the side indicating the book and page in which it was recorded in the county records. To be safe, however, I’d hold onto that document for 10 years. Whenever someone thinks you owe them money, you’ll want to hold on to proof of payment for longer than you would if the reverse was true. The burden of proof is going to be on a lender to prove you owe money. If you’ve paid it off, you’ll want to prove that you did. This can be done with the satisfaction of mortgage or with bank statements and canceled checks that your bank has on microfilm. However, if this information is readily available from the bank or county records, you don’t need to worry too much. If you are doing renovations, make sure you get the satisfaction of lien from the contractors doing the work. Keep that as long as you own the property. When you sell it, the lawyers will do a title search and the title insurance should take over after that.\nHome Records, including Repair Bills & Contracts Period of Ownership + 6 Years\nThis is another great area where a scanner might save you some significant file room.\n- Housing Records: As long as you own the home, plus 6 years.\n- Keep all records documenting the purchase price and the cost of all permanent improvements – such as remodeling, additions and installations.\n- Keep records of expenses incurred in selling and buying the property, such as legal fees and your real estate agent’s commission, for six years after you sell your home.\n- Holding on to these records is important because any improvements you make on your house, as well as expenses in selling it, are added to the original purchase price or cost basis. This adds up to a greater profit (also known as capital gains) when you sell your house. Therefore, you lower your capital gains tax.\nHome Insurance Life of Policy + 10 Years\nThe minimum timeframe suggested is five years. However, if you think that you may have any issues in the future, go with the ten year rule. You really shouldn’t rely on the insurance company to provide copies of your records since any burden of proof will fall to you.\nUtility Bills 3 Months\nIf you are writing off your utility bills for tax purposes, you may need to keep them as tax records. However, if you can’t write them off, you can keep a minimal amount of bills (last 3 months). The utility companies can recreate the others for you if you need them. Three months allows you to establish residency for purposes of drivers licenses, voter registration, mortgage application, etc.\nWarranty/Guarantee Documents Expiration Date\nRemember that old toaster oven? Well, keep in mind that anytime you get rid of an appliance, telephone, or anything else that had warranty documentation, you can safely discard the papers at the same time! The general guideline is to dispose of a warranty at the date of expiration.\nBig Purchase Items Ownership Period\nInvoices and receipts for big purchases (such as jewelry, rugs, appliances, antiques, cars, collectibles, furniture, computers, etc.) should be kept in an insurance file for proof of their value in the event of loss or damage.\nPaul Staib | Certified Financial Planner (CFP®), MBA\nPaul Staib, Certified Financial Planner (CFP®), is an independent Fee-Only financial planner. Staib Financial Planning, LLC provides comprehensive financial planning, retirement planning, and investment management services to help clients in all financial situations achieve their personal financial goals. Staib Financial Planning, LLC serves clients as a fiduciary and never earns a commission of any kind. Our offices are located in the south Denver metro area, enabling us to conveniently serve clients in Highlands Ranch, Littleton, Lone Tree, Aurora, Parker, Denver Tech Center, Centennial, Castle Pines and surrounding communities.\nPaul Staib | Certified Financial Planner (CFP®), MBA• Written By\nYou probably get an annual physical. Your doctor will ask about your concerns, take your blood pressure, perform other basic…"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2e308287-d460-488b-90e4-8b0c0103a58e>"],"error":null}
{"question":"How do environmental changes affect algae in both coral reefs and kelp forests, and what are the consequences for marine life?","answer":"In coral reefs, the loss of coralline algae due to sea urchin overgrazing reduces reef growth and affects coral larvae settlement. In New Zealand's kelp forests, environmental changes including ocean warming and coastal darkening from sediment are forcing seaweeds to retreat to shallower waters and threatening their survival. These changes have significant consequences for marine life - coral reefs support coastal protection and food sources for millions of people, while New Zealand's kelp forests provide crucial habitat for economically important species like pāua (abalone), kina (sea urchins), rock lobster and near-shore finfish.","context":["NZ’s Vital Kelp Forests are in Peril from Ocean Warming\nYears of almost non-stop marine heatwaves are stressing New Zealand’s kelp forests. But as we show in our new research, ongoing ocean warming is only one of several threats to these unique and important coastal seaweed ecosystems.\nMany seaweed species are sensitive to changes in the ocean’s acidity and coastal “darkening” – changes in colour and clarity – is forcing some to retreat to shallower waters. All these stress factors combined place these crucial habitats in peril, with consequences for all species that depend on them.\nNew Zealand has the ninth longest coastline in the world (at about 15,000 kilometres). This is almost twice the length of Australia’s great southern reef, which has been valued at AUS$10 billion annually.\nNo equivalent valuation has been calculated for New Zealand’s seaweed-dominated rocky reefs, but we know they provide crucial habitat for economically and culturally important species such as pāua (abalone), kina (sea urchins), rock lobster and near-shore finfish.\nOur ability to predict the future impacts on these species depends on our understanding of how the coastal habitats they require are changing.\nRising heat and dimming light\nNew Zealand’s seaweed ecosystems include canopy-forming large brown algae as well as several understorey species. Driven by rising ocean temperatures, more frequent, intense and longer marine heatwaves already place these kelp forests under thermal stress.\nWe predict that marine heatwaves will change the range and basic physiology of many seaweed species, removing sensitive ones from the northern edges of their ranges and slowing growth rates of most.\nSeaweeds require sunlight to photosynthesise and grow. But increasingly frequent erosion events from extreme weather like cyclone Gabrielle raise sediment levels in coastal seawater, leading to coastal darkening.\nSediment running off the land, made worse by storm events, has already lowered photosynthetic rates of seaweeds in several regions. New Zealand’s geologically young landmass is eroding rapidly in some areas and research shows seaweed communities are creeping further up the reef into shallow waters. They can no longer survive in deeper reefs because of low light.\nThis darkening will intensify if we don’t halt erosion and remediate lands. The effects of sedimentation have likely already limited the distribution of some seaweeds and will continue to do so in future.\nWhen the sea becomes more acidic\nOcean acidification, which makes seawater more acidic because of the excess carbon dioxide it is absorbing, threatens the ability of calcareous seaweeds to grow. It puts the survival of sensitive coralline algae (the pink algae covering coastal rocky reefs) at risk, slowing their growth and ability to spread to new space.\nThese red seaweeds form skeletons made of calcium carbonate and represent the seaweed group most sensitive to ocean acidification. Within coralline algae, some species are more sensitive than others.\nIn New Zealand, coralline algae provide important surfaces for larval settlement of many species, including pāua and kina. Whether the species of coralline algae that provide vital settlement substrates are sensitive or robust to ocean acidification remains unknown.\nEcological impacts on seaweed ecosystems\nSeaweed communities provide enormous benefits to New Zealand. They provide food and habitat for several marine invertebrates and finfish species of both cultural and commercial value, including pāua, kina, moki, snapper, rocklobster, blue cod and butterfish.\nIf seaweed ecosystems are altered, these species will experience changes in food supply and habitat availability.\nGiant kelp (Macrocystis pyrifera) and bull kelp (Durvillaea spp.) are important habitat formers and food suppliers for other species. But they are also extremely sensitive to the impacts of temperature stress.\nMarine heatwaves and ongoing ocean warming likely threaten them throughout their ranges, especially at their northern limit.\nShifting population ranges and invasions\nWarming oceans also facilitate the spread of Australian long-spined urchins (Centrostephanus rodgersii) which will threaten seaweed communities in northern New Zealand.\nPopulations of this warm-water species have already expanded in some parts of New Zealand, similar to Tasmania where it invaded during the 1950s and caused widespread “urchin barrens” where the urchins graze through kelp beds, leaving few seaweeds.\nNew Zealand is home to around 1,100 species of seaweeds. Many are poorly documented and with unknown ranges.\nThe combined effects of climate change will likely result in the extinction of some species with very narrow ranges. This is especially true for species that are sensitive to changes in light or temperature and currently live near the edges of their physiological limits.\nTo add to all of this, we don’t know how the impacts of the invasive caulerpa seaweeds Caulerpa brachypus and Caulerpa parvifolia will interact with climate change.\nThe degradation of coastal ecosystems is another reminder that we must move to eliminate the use of fossil fuels, as well as limiting overfishing and sedimentation.\nThere is potential to breed high-temperature, low-light and low-pH resistant strains of major species to help restore these ecosystems in the future, but we need strategic government investment in integrated coastal management and climate adaptation to save them in the first place.\nChristopher Cornwall, Lecturer, Te Herenga Waka — Victoria University of Wellington\nWendy Nelson, Senior Research Fellow, Auckland War Memorial Museum\n(Source: The Conversation)","This is a printer version of an UnderwaterTimes.com\nTo view the article online, visit: http://www.underwatertimes.com/news.php?article_id=70910231456\nA new study found that the removal of predatory fish such as triggerfish resulted in an overpopulation of sea urchins. © Wildlife Conservation Society\nBRONX, New York -- An 18-year study of Kenya's coral reefs by the Wildlife Conservation Society and the University of California at Santa Cruz has found that over-fished reef systems have more sea urchins—organisms that in turn eat coral algae that build tropical reef systems. By contrast, reef systems closed to fishing have fewer sea urchins—the result of predatory fish keeping urchins under control—and higher coral growth rates and more structure.\nThe paper appears in the December 2010 issue of the scientific journal Ecology. The authors include Jennifer O'Leary of the University of California at Santa Cruz and Tim McClanahan of the Wildlife Conservation Society.\nThe authors found that reefs with large numbers of grazing sea urchins reduced the abundance of crustose coralline algae, a species of algae that produce calcium carbonate. Coralline algae contribute to reef growth, specifically the kind of massive flat reefs that fringe most of the tropical reef systems of the world.\nThe study focused on two areas—one a fishery closure near the coastal city of Mombasa and another site with fished reefs. The researchers found that sea urchins were the dominant grazer in the fished reefs, where the predators of sea urchins—triggerfish and wrasses—were largely absent. The absence of predators caused the sea urchins to proliferate and coralline algae to become rare.\n\"These under-appreciated coralline algae are known to bind and stabilize reef skeletons and sand as well as enhance the recruitment of small corals by providing a place for their larvae to settle,\" said Dr. Tim McClanahan, WCS Senior Conservationist and head of the society's coral reef research and conservation program. \"This study illustrates the cascading effects of predator loss on a reef system and the importance of maintaining fish populations for coral health.\"\nThe study also focused on the effects of herbivorous fish—surgeonfish and parrotfish—on coral reefs. While these 'grazing' fish did measurably impact the growth rates of coralline algae in reef systems, they also removed fleshy algae that compete with coralline algae. Overall, reefs with more sea urchins grew significantly slower than ones with more complete fish communities.\nThe authors also found that the grazing effect was stronger and more persistent than the strong El Niño that devastated coral reefs throughout the tropics in 1998 (the study extended from 1987 until 2005). The study shows that managing coral reef fisheries can affect coral reef growth and improving the management of tropical fisheries can help these reefs to grow and persist in a changing climate.\n\"The survival of coral reefs is critical for hundreds of millions of people who depend on these complex systems for coastal protection, food, and tourism revenue around the globe,\" said Dr. Caleb McClennen, Director of WCS's Marine Program. \"This study demonstrates the importance of improving fisheries management on reefs so that corals can thrive, safeguarding some of the world's most fragile marine biodiversity and strengthening coastal economies.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cb9d915d-17a5-4459-8d29-65b4fbf54c40>","<urn:uuid:6cf565b4-5d43-4eec-82ea-e7af293ae986>"],"error":null}
{"question":"How do modern luxury watchmakers achieve extreme lightness in their timepieces, and what role does resonance technology play in improving accuracy?","answer":"Modern luxury watchmakers achieve extreme lightness through innovative materials and construction. For example, the RM 27-04 uses TitaCarb® (a blend of carbon fiber and polyamide) for its case, and grade 5 titanium for its mainplate and bridges, achieving a total weight of just 30 grams compared to typical mechanical watches weighing around 100 grams. As for resonance technology's role in accuracy, it involves two independent movements with separate balance wheels that synchronize their oscillations. This phenomenon creates a stabilizing effect on timekeeping, conserves energy, and reduces negative effects from external shocks. When one balance is disturbed, the other compensates by speeding up or slowing down accordingly, helping maintain precision through averaged-out errors.","context":["Richard Mille has unveiled its latest ultra-light RAFA timepiece with a steel cable system similar to the strings of a tennis racket — both visually and functionally.\nIntroducing the RM 27-04 Tourbillon Rafael Nadal, designed to withstand an acceleration of 12,000 Gs — a record for a Richard Mille timepiece.\nIn total, the RM 27-04 weighs 30 grams which is not as lightweight as the original Rafael Nadal RM 027 timepiece that weighs 19 grams, and that started the now decade long partnership — but it’s still incredibly lightweight relative to most mechanical watches which typically weigh closer to 100 grams on average.\nTo achieve this remarkable level of lightness and strength, the case — which measures 38.40 mm across and 11.40 mm (47.25 mm lug-to-lug) — has been made from a new material made exclusively for watches called TitaCarb® that blends carbon fiber with polyamide.\nWhile the watch only offers hours, minutes, and a tourbillon — the requisite design goal of being able to resist 12,000 Gs, complicates the manufacturing process significantly.\nAs with past Nadal watches, keeping the movement manually wound was crucial considering an automatic rotor could become damaged by the forces that occur while playing tennis, especially at the pro level.\nImpressively, the movement is entirely supported by a micro-blasted mesh that’s just 855 square millimeters, comprised of a single cable in braided steel measuring 0.27 mm in diameter, and held in place by two tensioners in PVD-treated 5N gold. This construction is unprecedented in the world of watchmaking.\n“Inspired by the same principle as the strings of a tennis racquet, the watchmaker anchors the steel cable to a tensioner positioned at 5 o’clock and then begins to create the mesh, tying each of the main strings before adding the cross strings. Weaving above and below the main strings, the cable passes 38 times through the hollow bezel in grade 5 titanium before finishing in a tensioner positioned at 10 o’clock. The movement is positioned diagonally, connected to the mesh by five grade 5 polished titanium hooks with 5N gold PVD coating that extend from the back of the baseplate,” according to Richard Mille.\nIn addition to the cable system, and composite case, the mainplate and bridges are crafted from grade 5 titanium (90% titanium, 6% aluminum, and 4% vanadium) which helps keep the weight minimal.\nThe movement itself measures 32.75 mm x 28.95 mm and is 5.85 mm thick. The free-sprung balance wheel is 10 mm in diameter and the tourbillon is 12.30 mm. Beating at a frequency of 3Hz, and utilizing 19 jewels, the caliber has a rather low 38-hour power reserve — but it’s a sacrifice likely made in order to focus all efforts on making the movement extremely lightweight and durable.\nThe RM 27-04 Tourbillon will be available in a limited edition of 50 pieces. The caseback is engraved with “RAFAEL NADAL” as well as the edition number XX/50. “RAFA” is engraved on the case flank opposite the crown side. Retail is $1,050,000.\nThe RM 27-04 Tourbillon is on Nadal’s wrist during the French Open (going on now) and will be worn at all future tennis matches until another model is created.\nLearn more at Richard Mille.","Today, we bring you some very exciting news from Armin Strom as they unveil one of the most complicated timepieces out there. We are talking about the new Mirrored Force Resonance Fire that taps into one of the most complicated horological feats and a complication only attempted and mastered by just a few like F.P. Journe and Breguet. Well, today Armin Strom joins the ranks along with these two other brands with the launch of the new Armin Strom Mirrored Force Resonance Fire timepiece that is the second complication from this young manufacture from Biel.\nBased on the physics principle of 'Resonance', this new timepiece from Armin Strom, shows that this manufacture is way more than just skeletonization and that in reality they are a true horological force. A 'Resonance Watch' is one that is equipped with two independent movements with two independent balance wheels, two independent escapements, two independent gear trains and two independent mainsprings. Based on the physics phenomenon where two oscillating bodies in close proximity influence each other and eventually synchronize known as 'resonance', the balance wheels generate a reverberance and the resonance between them allows for fine tuning aiming for maximum horological accuracy, precision and rate stability. Therefore, very precise adjustment of the distance between the two regulators is necessary to incite resonance, which sees the two balances finding a concurrent rhythm in opposite directions so as to continuously average out errors for maximum accuracy.\nOne body in motion relays its vibrations to its surroundings. When another body with a similar natural resonant frequency to the first receives these vibrations, it will absorb energy from the first and start vibrating at the same frequency in a sympathetic manner. The first body acts as the “exciter,” while the second acts as the “resonator.” The phenomenon of synchronized motion in horology has fascinated watchmakers since the time of Christiaan Huygens (1629-1695). Huygens, inventor of the pendulum clock, was the first to discover the resonance of two separate pendulum clocks, which he logically surmised should keep slightly different time. When hung from a common beam, however, the pendulums of the adjacent clocks synchronized; subsequent researchers confirmed that the common wooden beam coupled the vibrations and created resonance. The two pendulums functioned as one in a synchronous manner. In the eighteenth century, Abraham-Louis Breguet demonstrated his mastery of the phenomenon with his double pendulum resonance clock.\nThe advantages of resonance are:\n1. A stabilizing effect on timekeeping.\n2. A conservation of energy.\n3. A reduction of negative effects on timekeeping accuracy due to outside perturbation such as shock to the balance staff, which in turn keeps the rate more stable and increases precision.\nAn outside shock that slows one of the balances down increases the speed of the other one by the same amount; both balances will strive to get back in resonance, thereby averaging and minimizing the effects of the outside influence as they find their rhythm.\nTo provide an idea of how difficult the horological execution of this concept is, an exhaustive list of watchmakers that have successfully used resonance in an extremely limited number of timepieces includes Antide Janvier (1751-1855), Abraham-Louis Breguet (1747-1823), so few modern clock and watchmakers that they can likely be counted on one hand. Unlike the majority of watch companies operating today, Armin Strom is a full-fledged manufacture with in-house manufacturing capability. The resonant Caliber ARF15 composed of 226 parts and 43 jewels is a classically constructed manually wound movement that was conceived, manufactured, assembled, and regulated in-house. It beats at a frequency of 25,200 vph —3.5 Hz— thus allowing the observer to really appreciate the patented, resonant regulators in action. The movement is resolutely modern, with a novel, yet impeccably executed finish providing a roomy and stable stage for the real show: the symmetrical twin display of seconds, which are bound by a single spring.\nThe resonance clutch spring needed to realize the Mirrored Force Resonance’s twin display of seconds was so technical that the brand’s team, under the direction of technical director Claude Greisler, was left with no choice other than to create what it needed in house. Like Calibre ARF15, the resonance clutch spring comprises only a traditional horological material: steel. Greisler and his team spent fully two and a half years perfecting the shape and characteristics of the spring: calculating, optimizing, simulating, testing, and improving again and again until the spring had the optimal, unique form needed to connect Armin Strom’s two sets of oscillators, each comprising twin balance wheels and balance springs.\nSince the two connected oscillators make their revolutions in opposite directions, which is eminently visible on the dial side of the watch, one rotating clockwise and the other counterclockwise, the animated elements look much like they are performing a magic trick. If the 48-hour power reserve has been exhausted and the movement requires a fresh injection of energy through winding, the twin balance wheels need approximately 10 minutes to become synchronous. In case of any outside influence in the form of shock, it takes only a few minutes for the two balances to find their resonant rhythm once again. This is because it is not the balance wheels that Armin Strom’s technical team has connected using the resonance clutch spring, but rather the balance spring studs, which receive the impulses. The case band pusher at 2 o’clock resets the luminous twin seconds’ displays to zero, simultaneously resetting the twin balance wheels. The Mirrored Force Resonance is the most complicated timepiece that Armin Strom manufactures; it is not surprising that a patent has been registered on it.\nEncased in an 18K rose gold case measuring 43.40 mm in diameter and equipped with a brown alligator hornback strap and an additional rubber strap with deployant double-folding clasp, the new Armin Strom Mirrored Force Resonance Fire is available in a limited edition of only 50 pieces. Unveiled today at Salon QP in London, the watch will embark in a world tour afterwards and we will be bringing our usual live pictures in a couple of weeks. Stay tuned!\nSticker Price CHF 67,000 —Approximately $69,000 USD. For more info on Armin Strom click here."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9d010857-3da4-473b-9263-74790d3d5814>","<urn:uuid:db9eae81-685f-4c41-8b42-0452a2eb4ade>"],"error":null}
{"question":"How do the career pathways differ between MCB's Microbiology track and a Biomedical Science degree?","answer":"The MCB Microbiology track primarily prepares students for careers in microbiology, immunology, and host-pathogen research at institutions like research centers and infectious disease facilities. In contrast, a Biomedical Science degree offers broader career opportunities including work in biotechnology, the pharmaceutical industry, medical diagnostic laboratories, secondary and tertiary teaching, media and communications, and government sectors in health promotion and health economics. Additionally, the Biomedical Science degree can serve as a pathway to medicine programs, with some universities reserving specific spots for biomedical science graduates.","context":["To understand diverse microorganisms, their pathogenesis and resultant immune responses, research in MCB probes microbial physiology, the composition and function of microbiomes, innate and adaptive immunity, and translational strategies. This research and work helps us predict infection epidemics, design better therapies and reengineer host and microbe for improved health and industry.\nArea Directors help advise students about classes and rotations in their interest area. They also provide a listing of suggested courses for those interested in Microbiology, Infection & Immunity.\nFaculty Area Directors\n- Michael Lagunoff (email@example.com)\n- Anthony Rongvaux (firstname.lastname@example.org)\nStudent Area Directors\n- Amin Addetia (email@example.com)\n- Tayla Olsen (firstname.lastname@example.org)\nThe suggested curriculum outlined below is meant to guide you in choosing classes, they are not requirements. We highly encourage you to take the Foundational courses, while the Electives are more specialized and often cross between Areas of Interest. Remember to review the UW Time Schedule for the most accurate and up-to-date information regarding whether a course is currently being offered.\n2022-2023 Suggested Curriculum (document download)\nThis track is broadly divided into the related sub-tracks of immunology, virology, and bacteriology. The foundational courses include two courses focused on each sub-track, denoted as 1=Immunology, 2=Virology, and 3=Bacteriology. Interested students can focus on one sub-track or mix and match from these sub-tracks depending on their specific area of research. Area directors or more senior MCB students can discuss these sub-tracks with interested first-year students.\nFoundational Course 1A: IMMUN 532 – Intersection of innate and adaptive immunity in disease (“Advanced Immunology”)\nThis is the primary graduate-level survey of immunology. Many lectures are given by guest lecturers from the Dept. of Immunology who are renowned experts in these topics. Lectures are complemented by discussion and critique of relevant primary literature. Prerequisite: Undergraduate immunology course (e.g. IMMUN 441), or equivalent.\nOffered WIN, 4.0 credits, Offered every year\nFoundational Course 1B: IMMUN 537 – Immunological Methods\nThis course covers key methods required for immunological research. Prerequisite: Undergraduate Immunology course (e.g. IMMUN 441), or equivalent.\nOffered WIN, 4.0 credits, Offered every year\nFoundational Course 2A: MCB 532 – Human Pathogenic Viruses\nReplication, regulation, and pathogenesis of several groups of human viruses, including human immunodeficiency virus and papillomaviruses. Emphasis on the unique aspects of the viral-like cycles as they relate to effects on infected cells and organisms. Guest lecturers focus on viral immunology, measles. herpes simplex virus, and HHV-8.\nOffered AUT, 3.0 credits, Offered odd-numbered years, Will be offered in AUT 2023\nFoundational Course 2B: MICROM 540 – Virology\nThe molecular biology, transmission, and pathogenesis of human viruses will be explored. In addition to general principles of virology, lectures and paper discussions will focus on specific human pathogens including HIV, herpesviruses, ebolaviruses, alphaviruses, and adenoviruses, among others.\nOffered AUT, 3.0 credits, Offered even years\nFoundational Course 3A: CONJ 558 – Prokaryotic Biology\nBasic principles in prokaryotic cell structure, genomics, and metabolism. Introduction to prokaryotic physiology, bacterial pathogenesis, and microbial ecology.\nOffered WIN, 3.0 credits, Course not currently being offered\nFoundational Course 3B: CONJ 558 – Molecular Interactions of Bacteria with their hosts\nThe processes bacteria employ to shape interactions with their hosts will be explored in molecular detail through selected examples in the literature.\nOffered SPR, 3.0 credits, Offered odd-numbered years\nCONJ 539 – Modern Approaches to Vaccines\nCovers selected topics based on recent publications in viral and bacterial vaccine research. Emphasizes understanding the latest advanced and issues in vaccine discovery, mechanisms of action, and special topics in viral vaccines.\nOffered AUT/SPR, 1.5 credits, Offered every year\nCONJ 549 – Population Biology of Microorganisms\nPrinciples of ecology and evolution as they apply to microorganisms.\nOffered SPR, 1.5 credits, Offered even-numbered years\nCONJ 557 – Microbial Evolution\nSelected topics in microbial evolution including evidence for early life on Earth, molecular mechanisms of bacterial and viral evolution, speciation, adaptive niche differentiation, bioinformatics tools to detect selection, and evolution of the virulence and pandemic spread. Prerequisite: MICROM 412 or general biology background.\nOffered AUT/SPR, 1.5 credits, Offered every year, Course not currently being offered\nGLOBAL HEALTH 566 – Biochemistry and Genetics of Pathogens and Their Hosts\nProvides a strong foundation in biochemistry, molecular biology, and genetics for students interested in disease. Principles illustrated through examples focusing on pathogens, and infectious and non-infectious disease. Note: offered jointly with PABIO 551A.\nOffered AUT, 3.0 credits, Offered every year\nIMMUN 441 – Basic Immunology\nThis is an undergraduate class that presents a complete introduction to immunology. MCB students interested in this topic who have not taken a basic immunology course are encouraged to take or audit this course in preparation for more advanced immunology courses. Students must obtain approval from the MCB Co-Directors for this 400-level class to count toward their 18-graded credits.\nOffered AUT, 4.0 credits, Offered every year\nIMMUN 538 – Immune-based diseases and treatments\nThis course focuses on the role of the immune system in both causing and resolving disease. Topics include autoimmune disease, infection, and cancer immunology. Each class includes both a lecture component and a discussion of relevant primary literature.\nOffered SPR, 2.0 credits, Offered every year\nPABIO 552 – Cell Biology of Human Pathogens and Disease\nCell biology and immunology explored through diseases of public health importance. Examples of pathogen interaction with host cell biology and immune systems, unique aspects of the cell biology of pathogens, perturbations of these systems in non-infectious diseases, and design of therapeutics and vaccines to combat diseases of public health importance.\nOffered WIN, 3.0 credits, Offered every year\nMicrobes and the fight to contain them\nSeattle MCB combines leaders in microbiology, immunology, host-pathogen response and autoimmunity from premier institutions and departments including:\n- Fred Hutchinson Cancer Center\n- UW Departments of Microbiology and Immunology, which consistently rank in the top 10 programs for graduate studies according to the US News and World Report.\n- UW Center for Emerging and Reemerging Infectious Disease (CERID)\n- UW Center for Innate Immunity and Immune Diseases (CIID)\n- Benaroya Research Institute is a leader in autoimmune disease research.\n- Seattle Children’s Research Institute Center for Global Infectious Disease Research has intentional renowned scientists in the field of HIV/AIDS, tuberculosis, malaria, and emerging infectious diseases.\n- Institute for Systems Biology (ISB) explores environmental microbes and the host-pathogen interface.\n- Pacific Northwest Research Institute explores the genetics of human health and disease, including research on diverse infectious agents such as yeast, transposons, and transmissible cancers.","Biomedical science combines the fields of biology and medicine in order to focus on the health of humans. A biomedical science degree opens up career opportunities in biotechnology, the pharmaceutical industry, research centres, product development and technology, as well as a variety of postgraduate pathways within the health sector.\nBiomedical science is an interdisciplinary area of study and includes aspects of anatomy and developmental biology, biochemistry, cell biology, clinical medicine, epidemiology, genetics, immunology, microbiology, molecular biology, pharmacology, preventive medicine and physiology. Electives allow you to design a specialised program around any of these areas, or you can choose units from other faculties to broaden your horizon. Whatever your choice, you will gain the skills you need to understand and investigate human biology and make a difference to human health in a wide variety of career paths. You may also progress to a research-based honours year where you could contribute to our world-renowned work such as treating bowel cancer or repairing damaged brains in babies.\nVery high achieving students can apply for the Bachelor of Biomedical Science scholars program. Biomedical Science scholars complete the same academic program as other students but also have access to a range of development opportunities.\nWhether or not you join the scholars program, this course is your invitation to join one of the largest and most successful medical research hubs in Australia and the world. Our multidisciplinary approach, presence in major hospitals and links to international researchers is making a difference to people's lives worldwide. As a graduate, you may find work in the hospital and medical sector, including in medical diagnostic laboratories, in secondary and tertiary teaching, in media and communications, and in the government sector in such areas as health promotion and health economics. You will also have access to our Professional Development Program and may apply for a short term industry placement through our Work Integrated Learning unit, BME2032 Biomedical industry based learning.\nIf you are interested in the medicine program at Monash, undertaking the Bachelor of Biomedical Science will provide the best pathway option with at least 50 places being reserved in the course for Monash biomedical science graduates (as of 2018).\nThe Bachelor of Biomedical Science can be taken in combination with the following courses:\n- Bachelor of Commerce\n- Bachelor of Engineering (Honours)\n- Bachelor of Laws (Honours)\n- Bachelor of Science (the biomedical science and genetics majors are not available within the Bachelor of Science component)\nThis will lead to the award of two degrees - your biomedical science degree and the degree awarded by the partner course. You should refer to the course entry for the partner course in your double degree for the requirements of the other degree.\nThese course outcomes are aligned with the Australian Qualifications Framework level 7 and Monash Graduate AttributesAustralian Qualifications Framework level 7 and Monash Graduate Attributes (http://www.monash.edu.au/pubs/handbooks/alignmentofoutcomes.html).\nUpon successful completion of this course it is expected that you will be able to:\n- demonstrate a broad knowledge in the area of biomedical science spanning the molecular, cellular, organ and body systems levels\n- demonstrate an in depth knowledge in the area of biomedical science\n- demonstrate technical skills relevant to the area of biomedical science\n- develop, apply, integrate and generate biomedical science knowledge in professional contexts to analyse challenges and to develop effective solutions\n- collect, organise, analyse and interpret biomedical science data meaningfully using experimental and computational approaches\n- communicate ideas and results effectively to diverse audiences and in a variety of formats\n- work and learn in both independent and collaborative ways with others to encompass diverse abilities and perspectives\n- exercise personal, professional and social responsibility as a global citizen.\nThe course provides an interdisciplinary approach to the study of biomedical science, with five central themes: molecular and cellular biology, body systems, infection and immunity, disease and society, and diagnostic and research tools. These themes are interwoven in units throughout the course.\nA. Molecular and cellular biology\nThrough these studies you will learn how the cell functions and replicates itself in health and disease, particularly considering the structure of the cell and its evolution, the function of cells, DNA, genes and proteins, and the regulation of metabolism.\nB. Body systems\nThis theme addresses the principles of major body systems. You will learn how cells come together to form tissues and organs and how they work together in the body to provide it with its metabolic needs and remove waste products. You will study how structure follows function; homeostasis; the nutritional and gastro-intestinal system; the neural system and senses; endocrine, reproductive and renal systems; and cardiovascular and respiratory systems.\nC. Infection and immunity\nThe focus of these studies is the functional immune system of multicellular organisms and the disease states that result from pathogen infection and from autoimmunity. You will learn about molecular genetics and recombinant DNA (both important tools for the study of microbial disease and immunity), inflammation and disease, infection and infection control.\nD. Disease and society\nIn these studies you will learn about disease states that result from abnormal function in various body systems, including the cellular, genetic and molecular causes of the disease, with a focus on mechanisms of disease and patterns of disease and treatment. In studying the basis for human disease, you will also consider the societal and personal impacts of past, present and future diseases and the social, economic and environmental factors that are determinants of health.\nE. Diagnostic and research tools\nThese studies address both the molecular and cellular tools, including specialist imaging techniques, that can be used to study and diagnose diseases.\nF. Free elective study\nThis will enable you to broaden and deepen your knowledge of aspects of biomedical science, or to select units from across the University in which you are eligible to enrol.\nThe course comprises 144 points, of which 96 points are from biomedical science study and 48 points are available to provide additional depth or breadth through elective study. The course develops through theme studies in biomedical science covering: Part A. Molecular and cellular biology, Part B. Body systems, Part C. Infection and immunity, Part D. Disease and society, and Part E. Diagnostic and research tools. These themes are interwoven in units throughout the course.\nTo remain in the scholars program, you must maintain at least a distinction average (70%) across 48 points in each calendar year.\nThe course progression mapcourse progression map (http://www.monash.edu.au/pubs/2019handbooks/maps/map-m2003.pdf) provides guidance on unit enrolment for each semester of study.\nUnits are 6 credit points unless otherwise stated.\nA. Biomedical science studies (96 points)\nYou must complete\n- BMS1011 Biomedical chemistry\n- BMS1021 Cells, tissues and organisms\n- BMS1031 Medical biophysics\n- BMS1042 Public health and preventive medicine\n- BMS1052 Human neurobiology\n- BMS1062 Molecular biology\n- BMS2011 Structure of the human body: An evolutionary and functional perspective\n- BMS2021 Human molecular cell biology\n- BMS2031 Body systems\n- BMS2042 Human genetics\n- BMS2052 Microbes in health and disease\n- BMS2062 Introduction to bioinformatics\n- BMS3031 Molecular mechanisms of disease (12 points)\n- BMS3052 Biomedical basis and epidemiology of human disease (12 points)\nB. Free elective study (48 points)\nWithin the Bachelor of Biomedical Science students must complete 48 credit points (8 units) of free electives. Elective units can be chosen from across the Faculty and used to deepen your knowledge of the biomedical sciences. This includes those with the prefixes BCH, BME, DEV, GEN, HUP, IMM, MIC, MIS, NUT, PHA and PHY. Refer to the index of units by codeindex of units by code (http://www.monash.edu.au/pubs/handbooks/units/index-bycode.html) in the current edition of the Handbook.\nElective units may also be used to sample disciplines from across the University provided you have met the required prerequisites and there are no restrictions on enrolment into the units. The faculties of Arts, Business and Economics, Engineering, Information Technology and Science offer units particularly suitable as electives. MajorsMajors (http://www.monash.edu.au/pubs/handbooks/aos/index-bydomain_type-major.html) and minorsminors (http://www.monash.edu.au/pubs/handbooks/aos/index-bydomain_type-minor.html) are indexed in the Handbook.\nFree electives can also be identified using the browse unitsbrowse units (http://www.monash.edu.au/pubs/handbooks/units/search) tool and indexes of unitsindexes of units (http://www.monash.edu.au/pubs/handbooks/units/) in the current edition of the Handbook.\nMajorsMajors (http://www.monash.edu.au/pubs/handbooks/aos/index-bydomain_type-major.html) and minorsminors (http://www.monash.edu.au/pubs/handbooks/aos/index-bydomain_type-minor.html) are indexed in the Handbook.\nThe level of the unit is indicated by the first number in the unit code; undergraduate units are those that commence with the numbers 1-3. You may need permission from the owning faculty to enrol in some units taught by other faculties.\nElectives may be from any level, however, no more than four electives (24 points) are to be completed from level 1.\nIf you are in a double degree course, some units required for the partner degree are credited as electives towards the biomedical science degree.\nMajors and Minors\nYour free electives may be used to study a major (48 points) or minor (24 points) in a non-biomedical science discipline. This is an optional component of the degree. You cannot do a major or minor in the areas of Anatomy and Developmental Biology, Biochemistry, Genetics, Human Pathology, Immunology, Microbiology, Pharmacology and Physiology, but you may do individual units from these areas.\nIf you are in the scholars program you are required to include at least one of the following units in your choice of electives:\n- BCH3990 Action in biochemistry research project\n- BMS3990 Action in biomedical science minor research project\n- DEV3990 Action in developmental biology research project\n- HUP3990 Human pathology in action research project\n- IMM3990 Immunology in action research project\n- MIC3990 Action in microbiology research project\n- PHA3990 Action in pharmacology research project\n- PHY3990 Action in physiology research project\nProgression to further studies\nSuccessful completion of the Bachelor of Biomedical Science may provide a pathway to the one-year honours degree Bachelor of Biomedical Science (Honours), or the Bachelor of Medical Science and Doctor of Medicine course at Monash."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:50708a70-75fc-41e6-9a54-99b964aa50f6>","<urn:uuid:c11e736b-ff68-4f43-b850-cd7da504d9f9>"],"error":null}
{"question":"What does modern technology reveal about the Jericho Skull's social status and burial practices, and how does this compare to other historical burial customs involving human remains?","answer":"Modern 3D printing technology has revealed that the Jericho Skull, dating from 8200-7500 BC, showed evidence of head binding during growth, which was believed to reflect social status. The skull's broad forehead suggests it was male, and the ridges found on the inside of the 3D printed cross-section model indicate deliberate cranial modification. In comparison, historical burial practices, particularly during the Viking Age (800-1030 AD), also reflected social status through human remains. In Viking burials, the treatment of bodies, such as decapitation and positioning, along with dietary analysis through isotopes, indicated social hierarchies. Some graves contained both complete and partial remains, with evidence suggesting slaves were buried with their masters as grave goods, though the relationship between social status and diet was complex, as slaves and lower-status individuals showed similar dietary patterns based on marine resources.","context":["The Jericho Skull, excavated from the Palestinian Territories of Jerusalem, dates back to the final phase of the Stone Age between 8200 and 7500 BC. It is part of The British Museum’s collection in London, UK, and has been 3D printed to look as it would have done in life, all those thousands of years ago.\nThinkSee3D Ltd. is a 3D cultural heritage service provider, based in Oxford, UK, responsible for the 3D printed Jericho models. Speaking to Steven Dey, the MD and founder of ThinkSee3D, 3D Printing Industry discover the challenges faced in recreating the Jericho Skull, and the nuances of 3D printing that have revealed new scientific theories about the living skull’s conditions.\nCreated using 3D Systems industrial 3D printers\nThere are three 3D printed stages of the Jericho on display alongside the original – which has been filled with cement to stop the cranium from caving in. All three models were 3D printed in gypsum using a 3D Systems industrial 3D printer – potentially the ProJet CJP 660Pro that has case studies in the same material for 3D printing dinosaur bones.\nThe first 3D print of the Jericho skulls shows a partial reconstruction, missing a jaw, and part of the cranial cavity that was left out to get a good microCT scan image.\nIn this model, historians notice a broad forehead suggesting that the gender of the skull is male.\nDey explains how working with the CT scan files was the first challenge encountered in 3D printing Jericho;\nWe started the project with a very large digital model (>5Gb file size) which came out of the microCT scan work the museum had commissioned […] being so big, it had a lot of internal geometry and was very high resolution (beyond the resolution of the 3d printer and probably the human eye).\nLuckily, ThinkSee3D are used to working with such files throughout their projects, and were able to manipulate the images into a 3D model in just a few days.\nA happy accident\nThe second 3D print shows a cross-section of the Jericho Skull.\nRidges on the inside of this model, not caused by FFF layering, suggest new evidence that the skull had been bound during growth.\nBinding was a typical custom for the time, with supposed links to reflection of social status. It results in a more conical shape to the head when in adulthood, demonstrated more clearly in the final 3D model.\nFace to face with the past\nThis final reconstruction was created by RN-DS Partnership who specialize in facial reconstruction for archeological evidence of this kind. RN-DS Partnership used some of the data gathered by ThinkSee3D to map a historically-accurate figure of how the Jericho male may have looked. It is finished in Jesmonite – a composite material also based in gypsum, with added resin.\nIt’s astounding how 3D printed recreations of the Jericho Skull has had the ability to shed light on a thousand-year-old mystery. Dey optimistically explains how this is often a result of their projects;\nOne of our medical clients discovered some subtle structural differences in the 3D printed brains of 2 babies of identical age where one had gone full term and one had been born prematurely (this had not been noticed from the on-screen 3d models or the MRI data).\nThroughout 2017, ThinkSee3D have many other upcoming natural history projects, including 3D imaging of Jurassic reptiles, Egyptian artifacts, and CT scans of Viking skeleton.\nIf you enjoy stories on the application of 3D printing in natural and cultural history projects such as this one, remember to stay up-to-date by signing up to our newsletter here.\nApart from where cited, all photos featured in this article were taken by Beau Jackson at The British Museum’s ‘Creating an ancestor: the Jericho Skull’ exhibit for 3D Printing Industry.\nFeatured image shows the first and second 3D printed reconstructions of the Jericho skull. Photo by Beau Jackson for 3D Printing Industry.","Grave goods and burial gifts consist of any item given to the dead at burial or taken by the deceased into their grave. It may be an offering to the gods, an item for the next life, or a personal item of the deceased. We know that humans have been practicing intentional burial with placement of grave goods for the past 100,000 years. What one decides to take into the grave or what one is given, is usually determined by the conception of the afterlife and what would be necessary. The best and most renowned evidence for grave goods comes to us from Egypt. The ancient Egyptians provisioned the dead with a multitude of items for the afterlife because they believed that once they died they would need food, personal items and even workers to help them in the afterlife. Wealthier individuals had more grave goods than the poor, although this also meant that they could be targeted for grave-robbing. From King Tut’s tomb, they recovered seven hundred items including four chariots, a collapsible sunshade, a number of senet boards, four ritual couches, a gilded wooden half-length bust of Tut, two life-size wooden figures that flanked the north wall depicting Tut, clothing, baskets of food, and more. Grave goods are an important source of evidence for learning more about the beliefs and social structure of a culture.\nDuring the Viking Age (A.D. 800-1030), there is a high amount of variation in burial customs and the types of grave goods found. In a number of cases, individuals are buried together in the same grave, allowing archaeologists a unique opportunity to discuss social relationships between the individuals. Multiple burial was actually quite frequent during this period, and was a deliberate choice, not simply an expedient one. It is possible that the placement of individuals within the same grave could mean they were biologically related, a common burial type is a mother and child if both died at the same time, or one individual could be the primary deceased and secondary individuals could be part of offerings.\nOn the island of Flakstad in northern Norway, a burial ground was excavated. They recovered ten sets of human remains including three single burials, two double burials and one triple burial. All date to the Viking period. The burials had unusual features when compared with standard burial forms. In the double and triple burials, only one individual per each grave was complete and the others consisted of only the post-cranial remains. This burial form has been seen in other Norse areas, and is usually interpreted as indicating that slaves were buried with their masters. The interpretation of these secondary individuals as slaves is due primarily to the presence of maltreatment and trauma, including evidence of injury and stress during their life, decapitation, binding of hands and feet within the grave, and uneven distribution of grave gifts. The burials at Flakstad appear to fit this pattern of being slaves and grave goods. However, Naumann et al. (2013) argue that the contextual information for this site is limited, and a more in-depth analysis of the social relations is needed. The propose to do this using stable isotope and ancient DNA (aDNA) to determine the dietary and genetic patterns between all individuals.\nBased on the DNA analysis, they argue that there was no maternal genetic connection between the individuals buried within the multiple graves. Based on the stable isotope analysis, they found there were two distinct groups eating different types of food. The first group, consisting of single burials and individuals without crania, ate more marine resources and is thought to be of a lower class, and the second group, consisting of individuals with crania in multiple burials, ate primarily terrestrial foods suggesting a higher class. They argue therefore that those individuals who were not decapitated but were buried with decapitated individuals perhaps had a higher status. The lack of special grave goods within these ‘higher status’ multiple burials could mean that they weren’t necessarily wealthier, but had a special status within the group, or that the graves were plundered due to the high presence of better grave goods.\nWhat they found most interesting was that the slaves had the same isotopic ratios as the single burials, suggesting that both slaves and the lower status population ate the same food. It was hypothesized that the slave diet would be the lowest, followed by low status, then high status- however this is not the case. It is this that they argue needs to be explored further. However, I’m still thinking about the role of slaves as grave goods! There are some important facts here that need to be discussed- were the slaves killed to be buried with their master or were they already dead and reburied with this individual? How does gender and age figure into the grave goods? Most importantly, can you even argue that these individuals were considered grave goods? Determining that an individual is considered an item by the dominating culture is not something we can conclude lightly. If anyone happens to have any answers for these questions, send the evidence my way!\nElise Naumann, Maja Krzewińska, Anders Götherström, & Gunilla Eriksson (2013). Slaves as burial gifts in Viking Age Norway? Evidence from stable isotope and ancient DNA analyses Journal of Archaeological Science DOI: 10.1016/j.jas.2013.08.022"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:7b5108bb-bce8-4c7e-89d7-7de91ff3ae05>","<urn:uuid:8e8118e8-e1ad-42ba-8f3a-ea61d65da76d>"],"error":null}
{"question":"What maintenance aspects do brake fluid systems and derailleur hangers share in terms of determining when replacement versus repair is appropriate?","answer":"Both systems have specific guidelines for determining repair versus replacement. For derailleur hangers, they can often be repeatedly re-bent if they survive the initial repair, though some types like extremely thick or titanium hangers may be impossible to repair and require replacement. Similarly, with brake fluid systems, while the fluid itself needs regular replacement every few years, components like seals, brake lines, and hoses may either need repair or full replacement depending on their condition. The master cylinder might need replacement if internal leaking occurs, while brake pads and rotors typically require replacement when worn beyond serviceable limits. In both systems, regular inspection is crucial to determine whether repair or replacement is needed, and both can show signs of wear through performance issues - poor shifting for derailleur hangers and reduced braking efficiency for brake systems.","context":["Rear Derailleur Hanger Alignment\nThis article will discuss derailleur hanger alignment and use of the DAG-2.2 and DAG-2 Derailleur Alignment Gauge.\nA misaligned derailleur hanger will often result in poor shifting performance. A common cause of a misaligned hanger is from the bike falling over to the right side. This pushes the derailleur body inward, bending the hanger.\nIt is often possible to repeatedly re-bend many derailleur hangers. This is because there is very little stress from riding the bike or shifting gears. As a rule of thumb, if a hanger survives a repair by bending, it will survive use. However, there are some hangers that do not repair well. Extremely thick hangers and titanium hangers are difficult and sometimes impossible to repair. Bolt-on type hangers that are replaceable are alignable. However, these types of hangers can be tricky to align depending upon the frame hanger-to-dropout mounting design. Newly installed replaceable hangers should always be checked for correct alignment.\nAfter aligning and correcting the derailleur hanger, it will be necessary to check all rear derailleur adjustment, including limit screw and index settings. For rear derailleur adjustments see Rear Derailleur Adjustment.\nDerailleur Hanger Alignment\nThe DAG-2.2 (as well as newer DAG-2 models) comes with 2 O-rings for the slider. These will help prevent the slider from falling out and can be used as markers when racks or fenders prevent full DAG-2.2 rotation. This procedure will be described at the end of this article.\nBegin by mounting the bike in a repair stand with the wheels level as the bike would appear on flat ground. Check that the rear wheel is mounted straight in the frame. The wheel does not need to be dished or true for use of the tool. Remove rear derailleur. Install DAG-2.2 and tighten handle. NOTE: Do not use the DAG-2.2 threads as a “chaser” of bad derailleur hanger threads. Chase and clean the threads using the TAP-10.\nRotate the arm toward the left side of rim, at the “9:00 o’clock” position. Rotate the tire valve to the 9:00 position. Use this point on the rim as a constant reference when checking the hanger. By checking the same point on the rim, wheel trueness or dish will not affect alignment.\nLoosen the sliding gauger knob and move the sliding gauge to contact rim and secure knob.\nSlide gauge bracket toward hub before rotating arm. This prevents gauge from begin forced against rim.\nRotate DAG-2.2 and rotate rim valve 180 degrees to the 3:00 position. Slide indicator toward rim to same point near valve.\nThere are 3 possible results:\nResult A: The gauge is barely touching the rim. In this case the hanger is aligned horizontally.\nResult B: The pointer is away from the rim some distance. The hanger is misaligned.\nResult C: The pointer strikes inside the rim, indicating a misaligned hanger.\nIt is easier to determine the error by seeing the gap between rim and gauge. In result C, reset tool at the 9:00 position and rotate back to the 3:00 position. There will be a gap between rim and gauge.\nWhen bending hanger, it is best to bend small in amounts and recheck. The amount of error is actually one-half the gap between gauge and rim. As the gap is closed, it increases at the reference point 180 degrees away. Bend a bit, recheck both side, and then re-bend a bit more. Generally, it is best by having the DAG-2.2 arm next to the chainstay. This allows you to use the stay for leverage and control the amount of bending either inward or out. Repeat bending and checking until the gap is less then 4mm. Use a 4mm hex wrench as a “go/no-go” gauge.\nAfter getting the horizontal aligned, check and aligned, move on to check the 6:00 and 12:00 position. Set gauge to 6:00 position, then check at 12:00 position.\nAgain, bend only one-half the amount of gap. Reset pointer at each bending of hanger. When gap is less then 4mm, keep same the setting and check 3:00 position. If three points 90 degrees apart are within 4mm, hanger is aligned. Continue aligning as necessary.\nRack, Fender, or Other Interference\nThere may be fenders, racks or other situations where it the gauge is blocked from rotating to the various points. DAG-2.2 and newer DAG-2 models use two O-rings to hold the gauge in place. This helps prevent loosing the gauge. Additionally, the O-rings can be used as markers, allowing the gauge position to be accurately referenced, the gauge moved to pass the obstacle, and then the gauge returned to its original position. Below is an example of a bike with a rack the prevents rotation.\nPush outer O-ring to slider. This marks the position of the gauge relative to the slider.\nWhen the gauge cannot pass by a frame stay, fender stay, or rack, loosen gauge knob and pull gauge back to clear.\nReturn the gauge so the O-ring is back in the original position. In the image below, the gauge is contacting the rim, but the O-ring is positioned away from the slider. In this situation, the gauge would be resting inside the rim. There will be situations where the O-ring contacts the slider, but there will be a gap between rim and gauge end.","Complete Guide to Disc Brakes and Drum Brakes\nWhen it comes to driving safety, your brakes are critical. We’ve compiled a guide to help you understand your brakes. This includes disc and drum brakes, how they work, how they’re different and alike, why you may have both types on the same vehicle, what kind of wear to expect, and what parts will need maintenance.\nWhat you’ll find in this guide:\nWhat Is a Simple Brake System\nMaster Cylinder: Contains a piston assembly and brake fluid.\nBrake Fluid: Transfers the hydraulic pressure.\nDisc Brake Assembly: Includes caliper, pads, and rotor.\nBrake Lines and Hoses: Carries the brake fluid to the brake assemblies.\nDrum Brake Assembly: Includes shoes, wheel cylinder and drum.\nBraking System Basics\nDisc and drum brakes are based on a hydraulic pressure system. Braking starts with a mechanical force — your foot pressing the brake pedal. The end result is that your vehicle safely stops. Below is a quick explanation of the five points of the system.\nA piston compresses brake fluid inside the master cylinder located under your vehicle’s hood near your engine. This creates hydraulic pressure, generating a much bigger force than that of the small effort of pressing down on the pedal. See above image.\nThe pressure is transferred via the brake fluid through the brake lines and/or brake hoses (flexible tubes) that connect the lines with brake assemblies at each wheel.\nThere, wheel cylinders convert that hydraulic pressure back to mechanical force. Brake friction material is pushed against the brake disc or drum, slowing or stopping your vehicle.\nThe disc brake assembly is designed to safely stop your car when activated. More details about how this works below.\nThe drum brake assembly, equipped on some vehicles, is also designed to stop your vehicle when other points on the system are activated. More details below.\nBasics of Disc Brakes\nDisc brakes are found on most vehicles today. They are mounted on the front and/or rear axle. To stop a wheel (and your car), a disc brake uses a caliper fitted with brake pads to grab a spinning disc or rotor.\nWhat is a caliper?\nThe caliper is an assembly mounted to the vehicle with a bracket. It frames the rotor, looking and functions like a c-clamp. It contains the following components:\nBrake pads are a friction material bonded to a metal plate that helps stop your vehicle.\nOne or multiple pistons to push the brake pads against the rotor when you brake.\nA bleeder screw to allow for servicing the brakes and replacing the fluid.\nA rubber piston seal that prevents brake fluid leakage and retracts the piston when the brakes are released.\nA dust boot to keep contaminants out of the cylinder.\nAnti-rattle clips that add stability.\nWhat is a rotor?\nThe rotor is made of cast iron or a steel/cast iron composite. It’s attached to the wheel hub and turns with the wheel. This is the surface that the brake pads make contact with to slow your vehicle. Here’s how it works: when you step on the brakes, pressurized brake fluid pushes against the pistons inside the caliper, forcing the brake pads against the rotor. As the brake pads press against both sides of the disc, the friction stops the wheel’s rotation.\nRotors can either be solid or vented. Vented rotors have more surface area and can more easily dissipate heat.\nTwo Types of Disc Brakes: Floating & Fixed\nThere are two types of disc brakes. They are floating and fixed, named for the type of brake caliper used.\nA floating caliper (also called sliding) is the most common type. It has one or two pistons. When the brakes are applied, the inner brake pad is forced against the disc while, at the same time, the caliper body moves closer to the rotor. This action forces the outer brake pad against the rotor.\nThe fixed caliper design has one or more pistons mounted on each side of the rotor. The caliper itself doesn’t budge or move. It’s rigidly fastened to a brake caliper bracket or spindle. When the brakes are applied, only the caliper pistons move, pressing the brake pads against the disc.\nBasics of Drum Brakes\nDrum brakes are most often used on the rear axle of today’s vehicles. They are also very common on trailers. Manufacturers don’t use brake pads as the friction material in drum brakes. Instead, a drum brake system has a wheel cylinder with pistons that push brake shoes out against the inside of a spinning drum. This contact slows and stops the rotation of the brake drum and the wheel, bringing your vehicle to a stop.\nDrum vs. Disc Brakes\nMany of today’s vehicles have disc brakes on all four wheels. However, some models still have disc brakes on the front axle and drum brakes on the rear.\nWhy are discs put on the front? It’s due to weight factors. A typical, unloaded vehicle is about 10 percent heavier in front due to the engine. When you hit the brakes, the weight of the car transfers to the front. Therefore, more braking power is needed in the front, making disc brakes the perfect option for the job.\nDisc vs. Drum Brakes: Servicing\nServicing disc and drum brakes is important. When it comes to repairs, drum brakes have more hardware and can be more complex to service. But the materials typically cost less than disc brake pads, calipers, and rotors. Disc brakes generally consist of fewer components and are easier to service.\nBrake Maintenance: What Needs to Be Checked\nA lot of heat is created when you apply the brakes. When you press on the brake pedal, you’re converting the kinetic (moving) energy of the vehicle into thermal energy (heat), subjecting many parts to very high temperatures. This means a lot of wear and tear even in normal conditions. Some brake components will need to be replaced over the life of a vehicle, including:\nBrake Pad or Brake Shoe Friction Material: Disc brake pads slow the rotor through friction and wear with normal use. Eventually, they become too thin to function properly. The same applies to drum brakes. The friction material gets worn out and braking can become compromised. These components should be inspected regularly to avoid wearing them to the point that they cause damage to additional parts of your vehicle (rotor or drum).\nBrake Fluid: The brake system should be checked regularly for leaks and fluid should be replaced every few years (usually when the brakes are serviced). Any leak in the master cylinder, the brake fluid reservoir, the wheel cylinders, lines or hoses will reduce the hydraulic pressure that’s created when the brakes are activated. If the system can’t generate sufficient force needed to create braking power, you’ll notice your brake pedal needs to be pressed a lot further in order to slow or stop.\nHow often should I replace my brake fluid?: The brake fluid should be replaced every few years. This is usually done when brakes are serviced. While brake fluid is specifically formulated to prevent corrosion of the brake hydraulic components, time, heat, and moisture can lower the boiling point of the fluid. This results in less pressure in the hydraulic system forcing you to push down farther on the brake pedal to slow your vehicle. Other impurities, such as road grime, brake dust, rust, and more can get into the brake fluid, reducing braking efficiency and possibly damaging parts of the braking system.\nSeals: These rubber rings keep the hydraulic fluid from leaking and protect it from moisture and contaminants. They also cause the piston to return to its off position so the brake pads disengage properly when you release the brake pedal. If this doesn’t happen, you could experience brake drag, premature wear, and the vehicle may pull to one side when you brake.\nBrake Lines/Hoses: Brake lines and hoses are an essential part of the braking system. Rubber hoses can wear over time, while brake lines can be subject to corrosion or other damage. When this happens, it may compromise brake efficiency, loss of hydraulic pressure, pulling to one side, etc. Routine inspections, including hoses and lines, are critical to avoiding these issues.\nRotors: Rotors should be inspected regularly for even wear, physical damage, or cracking from excessive heat. These conditions can compromise braking power and efficiency. Brake rotors should be resurfaced or replaced when you have your brakes serviced (brake job or pads replaced). If you are experiencing any of the previously mentioned issues, your rotors may need to be repaired or replaced sooner.\nDust Boots: Brake components are constantly exposed to road debris and brake dust. The dust boot prevents grime from entering the caliper piston. If it fails and can’t do its job, piston damage can occur, causing brake drag, pulls and premature wear.\nMaster Cylinder: Regular fluid maintenance is important for prolonging cylinder life. Failing master cylinders can leak internally. In this case, you may get a low or soft pedal feel. You might not notice any visible fluid loss.\nNOTE: There are different approaches to brake service. See our Brake Servicing 101 and learn why it’s important to maintain more than just the brake pads or drum brake shoes.\nHow Often Do Brakes Need to Be Serviced?\nThere’s no set timeframe. The need to replace or service your brakes depends on your vehicle, driving style, climate and road conditions. To ensure proper braking when you need it, get your brakes checked regularly.\nStill have questions about your brakes? Review these frequently asked brake service questions.\nDisc and drum brakes are built differently, with somewhat different advantages. Both work as part of the hydraulic brake system. This system is constantly under high pressure, subject to heat, and can be compromised by road grime, air, brake dust and moisture. It’s important to get regular brake inspections to keep everything in proper working condition. Refer to your owner’s manual for a recommended schedule. Remember that odd brake sounds, smells or performance are indicators you need to get your vehicle to Les Schwab right away."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9ffd1ef0-038a-4d73-916e-b5b11653ad31>","<urn:uuid:88e9c02a-59ac-4cb0-a6f7-64420ed04c98>"],"error":null}
{"question":"What is the relationship between solvent extraction methods for lipids and dark roast coffee's acrylamide reduction process?","answer":"While both processes involve heat treatment, they work differently. Solvent extraction of lipids can be enhanced by using higher temperatures and pressure than normal to increase efficiency. In contrast, for coffee beans, the extended heat exposure during dark roasting actually leads to lower acrylamide levels compared to light roasting, as acrylamide forms at the beginning of the roasting cycle but declines steeply toward the end of the process due to higher rates of elimination. This shows how heat treatment can have different effects depending on the substance being processed.","context":["Continuous Solvent Extraction The Goldfish method is similar to the Soxhlet method except that the extraction chamber is designed so that the solvent just trickles through the sample rather than building up around it. In: 'Advances in Lipid Methodology - Two' , pp. Chromatographic analysis of ether-linked glycerolipids, including platelet-activating factor and related cell mediators. Lipids are an extremely diverse group of compounds consisting of tri-, di- and monoacylglycercols, free fatty acids, phospholipids, sterols, caretonoids and vitamins A and D. In:: 'Contemporary Lipid Analysis, 2nd Symposium Proceedings', pp. Cast, Sheffield Academic Press 1998. Lipids also were used as molecules to probe the basic mechanisms of ion fragmentation following electron ionization.\nA Practical Approach' edited by R. In: Applications of Modern Mass Spectrometry in Plant Science Research Proc. Kramer 16 Investigation of Protein-Lipid Interactions by Vibrational Spectroscopy, E. A simple test to determine the ability of lipids to withstand cold temperatures without forming crystals, is to ascertain whether or not a sample goes cloudy when stored for 5 hours at 0oC. Rheology is the science concerned with the deformation and flow of matter. Accelerated Solvent Extraction The efficiency of solvent extraction can be increased by carrying it out at a higher temperature and pressure than are normally used.\n. There have been several approaches to deal with complex mixtures of molecular species. In: 'Advances in Lipid Methodology - One' edited by W. Solvent Extraction The fact that lipids are soluble in organic solvents, but insoluble in water, provides the food analyst with a convenient method of separating the lipid components in foods from water soluble components, such as proteins, carbohydrates and minerals. Another curious feature is that the phospholipids and glycerolipids are observed as alkali metal adducts Na + and K + as well as the protonated species.\nIn: Analyses of Fats, Oils and Lipoproteins, pp. Normally, oxidation can take a long time to occur, e. In practice, the efficiency of solvent extraction depends on the polarity of the lipids present compared to the polarity of the solvent. This minireview will be reprinted in the 2011 Minireview Compendium, which will be available in January, 2012. The absence of reference standards for the different variants of structure observed in a molecular species series, e.\nResults given are the mean ± S. Many of these compounds are suspected xenoestrogens. Confirmation of conjugated linoleic acid isomers by capillary gas chromatography-Fourier-transform infrared spectroscopy. Nevertheless, over-consumption of certain lipid components can be detrimental to our health, e. Composition, Structure and Function', pp.\nThe selection of appropriate methods of ionization for analysis of various substances is also considered. B Injection in open-tubular supercritical fluid chromatography is usually accomplished using a dynamic-flow-splitting or time-splitting technique. The analysis of these lipids has always been a challenge because of the low quantities made within tissues or cells. Methods of Analyzing Lipid Oxidation in Foods 5. Perkins, American Oil Chemists' Society, Champaign, U. There have been several approaches to deal with complex mixtures of molecular species.\nChristie, Oily Press, Dundee 1996. Author by : Magdi M. Methods of determination in lipids. A sample is mixed with a combination of surfactants in a Babcock bottle. In: Analyses of Fats, Oils and Lipoproteins, pp.\nAnalysis of tocopherols by gas-liquid and high-performance liquid chromatography: a comparative study. Nevertheless, given the problems with accuracy, these measurements remain precise and useful when comparing changes of the same lipid molecular species within an experimental series. Rapid scanning tandem mass spectrometers are capable of quantitative analysis of hundreds of targeted lipids at high sensitivity in a single on-line chromatographic separation. The flask is heated and the solvent evaporates and moves up into the condenser where it is converted into a liquid that trickles into the extraction chamber containing the sample. Remarkable images are appearing as to the regional distribution of lipids in tissues such as brain and kidney as well as large biological structures such as embryos and entire organisms such a mouse.\nIt provides a broad overview of the applications of ionic liquids in various areas of analytical chemistry, including separation science, spectroscopy, mass spectrometry, and sensors. Advances in planar chromatography for the separation of food lipids. Perkins, American Oil Chemists' Society, Champaign, U. Microwaves: their potential applications in lipid chemistry. Size exclusion chromatography applied to the analysis of lipoproteins. The lipids are extracted from the food sample and then dissolved in an ethanol solution containing an indicator. This figure has been reprinted with permission from Elsevier.","In moderation, coffee seems pretty harmless; but there’s something bigger to worry about in your cup of joe than the wrong type of milk: acrylamide.\nWhy does this matter?\nAccording to the National Cancer Institute, “Acrylamide is a chemical used primarily as a building block in making polyacrylamide and acrylamide copolymers [which] are used in many industrial processes, such as the production of paper, dyes, and plastics.”\nFood and cigarette smoke are the main sources of acrylamide exposure, which The National Toxicology Program and the International Agency for Research on Cancer consider a “probable human carcinogen” based on studies in lab animals.\n“Acrylamide is formed when carbohydrate-containing substances are roasted or otherwise heated to high temperatures,” HealWithFood.org explains. Therefore coffee, due to its scalding preparation demands, becomes a major player in the dialogue surrounding the worrisome chemical. Luckily, the site offers multiple ways you can reduce your acrylamide intake. Among them are:\nTake your time to brew your java\n“If you want to reduce the amount of carcinogenic substances in your daily cup of coffee, brewing your own java may really pay off. Although instant coffee may seem attractive as it can save you time and effort, it has been reporter to contain more acrylamide than the brewed version.”\nBe aware of differences between brands\n“There also seems to be significant differences between brands when it comes to acrylamide levels in coffee. Based on data provided by the FDA, Folgers and Taster’s Choice had the highest levels on average–both in their instant and non-instant products. Yuban Coffee (a brand of Kraft Foods) stood out by having an exceptionally low acrylamide content in this analysis. It is worth noting, however, that the FDA only analyzed a limited number of samples and that there could be significant lot-to-lot variation.”\nDrink dark roast\n“According to an analysis based on the FDA data, dark-roasted beans seem to contain lower levels of acrylamide. This (perhaps surprising) observation is confirmed by a joint-study conducted by the European Commission and Nestlé Product Technology. The researchers responsible for the study found that light-roasted coffee beans tend to contain relatively higher amounts of acrylamide than dark-roasted beans. Why? According to research, acrylamide is formed at the beginning of the coffee bean roasting cycle and it declines steeply toward the end of the process due to higher rates of elimination.”\nCurious about how acrylamide levels are measured in coffee? It’s as easy as chromatography.\nEmploying a method utilizing Simplified Liquid Extraction (SLE) and HPLC, scientists can quantify the levels of acrylamide—holding coffee roasters accountable for reducing the risk of harmful toxins in every batch, while demonstrating the processes they use to improve our java’s quality. View the complete Acrylamide Extraction and Quantification Method here.\nHow does your coffee rate? Check this chart to find out:\n• Chlorogenic Acids from Green Coffee by HPLC-UV\n• iMethod™ Food – Quantification of acrylamide in food by LC/MS/MS\n• EPA Method 8316 – acrylamide, acrylonitrile, and acrolein\n• Green coffee on Kinetex 2.6u EVO C18 100×2.1mm @ 0.4mL/min\n• Caffeine in ground coffee beans on Kinetex 2.6µm C18 100 x 4.6mm ID\n• Chlorogenic acid in brewed black coffee using Kinetex 5u C18 100×4.6mm"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1708ec4d-9003-4973-bc44-89dbb825c03a>","<urn:uuid:fb941648-4d17-433e-97bb-a1d192298633>"],"error":null}
{"question":"How do the water protection policies compare between the Pecos Wilderness area and KVR camping grounds?","answer":"Both areas have strict water protection policies but implement them differently. The Pecos Wilderness focuses on protecting water quality for traditional irrigation systems (acequias) and maintaining uncontaminated, freely flowing water systems that local communities depend on for irrigation and crops. The KVR implements specific rules for campers to protect water sources, requiring them to maintain at least 200 steps distance from any water source when disposing of human waste, and prohibiting waste from entering water. Both areas emphasize preservation - the Pecos through formal watershed protection for community use, and the KVR through strict visitor guidelines and moving campsites out of flood plains to protect both water resources and camper safety.","context":["Preserving New Mexico Traditions in the Pecos Wilderness\nIncorporating the adjacent roadless areas into the existing Pecos Wilderness and designating some places as special management areas would continue to protect traditional users’ rights and the customary uses of the lands and waters.\nCurrent Grazing Will Continue in Wilderness\nLivestock grazing currently occurs in the Pecos Wilderness. Grazing that occurs today in the proposed additions to the Pecos Wilderness will continue if the area is designated as wilderness.\nCongressional direction on grazing in wilderness areas is very well established. The Wilderness Act of 1964 permits grazing to continue in wilderness and there are established guidelines for managing grazing in wilderness. Today, most wilderness legislation includes language directing that grazing management should follow the Congressional guidelines that reinforce section 4(d)(4)(2) of the Wilderness Act, which states that “the grazing of livestock, where established prior to the effective date of this Act, shall be permitted to continue subject to such reasonable regulations as are deemed necessary by the Secretary of Agriculture.” ¹\nIn the committee report accompanying 1980 legislation designating wilderness in several western states (PL 96-560), the House Interior and Insular Affairs Committee developed comprehensive guidance on grazing in National Forest Wilderness.²\nIn short, this guidance emphasizes that grazing should not be curtailed simply because an area is designated wilderness; facilities may be maintained; new improvements and facilities should be focused on resource protection; and motorized equipment should be used sparingly, and mostly in emergency situations or where permitted prior to designation.\nHunting and Fishing Traditions Thrive\nHunting and fishing in places like the Pecos are time-tested and important traditions in New Mexico. For centuries, sportsmen have been a leading part of the movement to conserve wildlife and wildlands. This tradition would continue in the incorporated roadless areas surrounding the existing Pecos Wilderness.\nHunting, fishing, horseback riding, hiking, camping, canoeing and other non-mechanized outdoor recreation are permitted in wilderness areas. The Pecos Wilderness and surrounding lands are critical habitat for elk, deer, bear, turkey, and one of America’s most robust herds of Rocky Mountain Bighorn Sheep. Its waters are home to the rainbow and brown trout, as well as New Mexico’s state fish – the Rio Grande cutthroat trout.\nProtecting Water Quality and Customs\nAcequias (traditional irrigation canals) and acequia associations are a cultural heritage in New Mexico that value uncontaminated and freely flowing water systems for irrigation and growing healthy crops. Several acequia associations in Northern New Mexico rely on clean water from the rivers and streams that originate in the existing Pecos Wilderness and its adjacent roadless areas.\nThere are no acequias, headgates or other infrastructure located within the existing or proposed wilderness areas. The roadless areas that are proposed for special management area designation would be protected from development while honoring the existing traditional uses that currently occur. Thus, special management area designations will not restrict access or limit maintenance or improvements to the irrigation infrastructure.\nMany communities in New Mexico were granted land that included mountains, pastures, and water, along with communal use rights, including but not limited to hunting (caza), pasture (pastos), and watering (acequias and abrevederos). The Treaty of Guadalupe Hidalgo guaranteed the property rights associated with these land grants, and generations of land grant heirs have survived off the land with these resources. Wilderness protects hunting, watersheds, pasture, and other traditional values and resources for present and future generations.\nFact Sheet: Traditional Use and the Pecos (11-5-2015)","Camping at the KVR\nPrimitive camping is available at 25 sites identified on the visitor guide map. About half of the sites are vehicle-accessible for $15/night; about one third are canoe-accessible and about one third are hike-in, bike-in, or horseback-accessible only for $10/night. Campsites are available on a first-come, first-served basis. All campers are required to register and obtain camping permits; camping permits are available at the 16 self-registration stations located throughout the Reserve. Sites do not have water or toilet facilities. Potable water and bathrooms are available at the Visitor Center near the parking lot and Village of La Farge Campground. Plan appropriately.\nHow to set up and pay for camping\nRiver access sites are challenging and difficult to get to with full camping gear; banks can be steep and muddy as the Kickapoo River fluctuates regularly. Sites are primitive with low maintenance. All riverbank camping has been eliminated due to dangers of flash floods. Please plan accordingly and expect that you may have to haul your gear up to 75 yards up hill from the river to a campsite.\nYou are allowed to burn dead and downed wood found on the Reserve. Don't burn garbage. Leave dead trees standing for wildlife. You can also purchase firewood from local residents. Take care of the trees within campsites, do not chop them, or break branches or abuse them. Many times they are the only shade for that campsite. Be sure to protect the trees that exist.\nLeave No Trace - Going in the Woods\nPlease follow these 3 simple steps when nature calls:\n1) Find a spot at least 200 steps from any water source.\n2) Dig a hole 6-8 inches deep and bury human waste.\n3) Pack out used toilet paper.\n|Leave No Trace|\nThe beauty of the Reserve is unrivaled. To preserve it for future generations, we ask that you leave no trace of your visit.\n\"Pack it in; pack it out\" - pick up and pack out all of your litter. Burying or leaving trash is unacceptable, as is burning items such as aluminum cans and foil, cigarette butts, bottle caps, glass, folding chairs, chemically treated wood, untreated lumber with nails or screws, plastics, or Styrofoam in your campfire. Only burn paper, cardboard, and untreated wood.\nPlease minimize your impact...\n- Use existing fire pits; don't burn garbage.\n- Observe posted camp area capacity limits.\n- Prevent human waste from entering water.\n- Leave the site better than you found it.\n- Respect the serenity of the area - keep noise to a minimum.\nPlease treat the Reserve with respect. Future access depends on your stewardship.\n|To the right are brief descriptions of campsites in the Reserve. Access listed at each site can be defined as……|\nNo Vehicle Access – Parking is greater than 1000 feet from the campsite. Site access is via trail or canoe.\nVehicle Access Nearby - Parking is less than 500 feet from the site, although not possible for a camper or trailer to be on the site.\nFull Vehicle Access – Parking is available on the site.\nRiver Access Only – The site does not have a trail leading to it. Access is best via canoe or kayak.\nEach site has a posted capacity limit. All are limited to 10 people. All campsites are intended for one group, family, or party.\nWillow Camp (D) has four individual campsites and is for self-contained units only.\nLarge groups camping on the Reserve need to apply for an event permit at least 30 days prior to the anticipated date. Contact the Reserve office for more information.\n2020 Updated Campsite Information:\nIn 2020 all campsites along the Kickapoo River have been moved out of the flood plain. Camping is allowed in designated sites only. River accessible sites are: E, L, M, S, U, V, W, X. All have primitive landings and require a short hike to higher ground. All campsites are first come first served. View our most current river guide for more campsite information.\nClick on site name to see info"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e8535e45-5fc9-4a9d-8f2b-f62795b4c87f>","<urn:uuid:10d0791a-8b61-48a7-9ad7-785d0398b910>"],"error":null}
{"question":"How do tax implications differ between a foreign disregarded entity receiving US parent payments versus a foreign unit trust's investment income?","answer":"For a foreign disregarded entity receiving payments from its US parent, the payments are disregarded for US tax purposes under Regs. Sec. 1.1503(d)-5(c)(1)(ii), potentially creating a dual consolidated loss situation that limits the ability to offset US income. The entity's only income source being parent payments can trigger DCL rules despite appearing profitable on paper. In contrast, a foreign unit trust's investment income is treated under PFIC rules, requiring Form 8621 reporting where selecting incorrect treatment can result in capital gains and ordinary income tax obligations even without actual distributions. The tax treatment differs fundamentally - disregarded entities face flow-through taxation while unit trusts face PFIC taxation.","context":["Foreign Income & Taxpayers\nGlobalization has led United States-based organizations to have legal entity structures that span the globe. U.S.-based businesses are constantly expanding into overseas markets. Reasons for expansion can vary greatly, but often they are tied to new market opportunities, enhanced procurement possibilities, and/or access to a cost-effective labor pool. Once a U.S.-based business has decided to enter a foreign country, it generally has several options for establishing a presence in the host country. Under most countries’ laws, corporations, partnerships, branches, or some combination thereof can be established to conduct business. The choice of the type of legal entity to establish in a foreign country is usually contingent upon a number of tax and legal requirements. The legal entity should carefully consider all of the organization’s needs, including, but not limited to, local employment law, currency restrictions, and intellectual property law. Once a legal entity has been formed in the foreign jurisdiction, the entity must determine how it will be treated for U.S. tax purposes.\nGenerally, under Regs. Sec. 301.7701-1, commonly referred to as the “check-the-box” regulations, the foreign entity can elect how it is treated for U.S. tax purposes. The regulations allow an entity to be treated differently for U.S. tax purposes than for the host country’s tax purposes. For instance, a foreign entity that is incorporated in India as a private limited company is generally taxed as a corporation under Indian tax law. Notwithstanding the Indian characterization, the check-the-box rules in the United States allow the foreign entity to elect to be treated as a passthrough entity for U.S. tax purposes. If the foreign entity has two owners, it will be treated as a partnership. If the foreign entity has one owner, it will be treated as a disregarded entity. On the other hand, an Indian partnership, for example, could elect to be taxed as a corporation in the United States. The regulations do not allow certain foreign entities to make this election; however, a discussion of these entities is beyond the scope of this item.\nIf a U.S. corporation owns all the outstanding interest in a foreign legal entity that is eligible and elects to be treated as a passthrough entity, it becomes disregarded for virtually all U.S. tax purposes. As a result, when the U.S. owner files its U.S. tax return, the U.S. owner includes all the foreign entity’s items of income, expense, gain, and loss in its taxable income as if it had incurred those items directly. For U.S. tax reporting purposes, the disregarded entity’s U.S. owner is required to include a Form 8858, Information Return of U.S. Persons with Respect to Foreign Disregarded Entities, with its U.S. tax return. This form reflects the books and records, and it reports certain other information about the disregarded entity.\nOnce a foreign entity checks the box and becomes disregarded for U.S. tax purposes, it gains a hybrid status. In other words, the entity is taxed differently in the two tax jurisdictions. Its host country taxes the entity as a separate entity, but in the United States it is taxed as a flowthrough entity. This hybrid status can subject the U.S. owner and the foreign entity to the dual consolidated loss (DCL) regulations.\nIt is important to first discuss the purpose of the rules and regulations related to DCLs. Congress enacted Sec. 1503(d), and Treasury promulgated the regulations thereunder, to prevent a dual-resident corporation, an entity that is subject to tax in two taxing jurisdictions, from “double dipping.” Double dipping occurs when a dual-resident corporation uses a single economic loss to offset income in two tax jurisdictions. The example in the next section illustrates the practice of double dipping.\nSec. 1503(d)(2)(A) defines a DCL as “any net operating loss of a domestic corporation which is subject to an income tax of a foreign country . . . or is subject to such a tax on a residence basis.” In other words, if a U.S. domestic corporation is also taxed in a foreign jurisdiction and has a net operating loss (or NOL, which is defined in the next paragraph), the U.S. domestic corporation has a DCL and must adhere to the regulations under Sec. 1503(d). These regulations generally disallow the “domestic use” of a DCL. Domestic use is “deemed to occur when the dual consolidated loss is made available to offset, directly or indirectly, the income of a domestic affiliate . . . in the taxable year in which the dual consolidated loss is recognized, or in any other taxable year” (Regs. Sec. 1.1503(d)-2). Under this regulation, a domestic use of a DCL can occur even if the U.S. operations of the U.S. owner generate a loss position. Domestic use of the DCL would increase the U.S. NOL, which would be carried forward and then used to offset U.S. taxable income in future tax years.\nRegs. Sec. 1.1503(d)-5 provides the rules for calculating the DCL. The starting point for calculating a DCL is always the books and records of the foreign entity translated (if necessary) into U.S. dollars. Tax adjustments (e.g., depreciation, M&E limits, and other items) are made to the book income to arrive at taxable income under U.S. tax principles. The regulations also provide other adjustments that must be made, but the discussion of all of these items is beyond the scope of this item. However, Regs. Sec. 1.1503(d)-5(c)(1)(ii) is very important. Overlooking this subparagraph can often cause a potential DCL to go unrecognized. The last sentence of the paragraph states that “items of income, gain, deduction, and loss that are otherwise disregarded for U.S. tax purposes shall not be regarded or taken into account for purposes of this section.” This concept is very important in cases where the foreign entity’s only source of income is payments received from its U.S. owner. The foreign entity’s books and records appear to reflect a net income position. However, because the income received from the U.S. owner is disregarded, the foreign entity is left with only its expenses. Therefore, the disregarded entity is considered to incur an NOL for U.S. tax purposes and is subject to the DCL rules.\nDCL Example and Application\nThe DCL rules can apply to several fact patterns. The provision of services by a foreign disregarded entity to its U.S. parent is common business practice that can implicate the DCL rules. Assume that USP is a U.S. corporation that is in the business of selling widgets, which are manufactured and sold in the United States. In year 1, USP sets up a wholly owned corporation in India called DRE1. DRE1 is treated as a corporation for Indian tax purposes, but elects under check-the-box regulations to become disregarded for U.S. tax purposes. DRE1 serves as the customer service center for USP. DRE1 generates revenue from USP each month via an intercompany services agreement. DRE1 is compensated by USP for its expenses plus a markup. In general, the amount of the markup and other aspects are subject to the transfer-pricing rules, the consequences of which are beyond the scope of this item. Assume that these payments are the only source of income for DRE1.\nIn the example above, USP’s arrangement with DRE1 provides the potential for USP to benefit from its expense in two taxing jurisdictions, and therefore USP must consider the DCL regulations. USP is a dual-resident corporation that is taxed in the United States and in India through its ownership of DRE1. The payment from USP to DRE1 is disregarded under Regs. Sec. 1.1503(d)-5(c)(1)(ii). As a result, DRE1 incurs an NOL as defined above for U.S. tax purposes. Generally, USP cannot use this NOL to offset income in the United States under Regs. Sec. 1.1503(d)-2.\nTax Planning Ideas/Solutions\nIf USP found itself in a situation similar to the one described above, there are ways to avoid loss disallowance under the DCL regulations. The most common solution is for USP to meet one of the exceptions under Regs. Sec. 1.1503(d)-6(a)(1). In addition, USP could consider structuring its operations so that DRE1 is considered a foreign partnership or changing the payer of the foreign entity.\nRegs. Sec. 1.1503(d)-6(a)(1) provides three exceptions to the domestic use limitation. The three exceptions are bilateral-elective agreements (Regs. Sec. 1.1503(d)-6(b)), “no possibility of a foreign use” (Regs. Sec. 1.1503(d)-6(c)), and domestic-use elections (Regs. Secs. 1.1503(d)-6(d) through (h)). The first two exceptions are beyond the scope of this item. However, the domestic-use election is a common exception for many taxpayers. The domestic-use election is a statement the taxpayer attaches to its tax return. In the statement, the taxpayer agrees to notify the IRS if certain “triggering events” occur in future tax years. Then the taxpayer certifies for each of the next five tax years that it has not had a triggering event during that year. If the taxpayer does have a triggering event, the taxpayer may be required to recapture the certified DCL and pay the tax and interest on the recaptured amount. There are special rules and exceptions to triggering events and recapture; however, they are beyond the scope of this item.\nIn addition to the exceptions, USP could also structure its operations or intercompany transactions in a manner that may mitigate the DCL issue. For example, USP could engage in tax planning so that DRE1 is a foreign partnership instead of a foreign disregarded entity, or USP could structure its intercompany agreements so that USP is not the payer of the revenue to DRE1. If DRE1 is treated as a foreign partnership, it is no longer treated as a disregarded entity for U.S. tax purposes, and the intercompany payments would therefore likely be respected for purposes of calculating USP’s DCL with respect to DRE1. Alternatively, USP could cause the intercompany revenue being paid to DRE1 to be paid by an entity other than USP. If the payment is carefully structured, USP may be able to cause the payment to be respected for U.S. tax purposes.\nBoth of these techniques would likely eliminate the DCL issues arising under Regs. Sec. 1.1503(d)-5(c)(1)(ii), which disregards the payment from USP to DRE1 for purposes of calculating the DCL. Both of these situations assume the foreign entity’s books and records reflect positive taxable income. If the foreign entity is in a loss position (while still including the payment), it could still be subject to DCL rules. In addition, there are possible triggering events that the taxpayers should consider if setting up these structures involving entities that have previously unrecaptured DCLs.\nThe rules and regulations pertaining to DCLs are very complex, and this item only scratches the surface. DCLs can also be very costly if the taxpayer has a domestic use of a DCL without filing a domestic-use agreement and annual certifications. However, the regulations provide a reasonable cause relief provision. Regs. Sec. 1.1503(d)-1(c)(1) provides a list of steps taxpayers can take to potentially remedy missed agreement and/or certification filings. Taxpayers and tax practitioners should always be diligent and aware of possible DCL scenarios, especially if they have a structure similar to the one described in the example above. The consequences of missing a DCL can be very costly.\nGreg Fairbanks is a tax senior manager with Grant Thornton LLP in Washington, DC.\nFor additional information about these items, contact Mr. Fairbanks at (202) 521-1503 or firstname.lastname@example.org.\nUnless otherwise noted, contributors are members of or associated with Grant Thornton LLP.","What is a Foreign Unit Trust and How is it Taxed & Reported in the US?\nIn the world of global taxation, where there is much complexity in terms of legal structures, compliance requirements, and changing laws, many financial and tax terms are used interchangeably, especially by those who are not familiar with the terms. While it may not be an issue while sitting around the dinner table having an informal discussion with your spouse or friends, getting the reporting of certain investments wrong can often spell disaster, especially in the United States.\nOne such example is the foreign unit trust, which, at first glance, to those who are unfamiliar with such structures, appears to be no different than a trust. However, that is certainly not the case when it comes to U.S. taxation, and getting the reporting of such structures wrong from a compliance perspective can cause much stress and potentially financial hardship in the way of penalties.\nBut what is a unit trust? Is it an investment? Is it a trust? In its most basic form, a unit trust is similar to a mutual fund, but the profits flow through to the individual unit owners, as opposed to being reinvested back into the fund. Another key difference is that a unit trust is established under a trust deed and the investor of the unit trust is also the beneficiary of the unit trust.\nUnit trusts are divided into units of investments, and the value of each asset in the unit trust is equal to the price per unit, minus operating expenses, and then this figure is multiplied by the total number of units. When you invest in a unit trust, you essentially buy units in the fund. A foreign unit trust, then, is a unit trust that is domiciled in a foreign country.\nAll of this is fine, but how are such investment vehicles taxed and reported in the U.S.? Simply put, a foreign unit trust is essentially treated as a foreign mutual fund in the United States. This means that for all intents and purposes, a foreign unit trust is treated as a passive foreign investment company, or PFIC (which we discussed in a previous article).\nBut what does this mean? It means that you will need to fill out Form 8621 to report not only the ownership in the foreign unit trust but also, to report the earnings and capital gains/losses of the foreign unit trust. This might not sound like a big deal until you realize that you will need to fill out a separate Form 8621 for each foreign unit trust you own.\nFurthermore, and more importantly, electing the wrong treatment of the foreign unit trust on Form 8621 means that you may end up paying capital gains and ordinary income tax, even if no money was distributed to you by the foreign unit trust. Therefore, it becomes very important to select the correct treatment of these unit trusts on your U.S. tax return.\nIn addition to the reporting requirements on Form 8621, you may also have a Form FinCEN 114 (FBAR) and Form 8938 filing requirement for the foreign unit trust. Therefore, the reporting requirements around such structures are numerous, and the implications of getting it wrong, are huge. If you are the owner of a foreign unit trust or have a portfolio of foreign investments, please get in touch with us today to discuss your options, as not reporting the foreign unit trust in the U.S. or reporting it incorrectly can be devastating to your finances."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:92228d7d-8ce8-4d14-b6e9-bd747d8b7283>","<urn:uuid:7f68ac9e-85c1-4f7c-8de4-bbf7c3e755a3>"],"error":null}
{"question":"How do banks in DRAM compare to storage capabilities of ROM in terms of data persistence?","answer":"DRAM banks and ROM have fundamentally different approaches to data persistence. DRAM banks, which typically number between 8 to 16 per chip, can only service one request at a time and lose all stored data when power is removed, making them volatile storage. Each bank stores tens of thousands of pages but requires constant power to maintain the data. In contrast, ROM is non-volatile and can hold data permanently without power, making it ideal for storing data that won't require modification for long periods. While DRAM banks offer parallel access capabilities through bank-level parallelism, ROM provides permanent storage but with much slower write speeds.","context":["In the traditional view, memory was a flat monolithic structure with a fixed access latency. Today’s computer systems use page-mode addressing and smart memory scheduling which makes this flat view look rather naive. In my opinion, there are four concepts every programmer must be familiar with: pages, banks, row conflicts, and FR-FCFS.\nData in the DRAM is stored at a page granularity. The size of a typical page is between 4K to 16K. In theory, this size is independent of the OS pages which are typically 4KB each. In practice, most DRAMs use pages bigger than the OS pages in order to improve locality in the row buffers (described below).\nTo reduce access latency, memory is split into multiple equal-sized units called banks. Most DRAM chips today have 8 to 16 banks. Each bank stores tens of thousands of pages.\nThe following is a practical example of how a 4 GB (32-bit) physical address will be split by the DRAM (credit to my friend Khubaib). Bits 12-0 identify the byte within the 8KB page. Bits 16-13 identify which of the 16 memory channels the system should use for this address. Bits 20-17 identify which of the 16 banks the address will be found in and Bits 32-21 identify which row within the bank is accessed. Channel bits are used to split banks across DRAM modules in order to increase the bandwidth to DRAM.\nA memory bank can only service one request at a time. Any other accesses to the same bank must wait for the previous access to complete, known as a bank-conflict. In contrast, memory access to different banks can proceed in parallel (known as bank-level parallelism).\nEach DRAM bank has one row-buffer, a structure which provides access to the page which is open at the bank. Before a memory location can be read, the entire page containing that memory location is opened and read into the row buffer. The page stays in the row buffer until it is explicitly closed. If an access to the open page arrives at the bank, it can be serviced immediately from the row buffer within a single memory cycle. This scenario is called a row-buffer hit (typically less than ten processor cycles). However, if an access to another row arrives, the current row must be closed and the new row must be opened before the request can be serviced. This is called a row-buffer conflict. A row-buffer conflict incurs substantial delay in DRAM (typically 70+ processor cycles).\nMemory accesses are not sent to the DRAM in the order they are sent by the core. Instead, they are buffered in the on-chip memory request buffers and dispatched to the DRAM in the order specified by the memory scheduler. To maximize throughput of the memory system, memory schedulers today employ a policy called the First Row-First Come First Serve (FR-FCFS). The idea is to prioritize accesses to the page which is currently open in order to minimize the number of row conflicts. While this helps with memory throughput, it further increases the penalty of a row buffer conflict. As a rule of thumb, a row buffer conflict takes at least 3x the time of a row-buffer hit. It is important to note that all known memory scheduling algorithms schedule requests for each bank completely independently and, thus, have no impact on bank-level parallelism.\nWhat can programmers do?\nAvoid memory accesses: I cannot stress this enough. Memory may seem free in terms of storage but it costs a lot more in performance if the working data set size becomes bigger than the cache or the available DRAM. It is ultimately a good idea to reduce the memory footprint as much as reasonably possible, especially on embedded devices.\nBank conflicts: In general, application programmers work with virtual addresses while the DRAM operates on physical addresses. Thus, programmers cannot reduce hazards like bank conflicts as they have no control over the placement of data in physical memory. However, there are some special scenarios where the programmers can help. For example, most ISAs have now introduced virtual pages that are as big as 1GB. Programmers are allowed to do their own mapping within these large pages. This gives the programmers a lot more control over what data goes to which bank and they can use this control to increase bank-level parallelism.\nRow-Buffer Conflicts: Programmers can also help by increasing the spatial locality in the row buffer, .i., try to make consecutive memory accesses as close to each other as possible. This often implies packing data which is likely to be accessed together in consecutive memory locations, e.g., building a struct of arrays is often better performance. By the way, this also implies that linked data structures that promote random accesses should be avoided as much as possible because each row buffer conflict has a latency greater than 100 computes.\nI did an experiment in this post on an ARM Cortex A8 which showed that sequential accesses cost only 20 processor cycles but random accesses cost close to 180 cycles. The same experiment on an i7 led to about a 2x boost from sequential accesses.\nProgrammers can save battery and increase performance via DRAM-aware programming. Such optimizations may not be practical in all scenarios but understanding the basic DRAM structure can help programmers identify existing or potential performance bottlenecks, specially on more constrained platforms like the iPhone.\nReference: What every Programmer should know about the memory system from our JCG partner Aater at the Future Chips blog.","Difference Between RAM and ROM\nRAM, otherwise called Random Access Memory, maintains the data only when it is in use. It stores the information in temporary forms and requires the power to store data. The size is more, and also, it is costly when compared with ROM. We can use RAM for both read and write operations. ROM (Read Only Memory) stores the content even if the system is not in use to store the information permanently. It can be used only for read operations, and the cost is very less. The size is small, and its capacity is also less. RAM is used in CPU cache, whereas ROM is used in microcontrollers.\n- Every computer has some form of storage, which stores data and machine code currently under execution. A RAM, i.e. random-access memory device, provides the same purpose, which allows the read and writes of data in the same amount of time. RAM devices have many data lines to address this storage requirement. Modern world RAM devices are known to have volatile types of memory; the information would be lost if power is removed, even though non-volatile RAM is also being developed. RAM is also available in the form of integrated circuits. There are other non-volatile memories in the market, which have certain restrictions like allowing random access for a read type operation but won’t allow write operations.\n- There are two types of RAM that are widely available in the modern computer world, SRAM, i.e. static RAM and DRAM, i.e. dynamic RAM. SRAM is expensive to produce, which stores every bit of data using a transistor memory cell state. SRAM is faster and needs less power than DRAM, thus utilized by the modern computer chiefly as cache memory. On the other hand, DRAM store a bit of data using a pair having a transistor and capacitor. Since it is cheaper to produce as compared to SRAM, they are widely used in computers across the world. Both SRAM and DRAM are volatile in nature since they lost their state when power is switched off for the system. More specifically, they are responsible for providing the main memory in the computer system.\n- ROM cannot be easily modified, so they are fit to store the data, which won’t require modification for a longer period of time. One can say that this type of memory is hard-wired, cannot be altered after its manufacture. A recent version of ROM has arrived, contain a memory that is read-only for normal operation, although it can be programmed. EPROM, i.e. Erasable programmable read-only memory and EEPROM, i.e. electrically erasable programmable read-only memory, can be removed and re-programmed. However, this process takes time, relatively slower in speed and can be achieved at certain attempts.\n- When the computer is powered on, a certain amount of memory is required to run the initial program, which is stored with ROM. This process is also known as booting or bootstrapping. In the modern computer, booting for the main processor is stored in ROM; other devices like a graphics card, hard disk, CD-DVD drives also utilizes ROM in the system. Mask-programmed ROM is the classical example of ROM, which physically encodes the data required to be stored. They are integrated circuits, and impossible to change their content. Another category of ROM, however, can be modified, which includes PROM, EPROM, EEPROM. ROM, which is electrically modified, their reading speed is faster than the writing speed. After applying for write protection, some reprogrammable ROMs become read-only memory.\nHead To Head Comparison Between RAM and ROM (Infographics)\nBelow is the top 6 difference between RAM vs ROM\nKey Difference between RAM and ROM\nBoth are popular choices in the market; let us discuss some of the major difference:\n- RAM is random access memory and cannot hold the data without the power, whereas ROM is a read-only memory and can hold the data even without the power.\n- RAM is a volatile storage medium to store the information, whereas ROM is a non-volatile storage medium to store the data.\n- With RAM, writing data is a much faster and lightening process, whereas ROM, writing data speed is much slower as compared to RAM.\n- RAM comes in two flavours, i.e. static RAM and dynamic RAM, whereas ROM has three varieties: Mask ROM, PROM, EPROM, and EEPROM.\n- Data on RAM is accessed, read and erased multiple times, whereas on ROM, writing data is a relatively very slow process.\n- RAM has usage with primary memory DRAM and CPU cache memory SRAM, whereas ROM is being used in BIOS, microcontrollers, and other electronic devices.\n- RAM is expensive and does not come cheap, whereas ROM is way cheaper as compared to RAM.\n- RAM has a large size with even higher capacity, whereas ROM is smaller in size and even with lesser capacity.\n- RAM is a high-speed memory with reading-write operations, which happen at a fast pace, whereas ROM is slower speed memory, which is less prone to modification and can be done via an external program.\n- With RAM, data on it can be altered multiple times, which explain its expansiveness, whereas ROM has data, which is permanent, even though it can be altered but at a very slow speed and that too, for a limited number of times.\nRAM vs ROM Comparison Table\nBelow is the topmost comparison between RAM vs ROM\n|The Basis Of Comparison||RAM||ROM|\n|Data||RAM cannot hold data without power||ROM can hold data without power|\n|Storage||A temporary medium of storage||A permanent medium of storage|\n|Power||RAM chip is volatile, loses information if power is gone||ROM is nonvolatile, does not require a constant source of power|\n|Operation||RAM chip is used in the normal operation of a computer||ROM is primarily used in the startup process of a computer or bootstrapping|\n|Speed||Writing data to RAM is faster||Writing to ROM is much slower as compared to RAM|\n|Example||RAM chips can store information, run the program and quickly switch between the tasks||PROM chip used with BIOS store the program need to begin the initial computer startup process|\nIn this RAM vs ROM article, we have seen both RAM vs ROM have their own set of merits and demerits. But both types of memory devices are mandatory for the efficient functioning of a computer device. ROM is cheap and can hold data permanently, but has its own set of limitations, like hard to modify the data and that too for a fixed number of time. RAM is expensive, but at the same time, data can be altered on it multiple times.\nHowever, with a modern electronic landscape, ROM is no less incompetent. Removable storage media like USB drives, Storage data cards etc., is a sophisticated implementation of EEPROM, a read-only memory. Thus, both RAM vs ROM are meeting the requirement of technologies and performance of applications, with a large volume of data processing.\nThis has been a guide to the top difference between RAM vs ROM. Here we also discuss the key differences with infographics and comparison table. You may also have a look at the following articles to learn more."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:60d879e1-80cb-4f77-a2ff-901d3f224ecf>","<urn:uuid:d2b31c14-0555-4e14-8807-3a6a827d3209>"],"error":null}
{"question":"How did manufacturing innovations in the 1920s photo booth industry compare to post-war precision camera production?","answer":"The manufacturing approaches were quite different. In the 1920s, Josepho's Photomaton represented innovation through automation, creating a self-operated device that could process photos quickly using standardized chemical processes. In contrast, post-war Leica camera production emphasized precision hand-crafting, with extensive use of specialized tools and fixtures. For instance, Leica used die-casting for improved body rigidity and required skilled craftsmen to hand-adjust components like shutter mechanisms through filing and careful hammer work. While both represented manufacturing innovations of their eras, Photomaton focused on automation and accessibility, while Leica emphasized precision craftsmanship and quality control.","context":["We are Tiny.\nBack to Blog\nThe photo booth machine was invented by a Siberian immigrant named Anatol Josepho.\nWho was Anatol Josepho?\nAnatol Josephewitz (later Josepho) was born in Omsk, Siberia, in 1894, to a prosperous jeweller and his wife. Anatol lost his mother at age three, and was quite attached to his father.\nEven when he was a little boy Anatol’s dream was to travel around the world. He was especially fascinated by America and was eager to see the 'Wild West'. He also had a great interest in cameras that were making photography available to the middle class.\nHe wanted to learn everything about the camera and was enrolled in a local technical institute. At the age of 15 Anatol was impatient, and told his father that he wanted to explore the world. He went to Berlin with the money his father gave him. At Berlin, he happened to see the beautiful hand-tinted photographic portraits in a studio and persuaded the owner to train him as a photographer.\nIt was here that he learnt more about photography, developing and printing. To create images that would make photographs available to the common man, he evolved the idea of creating a more efficient, faster and less costly way of taking photos.\nHis fascination for America urged him to travel to the US in 1912. However, the 18-year-old could not find any job or support in New York, and had to return to Europe.\nAnatol opened his own photo studio in Budapest with great optimism. He had already started to draw designs for an automated photo machine and experimented with photograph even though he was just 19. His aim was to create a self-operated device that would work with a coin. He was successful in his invention.\nDuring the First World War Anatol was put under strict military surveillance as he was a Russian. It was at this time that he started working on a photographic paper that would not require a film negative to create beautiful toned images. For years he continued in his pursuit. He returned home to his father in 1920. But left again, and ended up in Shanghai in 1921.\nPhotomaton is born\nIt was here that the 27-year-old Anatol changed his Russian spelling of Josephewitz to Josepho and started his own studio – Josepho Studio. He soon became popular in China. However, he was still working on his invention, which later came to be known as Photomaton. In Shanghai, an outline was approximately drawn out and the notations for the chemical process cautiously prepared. But still he was not satisfied. He wanted to invent an automatic photographic machine. He went back to America to get financial support to realise this dream.\nAfter a stint at Hollywood he came to New York City. Here he succeeded in raising the money to produce the first model. He also could find the right engineers and machinists to help him build his Photomaton machine.\n1925: His Photomaton Studio was opened in September 1925 on Broadway where he was taking photos charging just 25 cents for a strip of eight. People started to throng his studio to get pictures clicked. Finally, Anatol realised his American Dream. Soon he found buyers for his photo machines and the Photomaton patent. He was offered $1m for the American rights and Anatol accepted the offer. He sold the European rights for the Photomaton to an English/French group the next year.\nThe Photomaton started a journey that took the bulky and heavy booth to every country on earth. Everyone loved making faces, squeezing in friends, and kissing in the booth.\nPhotobooths & Hollywood\nPhotobooths can be traced to Hollywood films in the early 1950s. In the film The Band Wagon with Fred Astaire, Astaire dances into a Photomatic. Esquire magazine, in 1957, dared Richard Avedon to produce photographs and dragged one of Mutascope’s art deco booths into his studio in New York. The result was the stunning images of Audrey Hepburn, Marilyn Monroe, Truman Capote and Ethel Merman.\nAndy Warhol – the first art promoter of the photobooth\n1950s: Warhol understood the photobooth as an effective and cheap camera in the late 1950s. He envisioned the sense of movement and colour an artist could achieve by combining different poses from the booth.\nIn the late 1950s, Auto-Photo tried to market the Model 11A, designed for prison mug shots.\n1963: With his inclusion of photobooth photos of models in Harper's Bazaar, Warhol challenged the commercial portrait world in 1963.\nDigital colour photobooths\n1990s: Digital colour photobooths that used a computer to print strips faster came into existence in the 1990s, promoted by Photo-Me.\nPhoto booths are as fun as ever with today’s technology. They present a wide variety of options for printing, utilizing and sharing at all types of events.","dante stella stories photographs technical guestbook\n|Leica IIIc FAQ prepared by British Intelligence|\nThe following is a FAQ on the Leica IIIc, with answers prepared after a joint British-American \"visit\" to the Leica factory in Wetzlar. You can find the complete report (British Intelligence Objectives Sub-Committee Report No. 1436, on \"The 'Leica' Camera'\") on Peter Grisaffi's site, in all of its gruesome detail, right down to the age breakdown of Leica workers, and without my asinine commentary. The questions are mine; the answers are from the report, based on site visits and interviews with Ernst and Ludwig Leitz. It's hard to read the report as anything other than a blatant... er... courtesy call, but it does answer some fundamental questions.\nQuestion: How does the Leica IIIc vary from the IIIb?\nAnswer: The Leitz Company is now producing only the Model III C Leica Camera, this having by development, superseded the Model III B. The Model III C differs from the III B in several aspects, although it is functionally similar. It is approximately 1/8\" longer, it has an integral top plate and range finder cover, and the main body is now a single pressure die casting, which together with two other pressure die castings for the top and bottom of the unit forms a complete chassis for the assembly of all the other component parts. In the later models the focal plane shutter blind rollers have been fitted with ball bearings. The outer case of the Leica although 1/8\" longer is formed from extruded tube as was the case in the III B and earlier models. The coupled range and view finder has not been altered in any way.\nComment: Die-casting provided significantly improved rigidity in the camera body, which is important especially for high-speed wideangles.\nQuestion: What did Leitz make and what did it subcontract?\nAnswer: Leitz make in their own factories practically all their requirements, including the leather cases for cameras and accessories. Of the items bought out, the castings come from Mahle of Stuttgart, slow speed shutter mechanism from Gauthier of Calmbach, shutter tape from Soenecker & Pfaff of Wupperthal Barmen, and blind material from America through their associated company in that country.\nComment: The comment about shutter blinds in particular is interesting, because the material is now subcontracted to a company in Switzerland.\nQuestion: What kind of machines were being used at the plant?\nAnswer: In the machine tool side the following machines were noted:- Steinel Drilling Machines, Lindner Thread Grinder, Deckel Engraving and Horizontal Milling Machines, Lorch Lathes, Sooda Automatics, Mikron Gear Hobbers and Hauser Jig Borer. The chasing lathes were by Hille. The general machining of mall precision parts followed very closely the usual procedure in this country.\nQuestion: How did they cut multi-thread helicoids in the bad old days?\nAnswer: The cutting of the multi-start thread in the focussing mount of the Sumitar [sic] and the Elmar lenses was carried out on a converted Lorch Lathe. The conversion of this machine by Leitz was extremely ingenious. The component to be screw-cut was screwed on to an attachment on the head-stock of the lathe by a chucking-thread already cut in it. A single tool was employed in the tool post. The whole head-stock was then caused to oscillate backwards and forwards by a thread attached to the back of the head-stock engaging with a chasing nut which was in turn engaged and disengaged at the end of each stroke by a yoke which also operated four glass-mercury switches. These switches operated a solenoid attached to the drive counter shaft which, in turn, operated a reversing clutch in the drive. The head stock was caused to rotate for some ten revolutions in a clock-wise rotation which moved it forward by the chasing thread on it engaging with the chasing nut. At the end of this forward stroke the chasing nut disengaged and then re-engaged with the next thread, the counter shaft also re- versing, causing the head-stock to rotate in an anti-clockwise direction on the back stroke for some ten revolutions. As the head-stock oscillated backwards and forwards at each successive movement it engaged with a conveniently disposed paul which engaged with a ratchet connected to the feed on the tool post and at each backward and forward stroke the tool was automat- ically fed in. The extent of this was shown by a clock indicator attached to the tool post slide. The device worked well and was almost \"fool -proof\" to operate.\nThe male 5 start thread was cut first on one lathe of the type just described. The female member was cut in an identical manner but to suit the male thread. A certain amount of play was purposely left between the two, when dry, but after both had been well washed out in paraffin a liberal application of special grease was applied and they were assembled together. The special lubricant caused them to have a very fine \"velvety\" feel when moved in or out. As male and female members are machined to suit one another it is very doubtful if any two complete units are dead alike as regards inside and outside diameters of threads. Mr. L. Leitz agreed that this was so but added that once a lens focussing mount was assembled it became a unit for good and it did not need to be interchanged with parts from another complete lens mount.\nComment: It is interesting that people attack Soviet and Japanese lenses for using grease to take up the machining \"slack,\" as that was Leitz's classical method of attacking tolerances and smoothness.\nQuestion: Are Leica III lenses cammed the same way as Leica M lenses?\nAnswer: The pitch of the 5 start thread in the focussing mount of the Sumitar was 6.210 mms., but in the case of the Elmar a pitch of 6.00 mms. and also 6.138 mms. was employed. To ensure lineality between movement of the lens and the coupled telemeter a slight cam is machined on the end of the male member of the multi-start thread which engages with the roller which actuates the winging prism of the telemeter. The greatest depth of this cam face does not appear to exceed .007\".\nComment: The camming surface described here is a method for avoiding having a separate helicoid for the Elmar. It is also exactly the same method used to make Leica CL lens rangefinder couplings.\nQuestion: How did they make prisms?\nAnswer: The manufacture of small prisms as used in the telemeter [rangefinder] were by mouldings cemented in fours and then gang-milled by diamond milling tools. They were then taken apart and milled to the other face. Diamond tool milling machines are used for slotting the hypotenuse. Smoothing and polishing was achieved by normal type poker-arm machines. Edging was carried out on small lens in the telemeter by diamond wheels. These wheels are made for Leitz by Jung of Berlin.\nQuestion: What was the percentage transmission in the beamsplitters?\nAnswer: The small reflecting mirror in the range finder is very lightly alluminised, the degree of deposit being just sufficient to give an equal degree of brightness to both the directly and indirectly received images when seen through the range finder eyepiece.\nComment: This means that if you are really handy, you can buy a 50% aluminized beamsplitter from Edmund Optics and install it yourself.... yeah, right!\nQuestion: What were the steps for applying the satin chrome finish?\nAnswer: Sandblast, hot cleaner without current, cold cleaner with current, warm rinse, followed by cold rinse, hydrochloric dip, copper flash, cold rinse, sulphuric dip, cold rinse, bright nickel-plate, warm rinse, hydrochloric dip, bright chrome, drag-out rinse, cold rinse, hot rinse, and dry.\nComment: So \"bright\" marks may be polishing... or they may be the brighter nickel finish showing through.\nQuestion: Why were Leicas so expensive?\nAnswer: The Model III C takes longer to assemble than its predecessor, 33 hours as compared with 29 hours, but there is no doubt that as an engineering job it is far superior. Sub-assemblies were batched in tens and assembly practice was very similar to that employed in most instrument factories. Special jigs, fixtures, and tools were employed where-ever possible to assist rapid and accurate assembly. Female labour was used for the minor assemblies, the male labour being employed mainly on shutter and range finder assemblies and on testing.\nComment: This is an absolutely huge amount of labor. It only takes 22 man-hours to build an automobile, which has thousands of parts. Of course, they had no robots in Wetzlar.\nQuestion: Was there any change in the shutter from the IIIb?\nAnswer: The main design of the shutter is in no way basically changed; only slight modification of various components to suit the new assembly. In the assembly of the shutter fast- range escapement one component after another was tried till one was found that worked in a fairly satisfactory manner and then various minor alterations were made to it by filing, and in some cases, a light tap with a small watch-makers hammer. The skill of the operators was undoubtedly the chief asset in the efficient assembling of this shutter. The slow speed escapement of the shutter is made by Gauthier of Calmbach.\nComment: The shutters must have been built to very low tolerances if it took a filing and a small hammer to get some examples of it going.\nQuestion: How did they calibrate shutters before electronics?\nAnswer: The timing of the\nfast range of the shutter is carried out with the aid of a stroboscope\nof somewhat antique design. By means of this the 1/200, 1/500 and the\n1/1000 speeds are checked. The stroboscope consists of a revolving drum\nplaced horizontally, with 33 horizontal slits in its surface, illuminated\nfrom inside by a lamp of approximately 20 watts. The drum is driven by\na belt from an electric motor which may be controlled by a rheostat. The\ndrum is also coupled to a speedometer in order that its speed may be set.\nThe correct speed for the drum to rotate at was 280 r.p.m. The camera\nis held on a wooden block in such a manner that the light from the rotating\ndrum falls on the blinds of the focalplane shutter. The shutter is then\nfired and a series of\nThe checking of the lower speeds was only carried out on the 1/20 second and 1/4 second ettings by means of a revolving series of lights. The various speeds of the shutter were not accurate to the measurements on the shutter control knob and this fact was acknowledged by the Leitz executives. who pointed out, however, that the results obtained were quite good enough for all general requirements.\nA metronome was used in checking the one second escapement.\nComment: The method they were using to check the fast speeds is remarkably similar to the home-built shutter testers that use a record player to turn a wheel of black-line markings. The comment about the slow speeds is interesting; the IIIc was just not capable of efficient precision-setting of slow speeds. Today it's all done with computers.\nQuestion: How did (do) they make shutter curtains?\nAnswer: The fixing of roller blinds and tapes on the focal- plane shutter was carried out on a very ingenious fixture. The body of this fixture is all metal and accommodates the two roller blinds and apes on pivots ensuring that the blinds and tapes are of the right length and fixed at exactly the right distance apart. After the blinds and tapes have been stuck to the rollers they are allowed to dry for at least 48 hours before being assembled into a camera. In every possible operation special jigs and fixtures are used to increase speed and uniformity of production.\nQuestion: How did they deal with flange-to-film alignment?\nAnswer: The checking of the lens flange of the focalplane for squareness is carried out by a focalplane ollimator which is located by the lens flange and directs a beam of light on to a polished reflecting surface which is located on the focal plane. Lack of squareness and an accurate measurement of the extent of the error is at once visible on the graticule of the collimator. When correction is necessary a specially adapted vertical milling machine is used to correct the orientation of the base of the camera body which corrects the lens flange. If small errors in squareness occur which are not sufficient to warrant machining, the lens flange is packed with a small shim. The correct istance from the front of the lens flange to the film pressure plate, located at the back of the focal plane is 28.80 mms. which is the equivalent to 1.134\". Checking of the measurement was made by a special fixture and a clock indicator.\nComment: The IIIc did not take the curvature of film into consideration (but why would you with f/3.5 lenses?). Machining must have required some pretty big error if shims were the minor solution. Just as with the M cameras, there is no measurement for the front film rails. Interestingly, for quite some time, Japanese makers have used machining as the primary method of fixing film-plane alignment.\nQuestion: How did they set the rangefinder alignment?\nAnswer: The setting of the position of the telemeter prism to correspond to infinity is carried out by the aid of a rotating disc fitted with alternate colour filters which are illuminated from behind, the whole being fixed in the roof of the assembly shop. The measurement from the lens flange to the centre of the telemeter actuating roller with the telemeter set at infinity is 0.750 mms.\nComment: The exact testing apparatus is unclear, but it appears that the focusing target is on a distant rooftop, rather than inside the same building.\nQuestion: How did they test lenses before the dawn of MTF?\nAnswer: The Summitar lenses were tested for definition by projection at a focal length of 51.68 mm. and were stopped down to 3.2 aperture throughout the test. The total backward and forward movement was also checked by a clock coupled to the lens mount by a pivoted lever movement. The mounts shown by the clock were very closely checked with the readings of the footage scale engraved on the focussing mount. The projection test was in the form of a fine interlaced vertical and horizontal graticule, interspaced with fine figures and letters which were compared with a known standard ngraved on a vernier attachment mounted on the screen on to which the image of the graticule was projected.\nComment: I have seen a picture\nof the first part of this in a Minolta Autocord instruction manual. I\nam unclear as to what exactly the clock referenced in the second part\nAnswer: Each completed camera is tested finally by taking a number of black and white photographs between 1.25 to 10 metres. The result is checked with individual lens, a standard speed film and a metol hydroquinone developer being used. The tests are compiled for lens performance reference.\nQuestion: How many were they making? Who got them?\nAnswer: In November 1946, the production of Leica Cameras was 1100 per month of which 89% was allocated to the American forces, 6% for French forces and 5% for German sales. A small proportion of the American 89% was available for the British forces in exchange for Rolliflex cameras.\nAlthough the the camera was in great demand production was limited owing to shortage of materials, particularly optical glass. Stocks of brass and other metals appeared to be high.\nQuestion: What were 1946's lens selections?\nAnswer: Through the shortage of optical glass, lens manufacture was confined to the f/3.5 5 cm. (standard) and 3.5 cm. (wide angle) and the f/4 9 cm [Elmar] and f/4.5 13.5 cm. lenses.\nComment: That's about the best test there is.\nQuestion: What did the British think of Leicas after the war?\nAnswer: The team came away with the impression that the Leica camera is still worthy of its pre-eminent position and that the skill of the craftsmen is very much in evidence in the Leitz factory."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0a376afe-f706-4a8b-a24c-4c5817e25cb4>","<urn:uuid:2ab3d8a7-61bd-4ca1-b456-59e27f403de5>"],"error":null}
{"question":"What are the main similarities between language acquisition and contemplative practices in terms of brain activity patterns, and how do these patterns manifest differently in primates versus humans?","answer":"In both language acquisition and contemplative practices, specific brain wave patterns indicate cognitive processing. During controlled focus meditation, gamma and beta waves (13-50 Hz) are present, similar to effortful language processing. While humans show discontinuous jumps in language learning due to innate Universal Grammar, primates like Nim Chimpsky showed no such jumps, maintaining steady but limited progress. Studies indicate that primates lack the innate language capabilities humans possess, as evidenced by Nim's inability to form grammatically correct sentences beyond basic combinations. In contrast, both humans and experienced meditation practitioners demonstrate progressive systemic changes in brain structure, particularly in regions like the hippocampus and prefrontal cortex, leading to improved cognitive abilities and memory consolidation.","context":["What does the language experiment with Nim Chimpsky prove about Chomsky’s claim that language is exclusively a human attribute?\nIn 1986 Noam Chomsky presented a theory about language acquisition and capabilities for this within the human race. He claimed that children do not learn language and are instead born with ‘Universal Grammar’, which in turn causes them to have an ability to pick up any language (dubbed ‘language instinct’ by Steven Pinker, 1994). In order for this to be proven correct, one of the key determining features of language acquisition is discontinuous jumps in the child/primate’s progress. This essay will evaluate these three factors off which Chomsky’s theory is based and compare the generic human example to the experiment done on the chimpanzee Neam ‘Nim’ Chimpsky (Herbert Terrace, Thomas Bever, Laura-Ann Petitto, 1973-2000) who was raised in a human family environment from just after birth and treated as a human child (Pinker, 1994). This will be in attempt to prove that Chomsky’s claim, while may not necessarily wholly correct, does have some scientific grounding and may be proven correct in the future when further research is performed.\nThe ‘Universal Grammar’ that Chomsky claims humans have is a rudimentary grammar knowledge which is built into our DNA from birth. He defines it as ‘the system of categories, mechanisms and constraints shared by all human languages and considered to be innate’ (Chomsky, 1986). This system predicts that there will be nouns and has an inbuilt strategy for modifying them. The brain is constantly on the lookout for this article system from the minute the child enters the world. However, this rests solely on the premise that all languages around the globe share certain structural properties, otherwise the grammar would have to differentiate based upon the location in which you are born. It has since been proven there are many similarities within languages, experts citing approximately four hundred functional categories (Heine and Kuteva, 2002) although views on this vary widely. Chomsky also suggests that the process by which sentences are perceived as correct or incorrect is universal and independent of meaning regardless of the language they are spoken in (Dubuc, 2010). This theory is based on the existence of formal universals (i.e. the rules which enable sentences, phrases and syllables to be understood) and substantive universals (i.e. grammatical categories and functions) (Tomasello, 2013) and their importance to creating and sustaining conversations. From the study of Nim it cannot be said that he understood grammar or even had the capability to, thus proving (at least in the case of this chimpanzee) that Chomsky’s theory has some qualitative basis. Throughout the course of the study, Nim created over twenty thousand potentially grammatical sentences (e.g. more + object and verb + noun not vice versa); however the phrase banana + me was performed three times more than me + banana. Some of the researchers thought that this could be an example of a foundation for grammatical knowledge, however as the American Sign Language (ASL) Nim was taught did not differentiate between the signs for ‘me’ and ‘my’ it is impossible to tell whether he understood grammar (insomuch as he was attempting to say ‘give the banana to me’ or whether he meant ‘my banana’, in which case he was in error the majority of the time) (Wynne, 2007). In addition, whilst Nim learnt approximately one hundred and twenty-five signs and could string them together into sentences, the longer ones often had no grammar, as in the example of his longest sentence ‘give orange me give eat orange me eat orange give me eat orange give me you’. This shows that there is no real innate understanding of grammar as the normal phrase order is verb + noun and whilst this is true for the first three words, from then it immediately stops making sense ‘give eat’ (verb + verb), clearly incorrect grammar. In addition, the phrases signed often included ‘hug’, ‘me’ and ‘Nim’ which fit with almost every other sign he knew and he was often rewarded for using. This then shows more of an operant conditioning style of learning than an inbuilt knowledge of grammar (Tomasello, 2013). Nim could effectively string any two words together and be rewarded for this which would cause him to remember it as a stimulus for reward the next time. As a result of this, Chomsky’s claim may appear to have some scientific basis for further research.\nThe concept of ‘language instinct’, whist named thusly by Pinker, is actually a large proportion of Chomsky’s theory. It states that in the presence of Universal Grammar, it is easy to pick up any language in the world, due to the structural similarities between them combined with the inbuilt grammar knowledge it is thought all humans are born with. In order for this to be truly called an ‘instinct’ however, there must be little explicit instruction in how to craft sentences; in other words there must be some natural knowledge and input from the child rather than learning simply through observation and direct imitation. Darwin called language ‘an instinct tendency to acquire an art’ (Pinker, 1994) which suggests that this idea predates both Chomsky and Pinker and as such, must have some scientific grounding. When we look at the study of Nim Chimpsky, Chomsky’s theory becomes even more substantiated. He was rarely found to be the one to initiate conversations, instead only signing in response to a question or phrase from one of his teachers (90% of his signing was through this call-and-response method) and often simply imitated the signs they were making. It was recorded that at least 50% of Nim’s signing was just copying the signs made to him (Premack, 1971). It was noted that he also often grabbed objects before signing for them, which could be taken simply to be a lack of manners as it is so often portrayed within younger children; but it is more likely to show that Nim did not ever rely upon ASL as a primary form of communication and simply reverted to using it when there was a necessity or he was likely to be rewarded for doing so. This again links back into the idea not of Nim understanding or having an innate knowledge of language and grammar, but simply his use of language being the result of a long trial of operant conditioning. This is backed up by Terrace’s statement, “much of the apes’ behaviour is pure drill” (Wynne, 2007) which seems to show that there is no real language instinct acting here.\nThe final marker to show that language is an exclusively human attribute is that of the discontinuous jumps in the process of learning or gaining language. Chomsky’s theory suggests that due to the combination of Universal Grammar and a ‘language instinct’, when a phrase such as the + noun is heard, it should mean that the specific grammatical rule for that noun class (i.e. common nouns, proper nouns, pronouns, etc.) is immediately grasped and from that point onwards, can be freely applied to that entire class (Evans, 2014). This then leads to the discontinuous jumps in language knowledge, as every time a new rule is learnt there should be a large spike and advance in language usage. Chomsky describes language as a mutation which produces a phenotype well outside the range of variation previously existing in the population (Evans, 2014), and as such these easily discernible jumps in language acquisition becomes known as a macromutation. However, when we consider Nim’s case this does not seem to appear. Nim’s signs were often highly repetitive (Petitto and Seidenberg, 1979) and his sentences did not appear to increase in length as his vocabulary advanced, the average sentence length being approximately 1.25 signs (Hart, 2015). He also appeared to learn signs at a fairly steady rate, purportedly learning 125 signs in just two years (Adler, 2008), however Petitto estimated that Nim’s vocabulary was closer to 25 signs. This cannot be called language as this is a doubly articulated system (i.e. object + state combined syntactically to create a meaning) and as shown earlier in this essay it is questionable what the meanings of Nim’s sentences were (Wade, 1980). This proves another part of Chomsky’s theory to be correct as Nim’s language learning process did not contain any discontinuous jumps in his understanding and grasp of language.\nIn conclusion, it is safe to say that Chomsky’s claim is fairly valid. Whilst the study of Nim and his acquisition of language whilst being raised like a human clearly proves the theory correct, this cannot be said of all species or even of all higher primates. In this case though, it can be said that Nim did not have Universal Grammar due to the longer sentences being no more than a jumble of signs however this cannot be conclusively proven due to the lack of differentiation between the signs for ‘me’ and ‘my’. Neither did Nim have any sign of a ‘language instinct’ as he seemed to sign sentences and phrases that he had learnt gained rewards, showing this set of experiments to be flawed as they had become more of an operant conditioning experiment and as such made the results invalid. Finally there were no discontinuous leaps in Nim’s sentence structure and length as he increased his vocabulary, thus proving that the elements laid out by Chomsky as markers of language knowledge were simply absent from the beginning, showing them to be innate to humans, or at the very least, absent from chimpanzees.\nAdler, M. (2008). The Chimp That Learned Sign Language. Available: http://www.npr.org/2008/05/28/90516132/the-chimp-that-learned-sign-language. Last accessed 8th Jan 2017.\nChomsky, N (1986). Knowledge of Language: Its Nature, Origin and Use. New York: Praeger. p3.\nDubuc, B. (2010). Tool Module: Chomsky’s Universal Grammar. Available: http://thebrain.mcgill.ca/flash/capsules/outil_rouge06.html. Last accessed 8th Jan 2017.\nEvans, V (2014). The Language Myth. Cambridge: Cambridge University Press. p70-75.\nHart, S. (2015). Apes and Human Language. Available: acp.eugraph.com/apes/ . Last accessed 8th Jan 2017.\nHeine, B; Kuteva, T (2002). World Lexicon of Grammaticalization. Cambridge: Cambridge University Press.\nPetitto, L; Seidenberg, M. (1979). On the evidence for linguistic abilities in signing apes. Brain and Language. 8 (2), p162-183.\nPinker, S (1994). The Language Instinct: The New Science of Language and Mind. New York: William Morrow and Company. p20.\nPremack, D. (1971). Language in Chimpanzee?. Science. 172, p808-822.\nTomasello, M. (2013). Universals in formal linguistics. Available: http://www.ello.uos.de/field.php/CognitiveApproaches/UniversalsInFormalLinguistics . Last accessed 8th Jan 2017.\nWade N. (1980). Does Man Alone have Language? Apes reply in riddles, and a horse says neigh. Science. 208. p 1349-1351\nWynne, C. (2007). Aping Language: A Skeptical Analysis of the Evidence for Nonhuman Primate Language. Available: http://skepdic.com/essays/apinglanguage.html. Last accessed 8th Jan 2017.","Updated: Jul 26, 2021\nMeeting Your States of Consciousness\nPart II: Contemplation\nWe all need sleep, and for good reason—but all wish we could have that time back in our (conscious) life. Is there another state of consciousness that enhances our learning ability, balances our emotions, and repairs our body? If sleep is work, how about a career change?\nContemplation and Meditation\nContemplation; from the Latin contemplari, “to observe,” and meditation; from meditari, “to reflect” – together, they inform a mental practice which is distinct from ordinary thinking but is not exclusive from it. Contemplation and meditation are the meta-training for the mind: I wouldn’t meditate on a calculus problem but meditation, like sleep, makes learning and calculus an easier quest.\nBy endeavor, diligence, discipline, and self-mastery, let the wise man make (of himself) an island that no flood can overwhelm\n(Dhammapada, ‘sayings of the Buddha,’ Verse 25)\nContemplation and meditation—let’s just say, contempla-meditation—is taught in some form in every religion as a foundational practice for well-being. Brain scans and blood samples can never show the ineffable, inner experience of contempla-meditation, but we do have some grasp on how contempla-meditation transforms the mind. Contempla-meditation, like sleep, can take place in stages; in fact, being meta-training, these contempla-meditative stages correspond with certain cognitive modes:While I write this article, I am in “effortful processing” mode. I sustain my attention on the ultimate objective of synthesizing research on contempla-meditation, and I adjust my approach (and expectations) as I progress. Effortful processing can be trained by “controlled focus,” a contempla-meditation stage wherein the practitioner ruminates upon a selection of information. Catholicism teaches lectio divina, “sacred reading,” where the practitioner reads a passage of Scripture slowly and thoroughly and then hypothesizes its meaning by connecting the passage to other texts, by imagining one’s self in the scene described, by searching for subtext, and so on. Buddhism encourages similar practices, as in the famous lotus flower sermon: Gautama pulled a lotus up by its roots from a pond and held it aloft. His students gazed on the flower until one, Mahakasyapa, smiled—having understood the message of the lotus. Controlled focus elicits the same fast brain waves, gamma and beta (13-50 Hz), which signify effortful processing.\nRecall and reorganization of information without explicit problem-solving, or, planning, can be trained by open monitoring, where the practitioner permits their attention to wander around their sensory environment, their memories, or any experience at all. The practitioner engages with none of those thoughts. Lectio divina, Hindu Vedanta, Daoist Qigong, Zen Buddhism—most traditions encourage that controlled focus gives way to open monitoring.\nOpen monitoring, like planning, elicits theta brain waves (4-7 Hz), especially from the prefrontal cortex.\nWhen it seems that the mind ceases even to wander, the practitioner enters “self-transcendence.” The brain is still active: powerful alpha waves (8-12 Hz) surge throughout the default mode network (DMN). In waking life, large-amplitude alpha waves (10-12 Hz) suppress task-irrelevant processing and create an index or ‘ready’ state so that the brain can enter the other cognitive modes, and smaller alpha waves (8-10 Hz) signify mental imagery—usually, of the external world. In contempla-meditation, the focus shifts inward—effectively, the mind maps itself. DMN regions govern autobiographical reflection, including this mind-mapping, as well as theory of mind—our concept of how others think. Mental imagery filtered through this biographical-autobiographical scheme may explain the profound yet consistent perceptions of cosmic unity and interconnectedness which characterize self-transcendence.\nConsistent discipline in contempla-meditation induces progressive systemic changes in the form and function of the brain. A few months of practice grow the hippocampus, the medial prefrontal cortex, and the caudate nucleus of the basal ganglia. Working memory and attentional capacity increase, and anxiety and fatigue markedly decrease when handling new tasks. Old memories are also sent back to the hippocampus and reconsolidated just as in sleep with help from the caudate nucleus, working from the striatum of the basal ganglia. The caudate nucleus flexibly assesses goal-directed behavior and recommends new plans to the executive regions of the brain—especially those which govern problem-solving, verbal reasoning, empathy, and sociability. Bolstering these three structures also lays the groundwork for the long-term impact of contempla-meditation.\nOver months to years, continued practice realigns the brain with the priorities set by the caudate nucleus. Additional growth occurs in brain regions governing metacognition and self-regulation, such as the left rostrolateral prefrontal cortex and anterior cingulate cortex. Growth of the somatosensory cortices improves tactile acuity; coupled with improved processing in the insula, practitioners are able to consciously separate the emotional and sensory qualities of pain. Temporal lobe growth enhances visual imagery; with somatosensory improvements, this may augment spatial reasoning. So, whether you are a scholar, a student athlete, or a creative type—or all three—contempla-meditation can benefit you.\nThe hippocampus remains critical for the reassessment of old behaviors, and growth of the hippocampus, especially in glucocorticoid receptor density, further promotes emotional resilience and adaptability.\nStructural decreases are really only seen in specific nodes of the default mode network which are redundant in function to other nodes which increase in thickness. For example, the posterior cingulate cortex decreases, while the medial prefrontal cortex increases; both drive autobiographical recall and future planning, especially in regard to other people. Default mode network activity imbalanced in either direction results in mental dysfunction: hyperconnectivity correlates with depressive rumination, while hypoconnectivity correlates with avoidant patterns of attachment and autism.\nSo, all this sounds exciting—but will we have time for it? Can contempla-meditation replace the need for sleep?\nPartly. Total sleep deprivation will hurt contempla-meditation as much as it would hurt any form of learning, but up to three hours of sleep can be replaced by an equal time in contempla-meditation by experienced practitioners with no sacrifice in vigilance the next day. It’s equivalent to conventional sleep therapies like paradoxical intention and scheduled exercise. See, the brain activity elicited during contempla-meditation resembles the brain during sleep:\nWaking and dreaming (REM, rapid eye movement)) brain activity generate beta and gamma waves like in controlled focus, and intermediate NREM (memory consolidation) generates theta waves like in open monitoring. In fact, the augmented activity entrained during contempla-meditation redounds directly to the sleeping brain—and reappears.\nUsing an electroencephalographic map of the scalp, we can see differences between experienced meditators and novices emerge around the anterolateral regions of the cerebral cortex. The activity sites are consistent with the left rostrolateral and medial prefrontal cortices, the primary somatosensory cortex, and the middle temporal gyrus (which subsumes the hippocampus)—some of the cortical structures which develop most with long-term contempla-meditative practice. There could be a reciprocal relationship here: replicating the waking waves during sleep increases efficiency, and the efficient signaling during sleep assists the cortical growth.\nContempla-meditation cannot replace the deep delta-wave sleep, during which the body and brain repair themselves and clear out metabolic waste—but it’s clear that sleep and contempla-meditation are natural partners in creating a healthy, effective mind.\nWake up, go about the day, sleep, repeat—no more. Contempla-meditation serves as a perfect transition between waking and resting, adding a sense of calm and thoughtfulness to the day—and to ourselves!\nBasso, J., McHale, A., Ende, V., Oberlin, D., & Suzuki, W. (2019). Brief, daily meditation enhances attention, memory, mood, and emotional regulation in non-experienced meditators. Behavioural Brain Research, 356, 208-220. doi: 10.1016/j.bbr.2018.08.023\nBritton, W., Lindahl, J., Cahn, B., Davis, J., & Goldman, R. (2013). Awakening is not a metaphor: the effects of Buddhist meditation practices on basic wakefulness. Annals Of The New York Academy Of Sciences, 1307(1), 64-81. doi: 10.1111/nyas.12279\nChatterjee A, Ray K, Panjwani U, Thakur L, Anand JP. Meditation as an intervention for cognitive disturbances following total sleep deprivation. Indian J Med Res. 2012 Dec;136(6):1031-8. PMID: 23391801; PMCID: PMC3612308\nCooper, N., Burgess, A., Croft, R., & Gruzelier, J. (2006). Investigating evoked and induced electroencephalogram activity in task-related alpha power increases during an internally directed attention task. Neuroreport, 17(2), 205-208. doi: 10.1097/01.wnr.0000198433.29389.54\nDentico, D., Bachhuber, D., Riedner, B., Ferrarelli, F., Tononi, G., Davidson, R., & Lutz, A. (2018). Acute effects of meditation training on the waking and sleeping brain: Is it all about homeostasis?. European Journal Of Neuroscience, 48(6), 2310-2321. doi: 10.1111/ejn.14131\nFox, K., Nijeboer, S., Dixon, M., Floman, J., Ellamil, M., & Rumak, S. et al. (2014). Is meditation associated with altered brain structure? A systematic review and meta-analysis of morphometric neuroimaging in meditation practitioners. Neuroscience & Biobehavioral Reviews, 43, 48-73. doi: 10.1016/j.neubiorev.2014.03.016\nGrahn, J., Parkinson, J., & Owen, A. (2008). The cognitive functions of the caudate nucleus. Progress In Neurobiology, 86(3), 141-155. doi: 10.1016/j.pneurobio.2008.09.004\nHaber, S. (2003). The primate basal ganglia: parallel and integrative networks. Journal Of Chemical Neuroanatomy, 26(4), 317-330. doi: 10.1016/j.jchemneu.2003.10.003\nKandel, E. (2013). Principles of Neural Science (5th ed., pp. 986-981). Blacklick: McGraw-Hill Publishing.\nKang, D., Jo, H., Jung, W., Kim, S., Jung, Y., & Choi, C. et al. (2012). The effect of meditation on brain structure: cortical thickness mapping and diffusion tensor imaging. Social Cognitive And Affective Neuroscience, 8(1), 27-33. doi: 10.1093/scan/nss056\nKaul, P., Passafiume, J., Sargent, R., & O'Hara, B. (2010). Meditation acutely improves psychomotor vigilance, and may decrease sleep need. Behavioral And Brain Functions, 6(1). doi: 10.1186/1744-9081-6-47\nKlimesch, W., Doppelmayr, M., & Hanslmayr, S. (2006). Upper alpha ERD and absolute power: their meaning for memory performance. Progress In Brain Research, 151-165. doi: 10.1016/s0079-6123(06)59010-7\nRusch, H., Rosario, M., Levison, L., Olivera, A., Livingston, W., Wu, T., & Gill, J. (2018). The effect of mindfulness meditation on sleep quality: a systematic review and meta‐analysis of randomized controlled trials. Annals Of The New York Academy Of Sciences, 1445(1), 5-16. doi: 10.1111/nyas.13996\nScrivener, C., & Reader, A. (2021). Variability of EEG electrode positions and their underlying\nbrain regions: visualising gel artifacts from a simultaneous EEG-fMRI dataset. BioRxiv Preprint. doi: 10.1101/2021.03.08.434424\nSolomonova, E., Dubé, S., Blanchette-Carrière, C., Sandra, D., Samson-Richer, A., & Carr, M. et al. (2020). Different Patterns of Sleep-Dependent Procedural Memory Consolidation in Vipassana Meditation Practitioners and Non-meditating Controls. Frontiers In Psychology, 10. doi: 10.3389/fpsyg.2019.03014\nZhou, H., Chen, X., Shen, Y., Li, L., Chen, N., & Zhu, Z. et al. (2020). Rumination and the default mode network: Meta-analysis of brain imaging studies and implications for depression. Neuroimage, 206, 116287. doi: 10.1016/j.neuroimage.2019.116287"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8d3f915a-1ceb-4eda-8dc0-1605f5aa20af>","<urn:uuid:6d24dc71-eeda-4638-834f-3fc84fdeae7e>"],"error":null}
{"question":"What are the differences between common rail wiring in model railroads and the three-wire safety system in household electrical circuits?","answer":"Common rail wiring in model railroads involves one rail having electrical continuity throughout the layout without gaps, while the other rail (control rail) has electrical gaps for control purposes. In contrast, household electrical systems use a three-wire safety system where the neutral wire provides the return path for current, the live/hot wire supplies voltage and current to operate appliances, and a third earth/ground wire provides additional safety by forcing the appliance case to be at zero volts. The model railroad system focuses on controlling train operation, while the household system prioritizes safety through multiple grounding points and protective features.","context":["Atlas Wiring Glossary\nAcute Frog - The two frogs in a crossing which are less than 90°.\nAmmeter - A device which is used to measure amperes.\nA.C. (Alternating current) - An electric current that reverses\ndirection in a circuit at regular intervals. Ordinary house current\nis alternating current.\nAmpere - A measurement of current flow, example one ampere\nthrough an ohm resistor will give one current.\nBlock - An electrically isolated track section that can\nBrass - A yellow metal alloy consisting of copper and zinc,\na good electrical conductor.\nCab - A power pack used to control model trains.\nCab Control - A method of wiring a layout to allow control\nelectrical blocks, which may be passed back and forth between cabs.\nCircuit - A closed path through which an electrical current may flow.\nCommon Lead - A wire from the power supply to the common, or ungapped rail.\nCommon Rail - The rail in a model railroad layout without electrical gaps.\nCommon Rail Wiring - A method of wiring your layout where\nhas electrical continuity throughout the layout.\nConnector (ATLAS Item #205) - A device\nmanufactured by Atlas to turn\nelectric power on or off to track sections or accessories.\nController (ATLAS Item #220) - An electrical device manufactured\nATLAS, which provides the proper operation of reversing loops and\nControl Rail - The rail in a model railroad layout which has electrical gaps.\nCrossing - The point where two railroad tracks cross each\nCurrent - Electricity flowing in a circuit. Current flows\npositive (+) to negative (-); comparable to water flow through a pipe.\nCycle - One complete period of the reversal of an alternating\nfrom positive to negative and back again.\nD.C. (Direct Current) - An electric current that flows in one direction only.\nDogbone - A configuration of model railroad track which looks like a dogbone.\nDouble Pole Double Throw (DPDT) - An electrical switch that\nadapted to reverse the flow of electric current. Many DPDT switches\nhave an \"off\" position in the center. (a.k.a. - Two single pole\nsingle throw switches mounted and operating together.)\nDouble Track Wye - A track configuration in which two parallel\nboth create wyes. Traversing the complete double track wye will\nresult in the train running on the opposite track.\nDrill- To switch a train or cars from one track to another.\nFlange - The protruding edge on a wheel which acts as a guide against the rail.\nFlux - A paste used in soldering to assist the molten solder to flow freely.\nFrog - The location where two rails cross in a turnout or crossing.\nGap - An electrical break in a rail. In a common rail system,\nwould be in the control rail. The exception would be if there is a\nreversing section or wye; in which case the gap would be in both\nGauge - The distance between the rails of a railroad track,\nfrom the inside of the railheads.\nReal Railroad (Standard) = 4' 08 1/2\"\nN Scale Railroads = 9mm\nHO Scale Railroads = 16.5mm\nO Scale Railroads = 31.8mm\nInsulated Joiner - A plastic clip that joins two sections\nmechanically, but isolates them electrically.\nInsulated Frog - A frog that is electrically isolated from the rails.\nLadder Track - The track in a ladder yard from which all\nLayout - A model railroad.\nMain Line - The primary railroad track that carries through traffic.\nMulti-meter - An electrical instrument consisting of an\nohmmeter and voltmeter, all contained in one casing.\nNMRA - National Model Railroad Association\n4121 Cromwell Road\nChattanooga, TN 37421\nThe NMRA sets standards for the compatibility of model railroad\nequipment and is dedicated to the promotion and improvement of the\nNickel Silver - A silvery colored metal alloy consisting\nzinc and nickel. A good electrical conductor.\nOhm - A unit of electrical resistance.\nOhmmeter - A device used to measure ohms. Also used to find a shortcut.\nPassing Siding - A double-ended siding, long enough for\ntwo trains to\npass one another.\nPoints - The moving portion of a turnout, usually two connected\nrails in a turnout that move to change a train's route.\nPolarity - The direction a current moves through a circuit.\nPower - The work done by an electrical current. the unit of power is the watt.\nPower Pack - An electrical device that plugs into a household\nand converts 110 volts AC to 12 volts DC. HO and N scale model\nrailroads use 12 volts DC. (ATLAS Item #310)\nPrototype - The actual full-sized object a model is patterned after.\nRail Joiner - A small metal clip used to join two sections\nmechanically and electrically.\nRectifier - An electrical device which converts AC current to DC current.\nRelay - A power operated electrical switch. (See also, \"Snap-Relay\")\nRemote Control Turnout - A turnout (switch) whose points\nelectrically operated from a remote position.\nResistance - The property of a conductor which limits or\npassage of an electrical current. The amount of resistance is\nmeasured in ohms.\nResistor - An electrical device which limits the passage\nelectrical current. The value of a resistor is expressed in ohms.\nReverse Loop - A section of track that goes around in a\ncomes back on itself and is joined by a turnout at this point. A\nreverse loop can resemble a balloon on the end of a string, or, in\nmore complex plans, reverse loops/reversing sections may be less\nRheostat - An electrical device used to vary the amount\ndelivered to a motor, thus acting as a speed control. Also called a\nScale - The proportional size of model railroad items compared\nreal thing (prototype).\nN Scale - 1/160th the size of the real item\nHO Scale - 1/87th the size of the real item\nO Scale - 1/48th the size of the real item\nSelector - (ATLAS Item #215) An Atlas electrical device\nthe operation and control of two trains at the same time.\nShort Circuit - A usually unintended flow of current between\npoints, bypassing the normal electrical circuit.\nSiding - A side or secondary track, also called a \"spur\".\nSingle Pole Single Throw (SPST) - A simple \"On-Off\" electrical switch.\nSnap-Relay - (ATLAS Item #200) A DPDT relay with actuating\ndriven by a momentary-power, twin coil solenoid.\nSolder - A soft, wirelike metal comprised of tin and lead,\nwhen melted, is used to join two pieces of metal or wire.\nSoldering Iron - A device used to melt solder and accomplish\nSolenoid - A cylindrical coil of insulated wire in which\nan iron core\nis made to move back and forth by a flow of electric current.\nSPDT - An electrical switch that can allow power to be supplied\nany two components from one source.\nSpur - See Siding.\nStock Rail - The two outer rails in a turnout.\nStub Track - A track that comes to a dead end. Used for\nlocomotives and/or cars, or to lead to a structure on your layout.\nSwitch - 1. A device to route train wheels from one set\nof rails to\nanother by means of moveable points. Also called a \"turnout\". 2. An\nelectrical device that routes, reverses or interrupts the flow of\nSwitch Machine - A device, either manual or electrical,\nconnected to the points of a turnout to change the setting of the\nTender - The car directly behind a steam locomotive that\nfuel and water for the engine.\nTerminal Joiners - Rail joiners with wires affixed. Used\nelectricity from the power pack to the rails.\nTerminal Section - Also known as terminal track. A section\nrailroad track with attachments for wire. Used to conduct electricity\nfrom the power pack to the rails. Terminal sections may be straight\nTerminal Strip - A series of screw or clip-on wire terminals\non a strip of insulating material.\nThrowbar - The bar connecting the two point rails in a turnout.\nTin (or Tinning) - In soldering, a thin coat of solder on\nthe tip of\na soldering iron, on the surfaces of two pieces of metal to be\njoined, or on a piece of wire.\nToggle Switch - One of many variations of electrical switch.\ntypes of toggle switches include double pole, double throw; single\npole, single throw. etc.\nTrack Section - See \"Block\"\nTransformer - An electrical device which either increases\ncommonly, decreases electrical voltage.\nTrucks - The device beneath railroad cars or locomotives that hold the wheels.\nTurnout - A section of track with moveable rails to divert a train\nfrom one track to another. Same as a switch; that point where two\ndiverging tracks come together.\nTwin - (ATLAS Item #210) An Atlas electrical component consisting\ntwo double pole, double throw reversing switches wired in parallel.\nVolt/Voltage - A unit of electromotive force, comparable\nto water in\na water tank/tower.\nVoltmeter - A device used to measure volts.\nWatt - A unit of work done by an electrical current (power).\nWye - A triangular shaped track arrangement, using three\nwhere trains may be reversed. Can also refer to one turnout with two\ncurved diverging routes instead of in a straight and one curved\nroute, as is commonly found.\nX-Sections - A section of track, electrically isolated,\ntemporarily \"x\"-tend the length of adjacent sections (blocks).\nYard - A group of side tracks where railroad cars are stored\ntrains are made or broken up prior to or after a run.\nCopyright © 2006 Atlas Model Railroad Co., Inc. All Rights Reserved. \"You're on the Right Track®...With Atlas\"","23-9. Electrical Safety: Systems and Devices\nElectricity has two hazards. A\nFigure 1 shows the schematic for a simple AC circuit with no safety features. This is not how power is distributed in practice. Modern household and industrial wiring requires the\nThere are three connections to earth or ground (hereafter referred to as “earth/ground”) shown in Figure 2. Recall that an earth/ground connection is a low-resistance path directly to the earth. The two earth/ground connections on the neutral wire force it to be at zero volts relative to the earth, giving the wire its name. This wire is therefore safe to touch even if its insulation, usually white, is missing. The neutral wire is the return path for the current to follow to complete the circuit. Furthermore, the two earth/ground connections supply an alternative path through the earth, a good conductor, to complete the circuit. The earth/ground connection closest to the power source could be at the generating plant, while the other is at the user’s location. The third earth/ground is to the case of the appliance, through the green earth/ground wire, forcing the case, too, to be at zero volts. The live or hot wire (hereafter referred to as “live/hot”) supplies voltage and current to operate the appliance. Figure 3 shows a more pictorial version of how the three-wire system is connected through a three-prong plug to an appliance.\nA note on insulation color-coding: Insulating plastic is color-coded to identify live/hot, neutral and ground wires but these codes vary around the world. Live/hot wires may be brown, red, black, blue or grey. Neutral wire may be blue, black or white. Since the same color may be used for live/hot or neutral in different parts of the world, it is essential to determine the color code in your region. The only exception is the earth/ground wire which is often green but may be yellow or just bare wire. Striped coatings are sometimes used for the benefit of those who are colorblind.\nThe three-wire system replaced the older two-wire system, which lacks an earth/ground wire. Under ordinary circumstances, insulation on the live/hot and neutral wires prevents the case from being directly in the circuit, so that the earth/ground wire may seem like double protection. Grounding the case solves more than one problem, however. The simplest problem is worn insulation on the live/hot wire that allows it to contact the case, as shown in Figure 4. Lacking an earth/ground connection (some people cut the third prong off the plug because they only have outdated two hole receptacles), a severe shock is possible. This is particularly dangerous in the kitchen, where a good connection to earth/ground is available through water on the floor or a water faucet. With the earth/ground connection intact, the circuit breaker will trip, forcing repair of the appliance. Why are some appliances still sold with two-prong plugs? These have nonconducting cases, such as power tools with impact resistant plastic cases, and are called doubly insulated. Modern two-prong plugs can be inserted into the asymmetric standard outlet in only one way, to ensure proper connection of live/hot and neutral wires.\nElectromagnetic induction causes a more subtle problem that is solved by grounding the case. The AC current in appliances can induce an emf on the case. If grounded, the case voltage is kept near zero, but if the case is not grounded, a shock can occur as pictured in Figure 5. Current driven by the induced case emf is called a leakage current, although current does not necessarily pass from the resistor to the case.\nA ground fault interrupter (GFI) is a safety device found in updated kitchen and bathroom wiring that works based on electromagnetic induction. GFIs compare the currents in the live/hot and neutral wires. When live/hot and neutral currents are not equal, it is almost always because current in the neutral is less than in the live/hot wire. Then some of the current, again called a leakage current, is returning to the voltage source by a path other than through the neutral wire. It is assumed that this path presents a hazard, such as shown in Figure 6. GFIs are usually set to interrupt the circuit if the leakage current is greater than 5 mA, the accepted maximum harmless shock. Even if the leakage current goes safely to earth/ground through an intact earth/ground wire, the GFI will trip, forcing repair of the leakage.\nFigure 7 shows how a GFI works. If the currents in the live/hot and neutral wires are equal, then they induce equal and opposite emfs in the coil. If not, then the circuit breaker will trip.\nAnother induction-based safety device is the isolation transformer, shown in Figure 8. Most isolation transformers have equal input and output voltages. Their function is to put a large resistance between the original voltage source and the device being operated. This prevents a complete circuit between them, even in the circumstance shown. There is a complete circuit through the appliance. But there is not a complete circuit for current to flow through the person in the figure, who is touching only one of the transformer’s output wires, and neither output wire is grounded. The appliance is isolated from the original voltage source by the high resistance of the material between the transformer coils, hence the name isolation transformer. For current to flow through the person, it must pass through the high-resistance material between the coils, through the wire, the person, and back through the earth—a path with such a large resistance that the current is negligible.\nThe basics of electrical safety presented here help prevent many electrical hazards. Electrical safety can be pursued to greater depths. There are, for example, problems related to different earth/ground connections for appliances in close proximity. Many other examples are found in hospitals. Microshock-sensitive patients, for instance, require special protection. For these people, currents as low as 0.1 mA may cause ventricular fibrillation. The interested reader can use the material presented here as a basis for further study.\nDoes plastic insulation on live/hot wires prevent shock hazards, thermal hazards, or both?\nWhy are ordinary circuit breakers and fuses ineffective in preventing shocks?\nA GFI may trip just because the live/hot and neutral wires connected to it are significantly different in length. Explain why.\nProblems & Exercises\nA short circuit to the grounded metal case of an appliance occurs as shown in Figure 9. The person touching the case is wet and only has a resistance to earth/ground. (a) What is the voltage on the case if 5.00 mA flows through the person? (b) What is the current in the short circuit if the resistance of the earth/ground wire is ? (c) Will this trigger the 20.0 A circuit breaker supplying the appliance?\n(a) 15.0 V\n(b) 75.0 A"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:947e6d5b-4e0a-455a-8fd8-a653885769c7>","<urn:uuid:c0be49c4-1552-4b5b-bf42-461a55dce1fc>"],"error":null}
{"question":"What is the key difference between the refractory period in cardiac muscle versus skeletal muscle, and how does this affect their contraction patterns?","answer":"The key difference is that cardiac muscle has an extremely long refractory period due to its prolonged plateau phase caused by L-type Ca2+ currents, while skeletal muscle has a relatively short refractory period. As a result, skeletal muscle can fire repeated action potentials before relaxation begins, allowing tetanic summation. In contrast, cardiac muscle cannot undergo tetanic summation because the duration of the action potential is essentially the same as the duration of the twitch, meaning the muscle is nearly relaxed before another action potential can be fired.","context":["By the end of this section, you will be able to:\n- Describe intercalated discs and gap junctions\n- Describe a desmosome\nCardiac muscle tissue is only found in the heart. Highly coordinated contractions of cardiac muscle pump blood into the vessels of the circulatory system. Similar to skeletal muscle, cardiac muscle is striated and organized into sarcomeres, possessing the same banding organization as skeletal muscle (Figure 10.21). However, cardiac muscle fibers are shorter than skeletal muscle fibers and usually contain only one nucleus, which is located in the central region of the cell. Cardiac muscle fibers also possess many mitochondria and myoglobin, as ATP is produced primarily through aerobic metabolism. Cardiac muscle fibers cells also are extensively branched and are connected to one another at their ends by intercalated discs. An intercalated disc allows the cardiac muscle cells to contract in a wave-like pattern so that the heart can work as a pump.\nView the University of Michigan WebScope to explore the tissue sample in greater detail.\nIntercalated discs are part of the sarcolemma and contain two structures important in cardiac muscle contraction: gap junctions and desmosomes. A gap junction forms channels between adjacent cardiac muscle fibers that allow the depolarizing current produced by cations to flow from one cardiac muscle cell to the next. This joining is called electric coupling, and in cardiac muscle it allows the quick transmission of action potentials and the coordinated contraction of the entire heart. This network of electrically connected cardiac muscle cells creates a functional unit of contraction called a syncytium. The remainder of the intercalated disc is composed of desmosomes. A desmosome is a cell structure that anchors the ends of cardiac muscle fibers together so the cells do not pull apart during the stress of individual fibers contracting (Figure 10.22).\nContractions of the heart (heartbeats) are controlled by specialized cardiac muscle cells called pacemaker cells that directly control heart rate. Although cardiac muscle cannot be consciously controlled, the pacemaker cells respond to signals from the autonomic nervous system (ANS) to speed up or slow down the heart rate. The pacemaker cells can also respond to various hormones that modulate heart rate to control blood pressure.\nThe wave of contraction that allows the heart to work as a unit, called a functional syncytium, begins with the pacemaker cells. This group of cells is self-excitable and able to depolarize to threshold and fire action potentials on their own, a feature called autorhythmicity; they do this at set intervals which determine heart rate. Because they are connected with gap junctions to surrounding muscle fibers and the specialized fibers of the heart’s conduction system, the pacemaker cells are able to transfer the depolarization to the other cardiac muscle fibers in a manner that allows the heart to contract in a coordinated manner.\nAnother feature of cardiac muscle is its relatively long action potentials in its fibers, having a sustained depolarization “plateau.” The plateau is produced by Ca++ entry though voltage-gated calcium channels in the sarcolemma of cardiac muscle fibers. This sustained depolarization (and Ca++ entry) provides for a longer contraction than is produced by an action potential in skeletal muscle. Unlike skeletal muscle, a large percentage of the Ca++ that initiates contraction in cardiac muscles comes from outside the cell rather than from the SR.","What is the effective (absolute) refractory period (ERP)? What is the relative refractory period (RRP)? Explain what channels in cardiac myocytes lead the cardiac AP to have such a long refractory period and why cardiac APs cannot summate.\nERP: defines the time period before another AP of any kind can be evoked. Note that in cardiac muscle, the muscle has begun to relax before the ERP is over and even more so after the relative refractory period. During the RRP, another AP can be evoked but it will be weak and will conduct slowly.\nAs we know, the cardiac AP is much longer than in other cell types due to the L-type Ca2+ current that flows during the plateau and drives Vm toward Eca (120) mV. This is balanced by delayed rectifier K+ currents that work to bring Vm toward Ek (-95mV). As a result of the plateau, the ERP is extremely long. While Vm is depolarized, Na channel inactivation gates remain closed (remember these gates open upon repolarization). The channel doesnt begin recovering from inactivation until repolarization begins after the plateau phase. They begin to recover during the RRP but not completely.\nIn skeletal muscle, the refractory period is relatively short and so repeated APs can be fired well before relaxation begins. No tetanic summation of cardiac muscle twitches is possible bc the duration of the AP is essentially the same as the duration of the twitch-cardiac muscle is nearly relaxed before another AP can be fired.\nWhat 3 factors change the force of contraction?\n1. length of fibers at time of contraction: Frank-Starling effect\n3. contractility (IS)\nHow does the Frank-Starling effect impact force development? By what mechanisms is this though to possibly occur? What factors determine contractility? How is this different from contractility?\nIncreased lenght of myofibers results in increased force development. This is different from increased contractility, which is an increase in force developed at the same muscle length. The Frank-Starling effect increases EDV (preload) so does not effect inotropic state (fibers stretch more, can fill more, and contract with more force). Anything that is not considered preload or afterload is inotropic state. A change in IS is completely independent from changes in fiber length and is distinct from the Frank-Starling effect, which is an inherent property of the muscle in the absence of extrinsic factors. IS is effected by [Ca2]i, number of functional myocytes, and coronary artery supply.\nThe increase in force with an increasing length (Frank-Starling effect) may result from:\n1. a change in the lateral spacing btwn thick and thin filaments such that greater force developement is achieved.\n2. increased myofilament Ca2+ sensitivity\nWhy is the effect of afterload on SV limited? What is this phenomenon called?\nhomeometric autoregulation: Increased afterload → increased end systolic volume\n→ increased end diastolic volume → Frank-Starling\nIncreased stretch also causes the release of angiotensin II and endothelin that increase the inward flux of Ca2+: positive inotropic effect: also counters effects of\nConsider the attached pressure volume loop. If the aortic valve is insufficient, where will point A be relative to point B? Point C relative to point D?\npoint B goes to the right of A. pressure in aorta is greater than in ventricles so blood flows into ventricles\nC moves to the left of D: when aortic valve should be closed, blood leaks from the insufficient valve into the lower pressure ventricle and increases ventricular volume\nGOOD TEST QUESTION\nWhat is the physiologically predominant determinat of CO?\nBc HR varies over a wider range, when the body wants to increase CO it largely looks towards HR. Also, when the sympathetic NS increases both SV and HR, the effect on HR dominates.\nOf the determinants of SV, which is the most potent?\nIncreased EDV stretches the ventricular walls and increases the length of the cardiac muscle fibers. This causes increased force of contraction (Frank-Starling effect) and thus increased SV.\nWhat effect does increased wall tension have on a normal heart and a diseased heart? (hint: Law of LaPlace)\nExplain the reason for the shift in this curve with NE.\nWhen left ventricular EDV (LVEDV) increases, this increased end diastolic pressure and distends the ventricular walls. This stretches the ventricular msucel and an increase in the force of contraction results as noted in the figure. This manifests itself on the whole heart level as an increase in SV.\nIncreasing the IS of cardiac muscle (NE) results in greater ventricular output at any level of left ventricular fills. Increased IS is represented by an upward shift of the Starling curve, reprsented as stroke volume (eq to force development) vs left ventricular end diastolic pressure (equivalent to muscle length)\nExplain the reason for the shift in the ESPVR line in the attached pic with increased IS. (also answer what this line is). Which way does the graph shift in dilated cardiomyopathy and why?\nAs a heart beats over time, there are tiny shifts in the parameters that determine the force of contraction at every beat. For example, preload is never exactly the same on a beat-to-beat basis. End systolic pressures will fall along this line dependent upon the preload in th system. A higher preload will result in a higher ESP bc force of contraction increases. A lower preolad will result in a lower ESP bc force of contraction decreases. In each case, the end systolic pressure falls somewhere on the diagonal line.\nIf there is an increase in IS, the line shifts. Compared to the normal muscle, ESP increases and ESV decreases (i.e. SV increases) for the same preload (i.e. same initioal filling.\nThe graph shift down and to the right for a pt with dilated cardiomyopathy due to negative inotropic state.\nTrue or false: Inotropic state often serves to maintain CO when preload is decreased."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8b92e0a1-7794-411a-9b74-938b1032a53e>","<urn:uuid:857ff3aa-4151-47c8-ba8d-7da540c6d769>"],"error":null}
{"question":"How do weather conditions in Shanghai during summer months affect the recommended provisions in sales representation agreements for that region?","answer":"Given Shanghai's intense summer weather conditions, with temperatures reaching up to 40°C and heavy rainfall of over 150mm in June and September, plus the risk of typhoons from May to November, sales representation agreements for this region should include specific provisions for force majeure and liability protection. The agreement should particularly include the indemnification clause that protects the company from 'any and all claims, demands, costs, expenses or liabilities' arising from the representative's actions, which becomes especially relevant when severe weather conditions like typhoons could affect business operations and require evacuation of 1.6 million people as happened in September 2007.","context":["Detailed Sample Contract Agreement For Sales Representatives\nCategory: Sales Representatives\nWhy a Sales Representation Agreement?\nSigning an agreement with a sales representative formalizes the link between the representative and your company. It is strongly recommended when working with sales representatives in other countries to sign a sales representation agreement. This contract must define in details all the terms and conditions of the sales representation.\nIn this page we are going to define the clauses that your contract should contain. Not all clauses are mandatory; the agreement depends essentially on your industry, the products and services you sell, and the countries you are dealing with.\nAttention: The clauses contained thereafter are only given as examples. Some clauses will not be applicable; they might sometimes be contradicting local laws. It is strongly advised to have your sales representation agreement reviewed by a lawyer who specializes in the country where you want to sell your products.\nDefinition of Parties and Date of the Agreement\nYour company and the sales representatives must be clearly defined. It is usual to mention the name, address, and industry of the parties. Here is a typical introduction: This AGREEMENT is made as of the _[day]_ day of _[month]_, _[year]_, by and between __[name of sales representative]_ , whose address is _[address of sales representative]_, hereinafter referred to as \"Sales Representative\" and _[name of company]_, whose address is _[address of company]_, hereinafter referred to as \" Company\". WHEREAS, _[name of company]_ is engaged in the business of marketing and selling _[name of products]_, and WHEREAS, _[name of company]_ wishes to engage Sales Representative to provide sales solicitation, installation, training and other services. NOW THEREFORE, it is agreed as follows:\nTerritory and Products Definition\nThe sales agreeement must precisely define which products and services your sales representatives can sell. The agreement must also define the territory that the sales representative will cover.\nYou must determine if the sales rep is exclusive or not exclusive on the territory. You can also define the conditions upon which the sales representative is authorized to sell outside of his territory.\nCompany hereby appoints Sales Representative as an authorized __[exclusive / non-exclusive]__independent representative to sell and promote all services provided by Company in the following geographical area: __[geographical territory]__, hereinafter referred to as \"Territory\".\nIn the case of an exclusive agreement, the following sentence will be added:\nSales Representative shall be protected in the Territory so long as Sales Representative adheres to the terms and conditions described herein.\nIf you wish to define sales outside of the territory defined in the agreement:\nSales made to corporations residing outside of the Territory are subject to the following commissions: __[define commission structure]__. Notwithstanding the foregoing, Sales Representative may not sell the Products outside the Territory without the express written consent of the Company's _[person authorized to give consent]_.\nYou can also exclude certain types of products and services from the agreement:\nThis Agreement excludes the sale of the following products and services: __[define products and services excluded]__.\nIndependance of Sales Representative\nYour agreement must clearly define whether the sales representative is an independant party or if he has any ties to your company. Below is an example for an independant sales representative.\nIn making this appointment, Sales Representative is and shall remain an independent contractor. This agreement shall not constitute a partnership, joint venture, co-ownership or otherwise. Sales Representative shall not create or assume any obligation on behalf of Company for any purpose whatsoever. Sales Representative is not an employee of Company and is not entitled to any employee benefits. Sales Representative shall be responsible for paying all income taxes and other taxes charged to Sales Representative on amounts earned hereunder. All financial and other obligations associated with Sales Representative's business are the sole responsibility of Sales Representative.\nIf you want to empower your sales representative for some contractual aspects with the customer, you can use the section below:\nSales Representative shall not represent that Sales Representative has the power or authority to enter into any agreements or contractual obligations on behalf of Company unless Company provides a separate letter of authorization authorizing Sales Representative to execute agreements on behalf of Company.\nAn independant sales representative will be paid based on a commission structure. Be aware that if you offer your sales representative a fixed portion of payment regardless of sales, the sales representative can be considered an employee of your company. His statute will then be entirely different (he can for example sign contracts on behalf of your company, engage the responsability of your company, etc.)\nMake sure also to mark the difference between a sales representative and a distributor. A sales representative is an intermediary that gets a commission on the sale of your products. A distributor buys your products and resells them. A distributor should not get a commission.\nNevertheless, it is common to find sales representatives that get a \"double pay\": they get a commission on the sale, and they buy the product to resell it for a profit. These sales representatives get paid twice. Avoid them at all cost. The final price of the product is often outrageously expensive compared to the direct sales price. The end customers lose because they buy the product at a premium (and more often than not they figure it out). And your company's credibility is on the line. The only winner is the sales representative, but he will probably not sell many products this way.\nFor a commission with a fixed percentage:\nThe commission shall be [xxx] % of the net selling price.\nor for a variable commission depending on the price of the products:\nCommissions shall be based on the following compensation schedule:\nFor net selling prices to Representative of up to [define threshold amount], the commission shall be [xxx] % of the net selling price. For net selling prices above [same threshold amount as previous], the commission shall be [constant sum corresponding to the commission earned for the threshold amount] plus [yyy] % of the amount above [threshold amount].\nDefine precisely how commissions are calculated. For example:\nCommissions shall be computed on the net amount of the invoice after deducting discounts, allowances, shipping charges, travel and living expenses for a field service representative, taxes, cost of collection if any, rebates, and returns. Commissions shall be payable on or before the 15th day of the month following the month in which payment is made by the customer.\nConflict of Interest\nIt is recommended that you include a non-competition clause in your agreement, so that your sales representative cannot sell products from your competitors. This clause is often non applicable to distributors, which can resell the products of competitors.\nSales Representative warrants to Company that it does not currently represent or promote any lines or products that compete with the Products. During the term of this Agreement, and for a period of _[define time period]_after its termination, Sales Representative shall not represent, promote or otherwise try to sell within the Territory any lines or products that compete with the Products covered by this Agreement.\nTechnical and Commercial Support\nIn the course of your agreement with your sales representatives, you will have to give them loads of technical and commercia data. In some cases you will even have to give or loan them a demo unit of your product. Your sales representative can ask you to include a clause mentioning that you are giving them these technical and commercial data and tools free of charge.\nCompany agrees to provide Sales Representative, at no cost, written materials relating to the Products necessary to support product promotion and sales efforts.\nYou will also have to provide technical and commercial offers to your sales representatives, so they can present them to their customers.\nCompany agrees to respond, at no cost and within reasonable time, to requests for price and delivery of the Products covered by this Agreement.\nIn order to help your sales representatives sell your products, you have to give them information about your markets and your clients. You also have to give them technical data on your products. Make sure you protect your interests by including a confidentiality agreement, so that the sales representative cannot communicate this information to your competitors. The confidentiality agreement must be valid at least for the duration of the contract, and it can also extend beyond the termination of the agreement.\nDuring the term of this Agreement or within _[define time period]_ after its termination, Sales Representative shall not compete with Company, directly or indirectly, in the sale or promotion of products or services the same as or similar to Company's products and services within the Territory. Under no circumstances and at no time shall Sales Representative disclose to any person any of the secrets, methods or systems used by Company in its business. All customer lists, brochures, reports, and other such information of any nature made available to Sales Representative by virtue of Sales Representative's association with Company shall be held in strict confidence during the term of this Agreement and after its termination.\nTrademarks and Patents\nMake sure you protect your trademarks and patents in the countries where you commercialize your products. Trademarks in many countries are the property of the first person who registers them. Explicitly forbid your sales reps to register your trademarks or trademarks that could be confused with yours.\nAt no time during or after the term of this Agreement shall Sales Representative challenge or assist others to challenge Company's Trademarks or the registration thereof or attempt to register any trademarks, marks or trade names confusingly similar to those of Company.\nContract Duration and Termination\nYour sales representation agreement must include a duration. It must describe how you plan on renewing it and how you can terminate it. Be aware that some countries have strict laws regarding the minimum termination notice.\nBelow is an example of a contract that renews automatically from year to year, with a 30 days termination notice:\nThis Agreement shall extend from the date hereof for a period of [duration] and, unless sooner terminated by either party, will continue from year-to-year thereafter. This Agreement may be unilaterally terminated at any time by either party with or without cause, by giving thirty (30) days written notice.\nIt can be useful to mention that the agreement cannot be transfered to a third party.\nThis Agreement is not assignable by either party without the written consent of the other.\nIt is suggested to include the following section, to clarify that the written contract is the only agreement in force.\nThis Agreement is the entire agreement between Company and Sales Representative. This Agreement supersedes all prior agreements and understandings, whether written or oral, of the subject matter contained herein. This Agreement may be amended only in writing by an authorized officer of Company and Sales Representative.\nIn the case where part of the agreement is ruled illegal by local laws, it is prudent to include a sentence mentioning that the rest of the contract remains in place. This is extremely important in foreign countries where local laws can differ significantly from the legal system you are used to.\nIn the event that any portion of this Agreement is found to be void, illegal or unenforceable, the validity and enforceability of any other portion shall not be affected.\nIt is recommended to include the following clause in your agreement, so that the sales representative does not pass on liabilities and claims from the customer to your company.\nSales Representative agrees to indemnify and hold harmless Company from any and all claims, demands, costs, expenses or liabilities, including reasonable attorneys' fees, brought or imposed upon Company, relating to or arising out of any acts, omissions, fraud, misrepresentation or wrongdoing by Representative in connection with the performance of this Agreement.\nThe dispute resolution method must be defined. In particular the choice of mediation if it is preferred must be mentioned. The competent jurisdiction is an important question. It can be extremely costly to face trial in a foreign country. It is also hazardous to submit your contract to the laws of a foreign country.\nAny and all disputes, controversies, claims, or other disagreements arising out of or relating to this Agreement or the actual or alleged breach thereof shall be finally settled through arbitration in accordance with rules of [the American Arbitration Association]. The arbitration shall be held in [State or Country] and shall be conducted under and in accordance with [the American Arbitration Association Rules] applicable to [State or Country]. Such arbitration shall be conducted in English and will be conducted on confidential basis in accordance with the terms of the Agreement.\nThis Agreement shall be governed by and construed according to the laws of [State or Country].\nIn The Same Category• Evaluate The Performance Of Your Sales Representatives\n• Find Sales Representatives in Europe\n• What Are The Qualities Of A Good Sales Representative?\n• How To Become An Independent Sales Representative\n• Manage Your Sales Representatives\n• Prices and Commissions Policies For Sales Representatives\n• The Different Types Of Sales Representatives\n• How To Find Qualified Sales Representatives\n• Why Use Sales Representatives?","Weather Overview for Shanghai\nShanghai sits on the vast\nYangtze River delta on China’s east\ncoast, around half way between Hong Kong and Beijing. With a\nmassive 20 million inhabitants, it is large city and one of the world’s largest\nhas a humid subtropical climate marked by four distinct seasons. Climate is strongly\ninfluenced by the Asiatic monsoon winds and the city’s coastal position on the East China Sea. Temperatures vary from the mid 30s in\nsummer to below 0°C in winter. The weather is pretty good year round, although\nsummer can get a little hot and muggy for some and winter is pretty harsh. On\nbalance, autumn enjoys the best weather of the year.\nThis is the hottest and wettest time of year in Shanghai. In June the\naverage temperature climbs to 24°C with highs averaging at 27°C. July and\nAugust are the hottest months of the year when the average temperature reaches\nhighs climb to 32°C and beyond - even up to 40°C - and it doesn’t drop much\nbelow 25°C at night. From September it starts to cool, but it is still a very hot\nThe summer heat is often punctuated by heavy rains, usually\nfalling in afternoon thunderstorms that are heavy but short-lived. The heat and\nprecipitation make summer very humid and the overall effect is oppressive. The\nwettest months are June and September with over 150mm average\nrainfall each. July and August average around 130mm. Summer thunderstorms\ncan sometimes bring very heavy rain to Shanghai\nand there is a risk of flooding in certain parts of the city.\ncan occasionally be hit by typhoons, although direct hits are rare. These\nintense storms form in the northwest Pacific Ocean\nand if they make land they can cause enormous damage. Typhoon season runs from\nMay to November.\nSeptember 2007 saw Shanghai\ncome in the path of a very strong typhoon that led to 1.6million people being\nevacuated from the city and its surrounds. The region is very well prepared,\nhowever, and often it is just a case of closing the doors and windows and\nsheltering indoors for 24 hours.\nThis is one of the best times to visit Shanghai. Temperatures are very comfortable\nand rainfall is low. Late September and early October are\nespecially good times; the summer rains have died down and temperatures range\nbetween the mid teens and mid 20s. October averages just 60mm rainfall, a sharp\ndecline from the 160mm that is the norm for September. November rainfall is\neven less around 50mm and it’s another great month, though it sees the first\ncool temperatures with night time lows dropping to single figures. Daily highs\nare still in the high teens, however.\nWinter can be cold in Shanghai,\nalthough its coastal position does mean temperatures don’t drop as low as in\ninland areas. January is the coldest month of the year, with an average of 4°C,\nranging from lows of 0°C to highs of 8°C. December and January are only a\nlittle warmer, averaging around 6°C or 7°C. While temperatures\nbelow zero are rare they can occur during particularly cold snaps, and snow can\neven fall. Winter is the driest time of the year in Shanghai, and the abundance of sunshine\nmakes it very pretty. However, strong winds can make it feel much cooler than\nthe recorded temperature.\nSpring is another good time to visit Shanghai. Temperatures are a little cooler\nthan autumn and rain a little more frequent but the weather is generally very\npleasant. March is still cool, with an average of 9°C, but temperatures quickly\nincrease throughout spring with May averaging\n20°C. Rainfall quickly increases too, however, with April and May seeing the\nfirst of the heavy rains and averaging over 100mm."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ded98eaa-fc19-4370-a652-8271feb8ae96>","<urn:uuid:cd44dc91-35c5-4ca3-b826-2fd06c47bfcf>"],"error":null}
{"question":"Why is the Toronto Blue Jays season opener considered special for Canadian radio production?","answer":"The Blue Jays season opener is special because they are the only baseball team that has nationwide support across Canada, regardless of the team's performance. Unlike NHL, which has seven different market teams in Canada, the Blue Jays unite the country in a unique way, making the season opener production piece particularly significant as it kicks off the season across the entire national network.","context":["Sports History in the making – Sports Imaging reloaded or…\nI was stunned by Elisabeth’s Kobe tribute on soundlcoud and reached out. We chatted for a while and thought I needed to share her learnings, findings and awesome way of marrying plug and play to songs with you. Enter Elisabeth!\n1. Can you give me a bit background on yourself, your career, achievements.. I am sure a lot of the readers will know you or of you, but it puts a lot of the below in context I assume.\nWell, my name is Elisabeth Hart, and I graduated from the Radio Broadcasting program at Fanshawe College too many years ago. (Not THAT many years ago…) I started my full time radio career at 1240 CJCS in Stratford, Ontario, and a few years and the launch of 1 new station (107.7 Mix FM) later, I moved onto what at the time was CHUM Peterborough (Country 105/1420 CKPT). 9 years, 2 management changes and 1 station flip later (1420 became Energy 99.7), I moved on up to Rogers Radio Toronto. I was hired as a Commercial Producer for 98.1 CHFI, KiSS 92.5, Sportsnet 590 the FAN and 680 News. During my time there, my role has evolved, and I am currently the Imaging Producer for Sportsnet 590 the FAN. I still do some Commercial Production, as well as assisting with Imaging for the Country stations that Rogers has across Canada, and I’ve done Imaging for the News, AC and CHR stations as well. I also do a lot of voiceover work for commercials and various other projects within Rogers, so all of that keeps me hopping!\n2. How is it to work for some of the biggest radio and sports brands in Canada? How do the task differ between the different stations / brands? What is the stylistic approaches and how difficult is it to change hats?\nIt’s great to get to work with some of the best brands in Canada – but they don’t get to BE the best brands in Canada without the best people. It really is about having the best team around you to help you and support you. If I need help with something I’m working on, I know that I can go to any of my co-workers and ask for their help, and they’ll offer their advice. It’s then up to me to decide whether that advice works stylistically with what I’m working on or not. We’re not doing heart surgery here where the patient lives or dies by whether we cut the promo the right away – everything is open to interpretation and is subjective. But knowing that you can get different feedback from really talented people is always a help.\nAs far as changing hats, for me it’s important to focus. As much as I may need to multi-task in the course of a day, if I’m sitting there working on a Sports promo while I’m thinking about what I’m going to do with the Country splitters I need to work on, then there’s a chance that the Sports promo might accidentally end up with a Country feel to it. And for me, that’s not the right move. Plan things out, set aside the time to work on and focus on that one project for that format before you move on. And if an idea comes to me for something different than what I’m working on, I’ll write it down so that I remember it for later, rather than abandoning what I’m in the middle of to see that idea through. There will always be times where fires come up and something needs your immediate attention, but as much as you can, focus on one project at a time.\n3. How do your days look like? Is there a blueprint? A routine ?\nMy days are usually pretty different, which I like. There are, of course, always some projects that need to get completed every day, and I have to balance what commercial projects I’m working on that need my attention as well, but there’s not necessarily a blueprint to each day. I usually meet a couple of times a week with the Assistant Program Director of the FAN to talk about what’s happening that we might need to promote, any shows coming up that need our attention, any new seasons starting, etc. I usually try and get some time in a couple of times a week as well just to scour the internet and see what’s out there that other people are doing that’s cool that I can take inspiration from.\n4. What is your baby? Most fun project?\nFor me, every year my favourite project is the Season Opener for the Toronto Blue Jays baseball season. I’ve been a Blue Jays fan ever since I was young (wait, does that mean I’m not young anymore? DAMMIT!!), and to this day I still even hold a part time job through the summer working at the Jays games. As a very passionate fan, and someone who listened to the game broadcasts growing up, to be able to produce the big season opener piece that kicks off the season across the network, is just amazing. There’s NHL teams in 7 different markets across Canada, and while the whole country rallied behind the Raptors when they won the NBA Championship title, there’s still nothing quite like the way Canada supports the Blue Jays, no matter HOW good or bad the team is doing. So being able to hype people up for another baseball season all across the country like that is pretty special to me.\n5. How important is institutional knowledge for a format like sports?\nTo be honest, it’s not the most important thing. Again, it comes down to the people you have around you, and also your willingness to research. The guy who was doing Imaging for Sportsnet before me is an incredibly talented producer named Anthony Conte. He’s now imaging 98.1 CHFI, and is the AC Imaging Lead for Rogers Radio. He was doing an amazing job with 590, to the point that I was intimidated when I took over the job – I wasn’t sure I’d be able to keep up his standard! But he isn’t a big sports guy. He was able to do such a great job because he would research – and because the other people – the assistant PD, the writer, the jocks, the show producers, etc. would all help out too. Even with myself, I’m not a huge basketball fan. So when it came to producing that Kobe Bryant tribute that you heard, I had to do a lot of research. I knew of Kobe Bryant in the same way everyone did, but not in the same way a basketball fan knew him. So I Google’d him, I watched Youtube videos, I asked my Assistant PD what the *most* important thing for Kobe was, and we had people pulling audio clips from different sources and sharing them with me. While it may have been a piece that “I” produced, there were a bunch of different people involved in it. Can you do great Imaging without having an incredible knowledge of sports? 100%. As long as you’re willing to put in the time and effort to research what you don’t know.\n6. What DAW do you use?\nProTools 12 for PC.\n7. What are your favorite plugins?\nWell, I was recently introduced to a plug-in called RX7, and I’m really excited about it. It’s an audio repair plug-in that can create near-studio quality instrumentals and acapellas from just about any song. It’s way better than the Vocal Remove feature in Adobe, or any other commonly-used attempt to create instrumentals. Previously, I was always limited to using songs where I could find a really good quality instrumental version, so I’m excited to get to play around with this plug-in, and expand my horizons!\n8. What are new learnings? Ideas you work on? Inspirations?\nAn idea I’m working on right now is the Season Opener piece for the 2020 Blue Jays season, but it’s actually an idea I’ve been working on since October. I heard a song that I liked and thought would be cool to use in something, so I found an instrumental version, threw it into ProTools, started putting some play by play with it, and thought hey – this could really work to kick off next season! So I’ve been working on it as I have time, and even though it’s almost done, I still go back to it from time to time just to listen to it with fresh ears, and make sure it all still works. I get my inspiration a lot from commercial music – even if it’s not using that exact track in my material, because you can’t use artist songs in EVERYTHING – it’s how can I get that same emotion? I’ll go on Youtube and search for fan videos – there’s no end of hype videos that fans have made for their favourite sports teams, and they’ve often used great hype music in the background. It not only gives me ideas of what music to use, but also helps me connect with what sort of emotions those fans have tied to their favourite teams.\n9. Any new tools you discovered lately?\nThe tool I’ve been using quite a bit is actually nothing related to ProTools or production at all – it’s actually an app on my iPhone. I’ve found Shazam has been extremely helpful. If I’m watching those hype videos on Youtube and I don’t recognize the song, I can just Shazam it. I’ve even been sitting at home watching TV and heard a commercial with a song that had a cool beat to it that I didn’t know, so I just Shazam it. A few months ago, I was watching a hockey game and just before puck drop, they were playing a piece of music in the arena that I wanted to use in something. I wasn’t able to Shazam it at the time, and it took me about three days to FINALLY find out what the song was (I’d gone through Spotify playlists, Youtube, all looking for songs played at hockey games and come up empty.) Finally, I was able to find a broadcast of that game, find where that play happened, and there was enough of the song coming through that Shazam was able to pick it up. (It was Tsunami by DVBBS, but where it kicks in at 1:17, in case you’re wondering!)\n10. Your favorite piece of imaging / production ever?\nMy favourite piece ever is probably a piece that Chris Pottage, my boss, produced in 2015 back when the Blue Jays were doing really well and made it into the post season.He managed to weave together Eminem’s “One Shot” with the Beatles “Come Together” WITH the Ok Blue Jays theme song, along with play by play calls in an absolutely incredible way that really captured the excitement of the Blue Jays run, and hearing that piece takes me right back to those sold out crowds and the energy in the stadium. It was electric, and his promo was just as electric.\nBut if you mean my favourite piece of MY Production – that’d probably be the 2019 Season Opener for the Blue Jays – that season was all about the team re-setting, and all of these young prospects finally coming up to the big leagues, and I just love how I was able to find a song that not only captured that theme, but worked so well with the play by play, and I was super happy with how it turned out. The opener for the 2020 season may actually be even better, but it won’t air until the end of March, so I can’t share it just yet. And if you’re wondering if I’m biased that all of my favorite projects are based around the Blue Jays – absolutely I am. I still try and connect to the emotions for other sports for the listeners, but they don’t connect emotionally the same way for ME. There’s a difference between “Hey, that’s a really cool piece of Production”, and “Oh, I love the memories and feelings that that piece of Production connects me to.”\n11. What would be your career advice for a youngster your twenty year old self trying to enter the radio biz?\nIt’s not too late for law school.\nBut if you really MUST try to enter the radio biz – then stay passionate. Even if you find yourself working with a format that you don’t really like, find a way to discover passion within it. You spend so much of your life working, there’s no sense in spending all that time doing something you don’t enjoy, so always find a way to find the fun within a job. And when you DO get stressed out, always remember…we’re not dealing with life-or-death situations here. We get to PLAY Radio for a living. And that’s a pretty fun thing to do!\nCheck out Elisabeth’s soundcloud for more of her work:"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d4e20ee9-919f-4425-a58e-00e51f51fcda>"],"error":null}
{"question":"At what temperature and for how long should toast bread be baked, and what special step is needed during baking to prevent burning?","answer":"Toast bread should be baked at 175 degrees Celsius for 35 to 40 minutes. After about ten minutes of baking, when color appears on the surface, the toast should be covered with tin foil to prevent the surface from burning.","context":["Toast bread may seem troublesome, but it is actually relatively simple to make. As long as the recipe is right and the dough is kneaded well, there is basically no big problem.The inside is delicate and non-porous, soft and silky, and it is really delicious.\n- 540g high-gluten flour (bread flour)\n- 390g iced milk\n- 40g sugar\n- 5g high-sugar resistant yeast\n- 5g salt\n- 40g butter\n- Pour all the ingredients except butter into the kneading bucket (540 grams of bread flour, 5 grams of high-sugar resistant yeast, 5 grams of salt, 40 grams of white sugar, 390 grams of pure milk. The milk here must be frozen. Yes, the one with a little bit of ice is best)\n- Use chopsticks to mix all the ingredients evenly, which will make it easier to knead the dough later and also speed up the dough forming into a ball.\n- Use a chef’s machine to knead the dough until a thick film can be pulled out, which will take about 8 to 10 minutes. Then add 40 grams of room temperature softened butter.\n- Knead the butter into the dough at low speed first, then turn to medium-high speed and knead for about 8 minutes. Stop every 1 to 2 minutes and cut a small piece of dough to check until you can pull out a thin and transparent glove film. . This membrane is tough and the holes are smooth and jagged, so the dough meets the standards for making toast. You can stop kneading the dough at this point.\n- Arrange the dough, put the smooth side up, place it in the kneading bucket, cover it with plastic wrap and seal it for the first fermentation.\n- Dip your fingers in flour and press on the surface of the dough. If there are obvious fingerprints, but the dough does not collapse or rebound, and the volume is about twice as big as the beginning, this means that the dough has risen.\n- Move the dough to the kneading mat, press it flat with the smooth side facing up, and divide it into 6 equal parts.\n- Still with the smooth side facing up, gather each small dot from top to bottom, and then roll it into a round shape. Pinch the opening tightly and place it with the seal facing down.\n- Arrange all 6 pieces of dough, cover with plastic wrap and let rest for ten minutes.\n- After ten minutes, pick up the first prepared dough, with the smooth side facing up, and roll it into a long tongue shape.\n- Turn it over and roll it up from top to bottom. The bottom part should be pressed thin so that the ending part can be tightly attached.\n- Roll out all the dough for the first time, cover with plastic wrap, and rest for another ten minutes.\n- Next, roll out the roll for the second time. Take a relaxed dough roll, smooth side facing up, flatten it, and then roll it out.\n- Roll it up from top to bottom, being careful not to roll it too tightly. The final finishing position should also be rolled out thinly.\n- After rolling the dough for the second time, put it into a toast box and place it in a warm and humid environment for the second proofing.\n- After the second proofing, the toast box should be 90% full. Brush the surface with egg wash, put it in the middle and lower rack of the preheated oven, 175 degrees Celsius, and bake for 35 to 40 minutes.\n- Bake the toast for about ten minutes. When you see the color on the surface, it is basically ready. At this time, cover the toast with tin foil to prevent the surface from burning.\n- After taking it out of the oven, shake the toast box out of the heat and pour the bread onto the drying rack to cool down.\n- Let it cool until it is no longer hot and you can eat it. The inside is delicate and non-porous, soft and silky, and it is really delicious."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:49be9a93-3c1b-4de3-9561-a79553b89492>"],"error":null}
{"question":"How are museums adapting to resource constraints through skill sharing, and what role has virtual programming played in expanding their reach during recent challenges?","answer":"Museums are adapting to resource constraints through innovative skill sharing initiatives and virtual programming. In terms of skill sharing, institutions are implementing cost-effective solutions like swap shop arrangements between museums, where staff members exchange expertise through practical, hands-on training. For example, curators from different museums have participated in one-day-a-week exchanges, gaining specialized skills in areas like conservation, loans management, and collections care without financial cost. These arrangements provide practical skills development, strengthen peer networks, and effectively expand institutional expertise. Regarding virtual programming, museums have significantly expanded their reach through online initiatives, particularly during the pandemic. Many institutions have launched virtual tours, online lectures, and digital exhibits, allowing them to reach audiences worldwide. For instance, some museums have reported engaging visitors from multiple continents, with virtual events often attracting three times the usual attendance of in-person programs. This digital transformation has removed geographic and financial barriers to access, though it has required museums to develop new skills in areas like video editing and virtual event management.","context":["At a time when museum staff and museums are vanishing and public sector institutions are under pressure to provide high quality services with restricted resources, London Museums Group (LMG) has been investigating how museums are reacting to the emerging landscape.\nFunding is reduced or disappearing, staff are being made redundant and some museums are closing altogether. But our members are not letting these barriers stop them. Rather, they are finding innovative ways to share knowledge and skills so they can carry on providing services and experiences to their users and visitors.\nSince our chair Judy Lindsay blogged here six months ago about Share London – a free online scheme enabling museums to share skills and resources – LMG membership has grown to 626 members and the site is buzzing with professionals offering career advice (British Museum), mentoring opportunities (Westminster Archives), volunteer visits (RAF Museum), and social enterprise consultancy for museum CEOs (GK Partners). We can see that museums are responding to changes in the sector and helping each other more than ever.\nAs the LMG blogger-in-residence I was asked to investigate: what skill sharing is happening in the sector, what are its benefits and what useful lessons can we pass on to our members? A little and large case study where curators swapped skills between the Natural History Museum and the Hunterian Museum provided a great opportunity to research these questions.\nThe partnership started with a conversation between museum professionals who realised that they could gain highly specialised skills through a one-day a week swap shop in each other's museum. How did this conversation turn into a valuable and tangible scheme? Both curators, like any museum professionals, had multiple demands on their time and energy. Time out of the office needed to be justified to show their organisations wouldn't be losing out.\nWith this in mind, they created professional development plans that demonstrated what skills and knowledge they would gain and how these would be applied in their working practice. The case was made that these skills could not be obtained at conferences or workshops; that practical one-to-one training was needed. A skill sharing scheme provided the solution as both curators could spend one day a week with colleagues working on specialist projects at no official cost to their museum.\nFor the curator at the National History Museum, spending time with the conservator at the Hunterian would fill a gap in her knowledge base. The Hunterian curator, who is part of a small team, argued that it would be beneficial to work with a number of specialist museum staff to gain knowledge and skills in loans, chemicals and analysis.\nEight key features of swap shop\n• Practical skills: Working hands-on with objects meant specialised skills in chemistry, aesthetics and storage were gained.\n• Peer networks: The Hunterian curator worked with a variety of professionals at the NHM and these new peer contacts remained at the other end of the phone or email to offer further advice.\n• Policy change: Observing the loans officer put Natural History Museum policy into practice enabled the Hunterian curator to contribute to a loans policy in her own institution.\n• Fluid learning: Because the scheme wasn't official and regimented, it was adaptable to the curators' particular learning needs and time-pressured working situations.\n• Knowledge sharing: Having the space and time to share knowledge with peers was important to the learning process. In the words of the curators: \"Swapping of knowledge is more casual; it's at an easier rate; knowledge isn't crammed into one training day.\"\n• Focus: The training was individually tailored to each museum professional ensuring a high quality, relevant and focused learning experience.\n• Reflection: Both curators found that being away from their offices, they were able to focus on tasks and reflect on processes that amplified their practical learning.\n• Cost-effective: There was no financial payment required and while each organisation had to release a member of staff, they also gained a new employee for the duration of the project.\nThis research shows that there is good practice going on in the museums sector. Skills sharing is a cost-effective way for organisations to develop self-led, bespoke and high-quality professional development modules for their staff. It's a way to strengthen peer networks and develop relevant communities of practice. It's a training scheme where organisations don't lose members of staff but gain them instead. Both curators spoke of feeling that they had an expanded team of colleagues. Can other creative sectors use a skill sharing model? We think so.","Virtual Programming is Here to Stay—and so are Its Audiences\nMuseums have evolved during the past twelve months. Because of the ongoing coronavirus pandemic, an extraordinary number of museums have temporarily shut their doors and have had to find a way to meet their visitors online. While some museums were already engaged in virtual programming, others had to catch up–and quickly–if they wanted to survive. If these responses can lead to real and fundamental institutional changes, then museums may have just tapped into their true audience potential.\nBefore the pandemic began, the Knight Foundation commissioned HG&Co to conduct a survey about museums’ digital strategies and readiness. This report looks at 480 museums of all types and sizes across the United States. While things have certainly changed since quarantine began, it does offer a snapshot of what the situation was like heading into the pandemic.\nFor example, the report found that only 25% of museums had an established digital strategy pre-pandemic. While leadership support was high, digital projects often were not being tracked and insights were shallow, if they were even collected at all. Quarantine has forced many museums to reckon with digital content in a way that they hadn’t before as programming shifted to online.\nFor many museums, like Musée d’Orsay in Paris, this change means virtual tours. For others, like the National Gallery of Art in Washington, D.C., online lectures and events. Some, like the Charles Dickens Museum in London, have worked on digitizing their collections or hosting exhibits online. Some museums are engaged in all three as well as other creative forms of virtual programming, like art classes or scavenger hunts.\nSome of these offerings include museums adapting their already planned in-person events to a digital format. Others are specially designed to be online content. Museums have been able to secure speakers for online events—speakers who might not otherwise have been able or willing to travel in order to participate.\nOne of the most exciting aspects of this shift to virtual programming has been museums’ ability to reach audiences far outside of their normal geographic ranges. While tourists obviously make up a large portion of a museum’s normal visitors, online content has removed geographic and financial barriers to experiencing lectures, exhibits, and tours.\nFor example, a few months ago, from the comfort of my own bedroom in North Carolina, I logged on to a virtual guided Zoom tour of the Jane Austen House in Chawton, England. At one point, those of us attending used the chat feature to chime in and share where we were. While there were some people only a short drive away from the house museum, others were tuning in from other continents around the world.\nWhen we interviewed Ken Howard, he told us that the North Carolina Museum of History is focusing on creating more digital content, partially because of the ability to reach audiences who might never set foot in the museum. He said, “The benefit of that is we’re now reaching people around the world! We’ve had people who have been accessing our resources from England, and Australia, and France, and Canada, and all over the place. Having these digital resources was a huge benefit because now our reach has grown beyond the borders of North Carolina.”\nThe Museum of Natural History in London is purposefully reaching out to Asian audiences, who would normally be a major part of their tourist traffic. They’ve partnered with Fliggy, an online travel brand, to livestream virtual tours to China. It’s a fantastic example of a museum realizing they must find new ways to reach audiences during this unprecedented time, but that those audiences can be a lot further away than normal.\nKimberley Floyd, site manager of the Zebulon B. Vance Birthplace in Asheville, North Carolina, told us they’re able to reach wider audiences with their virtual events than they have in the past with their in-person programming. For example, an embroidery workshop that would normally be attended by ten people had three times that many attend it virtually–and from several different states.\nMany museums seem to plan to continue their virtual programming even as the pandemic begins to wane. Both Floyd and Howard said their organizations plan to continue, and many museums have kept up their virtual programming as they begin to reopen. Obviously, it may not endure at the same level that is has during the pandemic, but it seems to be a trend that is here to stay.\nOf course, this not only affects museum visitors, but museum staff as well. It creates new work for those employed at museums and demands they are well-versed in skills that were not previously required, from video editing to handling large Zoom calls. For state-owned historic sites, it also may influence how funding is allocated as many budgets are based on visitation numbers. Floyd said, “How many staff we have is based on our visitation. The digital content is opening it up in a way and may complicate that.”\nWhat is clear now is that virtual programming is likely to be one of the most lasting effects of the coronavirus pandemic for the museum industry. Not only has it been practical over the last year, but it also increases accessibility to museums. It allows people who might not have been able to visit – whether because of physical accessibility issues or because of barriers to travel – to experience exhibits, collections, and events.\nAs virtual programming continues to grow, it is important that museums incorporate it into their strategic plans and collect demographic information about the people with whom they engage online. This shift towards virtual programming might have been coming all along and perhaps it was only accelerated by a global pandemic. Regardless of why or how it occurred, it is something that all museums should strongly consider embracing in the interest of realizing their true audience potential."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0b655b4d-678c-43a2-aabb-7ae84f9d27d7>","<urn:uuid:f24496c5-d599-450b-97ce-ce6ed1e283c2>"],"error":null}
{"question":"How do Howardena Pindell and Elly Clarke differ in their artistic approaches to exploring digital and social interactions?","answer":"Howardena Pindell focuses on traditional abstract methodology using physical materials like grids of punched-out circles and acrylic paint, while also addressing social issues through video work. In contrast, Elly Clarke specifically explores the impact of networked culture on identity and relationships, creating conversations and performances in both online and offline spaces to examine instant mobility and communication in the digital age.","context":["Shows & Exhibitions\n‘I’m Staying Away From Trump as Subject Matter’: American Artist Howardena Pindell on How She Balances Her Abstract and Political Work\nAs her first solo show in London opens, the pioneering artist says she's glad the US president isn't interested in culture.\nThe American artist Howardena Pindell arrived in London for her first solo show in the UK just as the 45th US President landed for a state visit. A veteran activist and co-founder of New York’s pioneering feminist gallery collective A.I.R., Pindell tells artnet News that while her work is often political, Donald Trump will never appear in her art. “I don’t want him to engage me,” she says.\nBorn in Philadelphia in 1943, Pindell studied painting at Boston University and Yale. In 1967, she became one of the first black women to join the curatorial staff of New York’s Museum of Modern Art. During the 12 years she worked at the museum, she continued making art, developing an abstract methodology based on grids of punched-out circles through which she applied acrylic paint to build up shimmering surfaces of color on a shifting grid.\nPindell stepped down from MoMA in 1979, shortly after protesting the use of racist language in an exhibition at New York’s Artists Space. The same year, she suffered a near-fatal car accident: the experience cemented her sense that it was urgent to use her voice to make work and speak out. Her video Free, White, and 21 (1980) presented her experiences of overt and systemic racism, and the dismissive responses she had received from white colleagues.\nPindell, who has taught at the Stony Brook University in New York since 1979, continues to make both abstract and issue-led work. Last year, she was the subject of the major touring retrospective “Howardena Pindell: What Remains to Be Seen,” organized by the Museum of Contemporary Art, Chicago. We spoke to the artist about the evolution of her work, her plans for her legacy, and her distaste for Trump on the occasion of her first solo exhibition in London, which opens this week at Victoria Miro Gallery.\nA lot of us in Europe are better acquainted with your issue-led work and your activism. What’s the relation between that and your abstract work?\nI try to do it to balance out what I’m doing, because doing the research on the issue-related work can be very frightening and very upsetting. So I do both at the same time. The next video I’m doing is about lynching, the civil rights era, and slavery, and I will be the voice. The material is really upsetting, so at the same time, I’m working on the abstract [painting].\nI see Free, White and 21 getting shared a lot by students. Do you feel the issues that you were discussing in 1980 are still very relevant to your students?\nMy students aren’t necessarily aware of the video. I basically teach painting. Where it comes into the classroom is other professors show it: the art historians. Occasionally it’s shown around New York City. For a while, it was on YouTube and then my distributor took it off. I don’t mix my politics in with my teaching. I mean a lot of people are very upset about Trump: it’s almost like an unspoken sadness.\nThe opening of your exhibition here, of course, coincides with Trump’s state visit.\nI’m glad he’s not interested in culture. He has not made any forays into the cultural world, whereas Hitler was an artist, and if you did not live up to his standards for Nazi art you were considered degenerate.\nTrump has put artists who are politically engaged in a very particular position: it’s given them a really urgent role.\nI’m staying away from Trump as subject matter, because I don’t want him to engage me. I don’t want anything to do with him, but I’m empathetic with others who feel very afraid of where we’re headed with him, especially if he gets a second term.\nWhat did you decide to include in your first London show?\nVictoria [Miro] and Garth [of Garth Greenan Gallery, Pindell’s New York dealer] chose to show early work from the 1970s, and then later work on paper, where the circle has migrated from the canvas to actual punched large circles.\nDo you make works with the express intention of punching them out for assemblages or do you re-use older works?\nNo, I make drawings intentionally to destroy them. That’s not true of my earlier work where I made drawings that stood on their own merit. Now the drawings I do are very abstract and playful: I destroy them and re-assemble them in a three-dimensional way. I use thread also. It’s all about playing.\nThe thread relates to your interest in the grid, which comes right from your earliest abstract work.\nThe early interest in the grids was kind of sarcastic, because everyone was interested in structuralism, the golden mean, and the grid and so forth. In a form of rebellion I made a portable grid: literally a grid held together by big grommets. I enjoyed working on graph paper, but I think the early influence in that was my father, who was a mathematician. I remember seeing him write numbers in a journal that had a grid: he liked to record his mileage. I was a child. They didn’t mean anything to me. I saw numbers as functional, but also I saw them as beauty.\nYou’ve also made serial works using numbers.\nSome people would almost think of them as conceptual. I literally saved all the holes I had punched for the paintings that are in the show. An art dealer had come to see my studio and blindly said, “How many circles on the painting?” I started to count them. I had bags of these circles. I would collect them and then I would get graph paper and use matte medium and attach the dots on the graph paper sequentially. Sometimes, depending on the density of the numbers, they would cluster and create shapes or forms.\nThat also happens with the early paintings you’ve done through stencils: in places it feels like there’s pointillist form emerging as well.\nThat’s interesting. Yes, when I worked at the Museum of Modern Art I was very interested in pointillism but I also found inspiration with the color in [Odilon] Redon: in his pastels of flowers, the color was so beautiful. I was really trained as a figurative painter, as an academic painter. Then I made this break into using the circle. I started making drawings on graph paper and spraying the drawings with acrylic.\nWhen your retrospective opened last year, MCA Chicago put a lot of your writing online as an open archive. Are you working toward making more available to view through a foundation or archive?\nAt the Smithsonian my papers will be in the Archives of American Art. That’s already been decided, and Garth [Greenan] will be my art executor. What I’m aiming for is to have a philanthropic foundation, so as [work] sells, [grants] will go out to different charities after I’m gone.\nHow did you feel about that show at the MCA Chicago?\nI was stunned when I saw it. Thirty years ago, I’d probably be very ecstatic, but now I’m more stunned, because I’m old. I’ve had so many different kinds of responses to my work.\nDo you still? Or do you feel that time is catching up with you?\nEvery now and then, maybe there’s a pop or squeak, but it’s interesting to see work that was put down 30, 40 years ago, that now people are able to tolerate and see.\nWhat do you think’s changed?\nI have no idea. The work’s still the same. Maybe the eyes are different? Maybe just the way things have changed has made them different?\n“Howardena Pindell” is on view from June 5 through July 27 at Victoria Miro in Mayfair, London, 16 St George Street, W1S 1FE.\nFollow Artnet News on Facebook:\nWant to stay ahead of the art world? Subscribe to our newsletter to get the breaking news, eye-opening interviews, and incisive critical takes that drive the conversation forward.","Right Here, Right Now @ The Lowry, Salford (UK)\nPublished on November 8th, 2015\n* * * * * * * * * * * *\nRight Here, Right Now Digitally-influenced work which seeks to challenge perceptions\nDigitally-influenced work which seeks to challenge perceptions\nThe Lowry, Salford\nfrom 11 November 2015 – 28 February 2016\nThis is an exhibition of some of the most exciting contemporary art in our time. A time that is increasingly dominated by our relationship with digital technology. A team of international guest curators nominated a number of artists for the exhibition, which was then curated by The Lowry. We have chosen a selection of artists who are defined as digital by their use of digital technology as a significant part and influence in their artistic practice. At this moment we are living through a digital change that is shaping our behaviour, understanding and view of humanity and the world around us.\nThese 16 international artists and their artworks expose new ways of visioning, creating and reshaping how we think about ourselves. They offer a series of methods that explore how digital technology is becoming part of our visual, sound and intellectual experience. The art is playful, thoughtful and challenges us to consider the digital systems embedded around us, either visible or hidden. Using surveillance, artificial intelligence, voyeurism, interruptions and distortions – we are able to immerse ourselves in art that considers beauty, privacy, self, permissions and collaboration. Society is being altered as we become embedded into a digital age shaped by technology systems. From the phone in your pocket to the data you unwittingly give away, artists take a leading role in continuing to explore and question the cultural impact of these happenings.\nToday’s creative innovations still attract dreamers, critics and aggravators through the interrogation of digital social systems, economic systems and infrastructure systems. The Lowry is taking a moment to artistically see where we are, right here right now.\nSarah Cook, Annet Dekker, Lucy Dusgate, Beryl Graham (text), Karen Newman, Hannah Redler, Katrina Sluis and Lindsay Taylor.\nDaniel Rozin – New York based artist, creates interacvitve installations and sculptures that have the unique ability to change and respond to the presence of a viewer.\nRobert Henke – an artist working in the fields of audio visual installation, music and performance. Coming from a strong engineering background, Henke is fascinated by the beauty of technical objects.\nBranger Briz – a collective of artists, educators and programmers formed in 1998 by Nick Briz, Paul Briz and Ramon Branger.\nJulie Freeman – who’s work spans visual, audio and digital art forms and explores how science and technology change our relationship to nature.\nMishka Henner – The artist is based in Greater Manchester. Even though he uses new technologies, Henner sees himself as an old school reporter – showing the public what is in front of them, but that they cannot see.\nStephanie Rothenberg – uses performance, installation and networked media to create provocative public interactions.\nFelicity Hammond – is a recent graduate of the Royal College of Art (2014), where she completed her MA in Photography and was awarded the Metro Imaging printing award.\nEd Carter – devises and creates interdisciplinary projects that are context-specific, with a focus on sound, composition, architecture, and process.\nJoe Hamilton’s practice is concerned with rethinking the distinction between nature and the built environment and in considering the fragmented way we experience the world, through multiple series of continually overlapping windows on the screen.\nUK-based artist Nikki Pugh explores relationships between people and places. Her practice encompasses locative and digital media, walking, performative actions in public spaces (including pervasive games), installation, physical computing and collaboration.\nThomson & Craighead live and work in London and Kingussie in Scotland. Much of their recent work looks at live networks such as the web and how they are changing the way we all understand the world around us.\nPioneers of Net Art, Eva and Franco Mattes explore the ethical and moral issues arising when people interact remotely, especially through social media, creating situations where it is difficult to distinguish reality from a simulation.\nBerlin-based artist Elly Clarke explores the impact of networked culture on our sense of identity and relationships. She creates conversations, exchanges and performances in spaces online and offline, investigating an age of instant mobility and communication and questioning the importance of the physical body and object in today’s digital world.\nTimo Arnall’s design, photography and filmmaking work is about developing and explaining emerging technologies through visual experiments, films, visualisations, speculative products and interfaces.\n* * * * * * * * * * * *\nT O    T O P"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:38310c21-854b-43a9-8e17-de537b609a65>","<urn:uuid:17fdd5e3-0044-41cc-b7bc-74935b44b4ab>"],"error":null}
{"question":"How is Kölsch beer different from regular lagers?","answer":"While Kölsch beers may look like light lagers due to their straw blond color, they are distinguished by their subtle but noticeable fruity flavors. They are light in both body and appearance, with subdued maltiness and assertive but unobtrusive hoppiness. Germans classify Kölsch as 'Obergäriges Lagerbier' (top-fermenting lager beer), and it ferments at near ale temperatures followed by several weeks of lagering at cold temperatures.","context":["Finally, we have seen the back of winter (fingers crossed). Daffodils have flowered, blossom has bloomed and most of us are getting up when it’s light and getting home from work in time to crack open a beer in the garden. Thoughts of the average beer lover turn away from the heavy brews; warming imperial stouts are placed at the back of the cupboard, barley wines are stuffed into their hiding places until their restorative qualities are needed again. It’s time for warm weather beer – light, refreshing and quaffable.\nOne such beer is Kölsch, the famous beer of Cologne (Köln), Germany’s fourth largest city, situated in North Rhine Westphalia. Kölsch is a very specific beer style – in 1985 the Köln Brewery Association prepared the Kölsch Konvention, that was then published in the Bundesanzeiger and stipulates that Kölsch can only be called so if meets the following criteria: - it is brewed in the Cologne metropolitan area, it is pale in colour, it is a ‘Vollbier’ (a German beer tax category beer with a starting gravity of between 11 and 14 Plato), it is hop accented and it is also filtered. In 1997 the beer style received protected designation of origin status from the EU, meaning it is now afforded the same protection as our beloved Cornish Pasty. Craft breweries wishing to emulate this crisp, snappy and refreshing brew circumvent this legislation by naming their beers ‘Kölsch style’ or ‘Köln style’. And examples are growing in number; Goose Island Summertime, Ballast Point Yellow Tail, Harpoon Summer Beer and our utterly marvellous Tzara. One common thread unites these Kölsch style beers – an attempt to stay true to the basic principles of Kölsch brewing.\nKölsch beers are sometimes mistaken for light lagers because of their straw blond colour. They are, however, distinguished by their very subtle but noticeably fruity flavours. It is light in both body and appearance, its maltiness is subdued; its hoppiness is assertive but unobtrusive. This is a beer designed to be drunk all day every day!\nSo what goes into a Kölsch then? Well, for starters, consider the malt bill. Mostly Pilsner malt and Wheat malt (between 5-10%). We got ours from our favourite German Maltster, Bamberger, adding a dash of Carapils from Weyermann. Pale Ale malt and Torrefied Wheat do not make the cut with Kölsch. Hops must also be classically German – traditionally, Perle or Spalter Select are used for bitterness (we went with Perle), Tettnanger and/or Hallertau Tradition added at the end of boil for aroma. Our Rolec Hopnik is filled with the same amount of hops as a brew of Kipling with a combination of both. The nature of these noble hops used will not overpower the essence of the brew. Our liquor needs to be soft so we get that distinctive smooth mouthfeel – luckily Bakewell water comes off the millstone grit of the Peaks, so is, thankfully, lovely and soft (also allowing us a blank canvas for other beers that need harder liquor). And most importantly of all - the yeast strain. This must be a classic Kölsch strain, or you may as well give up and brew something else. As Matthew, one of our brewers likes to say, we have the skills to pay the bills, so we obtain a Kölsch strain from our good friends at White Labs, San Diego and propagate it to an ideal pitching rate via various flasks and finally our yeast propagation vessel. So our ingredients are indeed simple but of the absolute highest quality. But that is not all that is required for the perfect brew of Kölsch…\nLook in many English and American brewing texts and you will see Kölsch referred to incorrectly as an ‘Ale’. It does ferment at near Ale temperatures, but one has to consider how the Germans themselves classify Kölsch – ‘Obergäriges Lagerbier’ – top-fermenting lager beer. Calling all top-fermenting beers ‘Ales’ is simply misusing the name. To the Germans, Ale is a British thing and doesn’t ever get mentioned in the same breath as Kölsch. What is now needed is a few weeks of lagering at 4˚C, slowly moving down to -1˚C, to round out those fruity flavours from the Kölsch yeast strain. Having nothing in the way of spare capacity and a completely full production schedule doesn’t stop us from completing this lagering period fully – it is essential for a perfect Kölsch. Thankfully our bosses understand, primarily because they like good beer too.\nAfter fermentation and lagering, the other key difference now occurs between a true Kölsch and Tzara, our Köln-style beer. We don’t have a filter, so instead we use our centrifuge to clarify the beer. A true Kölsch is always filtered, but centrifuging allows us to clarify the beer without stealing those delicate flavours we put into the beer in the first place.\nBrewing Kölsch-style beer ourselves guarantees we can appreciate the Kölsch style at its best. We do everything we possibly can to keep oxygen away from the beer after primary fermentation. Dissolved oxygen can produce off-flavours, ruining the delicate taste for which Kölsch is known. We keep levels typically below 10 µg/l (something we do for all our beers). It is a style which has no strong flavours to hide behind; no massive hop pungency, no crystal malt sweetness, no boozy alcohol. Brewing excellence is thus required throughout the whole process. It is a massive challenge to get it just right; we bloody love making Tzara and hopefully you enjoy drinking it as much as we like brewing it."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9cf807e6-2257-4256-917f-d2f93d5af751>"],"error":null}
{"question":"How does location affect vertical farming success, and what legal processes must be followed before starting mining operations on private land?","answer":"Location significantly impacts vertical farming success through several factors: energy costs are lower on city outskirts near distribution centers, water scarcity regions like Sub-Saharan Africa offer better opportunities, and areas with extreme climates like Scandinavian countries are more suitable. Additionally, regions lacking fresh produce access, like the Middle East, present better market conditions. Regarding mining operations on private land, section 54 of the MPRDA requires mining right holders to first notify the Regional Manager if landowners refuse access or make unreasonable demands. The Regional Manager must then facilitate negotiations for compensation, and mining operations cannot commence until these processes are completed, unless the landowner's conduct is deemed obstructive to the MPRDA's objectives.","context":["When is section 54 of the MPRDA applicable?\nSection 54 of the Minerals and Petroleum Development Act (“MPRDA”) provides that the holder of a reconnaissance permission, prospecting right, mining right or mining permit should notify the Regional Manager if the owner or lawful occupier refuses to allow the holder of such right to enter the land, places unreasonable demands in return for access to the land or cannot be found in order to apply for access to the land. Once the holder of the relevant right has done so, the Regional Manager has to notify the owner or lawful occupier to enable him or her to make representations in relation to the notification which has been made.\nIf the Regional Manager concludes that the owner or lawful occupier is likely to suffer loss or damages as a result of the activities of the mining operations, the Regional Manager must request the parties concerned to endeavour to reach an agreement for the payment of compensation for such loss or damages. If an agreement cannot be reached, the matter should be resolved by way of arbitration or by a competent court.\nHowever, in the event that the Regional Manager, having considered the issues raised by the holder of a reconnaissance permission, prospecting right, mining right or mining permit, the submissions made on behalf of the owner or lawful occupier, and also the Regional Mining Development Environmental Committee (“REMDEC”), concludes that the objects of the MPRDA in sections 2(c), (d), (f) or (g) will be undermined by further negotiations arising in relation to the refusal to permit access, the Regional Manager may recommend that the Minister expropriate the land in terms of section 55 of the MPRDA.\nShould the Regional Manager find that the failure to reach an agreement is the fault of the holder of the relevant right, the Regional Manager has the power to prohibit the commencement or continuation of mining operations until the dispute has been resolved by arbitration or by a competent court. This provides some form of protection for an owner or lawful occupier.\nIn essence, section 54 appears to envisage a form of mediation by the Regional Manager to assist the parties to reach an agreement and to avoid the need for litigation (which is time consuming and costly).\nIn the Maledu and Others v Itereleng Bakgatla Mineral Resources (Pty) Limited and Another matter (“the Maledu matter”) it was argued by the respondents that section 54 would only be triggered if there is a dispute concerning compensation as the heading of this section reads “compensation payable under certain circumstances”. The Constitutional Court held that this interpretation is incorrect, and that section 54 has a “much broader application than simply disputes relating to compensation”.\nShould a right holder first exhaust the processes provided for in section 54 before resorting to common law remedies?\nIn the Maledu matter, the Constitutional Court had to answer the question whether the respondents first had to exhaust the internal remedy provided for in section 54. The Constitutional Court referred to the requirements for the granting of an interdict (authoritatively set out in Setlogelo v Setlogelo). One of these is the “absence of any other satisfactory remedy”. It appears that the purpose of section 54 is to create a mechanism to resolve disputes arising from differences between the interests of a holder of a reconnaissance permission, prospecting right, mining right or mining permit and those of the owner or lawful occupier. Therefore, it would be difficult to argue that section 54 is not (in principle) another satisfactory remedy for the purposes of the Setlogelo test.\nThe Constitutional Court also held that because the applicants’ rights derived from the MPRDA, the processes as set out in section 54 first had to be exhausted before they could resort to the enforcement of their common law remedies by way of an interdict. In this regard the Constitutional Court also emphasised,\n‘section 54 provides for a remedy must mean that resort cannot be had to an alternative remedy available under the common law [and that this] must be so because section 4(2) of the MPRDA expressly provides that “in so far as the common law is inconsistent with [the MPRDA], the [MPRDA] prevails”’.\nBy way of analogy, in the matter of Dengetenge Holdings (Pty) Ltd v Southern Sphere Mining and Development Company Ltd and Others (“the Dengetenge matter”) the question which the Constitutional Court had to answer was whether a court could consider a review application where the applicant had launched the review application without first exhausting the relevant internal remedies. In this case the Minister had (actually) agreed that the dispute relating to the contested competing claims regarding the grant of prospecting rights should be heard by a court as opposed to the matter being determined by way of an internal appeal in terms of section 96 of the MPRDA. The majority of the Constitutional Court concluded that the Minister does not have the power to waive the requirement to first exhaust the internal remedies as provided for in the MPRDA.\nDespite the fact that the Dengetenge and Maledu matters differ in relation to their facts (as the first case relates to the appeal process as provided for in section 96 of the MPRDA and the latter case relates to a mechanism which aims to resolve disputes between the owner or lawful occupier and the relevant right holder as set out in section 54), it is clear that the courts have generally followed the principle that where the MPRDA provides for an internal remedy, the internal remedy should first be exhausted before the aggrieved party can utilise any other remedy (unless the court has exempted the applicant from having to comply with this requirement).\nThe billion dollar question – can mining continue when section 54 processes have been triggered?\nThe Constitutional Court the Maledu matter had to decide whether mining operations should be halted until the finalisation of the section 54 processes. In this regard the respondents sought to argue that, if the processes as set out in section 54 first had to be exhausted, it would ‘unjustifiably prevent’ the respondents from commencing with mining pending the finalisation of the processes in terms of section 54 and that if they had ‘attempted in good faith to comply with [their] consultative duties’ under the MPRDA they were free to commence with their mining operations notwithstanding the fact that the processes envisaged in section 54 had not been finalised. However, the Constitutional Court dismissed this argument and held that the respondents had overlooked the fact that section 54 itself is intended to be a speedy dispute resolution process and were mining operations allowed to commence mining activities before the section 54 processes had been finalised, it would defeat the purposes of section 54 and render the process redundant. The Constitutional Court also noted that if the parties failed to reach an agreement for the purposes of the section, the Regional Manager could recommend to the Minster that the land be expropriated.\nThe Constitutional Court did not expressly overturn the Supreme Court of Appeal’s decision in Joubert v Maranda Mining Company (Pty) Ltd, (“the Maranda matter”) where an interdict was granted against the landowner which effectively permitted Maranda to conduct mining activities in terms of its mining permit while the processes envisaged in section 54 were in progress.\nThe Constitutional Court in the Maledu matter held that the Maranda matter was distinguishable on the basis that the applicant in the Maledu matter did not have the objective of frustrating the legitimate endeavours of the mining right holder – which it held has occurred in the Maranda matter. The Constitutional Court pointed out that in the Maranda matter it was:\n‘clear that the landowner was not only intent on refusing consent but was also not prepared to even enter into negotiations with the mining right holder [and that the] landowner’s conduct was found not only obstructive but also subversive of the objects of the MPRDA’.\nAs such, the Constitutional Court noted that an interdict was appropriate in those circumstances. This creates the impression that the court may have been suggesting that if the owner or lawful occupier is willing to negotiate with the holder of a reconnaissance permission, prospecting right, mining right or mining permit, mining should stop until the section 54 processes have been finalised. On the other hand, if the owner or lawful occupier’s conduct was found to be obstructive and ‘subversive of the objects of the MPRDA’, the mining right holder may potentially be able to obtain an interdict to prevent any delays caused by disgruntled owners or lawful occupiers.\nIt would seem that the Constitutional Court has created some wriggle room for holder of a reconnaissance permission, prospecting right, mining right or mining permit to argue that mining should be allowed to continue while the section 54 processes take place, if it can be argued that the behaviour of the owner or lawful occupier is obstructive, for example - imposing unreasonable demands in return for access to the land.\nHowever, it may be the case that the Maranda matter is simply no longer applicable following the repeal of the previous section 5(4)(c) of the MPRDA, given the statement by the Constitutional Court that ‘[f]ollowing the repeal of section 5(4)(c), section 54 must be exhausted to ensure that the MPRDA’s purpose of balancing the rights of the mining right holders on the one hand and those of the surface right holder on the other is fulfilled’.\nThe effect of the Maledu judgment on right holders\nTaking the above into consideration, it would be advisable for a mining company which is a holder of a reconnaissance permission, prospecting right, mining right or mining permit to first exhaust the processes provided for in terms of section 54 as the Constitutional Court held in the Maledu matter that, ‘section 54 must be exhausted to ensure that the MPRDA’s purpose of balancing the rights of the mining right holders on the one hand and those of the surface rights holders on the other is fulfilled’ and that section 54 is a ‘speedy dispute resolution process and were mining operations allowed to continue before the section 54 processes have been finalised, it would defeat the purposes of section 54 and render the process redundant’.\nThis may be a bitter pill for mining companies to swallow, as effectively this judgment means the following:\n- if an owner or lawful occupier does not act unreasonably and seeks to negotiate with the holder of a reconnaissance permission, prospecting right, mining right or mining permit in a bona fide manner – placing, inter alia, unreasonable demands in return for access to the land in question, would potentially preclude a mining company which holds the relevant right under the MPRDA to resort to any common law remedies, such as interdicting the owner from obstructing the mining company from commencing its operations on the land in question; and\n- until the section 54 processes have been finalised, the mining company is not allowed to exercise its rights in terms of the relevant right in relation to the land in question.\nThe legal position explained above will only be the case in relation to owners and lawful occupiers’ conduct, and not in relation to the conduct of the holder of the relevant right – as the MPRDA already stipulates that ‘If the Regional Manager determines that the failure of the parties to reach an agreement or to resolve the dispute is due to the fault of the holder of the reconnaissance permission, prospecting right, mining right or mining permit, the Regional Manager may in writing prohibit such holder from commencing or continuing with prospecting or mining operations on the land in question until such time as the dispute has been resolved by arbitration or by a competent court’.\nIn other words, when owners and lawful occupiers act ‘reasonably’, mining should stop, but when holders of rights under the MPRDA act ‘reasonably’, it is not a given that mining can continue because of the Constitutional Court’s decision.\nThe inherent tension\nThe Constitutional Court explained in the Agri South Africa v Minister for Minerals and Energy matter that the MPRDA ‘had the deliberate and immediate effect of abolishing the entitlement to sterilise mineral rights’ as under the old mineral rights regime, landowners had the ability to sterilise mineral rights for as long as they wanted, if the right was not severed from landownership. As the Court pointed out, this position changed after the promulgation of the MPRDA as the MPRDA makes it clear that the custodianship of mineral and petroleum resources is vested in the state on behalf of the people of South Africa and that one of the objectives of the MPRDA was to preclude the sterilisation of mineral resources.\nHowever, the Maledu decision introduces a requirement that (where there is a dispute as set out in section 54) mining companies which are holders of reconnaissance permissions, prospecting rights, mining rights or mining permits must first complete the section 54 processes as set out in the MPRDA before it can resort to the enforcement of their common law remedies.\nThe fact that the section 54 is not regulated from a timing perspective and also that the Constitutional Court confirmed that mining may not continue during this period, potentially has the effect of sterilising the mineral resources until the finalisation of the section 54 processes, which could have detrimental implications for the holder of the relevant right.\nOn the other hand, the Constitutional Court also recognised the fact (in the context of the protection of informal land rights) that if mining may continue while the section 54 processes are proceeding, it may render the protections provided by section 54 nugatory. The same issue was raised by the applicants in the Baleni v Minister of Mineral Resources matter in the context of a section 96 appeal as section 96(2)(a) stipulates that an appeal in terms of section 96 does not suspend the relevant administrative decision (for example, the granting of a mining right). In this matter the applicants explained that ‘Whether the consent of the applicants is required in terms of IPILRA is central to the dispute and, while it may be possible for the applicants to review the eventual decision to grant a mining right without the applicants’ consent, the applicants fear that that it is possible that mining will commence whilst they pursue their internal remedies and possibly rendering the consent meaningless’.\nIt could be that the ultimate solution would be to amend the MPRDA to set out time periods to regulate the section 54 processes. This may to some extent lessen the inherent tension in seeking to ensure that mining can proceed as rapidly as possible (where rights have been granted) and at the same time affording appropriate protection for the owner or lawful occupier of the land in question.\nNina Greyling, attorney at Nortons Inc","Vertical Farming: Location a Key Factor to Success, Says IDTechEx\nVertical farming, the practice of growing crops indoors on vertically stacked layers, has received no small amount of interest over the last few years. Vertical farms commonly tout impressive numbers, such as using 95% less water and providing crop yields 20-30 times that of conventional agriculture. These claims, among many others, have seen many vertical farming start-ups being founded alongside large amounts of industry funding; funding for the industry reached a record high in 2021, with over US$1 billion being raised across the entire industry. The recent IDTechEx report, \"Vertical Farming 2022-2032\", details the economic and technological factors shaping this rapidly growing industry.\nWith crops being grown indoors under controlled environments, a selling point used by multiple vertical farms is that they can grow crops anywhere – even in the heart of a city. This has led to proponents of the industry envisioning \"smart cities\", where vertical farms in city skyscrapers help feed the urban population. While this is achievable in principle, the truth is that the choice of location for vertical farming is much more involved and intricate than it may appear from these claims alone. Choosing an ideal location can be one of the most important factors in determining the success of a vertical farm.\nSome vertical farms may choose to set up their facilities in pre-existing facilities, such as abandoned warehouses. In these cases, identifying the suitability of the venue is the first point of consideration: vertical farms are very energy intensive, and it is important to ensure the facilities chosen can support these energy loads. In addition, the ergonomics of the facility is also important; should the layout not be given proper consideration, this can impede workers and decrease worker efficiency. As labor costs are typically among the largest sources of expenditure for a vertical farm, improving labor efficiency to reduce these costs is of paramount importance.\nWhile growing crops in the center of a city may seem ideal, the reality is that this may be counterproductive. Obtaining and maintaining such a location is expensive and can contribute significantly to the operating expenditure of a vertical farm while presenting logistical challenges in distributing produce; the \"last mile\" of food distribution is often the hardest. Having a farm right next to the consumers themselves may also be less ideal than instead choosing a location near food distribution centers, as this allows for more efficient delivery of produce. As distribution centers are typically located on the outskirts of cities, the cost of land is also much cheaper. This is the approach chosen by UK-based Jones Food Company, which chose Scunthorpe as a location for its vertical farm – this is a relatively low-cost location located near food distribution centers and a network of motorways that could still reach many consumers in a day, even if it isn't right in the middle of the capital city. Vertical farms should carefully consider their place in the supply chain before establishing a base.\nOn a larger scale, vertical farms may prove more profitable in different geographical regions. Vertical farms can reduce water usage significantly over conventional agriculture, and the high degree of control over the growing environment allows them to grow crops in extreme climates – where such crops may not otherwise be able to grow. In return, vertical farms demand more energy to carry out growing operations. To maximize their potential, vertical farms would ideally be located in regions of water scarcity, such as Sub-Saharan Africa and the Middle East, or in areas with extreme climates, such as in Scandinavian countries, where the low amounts of sunlight and high costs of regulating greenhouse environments single out vertical farms as an optimal solution. The amount of agricultural land available is also an important factor – regions looking to increase food security and reduce reliance on imports while facing challenges in acquiring sufficient agricultural land would find vertical farms to be ideal. A particularly prominent example of such a country is Singapore, which has demonstrated much interest in vertical farming over the last few years.\nBeyond the considerations of water scarcity and temperature, the general availability of fresh produce and the distribution networks of given countries should also be considered. Vertical farms use the added freshness and higher quality of their crops as a primary selling point, but these are typically offset by higher prices. Should there already be a large supply of high-quality produce made available at lower costs, vertical farms will find it hard to distinguish their own produce and may struggle to establish a significant market share. The converse would also be true; should a country lack easy access to fresh produce, vertical farms are expected to see much demand for their produce. An example of such a region would be the Middle East: leafy greens typically travel several thousand miles to reach stores, resulting in consumers facing high prices and low-quality products. The high price of conventionally farmed leafy greens, alongside government subsidies, makes it easier for vertically farmed produce to approach price parity while providing much fresher, higher-quality products.\nWhile the choice of location is an important consideration, it is only one of many others that must be given proper thought. Only through proper optimization of growing operations to improve efficiency and reduce costs can vertical farms reach their true potential. In the IDTechEx report, \"Vertical Farming 2022-2032\", many further important factors for consideration are discussed in detail, and the future of vertical farming is evaluated through 10-year market forecasts.\nIDTechEx guides your strategic business decisions through its Research, Subscription and Consultancy products, helping you profit from emerging technologies. For more information, contact research@IDTechEx.com or visit www.IDTechEx.com.\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:44e22f92-f038-43db-8310-1ac6977896b9>","<urn:uuid:98c0eed7-8d91-42cd-b0d1-cc7c105f9a4d>"],"error":null}
{"question":"How to build safe stone block steps on a hillside with multiple landings? Need step-by-step guide for digging, spacing and safety standards.","answer":"Here's how to build stone block steps: 1) Start by digging at hill's bottom at 90-degree angle, making level surface as deep as stone block length. Make stairs 4-6 feet wide for two people to pass. 2) Spread and compact sand using power plate compactor to prevent uneven settling. 3) Lay stone blocks lengthwise front-to-back, ensuring 10-12 inches of tread space. Backfill with sand and gravel, then compact. 4) Create landings every 8 feet of rise - dig 6-8 inches deep, build form with lumber, fill with gravel, compact, add sand layer, compact again, then lay stone blocks in pattern. 5) Continue process until reaching top. Optional: add ornamental patio at top if space allows.","context":["Stone block steps are a beautiful addition on a hillside. They provide easier access from hilltop to ground level than a dirt or grass path and also prevent erosion. Stone blocks come in a variety of shapes and colors. You can create intriguing patterns and effects, including scalloped kick plates, curved landings and crenelated borders. A stone block staircase can be designed to draw the eye to a particular point in the yard, such as an ornamental planting, terrace garden or water feature. Block ends can be exposed, or they can be hidden by overhanging border flowers, softening the formality of the straight lines and rough surfaces.\nCheck with your local building department before designing your stairway. Obtain all necessary permits before proceeding. Decide whether you want your stone block steps to go straight up the hill, switch back and forth from one landing to another, or flow uphill in a gentle curve. A staircase with multiple landings is easier to climb and provides space for several mini patio gardens which you can landscape with flowers and ornamental shrubs. These mini gardens will draw song birds and beneficial insects such as butterflies, bees, ladybugs and praying mantis. A flowing, curved staircase or a switchback stair will both increase the aesthetic appeal of your hillside from the street. A straight stone block staircase, flowing uninterrupted up the hill, will require less labor and fewer materials, but will not increase the curb appeal of your hillside nearly as much.\nBegin digging at the bottom of the hillside. Cut into the hill at a 90-degree angle to make a level surface, as deep from front to back as the length of the stone blocks. Make each stair wide enough for two people to pass one another without touching, about 4 to 6 feet.\nSpread sand across the first cut into the hillside. Use a power plate compactor to compact the sand and dirt. This will prevent uneven settling after the installation of your stone block steps.\nLay each stone block with the length going from front to back on the stair. If you are using hexagonal, petal-shaped, pentagonal or square blocks, be sure they provide 10 to 12 inches of tread space. This is the minimum safe tread depth for a hillside stair. Backfill each step with sand and gravel. Use a power plate compactor again to ensure they will not settle later.\nRepeat this process for each step until you reach the first landing, or continue to the top if you are not including any landings at all. Reposition and compact blocks as needed before proceeding.\nCreate landings as needed each time you reach 8 feet of rise in your hillside steps. Lay stone block as you would for a patio. Dig down six to eight inches, the length and width of your landing. Build a form using 2 by 6 inch lumber and fill it in with gravel. Compact the gravel until level with the top edge of the form, then backfill form with sand and compact again. Lay your stone block in the desired pattern.\nLay an ornamental patio at the top if space permits."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:f3a013e4-f19a-43b2-ac73-07893a8f954b>"],"error":null}
{"question":"What is Lake Vostok in Antarctica and why is it scientifically significant?","answer":"Lake Vostok is the largest subglacial lake in Antarctica, comparable in size to Lake Ontario with an area of 15,000 km² and depths up to 1 kilometre. It is scientifically significant because its water is oversaturated with oxygen, exceeding normal freshwater lake levels by 50 times. The lake has been frozen for at least half a million years and likely contains a unique habitat of microorganisms, though this habitat has not yet been reached or investigated.","context":["Antarctica is unusual continent, little known to general people and comparatively little investigated. Here are missing numerous kinds of landmarks common in all other continents of the world - but this is compensated by several kinds of landmarks unique to Antarctica.\nMost impressive landmarks in Antarctica are created by nature. Part of them is temporary, such as the giant and visually impressive ice formations - sky-high ice cliffs with impossible colors, natural arches in ice, perennial meltwater falls over the blue ice walls and similar.\nDue to the extreme weather of Antarctica some monuments are unique to this continent - such as lakes with highly unusual water chemistry and fumarolic ice towers.\nFumarolic ice towers and other volcanic phenomena\n- Mount Berlin fumarolic ice towers - Marie Byrd Land. Near the rims of northern and western calderas have been noticed ice towers shaped by fumaroles.\n- Mount Erebus fumarolic ice towers - Ross Island. Constant effluxes of fumaroles have created hundreds of unusual ice towers, some up to 18 metres high.\n- Mount Erebus lava lake - Ross Island. One of the few (five) constant glowing lava lakes in the world.\n- Mount Melbourne fumarolic ice towers - Victoria Land. Up to 7 m high ice towers.\n- Mount Pond - Deception Island. Location with numerous active fumaroles and unique species of moss.\n- Don Juan Pond - Victoria Land. Small hypersaline lake, saltiest waterbody on Earth with salt content exceeding 40%. It does not freeze over in harsh Antarctic winters. Salt consists mainly of CaCl with some NaCl.\n- Kroner Lake - Deception Island. The only lagoon with hot springs (up to 70°C) in Antarctica, contains unique community of brackish water algae.\n- Lake Bonney - Victoria Land. Frozen lake with unique geochemistry. The east lobe of the lake is saturated with nitric oxide - \"laughing gas\". Another part of lake contains dimethylsulfoxide - possibly produced by microorganisms.\n- Lake Ellsworth - West Antarctica. Large lake which potentially contains unique life forms below the ice.\n- Lake Untersee - East Antarctica. Permanently frozen freshwater lake with extremely high pH - between 9.8 - 12.1. Lake is supersaturated with oxygen (150%), sediments of this lake may produce more methane than any other natural waterbody on Earth. Contains unique microorganisms.\n- Lake Vanda - Victoria Land. Lake with extremely clear water (and when it freezes - extremely lucid ice) because it is located in enclosed valley sealed off from the winds and thus does not get any dust. One of the saltiest natural waters in the world, containing mostly calcium chloride. Bottom layer is an enclosed hydrological system with specific chemical processes.\n- Lake Vida - Victoria Land. Lake is covered with at least 19 m thick layer of ice - thickest layer of non-glacial ice on Earth. Below the ice is hypersaline lake with unique microorganisms.\n- Lake Vostok - central part of continent. The largest subglacial lake in Antarctica, similar in size to Lake Ontario, up to 1 kilometre deep, with area 15,000 km². Lake water is oversaturated with oxygen, exceeding the level of oxygen in ordinary freshwater lakes 50 times. Lake has been frozen for half a million years at least and here most likely has been developed unique habitat of microorganisms. This habitat has not been reached and investigated yet.\n- Onyx River - Victoria Land at the Ross ice Shelf. The largest and longest river in Antarctica, meltwater stream flowing for a few months in summer. While flowing towards the Lake Vanda, it dissolves the salt in ground, gradually becoming saltwater flow.\n- Airdevronsix icefall - Victoria Land. Giant, approximately 5 km wide and 400 m tall icefall.\n- Blood Falls - Victoria Land. Unusual natural feature - outflow of hypersaline water, seeping through ice, tainted with iron oxides in blood color. This approximately 15 m tall fall provides insight into unique ecosystem which has been isolated from the outside world for 1.5 million years.\nUnique ecosystems and animal colonies\n- Geothermal areas of Cryptogam Ridge - East Antarctica. Here geothermal heat has been keeping ice-free areas of soil. Over the time in this very isolated area have developed endemic species of microorganisms, as well as mosses and liverworts.\n- Baily Head penguin colony - Deception Island. Largest colony of chinstrap penguins (Pygoscelis antarctica Forster, 1781) in the world, with more than 100,000 pairs.\n- Cape Adarie penguin colony (Cape Adare) - south-east part of Ross Sea. Largest single colony of Adelie penguins (Pygoscelis adeliae Hombron & Jacquinot, 1841) in the world, with some 250,000 breeding pairs.\n- Cape Washington colony of Emperor Penguins - at the Ross Sea. Largest colony of the largest penguin - emperor penguin (Aptenodytes forsteri Gray, 1844). Here are breeding 20 - 25 thousand pairs of this unusual bird.\n- Svarthamaren cliffs - Dronning Maud Land. Ice free cliffs with a colony of some 820,000 Antarctic petrels (Thalassoica antarctica J. F. Gmelin 1789). Largest bird colony in the inland of Antarctica, 200 km from the coast.\nOther unique natural landmarks\n- Campbell Glacier glacial caves - Victoria Land. At least 200 m long ice caves created by thermal processes on the contact of sea ice and glacier ice.\n- Katabatic winds at Commonwealth Bay - Commonwealth Bay. Windiest place on Earth with very frequent storms. The speed of wind here occasionally exceeds 90 m/s - 320 km/h. These specific winds are called katabatic winds which might last for days. Fast moving ice crystals may cause unusual electric effects.\n- Cape Renard - Antarctic Peninsula, Lemaire Channel. One of the most impressive vertical cliff mountains not only in Antarctica but in the world, a pinnacle of basalt and ice rising more than 700 m from the sea.\n- Dome A (Dome Argus) - East Antarctica. The highest ice feature in Antarctica (4,093 m), under the dome are Gamburtsev Mountains - mountain range larger than European Alps. Dome A has very dry and rather calm climate. Satellite data and climatic models show that this is the coldest place on Earth. Average winter temperature here is -70°C but theoretically it may reach -102°C. Exploration started by Chinese in 2005. Theoretically this is of the best suited location on Earth for space exploration with telescope - beating by far any of the locations used for space research today. Not too far is located Ridge A - place with the clearest sky on Earth and another potential contender for the coldest spot on Earth. One more contender for the coldest spot on Earth is Dome Valkyrie (Dome Fuji) where the coldest temperature -91.2 °C has been registered.\n- Erebus Crystals - Victoria Land. Deposit of large anorthoclase crystals in the center of caldera of volcano.\n- Icebergs at Pleneau and Booth islands - Antarctic Peninsula, between Pleneau and Booth Islands. Field of stranded icebergs (seen here for most time) of highly unusual forms and shapes. The icebergs are stranded on the ground, thus they can be safely examined while navigating among them.\n- McMurdo Dry Valleys - Antarctica. Another contender for the honour of driest place on Earth, this valley has not seen precipitation for more than thousands of years, moisture (some 30 mm per year) reaches it from the meltwater. Any moisture is removed from here by extremely powerful winds. Unique ecosystem of microorganisms.\n- Mount Kirkpatrick find of fossils - Queen Alexandra Range. One of the richest and most interesting fossil finds in Antarctica. Found remnants of several species of Jurassic dynosaurs.\n- Tsarsporten (Tsars Porten) - Peter I Island. Enormous natural arch, the only way to access the beach of Norvegiabukta - one of the few beaches of the island.\n- Vanderford Valley - located in the ocean at Wilkes Land. Undersea valley carved by glacier, reaches depth of 2,287 m. Possibly the deepest glacier-carved valley in the world.\nMan made landmarks\nThis extremely cold and inhospitable continent has very short human related history - Antarctica was first seen in 1820 and first building appeared here in 1899. It totally misses landmarks related to distant human past and has extremely sparse network of settlements also today. Nevertheless there are few man made landmarks worth mentioning:\nHuts of pioneers\nFour Antarctic Bases were created by pioneers, who arrived here before the First World War. Each of these bases contains valuable legacy to heroism of human race, they are the first buildings where humans lived on this continent.\n- Borchgrevingk's Huts - Cape Adarie, south-east of Ross Sea. Oldest buildings in Antarctica, two Norwegian prefabricated wooden huts from 1899.\n- Discovery Hut - Ross Island. Wooden hut, built by Robert Scott in 1902, one of the oldest buildings in Antarctica, with certain role in early history of exploration.\n- Shackleton's Hut - Cape Royds, Ross Island. Hut built by Ernest Shackleton and his team, Nimrod Expedition in 1908.\n- Scott's (Cape Evans) Hut - Cape Evans, Ross Island. Hut built by Robert Scott and team in 1911 before their tragic trip to South Pole.\nContemporary Antarctic architecture\nOver the last 100 years people have learned a lot about Antarctica and there have appeared more and more well adjusted buildings, able to persist in the extreme weather of Antarctica. Functional design is beautiful design - those stations, which have been developed with much effort and foresight, look very impressive. Some fine examples of true contemporary Antarctic architecture are:\n- SANAE IV - Queen Maud Land. Built in 1997 with innovative design for its time - a structure raised on stilts. This allows the snow to pass under it - otherwise the expensive station gets buried in snow soon. Stilts since this have been applied to other Antarctic stations as well.\n- Princess Elisabeth Base - Dronning Maud Land. Belgian polar station, built in 2009, the first zero-emission polar station, buildings of station have exceptional hi-tech architecture.\n- Neumayer III - Ekstrom Ice Shelf. The newest German Antarctic station of innovative, high technology design, built in 2009.\n- Halley VI Station - Brunt Ice Shelf. Polar Station of United Kingdom, under construction (2010 - 2013). Station of unique design (although nearly anything designed for Antarctica is unique) built on moving ice shelf and standing on enormous skis. Upon necessity this large structure can be pulled to new location. Station will form a long chain of separate modules, linked together.\nList of described attractions\nAntarctica has no formal division. Landmarks described by Wondermondo here are listed in alphabetic order.\n|Blood Falls||Waterfalls, Springs, Mineral springs, Ecosystems|\n|Dome A||Meteorological phenomena, Other natural landmarks|\n|Katabatic winds at Commonwealth Bay||Meteorological phenomena|\n|Ridge A||Meteorological phenomena|\nMap of Antarctica\nFeatured: Airdevronsix icefalls\nYou may think that you know all about the greatest natural landmarks of our planet... But have you heard about Airdevronsix icefall - 5 km wide and 400 m tall icefall?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0c447b63-f204-4af8-8d73-4422cdfbf83f>"],"error":null}
{"question":"How does Chinese financial market reform relate to foreign investment, and what data protection requirements must businesses consider when handling international transactions?","answer":"Chinese financial market reforms aim to open up the economy to foreign investors in capital and financial markets, which can broaden risk-sharing possibilities and bring new financial resources to support ventures in the Chinese economy. Rather than destabilizing the economy, these reforms can help provide buffers against economic turbulence, though they do introduce some foreign market uncertainties. Regarding data protection, businesses engaging in international transactions must comply with regulations like GDPR for EU individuals and CCPA for California residents. These regulations require explicit consent before collecting or processing data, transparency about data usage, and giving customers rights to access, correct, and delete their data. Businesses must also implement secure data storage systems with strong passwords and encryption while regularly auditing for vulnerabilities.","context":["As the second largest economy in the world, China has seen immense growth over the last 40 years, a result of more open markets and an innovative approach to advancing financial technologies. The Macro Finance Research Program at the University of Chicago’s Becker Friedman Institute for Economics and Tsinghua University School of Economics and Management co-hosted a two-day academic conference to explore the interplay between finance and macroeconomics in China. The conference ignited an important conversation on the inner workings of the Chinese economy, the key players in economic policy and challenges for future research related to the Chinese financial market. During the conference, sessions dug deeper into the history of China’s financial markets, Chinese state-owned enterprise reform, and the productive role of finance in supporting economic growth in the future.\nIn his opening remarks, Lars Peter Hansen, Nobel Prize recipient, David Rockefeller Distinguished Service Professor in Economics, University of Chicago, stressed why this was an ideal time to host this conference in China, “Given the economic interconnections around the world and the increasing importance of China in the world economy, it incumbent for academic economists understand better first, what is unique about the Chinese economy and its challenges in future and second, what lessons can learned from the experiences of other economies on their paths towards economic advancement. By bringing together top scholars with knowledge and expertise about finance and the macroeconomy in China, we hope to nurture future research in this area.”\nChong-En Bai, Mansfield Freeman Chair Professor, Department of Economics; Dean, Tsinghua School of Economics and Management echoed this sentiment: “After launching the Tsinghua-Chicago University Joint Research Center for Economics and Finance in last September 2018, this conference is the first joint event organized by two institutions. The Joint Research Center aims to encourage the research on the Chinese economy and just sent out the call for proposals to people who is doing the study on the economy and research. This conference will feature several experts study findings and share the insights of economic development from different perspectives.”\nProfessor Hansen mentioned during the interview, as a scholar, he hopes to hear more from outside of the academic community and conduct multi-level and comprehensive exchanges with professionals from the government, private sector and other fields, so to better frame future research on the economy of China and to extract insights pertinent to economics more generally. “I should say at the outset, I only know about these reforms in the broadest of terms and I haven’t studied all the specific details of it. But the basic aim I understand, is to open up the financial markets in the Chinese economy, to foreign investors in the capital and financial markets, which I view as overall a good aim and a good policy, and I really hope that it can play out in these terms.”\n“I don’t see this as necessarily destabilizing because in many respects. By allowing these foreign investors you might well be broadening the reach of risk-sharing possibilities of coping with uncertainty as well as bringing in new financial resources in the Chinese economy to help support new ventures.” So, Professor Hansen thought it can actually nurture growth and help to provide a buffer for some of the Chinese economy’s own turbulence but China does hold the door to some other source of uncertainty that comes from foreign markets. But overall, he thinks it’s very productive and is pleased to see the Chinese government pursuing such reforms.\nOver the past years, new finance including fintech, internet finance and blockchain have seen rapid growth, but also many challenges too. Thomas Sargent, Nobel Prize recipient and professor of economics at New York University, expressed, China’s really been a worldwide leader in applying artificial intelligence to internet funding and microfinance. It is important in terms of lowering costs of doing transactions. China is indeed at the forefront of the world in the field of fintech and has developed very rapidly. “There will be huge benefits to getting very poor people access to loans which they didn’t have. For example, in very poor villages, people who had ideas to start a business, but they couldn’t get the money need to get money from others. It is important to set up things in these villages where people could get a reputation for repaying even small loans. Because if they had a reputation for repaying, they could get the loans.” Mr. Sargent mentioned. “Ant Financial is doing this. It is with millions of people. China is the leader in this.”\nArtificial intelligence and machine learning could help to create this social credit system. But Professor Sargent reminded us to notice that using artificial intelligence to figure out who is creditworthy, and the like, is important to support financial transactions but at some point, it also raises problems about privacy. This isn’t special to China, it’s a problem worldwide. How do we on the one hand encourage this type of information sharing that’s critical to financial transactions while at the same time figure out ways to protect privacy? This is a tremendously challenging problem going forward.","The Role of Data Privacy in AI-Driven Lead Generation Strategies\nArtificial intelligence (AI) has revolutionized the way businesses generate leads, providing a more efficient and effective means of identifying potential customers. However, with the vast amounts of data involved in AI-driven lead generation, there are increasing concerns around data privacy. In this blog post, we will explore the role of data privacy in AI-driven lead generation strategies and discuss how businesses can ensure they are using customer data in an ethical and responsible manner.\nThe Importance of Data Privacy\nData privacy refers to the protection of personal information, such as names, email addresses, phone numbers, and other sensitive data. With the rise of AI-powered lead generation tools, businesses have access to vast amounts of customer data, which can be used to target potential customers more effectively. However, this data is also sensitive and must be protected to ensure customer trust.\nThe Role of GDPR and CCPA\nTwo regulations that have a significant impact on data privacy in AI-driven lead generation are the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). The GDPR applies to all businesses that handle data belonging to individuals in the European Union (EU), while the CCPA applies to businesses operating in California that handle the data of California residents.\nUnder these regulations, businesses must obtain explicit consent from customers before collecting, processing, or storing their data. They must also be transparent about what data they are collecting and how they will use it. Additionally, customers have the right to access, correct, and delete their data at any time.\nEnsuring Ethical and Responsible Use of Data\nTo ensure ethical and responsible use of customer data in AI-driven lead generation strategies, businesses can take several steps:\nBe Transparent about Data Use\nTo build trust with customers, businesses must be transparent about what data they are collecting and how they will use it. This includes providing clear and concise privacy policies that outline the purpose of data collection and how it will be stored and used.\nObtain Explicit Consent\nBusinesses must obtain explicit consent from customers before collecting and using their data. This can be achieved through opt-in forms or checkboxes that customers must select to indicate their consent.\nUse Data for Intended Purpose Only\nTo maintain trust, businesses must use customer data only for its intended purpose. For example, if a customer provides their email address to download a whitepaper, that email address should not be used for any other purpose, such as marketing emails, unless the customer has explicitly consented to such use.\nSecure Data Storage\nTo protect customer data, businesses must ensure that it is stored securely. This includes implementing strong passwords and encryption, as well as regularly auditing data storage systems for vulnerabilities.\nConclusion AI-driven lead generation strategies have the potential to transform the way businesses identify and engage with potential customers. However, the use of customer data in these strategies must be ethical and responsible, ensuring that customer data is protected and used only for its intended purpose. By being transparent about data use, obtaining explicit consent, using data for its intended purpose only, and securing data storage, businesses can leverage AI to generate leads while maintaining trust with their customers."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:96d86537-e8a0-4cda-b365-ac21f6e97cf1>","<urn:uuid:e5b7073c-83c9-48d1-9a1f-3b4c9f191a18>"],"error":null}
{"question":"In a formal dinner setting, should the salad be served before or after the main course?","answer":"In a formal dinner setting, the salad is typically served after the main course/entree, not before it as many might expect. This is part of the traditional formal dinner sequence.","context":["In today's hectic world of fast-food restaurants and TV dinners, it's easy to forget how to properly set the table for a formal dinner. While it may not be a skill you need often, occasions do still arise for which formal place settings are an absolute must. Learn the basics and you'll be ready to host (or attend!) any formal dinner with ease.\nFormal Place Setting Template\nPart 1 of 2: Arranging the Original Setting\n1Decide what courses you’re going to serve. The ultimate setting you provide your guests with will depend on which courses you decide to serve; a five or seven course meal is typical for a formal dinner. Decide on your menu, keeping in mind that the typical courses are served in the following order:\n- First course: Appetizer/shellfish\n- Second course: Soup\n- Third course: Fish\n- Fourth course: Roast\n- Fifth course: Game (for a 5 course meal, the fourth/fifth courses are combined as the entree of choice).\n- Sixth course: Salad (yes, salad really does come after the entree)\n- Seventh course: Dessert\n- Eighth course: Fruit, cheese, and coffee (optional)\n- Ninth Course: Nuts and raisins (optional).\n2Select your utensils and dishes. Prior to setting your table, you’ll need to make sure you have the proper utensils and dishes prepared. You’ll need one fork for each of the meat dishes (a seafood fork should be used for a seafood appetizer), a spoon for the soup and dessert, knives for the entree, butter, and fish (if served), a charger, a dish for the butter/bread, and a selection of glasses (a water goblet, glass for white wine, glass for red wine, and a champagne flute are all options).\n- Each course is brought out of the kitchen on its own dish, so don’t worry about providing the dishes in the setting.\n- Prepare cloth napkins with napkin rings as an additional decorative element on the table.\n3Set the dishes. The centerpiece of the place setting is the charger. This is a large dish that goes beneath each of the plates the courses are brought out on. The charger will remain on the table until after the entree has been consumed, and then it should be removed along with the entree plate. Place the charger in the center of each setting. The second dish that you should have is the butter/bread dish. This will be placed up and to the left of the charger.\n- When you remove plates before the entree, leave the charger and take just the empty plates.\n- You should have an assortment of bread for your guests to eat, which is the purpose of the bread/butter dish.\n- Your cloth napkin should be placed on top of the charger.\n4Set your utensils. Although having three forks, two knives, and two spoons may seem like a frightening prospect, their placement is actually quite simple. With you utensils, you use them from the outside in. So, on the left side of the charger, you should have your fish fork > salad fork > entree fork. On the right side of your charger, you will place your dinner knife > fish knife > soup spoon. Above your plate aligned horizontally, you should place your dessert spoon and optional dessert fork. The butter knife should be placed diagonally across the butter/bread dish.\n- Each utensil will be removed from the table once it has been used.\n- If you’re not serving fish, then there is no need to place a fish fork and fish knife on the table.\n- If you serve shellfish as an appetizer, a shellfish fork should be placed to the right side of the soup spoon. This is the only fork that may be placed on the right side of the table.\n- Each of the utensils should be evenly spaced from each other and the charger.\n5Set your glasses. The glasses you choose to use will vary on what you are serving with dinner. Traditionally, there is at least a water goblet and a glass for wine, but this may vary. Place the water goblet directly above the knife, level with the bread/butter dish. Add your wine glass to the right of that, typically above the soup spoon. If you add a third wine glass (for a different kind of wine), place it above and between the water glass and the first wine glass. An optional champagne flute can be included as well, and should be placed above and to the right of the first wine glass.\n- Similar to the utensils, your glasses should be placed in order of use.\n- Water is often served already in the glass, while wine and champagne are poured during the courses.\n- If you choose to serve coffee (as in a nine course meal), the coffee should be brought out in a demi-tasse (a type of espresso cup) at the end, and removed with the fruit/cheese plates.\nPart 2 of 2: Adjusting the Table Setting for Each Course\n1Set the table for soup. For the first course of soup, there are two options: bring bowls of the same soup from the kitchen, or offer water or cream based soups and serve them in fresh dishes at the table. The former is served already poured in bowls and brought out from the kitchen. The latter is served (carefully) at the table into clean bowls. The soup bowls should be brought in on serving dishes in case of a spill. When everyone has finished eating their soup, the soup spoons should be placed (bowl-side up) on the right side of their bowl, on the serving plate.\n- The plate, bowl, and spoon should be removed from the table after the first course.\n- The bread and butter dish should remain on the table, even if they were used with the soup.\n2Set the table for fish. With the soup removed, the fish course should be brought in on its own dish. This should be placed on the charger, and eaten with the fish knife and fish fork (the utensils furthest from the charger on either side). When the fish has been consumed, the fish fork and fish knife should be placed diagonally across the dish, with the handles of each placed on the ‘4:00’ mark as if the plate were a clock.\n3Set the table for the main course. The main course should be brought out on a large plate that has been pre-warmed. This should go on the charger, and is eaten with the dinner fork and knife. When everyone is finished with the entree, the plate can be removed along with the charger, the dinner fork, and the knife. The knife and fork are typically placed diagonally across the plate, in a similar fashion to the utensils used for the fish.\n4Set the table for the salad. The salad is typically eaten after the entree in a formal dinner. With the charger removed, place the salad plate in the center of the setting. This should be eaten with the last remaining fork. When the salad course is finished, the salad plate, salad fork, and bread/butter dish with the butter knife, and the wine/champagne glasses should all be removed. All that should be left is the water goblet and the dessert spoon (and optional dessert fork).\n5Set the table for dessert. The final course of the evening is typically dessert and coffee, unless you’re serving a very formal nine course dinner. Regardless, the desert should be brought in on a plate and placed in the center of the setting, and a demi-tasse or teacup should be placed to the right of it below the water goblet with a teaspoon. Cream and sugar may be placed on the table for use in the coffee or tea, if desired. When dessert is completed, all the dishes should be removed, leaving a bare table.Ad\nAre you an expert on living abroad?\nIf so, share what you know about how to move abroad.\nThanks for sharing your knowledge.\nWe are a bit smarter today because of you.\n- Choose low centerpieces. You don't want to obstruct guests' views of each other or their conversations.\n- In all but the most formal settings, don't be afraid to mix and match if you don't have enough matching tableware. Mixing and matching has become increasingly popular.\n- The most important thing to keep in mind when setting a table is to make sure your guests will be comfortable. With the increase in more casual dining, it's fun to pull out all the stops and set a formal table. Don't, however, lose sight of your guests' comfort and your own fun (that's often why we entertain). If you don't have all the trappings of a formal table, you can rent items or have some fun and improvise. Some of the best looking tables are the result of improvisation and using unexpected items.\nCategories: Dinner Parties\nRecent edits by: Allie, Darahberserakan, Ciccio Veronese\nIn other languages:\nDeutsch: Wie man einen Tisch für ein formelles Abendessen deckt, Español: Cómo arreglar la mesa para una cena formal, Italiano: Come Apparecchiare la Tavola per un Evento Formale, Français: Comment dresser une table pour un diner officiel, Português: Como Arrumar a Mesa Para um Jantar Formal, Русский: сервировать стол для официального обеда, 中文: 为正式的晚餐摆设餐桌, Nederlands: Tafeldekken voor een formeel diner\nThanks to all authors for creating a page that has been read 266,135 times."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:ec83feb2-a166-4884-bc95-cb9f71f952a4>"],"error":null}